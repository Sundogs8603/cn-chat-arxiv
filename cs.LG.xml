<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25511;&#21046;&#20026;&#20013;&#24515;&#30340;&#35270;&#39057;&#39044;&#27979;&#22522;&#20934;&#65292;&#35780;&#20272;&#32473;&#23450;&#27169;&#22411;&#22312;&#36890;&#36807;&#37319;&#26679;&#35268;&#21010;&#23545;&#27169;&#25311;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#34920;&#29616;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;&#26377;11&#20010;&#20219;&#21153;&#31867;&#21035;&#21644;310&#20010;&#20219;&#21153;&#23454;&#20363;&#23450;&#20041;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#20197;&#21450;&#23436;&#25972;&#30340;&#35268;&#21010;&#23454;&#29616;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25351;&#26631;&#22312;&#39044;&#27979;&#20219;&#21153;&#25191;&#34892;&#25104;&#21151;&#26041;&#38754;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13723</link><description>&lt;p&gt;
&#19968;&#31181;&#20197;&#25511;&#21046;&#20026;&#20013;&#24515;&#30340;&#35270;&#39057;&#39044;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Control-Centric Benchmark for Video Prediction. (arXiv:2304.13723v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25511;&#21046;&#20026;&#20013;&#24515;&#30340;&#35270;&#39057;&#39044;&#27979;&#22522;&#20934;&#65292;&#35780;&#20272;&#32473;&#23450;&#27169;&#22411;&#22312;&#36890;&#36807;&#37319;&#26679;&#35268;&#21010;&#23545;&#27169;&#25311;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#34920;&#29616;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;&#26377;11&#20010;&#20219;&#21153;&#31867;&#21035;&#21644;310&#20010;&#20219;&#21153;&#23454;&#20363;&#23450;&#20041;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#20197;&#21450;&#23436;&#25972;&#30340;&#35268;&#21010;&#23454;&#29616;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25351;&#26631;&#22312;&#39044;&#27979;&#20219;&#21153;&#25191;&#34892;&#25104;&#21151;&#26041;&#38754;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#26159;&#23398;&#20064;&#19990;&#30028;&#21160;&#24577;&#27169;&#22411;&#30340;&#20307;&#29616;&#20195;&#29702;&#20154;&#30340;&#26377;&#24076;&#26395;&#30340;&#30693;&#35782;&#26469;&#28304;&#12290;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#22312;&#33258;&#25105;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36234;&#26469;&#36234;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#35270;&#39057;&#25968;&#25454;&#65292;&#35780;&#20272;&#22522;&#20110;&#20154;&#31867;&#24863;&#30693;&#30456;&#20284;&#24615;&#25110;&#20687;&#32032;&#27604;&#36739;&#30340;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25351;&#26631;&#26159;&#21542;&#20934;&#30830;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#34920;&#29616;&#20173;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#20110;&#35268;&#21010;&#26426;&#22120;&#20154;&#25805;&#20316;&#26469;&#35828;&#65292;&#29616;&#26377;&#25351;&#26631;&#22312;&#39044;&#27979;&#20219;&#21153;&#25191;&#34892;&#25104;&#21151;&#26041;&#38754;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#21160;&#26465;&#20214;&#19979;&#30340;&#35270;&#39057;&#39044;&#27979;&#22522;&#20934;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#35268;&#21010;&#23545;&#32473;&#23450;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#25511;&#21046;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35270;&#35273;&#35268;&#21010;&#30340;&#35270;&#39057;&#39044;&#27979; ($VP^2$)&#65292;&#21253;&#25324;11&#20010;&#20219;&#21153;&#31867;&#21035;&#21644;310&#20010;&#20219;&#21153;&#23454;&#20363;&#23450;&#20041;&#30340;&#27169;&#25311;&#29615;&#22659;&#12289;&#23436;&#25972;&#30340;&#35268;&#21010;&#23454;&#29616;&#21644;&#21253;&#21547;&#33050;&#26412;&#20132;&#20114;&#36712;&#36857;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video is a promising source of knowledge for embodied agents to learn models of the world's dynamics. Large deep networks have become increasingly effective at modeling complex video data in a self-supervised manner, as evaluated by metrics based on human perceptual similarity or pixel-wise comparison. However, it remains unclear whether current metrics are accurate indicators of performance on downstream tasks. We find empirically that for planning robotic manipulation, existing metrics can be unreliable at predicting execution success. To address this, we propose a benchmark for action-conditioned video prediction in the form of a control benchmark that evaluates a given model for simulated robotic manipulation through sampling-based planning. Our benchmark, Video Prediction for Visual Planning ($VP^2$), includes simulated environments with 11 task categories and 310 task instance definitions, a full planning implementation, and training datasets containing scripted interaction traje
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#31232;&#30095;&#21270;&#26041;&#27861;&#22312;&#27169;&#22411;&#26063;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#24133;&#24230;&#20462;&#21098;&#26041;&#27861;&#20248;&#20110;&#21464;&#20998;&#20002;&#22833;&#26041;&#27861;&#65292;&#38500;&#20102;&#39640;&#20110;80%&#30340;&#36739;&#39640;&#31232;&#30095;&#21270;&#27604;&#29575;&#20043;&#22806;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#38750;&#24120;&#31283;&#20581;&#12290;</title><link>http://arxiv.org/abs/2304.13718</link><description>&lt;p&gt;
&#31232;&#30095;&#21270;&#27169;&#22411;&#21160;&#29289;&#22253;&#21452;&#32990;&#32974;&#65306;&#30740;&#31350;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26063;&#30340;&#34892;&#20026;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sparsified Model Zoo Twins: Investigating Populations of Sparsified Neural Network Models. (arXiv:2304.13718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#31232;&#30095;&#21270;&#26041;&#27861;&#22312;&#27169;&#22411;&#26063;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#24133;&#24230;&#20462;&#21098;&#26041;&#27861;&#20248;&#20110;&#21464;&#20998;&#20002;&#22833;&#26041;&#27861;&#65292;&#38500;&#20102;&#39640;&#20110;80%&#30340;&#36739;&#39640;&#31232;&#30095;&#21270;&#27604;&#29575;&#20043;&#22806;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#38750;&#24120;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#35268;&#27169;&#22686;&#38271;&#65292;&#27169;&#22411;&#31232;&#30095;&#21270;&#20197;&#20943;&#23569;&#27169;&#22411;&#25512;&#29702;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#20869;&#23384;&#38656;&#27714;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#21644;&#29983;&#20135;&#30340;&#20851;&#38190;&#20852;&#36259;&#12290;&#34429;&#28982;&#35768;&#22810;&#31232;&#30095;&#21270;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#20010;&#20307;&#27169;&#22411;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20204;&#30340;&#34892;&#20026;&#21644;&#40065;&#26834;&#24615;&#23578;&#26410;&#22312;&#22823;&#37327;&#27169;&#22411;&#26063;&#19978;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#20004;&#31181;&#27969;&#34892;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#27169;&#22411;&#26063;&#65288;&#25152;&#35859;&#30340;&#27169;&#22411;&#21160;&#29289;&#22253;&#65289;&#19978;&#65292;&#21019;&#24314;&#21407;&#22987;&#21160;&#29289;&#22253;&#30340;&#31232;&#30095;&#21270;&#29256;&#26412;&#65292;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#27599;&#20010;&#21160;&#29289;&#22253;&#20013;&#30340;&#34920;&#29616;&#65292;&#36880;&#23618;&#27604;&#36739;&#31232;&#30095;&#21270;&#65292;&#24182;&#20998;&#26512;&#21407;&#22987;&#26063;&#32676;&#21644;&#31232;&#30095;&#21270;&#21518;&#26063;&#32676;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38500;&#20102;&#39640;&#20110;80%&#30340;&#36739;&#39640;&#31232;&#30095;&#21270;&#27604;&#29575;&#20043;&#22806;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#38750;&#24120;&#31283;&#20581;&#65292;&#32780;&#24133;&#24230;&#20462;&#21098;&#33021;&#22815;&#20248;&#20110;&#21464;&#20998;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#31232;&#30095;&#21270;&#27169;&#22411;&#19982;&#21407;&#22987;&#27169;&#22411;&#39640;&#24230;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
With growing size of Neural Networks (NNs), model sparsification to reduce the computational cost and memory demand for model inference has become of vital interest for both research and production. While many sparsification methods have been proposed and successfully applied on individual models, to the best of our knowledge their behavior and robustness has not yet been studied on large populations of models. With this paper, we address that gap by applying two popular sparsification methods on populations of models (so called model zoos) to create sparsified versions of the original zoos. We investigate the performance of these two methods for each zoo, compare sparsification layer-wise, and analyse agreement between original and sparsified populations. We find both methods to be very robust with magnitude pruning able outperform variational dropout with the exception of high sparsification ratios above 80%. Further, we find sparsified models agree to a high degree with their origin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#31639;&#27861;ARM-AE&#65292;&#21487;&#22312;&#20445;&#35777;&#35268;&#21017;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#65292;&#25366;&#25496;&#20986;&#39640;&#25903;&#25345;&#24230;&#21644;&#32622;&#20449;&#24230;&#35268;&#21017;&#38598;&#65292;&#24182;&#20855;&#26377;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.13717</link><description>&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#30340;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Association Rules Mining with Auto-Encoders. (arXiv:2304.13717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#31639;&#27861;ARM-AE&#65292;&#21487;&#22312;&#20445;&#35777;&#35268;&#21017;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#65292;&#25366;&#25496;&#20986;&#39640;&#25903;&#25345;&#24230;&#21644;&#32622;&#20449;&#24230;&#35268;&#21017;&#38598;&#65292;&#24182;&#20855;&#26377;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#26159;&#25968;&#25454;&#25366;&#25496;&#20013;&#30740;&#31350;&#26368;&#24191;&#27867;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#24212;&#29992;&#39046;&#22495;&#20174;&#26434;&#36135;&#31726;&#38382;&#39064;&#21040;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#31995;&#32479;&#22343;&#26377;&#28041;&#21450;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#31639;&#27861;&#23384;&#22312;&#22810;&#20010;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#25191;&#34892;&#26102;&#38388;&#21644;&#29983;&#25104;&#35268;&#21017;&#25968;&#26041;&#38754;&#12290;&#36817;&#21313;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#20248;&#21270;&#38382;&#39064;&#65292;&#22914;&#20998;&#31867;&#12289;&#22238;&#24402;&#25110;&#32858;&#31867;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#25366;&#25496;&#20851;&#32852;&#35268;&#21017;&#26041;&#38754;&#20173;&#28982;&#27809;&#26377;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#31639;&#27861;ARM-AE&#65292;&#24182;&#20197;&#19977;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#20026;&#20363;&#23558;&#20854;&#19982;FP-Growth&#21644;NSGAII&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20445;&#25345;&#25152;&#20135;&#29983;&#30340;&#35268;&#21017;&#38598;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#25366;&#25496;&#20986;&#39640;&#25903;&#25345;&#24230;&#21644;&#32622;&#20449;&#24230;&#35268;&#21017;&#38598;&#65292;&#24182;&#20855;&#26377;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Association rule mining is one of the most studied research fields of data mining, with applications ranging from grocery basket problems to explainable classification systems. Classical association rule mining algorithms have several limitations, especially with regards to their high execution times and number of rules produced. Over the past decade, neural network solutions have been used to solve various optimization problems, such as classification, regression or clustering. However there are still no efficient way association rules using neural networks. In this paper, we present an auto-encoder solution to mine association rule called ARM-AE. We compare our algorithm to FP-Growth and NSGAII on three categorical datasets, and show that our algorithm discovers high support and confidence rule set and has a better execution time than classical methods while preserving the quality of the rule set produced.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;LLMs&#30340;&#20351;&#29992;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.13712</link><description>&lt;p&gt;
&#21457;&#25381;LLMs&#22312;&#23454;&#36341;&#20013;&#30340;&#21147;&#37327;&#65306;ChatGPT&#21450;&#20854;&#24212;&#29992;&#30340;&#32508;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;LLMs&#30340;&#20351;&#29992;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20174;&#20107;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#23454;&#29992;&#30340;&#25351;&#21335;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;Large Language Models&#65288;LLMs&#65289;&#12290;&#25105;&#20204;&#20174;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;LLMs&#30340;&#20351;&#29992;&#35752;&#35770;&#21644;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24403;&#21069;&#30340;GPT&#21644;BERT&#26679;&#24335;&#30340;LLMs&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38750;&#20351;&#29992;&#24773;&#20917;&#65292;&#20363;&#22914;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12289;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12289;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12289;&#32039;&#24613;&#33021;&#21147;&#20197;&#21450;&#29305;&#23450;&#20219;&#21153;&#30340;&#32771;&#34385;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#21508;&#31181;&#20351;&#29992;&#21644;&#38750;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#35828;&#26126;LLMs&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#23454;&#38469;&#24212;&#29992;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#35797;&#22270;&#20102;&#35299;&#25968;&#25454;&#23545;&#20110;LLMs&#24212;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of dat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24072;&#29983;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;Hopfield&#27169;&#22411;&#65292;&#33021;&#22815;&#24110;&#21161;&#26426;&#22120;&#21033;&#29992;&#32467;&#26500;&#21270;&#30340;&#27169;&#24335;&#26469;&#23398;&#20064;&#65292;&#34429;&#28982;&#19968;&#20123;&#26465;&#20214;&#23545;&#20110;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#36825;&#31181;&#23398;&#20064;&#27169;&#24335;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.13710</link><description>&lt;p&gt;
&#24102;&#31181;&#26893;&#27169;&#24335;&#30340;Hopfield&#27169;&#22411;&#65306;&#19968;&#31181;&#24072;&#29983;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hopfield model with planted patterns: a teacher-student self-supervised learning model. (arXiv:2304.13710v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24072;&#29983;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;Hopfield&#27169;&#22411;&#65292;&#33021;&#22815;&#24110;&#21161;&#26426;&#22120;&#21033;&#29992;&#32467;&#26500;&#21270;&#30340;&#27169;&#24335;&#26469;&#23398;&#20064;&#65292;&#34429;&#28982;&#19968;&#20123;&#26465;&#20214;&#23545;&#20110;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#36825;&#31181;&#23398;&#20064;&#27169;&#24335;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Hopfield&#32593;&#32476;&#34987;&#35748;&#20026;&#26159;&#35760;&#24518;&#23384;&#20648;&#21644;&#26816;&#32034;&#30340;&#20856;&#22411;&#27169;&#22411;&#65292;&#20294;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20855;&#26377;&#32467;&#26500;&#21270;&#27169;&#24335;&#30340;Hopfield&#27169;&#22411;&#30340;&#36866;&#24403;&#25512;&#24191;&#26469;&#26500;&#24314;Boltzmann&#26426;&#30340;&#24072;&#29983;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#33258;&#26059;&#21464;&#37327;&#26159;&#26426;&#22120;&#26435;&#37325;&#65292;&#27169;&#24335;&#23545;&#24212;&#20110;&#35757;&#32451;&#38598;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#30456;&#22270;&#26469;&#20998;&#26512;&#23398;&#20064;&#24615;&#33021;&#65292;&#36825;&#20123;&#30456;&#22270;&#26159;&#36890;&#36807;&#35757;&#32451;&#38598;&#22823;&#23567;&#12289;&#25968;&#25454;&#38598;&#22122;&#22768;&#21644;&#25512;&#26029;&#28201;&#24230;&#65288;&#21363;&#26435;&#37325;&#27491;&#21017;&#21270;&#65289;&#26469;&#26500;&#24314;&#30340;&#12290;&#20351;&#29992;&#23567;&#32780;&#23500;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#65292;&#26426;&#22120;&#21487;&#20197;&#36890;&#36807;&#35760;&#24518;&#26469;&#23398;&#20064;&#12290;&#20351;&#29992;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#65292;&#21017;&#38656;&#35201;&#22823;&#37327;&#30340;&#31034;&#20363;&#25968;&#20197;&#36229;&#36807;&#20020;&#30028;&#38408;&#20540;&#12290;&#22312;&#36825;&#20010;&#21306;&#22495;&#65292;&#31995;&#32479;&#30340;&#23384;&#20648;&#38480;&#21046;&#25104;&#20026;&#20135;&#29983;&#19968;&#31181;&#23398;&#20064;&#27169;&#24335;&#30340;&#26426;&#20250;&#65292;&#22312;&#36825;&#31181;&#27169;&#24335;&#19979;&#65292;&#31995;&#32479;&#21487;&#20197;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Hopfield networks are known as paradigmatic models for memory storage and retrieval, modern artificial intelligence systems mainly stand on the machine learning paradigm. We show that it is possible to formulate a teacher-student self-supervised learning problem with Boltzmann machines in terms of a suitable generalization of the Hopfield model with structured patterns, where the spin variables are the machine weights and patterns correspond to the training set's examples. We analyze the learning performance by studying the phase diagram in terms of the training set size, the dataset noise and the inference temperature (i.e. the weight regularization). With a small but informative dataset the machine can learn by memorization. With a noisy dataset, an extensive number of examples above a critical threshold is needed. In this regime the memory storage limits of the system becomes an opportunity for the occurrence of a learning regime in which the system can generalize.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#39640;&#31471;&#26426;&#22120;&#20154;&#12289;&#20934;&#30830;&#30340;&#20256;&#24863;&#22120;&#25110;&#31934;&#24515;&#30340;&#26657;&#20934;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31934;&#32454;&#21452;&#25163;&#25805;&#20316;&#30340;&#27169;&#20223;&#23398;&#20064;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;Action Chunking with Transformers (ACT)&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#20010;&#24102;&#26377;&#21160;&#20316;&#24207;&#21015;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#32780;&#20801;&#35768;&#26426;&#22120;&#20154;&#23398;&#20064;&#22797;&#26434;&#30340;&#21452;&#25163;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2304.13705</link><description>&lt;p&gt;
&#20351;&#29992;&#20302;&#25104;&#26412;&#30828;&#20214;&#23398;&#20064;&#31934;&#32454;&#30340;&#21452;&#25163;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware. (arXiv:2304.13705v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#39640;&#31471;&#26426;&#22120;&#20154;&#12289;&#20934;&#30830;&#30340;&#20256;&#24863;&#22120;&#25110;&#31934;&#24515;&#30340;&#26657;&#20934;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31934;&#32454;&#21452;&#25163;&#25805;&#20316;&#30340;&#27169;&#20223;&#23398;&#20064;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;Action Chunking with Transformers (ACT)&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#20010;&#24102;&#26377;&#21160;&#20316;&#24207;&#21015;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#32780;&#20801;&#35768;&#26426;&#22120;&#20154;&#23398;&#20064;&#22797;&#26434;&#30340;&#21452;&#25163;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#36827;&#34892;&#31934;&#32454;&#25805;&#32437;&#20219;&#21153;&#65292;&#22914;&#31359;&#32518;&#24102;&#25110;&#25554;&#20837;&#30005;&#27744;&#20043;&#31867;&#30340;&#20219;&#21153;&#65292;&#19968;&#30452;&#26159;&#38590;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#31934;&#30830;&#30340;&#21327;&#35843;&#12289;&#32039;&#38381;&#22238;&#36335;&#35270;&#35273;&#21453;&#39304;&#12290;&#36890;&#24120;&#38656;&#35201;&#39640;&#31471;&#26426;&#22120;&#20154;&#12289;&#20934;&#30830;&#30340;&#20256;&#24863;&#22120;&#25110;&#31934;&#24515;&#30340;&#26657;&#20934;&#65292;&#36825;&#21487;&#33021;&#26159;&#26114;&#36149;&#19988;&#38590;&#20197;&#23433;&#35013;&#12290;&#26426;&#22120;&#23398;&#20064;&#26159;&#21542;&#33021;&#22815;&#20351;&#20302;&#25104;&#26412;&#21644;&#19981;&#31934;&#30830;&#30340;&#30828;&#20214;&#20063;&#33021;&#25191;&#34892;&#36825;&#20123;&#31934;&#32454;&#25805;&#32437;&#20219;&#21153;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#31995;&#32479;&#65292;&#30452;&#25509;&#20174;&#20351;&#29992;&#33258;&#23450;&#20041;&#36828;&#31243;&#25805;&#20316;&#30028;&#38754;&#25910;&#38598;&#30340;&#30495;&#23454;&#28436;&#31034;&#20013;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#27169;&#20223;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#27169;&#20223;&#23398;&#20064;&#26412;&#36523;&#20063;&#23384;&#22312;&#30528;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#31934;&#24230;&#39046;&#22495;&#65306;&#31574;&#30053;&#20013;&#30340;&#38169;&#35823;&#21487;&#33021;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#19981;&#26029;&#32047;&#31215;&#65292;&#24182;&#19988;&#20154;&#31867;&#28436;&#31034;&#21487;&#33021;&#26159;&#38750;&#38745;&#24577;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;Action Chunking with Transformers (ACT)&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#20010;&#24102;&#26377;&#21160;&#20316;&#24207;&#21015;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;ACT&#20801;&#35768;&#26426;&#22120;&#20154;&#23398;&#20064;&#22797;&#26434;&#30340;&#21452;&#25163;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers (ACT), which learns a generative model over action sequences. ACT allows the robot to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25968;&#25454;&#21644;&#27169;&#22411;&#39537;&#21160;&#25216;&#26415;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#20272;&#35745;&#30005;&#27744;&#20581;&#24247;&#29366;&#20917;&#12290;&#36890;&#36807;&#36882;&#24402;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#23398;&#20064;&#21442;&#25968;&#21160;&#24577;&#65292;&#24182;&#19988;&#23545;&#38388;&#38553;&#21644;&#21464;&#21270;&#30340;&#25805;&#20316;&#26465;&#20214;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13666</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#23398;&#20064;&#30005;&#27744;&#27169;&#22411;&#21442;&#25968;&#21160;&#24577;&#30340;&#36882;&#24402;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Learning battery model parameter dynamics from data with recursive Gaussian process regression. (arXiv:2304.13666v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25968;&#25454;&#21644;&#27169;&#22411;&#39537;&#21160;&#25216;&#26415;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#20272;&#35745;&#30005;&#27744;&#20581;&#24247;&#29366;&#20917;&#12290;&#36890;&#36807;&#36882;&#24402;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#23398;&#20064;&#21442;&#25968;&#21160;&#24577;&#65292;&#24182;&#19988;&#23545;&#38388;&#38553;&#21644;&#21464;&#21270;&#30340;&#25805;&#20316;&#26465;&#20214;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27744;&#31649;&#29702;&#31995;&#32479;&#20013;&#20272;&#35745;&#20581;&#24247;&#29366;&#24577;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#21151;&#33021;&#65292;&#20294;&#30001;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#25805;&#20316;&#26465;&#20214;&#21644;&#20351;&#29992;&#35201;&#27714;&#30340;&#21464;&#21270;&#24615;&#65292;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#31561;&#25928;&#30005;&#36335;&#27169;&#22411;&#30340;&#25216;&#26415;&#22312;&#24615;&#33021;&#26497;&#31471;&#21644;&#38271;&#26399;&#32769;&#21270;&#26102;&#21487;&#33021;&#20986;&#29616;&#19981;&#20934;&#30830;&#24615;&#25110;&#21442;&#25968;&#20272;&#35745;&#19981;&#31283;&#23450;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#26415;&#21463;&#38480;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#20043;&#22806;&#30340;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25968;&#25454;&#21644;&#27169;&#22411;&#39537;&#21160;&#25216;&#26415;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#20272;&#35745;&#30005;&#27744;&#20581;&#24247;&#29366;&#20917;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#23558;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#20026;&#29366;&#24577;&#12289;&#25805;&#20316;&#26465;&#20214;&#21644;&#23551;&#21629;&#30340;&#20989;&#25968;&#12290;&#36890;&#36807;&#19968;&#31181;&#36882;&#24402;&#26041;&#27861;&#30830;&#20445;&#35745;&#31639;&#25928;&#29575;&#65292;&#24471;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#32852;&#21512;&#29366;&#24577;-&#21442;&#25968;&#20272;&#35745;&#22120;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21442;&#25968;&#21160;&#24577;&#65292;&#24182;&#19988;&#23545;&#38388;&#38553;&#21644;&#21464;&#21270;&#30340;&#25805;&#20316;&#26465;&#20214;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating state of health is a critical function of a battery management system but remains challenging due to the variability of operating conditions and usage requirements of real applications. As a result, techniques based on fitting equivalent circuit models may exhibit inaccuracy at extremes of performance and over long-term ageing, or instability of parameter estimates. Pure data-driven techniques, on the other hand, suffer from lack of generality beyond their training dataset. In this paper, we propose a hybrid approach combining data- and model-driven techniques for battery health estimation. Specifically, we demonstrate a Bayesian data-driven method, Gaussian process regression, to estimate model parameters as functions of states, operating conditions, and lifetime. Computational efficiency is ensured through a recursive approach yielding a unified joint state-parameter estimator that learns parameter dynamics from data and is robust to gaps and varying operating conditions. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13646</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#29992;&#20110;&#24102;&#21327;&#21464;&#20449;&#24687;&#30340;&#38543;&#26426;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Data-driven Piecewise Affine Decision Rules for Stochastic Programming with Covariate Information. (arXiv:2304.13646v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24102;&#21327;&#21464;&#20449;&#24687;&#30340;&#38543;&#26426;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;(PADR)&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#26041;&#27861;&#65292;&#26088;&#22312;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;PADR&#30340;ERM&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#19968;&#33268;&#24615;&#32467;&#26524;&#65292;&#21487;&#29992;&#20110;&#26080;&#32422;&#26463;&#38382;&#39064;&#65292;&#20197;&#21450;&#32422;&#26463;&#38382;&#39064;&#30340;&#28176;&#36817;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;ERM&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#38543;&#26426;&#20027;&#23548;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#27839;&#65288;&#22797;&#21512;&#24378;&#65289;&#26041;&#21521;&#31283;&#23450;&#24615;&#30340;&#28176;&#36817;&#25910;&#25947;&#20197;&#21450;&#22797;&#26434;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;PADR-based ERM&#26041;&#27861;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#19968;&#33268;&#24615;&#20445;&#35777;&#21644;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#12290;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#65292;PADR-based ERM&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Focusing on stochastic programming (SP) with covariate information, this paper proposes an empirical risk minimization (ERM) method embedded within a nonconvex piecewise affine decision rule (PADR), which aims to learn the direct mapping from features to optimal decisions. We establish the nonasymptotic consistency result of our PADR-based ERM model for unconstrained problems and asymptotic consistency result for constrained ones. To solve the nonconvex and nondifferentiable ERM problem, we develop an enhanced stochastic majorization-minimization algorithm and establish the asymptotic convergence to (composite strong) directional stationarity along with complexity analysis. We show that the proposed PADR-based ERM method applies to a broad class of nonconvex SP problems with theoretical consistency guarantees and computational tractability. Our numerical study demonstrates the superior performance of PADR-based ERM methods compared to state-of-the-art approaches under various settings,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PVP&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32479;&#35745;&#20449;&#24687;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#21021;&#22987;&#21270;&#25552;&#31034;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#26497;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#65288;&#27599;&#31867;&#19968;&#21040;&#20004;&#20010;&#31034;&#20363;&#65289;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13639</link><description>&lt;p&gt;
PVP: &#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PVP: Pre-trained Visual Parameter-Efficient Tuning. (arXiv:2304.13639v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PVP&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32479;&#35745;&#20449;&#24687;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#21021;&#22987;&#21270;&#25552;&#31034;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#26497;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#65288;&#27599;&#31867;&#19968;&#21040;&#20004;&#20010;&#31034;&#20363;&#65289;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23436;&#20840;&#24494;&#35843;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#30528;&#26497;&#22823;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#27425;&#36890;&#36807;&#32463;&#39564;&#25506;&#31350;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;PETuning&#26041;&#27861;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#19979;&#28216;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PVP&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#21021;&#22987;&#21270;&#25552;&#31034;&#27169;&#22359;&#12290;PVP&#21487;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#19978;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#65288;&#20363;&#22914;&#27599;&#31867;&#19968;&#21040;&#20004;&#20010;&#31034;&#20363;&#65289;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained transformers have demonstrated remarkable success in various computer vision tasks. However, it is still highly challenging to fully fine-tune these models for downstream tasks due to their high computational and storage costs. Recently, Parameter-Efficient Tuning (PETuning) techniques, e.g., Visual Prompt Tuning (VPT) and Low-Rank Adaptation (LoRA), have significantly reduced the computation and storage cost by inserting lightweight prompt modules into the pre-trained models and tuning these prompt modules with a small number of trainable parameters, while keeping the transformer backbone frozen. Although only a few parameters need to be adjusted, most PETuning methods still require a significant amount of downstream task training data to achieve good results. The performance is inadequate on low-data regimes, especially when there are only one or two examples per class. To this end, we first empirically identify the poor performance is mainly due to the inappr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;84000&#22810;&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#34429;&#28982;&#24471;&#20998;&#19981;&#38169;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#38169;&#35273;&#12289;&#28431;&#25481;&#37325;&#35201;&#25968;&#25454;&#28857;&#20197;&#21450;&#19981;&#27491;&#30830;&#35299;&#37322;&#22797;&#26434;&#36235;&#21183;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13620</link><description>&lt;p&gt;
ChartSumm&#65306;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#20840;&#38754;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. (arXiv:2304.13620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;84000&#22810;&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#34429;&#28982;&#24471;&#20998;&#19981;&#38169;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#38169;&#35273;&#12289;&#28431;&#25481;&#37325;&#35201;&#25968;&#25454;&#28857;&#20197;&#21450;&#19981;&#27491;&#30830;&#35299;&#37322;&#22797;&#26434;&#36235;&#21183;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23558;&#22270;&#34920;&#36716;&#25442;&#20026;&#25991;&#26412;&#25688;&#35201;&#26159;&#35270;&#38556;&#20154;&#22763;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#21516;&#26102;&#20026;&#29992;&#25143;&#25552;&#20379;&#34920;&#26684;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#31934;&#30830;&#27934;&#23519;&#21147;&#12290;&#22823;&#22411;&#12289;&#32467;&#26500;&#33391;&#22909;&#30340;&#25968;&#25454;&#38598;&#22987;&#32456;&#26159;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20849;84363&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#65292;&#28085;&#30422;&#24191;&#27867;&#30340;&#20027;&#39064;&#21644;&#22270;&#34920;&#31867;&#22411;&#65292;&#21487;&#29983;&#25104;&#38271;&#30701;&#25688;&#35201;&#12290;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23454;&#29616;&#21508;&#31181;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#30340;&#24471;&#20998;&#26469;&#29983;&#25104;&#27969;&#30021;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#36935;&#21040;&#19968;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#20135;&#29983;&#38169;&#35273;&#65292;&#28431;&#25481;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#20197;&#21450;&#19981;&#27491;&#30830;&#22320;&#35299;&#37322;&#22270;&#34920;&#20013;&#30340;&#22797;&#26434;&#36235;&#21183;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#24037;&#20855;&#25506;&#35752;&#20102;&#23558;ChartSumm&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25104;&#20026;&#19968;&#20010;&#26377;&#25361;&#25112;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic chart to text summarization is an effective tool for the visually impaired people along with providing precise insights of tabular data in natural language to the user. A large and well-structured dataset is always a key part for data driven models. In this paper, we propose ChartSumm: a large-scale benchmark dataset consisting of a total of 84,363 charts along with their metadata and descriptions covering a wide range of topics and chart types to generate short and long summaries. Extensive experiments with strong baseline models show that even though these models generate fluent and informative summaries by achieving decent scores in various automatic evaluation metrics, they often face issues like suffering from hallucination, missing out important data points, in addition to incorrect explanation of complex trends in the charts. We also investigated the potential of expanding ChartSumm to other languages using automated translation tools. These make our dataset a challeng
&lt;/p&gt;</description></item><item><title>CROP&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32039;&#20945;&#37325;&#22609;&#35266;&#23519;&#22788;&#29702;&#26469;&#20943;&#23569;&#29992;&#20110;&#25919;&#31574;&#20248;&#21270;&#30340;&#29366;&#24577;&#20449;&#24687;&#65292;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#30340;&#35757;&#32451;&#24067;&#23616;&#65292;&#24182;&#25552;&#39640;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.13616</link><description>&lt;p&gt;
CROP: &#20351;&#29992;&#32039;&#20945;&#37325;&#22609;&#35266;&#23519;&#22788;&#29702;&#23454;&#29616;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CROP: Towards Distributional-Shift Robust Reinforcement Learning using Compact Reshaped Observation Processing. (arXiv:2304.13616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13616
&lt;/p&gt;
&lt;p&gt;
CROP&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32039;&#20945;&#37325;&#22609;&#35266;&#23519;&#22788;&#29702;&#26469;&#20943;&#23569;&#29992;&#20110;&#25919;&#31574;&#20248;&#21270;&#30340;&#29366;&#24577;&#20449;&#24687;&#65292;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#30340;&#35757;&#32451;&#24067;&#23616;&#65292;&#24182;&#25552;&#39640;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#23433;&#20840;&#24212;&#29992;&#38656;&#35201;&#20174;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#25512;&#24191;&#21040;&#26410;&#30693;&#24773;&#22659;&#12290;&#28982;&#32780;&#65292;&#24212;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#20917;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#25361;&#25112;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36890;&#29992;&#21270;&#26041;&#27861;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;&#23613;&#31649;&#36825;&#21487;&#20197;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#29615;&#22659;&#65292;&#20294;&#20063;&#20250;&#38459;&#30861;&#25919;&#31574;&#20248;&#21270;&#12290;&#35774;&#35745;&#19968;&#20010;&#21512;&#36866;&#30340;&#35266;&#23519;&#20449;&#24687;&#65292;&#21482;&#21253;&#21547;&#20851;&#38190;&#20449;&#24687;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32039;&#20945;&#37325;&#22609;&#35266;&#23519;&#22788;&#29702;&#65288;CROP&#65289;&#65292;&#20197;&#20943;&#23569;&#29992;&#20110;&#25919;&#31574;&#20248;&#21270;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#36890;&#36807;&#25552;&#20379;&#21482;&#26377;&#30456;&#20851;&#20449;&#24687;&#65292;&#21487;&#20197;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#30340;&#35757;&#32451;&#24067;&#23616;&#65292;&#24182;&#25552;&#39640;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19977;&#31181;CROP&#65292;&#21487;&#24212;&#29992;&#20110;&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#35266;&#23519;&#21644;&#34892;&#21160;&#31354;&#38388;&#65292;&#24182;&#25552;&#20379;&#26041;&#27861;ologically&#22320;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The safe application of reinforcement learning (RL) requires generalization from limited training data to unseen scenarios. Yet, fulfilling tasks under changing circumstances is a key challenge in RL. Current state-of-the-art approaches for generalization apply data augmentation techniques to increase the diversity of training data. Even though this prevents overfitting to the training environment(s), it hinders policy optimization. Crafting a suitable observation, only containing crucial information, has been shown to be a challenging task itself. To improve data efficiency and generalization capabilities, we propose Compact Reshaped Observation Processing (CROP) to reduce the state information used for policy optimization. By providing only relevant information, overfitting to a specific training layout is precluded and generalization to unseen environments is improved. We formulate three CROPs that can be applied to fully observable observation- and action-spaces and provide methodi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23376;&#39640;&#26031;&#22870;&#21169;&#24773;&#22659;&#19979;&#30340; Thompson &#25277;&#26679;&#31639;&#27861;&#22312;&#24773;&#22659; Bandit &#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#20102;&#25552;&#39640;&#20449;&#24687;&#27604;&#29575;&#30340;&#26032;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2304.13593</link><description>&lt;p&gt;
&#22522;&#20110;&#20114;&#20449;&#24687;&#27604;&#20363;&#30340; Thompson &#25277;&#26679;&#31639;&#27861;&#22312;&#23376;&#39640;&#26031;&#22870;&#21169;&#24773;&#22659;&#19979;&#30340;&#36951;&#25022;&#30028;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Thompson Sampling Regret Bounds for Contextual Bandits with sub-Gaussian rewards. (arXiv:2304.13593v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23376;&#39640;&#26031;&#22870;&#21169;&#24773;&#22659;&#19979;&#30340; Thompson &#25277;&#26679;&#31639;&#27861;&#22312;&#24773;&#22659; Bandit &#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#20102;&#25552;&#39640;&#20449;&#24687;&#27604;&#29575;&#30340;&#26032;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110; Neu et al. &#30340;&#26694;&#26550;&#21644;&#20854;&#25552;&#20986;&#30340;&#20114;&#20449;&#24687;&#27604;&#20363;&#27010;&#24565;&#30340;&#24773;&#22659; Bandit &#38382;&#39064;&#20013;&#30340; Thompson &#25277;&#26679;&#31639;&#27861;&#34920;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102; Thompson &#25277;&#26679;&#26399;&#26395;&#32047;&#35745;&#36951;&#25022;&#30340;&#20840;&#38754;&#36793;&#30028;&#21462;&#20915;&#20110;&#29615;&#22659;&#21442;&#25968;&#21644;&#21382;&#21490;&#30340;&#20114;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#23376;&#39640;&#26031;&#22870;&#21169;&#25104;&#31435;&#30340;&#25552;&#39640;&#20449;&#24687;&#27604;&#29575;&#30340;&#26032;&#36793;&#30028;&#65292;&#20174;&#32780;&#25512;&#24191;&#20102; Neu &#31561;&#20154;&#30340;&#32467;&#26524;&#65292;&#20854;&#20998;&#26512;&#35201;&#27714;&#20108;&#36827;&#21046;&#22870;&#21169;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20026;&#38750;&#32467;&#26500;&#21270;&#26377;&#30028;&#24773;&#22659; Bandit&#12289;&#32467;&#26500;&#21270;&#26377;&#30028;&#24773;&#22659; Bandit&#65288;&#25289;&#26222;&#25289;&#26031;&#20284;&#28982;&#20989;&#25968;&#65289;&#12289;&#32467;&#26500;&#21270; Bernoulli Bandit &#21644;&#26377;&#30028;&#32447;&#24615;&#24773;&#22659; Bandit &#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the performance of the Thompson Sampling algorithm for Contextual Bandit problems based on the framework introduced by Neu et al. and their concept of lifted information ratio. First, we prove a comprehensive bound on the Thompson Sampling expected cumulative regret that depends on the mutual information of the environment parameters and the history. Then, we introduce new bounds on the lifted information ratio that hold for sub-Gaussian rewards, thus generalizing the results from Neu et al. which analysis requires binary rewards. Finally, we provide explicit regret bounds for the special cases of unstructured bounded contextual bandits, structured bounded contextual bandits with Laplace likelihood, structured Bernoulli bandits, and bounded linear contextual bandits.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#22312;&#33521;&#35821;&#21644;&#26085;&#35821;&#20043;&#38388;&#30340;&#32763;&#35793;&#20013;&#23384;&#22312;&#30007;&#24615;&#20195;&#35789;&#20559;&#35265;&#65292;&#21516;&#26102;&#20063;&#26816;&#27979;&#21040;&#20102;&#23545;&#22899;&#24615;&#12289;&#20013;&#24615;&#21644;/&#25110;&#38750;&#20108;&#20803;&#20195;&#35789;&#23384;&#22312;&#30340;&#24494;&#22937;&#21453;&#24212;&#30340;&#20559;&#35265;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20195;&#35789;&#32763;&#35793;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23558;&#22797;&#25968;&#23884;&#20837;NLP&#25968;&#25454;&#38598;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.13557</link><description>&lt;p&gt;
&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#20013;&#8220;&#25105;&#8221;&#36855;&#22833;&#22312;&#32763;&#35793;&#20013;&#65306;&#20195;&#35789;&#38169;&#35823;&#27493;&#39588;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
"I'm" Lost in Translation: Pronoun Missteps in Crowdsourced Data Sets. (arXiv:2304.13557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13557
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#22312;&#33521;&#35821;&#21644;&#26085;&#35821;&#20043;&#38388;&#30340;&#32763;&#35793;&#20013;&#23384;&#22312;&#30007;&#24615;&#20195;&#35789;&#20559;&#35265;&#65292;&#21516;&#26102;&#20063;&#26816;&#27979;&#21040;&#20102;&#23545;&#22899;&#24615;&#12289;&#20013;&#24615;&#21644;/&#25110;&#38750;&#20108;&#20803;&#20195;&#35789;&#23384;&#22312;&#30340;&#24494;&#22937;&#21453;&#24212;&#30340;&#20559;&#35265;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20195;&#35789;&#32763;&#35793;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23558;&#22797;&#25968;&#23884;&#20837;NLP&#25968;&#25454;&#38598;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#34394;&#25311;&#21161;&#25163;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#26222;&#21450;&#65292;&#36234;&#26469;&#36234;&#38656;&#35201;&#36825;&#20123;&#35821;&#38899;&#31995;&#32479;&#20197;&#21508;&#31181;&#35821;&#35328;&#33258;&#28982;&#22320;&#36827;&#34892;&#20132;&#27969;&#12290;&#20247;&#21253;&#20513;&#35758;&#24050;&#32463;&#19987;&#27880;&#20110;&#23545;&#22823;&#22411;&#24320;&#25918;&#25968;&#25454;&#38598;&#36827;&#34892;&#22810;&#35821;&#35328;&#32763;&#35793;&#65292;&#20197;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#32763;&#35793;&#36890;&#24120;&#19981;&#26159;&#19968;&#23545;&#19968;&#30340;&#65292;&#24182;&#19988;&#20559;&#35265;&#21487;&#33021;&#20250;&#36880;&#28176;&#28183;&#20837;&#12290;&#22312;&#36825;&#39033;&#26368;&#26032;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20102;&#22312;&#20247;&#21253;Tatoeba&#25968;&#25454;&#24211;&#20013;&#33521;&#35821;&#21644;&#26085;&#35821;&#20043;&#38388;&#32763;&#35793;&#30340;&#20195;&#35789;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#25972;&#20307;&#19978;&#23384;&#22312;&#30007;&#24615;&#20195;&#35789;&#20559;&#35265;&#65292;&#21363;&#20351;&#22312;&#20854;&#20182;&#26041;&#24335;&#20013;&#32771;&#34385;&#21040;&#35821;&#35328;&#30340;&#22797;&#25968;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#26816;&#27979;&#21040;&#32763;&#35793;&#36807;&#31243;&#20013;&#21453;&#26144;&#20102;&#23545;&#22899;&#24615;&#12289;&#20013;&#24615;&#21644;/&#25110;&#38750;&#20108;&#20803;&#20195;&#35789;&#23384;&#22312;&#30340;&#24494;&#22937;&#21453;&#24212;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20195;&#35789;&#32763;&#35793;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23558;&#22797;&#25968;&#23884;&#20837;NLP&#25968;&#25454;&#38598;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As virtual assistants continue to be taken up globally, there is an ever-greater need for these speech-based systems to communicate naturally in a variety of languages. Crowdsourcing initiatives have focused on multilingual translation of big, open data sets for use in natural language processing (NLP). Yet, language translation is often not one-to-one, and biases can trickle in. In this late-breaking work, we focus on the case of pronouns translated between English and Japanese in the crowdsourced Tatoeba database. We found that masculine pronoun biases were present overall, even though plurality in language was accounted for in other ways. Importantly, we detected biases in the translation process that reflect nuanced reactions to the presence of feminine, neutral, and/or non-binary pronouns. We raise the issue of translation bias for pronouns and offer a practical solution to embed plurality in NLP data sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#23454;&#29616;FLCC&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39057;&#29575;&#22797;&#29992;&#21644;&#31354;&#38388;&#32858;&#31867;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#21534;&#21520;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13549</link><description>&lt;p&gt;
FLCC: &#26080;&#32447;CSMA/CA&#32593;&#32476;&#19978;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLCC: Efficient Distributed Federated Learning on IoMT over CSMA/CA. (arXiv:2304.13549v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#23454;&#29616;FLCC&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39057;&#29575;&#22797;&#29992;&#21644;&#31354;&#38388;&#32858;&#31867;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#21534;&#21520;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#29992;&#25143;&#21644;&#20113;&#26381;&#21153;&#22120;&#20043;&#38388;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#65292;&#32780;&#19981;&#26159;&#21407;&#22987;&#30340;&#26412;&#22320;&#25968;&#25454;&#12290;&#22312;&#20998;&#24067;&#24335;&#26080;&#32447;&#33410;&#28857;&#20013;&#23454;&#29616;FL&#26102;&#65292;FL&#26041;&#27861;&#22312;&#36890;&#20449;&#21644;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#21576;&#29616;&#20986;&#26377;&#36259;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#32593;&#32476;&#21160;&#24577;&#21644;&#23398;&#20064;&#37117;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24212;&#29992;&#31243;&#24207;&#19978;&#20351;&#29992;FL&#26469;&#25913;&#36827;&#36828;&#31243;&#21307;&#30103;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#35813;&#31995;&#32479;&#22312;&#37319;&#29992;CSMA/CA&#36827;&#34892;&#35843;&#24230;&#26102;&#20351;&#29992;&#33258;&#32452;&#32455;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;FLCC&#65288;&#22312;CSMA/CA&#19978;&#23454;&#29616;&#30340;FL&#65289;&#27169;&#22411;&#26088;&#22312;&#28040;&#38500;&#19981;&#21487;&#20449;&#30340;&#35774;&#22791;&#65292;&#24182;&#21033;&#29992;&#39057;&#29575;&#22797;&#29992;&#21644;&#31354;&#38388;&#32858;&#31867;&#25216;&#26415;&#65292;&#25552;&#39640;&#21327;&#35843;&#20998;&#24067;&#24335;&#23454;&#29616;FL&#25152;&#38656;&#30340;&#21534;&#21520;&#37327;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20013;&#65292;&#39057;&#29575;&#21644;&#20301;&#32622;&#30340;&#36873;&#25321;&#23545;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a promising approach for privacy preservation, allowing sharing of the model parameters between users and the cloud server rather than the raw local data. FL approaches have been adopted as a cornerstone of distributed machine learning (ML) to solve several complex use cases. FL presents an interesting interplay between communication and ML performance when implemented over distributed wireless nodes. Both the dynamics of networking and learning play an important role. In this article, we investigate the performance of FL on an application that might be used to improve a remote healthcare system over ad hoc networks which employ CSMA/CA to schedule its transmissions. Our FL over CSMA/CA (FLCC) model is designed to eliminate untrusted devices and harness frequency reuse and spatial clustering techniques to improve the throughput required for coordinating a distributed implementation of FL in the wireless network.  In our proposed model, frequency a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#22522;&#20110;&#37327;&#21270;&#21516;&#26102;&#23454;&#29616;&#36890;&#35759;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#19988;&#21521;&#22343;&#21248;&#37327;&#21270;&#30340;&#26799;&#24230;&#28155;&#21152;&#20108;&#39033;&#22122;&#22768;&#20197;&#36798;&#21040;&#25152;&#38656;&#30340;&#24046;&#20998;&#38544;&#31169;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.13545</link><description>&lt;p&gt;
&#19968;&#31661;&#21452;&#38613;&#65306;&#37327;&#21270;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#19982;&#36890;&#35759;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Killing Two Birds with One Stone: Quantization Achieves Privacy in Distributed Learning. (arXiv:2304.13545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#22522;&#20110;&#37327;&#21270;&#21516;&#26102;&#23454;&#29616;&#36890;&#35759;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#19988;&#21521;&#22343;&#21248;&#37327;&#21270;&#30340;&#26799;&#24230;&#28155;&#21152;&#20108;&#39033;&#22122;&#22768;&#20197;&#36798;&#21040;&#25152;&#38656;&#30340;&#24046;&#20998;&#38544;&#31169;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36890;&#35759;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#26159;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20998;&#24320;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#21487;&#33021;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#24212;&#29992;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#36890;&#35759;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#36890;&#35759;&#21644;&#38544;&#31169;&#30456;&#20851;&#24615;&#36136;&#30340;&#26032;&#35265;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21521;&#22343;&#21248;&#37327;&#21270;&#30340;&#26799;&#24230;&#28155;&#21152;&#20108;&#39033;&#22122;&#22768;&#20197;&#36798;&#21040;&#25152;&#38656;&#30340;&#24046;&#20998;&#38544;&#31169;&#32423;&#21035;&#65292;&#20174;&#32780;&#22312;&#31245;&#24494;&#29306;&#29298;&#36890;&#35759;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477; (SGD) &#26694;&#26550;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#25429;&#25417;&#20102;&#36890;&#35759;&#12289;&#38544;&#31169;&#21644;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#30340;&#26032;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication efficiency and privacy protection are two critical issues in distributed machine learning. Existing methods tackle these two issues separately and may have a high implementation complexity that constrains their application in a resource-limited environment. We propose a comprehensive quantization-based solution that could simultaneously achieve communication efficiency and privacy protection, providing new insights into the correlated nature of communication and privacy. Specifically, we demonstrate the effectiveness of our proposed solutions in the distributed stochastic gradient descent (SGD) framework by adding binomial noise to the uniformly quantized gradients to reach the desired differential privacy level but with a minor sacrifice in communication efficiency. We theoretically capture the new trade-offs between communication, privacy, and learning performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#23481;&#38169;ML&#23450;&#20041;&#8212;&#8212;&#27169;&#22411;&#20849;&#35782;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#19968;&#31867;&#26080;&#26799;&#24230;ML&#31639;&#27861;&#21644;&#32463;&#20856;&#20998;&#24067;&#24335;&#20849;&#35782;&#31639;&#27861;&#29983;&#25104;&#26080;&#26799;&#24230;&#25308;&#21344;&#24237;&#23481;&#38169;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13540</link><description>&lt;p&gt;
&#22522;&#20110;&#28436;&#21270;&#25628;&#32034;&#30340;&#25308;&#21344;&#24237;&#23481;&#38169;&#23398;&#20064;&#65306;&#36229;&#36234;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Byzantine-Resilient Learning Beyond Gradients: Distributing Evolutionary Search. (arXiv:2304.13540v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#23481;&#38169;ML&#23450;&#20041;&#8212;&#8212;&#27169;&#22411;&#20849;&#35782;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#19968;&#31867;&#26080;&#26799;&#24230;ML&#31639;&#27861;&#21644;&#32463;&#20856;&#20998;&#24067;&#24335;&#20849;&#35782;&#31639;&#27861;&#29983;&#25104;&#26080;&#26799;&#24230;&#25308;&#21344;&#24237;&#23481;&#38169;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;&#33021;&#21147;&#30340;&#25552;&#21319;&#19981;&#20165;&#22240;&#20110;&#20854;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#36824;&#22312;&#20110;&#29992;&#20110;&#35757;&#32451;&#23427;&#20204;&#30340;&#35745;&#31639;&#33021;&#21147;&#30340;&#24040;&#22823;&#25552;&#21319;&#12290;&#36825;&#31181;&#24040;&#22823;&#25552;&#21319;&#23548;&#33268;&#23545;&#20998;&#24067;&#24335;ML&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#32780;&#36825;&#21453;&#36807;&#26469;&#21448;&#20351;&#24037;&#20154;&#25925;&#38556;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#21464;&#24471;&#36234;&#26469;&#36234;&#32039;&#36843;&#12290;&#34429;&#28982;&#22312;&#21487;&#24494;&#35774;&#32622;&#20013;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#25308;&#21344;&#24237;&#23481;&#38169;&#31639;&#27861;&#65292;&#20294;&#22312;&#26080;&#26799;&#24230;&#35774;&#32622;&#20013;&#19981;&#23384;&#22312;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#23601;&#26159;&#35299;&#20915;&#36825;&#20010;&#32570;&#38519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#25308;&#21344;&#24237;&#23481;&#38169;ML&#23450;&#20041;&#8212;&#8212;&#27169;&#22411;&#20849;&#35782;&#65292;&#35813;&#23450;&#20041;&#25193;&#23637;&#20102;&#32463;&#20856;&#20998;&#24067;&#24335;&#20849;&#35782;&#30340;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#23450;&#20041;&#26469;&#23637;&#31034;&#19968;&#33324;&#31867;&#30340;&#26080;&#26799;&#24230;ML&#31639;&#27861;&#8212;&#8212;&#65288;1&#65292;&#955;&#65289;-&#28436;&#21270;&#25628;&#32034;&#21487;&#20197;&#19982;&#32463;&#20856;&#20998;&#24067;&#24335;&#20849;&#35782;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#26080;&#26799;&#24230;&#25308;&#21344;&#24237;&#23481;&#38169;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning (ML) models are capable of impressive performances. However, their prowess is not due only to the improvements in their architecture and training algorithms but also to a drastic increase in computational power used to train them.  Such a drastic increase led to a growing interest in distributed ML, which in turn made worker failures and adversarial attacks an increasingly pressing concern. While distributed byzantine resilient algorithms have been proposed in a differentiable setting, none exist in a gradient-free setting.  The goal of this work is to address this shortcoming. For that, we introduce a more general definition of byzantine-resilience in ML - the \textit{model-consensus}, that extends the definition of the classical distributed consensus. We then leverage this definition to show that a general class of gradient-free ML algorithms - ($1,\lambda$)-Evolutionary Search - can be combined with classical distributed consensus algorithms to generate gradi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20845;&#31181;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#23618;&#26367;&#25442;&#20026;&#20302;&#31209;&#24352;&#37327;&#36924;&#36817;&#30340;&#26041;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#65292;&#24182;&#19988;&#36866;&#21512;&#20110;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.13539</link><description>&lt;p&gt;
&#24352;&#37327;&#20998;&#35299;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Tensor Decomposition for Model Reduction in Neural Networks: A Review. (arXiv:2304.13539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20845;&#31181;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#23618;&#26367;&#25442;&#20026;&#20302;&#31209;&#24352;&#37327;&#36924;&#36817;&#30340;&#26041;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#65292;&#24182;&#19988;&#36866;&#21512;&#20110;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#23427;&#20204;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#65292;&#38656;&#35201;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#19968;&#31181;&#30452;&#25509;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#19981;&#21516;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#23558;&#32593;&#32476;&#30340;&#23618;&#26367;&#25442;&#20026;&#20854;&#20302;&#31209;&#24352;&#37327;&#36817;&#20284;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#20845;&#31181;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#24182;&#38416;&#36848;&#20102;&#23427;&#20204;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#27169;&#22411;&#21442;&#25968;&#21387;&#32553;&#20013;&#30340;&#33021;&#21147;&#12290;&#19968;&#20123;&#21387;&#32553;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#29978;&#33267;&#21487;&#20197;&#39640;&#20110;&#21407;&#22987;&#29256;&#26412;&#12290;&#35780;&#20272;&#34920;&#26126;&#65292;&#24352;&#37327;&#20998;&#35299;&#21487;&#20197;&#22312;&#27169;&#22411;&#22823;&#23567;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#23454;&#29616;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#21512;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern neural networks have revolutionized the fields of computer vision (CV) and Natural Language Processing (NLP). They are widely used for solving complex CV tasks and NLP tasks such as image classification, image generation, and machine translation. Most state-of-the-art neural networks are over-parameterized and require a high computational cost. One straightforward solution is to replace the layers of the networks with their low-rank tensor approximations using different tensor decomposition methods. This paper reviews six tensor decomposition methods and illustrates their ability to compress model parameters of convolutional neural networks (CNNs), recurrent neural networks (RNNs) and Transformers. The accuracy of some compressed models can be higher than the original versions. Evaluations indicate that tensor decompositions can achieve significant reductions in model size, run-time and energy consumption, and are well suited for implementing neural networks on edge devices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#30524;&#21160;&#20107;&#20214;&#20316;&#20026;&#21487;&#35299;&#37322;&#27010;&#24565;&#65292;&#23545;&#28145;&#24230;&#31070;&#32463;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#35299;&#37322;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#25195;&#35270;&#36755;&#20837;&#29305;&#24449;&#27604;&#22266;&#23450;&#28857;&#36755;&#20837;&#29305;&#24449;&#26356;&#20026;&#37325;&#35201;&#65292;&#19988;&#25509;&#36817;&#25195;&#35270;&#23792;&#20540;&#36895;&#24230;&#30340;&#27880;&#35270;&#26679;&#26412;&#26368;&#20855;&#24433;&#21709;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.13536</link><description>&lt;p&gt;
&#31435;&#36275;&#30524;&#21160;&#20107;&#20214;&#65306;&#20351;&#29992;&#21487;&#35299;&#37322;&#27010;&#24565;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: Gaze Events as Interpretable Concepts to Explain Deep Neural Sequence Models. (arXiv:2304.13536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#30524;&#21160;&#20107;&#20214;&#20316;&#20026;&#21487;&#35299;&#37322;&#27010;&#24565;&#65292;&#23545;&#28145;&#24230;&#31070;&#32463;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#35299;&#37322;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#25195;&#35270;&#36755;&#20837;&#29305;&#24449;&#27604;&#22266;&#23450;&#28857;&#36755;&#20837;&#29305;&#24449;&#26356;&#20026;&#37325;&#35201;&#65292;&#19988;&#25509;&#36817;&#25195;&#35270;&#23792;&#20540;&#36895;&#24230;&#30340;&#27880;&#35270;&#26679;&#26412;&#26368;&#20855;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#30524;&#21160;&#25968;&#25454;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#24037;&#20316;&#23545;&#20110;&#35299;&#37322;&#29992;&#20110;&#30524;&#21160;&#27979;&#37327;&#29983;&#29289;&#35782;&#21035;&#30340;&#28145;&#24230;&#31070;&#32463;&#24207;&#21015;&#27169;&#22411;&#30340;&#36755;&#20986;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#26174;&#30528;&#24615;&#22320;&#22270;&#26469;&#31361;&#20986;&#29305;&#23450;&#30524;&#27880;&#35270;&#24207;&#21015;&#30340;&#37325;&#35201;&#36755;&#20837;&#29305;&#24449;&#12290;&#20294;&#26159;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#23427;&#30340;&#26412;&#22320;&#21270;&#20998;&#26512;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#32570;&#23569;&#25968;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#24050;&#32463;&#24314;&#31435;&#30340;&#22266;&#23450;&#28857;&#21644;&#25195;&#35270;&#30524;&#21160;&#20107;&#20214;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#30830;&#23450;&#23427;&#20204;&#30340;&#27010;&#24565;&#24433;&#21709;&#26469;&#23450;&#37327;&#35780;&#20272;&#36825;&#20123;&#20107;&#20214;&#30340;&#24433;&#21709;&#12290;&#23646;&#20110;&#25195;&#35270;&#30340;&#36755;&#20837;&#29305;&#24449;&#27604;&#23646;&#20110;&#22266;&#23450;&#28857;&#30340;&#29305;&#24449;&#37325;&#35201;&#24471;&#22810;&#12290;&#36890;&#36807;&#23558;&#25195;&#35270;&#20107;&#20214;&#20998;&#35299;&#20026;&#23376;&#20107;&#20214;&#65292;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;&#25509;&#36817;&#25195;&#35270;&#23792;&#20540;&#36895;&#24230;&#30340;&#27880;&#35270;&#26679;&#26412;&#26159;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20107;&#20214;&#23646;&#24615;&#22914;&#25195;&#35270;&#24133;&#24230;&#25110;&#22266;&#23450;&#31163;&#25955;&#24230;&#23545;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#20351;&#29992;&#30524;&#21160;&#20107;&#20214;&#20316;&#20026;&#21487;&#35299;&#37322;&#27010;&#24565;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#24207;&#21015;&#27169;&#22411;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in XAI for eye tracking data has evaluated the suitability of feature attribution methods to explain the output of deep neural sequence models for the task of oculomotric biometric identification. These methods provide saliency maps to highlight important input features of a specific eye gaze sequence. However, to date, its localization analysis has been lacking a quantitative approach across entire datasets. In this work, we employ established gaze event detection algorithms for fixations and saccades and quantitatively evaluate the impact of these events by determining their concept influence. Input features that belong to saccades are shown to be substantially more important than features that belong to fixations. By dissecting saccade events into sub-events, we are able to show that gaze samples that are close to the saccadic peak velocity are most influential. We further investigate the effect of event properties like saccadic amplitude or fixational dispersion on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22343;&#22330;&#21338;&#24328;&#20316;&#20026;&#23454;&#39564;&#23460;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#19982;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25552;&#39640;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36924;&#30495;&#24230;&#30340;&#21516;&#26102;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13534</link><description>&lt;p&gt;
&#29992;&#22343;&#22330;&#21338;&#24328;&#20026;&#29983;&#25104;&#27169;&#22411;&#25645;&#24314;&#23454;&#39564;&#23460;
&lt;/p&gt;
&lt;p&gt;
A mean-field games laboratory for generative modeling. (arXiv:2304.13534v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22343;&#22330;&#21338;&#24328;&#20316;&#20026;&#23454;&#39564;&#23460;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#19982;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25552;&#39640;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36924;&#30495;&#24230;&#30340;&#21516;&#26102;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22343;&#22330;&#21338;&#24328; (MFGs) &#20316;&#20026;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#12289;&#22686;&#24378;&#21644;&#35774;&#35745;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102; MFGs &#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#31890;&#23376;&#21160;&#21147;&#23398;&#21644;&#20195;&#20215;&#20989;&#25968;&#25512;&#23548;&#20102;&#36825;&#19977;&#20010;&#31867;&#21035;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#8212;&#8212;&#19968;&#32452;&#32806;&#21512;&#30340;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#26469;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#23398;&#32467;&#26500;&#21644;&#29305;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#21512;&#25104;&#26679;&#26412;&#65292;&#21478;&#19968;&#20010;&#20195;&#29702;&#23545;&#26679;&#26412;&#36827;&#34892;&#35782;&#21035;&#65292;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#22810;&#26679;&#19988;&#36924;&#30495;&#65292;&#21516;&#26102;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;&#24635;&#20043;&#65292;&#26412;&#25991;&#31361;&#26174;&#20102; MFGs &#20316;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#39564;&#23460;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we demonstrate the versatility of mean-field games (MFGs) as a mathematical framework for explaining, enhancing, and designing generative models. There is a pervasive sense in the generative modeling community that the various flow and diffusion-based generative models have some foundational common structure and interrelationships. We establish connections between MFGs and major classes of flow and diffusion-based generative models including continuous-time normalizing flows, score-based models, and Wasserstein gradient flows. We derive these three classes of generative models through different choices of particle dynamics and cost functions. Furthermore, we study the mathematical structure and properties of each generative model by studying their associated MFG's optimality condition, which is a set of coupled nonlinear partial differential equations (PDEs). The theory of MFGs, therefore, enables the study of generative models through the theory of nonlinear PDEs. Throu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#30149;&#29702;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#20559;&#31227;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31751;&#29109;&#26469;&#36873;&#25321;&#26377;&#25928;&#30340;WSI&#24182;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21322;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13513</link><description>&lt;p&gt;
&#31751;&#29109;&#65306;&#30149;&#29702;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20027;&#21160;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Cluster Entropy: Active Domain Adaptation in Pathological Image Segmentation. (arXiv:2304.13513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#30149;&#29702;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#20559;&#31227;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31751;&#29109;&#26469;&#36873;&#25321;&#26377;&#25928;&#30340;WSI&#24182;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21322;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30149;&#29702;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#22495;&#20559;&#31227;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#28304;&#22495;&#65288;&#22312;&#29305;&#23450;&#21307;&#38498;&#25910;&#38598;&#30340;&#22270;&#20687;&#65289;&#35757;&#32451;&#30340;&#32593;&#32476;&#22312;&#30446;&#26631;&#22495;&#65288;&#26469;&#33258;&#19981;&#21516;&#21307;&#38498;&#65289;&#20013;&#30001;&#20110;&#19981;&#21516;&#30340;&#22270;&#20687;&#29305;&#24449;&#34920;&#29616;&#19981;&#20339;&#12290;&#30001;&#20110;&#30149;&#29702;&#23398;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#19981;&#21516;&#31867;&#21035;&#20808;&#39564;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#20856;&#22411;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#19981;&#33021;&#36890;&#36807;&#23545;&#40784;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#20998;&#24067;&#26469;&#24456;&#22909;&#22320;&#22788;&#29702;&#35813;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31751;&#29109;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#29992;&#20110;&#21322;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#30340;&#26377;&#25928;WSI&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#31751;&#30340;&#29109;&#26469;&#24230;&#37327;WSI&#30340;&#22270;&#20687;&#29305;&#24449;&#22914;&#20309;&#35206;&#30422;&#30446;&#26631;&#22495;&#30340;&#25972;&#20010;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;&#20004;&#23478;&#21307;&#38498;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The domain shift in pathological segmentation is an important problem, where a network trained by a source domain (collected at a specific hospital) does not work well in the target domain (from different hospitals) due to the different image features. Due to the problems of class imbalance and different class prior of pathology, typical unsupervised domain adaptation methods do not work well by aligning the distribution of source domain and target domain. In this paper, we propose a cluster entropy for selecting an effective whole slide image (WSI) that is used for semi-supervised domain adaptation. This approach can measure how the image features of the WSI cover the entire distribution of the target domain by calculating the entropy of each cluster and can significantly improve the performance of domain adaptation. Our approach achieved competitive results against the prior arts on datasets collected from two hospitals.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;KeepMask&#21644;KeepMix&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#28040;&#32791;&#39069;&#22806;&#35745;&#31639;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#26356;&#22909;&#22320;&#35782;&#21035;&#22120;&#23448;&#36793;&#30028;&#65292;&#20174;&#32780;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#25552;&#20379;&#39640;&#31934;&#24230;&#30340;&#24615;&#33021;&#21644;&#26356;&#31934;&#30830;&#30340;&#20998;&#21106;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2304.13490</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#19982;&#21069;&#26223;&#21306;&#22495;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Mixing Data Augmentation with Preserving Foreground Regions in Medical Image Segmentation. (arXiv:2304.13490v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;KeepMask&#21644;KeepMix&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#28040;&#32791;&#39069;&#22806;&#35745;&#31639;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#26356;&#22909;&#22320;&#35782;&#21035;&#22120;&#23448;&#36793;&#30028;&#65292;&#20174;&#32780;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#25552;&#20379;&#39640;&#31934;&#24230;&#30340;&#24615;&#33021;&#21644;&#26356;&#31934;&#30830;&#30340;&#20998;&#21106;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#30340;&#24212;&#29992;&#21487;&#20197;&#26174;&#33879;&#25903;&#25345;&#21307;&#29983;&#30340;&#35786;&#26029;&#12290;&#28145;&#24230;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#26469;&#25193;&#23637;&#22810;&#26679;&#24615;&#20197;&#38450;&#27490;&#36807;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#38656;&#35201;&#26356;&#26032;&#21442;&#25968;&#21644;&#28040;&#32791;&#39069;&#22806;&#35745;&#31639;&#36164;&#28304;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#35757;&#32451;&#39640;&#31934;&#24230;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#34987;&#31216;&#20026;KeepMask&#21644;KeepMix&#65292;&#21487;&#20197;&#36890;&#36807;&#26356;&#22909;&#22320;&#35782;&#21035;&#22120;&#23448;&#36793;&#30028;&#26469;&#21019;&#24314;&#21307;&#23398;&#22270;&#20687;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#31934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#36793;&#30028;&#12290;&#22312;CHAOS&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;Dice&#31995;&#25968;&#36798;&#21040;&#20102;94.15&#65285;&#65288;&#27604;&#22522;&#32447;&#39640;3.04&#65285;&#65289;&#65292;&#22312;MSD&#33086;&#33039;&#19978;&#36798;&#21040;&#20102;74.70&#65285;&#65288;&#27604;&#22522;&#32447;&#39640;5.25&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of medical image segmentation using deep learning can significantly support doctors' diagnoses. Deep learning needs large amounts of data for training, which also requires data augmentation to extend diversity for preventing overfitting. However, the existing methods for data augmentation of medical image segmentation are mainly based on models which need to update parameters and cost extra computing resources. We proposed data augmentation methods designed to train a high accuracy deep learning network for medical image segmentation. The proposed data augmentation approaches are called KeepMask and KeepMix, which can create medical images by better identifying the boundary of the organ with no more parameters. Our methods achieved better performance and obtained more precise boundaries for medical image segmentation on datasets. The dice coefficient of our methods achieved 94.15% (3.04% higher than baseline) on CHAOS and 74.70% (5.25% higher than baseline) on MSD splee
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20808;&#39564;&#20449;&#24687;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#65292;&#24341;&#20837;&#20102;&#20248;&#20808;&#39118;&#38505;&#27010;&#24565;&#65292;&#24182;&#20026;&#32479;&#35745;&#20272;&#35745;&#38382;&#39064;&#25552;&#20379;&#20102;&#19979;&#30028;&#65292;&#23637;&#29616;&#20102;&#26694;&#26550;&#22312;&#19981;&#21516;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.13479</link><description>&lt;p&gt;
&#20808;&#39564;&#20449;&#24687;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Fundamental Tradeoffs in Learning with Prior Information. (arXiv:2304.13479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20808;&#39564;&#20449;&#24687;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#65292;&#24341;&#20837;&#20102;&#20248;&#20808;&#39118;&#38505;&#27010;&#24565;&#65292;&#24182;&#20026;&#32479;&#35745;&#20272;&#35745;&#38382;&#39064;&#25552;&#20379;&#20102;&#19979;&#30028;&#65292;&#23637;&#29616;&#20102;&#26694;&#26550;&#22312;&#19981;&#21516;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#23398;&#20064;&#32773;&#22312;&#25152;&#23398;&#38382;&#39064;&#19978;&#30340;&#20808;&#39564;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#21644;&#20854;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20248;&#20808;&#39118;&#38505;&#30340;&#27010;&#24565;&#65292;&#23427;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#26497;&#23567;&#26497;&#22823;&#21644;&#36125;&#21494;&#26031;&#39118;&#38505;&#65292;&#21487;&#20197;&#35753;&#25105;&#20204;&#30740;&#31350;&#29616;&#23454;&#19981;&#19968;&#23450;&#31526;&#21512;&#23398;&#20064;&#32773;&#20808;&#39564;&#30340;&#24773;&#20917;&#19979;&#36825;&#20123;&#22522;&#26412;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32553;&#20943;&#30340;&#26041;&#27861;&#26469;&#25193;&#23637;&#32463;&#20856;&#30340;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;&#25216;&#26415;&#65292;&#20197;&#20415;&#20026;&#32479;&#35745;&#20272;&#35745;&#38382;&#39064;&#30340;&#20248;&#20808;&#39118;&#38505;&#25552;&#20379;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#35834;&#19981;&#31561;&#24335;&#30340;&#25512;&#24191;&#65288;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#65289;&#65292;&#29992;&#20110;&#22312;&#28041;&#21450;&#26080;&#38480;&#25439;&#22833;&#30340;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#19979;&#65292;&#19979;&#30028;&#20248;&#20808;&#39118;&#38505;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#25581;&#31034;&#20102;&#22312;&#20272;&#35745;&#12289;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#20808;&#39564;&#20449;&#24687;&#19982;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#26435;&#34913;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to understand fundamental tradeoffs between the accuracy of prior information that a learner has on a given problem and its learning performance. We introduce the notion of prioritized risk, which differs from traditional notions of minimax and Bayes risk by allowing us to study such fundamental tradeoffs in settings where reality does not necessarily conform to the learner's prior. We present a general reduction-based approach for extending classical minimax lower-bound techniques in order to lower bound the prioritized risk for statistical estimation problems. We also introduce a novel generalization of Fano's inequality (which may be of independent interest) for lower bounding the prioritized risk in more general settings involving unbounded losses. We illustrate the ability of our framework to provide insights into tradeoffs between prior information and learning performance for problems in estimation, regression, and reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27425;&#26631;&#27880;&#22270;&#20687;&#30340;&#20998;&#21106;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24191;&#20041;&#27010;&#29575;U-Net&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;&#28508;&#22312;&#31354;&#38388;&#20998;&#24067;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#32467;&#26524;&#30340;&#22810;&#26679;&#24615;&#21644;&#21442;&#32771;&#20998;&#21106;&#30340;&#37325;&#21472;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13476</link><description>&lt;p&gt;
&#22810;&#27425;&#26631;&#27880;&#22270;&#20687;&#20998;&#21106;&#20013;&#28508;&#22312;&#31354;&#38388;&#20998;&#24067;&#23545;&#25928;&#26524;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of latent space distribution on the segmentation of images with multiple annotations. (arXiv:2304.13476v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27425;&#26631;&#27880;&#22270;&#20687;&#30340;&#20998;&#21106;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24191;&#20041;&#27010;&#29575;U-Net&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;&#28508;&#22312;&#31354;&#38388;&#20998;&#24067;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#32467;&#26524;&#30340;&#22810;&#26679;&#24615;&#21644;&#21442;&#32771;&#20998;&#21106;&#30340;&#37325;&#21472;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#27010;&#29575;U-Net&#65292;&#20854;&#36890;&#36807;&#20801;&#35768;&#26356;&#19968;&#33324;&#24418;&#24335;&#30340;&#39640;&#26031;&#20998;&#24067;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#20998;&#24067;&#26469;&#25193;&#23637;&#27010;&#29575;U-Net&#65292;&#36825;&#21487;&#20197;&#26356;&#22909;&#22320;&#36817;&#20284;&#20110;&#21442;&#32771;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#28508;&#22312;&#31354;&#38388;&#20998;&#24067;&#30340;&#36873;&#25321;&#23545;&#32954;&#32959;&#30244;&#21644;&#33041;&#30333;&#36136;&#39640;&#20449;&#21495;&#28790;&#21442;&#32771;&#20998;&#21106;&#21464;&#21270;&#25429;&#25417;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20998;&#24067;&#30340;&#36873;&#25321;&#20250;&#24433;&#21709;&#39044;&#27979;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#20197;&#21450;&#19982;&#21442;&#32771;&#20998;&#21106;&#30340;&#37325;&#21472;&#24230;&#12290;&#25105;&#20204;&#22312;https://github.com/ishaanb92/GeneralizedProbabilisticUNet&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Generalized Probabilistic U-Net, which extends the Probabilistic U-Net by allowing more general forms of the Gaussian distribution as the latent space distribution that can better approximate the uncertainty in the reference segmentations. We study the effect the choice of latent space distribution has on capturing the variation in the reference segmentations for lung tumors and white matter hyperintensities in the brain. We show that the choice of distribution affects the sample diversity of the predictions and their overlap with respect to the reference segmentations. We have made our implementation available at https://github.com/ishaanb92/GeneralizedProbabilisticUNet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;SOM&#65289;&#30340;&#32858;&#31867;&#26041;&#27861;&#24212;&#29992;&#20110;&#31561;&#31163;&#23376;&#20307;&#30913;&#38464;&#34746;&#19981;&#31283;&#23450;&#23436;&#20840;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32858;&#31867;&#32467;&#26524;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20316;&#20026;&#27169;&#25311;&#21644;&#35266;&#27979;&#25968;&#25454;&#21487;&#38752;&#20998;&#26512;&#24037;&#20855;&#65292;&#24182;&#32473;&#20986;&#20102;&#26377;&#30410;&#30340;&#29289;&#29702;&#35265;&#35299;&#21644;&#31354;&#38388;&#21306;&#22495;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.13469</link><description>&lt;p&gt;
&#37319;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;SOM&#65289;&#30340;&#31561;&#31163;&#23376;&#20307;&#30913;&#38464;&#34746;&#19981;&#31283;&#23450;&#23436;&#20840;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#26080;&#30417;&#30563;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Unsupervised classification of fully kinetic simulations of plasmoid instability using Self-Organizing Maps (SOMs). (arXiv:2304.13469v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;SOM&#65289;&#30340;&#32858;&#31867;&#26041;&#27861;&#24212;&#29992;&#20110;&#31561;&#31163;&#23376;&#20307;&#30913;&#38464;&#34746;&#19981;&#31283;&#23450;&#23436;&#20840;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32858;&#31867;&#32467;&#26524;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20316;&#20026;&#27169;&#25311;&#21644;&#35266;&#27979;&#25968;&#25454;&#21487;&#38752;&#20998;&#26512;&#24037;&#20855;&#65292;&#24182;&#32473;&#20986;&#20102;&#26377;&#30410;&#30340;&#29289;&#29702;&#35265;&#35299;&#21644;&#31354;&#38388;&#21306;&#22495;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#29289;&#29702;&#36807;&#31243;&#27169;&#25311;&#21644;&#35266;&#27979;&#20135;&#29983;&#30340;&#25968;&#25454;&#37327;&#22686;&#21152;&#65292;&#40723;&#21169;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#21644;&#29289;&#29702;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;SOM&#65289;&#30340;&#32858;&#31867;&#26041;&#27861;&#24212;&#29992;&#20110;&#31561;&#31163;&#23376;&#20307;&#30913;&#38464;&#34746;&#19981;&#31283;&#23450;&#23436;&#20840;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#26088;&#22312;&#35780;&#20272;&#20854;&#20316;&#20026;&#27169;&#25311;&#21644;&#35266;&#27979;&#25968;&#25454;&#21487;&#38752;&#20998;&#26512;&#24037;&#20855;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32858;&#31867;&#32467;&#26524;&#65292;&#19982;&#25105;&#20204;&#23545;&#36807;&#31243;&#30340;&#30693;&#35782;&#30456;&#31526;&#65306;&#32858;&#31867;&#28165;&#26970;&#22320;&#35782;&#21035;&#20102;&#36827;&#27969;&#21306;&#22495;&#12289;&#20869;&#37096;&#30913;&#38464;&#34746;&#21306;&#22495;&#12289;&#20998;&#31163;&#21306;&#22495;&#20197;&#21450;&#19982;&#30913;&#38464;&#34746;&#21512;&#24182;&#30456;&#20851;&#30340;&#21306;&#22495;&#12290;SOM&#29305;&#23450;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#20363;&#22914;&#29305;&#24449;&#26144;&#23556;&#21644;&#32479;&#19968;&#36317;&#31163;&#30697;&#38453;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#20851;&#20110;&#29289;&#29702;&#21644;&#29305;&#23450;&#31354;&#38388;&#21306;&#22495;&#30340;&#35265;&#35299;&#12290;&#35813;&#26041;&#27861;&#20284;&#20046;&#26159;&#20998;&#26512;&#27169;&#25311;&#21644;&#35266;&#27979;&#25968;&#25454;&#65292;&#19988;&#28508;&#22312;&#22320;&#20063;&#21487;&#20197;&#29992;&#20110;&#26174;&#31034;&#20174;&#20256;&#32479;&#26041;&#27861;&#21040;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#26041;&#27861;&#30340;&#36716;&#25442;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing amount of data produced by simulations and observations of space physics processes encourages the use of methods rooted in Machine Learning for data analysis and physical discovery. We apply a clustering method based on Self-Organizing Maps (SOM) to fully kinetic simulations of plasmoid instability, with the aim of assessing its suitability as a reliable analysis tool for both simulated and observed data. We obtain clusters that map well, a posteriori, to our knowledge of the process: the clusters clearly identify the inflow region, the inner plasmoid region, the separatrices, and regions associated with plasmoid merging. SOM-specific analysis tools, such as feature maps and Unified Distance Matrix, provide one with valuable insights into both the physics at work and specific spatial regions of interest. The method appears as a promising option for the analysis of data, both from simulations and from observations, and could also potentially be used to trigger the switch to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#25955;&#26080;&#31351;&#33539;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#27714;&#35299;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.13467</link><description>&lt;p&gt;
&#31163;&#25955;&#26080;&#31351;&#33539;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Polynomial-Time Solvers for the Discrete $\infty$-Optimal Transport Problems. (arXiv:2304.13467v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#25955;&#26080;&#31351;&#33539;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#27714;&#35299;&#31163;&#25955;&#26377;&#38480;&#30340;$\infty$-&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#20013;&#30340;Monge&#21644;Kantorovich&#34920;&#36848;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#25552;&#20986;&#36825;&#20123;&#38382;&#39064;&#30340;&#26377;&#25928;&#25968;&#20540;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note, we propose polynomial-time algorithms solving the Monge and Kantorovich formulations of the $\infty$-optimal transport problem in the discrete and finite setting. It is the first time, to the best of our knowledge, that efficient numerical methods for these problems have been proposed.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#39044;&#27979;&#33437;&#21152;&#21733;&#24066;&#30423;&#31363;&#29359;&#32618;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#37319;&#29992;XGBoost&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26368;&#20339;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;F1&#20998;&#25968;&#20026;0.86&#12290;</title><link>http://arxiv.org/abs/2304.13464</link><description>&lt;p&gt;
&#33437;&#21152;&#21733;&#24066;&#39044;&#27979;&#29305;&#23450;&#29359;&#32618;&#31867;&#22411;&#30340;&#22810;&#31181;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Multiple Methods for Predicting a Specific Type of Crime in the City of Chicago. (arXiv:2304.13464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#39044;&#27979;&#33437;&#21152;&#21733;&#24066;&#30423;&#31363;&#29359;&#32618;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#37319;&#29992;XGBoost&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26368;&#20339;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;F1&#20998;&#25968;&#20026;0.86&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#35748;&#20026;&#65292;&#29359;&#32618;&#26159;&#21463;&#21040;&#22810;&#31181;&#29289;&#29702;&#12289;&#31038;&#20250;&#21644;&#32463;&#27982;&#22240;&#32032;&#24433;&#21709;&#30340;&#31038;&#20250;&#29616;&#35937;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#29359;&#32618;&#25454;&#35828;&#26377;&#19981;&#21516;&#30340;&#21160;&#26426;&#12290;&#20363;&#22914;&#65292;&#30423;&#31363;&#26159;&#19968;&#31181;&#22522;&#20110;&#26426;&#20250;&#30340;&#29359;&#32618;&#65292;&#32780;&#35851;&#26432;&#26159;&#20986;&#20110;&#24773;&#24863;&#39537;&#21160;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#24403;&#39044;&#27979;&#21333;&#19968;&#29359;&#32618;&#26102;&#65292;&#20165;&#20351;&#29992;&#26102;&#31354;&#20449;&#24687;&#26102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26088;&#22312;&#39044;&#27979;&#30423;&#31363;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#31181;&#21487;&#20197;&#20351;&#29992;&#26102;&#31354;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#30340;&#29359;&#32618;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;&#8220;&#25105;&#20204;&#33021;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#26469;&#39044;&#27979;&#30423;&#31363;&#30340;&#34920;&#29616;&#22914;&#20309;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#19981;&#24179;&#34913;&#25216;&#26415;&#21644;&#36229;&#21442;&#25968;&#65292;&#32771;&#23519;&#20102;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#32447;&#24615;&#22238;&#24402;&#12289;XGBoost&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;k-&#26368;&#36817;&#37051;&#31561;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;XGBoost&#34920;&#29616;&#26368;&#20339;&#65292;F1&#20998;&#25968;&#20026;0.86&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers regard crime as a social phenomenon that is influenced by several physical, social, and economic factors. Different types of crimes are said to have different motivations. Theft, for instance, is a crime that is based on opportunity, whereas murder is driven by emotion. In accordance with this, we examine how well a model can perform with only spatiotemporal information at hand when it comes to predicting a single crime. More specifically, we aim at predicting theft, as this is a crime that should be predictable using spatiotemporal information. We aim to answer the question: "How well can we predict theft using spatial and temporal features?". To answer this question, we examine the effectiveness of support vector machines, linear regression, XGBoost, Random Forest, and k-nearest neighbours, using different imbalanced techniques and hyperparameters. XGBoost showed the best results with an F1-score of 0.86.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gromov-Wasserstein Discrepancy&#36873;&#25321;&#26368;&#20339;&#20107;&#20214;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#34920;&#31034;&#12289;&#32593;&#32476;&#39592;&#24178;&#21644;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20219;&#21153;&#24615;&#33021;&#25490;&#21517;&#30340;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#26041;&#27861;&#65292;&#26412;&#25991;&#23545;&#22823;&#22411;&#20107;&#20214;&#34920;&#31034;&#27861;&#23478;&#26063;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#29289;&#20307;&#26816;&#27979;&#30340;&#34920;&#31034;&#27861;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.13455</link><description>&lt;p&gt;
&#20174;&#28151;&#27788;&#20013;&#36856;&#21457;&#20986;&#31209;&#24207;&#65306;&#20026;&#29289;&#20307;&#26816;&#27979;&#25490;&#24207;&#20107;&#20214;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Chaos Comes Order: Ordering Event Representations for Object Detection. (arXiv:2304.13455v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gromov-Wasserstein Discrepancy&#36873;&#25321;&#26368;&#20339;&#20107;&#20214;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#34920;&#31034;&#12289;&#32593;&#32476;&#39592;&#24178;&#21644;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20219;&#21153;&#24615;&#33021;&#25490;&#21517;&#30340;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#26041;&#27861;&#65292;&#26412;&#25991;&#23545;&#22823;&#22411;&#20107;&#20214;&#34920;&#31034;&#27861;&#23478;&#26063;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#29289;&#20307;&#26816;&#27979;&#30340;&#34920;&#31034;&#27861;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22788;&#29702;&#20107;&#20214;&#30340;&#39030;&#23574;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;&#29616;&#25104;&#32593;&#32476;&#20043;&#21069;&#65292;&#39318;&#20808;&#23558;&#20854;&#36716;&#25442;&#20026;&#31264;&#23494;&#30340;&#32593;&#26684;&#29366;&#36755;&#20837;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#19978;&#20026;&#20219;&#21153;&#36873;&#25321;&#36866;&#24403;&#30340;&#34920;&#31034;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#34920;&#31034;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26681;&#25454;&#39564;&#35777;&#20998;&#25968;&#36873;&#25321;&#26368;&#20339;&#34920;&#31034;&#65292;&#36825;&#38750;&#24120;&#32791;&#26102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#21407;&#22987;&#20107;&#20214;&#21450;&#20854;&#34920;&#31034;&#20043;&#38388;&#30340;Gromov-Wasserstein Discrepancy (GWD)&#36873;&#25321;&#26368;&#20339;&#34920;&#31034;&#26469;&#28040;&#38500;&#36825;&#20010;&#29942;&#39048;&#12290;&#23427;&#30340;&#35745;&#31639;&#36895;&#24230;&#22823;&#32422;&#27604;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24555;200&#20493;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#34920;&#31034;&#12289;&#32593;&#32476;&#39592;&#24178;&#21644;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20107;&#20214;&#34920;&#31034;&#27861;&#20219;&#21153;&#24615;&#33021;&#25490;&#21517;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#25214;&#21040;&#20855;&#26377;&#39640;&#20219;&#21153;&#20998;&#25968;&#30340;&#34920;&#31034;&#30456;&#24403;&#20110;&#25214;&#21040;&#20855;&#26377;&#20302;GWD&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#39318;&#27425;&#23545;&#22823;&#22411;&#20107;&#20214;&#34920;&#31034;&#27861;&#23478;&#26063;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#29289;&#20307;&#26816;&#27979;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Moving MNIST&#21644;N-Caltech101&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#21518;&#32773;&#36798;&#21040;&#20102;83.0%&#30340;1%&#35823;&#25253;&#29575;&#19979;&#30340;mAP&#26032;&#30340;&#26368;&#39640;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input representations before using an off-the-shelf network. However, selecting the appropriate representation for the task traditionally requires training a neural network for each representation and selecting the best one based on the validation score, which is very time-consuming. In this work, we eliminate this bottleneck by selecting the best representation based on the Gromov-Wasserstein Discrepancy (GWD) between the raw events and their representation. It is approximately 200 times faster to compute than training a neural network and preserves the task performance ranking of event representations across multiple representations, network backbones, and datasets. This means that finding a representation with a high task score is equivalent to finding a representation with a low GWD. We use this insight to, for the first time, perform a hyperparameter search on a large family o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38544;&#24335;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65288;ICDA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#26679;&#26412;&#22686;&#24378;&#31574;&#30053;&#12289;&#26131;&#20110;&#35745;&#31639;&#30340;&#20195;&#29702;&#25439;&#22833;&#21644;&#20855;&#20307;&#26041;&#26696;&#65292;&#28040;&#38500;&#20102;&#34394;&#20551;&#20851;&#32852;&#24182;&#36827;&#34892;&#20102;&#31283;&#20581;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.13431</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24335;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Implicit Counterfactual Data Augmentation for Deep Neural Networks. (arXiv:2304.13431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38544;&#24335;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65288;ICDA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#26679;&#26412;&#22686;&#24378;&#31574;&#30053;&#12289;&#26131;&#20110;&#35745;&#31639;&#30340;&#20195;&#29702;&#25439;&#22833;&#21644;&#20855;&#20307;&#26041;&#26696;&#65292;&#28040;&#38500;&#20102;&#34394;&#20551;&#20851;&#32852;&#24182;&#36827;&#34892;&#20102;&#31283;&#20581;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26131;&#20110;&#25429;&#25417;&#38750;&#22240;&#26524;&#23646;&#24615;&#21644;&#31867;&#21035;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26159;&#30772;&#38500;&#36825;&#20123;&#34394;&#20551;&#30340;&#32852;&#24819;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26126;&#30830;&#29983;&#25104;&#21453;&#20107;&#23454;&#25968;&#25454;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#35757;&#32451;&#25928;&#29575;&#20250;&#38477;&#20302;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65288;Implicit Counterfactual Data Augmentation&#65292;ICDA&#65289;&#26041;&#27861;&#26469;&#28040;&#38500;&#34394;&#20551;&#20851;&#32852;&#24182;&#36827;&#34892;&#31283;&#20581;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26679;&#26412;&#22686;&#24378;&#31574;&#30053;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#29983;&#25104;&#22312;&#35821;&#20041;&#21644;&#21453;&#20107;&#23454;&#24847;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#28145;&#24230;&#29305;&#24449;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#30340;&#22686;&#24378;&#24378;&#24230;&#12290;&#20854;&#27425;&#65292;&#24403;&#22686;&#24191;&#26679;&#26412;&#25968;&#21464;&#20026;&#26080;&#31351;&#22823;&#26102;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23545;&#20110;&#22686;&#24191;&#29305;&#24449;&#38598;&#30340;&#26131;&#20110;&#35745;&#31639;&#30340;&#20195;&#29702;&#25439;&#22833;&#12290;&#31532;&#19977;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20855;&#20307;&#30340;&#26041;&#26696;&#65292;&#21253;&#25324;&#30452;&#25509;&#37327;&#21270;&#21644;&#20803;&#23398;&#20064;&#65292;&#20197;&#30830;&#23450;&#40065;&#26834;&#24615;&#25439;&#22833;&#30340;&#20851;&#38190;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#20174;&#23454;&#39564;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;ICDA&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning models are prone to capturing the spurious correlations between non-causal attributes and classes, with counterfactual data augmentation being a promising direction for breaking these spurious associations. However, explicitly generating counterfactual data is challenging, with the training efficiency declining. Therefore, this study proposes an implicit counterfactual data augmentation (ICDA) method to remove spurious correlations and make stable predictions. Specifically, first, a novel sample-wise augmentation strategy is developed that generates semantically and counterfactually meaningful deep features with distinct augmentation strength for each sample. Second, we derive an easy-to-compute surrogate loss on the augmented feature set when the number of augmented samples becomes infinite. Third, two concrete schemes are proposed, including direct quantification and meta-learning, to derive the key parameters for the robust loss. In addition, ICDA is explained from 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;AACR GENIE&#25968;&#25454;&#38598;&#30340;&#28082;&#24577;&#31070;&#32463;&#32593;&#32476;&#65288;LTC&#65289;&#35786;&#26029;&#31070;&#32463;&#32420;&#32500;&#30244;&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20197;99.86%&#30340;&#20934;&#30830;&#29575;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#24615;&#21644;&#40657;&#30418;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.13429</link><description>&lt;p&gt;
GENIE-NF-AI: &#20351;&#29992;&#22522;&#20110;AACR GENIE&#25968;&#25454;&#38598;&#30340;&#28082;&#24577;&#31070;&#32463;&#32593;&#32476;&#65288;LTC&#65289;&#35782;&#21035;&#31070;&#32463;&#32420;&#32500;&#30244;&#32959;&#30244;
&lt;/p&gt;
&lt;p&gt;
GENIE-NF-AI: Identifying Neurofibromatosis Tumors using Liquid Neural Network (LTC) trained on AACR GENIE Datasets. (arXiv:2304.13429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;AACR GENIE&#25968;&#25454;&#38598;&#30340;&#28082;&#24577;&#31070;&#32463;&#32593;&#32476;&#65288;LTC&#65289;&#35786;&#26029;&#31070;&#32463;&#32420;&#32500;&#30244;&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20197;99.86%&#30340;&#20934;&#30830;&#29575;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#24615;&#21644;&#40657;&#30418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21307;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#26469;&#25552;&#20379;&#26356;&#24555;&#65292;&#26356;&#20934;&#30830;&#30340;&#30142;&#30149;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#65292;&#20351;&#29992;&#34880;&#28082;&#26816;&#27979;&#21644;&#33268;&#30149;&#21464;&#37327;&#26469;&#35786;&#26029;&#24739;&#26377;&#31070;&#32463;&#32420;&#32500;&#30244;&#30340;&#24739;&#32773;&#12290;&#25105;&#20204;&#20351;&#29992;AACR GENIE&#39033;&#30446;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#29616;&#20195;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20197;99.86%&#30340;&#20934;&#30830;&#29575;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;NF1&#21644;&#21487;&#35299;&#37322;&#30340;AI&#27979;&#35797;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#21644;&#35828;&#26126;&#24615;&#21050;&#28608;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#20197;&#21450;&#19968;&#20010;&#40657;&#30418;&#27169;&#22411;&#12290;&#21487;&#35828;&#26126;&#30340;&#27169;&#22411;&#26377;&#21161;&#20110;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#32780;&#29627;&#29827;&#30418;&#27169;&#22411;&#25552;&#20379;&#26368;&#20339;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#65292;&#29992;&#20110;&#35786;&#26029;&#31070;&#32463;&#32420;&#32500;&#30244;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of medicine has been increasingly adopting artificial intelligence (AI) technologies to provide faster and more accurate disease detection, prediction, and assessment. In this study, we propose an interpretable AI approach to diagnose patients with neurofibromatosis using blood tests and pathogenic variables. We evaluated the proposed method using a dataset from the AACR GENIE project and compared its performance with modern approaches. Our proposed approach outperformed existing models with 99.86% accuracy. We also conducted NF1 and interpretable AI tests to validate our approach. Our work provides an explainable approach model using logistic regression and explanatory stimulus as well as a black-box model. The explainable models help to explain the predictions of black-box models while the glass-box models provide information about the best-fit features. Overall, our study presents an interpretable AI approach for diagnosing patients with neurofibromatosis 
&lt;/p&gt;</description></item><item><title>FLEX&#26159;&#19968;&#31181;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#25506;&#32034;&#31639;&#27861;&#65292;&#20351;&#29992;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#38656;&#35201;&#26368;&#23569;&#30340;&#36164;&#28304;&#65292;&#24182;&#29992;&#20110;&#19979;&#28216;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#32463;&#20856;&#25511;&#21046;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.13426</link><description>&lt;p&gt;
FLEX&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#25506;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FLEX: an Adaptive Exploration Algorithm for Nonlinear Systems. (arXiv:2304.13426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13426
&lt;/p&gt;
&lt;p&gt;
FLEX&#26159;&#19968;&#31181;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#25506;&#32034;&#31639;&#27861;&#65292;&#20351;&#29992;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#38656;&#35201;&#26368;&#23569;&#30340;&#36164;&#28304;&#65292;&#24182;&#29992;&#20110;&#19979;&#28216;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#32463;&#20856;&#25511;&#21046;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#21152;&#24378;&#23398;&#20064;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#25910;&#38598;&#36866;&#21512;&#31995;&#32479;&#30340;&#31934;&#30830;&#27169;&#22411;&#30340;&#25968;&#25454;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#22240;&#27492;&#20197;&#26679;&#26412;&#26377;&#25928;&#30340;&#26041;&#24335;&#25506;&#32034;&#26410;&#30693;&#29615;&#22659;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21160;&#21147;&#23398;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#23454;&#38469;&#31995;&#32479;&#30340;&#35745;&#31639;&#38480;&#21046;&#20351;&#24471;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FLEX&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#25506;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#26368;&#22823;&#21270;&#19979;&#19968;&#27493;&#20449;&#24687;&#65292;&#20174;&#32780;&#24471;&#21040;&#33258;&#36866;&#24212;&#25506;&#32034;&#31639;&#27861;&#65292;&#19982;&#36890;&#29992;&#21442;&#25968;&#21270;&#23398;&#20064;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#19988;&#38656;&#35201;&#26368;&#23569;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#19981;&#21516;&#35774;&#32622;&#30340;&#33509;&#24178;&#38750;&#32447;&#24615;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26102;&#21464;&#21160;&#21147;&#23398;&#12290;&#29282;&#35760;&#25506;&#32034;&#26159;&#20026;&#20102;&#26381;&#21153;&#20110;&#24320;&#21457;&#24615;&#30446;&#26631;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#19979;&#28216;&#22522;&#20110;&#27169;&#22411;&#30340;&#32463;&#20856;&#25511;&#21046;&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#27169;&#22411;&#33258;&#30001;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning is a powerful tool, but collecting data to fit an accurate model of the system can be costly. Exploring an unknown environment in a sample-efficient manner is hence of great importance. However, the complexity of dynamics and the computational limitations of real systems make this task challenging. In this work, we introduce FLEX, an exploration algorithm for nonlinear dynamics based on optimal experimental design. Our policy maximizes the information of the next step and results in an adaptive exploration algorithm, compatible with generic parametric learning models and requiring minimal resources. We test our method on a number of nonlinear environments covering different settings, including time-varying dynamics. Keeping in mind that exploration is intended to serve an exploitation objective, we also test our algorithm on downstream model-based classical control tasks and compare it to other state-of-the-art model-based and model-free approaches. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23545;&#36229;&#20986;&#20998;&#24067;&#30340;&#8220;&#21487;&#25511;&#8221;&#29366;&#24577;&#30340;&#8220;&#25509;&#21147;&#27867;&#21270;&#8221;&#24615;&#33021;&#12290;&#36890;&#36807;&#35753;&#27979;&#35797;&#20195;&#29702;&#20174;&#20854;&#20182;&#38476;&#29983;&#20195;&#29702;&#30340;&#36712;&#36857;&#20013;&#38388;&#24320;&#22987;&#65292;&#21457;&#29616;&#36825;&#31181;&#27867;&#21270;&#26222;&#36941;&#23384;&#22312;&#27867;&#21270;&#22833;&#25928;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13424</link><description>&lt;p&gt;
&#19982;&#38476;&#29983;&#20154;&#19968;&#36215;&#36305;&#25509;&#21147;&#36187;&#65311;&#24378;&#21270;&#23398;&#20064;&#22312;&#36229;&#20986;&#20998;&#24067;&#36712;&#36857;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories. (arXiv:2304.13424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23545;&#36229;&#20986;&#20998;&#24067;&#30340;&#8220;&#21487;&#25511;&#8221;&#29366;&#24577;&#30340;&#8220;&#25509;&#21147;&#27867;&#21270;&#8221;&#24615;&#33021;&#12290;&#36890;&#36807;&#35753;&#27979;&#35797;&#20195;&#29702;&#20174;&#20854;&#20182;&#38476;&#29983;&#20195;&#29702;&#30340;&#36712;&#36857;&#20013;&#38388;&#24320;&#22987;&#65292;&#21457;&#29616;&#36825;&#31181;&#27867;&#21270;&#26222;&#36941;&#23384;&#22312;&#27867;&#21270;&#22833;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#25913;&#36827;&#21508;&#31181;&#29366;&#24577;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#23545;&#36229;&#20986;&#20998;&#24067;&#30340;&#8220;&#21487;&#25511;&#8221;&#29366;&#24577;&#30340;&#8220;&#25509;&#21147;&#27867;&#21270;&#8221;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#27979;&#35797;&#20195;&#29702;&#20174;&#20854;&#20182;&#29420;&#31435;&#35757;&#32451;&#33391;&#22909;&#30340;&#8220;&#38476;&#29983;&#8221;&#20195;&#29702;&#30340;&#36712;&#36857;&#30340;&#20013;&#38388;&#24320;&#22987;&#65292;&#25105;&#20204;&#23454;&#38469;&#35780;&#20272;&#20102;&#36825;&#31181;&#27867;&#21270;&#31867;&#22411;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26469;&#33258;&#38476;&#29983;&#20195;&#29702;&#30340;&#21487;&#25511;&#29366;&#24577;&#20960;&#20046;&#26222;&#36941;&#23384;&#22312;&#27867;&#21270;&#22833;&#25928;&#12290;&#20363;&#22914;&#65292;&#22312;&#20154;&#24418;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;PPO&#20195;&#29702;&#65292;&#22312;&#27491;&#24120;&#27979;&#35797;&#26399;&#38388;&#21482;&#26377;3.9&#65285;&#30340;&#22833;&#36133;&#29575;&#65292;&#20294;&#22312;10&#20010;&#38476;&#29983;&#20195;&#29702;&#30340;&#36712;&#36857;&#20013;&#65292;&#22833;&#36133;&#29575;&#21319;&#39640;&#21040;31.4&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we define, evaluate, and improve the ``relay-generalization'' performance of reinforcement learning (RL) agents on the out-of-distribution ``controllable'' states. Ideally, an RL agent that generally masters a task should reach its goal starting from any controllable state of the environment instead of memorizing a small set of trajectories. For example, a self-driving system should be able to take over the control from humans in the middle of driving and must continue to drive the car safely. To practically evaluate this type of generalization, we start the test agent from the middle of other independently well-trained \emph{stranger} agents' trajectories. With extensive experimental evaluation, we show the prevalence of \emph{generalization failure} on controllable states from stranger agents. For example, in the Humanoid environment, we observed that a well-trained Proximal Policy Optimization (PPO) agent, with only 3.9\% failure rate during regular testing, failed on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22788;&#29702;&#19981;&#23436;&#25972;&#35266;&#27979;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22238;&#24402;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#26631;&#31614;&#20540;&#30001;&#20110;&#19981;&#23436;&#25972;&#35266;&#27979;&#23548;&#33268;&#23398;&#20064;&#32467;&#26524;&#20559;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13415</link><description>&lt;p&gt;
&#21253;&#21547;&#19981;&#23436;&#25972;&#35266;&#27979;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#22238;&#24402;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Regression with Sensor Data Containing Incomplete Observations. (arXiv:2304.13415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22788;&#29702;&#19981;&#23436;&#25972;&#35266;&#27979;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22238;&#24402;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#26631;&#31614;&#20540;&#30001;&#20110;&#19981;&#23436;&#25972;&#35266;&#27979;&#23548;&#33268;&#23398;&#20064;&#32467;&#26524;&#20559;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#36755;&#20986;&#26631;&#31614;&#20540;&#26159;&#24863;&#24212;&#29616;&#35937;&#24133;&#24230;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#26631;&#31614;&#20540;&#20302;&#21487;&#33021;&#24847;&#21619;&#30528;&#29616;&#35937;&#30340;&#23454;&#38469;&#24133;&#24230;&#20302;&#25110;&#20256;&#24863;&#22120;&#20570;&#20986;&#20102;&#19981;&#23436;&#25972;&#30340;&#35266;&#27979;&#12290;&#36825;&#23548;&#33268;&#26631;&#31614;&#20540;&#20559;&#20302;&#65292;&#23398;&#20064;&#32467;&#26524;&#20063;&#20559;&#20302;&#65292;&#22240;&#20026;&#26631;&#31614;&#20540;&#21487;&#33021;&#30001;&#20110;&#19981;&#23436;&#25972;&#30340;&#35266;&#27979;&#32780;&#20559;&#20302;&#65292;&#21363;&#20351;&#29616;&#35937;&#30340;&#23454;&#38469;&#24133;&#24230;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#65292;&#26174;&#24335;&#22320;&#27169;&#25311;&#20102;&#24102;&#26377;&#36127;&#20540;&#30340;&#19981;&#23545;&#31216;&#22122;&#22768;&#30340;&#19981;&#23436;&#25972;&#35266;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#26080;&#20559;&#30340;&#65292;&#23601;&#20687;&#20174;&#19981;&#21253;&#21547;&#19981;&#23436;&#25972;&#35266;&#27979;&#30340;&#26410;&#25439;&#22351;&#25968;&#25454;&#23398;&#20064;&#19968;&#26679;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses a regression problem in which output label values are the results of sensing the magnitude of a phenomenon. A low value of such labels can mean either that the actual magnitude of the phenomenon was low or that the sensor made an incomplete observation. This leads to a bias toward lower values in labels and its resultant learning because labels may have lower values due to incomplete observations, even if the actual magnitude of the phenomenon was high. Moreover, because an incomplete observation does not provide any tags indicating incompleteness, we cannot eliminate or impute them. To address this issue, we propose a learning algorithm that explicitly models incomplete observations corrupted with an asymmetric noise that always has a negative value. We show that our algorithm is unbiased as if it were learned from uncorrupted data that does not involve incomplete observations. We demonstrate the advantages of our algorithm through numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#37327;&#23376;&#23494;&#30721;&#30340;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#25910;&#25947;&#21644;&#23433;&#20840;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.13413</link><description>&lt;p&gt;
&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#36890;&#20449;&#27169;&#22411;&#65306;&#19968;&#31181;&#21518;&#37327;&#23376;&#23494;&#30721;&#65288;PQC&#65289;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Secure Communication Model For Quantum Federated Learning: A Post Quantum Cryptography (PQC) Framework. (arXiv:2304.13413v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#37327;&#23376;&#23494;&#30721;&#30340;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#25910;&#25947;&#21644;&#23433;&#20840;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21518;&#37327;&#23376;&#23494;&#30721;&#65288;PQC&#65289;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#65288;QFL&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#21160;&#24577;&#26381;&#21153;&#22120;&#36873;&#25321;&#30340;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#25910;&#25947;&#21644;&#23433;&#20840;&#26465;&#20214;&#12290;&#23454;&#29616;&#21644;&#32467;&#26524;&#21487;&#20844;&#24320;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design a model of Post Quantum Cryptography (PQC) Quantum Federated Learning (QFL). We develop a framework with a dynamic server selection and study convergence and security conditions. The implementation and results are publicly available1.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20013;&#38388;&#23618;&#25200;&#21160;&#34928;&#20943;&#65288;ILPD&#65289;&#30340;&#26032;&#22411;&#20013;&#38388;&#23618;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20248;&#21270;&#38454;&#27573;&#21046;&#20316;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#40723;&#21169;&#20013;&#38388;&#23618;&#25200;&#21160;&#22788;&#20110;&#26377;&#25928;&#30340;&#25932;&#23545;&#26041;&#21521;&#65292;&#24182;&#21516;&#26102;&#20855;&#26377;&#24456;&#22823;&#30340;&#24133;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13410</link><description>&lt;p&gt;
&#20351;&#29992;&#20013;&#38388;&#23618;&#25200;&#21160;&#34928;&#20943;&#26469;&#25552;&#39640;&#25932;&#23545;&#36801;&#31227;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Transferability by Intermediate-level Perturbation Decay. (arXiv:2304.13410v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20013;&#38388;&#23618;&#25200;&#21160;&#34928;&#20943;&#65288;ILPD&#65289;&#30340;&#26032;&#22411;&#20013;&#38388;&#23618;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20248;&#21270;&#38454;&#27573;&#21046;&#20316;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#40723;&#21169;&#20013;&#38388;&#23618;&#25200;&#21160;&#22788;&#20110;&#26377;&#25928;&#30340;&#25932;&#23545;&#26041;&#21521;&#65292;&#24182;&#21516;&#26102;&#20855;&#26377;&#24456;&#22823;&#30340;&#24133;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#38388;&#23618;&#25915;&#20987;&#26159;&#25351;&#36890;&#36807;&#36981;&#24490;&#23545;&#25239;&#26041;&#21521;&#65292;&#23581;&#35797;&#25200;&#21160;&#29305;&#24449;&#34920;&#31034;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#22312;&#21046;&#20316;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#26679;&#26412;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#36825;&#31867;&#25915;&#20987;&#26041;&#27861;&#36890;&#24120;&#30001;&#20004;&#20010;&#20998;&#31163;&#30340;&#38454;&#27573;&#26500;&#25104;&#65292;&#39318;&#20808;&#38656;&#35201;&#30830;&#23450;&#19968;&#20010;&#26041;&#21521;&#21521;&#23548;&#65292;&#28982;&#21518;&#25193;&#22823;&#20013;&#38388;&#23618;&#25200;&#21160;&#23545;&#35813;&#26041;&#21521;&#21521;&#23548;&#30340;&#26631;&#37327;&#25237;&#24433;&#12290;&#28982;&#32780;&#65292;&#24471;&#21040;&#30340;&#25200;&#21160;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#38590;&#20813;&#20250;&#20559;&#31163;&#21521;&#23548;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#36825;&#31181;&#20559;&#31163;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#20013;&#38388;&#23618;&#25200;&#21160;&#34928;&#20943;&#65288;ILPD&#65289;&#30340;&#26032;&#22411;&#20013;&#38388;&#23618;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20248;&#21270;&#38454;&#27573;&#21046;&#20316;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26041;&#27861;&#40723;&#21169;&#20013;&#38388;&#23618;&#25200;&#21160;&#22788;&#20110;&#26377;&#25928;&#30340;&#25932;&#23545;&#26041;&#21521;&#65292;&#24182;&#21516;&#26102;&#20855;&#26377;&#24456;&#22823;&#30340;&#24133;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intermediate-level attacks that attempt to perturb feature representations following an adversarial direction drastically have shown favorable performance in crafting transferable adversarial examples. Existing methods in this category are normally formulated with two separate stages, where a directional guide is required to be determined at first and the scalar projection of the intermediate-level perturbation onto the directional guide is enlarged thereafter. The obtained perturbation deviates from the guide inevitably in the feature space, and it is revealed in this paper that such a deviation may lead to sub-optimal attack. To address this issue, we develop a novel intermediate-level method that crafts adversarial examples within a single stage of optimization. In particular, the proposed method, named intermediate-level perturbation decay (ILPD), encourages the intermediate-level perturbation to be in an effective adversarial direction and to possess a great magnitude simultaneous
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;FedVS&#65292;&#19968;&#31181;&#21516;&#26102;&#35299;&#20915;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#28382;&#21518;&#23458;&#25143;&#31471;&#21644;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26412;&#22320;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#31192;&#23494;&#20849;&#20139;&#26041;&#26696;&#65292;&#20197;&#20445;&#35777;&#20449;&#24687;&#29702;&#35770;&#38544;&#31169;&#65292;&#24182;&#36890;&#36807;&#35299;&#23494;&#35745;&#31639;&#32929;&#20221;&#65292;&#26080;&#25439;&#37325;&#26500;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#23884;&#20837;&#30340;&#27719;&#24635;&#12290;</title><link>http://arxiv.org/abs/2304.13407</link><description>&lt;p&gt;
FedVS: &#38754;&#21521;&#20998;&#21106;&#27169;&#22411;&#30340;&#23481;&#38169;&#21644;&#38544;&#31169;&#20445;&#25252;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedVS: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models. (arXiv:2304.13407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;FedVS&#65292;&#19968;&#31181;&#21516;&#26102;&#35299;&#20915;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#28382;&#21518;&#23458;&#25143;&#31471;&#21644;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26412;&#22320;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#31192;&#23494;&#20849;&#20139;&#26041;&#26696;&#65292;&#20197;&#20445;&#35777;&#20449;&#24687;&#29702;&#35770;&#38544;&#31169;&#65292;&#24182;&#36890;&#36807;&#35299;&#23494;&#35745;&#31639;&#32929;&#20221;&#65292;&#26080;&#25439;&#37325;&#26500;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#23884;&#20837;&#30340;&#27719;&#24635;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#30001;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#35768;&#22810;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#32452;&#25104;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#34987;&#22402;&#30452;&#20998;&#21106;&#65292;&#19981;&#21516;&#30340;&#29305;&#24449;&#23384;&#20648;&#22312;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#19978;&#12290;&#20998;&#21106;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#38382;&#39064;&#26159;&#35757;&#32451;&#19968;&#20010;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#21010;&#20998;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#20998;&#21106;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#36831;&#28382;&#30340;&#23458;&#25143;&#31471;&#36896;&#25104;&#30340;&#24615;&#33021;&#19979;&#38477;&#65307;2&#65289;&#23458;&#25143;&#31471;&#19978;&#20256;&#25968;&#25454;&#23884;&#20837;&#23548;&#33268;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FedVS&#26469;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;FedVS&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#35774;&#35745;&#26412;&#22320;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#31192;&#23494;&#20849;&#20139;&#26041;&#26696;&#65292;&#20174;&#32780;&#20445;&#35777;&#38024;&#23545;&#21246;&#32467;&#23458;&#25143;&#21644;&#22909;&#22855;&#26381;&#21153;&#22120;&#30340;&#20449;&#24687;&#29702;&#35770;&#38544;&#31169;&#65292;&#24182;&#19988;&#36890;&#36807;&#35299;&#23494;&#35745;&#31639;&#32929;&#20221;&#65292;&#26080;&#25439;&#37325;&#26500;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#23884;&#20837;&#30340;&#27719;&#24635;&#12290;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;VFL&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#34920;&#26684;&#65292;CV&#65292;&#22270;&#20687;&#65292;NLP&#65289;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;FedVS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a vertical federated learning (VFL) system consisting of a central server and many distributed clients, the training data are vertically partitioned such that different features are privately stored on different clients. The problem of split VFL is to train a model split between the server and the clients. This paper aims to address two major challenges in split VFL: 1) performance degradation due to straggling clients during training; and 2) data and model privacy leakage from clients' uploaded data embeddings. We propose FedVS to simultaneously address these two challenges. The key idea of FedVS is to design secret sharing schemes for the local data and models, such that information-theoretical privacy against colluding clients and curious server is guaranteed, and the aggregation of all clients' embeddings is reconstructed losslessly, via decrypting computation shares from the non-straggling clients. Extensive experiments on various types of VFL datasets (including tabular, CV, a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;Simultaneous label hierarchy Exploration And Learning (SEAL)&#65292;&#36890;&#36807;&#22686;&#21152;&#36981;&#24490;&#20808;&#39564;&#23618;&#27425;&#32467;&#26500;&#30340;&#28508;&#22312;&#26631;&#31614;&#26469;&#25506;&#32034;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;1-Wasserstein&#24230;&#37327;&#22312;&#26641;&#24230;&#37327;&#31354;&#38388;&#19978;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#21644;&#25191;&#34892;&#65288;&#21322;&#65289;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#21644;&#23500;&#26377;&#35265;&#22320;&#30340;&#26631;&#31614;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.13374</link><description>&lt;p&gt;
SEAL:&#21516;&#26102;&#25506;&#32034;&#21644;&#23398;&#20064;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SEAL: Simultaneous Label Hierarchy Exploration And Learning. (arXiv:2304.13374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13374
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;Simultaneous label hierarchy Exploration And Learning (SEAL)&#65292;&#36890;&#36807;&#22686;&#21152;&#36981;&#24490;&#20808;&#39564;&#23618;&#27425;&#32467;&#26500;&#30340;&#28508;&#22312;&#26631;&#31614;&#26469;&#25506;&#32034;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;1-Wasserstein&#24230;&#37327;&#22312;&#26641;&#24230;&#37327;&#31354;&#38388;&#19978;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#21644;&#25191;&#34892;&#65288;&#21322;&#65289;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#21644;&#23500;&#26377;&#35265;&#22320;&#30340;&#26631;&#31614;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#26159;&#22686;&#24378;&#20998;&#31867;&#24615;&#33021;&#30340;&#37325;&#35201;&#22806;&#37096;&#30693;&#35782;&#28304;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#21487;&#33021;&#26080;&#27861;&#21305;&#37197;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Simultaneous label hierarchy Exploration And Learning (SEAL)&#65292;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#36981;&#24490;&#20808;&#39564;&#23618;&#27425;&#32467;&#26500;&#30340;&#28508;&#22312;&#26631;&#31614;&#26469;&#25506;&#32034;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26641;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;1-Wasserstein&#24230;&#37327;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#21644;&#25191;&#34892;&#65288;&#21322;&#65289;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#26377;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24773;&#20917;&#19979;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#24182;&#25581;&#31034;&#20102;&#23500;&#26377;&#35265;&#22320;&#30340;&#26631;&#31614;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21487;&#22312;https://github.com/tzq1999/SEAL&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label hierarchy is an important source of external knowledge that can enhance classification performance. However, most existing methods rely on predefined label hierarchies that may not match the data distribution. To address this issue, we propose Simultaneous label hierarchy Exploration And Learning (SEAL), a new framework that explores the label hierarchy by augmenting the observed labels with latent labels that follow a prior hierarchical structure. Our approach uses a 1-Wasserstein metric over the tree metric space as an objective function, which enables us to simultaneously learn a data-driven label hierarchy and perform (semi-)supervised learning. We evaluate our method on several datasets and show that it achieves superior results in both supervised and semi-supervised scenarios and reveals insightful label structures. Our implementation is available at https://github.com/tzq1999/SEAL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24310;&#36831;&#21453;&#39304;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;F^3&#65292;&#20351;&#29992;&#24310;&#36831;&#30340;&#35823;&#24046;&#20449;&#24687;&#26469;&#32553;&#25918;&#26799;&#24230;&#20174;&#32780;&#25552;&#39640;&#29983;&#29289;&#21487;&#34892;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20026;&#20302;&#33021;&#37327;&#35757;&#32451;&#21644;&#24182;&#34892;&#21270;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.13372</link><description>&lt;p&gt;
&#24310;&#36831;&#21453;&#39304;&#30340;&#21069;&#39304;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Feed-Forward Optimization With Delayed Feedback for Neural Networks. (arXiv:2304.13372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24310;&#36831;&#21453;&#39304;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;F^3&#65292;&#20351;&#29992;&#24310;&#36831;&#30340;&#35823;&#24046;&#20449;&#24687;&#26469;&#32553;&#25918;&#26799;&#24230;&#20174;&#32780;&#25552;&#39640;&#29983;&#29289;&#21487;&#34892;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20026;&#20302;&#33021;&#37327;&#35757;&#32451;&#21644;&#24182;&#34892;&#21270;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#29983;&#29289;&#23398;&#19978;&#30340;&#25209;&#35780;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#33258;&#28982;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21487;&#34892;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#35299;&#20915;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#21363;&#26435;&#37325;&#20256;&#36755;&#21644;&#26356;&#26032;&#38145;&#23450;&#65292;&#20197;&#23454;&#29616;&#29983;&#29289;&#21487;&#34892;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24310;&#36831;&#21453;&#39304;&#30340;&#21069;&#39304;&#65288;F^3&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#24310;&#36831;&#30340;&#35823;&#24046;&#20449;&#24687;&#20316;&#20026;&#26679;&#26412;&#32423;&#32553;&#25918;&#22240;&#23376;&#26469;&#26356;&#20934;&#30830;&#22320;&#36817;&#20284;&#26799;&#24230;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;F^3&#23558;&#29983;&#29289;&#21487;&#34892;&#24615;&#35757;&#32451;&#31639;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#20043;&#38388;&#30340;&#39044;&#27979;&#24615;&#33021;&#24046;&#36317;&#32553;&#23567;&#20102;&#39640;&#36798;96&#65285;&#12290;&#36825;&#35777;&#26126;&#20102;&#29983;&#29289;&#21487;&#34892;&#24615;&#35757;&#32451;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#20026;&#20302;&#33021;&#37327;&#35757;&#32451;&#21644;&#24182;&#34892;&#21270;&#24320;&#36767;&#20102;&#26377; promising &#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation has long been criticized for being biologically implausible, relying on concepts that are not viable in natural learning processes. This paper proposes an alternative approach to solve two core issues, i.e., weight transport and update locking, for biological plausibility and computational efficiency. We introduce Feed-Forward with delayed Feedback (F$^3$), which improves upon prior work by utilizing delayed error information as a sample-wise scaling factor to approximate gradients more accurately. We find that F$^3$ reduces the gap in predictive performance between biologically plausible training algorithms and backpropagation by up to 96%. This demonstrates the applicability of biologically plausible training and opens up promising new avenues for low-energy training and parallelization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#20010;&#22522;&#20110;LoRaWAN&#30340;&#26234;&#33021;&#26657;&#22253;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;k&#36817;&#37051;&#21644;LSTM&#26041;&#27861;&#22788;&#29702;&#20002;&#22833;&#20540;&#21644;&#39044;&#27979;&#26410;&#26469;&#35835;&#25968;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#25151;&#38388;&#20869;&#20154;&#25968;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;95&#65285;&#12290;</title><link>http://arxiv.org/abs/2304.13366</link><description>&lt;p&gt;
&#22522;&#20110;LoRaWAN&#30340;&#26234;&#33021;&#26657;&#22253;&#65306;&#25968;&#25454;&#38598;&#21644;&#20154;&#27969;&#35745;&#25968;&#22120;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
LoRaWAN-enabled Smart Campus: The Dataset and a People Counter Use Case. (arXiv:2304.13366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#20010;&#22522;&#20110;LoRaWAN&#30340;&#26234;&#33021;&#26657;&#22253;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;k&#36817;&#37051;&#21644;LSTM&#26041;&#27861;&#22788;&#29702;&#20002;&#22833;&#20540;&#21644;&#39044;&#27979;&#26410;&#26469;&#35835;&#25968;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#25151;&#38388;&#20869;&#20154;&#25968;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;95&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#22312;&#26234;&#33021;&#26657;&#22253;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;LoRaWAN&#30340;&#26234;&#33021;&#26657;&#22253;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#25551;&#36848;&#12290;LoRaWAN&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#21487;&#20197;&#20026;&#25968;&#30334;&#20010;&#29289;&#32852;&#32593;&#35774;&#22791;&#25552;&#20379;&#26381;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#23558;&#35774;&#22791;&#36830;&#25509;&#21040;&#26381;&#21153;&#22120;&#30340;LoRa&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20002;&#22833;&#30340;&#20256;&#36755;&#24182;&#25552;&#20986;&#20102;k&#36817;&#37051;&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#32570;&#22833;&#20540;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#35835;&#25968;&#12290;&#26368;&#21518;&#65292;&#20316;&#20026;&#19968;&#20010;&#31034;&#20363;&#24212;&#29992;&#31243;&#24207;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#22522;&#20110;&#25152;&#36873;&#20256;&#24863;&#22120;&#30340;&#35835;&#25968;&#39044;&#27979;&#25151;&#38388;&#20869;&#20154;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#39044;&#27979;&#20154;&#25968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;95&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#65292;&#24182;&#19988;&#26377;&#35814;&#32454;&#35828;&#26126;&#65292;&#36825;&#26159;&#25506;&#32034;&#20854;&#20182;&#21151;&#33021;&#21644;&#24212;&#29992;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoT has a significant role in the smart campus. This paper presents a detailed description of the Smart Campus dataset based on LoRaWAN. LoRaWAN is an emerging technology that enables serving hundreds of IoT devices. First, we describe the LoRa network that connects the devices to the server. Afterward, we analyze the missing transmissions and propose a k-nearest neighbor solution to handle the missing values. Then, we predict future readings using a long short-term memory (LSTM). Finally, as one example application, we build a deep neural network to predict the number of people inside a room based on the selected sensor's readings. Our results show that our model achieves an accuracy of $95 \: \%$ in predicting the number of people. Moreover, the dataset is openly available and described in detail, which is opportunity for exploration of other features and applications.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807; Concept-Monitor&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#26469;&#33258;&#21160;&#35299;&#23494;&#40657;&#21283;&#23376; DNN &#35757;&#32451;&#36807;&#31243;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#29702;&#35299; DNN &#22312;&#35757;&#32451;&#26399;&#38388;&#22914;&#20309;&#21457;&#23637;&#65292;&#21516;&#26102;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#28608;&#21169;&#38544;&#34255;&#31070;&#32463;&#20803;&#23398;&#20064;&#22810;&#26679;&#30340;&#27010;&#24565;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36816;&#29992; Concept-Monitor &#23545;&#19981;&#21516;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20102;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#32593;&#32476;&#20462;&#21098;&#12290;</title><link>http://arxiv.org/abs/2304.13346</link><description>&lt;p&gt;
Concept-Monitor: &#36890;&#36807;&#20010;&#21035;&#31070;&#32463;&#20803;&#29702;&#35299; DNN &#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Concept-Monitor: Understanding DNN training through individual neurons. (arXiv:2304.13346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13346
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807; Concept-Monitor&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#26469;&#33258;&#21160;&#35299;&#23494;&#40657;&#21283;&#23376; DNN &#35757;&#32451;&#36807;&#31243;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#29702;&#35299; DNN &#22312;&#35757;&#32451;&#26399;&#38388;&#22914;&#20309;&#21457;&#23637;&#65292;&#21516;&#26102;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#28608;&#21169;&#38544;&#34255;&#31070;&#32463;&#20803;&#23398;&#20064;&#22810;&#26679;&#30340;&#27010;&#24565;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36816;&#29992; Concept-Monitor &#23545;&#19981;&#21516;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20102;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#32593;&#32476;&#20462;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; Concept-Monitor &#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#32479;&#19968;&#23884;&#20837;&#31354;&#38388;&#21644;&#27010;&#24565;&#22810;&#26679;&#24615;&#24230;&#37327;&#26469;&#24110;&#21161;&#33258;&#21160;&#21270;&#22320;&#35299;&#23494;&#40657;&#21283;&#23376; DNN &#35757;&#32451;&#36807;&#31243;&#12290;Concept-Monitor &#21487;&#20197;&#21576;&#29616;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#21644;&#25351;&#26631;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#28145;&#20837;&#20102;&#35299; DNN &#22312;&#35757;&#32451;&#26399;&#38388;&#22914;&#20309;&#21457;&#23637;&#12290;&#21463;&#36825;&#20123;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#27491;&#21017;&#21270;&#22120;&#65292;&#28608;&#21169;&#38544;&#34255;&#31070;&#32463;&#20803;&#23398;&#20064;&#22810;&#26679;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#27491;&#21017;&#21270;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36816;&#29992; Concept-Monitor &#26469;&#23545;&#19981;&#21516;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#36890;&#36807;&#8220;&#20013;&#22870;&#24425;&#31080;&#20551;&#35774;&#8221;&#36827;&#34892;&#32593;&#32476;&#20462;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a general framework called Concept-Monitor to help demystify the black-box DNN training processes automatically using a novel unified embedding space and concept diversity metric. Concept-Monitor enables human-interpretable visualization and indicators of the DNN training processes and facilitates transparency as well as deeper understanding on how DNNs develop along the during training. Inspired by these findings, we also propose a new training regularizer that incentivizes hidden neurons to learn diverse concepts, which we show to improve training performance. Finally, we apply Concept-Monitor to conduct several case studies on different training paradigms including adversarial training, fine-tuning and network pruning via the Lottery Ticket Hypothesis
&lt;/p&gt;</description></item><item><title>OpenBox&#26159;&#19968;&#20010;&#36890;&#29992;&#40657;&#30418;&#20248;&#21270;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#21487;&#35270;&#21270;&#21151;&#33021;&#65292;&#27169;&#22359;&#21270;&#35774;&#35745;&#33021;&#22815;&#22312;&#29616;&#26377;&#31995;&#32479;&#20013;&#28789;&#27963;&#37096;&#32626;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20854;&#27604;&#29616;&#26377;&#31995;&#32479;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.13339</link><description>&lt;p&gt;
OpenBox&#65306;&#36890;&#29992;&#40657;&#30418;&#20248;&#21270;&#30340; Python &#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
OpenBox: A Python Toolkit for Generalized Black-box Optimization. (arXiv:2304.13339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13339
&lt;/p&gt;
&lt;p&gt;
OpenBox&#26159;&#19968;&#20010;&#36890;&#29992;&#40657;&#30418;&#20248;&#21270;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#21487;&#35270;&#21270;&#21151;&#33021;&#65292;&#27169;&#22359;&#21270;&#35774;&#35745;&#33021;&#22815;&#22312;&#29616;&#26377;&#31995;&#32479;&#20013;&#28789;&#27963;&#37096;&#32626;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20854;&#27604;&#29616;&#26377;&#31995;&#32479;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#20248;&#21270;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#12289;&#23454;&#39564;&#35774;&#35745;&#21644;&#25968;&#25454;&#24211;&#21442;&#25968;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#29616;&#26377;&#36719;&#20214;&#21253;&#26102;&#65292;&#29992;&#25143;&#22312;&#36866;&#29992;&#24615;&#12289;&#24615;&#33021;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; OpenBox&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#40657;&#30418;&#20248;&#21270;&#24037;&#20855;&#21253;&#65292;&#25552;&#39640;&#20102;&#20854;&#21487;&#29992;&#24615;&#12290;&#23427;&#23454;&#29616;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#21487;&#35270;&#21270;&#21151;&#33021;&#65292;&#35753;&#29992;&#25143;&#33021;&#22815;&#23450;&#20041;&#21644;&#31649;&#29702;&#20219;&#21153;&#12290;OpenBox &#30340;&#27169;&#22359;&#21270;&#35774;&#35745;&#26377;&#21161;&#20110;&#22312;&#29616;&#26377;&#31995;&#32479;&#20013;&#28789;&#27963;&#37096;&#32626;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenBox&#27604;&#29616;&#26377;&#31995;&#32479;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;OpenBox &#30340;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/PKU-DAIR/open-box &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box optimization (BBO) has a broad range of applications, including automatic machine learning, experimental design, and database knob tuning. However, users still face challenges when applying BBO methods to their problems at hand with existing software packages in terms of applicability, performance, and efficiency. This paper presents OpenBox, an open-source BBO toolkit with improved usability. It implements user-friendly inferfaces and visualization for users to define and manage their tasks. The modular design behind OpenBox facilitates its flexible deployment in existing systems. Experimental results demonstrate the effectiveness and efficiency of OpenBox over existing systems. The source code of OpenBox is available at https://github.com/PKU-DAIR/open-box.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#20934;&#30830;&#19988;&#31616;&#26126;&#22320;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#30340;&#26041;&#27861;&#12290;&#38024;&#23545;&#27492;&#30446;&#30340;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#35774;&#35745;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13312</link><description>&lt;p&gt;
&#25216;&#26415;&#31508;&#35760;&#65306;&#23450;&#20041;&#21644;&#37327;&#21270;DNN&#30340;AND-OR&#20132;&#20114;&#20197;&#36827;&#34892;&#20934;&#30830;&#21644;&#31616;&#26126;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Technical Note: Defining and Quantifying AND-OR Interactions for Faithful and Concise Explanation of DNNs. (arXiv:2304.13312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#20934;&#30830;&#19988;&#31616;&#26126;&#22320;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#30340;&#26041;&#27861;&#12290;&#38024;&#23545;&#27492;&#30446;&#30340;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#35774;&#35745;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#37325;&#26032;&#24605;&#32771;&#20132;&#20114;&#30340;&#23450;&#20041;&#65292;&#28982;&#21518;&#27491;&#24335;&#23450;&#20041;&#20102;&#22522;&#20110;&#20132;&#20114;&#30340;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#31616;&#27905;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#12290;&#38024;&#23545;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AND&#65288;OR&#65289;&#20132;&#20114;&#22312;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;AND&#65288;OR&#65289;&#20851;&#31995;&#25928;&#24212;&#26041;&#38754;&#30340;&#21807;&#19968;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;AND-OR&#20132;&#20114;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;DNN&#30340;&#25512;&#29702;&#36923;&#36753;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#31526;&#21495;&#27010;&#24565;&#20934;&#30830;&#32780;&#31616;&#26126;&#22320;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical note, we aim to explain a deep neural network (DNN) by quantifying the encoded interactions between input variables, which reflects the DNN's inference logic. Specifically, we first rethink the definition of interactions, and then formally define faithfulness and conciseness for interaction-based explanation. To this end, we propose two kinds of interactions, i.e., the AND interaction and the OR interaction. For faithfulness, we prove the uniqueness of the AND (OR) interaction in quantifying the effect of the AND (OR) relationship between input variables. Besides, based on AND-OR interactions, we design techniques to boost the conciseness of the explanation, while not hurting the faithfulness. In this way, the inference logic of a DNN can be faithfully and concisely explained by a set of symbolic concepts.
&lt;/p&gt;</description></item><item><title>HiQ&#26159;&#19968;&#31181;&#21487;&#36879;&#26126;&#30417;&#25511;Python&#31243;&#24207;&#36816;&#34892;&#26102;&#20449;&#24687;&#30340;&#31995;&#32479;&#65292;&#20855;&#26377;&#38750;&#20405;&#20837;&#24615;&#21644;&#21160;&#24577;&#24615;&#65292;&#21487;&#24212;&#29992;&#20110;&#31163;&#32447;/&#22312;&#32447;&#24212;&#29992;&#31243;&#24207;&#21644;&#20998;&#24067;&#24335;&#31995;&#32479;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#25429;&#25417;&#29942;&#39048;&#65292;&#32780;&#19981;&#24433;&#21709;&#20195;&#30721;&#30340;&#24178;&#20928;&#31243;&#24230;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13302</link><description>&lt;p&gt;
HiQ -- &#19968;&#31181;&#22768;&#26126;&#24615;&#12289;&#38750;&#20405;&#20837;&#24335;&#12289;&#21160;&#24577;&#21644;&#36879;&#26126;&#30340;&#21487;&#35266;&#23519;&#24615;&#21644;&#20248;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
HiQ -- A Declarative, Non-intrusive, Dynamic and Transparent Observability and Optimization System. (arXiv:2304.13302v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13302
&lt;/p&gt;
&lt;p&gt;
HiQ&#26159;&#19968;&#31181;&#21487;&#36879;&#26126;&#30417;&#25511;Python&#31243;&#24207;&#36816;&#34892;&#26102;&#20449;&#24687;&#30340;&#31995;&#32479;&#65292;&#20855;&#26377;&#38750;&#20405;&#20837;&#24615;&#21644;&#21160;&#24577;&#24615;&#65292;&#21487;&#24212;&#29992;&#20110;&#31163;&#32447;/&#22312;&#32447;&#24212;&#29992;&#31243;&#24207;&#21644;&#20998;&#24067;&#24335;&#31995;&#32479;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#25429;&#25417;&#29942;&#39048;&#65292;&#32780;&#19981;&#24433;&#21709;&#20195;&#30721;&#30340;&#24178;&#20928;&#31243;&#24230;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;HiQ&#8221;&#30340;&#38750;&#20405;&#20837;&#24335;&#12289;&#22768;&#26126;&#24615;&#12289;&#21160;&#24577;&#21644;&#36879;&#26126;&#31995;&#32479;&#65292;&#29992;&#20110;&#36319;&#36394;Python&#31243;&#24207;&#30340;&#36816;&#34892;&#26102;&#20449;&#24687;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#36816;&#34892;&#26102;&#31995;&#32479;&#24615;&#33021;&#21644;&#25439;&#22833;&#27934;&#23519;&#21147;&#12290; HiQ&#21487;&#20197;&#29992;&#20110;&#21333;&#29255;&#21644;&#20998;&#24067;&#24335;&#31995;&#32479;&#12289;&#31163;&#32447;&#21644;&#22312;&#32447;&#24212;&#29992;&#31243;&#24207;&#65292;&#25105;&#20204;&#24050;&#32463;&#23558;&#20854;&#29992;&#20110;&#20248;&#21270;&#20351;&#29992;Python&#32534;&#20889;&#30340;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#65292;&#24182;&#21487;&#25512;&#24191;&#21040;&#20219;&#20309;Python&#31243;&#24207;&#25110;&#20998;&#24067;&#24335;&#31995;&#32479;&#65292;&#29978;&#33267;&#26159;Java&#31561;&#20854;&#20182;&#35821;&#35328;&#12290; &#25105;&#20204;&#24050;&#32463;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;HiQ&#65292;&#24182;&#37319;&#29992;&#23427;&#26469;&#25429;&#25417;&#29942;&#39048;&#65292;&#21516;&#26102;&#20445;&#25345;&#25105;&#20204;&#30340;&#29983;&#20135;&#20195;&#30721;&#24178;&#20928;&#19988;&#39640;&#24615;&#33021;&#12290; &#25105;&#20204;&#30340;&#23454;&#29616;&#24050;&#32463;&#22312; [https://github.com/oracle/hiq](https://github.com/oracle/hiq) &#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a non-intrusive, declarative, dynamic and transparent system called `HiQ` to track Python program runtime information without compromising on the run-time system performance and losing insight. HiQ can be used for monolithic and distributed systems, offline and online applications. HiQ is developed when we optimize our large deep neural network (DNN) models which are written in Python, but it can be generalized to any Python program or distributed system, or even other languages like Java. We have implemented the system and adopted it in our deep learning model life cycle management system to catch the bottleneck while keeping our production code clean and highly performant. The implementation is open-sourced at: [https://github.com/oracle/hiq](https://github.com/oracle/hiq).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#20195;&#29702;&#26799;&#24230;&#65288;PSG&#65289;&#26041;&#27861;&#21644;&#28508;&#22312;&#20998;&#24067;&#35843;&#25972;&#65288;PDA&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20013;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13289</link><description>&lt;p&gt;
&#33180;&#30005;&#20301;&#20998;&#24067;&#35843;&#25972;&#21644;&#21442;&#25968;&#21270;&#20195;&#29702;&#26799;&#24230;&#22312;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Membrane Potential Distribution Adjustment and Parametric Surrogate Gradient in Spiking Neural Networks. (arXiv:2304.13289v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#20195;&#29702;&#26799;&#24230;&#65288;PSG&#65289;&#26041;&#27861;&#21644;&#28508;&#22312;&#20998;&#24067;&#35843;&#25972;&#65288;PDA&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20013;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#37325;&#22823;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#39640;&#25928;&#33021;&#30340;&#20108;&#36827;&#21046;&#33033;&#20914;&#20449;&#21495;&#24182;&#19981;&#36866;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#20195;&#29702;&#26799;&#24230;&#65288;SG&#65289;&#31574;&#30053;&#34987;&#30740;&#31350;&#21644;&#24212;&#29992;&#20110;&#32469;&#36807;&#36825;&#20010;&#38382;&#39064;&#24182;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;SNN&#12290;&#30001;&#20110;&#32570;&#20047;&#20844;&#35748;&#30340;SG&#36873;&#25321;&#35268;&#21017;&#65292;&#22823;&#22810;&#25968;SG&#34987;&#30452;&#35266;&#22320;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#21270;&#20195;&#29702;&#26799;&#24230;&#65288;PSG&#65289;&#26041;&#27861;&#26469;&#36845;&#20195;&#22320;&#26356;&#26032;SG&#65292;&#24182;&#26368;&#32456;&#30830;&#23450;&#26368;&#20339;&#20195;&#29702;&#26799;&#24230;&#21442;&#25968;&#65292;&#35813;&#21442;&#25968;&#26657;&#20934;&#20102;&#20505;&#36873;SG&#30340;&#24418;&#29366;&#12290;&#22312;SNN&#20013;&#65292;&#30001;&#20110;&#37327;&#21270;&#35823;&#24046;&#65292;&#31070;&#32463;&#30005;&#20301;&#20998;&#24067;&#24448;&#24448;&#20250;&#20986;&#29616;&#19981;&#21487;&#39044;&#27979;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#31181;&#28508;&#22312;&#30340;&#20559;&#31227;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#20998;&#24067;&#35843;&#25972;&#65288;PDA&#65289;&#30340;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#19981;&#24076;&#26395;&#30340;&#39044;&#28608;&#27963;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#19982;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#65288;BPTT&#65289;&#31639;&#27861;&#38598;&#25104;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an emerging network model, spiking neural networks (SNNs) have aroused significant research attentions in recent years. However, the energy-efficient binary spikes do not augur well with gradient descent-based training approaches. Surrogate gradient (SG) strategy is investigated and applied to circumvent this issue and train SNNs from scratch. Due to the lack of well-recognized SG selection rule, most SGs are chosen intuitively. We propose the parametric surrogate gradient (PSG) method to iteratively update SG and eventually determine an optimal surrogate gradient parameter, which calibrates the shape of candidate SGs. In SNNs, neural potential distribution tends to deviate unpredictably due to quantization error. We evaluate such potential shift and propose methodology for potential distribution adjustment (PDA) to minimize the loss of undesired pre-activations. Experimental results demonstrate that the proposed methods can be readily integrated with backpropagation through time (B
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#38190;&#32452;&#20214;&#27880;&#24847;&#26426;&#21046;&#30340; softmax &#21333;&#20803;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#25506;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;softmax&#21333;&#20803;&#30340;&#26435;&#37325;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2304.13276</link><description>&lt;p&gt;
&#22522;&#20110;Softmax&#22238;&#24402;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26435;&#37325;&#35843;&#25972;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
The Closeness of In-Context Learning and Weight Shifting for Softmax Regression. (arXiv:2304.13276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#38190;&#32452;&#20214;&#27880;&#24847;&#26426;&#21046;&#30340; softmax &#21333;&#20803;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#25506;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;softmax&#21333;&#20803;&#30340;&#26435;&#37325;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#38395;&#21517;&#65292;&#36866;&#29992;&#20110;&#35768;&#22810;&#19982;&#20154;&#31867;&#29983;&#27963;&#25110;&#24037;&#20316;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;Transformer&#26550;&#26500;&#20013;&#30340;&#27880;&#24847;&#26426;&#21046;&#26159;LLMs&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#27169;&#22411;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#29305;&#23450;&#30340;&#36755;&#20837;&#37096;&#20998;&#12290;softmax&#21333;&#20803;&#20316;&#20026;&#27880;&#24847;&#26426;&#21046;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#35268;&#33539;&#21270;&#20102;&#27880;&#24847;&#24471;&#20998;&#12290;&#22240;&#27492;&#65292;LLMs&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;softmax&#21333;&#20803;&#19982;&#27880;&#24847;&#26426;&#21046;&#21457;&#25381;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#24037;&#20316;[RTH+22&#65292;ASA+22&#65292;GTLV22&#65292;ONR+22]&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are known for their exceptional performance in natural language processing, making them highly effective in many human life-related or even job-related tasks. The attention mechanism in the Transformer architecture is a critical component of LLMs, as it allows the model to selectively focus on specific input parts. The softmax unit, which is a key part of the attention mechanism, normalizes the attention scores. Hence, the performance of LLMs in various NLP tasks depends significantly on the crucial role played by the attention mechanism with the softmax unit.  In-context learning, as one of the celebrated abilities of recent LLMs, is an important concept in querying LLMs such as ChatGPT. Without further parameter updates, Transformers can learn to predict based on few in-context examples. However, the reason why Transformers becomes in-context learners is not well understood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the in-context learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#23458;&#25143;&#32858;&#31867;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36710;&#38431;&#25925;&#38556;&#35786;&#26029;&#65292;&#36890;&#36807;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#30340;&#26550;&#26500;&#21644;&#23458;&#25143;&#32858;&#31867;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13275</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#23458;&#25143;&#32858;&#31867;&#30340;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#36710;&#38431;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Uncertainty-Based Client Clustering for Fleet-Wide Fault Diagnosis. (arXiv:2304.13275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#23458;&#25143;&#32858;&#31867;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36710;&#38431;&#25925;&#38556;&#35786;&#26029;&#65292;&#36890;&#36807;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#30340;&#26550;&#26500;&#21644;&#23458;&#25143;&#32858;&#31867;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#34892;&#21508;&#19994;&#30340;&#25805;&#20316;&#21592;&#19968;&#30452;&#22312;&#25512;&#24191;&#26080;&#32447;&#20256;&#24863;&#22120;&#33410;&#28857;&#29992;&#20110;&#24037;&#19994;&#30417;&#25511;&#65292;&#36825;&#20123;&#21162;&#21147;&#20135;&#29983;&#20102;&#21487;&#29992;&#20110;&#26500;&#24314;&#35786;&#26029;&#31639;&#27861;&#30340;&#22823;&#35268;&#27169;&#26465;&#20214;&#30417;&#27979;&#25968;&#25454;&#38598;&#65292;&#20197;&#35686;&#21578;&#32500;&#25252;&#24037;&#31243;&#24072;&#21363;&#23558;&#21457;&#29983;&#30340;&#25925;&#38556;&#25110;&#35782;&#21035;&#24403;&#21069;&#31995;&#32479;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#21333;&#20010;&#25805;&#20316;&#21592;&#21487;&#33021;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#31995;&#32479;&#25110;&#37096;&#20214;&#25968;&#37327;&#26469;&#25910;&#38598;&#36275;&#22815;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#30340;&#31639;&#27861;&#12290;&#25910;&#38598;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#30340;&#28385;&#24847;&#25968;&#37327;&#30340;&#25925;&#38556;&#27169;&#24335;&#29305;&#21035;&#22256;&#38590;&#65292;&#22240;&#20026;&#25925;&#38556;&#30340;&#31232;&#26377;&#24615;&#12290;&#32852;&#37030;&#23398;&#20064; (FL) &#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22810;&#20010;&#25805;&#20316;&#21592;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21435;&#20013;&#24515;&#21270;&#36164;&#20135;&#25925;&#38556;&#35786;&#26029;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#30340;&#26426;&#23494;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#32852;&#37030;&#31574;&#30053;&#26102;&#20173;&#26377;&#30456;&#24403;&#22823;&#30340;&#38556;&#30861;&#38656;&#35201;&#20811;&#26381;&#65292;&#20197;&#36991;&#20813;&#27844;&#28431;&#25935;&#24863;&#25968;&#25454;&#24182;&#35299;&#20915;&#30001;&#25968;&#25454;&#24322;&#36136;&#24615;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Operators from various industries have been pushing the adoption of wireless sensing nodes for industrial monitoring, and such efforts have produced sizeable condition monitoring datasets that can be used to build diagnosis algorithms capable of warning maintenance engineers of impending failure or identifying current system health conditions. However, single operators may not have sufficiently large fleets of systems or component units to collect sufficient data to develop data-driven algorithms. Collecting a satisfactory quantity of fault patterns for safety-critical systems is particularly difficult due to the rarity of faults. Federated learning (FL) has emerged as a promising solution to leverage datasets from multiple operators to train a decentralized asset fault diagnosis model while maintaining data confidentiality. However, there are still considerable obstacles to overcome when it comes to optimizing the federation strategy without leaking sensitive data and addressing the i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#26356;&#27973;&#65292;&#20855;&#20307;&#22320;&#65292;&#21033;&#29992;&#21367;&#31215;&#22359;&#30340;ReLU&#28789;&#25935;&#24230;&#26469;&#21512;&#24182;&#21367;&#31215;&#23618;&#21644;&#21435;&#38500;ReLU&#23618;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;ReLU&#21644;&#32447;&#24615;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#20943;&#23567;&#27169;&#22411;&#27169;&#22411;&#24310;&#36831;&#21644;&#22823;&#23567;&#65292;&#20934;&#30830;&#29575;&#19981;&#26174;&#33879;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2304.13274</link><description>&lt;p&gt;
&#35753;&#27169;&#22411;&#21464;&#24471;&#26356;&#27973;&#65306;&#32852;&#21512;&#23398;&#20064;&#38477;&#20302;&#28145;&#24230;&#21644;&#38750;&#32447;&#24615;&#20197;&#23454;&#29616;&#24310;&#36831;&#39640;&#25928;&#30340;&#31169;&#20154;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Making Models Shallow Again: Jointly Learning to Reduce Non-Linearity and Depth for Latency-Efficient Private Inference. (arXiv:2304.13274v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#26356;&#27973;&#65292;&#20855;&#20307;&#22320;&#65292;&#21033;&#29992;&#21367;&#31215;&#22359;&#30340;ReLU&#28789;&#25935;&#24230;&#26469;&#21512;&#24182;&#21367;&#31215;&#23618;&#21644;&#21435;&#38500;ReLU&#23618;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;ReLU&#21644;&#32447;&#24615;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#20943;&#23567;&#27169;&#22411;&#27169;&#22411;&#24310;&#36831;&#21644;&#22823;&#23567;&#65292;&#20934;&#30830;&#29575;&#19981;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#37327;ReLU&#21644;MAC&#25805;&#20316;&#20351;&#23427;&#20204;&#19981;&#36866;&#21512;&#36827;&#34892;&#24310;&#36831;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#31169;&#20154;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#20801;&#35768;&#27169;&#22411;&#23398;&#20064;&#21464;&#24471;&#27973;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#21367;&#31215;&#22359;&#30340;ReLU&#28789;&#25935;&#24230;&#26469;&#31227;&#38500;ReLU&#23618;&#65292;&#24182;&#23558;&#20854;&#21069;&#21518;&#30340;&#21367;&#31215;&#23618;&#21512;&#24182;&#20026;&#19968;&#20010;&#27973;&#23618;&#12290;&#19982;&#29616;&#26377;&#30340;ReLU&#38477;&#20302;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32852;&#21512;&#38477;&#20302;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;ReLU&#21644;&#32447;&#24615;&#25805;&#20316;&#38477;&#20302;&#65292;&#20998;&#21035;&#25552;&#39640;&#20102;1.73&#20493;&#21644;1.47&#20493;&#65292;&#20351;&#29992;CIFAR-100&#19978;&#30340;ResNet18&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#20960;&#20046;&#27809;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large number of ReLU and MAC operations of Deep neural networks make them ill-suited for latency and compute-efficient private inference. In this paper, we present a model optimization method that allows a model to learn to be shallow. In particular, we leverage the ReLU sensitivity of a convolutional block to remove a ReLU layer and merge its succeeding and preceding convolution layers to a shallow block. Unlike existing ReLU reduction methods, our joint reduction method can yield models with improved reduction of both ReLUs and linear operations by up to 1.73x and 1.47x, respectively, evaluated with ResNet18 on CIFAR-100 without any significant accuracy-drop.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#65306;&#36890;&#36807;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#26377;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.13273</link><description>&lt;p&gt;
&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#65306;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#30340;&#32431;&#25991;&#26412;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping. (arXiv:2304.13273v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#65306;&#36890;&#36807;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#26377;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20197;CLIP&#21644;ALIGN&#20026;&#20195;&#34920;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;CLIP&#30340;&#38646;-shot&#33021;&#21147;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#31561;&#22522;&#20110;&#20851;&#32852;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#20294;&#26159;&#65292;CLIP&#38590;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#30001;&#20110;&#32570;&#20047;&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;K&#26368;&#36817;&#37051;&#36328;&#27169;&#24577;&#26144;&#23556;&#65288;Knight&#65289;&#65292;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#12290;&#36890;&#36807;&#31364;&#23383;&#24149;&#20219;&#21153;&#30340;&#32431;&#25991;&#26412;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Knight&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#19988;&#23558;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of Vision-Language Pre-training Models (VLPMs) represented by CLIP and ALIGN, significant breakthroughs have been achieved for association-based visual tasks such as image classification and image-text retrieval by the zero-shot capability of CLIP without fine-tuning. However, CLIP is hard to apply to generation-based tasks. This is due to the lack of decoder architecture and pre-training tasks for generation. Although previous works have created generation capacity for CLIP through additional language models, a modality gap between the CLIP representations of different modalities and the inability of CLIP to model the offset of this gap, which fails the concept to transfer across modalities. To solve the problem, we try to map images/videos to the language modality and generate captions from the language modality. In this paper, we propose the K-nearest-neighbor Cross-modality Mapping (Knight), a zero-shot method from association to generation. With text-only unsu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#20998;&#31867;&#65292;&#21253;&#25324;&#23458;&#25143;&#31471;&#12289;&#26381;&#21153;&#22120;&#31471;&#21644;&#22522;&#20110;FL&#30340;BFL&#26041;&#27861;&#12290;BFL&#26159;&#35299;&#20915;&#29616;&#26377;FL&#26041;&#27861;&#20013;&#21463;&#38480;&#21644;&#21160;&#24577;&#30340;&#25968;&#25454;&#21644;&#26465;&#20214;&#12289;&#24322;&#36136;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#20998;&#26512;&#35299;&#37322;&#33021;&#21147;&#25361;&#25112;&#30340;&#26377;&#21069;&#36884;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13267</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Bayesian Federated Learning: A Survey. (arXiv:2304.13267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#20998;&#31867;&#65292;&#21253;&#25324;&#23458;&#25143;&#31471;&#12289;&#26381;&#21153;&#22120;&#31471;&#21644;&#22522;&#20110;FL&#30340;BFL&#26041;&#27861;&#12290;BFL&#26159;&#35299;&#20915;&#29616;&#26377;FL&#26041;&#27861;&#20013;&#21463;&#38480;&#21644;&#21160;&#24577;&#30340;&#25968;&#25454;&#21644;&#26465;&#20214;&#12289;&#24322;&#36136;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#20998;&#26512;&#35299;&#37322;&#33021;&#21147;&#25361;&#25112;&#30340;&#26377;&#21069;&#36884;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20197;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#24335;&#25972;&#21512;&#20102;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#12289;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;FL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#33021;&#21147;&#21463;&#21040;&#26377;&#38480;&#21644;&#21160;&#24577;&#30340;&#25968;&#25454;&#21644;&#26465;&#20214;&#65292;&#21253;&#25324;&#24322;&#36136;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;&#20998;&#26512;&#35299;&#37322;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26412;&#32508;&#36848;&#23545;BFL&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#22522;&#26412;&#27010;&#24565;&#65292;&#20854;&#22312;FL&#19978;&#19979;&#25991;&#20013;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20174;&#36125;&#21494;&#26031;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;BFL&#36827;&#34892;&#20998;&#31867;&#21644;&#35752;&#35770;&#12290;&#25105;&#20204;&#23558;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#31471;&#30340;BFL&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;FL&#30340;BFL&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#35752;&#35770;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;BFL&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;BFL&#30740;&#31350;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#20197;&#36827;&#19968;&#27493;&#28385;&#36275;&#29616;&#23454;&#29983;&#27963;&#20013;FL&#24212;&#29992;&#30340;&#22797;&#26434;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) demonstrates its advantages in integrating distributed infrastructure, communication, computing and learning in a privacy-preserving manner. However, the robustness and capabilities of existing FL methods are challenged by limited and dynamic data and conditions, complexities including heterogeneities and uncertainties, and analytical explainability. Bayesian federated learning (BFL) has emerged as a promising approach to address these issues. This survey presents a critical overview of BFL, including its basic concepts, its relations to Bayesian learning in the context of FL, and a taxonomy of BFL from both Bayesian and federated perspectives. We categorize and discuss client- and server-side and FL-based BFL methods and their pros and cons. The limitations of the existing BFL methods and the future directions of BFL research further address the intricate requirements of real-life FL applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102; SHIELD &#26469;&#26816;&#26597;&#20195;&#30721;&#20316;&#32773;&#24402;&#23646;&#26041;&#27861;&#23545;&#25239;&#24615;&#20195;&#30721;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#20316;&#32773;&#24402;&#23646;&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#25200;&#21160;&#20855;&#26377;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13255</link><description>&lt;p&gt;
SHIELD&#65306;&#38450;&#24481;&#20195;&#30721;&#20316;&#32773;&#24402;&#23646;&#12290;
&lt;/p&gt;
&lt;p&gt;
SHIELD: Thwarting Code Authorship Attribution. (arXiv:2304.13255v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102; SHIELD &#26469;&#26816;&#26597;&#20195;&#30721;&#20316;&#32773;&#24402;&#23646;&#26041;&#27861;&#23545;&#25239;&#24615;&#20195;&#30721;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#20316;&#32773;&#24402;&#23646;&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#25200;&#21160;&#20855;&#26377;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#30340;&#20934;&#30830;&#24615;&#19981;&#26029;&#25552;&#39640;&#65292;&#36825;&#23545;&#20110;&#24076;&#26395;&#20445;&#25345;&#21311;&#21517;&#30340;&#31243;&#24207;&#21592;&#26500;&#25104;&#20102;&#20005;&#37325;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; SHIELD&#65292;&#29992;&#20110;&#26816;&#26597;&#19981;&#21516;&#30340;&#20195;&#30721;&#20316;&#32773;&#24402;&#23646;&#26041;&#27861;&#23545;&#25239;&#24615;&#20195;&#30721;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22235;&#31181;&#25915;&#20987;&#24402;&#23646;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26377;&#38024;&#23545;&#24615;&#21644;&#38750;&#26377;&#38024;&#23545;&#24615;&#30340;&#25915;&#20987;&#65292;&#24182;&#20351;&#29992;&#23545;&#25239;&#24615;&#20195;&#30721;&#25200;&#21160;&#26469;&#23454;&#29616;&#23427;&#20204;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258; Google Code Jam &#31454;&#36187;&#30340; 200 &#20010;&#31243;&#24207;&#21592;&#30340;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#37319;&#29992;&#21508;&#31181;&#25216;&#26415;&#20174;&#28304;&#20195;&#30721;&#20013;&#25552;&#21462;&#20316;&#32773;&#23646;&#24615;&#30340;&#20845;&#31181;&#26368;&#26032;&#20316;&#32773;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#65292;&#21253;&#25324; RNN&#12289;CNN &#21644;&#20195;&#30721;&#26679;&#24335;&#23398;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#24403;&#21069;&#20316;&#32773;&#24402;&#23646;&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#28431;&#27934;&#12290;&#23545;&#20110;&#38750;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#24403;&#21069;&#20316;&#32773;&#24402;&#23646;&#26041;&#27861;&#22312;&#25915;&#20987;&#19979;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Authorship attribution has become increasingly accurate, posing a serious privacy risk for programmers who wish to remain anonymous. In this paper, we introduce SHIELD to examine the robustness of different code authorship attribution approaches against adversarial code examples. We define four attacks on attribution techniques, which include targeted and non-targeted attacks, and realize them using adversarial code perturbation. We experiment with a dataset of 200 programmers from the Google Code Jam competition to validate our methods targeting six state-of-the-art authorship attribution methods that adopt a variety of techniques for extracting authorship traits from source-code, including RNN, CNN, and code stylometry. Our experiments demonstrate the vulnerability of current authorship attribution methods against adversarial attacks. For the non-targeted attack, our experiments demonstrate the vulnerability of current authorship attribution methods against the attack with an attack 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27983;&#35272;&#22120;&#20013;&#21152;&#23494;&#25366;&#30719;&#30340;&#38745;&#24577;&#12289;&#21160;&#24577;&#21644;&#32463;&#27982;&#26041;&#38754;&#65292;&#23545;&#21152;&#23494;&#25366;&#30719;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21306;&#20998;&#24182;&#20998;&#26512;&#20102;&#20854;&#23545;&#20110;&#35745;&#31639;&#26426;&#31995;&#32479;&#36164;&#28304;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.13253</link><description>&lt;p&gt;
&#20998;&#26512;&#27983;&#35272;&#22120;&#20013;&#30340;&#21152;&#23494;&#25366;&#30719;
&lt;/p&gt;
&lt;p&gt;
Analyzing In-browser Cryptojacking. (arXiv:2304.13253v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27983;&#35272;&#22120;&#20013;&#21152;&#23494;&#25366;&#30719;&#30340;&#38745;&#24577;&#12289;&#21160;&#24577;&#21644;&#32463;&#27982;&#26041;&#38754;&#65292;&#23545;&#21152;&#23494;&#25366;&#30719;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21306;&#20998;&#24182;&#20998;&#26512;&#20102;&#20854;&#23545;&#20110;&#35745;&#31639;&#26426;&#31995;&#32479;&#36164;&#28304;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#23494;&#25366;&#30719;&#26159;&#26410;&#32463;&#35768;&#21487;&#20351;&#29992;&#30446;&#26631;&#35774;&#22791;&#36827;&#34892;&#38544;&#31192;&#25366;&#25496;&#21152;&#23494;&#36135;&#24065;&#30340;&#34892;&#20026;&#12290;&#25915;&#20987;&#32773;&#20351;&#29992;&#24694;&#24847;JavaScript&#20195;&#30721;&#24378;&#21046;&#27983;&#35272;&#22120;&#35299;&#20915;&#24037;&#20316;&#37327;&#35777;&#26126;&#38382;&#39064;&#65292;&#20174;&#32780;&#21033;&#29992;&#32593;&#31449;&#35775;&#23458;&#30340;&#36164;&#28304;&#36186;&#38065;&#12290;&#20026;&#20102;&#29702;&#35299;&#21644;&#23545;&#25239;&#36825;&#26679;&#30340;&#25915;&#20987;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#27983;&#35272;&#22120;&#20013;&#21152;&#23494;&#25366;&#25496;&#30340;&#38745;&#24577;&#12289;&#21160;&#24577;&#21644;&#32463;&#27982;&#26041;&#38754;&#12290;&#23545;&#20110;&#38745;&#24577;&#20998;&#26512;&#65292;&#25105;&#20204;&#23545;&#21152;&#23494;&#25366;&#25496;&#26679;&#26412;&#36827;&#34892;&#20869;&#23481;&#12289;&#36135;&#24065;&#21644;&#22522;&#20110;&#20195;&#30721;&#30340;&#20998;&#31867;&#65292;&#20197;1&#65289;&#27979;&#37327;&#23427;&#20204;&#22312;&#32593;&#31449;&#19978;&#30340;&#20998;&#24067;&#65292;2&#65289;&#31361;&#20986;&#23427;&#20204;&#30340;&#24179;&#21488;&#20146;&#21644;&#24615;&#65292;&#20197;&#21450;3&#65289;&#30740;&#31350;&#23427;&#20204;&#30340;&#20195;&#30721;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23558;&#21152;&#23494;&#25366;&#25496;&#33050;&#26412;&#19982;&#33391;&#24615;&#21644;&#24694;&#24847;JavaScript&#26679;&#26412;&#21306;&#20998;&#24320;&#26469;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;100&#65285;&#12290;&#23545;&#20110;&#21160;&#24577;&#20998;&#26512;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21152;&#23494;&#25366;&#25496;&#23545;&#20851;&#38190;&#31995;&#32479;&#36164;&#28304;&#65288;&#22914;CPU&#21644;&#30005;&#27744;&#20351;&#29992;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25191;&#34892;Web&#27983;&#35272;&#22120;&#25351;&#32441;&#35782;&#21035;&#20197;&#20998;&#26512;&#20449;&#24687;&#20132;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cryptojacking is the permissionless use of a target device to covertly mine cryptocurrencies. With cryptojacking, attackers use malicious JavaScript codes to force web browsers into solving proof-of-work puzzles, thus making money by exploiting the resources of the website visitors. To understand and counter such attacks, we systematically analyze the static, dynamic, and economic aspects of in-browser cryptojacking. For static analysis, we perform content, currency, and code-based categorization of cryptojacking samples to 1) measure their distribution across websites, 2) highlight their platform affinities, and 3) study their code complexities. We apply machine learning techniques to distinguish cryptojacking scripts from benign and malicious JavaScript samples with 100\% accuracy. For dynamic analysis, we analyze the effect of cryptojacking on critical system resources, such as CPU and battery usage. We also perform web browser fingerprinting to analyze the information exchange betw
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20197;&#36739;&#30701;&#26102;&#38388;&#39564;&#35777;&#23494;&#30721;&#21327;&#35758;&#23433;&#20840;&#24615;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.13249</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#23494;&#30721;&#21327;&#35758;&#23433;&#20840;&#39564;&#35777;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Security Verification Framework of Cryptographic Protocols Using Machine Learning. (arXiv:2304.13249v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13249
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20197;&#36739;&#30701;&#26102;&#38388;&#39564;&#35777;&#23494;&#30721;&#21327;&#35758;&#23433;&#20840;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#23494;&#30721;&#21327;&#35758;&#23433;&#20840;&#39564;&#35777;&#30340;&#26694;&#26550;&#12290;&#38543;&#30528;&#23494;&#30721;&#21327;&#35758;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#33258;&#21160;&#39564;&#35777;&#25216;&#26415;&#30340;&#30740;&#31350;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20027;&#35201;&#30340;&#25216;&#26415;&#26159;&#24418;&#24335;&#21270;&#39564;&#35777;&#12290;&#28982;&#32780;&#65292;&#24418;&#24335;&#21270;&#39564;&#35777;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;&#23427;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#21487;&#20915;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20197;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#39564;&#35777;&#23494;&#30721;&#21327;&#35758;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a security verification framework for cryptographic protocols using machine learning. In recent years, as cryptographic protocols have become more complex, research on automatic verification techniques has been focused on. The main technique is formal verification. However, the formal verification has two problems: it requires a large amount of computational time and does not guarantee decidability. We propose a method that allows security verification with computational time on the order of linear with respect to the size of the protocol using machine learning. In training machine learning models for security verification of cryptographic protocols, a sufficient amount of data, i.e., a set of protocol data with security labels, is difficult to collect from academic papers and other sources. To overcome this issue, we propose a way to create arbitrarily large datasets by automatically generating random protocols and assigning security labels to them using formal verification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#36890;&#36807;&#23616;&#37096;&#35266;&#27979;&#23398;&#20064;&#39044;&#27979;&#30495;&#23454;&#29615;&#22659;&#20013;&#23548;&#33322;&#27169;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#32988;&#36807;&#20004;&#20010;&#26377;&#30417;&#30563;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26080;&#20559;&#26412;&#22320;&#26041;&#21521;&#36719;&#36710;&#36947;&#27010;&#29575;&#22330;&#12290;</title><link>http://arxiv.org/abs/2304.13242</link><description>&lt;p&gt;
&#20174;&#23616;&#37096;&#35266;&#27979;&#23398;&#20064;&#39044;&#27979;&#23548;&#33322;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learning to Predict Navigational Patterns from Partial Observations. (arXiv:2304.13242v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#36890;&#36807;&#23616;&#37096;&#35266;&#27979;&#23398;&#20064;&#39044;&#27979;&#30495;&#23454;&#29615;&#22659;&#20013;&#23548;&#33322;&#27169;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#32988;&#36807;&#20004;&#20010;&#26377;&#30417;&#30563;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26080;&#20559;&#26412;&#22320;&#26041;&#21521;&#36719;&#36710;&#36947;&#27010;&#29575;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#36981;&#23432;&#30456;&#20114;&#30693;&#26195;&#30340;&#23548;&#33322;&#27169;&#24335;&#22312;&#36981;&#24490;&#35268;&#21017;&#30340;&#29615;&#22659;&#19979;&#36827;&#34892;&#21512;&#20316;&#23548;&#33322;&#65292;&#36825;&#20123;&#27169;&#24335;&#21487;&#20197;&#34920;&#31034;&#20026;&#26041;&#21521;&#36335;&#24452;&#25110;&#36947;&#36335;&#36710;&#36947;&#12290;&#20174;&#19981;&#23436;&#20840;&#35266;&#27979;&#21040;&#30340;&#29615;&#22659;&#20013;&#25512;&#26029;&#20986;&#36825;&#20123;&#23548;&#33322;&#27169;&#24335;&#26159;&#26234;&#33021;&#31227;&#21160;&#26426;&#22120;&#20154;&#22312;&#26410;&#26144;&#23556;&#20301;&#32622;&#25805;&#20316;&#25152;&#24517;&#38656;&#30340;&#12290;&#28982;&#32780;&#65292;&#31639;&#27861;&#23450;&#20041;&#36825;&#20123;&#23548;&#33322;&#27169;&#24335;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20165;&#20174;&#23616;&#37096;&#35266;&#27979;&#20013;&#23398;&#20064;&#25512;&#26029;&#30495;&#23454;&#29615;&#22659;&#20013;&#23548;&#33322;&#27169;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#20351;&#29992;&#20960;&#20309;&#25968;&#25454;&#22686;&#24378;&#65292;&#39044;&#27979;&#19990;&#30028;&#24314;&#27169;&#21644;&#20449;&#24687;&#35770;&#27491;&#21017;&#21270;&#22120;&#23454;&#29616;&#20102;&#22312;&#26080;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#39044;&#27979;&#26080;&#20559;&#26412;&#22320;&#26041;&#21521;&#36719;&#36710;&#36947;&#27010;&#29575;&#65288;DSLP&#65289;&#22330;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#26368;&#22823;&#20284;&#28982;&#22270;&#25311;&#21512;&#21040;DSLP&#22330;&#20013;&#26469;&#25512;&#26029;&#20840;&#23616;&#23548;&#33322;&#27169;&#24335;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#20174;&#23616;&#37096;&#35266;&#27979;&#20013;&#23398;&#20064;&#39044;&#27979;&#23548;&#33322;&#27169;&#24335;&#30340;&#20219;&#21153;&#20013;&#32988;&#36807;&#20102;&#20004;&#20010;SOTA&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human beings cooperatively navigate rule-constrained environments by adhering to mutually known navigational patterns, which may be represented as directional pathways or road lanes. Inferring these navigational patterns from incompletely observed environments is required for intelligent mobile robots operating in unmapped locations. However, algorithmically defining these navigational patterns is nontrivial. This paper presents the first self-supervised learning (SSL) method for learning to infer navigational patterns in real-world environments from partial observations only. We explain how geometric data augmentation, predictive world modeling, and an information-theoretic regularizer enables our model to predict an unbiased local directional soft lane probability (DSLP) field in the limit of infinite data. We demonstrate how to infer global navigational patterns by fitting a maximum likelihood graph to the DSLP field. Experiments show that our SSL model outperforms two SOTA supervis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20934;&#21017;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;HT&#26816;&#27979;&#24037;&#20855;&#65292;&#21487;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;HT&#26816;&#27979;&#22330;&#26223;&#65292;&#25552;&#39640;&#20102;HT&#26816;&#27979;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.13232</link><description>&lt;p&gt;
&#22810;&#20934;&#21017;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#65306;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-criteria Hardware Trojan Detection: A Reinforcement Learning Approach. (arXiv:2304.13232v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20934;&#21017;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;HT&#26816;&#27979;&#24037;&#20855;&#65292;&#21487;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;HT&#26816;&#27979;&#22330;&#26223;&#65292;&#25552;&#39640;&#20102;HT&#26816;&#27979;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30828;&#20214;&#29305;&#27931;&#20234;&#65288;HT&#65289;&#26159;&#19968;&#31181;&#19981;&#21463;&#27426;&#36814;&#30340;&#35774;&#35745;&#25110;&#21046;&#36896;&#20462;&#25913;&#65292;&#20250;&#20005;&#37325;&#24433;&#21709;&#25968;&#23383;&#38598;&#25104;&#30005;&#36335;&#30340;&#23433;&#20840;&#24615;&#21644;&#21151;&#33021;&#12290;HT&#21487;&#20197;&#26681;&#25454;&#21508;&#31181;&#35774;&#35745;&#20934;&#21017;&#25554;&#20837;&#65292;&#20363;&#22914;&#65292;&#32593;&#32476;&#20999;&#25442;&#27963;&#21160;&#65292;&#21487;&#35266;&#27979;&#24615;&#65292;&#21487;&#25511;&#24615;&#31561;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#22823;&#22810;&#25968;HT&#26816;&#27979;&#26041;&#27861;&#21482;&#22522;&#20110;&#21333;&#19968;&#26631;&#20934;&#65292;&#21363;&#32593;&#32476;&#20999;&#25442;&#27963;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20934;&#21017;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;HT&#26816;&#27979;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#20855;&#26377;&#21487;&#35843;&#33410;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#21487;&#29992;&#20110;&#19981;&#21516;&#30340;HT&#26816;&#27979;&#22330;&#26223;&#12290;&#35813;&#24037;&#20855;&#20801;&#35768;&#25506;&#32034;&#29616;&#26377;&#30340;&#26816;&#27979;&#31574;&#30053;&#65292;&#24182;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#21162;&#21147;&#36866;&#24212;&#26032;&#30340;&#26816;&#27979;&#22330;&#26223;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#27491;&#27604;&#36739;HT&#26816;&#27979;&#26041;&#27861;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;ISCAS-85&#22522;&#20934;&#27979;&#35797;&#20013;84.2&#65285;&#30340;HT&#26816;&#27979;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hardware Trojans (HTs) are undesired design or manufacturing modifications that can severely alter the security and functionality of digital integrated circuits. HTs can be inserted according to various design criteria, e.g., nets switching activity, observability, controllability, etc. However, to our knowledge, most HT detection methods are only based on a single criterion, i.e., nets switching activity. This paper proposes a multi-criteria reinforcement learning (RL) HT detection tool that features a tunable reward function for different HT detection scenarios. The tool allows for exploring existing detection strategies and can adapt new detection scenarios with minimal effort. We also propose a generic methodology for comparing HT detection methods fairly. Our preliminary results show an average of 84.2% successful HT detection in ISCAS-85 benchmark
&lt;/p&gt;</description></item><item><title>UNADON&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#20840;&#22522;&#22240;&#32452;&#30340;&#26579;&#33394;&#20307;&#31354;&#38388;&#20301;&#32622;&#12290;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#29305;&#24449;&#21644;&#34920;&#35266;&#36951;&#20256;&#20449;&#21495;&#65292;UNADON&#22312;&#35757;&#32451;&#21333;&#20010;&#32454;&#32990;&#31995;&#26102;&#33021;&#22815;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#26579;&#33394;&#36136;&#31354;&#38388;&#23450;&#20301;&#21040;&#26680;&#20307;&#65292;&#24182;&#25581;&#31034;&#20102;&#28508;&#22312;&#24433;&#21709;&#26579;&#33394;&#36136;&#21306;&#38548;&#30340;&#24207;&#21015;&#21644;&#34920;&#35266;&#36951;&#20256;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.13230</link><description>&lt;p&gt;
UNADON&#65306;&#29992;&#20110;&#39044;&#27979;&#20840;&#22522;&#22240;&#32452;&#26579;&#33394;&#20307;&#31354;&#38388;&#20301;&#32622;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UNADON: Transformer-based model to predict genome-wide chromosome spatial position. (arXiv:2304.13230v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13230
&lt;/p&gt;
&lt;p&gt;
UNADON&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#20840;&#22522;&#22240;&#32452;&#30340;&#26579;&#33394;&#20307;&#31354;&#38388;&#20301;&#32622;&#12290;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#29305;&#24449;&#21644;&#34920;&#35266;&#36951;&#20256;&#20449;&#21495;&#65292;UNADON&#22312;&#35757;&#32451;&#21333;&#20010;&#32454;&#32990;&#31995;&#26102;&#33021;&#22815;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#26579;&#33394;&#36136;&#31354;&#38388;&#23450;&#20301;&#21040;&#26680;&#20307;&#65292;&#24182;&#25581;&#31034;&#20102;&#28508;&#22312;&#24433;&#21709;&#26579;&#33394;&#36136;&#21306;&#38548;&#30340;&#24207;&#21015;&#21644;&#34920;&#35266;&#36951;&#20256;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26579;&#33394;&#20307;&#30456;&#23545;&#20110;&#21151;&#33021;&#24615;&#26680;&#20307;&#30340;&#31354;&#38388;&#23450;&#20301;&#19982;&#22522;&#22240;&#32452;&#21151;&#33021;&#65288;&#22914;&#36716;&#24405;&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#24433;&#21709;&#20840;&#22522;&#22240;&#32452;&#33539;&#22260;&#20869;&#26579;&#33394;&#36136;&#31354;&#38388;&#23450;&#20301;&#30340;&#24207;&#21015;&#27169;&#24335;&#21644;&#34920;&#35266;&#36951;&#20256;&#29305;&#24449;&#23578;&#26410;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;UNADON&#65292;&#23427;&#20351;&#29992;&#24207;&#21015;&#29305;&#24449;&#21644;&#34920;&#35266;&#36951;&#20256;&#20449;&#21495;&#65292;&#39044;&#27979;&#20102;&#36890;&#36807;TSA-seq&#27979;&#37327;&#30340;&#29305;&#23450;&#31867;&#22411;&#26680;&#20307;&#30340;&#20840;&#22522;&#22240;&#32452;&#32454;&#32990;&#23398;&#36317;&#31163;&#12290;&#22312;&#22235;&#31181;&#32454;&#32990;&#31995;&#65288;K562&#65292;H1&#65292;HFFc6&#65292;HCT116&#65289;&#20013;&#23545;UNADON&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#21333;&#20010;&#32454;&#32990;&#31995;&#35757;&#32451;&#26102;&#65292;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#26579;&#33394;&#36136;&#31354;&#38388;&#23450;&#20301;&#21040;&#26680;&#20307;&#12290;UNADON&#22312;&#26410;&#35265;&#36807;&#30340;&#32454;&#32990;&#31867;&#22411;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24433;&#21709;&#22823;&#23610;&#24230;&#26579;&#33394;&#36136;&#21306;&#38548;&#21040;&#26680;&#20307;&#30340;&#28508;&#22312;&#24207;&#21015;&#21644;&#34920;&#35266;&#36951;&#20256;&#22240;&#32032;&#12290;&#32508;&#19978;&#65292;UNADON&#20026;&#20102;&#35299;&#22522;&#22240;&#32452;&#20013;&#30340;&#24207;&#21015;&#29305;&#24449;&#21644;&#26579;&#33394;&#36136;&#31354;&#38388;&#23450;&#20301;&#30340;&#21407;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#20026;&#30740;&#31350;&#26680;&#30340;&#21151;&#33021;&#32452;&#32455;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spatial positioning of chromosomes relative to functional nuclear bodies is intertwined with genome functions such as transcription. However, the sequence patterns and epigenomic features that collectively influence chromatin spatial positioning in a genome-wide manner are not well understood. Here, we develop a new transformer-based deep learning model called UNADON, which predicts the genome-wide cytological distance to a specific type of nuclear body, as measured by TSA-seq, using both sequence features and epigenomic signals. Evaluations of UNADON in four cell lines (K562, H1, HFFc6, HCT116) show high accuracy in predicting chromatin spatial positioning to nuclear bodies when trained on a single cell line. UNADON also performed well in an unseen cell type. Importantly, we reveal potential sequence and epigenomic factors that affect large-scale chromatin compartmentalization to nuclear bodies. Together, UNADON provides new insights into the principles between sequence features a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;MOO&#8221;&#26041;&#27861;&#26469;&#23454;&#29616;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#20197;&#21516;&#26102;&#23454;&#29616;&#22810;&#20010;&#30446;&#26631;&#65292;&#36991;&#20813;&#20102;&#26420;&#32032;MOO&#26368;&#22823;&#21270;&#25152;&#26377;&#30446;&#26631;&#30340;&#24330;&#31471;&#12290;</title><link>http://arxiv.org/abs/2304.13229</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generating Adversarial Examples with Task Oriented Multi-Objective Optimization. (arXiv:2304.13229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;MOO&#8221;&#26041;&#27861;&#26469;&#23454;&#29616;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#20197;&#21516;&#26102;&#23454;&#29616;&#22810;&#20010;&#30446;&#26631;&#65292;&#36991;&#20813;&#20102;&#26420;&#32032;MOO&#26368;&#22823;&#21270;&#25152;&#26377;&#30446;&#26631;&#30340;&#24330;&#31471;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20063;&#24456;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#23545;&#25239;&#35757;&#32451;&#26159;&#25552;&#39640;&#27169;&#22411;&#31283;&#20581;&#24615;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#23545;&#20110;&#23545;&#25239;&#35757;&#32451;&#30340;&#25104;&#21151;&#26469;&#35828;&#65292;&#20851;&#38190;&#22240;&#32032;&#26159;&#35201;&#33021;&#22815;&#29983;&#25104;&#28385;&#36275;&#26576;&#20123;&#30446;&#26631;/&#30446;&#26631;&#30340;&#21512;&#26684;&#19988;&#26377;&#24046;&#24322;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;&#20363;&#22914;&#65292;&#25214;&#21040;&#26368;&#22823;&#21270;&#27169;&#22411;&#25439;&#22833;&#20197;&#21516;&#26102;&#25915;&#20987;&#22810;&#20010;&#27169;&#22411;&#30340;&#23545;&#25239;&#26679;&#26412;&#65289;&#12290;&#22240;&#27492;&#65292;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26159;&#23454;&#29616;&#23545;&#25239;&#24615;&#26679;&#26412;&#29983;&#25104;&#20197;&#21516;&#26102;&#23454;&#29616;&#22810;&#20010;&#30446;&#26631;/&#30446;&#26631;&#30340;&#33258;&#28982;&#24037;&#20855;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#65292;MOO&#30340;&#26420;&#32032;&#24212;&#29992;&#24448;&#24448;&#20250;&#24179;&#31561;&#22320;&#26368;&#22823;&#21270;&#25152;&#26377;&#30446;&#26631;/&#30446;&#26631;&#65292;&#32780;&#19981;&#20851;&#24515;&#30446;&#26631;/&#30446;&#26631;&#26159;&#21542;&#24050;&#32463;&#23454;&#29616;&#12290;&#36825;&#23548;&#33268;&#20102;&#22312;&#24050;&#23454;&#29616;&#30446;&#26631;/&#30446;&#26631;&#30340;&#20219;&#21153;&#19978;&#20570;&#26080;&#29992;&#30340;&#21162;&#21147;&#65292;&#32780;&#22312;&#26410;&#23454;&#29616;&#30446;&#26631;/&#30446;&#26631;&#30340;&#20219;&#21153;&#19978;&#21017;&#25237;&#20837;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;MOO&#8221;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#22312;&#27492;&#24773;&#20917;&#19979;...
&lt;/p&gt;
&lt;p&gt;
Deep learning models, even the-state-of-the-art ones, are highly vulnerable to adversarial examples. Adversarial training is one of the most efficient methods to improve the model's robustness. The key factor for the success of adversarial training is the capability to generate qualified and divergent adversarial examples which satisfy some objectives/goals (e.g., finding adversarial examples that maximize the model losses for simultaneously attacking multiple models). Therefore, multi-objective optimization (MOO) is a natural tool for adversarial example generation to achieve multiple objectives/goals simultaneously. However, we observe that a naive application of MOO tends to maximize all objectives/goals equally, without caring if an objective/goal has been achieved yet. This leads to useless effort to further improve the goal-achieved tasks, while putting less focus on the goal-unachieved tasks. In this paper, we propose \emph{Task Oriented MOO} to address this issue, in the contex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BSDE&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#29616;&#26377;&#30340;&#20998;&#25968;&#20989;&#25968;&#30830;&#23450;&#21040;&#36798;&#25152;&#38656;&#32456;&#31471;&#20998;&#24067;&#25152;&#38656;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#20026;&#25193;&#25955;&#24314;&#27169;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#25193;&#25955;&#21453;&#28436;&#65292;&#26465;&#20214;&#25193;&#25955;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.13224</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;BSDE&#25193;&#25955;&#27169;&#22411;&#65306;&#21453;&#28436;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based Generative Modeling Through Backward Stochastic Differential Equations: Inversion and Generation. (arXiv:2304.13224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BSDE&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#29616;&#26377;&#30340;&#20998;&#25968;&#20989;&#25968;&#30830;&#23450;&#21040;&#36798;&#25152;&#38656;&#32456;&#31471;&#20998;&#24067;&#25152;&#38656;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#20026;&#25193;&#25955;&#24314;&#27169;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#25193;&#25955;&#21453;&#28436;&#65292;&#26465;&#20214;&#25193;&#25955;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;BSDE&#30340;&#25193;&#25955;&#27169;&#22411;&#20026;&#25193;&#25955;&#24314;&#27169;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#19982;&#20256;&#32479;&#30340;SDE-based&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#29616;&#26377;&#30340;&#20998;&#25968;&#20989;&#25968;&#30830;&#23450;&#21040;&#36798;&#25152;&#38656;&#32456;&#31471;&#20998;&#24067;&#25152;&#38656;&#30340;&#21021;&#22987;&#26465;&#20214;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20351;&#29992;Lipschitz&#32593;&#32476;&#36827;&#34892;&#20998;&#25968;&#21305;&#37197;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#22312;&#25193;&#25955;&#21453;&#28436;&#65292;&#26465;&#20214;&#25193;&#25955;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20026;&#35299;&#20915;&#29616;&#23454;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proposed BSDE-based diffusion model represents a novel approach to diffusion modeling, which extends the application of stochastic differential equations (SDEs) in machine learning. Unlike traditional SDE-based diffusion models, our model can determine the initial conditions necessary to reach a desired terminal distribution by adapting an existing score function. We demonstrate the theoretical guarantees of the model, the benefits of using Lipschitz networks for score matching, and its potential applications in various areas such as diffusion inversion, conditional diffusion, and uncertainty quantification. Our work represents a contribution to the field of score-based generative learning and offers a promising direction for solving real-world problems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29615;&#22659;&#23436;&#20840;&#26080;&#30693;&#21644;&#23436;&#32654;&#30693;&#35782;&#20043;&#38388;&#30340;&#26426;&#36935;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23616;&#37096;&#27169;&#22411;&#21644;&#20445;&#25345;&#25968;&#25454;&#39537;&#21160;&#35843;&#25972;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#22312;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.13223</link><description>&lt;p&gt;
&#20855;&#26377;&#23616;&#37096;&#21442;&#25968;&#27169;&#22411;&#30693;&#35782;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Partial Parametric Model Knowledge. (arXiv:2304.13223v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13223
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29615;&#22659;&#23436;&#20840;&#26080;&#30693;&#21644;&#23436;&#32654;&#30693;&#35782;&#20043;&#38388;&#30340;&#26426;&#36935;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23616;&#37096;&#27169;&#22411;&#21644;&#20445;&#25345;&#25968;&#25454;&#39537;&#21160;&#35843;&#25972;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#22312;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#36830;&#32493;&#25511;&#21046;&#65292;&#20197;&#22635;&#34917;&#22312;&#29615;&#22659;&#23436;&#20840;&#26080;&#30693;&#21644;&#23436;&#32654;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Partial Knowledge Least Squares Policy Iteration(PLSPI)&#65292;&#26082;&#20511;&#37492;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20063;&#20511;&#37492;&#20102;&#27169;&#22411;&#22522;&#30784;&#25511;&#21046;&#12290;&#23427;&#21033;&#29992;&#23616;&#37096;&#27169;&#22411;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#65292;&#24182;&#20445;&#30041;&#24378;&#21270;&#23398;&#20064;&#26397;&#21521;&#26368;&#20248;&#24615;&#33021;&#30340;&#25968;&#25454;&#39537;&#21160;&#35843;&#25972;&#12290;&#25105;&#20204;&#20197;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#20026;&#26696;&#20363;&#30740;&#31350;&#65307;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We adapt reinforcement learning (RL) methods for continuous control to bridge the gap between complete ignorance and perfect knowledge of the environment. Our method, Partial Knowledge Least Squares Policy Iteration (PLSPI), takes inspiration from both model-free RL and model-based control. It uses incomplete information from a partial model and retains RL's data-driven adaption towards optimal performance. The linear quadratic regulator provides a case study; numerical experiments demonstrate the effectiveness and resulting benefits of the proposed method.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102; ZRG &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#26679;&#26412;&#30340;&#39640;&#20998;&#36776;&#29575;&#20303;&#23429;&#23627;&#39030;&#27491;&#20132;&#22270;&#20687;&#25340;&#25509;&#12289;&#23545;&#24212; DSM&#12289;3D &#23627;&#39030;&#26694;&#26550;&#21644;&#22810;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#30340;&#28857;&#20113;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#20303;&#23429;&#23627;&#39030;&#32467;&#26500;&#30340;&#20960;&#20309;&#24314;&#27169;&#21644;&#22330;&#26223;&#29702;&#35299;&#65292;&#22914;&#23627;&#39030;&#36718;&#24275;&#25552;&#21462;&#12289;&#21333;&#30446;&#39640;&#24230;&#20272;&#35745;&#21644;&#24179;&#38754;&#23627;&#39030;&#32467;&#26500;&#25552;&#21462;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.13219</link><description>&lt;p&gt;
ZRG: &#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39640;&#20998;&#36776;&#29575;&#20303;&#23429;&#23627;&#39030;&#19977;&#32500;&#20960;&#20309;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ZRG: A High Resolution 3D Residential Rooftop Geometry Dataset for Machine Learning. (arXiv:2304.13219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102; ZRG &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#26679;&#26412;&#30340;&#39640;&#20998;&#36776;&#29575;&#20303;&#23429;&#23627;&#39030;&#27491;&#20132;&#22270;&#20687;&#25340;&#25509;&#12289;&#23545;&#24212; DSM&#12289;3D &#23627;&#39030;&#26694;&#26550;&#21644;&#22810;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#30340;&#28857;&#20113;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#20303;&#23429;&#23627;&#39030;&#32467;&#26500;&#30340;&#20960;&#20309;&#24314;&#27169;&#21644;&#22330;&#26223;&#29702;&#35299;&#65292;&#22914;&#23627;&#39030;&#36718;&#24275;&#25552;&#21462;&#12289;&#21333;&#30446;&#39640;&#24230;&#20272;&#35745;&#21644;&#24179;&#38754;&#23627;&#39030;&#32467;&#26500;&#25552;&#21462;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; Zeitview Rooftop Geometry (ZRG) &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#39640;&#20998;&#36776;&#29575;&#20303;&#23429;&#23627;&#39030;&#27491;&#20132;&#22270;&#20687;&#25340;&#25509;&#12289;&#23545;&#24212;&#25968;&#23383;&#34920;&#38754;&#27169;&#22411; (DSM)&#12289;3D &#23627;&#39030;&#26694;&#26550;&#21644;&#22810;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#30340;&#28857;&#20113;&#29992;&#20110;&#20303;&#23429;&#23627;&#39030;&#20960;&#20309;&#21644;&#22330;&#26223;&#29702;&#35299;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#27492;&#25968;&#25454;&#38598;&#35299;&#38145;&#30340;&#20247;&#22810;&#24212;&#29992;&#65292;&#24182;&#20026;&#23627;&#39030;&#36718;&#24275;&#25552;&#21462;&#12289;&#21333;&#30446;&#39640;&#24230;&#20272;&#35745;&#21644;&#24179;&#38754;&#23627;&#39030;&#32467;&#26500;&#25552;&#21462;&#31561;&#20219;&#21153;&#25552;&#20379;&#20102;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present the Zeitview Rooftop Geometry (ZRG) dataset. ZRG contains thousands of samples of high resolution orthomosaics of aerial imagery of residential rooftops with corresponding digital surface models (DSM), 3D rooftop wireframes, and multiview imagery generated point clouds for the purpose of residential rooftop geometry and scene understanding. We perform thorough benchmarks to illustrate the numerous applications unlocked by this dataset and provide baselines for the tasks of roof outline extraction, monocular height estimation, and planar roof structure extraction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#21333;&#35270;&#22270;&#39640;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#20809;&#23398;&#21644;DSM&#22270;&#20687;&#20043;&#38388;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#23454;&#29616;&#29983;&#25104;&#36924;&#30495;&#30340;&#39640;&#20998;&#36776;&#29575;3D&#34920;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.13214</link><description>&lt;p&gt;
&#24102;&#26377;&#26465;&#20214;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#21333;&#35270;&#22270;&#39640;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Single-View Height Estimation with Conditional Diffusion Probabilistic Models. (arXiv:2304.13214v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#21333;&#35270;&#22270;&#39640;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#20809;&#23398;&#21644;DSM&#22270;&#20687;&#20043;&#38388;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#23454;&#29616;&#29983;&#25104;&#36924;&#30495;&#30340;&#39640;&#20998;&#36776;&#29575;3D&#34920;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#22320;&#34920;&#27169;&#22411;&#65288;DSM&#65289;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#39640;&#24230;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#29702;&#35299;&#22320;&#29699;&#34920;&#38754;&#20197;&#21450;&#30417;&#27979;&#33258;&#28982;&#21644;&#20154;&#36896;&#32467;&#26500;&#30340;&#23384;&#22312;&#25110;&#21464;&#21270;&#12290;&#20256;&#32479;&#30340;&#39640;&#24230;&#20272;&#35745;&#38656;&#35201;&#22810;&#35270;&#35282;&#22320;&#29702;&#31354;&#38388;&#24433;&#20687;&#25110;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#65292;&#32780;&#36825;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#12290;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21333;&#35270;&#22270;&#39640;&#24230;&#20272;&#35745;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#21069;&#26223;&#65292;&#20294;&#23427;&#21487;&#33021;&#38590;&#20197;&#37325;&#24314;&#39640;&#20998;&#36776;&#29575;&#29305;&#24449;&#12290;&#23578;&#26410;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#21644;&#32534;&#36753;&#30340;&#26368;&#26032;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#26469;&#36827;&#34892;&#36965;&#24863;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#39640;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35757;&#32451;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#20809;&#23398;&#21644;DSM&#22270;&#20687;&#22312;&#20004;&#20010;&#22495;&#20013;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#38142;&#12290;&#36825;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#21435;&#22122;&#35780;&#20998;&#21305;&#37197;&#30446;&#26631;&#26469;&#23454;&#29616;&#30340;&#65292;&#21516;&#26102;&#20197;&#28304;&#22270;&#20687;&#20026;&#26465;&#20214;&#29983;&#25104;&#36924;&#30495;&#30340;&#39640;&#20998;&#36776;&#29575;3D&#34920;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital Surface Models (DSM) offer a wealth of height information for understanding the Earth's surface as well as monitoring the existence or change in natural and man-made structures. Classical height estimation requires multi-view geospatial imagery or LiDAR point clouds which can be expensive to acquire. Single-view height estimation using neural network based models shows promise however it can struggle with reconstructing high resolution features. The latest advancements in diffusion models for high resolution image synthesis and editing have yet to be utilized for remote sensing imagery, particularly height estimation. Our approach involves training a generative diffusion model to learn the joint distribution of optical and DSM images across both domains as a Markov chain. This is accomplished by minimizing a denoising score matching objective while being conditioned on the source image to generate realistic high resolution 3D surfaces. In this paper we experiment with condition
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27491;&#21521;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#20998;&#35010;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#27714;&#35299;&#25972;&#25968;&#21644;&#20998;&#25968;&#38454;&#31070;&#32463;&#20803;&#27169;&#22411;&#24182;&#23637;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.13205</link><description>&lt;p&gt;
&#23558;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#25972;&#25968;&#21644;&#20998;&#25968;&#38454;&#31070;&#32463;&#20803;&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#25512;&#26029;&#30340;&#20998;&#35010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Splitting physics-informed neural networks for inferring the dynamics of integer- and fractional-order neuron models. (arXiv:2304.13205v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27491;&#21521;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#20998;&#35010;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#27714;&#35299;&#25972;&#25968;&#21644;&#20998;&#25968;&#38454;&#31070;&#32463;&#20803;&#27169;&#22411;&#24182;&#23637;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#25286;&#20998;&#26041;&#27861;&#21644;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#30340;&#27491;&#21521;&#31995;&#32479;&#12290;&#25152;&#25552;&#20986;&#30340;&#20998;&#35010;PINN&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22312;&#27491;&#21521;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#24212;&#29992;PINN&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#20854;&#22312;&#31070;&#32463;&#20803;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24212;&#29992;&#31639;&#23376;&#25286;&#20998;&#23558;&#21407;&#22987;&#31070;&#32463;&#20803;&#27169;&#22411;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#28982;&#21518;&#20351;&#29992;PINN&#26469;&#35299;&#20915;&#36825;&#20123;&#23376;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;$L^1$&#26041;&#26696;&#65292;&#29992;&#20110;&#31163;&#25955;&#20998;&#25968;&#23548;&#25968;&#22312;&#20998;&#25968;&#31070;&#32463;&#20803;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#20998;&#35010;PINN&#22312;&#35299;&#20915;&#25972;&#25968;&#21644;&#20998;&#25968;&#38454;&#31070;&#32463;&#20803;&#27169;&#22411;&#20197;&#21450;&#20854;&#20182;&#31867;&#20284;&#30340;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new approach for solving forward systems of differential equations using a combination of splitting methods and physics-informed neural networks (PINNs). The proposed method, splitting PINN, effectively addresses the challenge of applying PINNs to forward dynamical systems and demonstrates improved accuracy through its application to neuron models. Specifically, we apply operator splitting to decompose the original neuron model into sub-problems that are then solved using PINNs. Moreover, we develop an $L^1$ scheme for discretizing fractional derivatives in fractional neuron models, leading to improved accuracy and efficiency. The results of this study highlight the potential of splitting PINNs in solving both integer- and fractional-order neuron models, as well as other similar systems in computational science and engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26680;&#26041;&#27861;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#23545;&#22810;&#32452;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#21518;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#37117;&#26159;&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13202</link><description>&lt;p&gt;
&#26680;&#26041;&#27861;&#22312;&#31639;&#23376;&#23398;&#20064;&#20013;&#34920;&#29616;&#31454;&#20105;&#21147;
&lt;/p&gt;
&lt;p&gt;
Kernel Methods are Competitive for Operator Learning. (arXiv:2304.13202v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26680;&#26041;&#27861;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#23545;&#22810;&#32452;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#21518;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#37117;&#26159;&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26680;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#20808;&#39564;&#35823;&#24046;&#20998;&#26512;&#21644;&#19982;&#27969;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;&#22914;Deep Operator Net&#65288;DeepONet&#65289;[Lu et al.]&#21644;Fourier&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;[Li et al.]&#65289;&#30340;&#20840;&#38754;&#25968;&#23383;&#27604;&#36739;&#12290;&#25105;&#20204;&#32771;&#34385;&#30446;&#26631;&#31639;&#23376;$\mathcal{G}^\dagger:\mathcal{U}\to\mathcal{V}$&#30340;&#36755;&#20837;/&#36755;&#20986;&#31354;&#38388;&#26159;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#30340;&#24773;&#20917;&#65292;&#25968;&#25454;&#20197;&#36755;&#20837;/&#36755;&#20986;&#20989;&#25968;&#30340;&#37096;&#20998;&#35266;&#27979;$\varphi(v_i),\phi(u_i)$&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#20854;&#20013;$v_i=\mathcal{G}^\dagger(u_i)$&#65288;$i=1,\ldots,N$&#65289;&#65292;&#27979;&#37327;&#31639;&#23376;$\varphi:\mathcal{V}\to\mathbb{R}^m$&#21644;$\phi:\mathcal{U}\to\mathbb{R}^n$&#26159;&#32447;&#24615;&#30340;&#12290;&#22312;&#20889;$\psi:\mathbb{R}^n\to\mathcal{U}$&#21644;$\chi:\mathbb{R}^m\to\mathcal{V}$&#20316;&#20026;&#19982;$\phi$&#21644;$\varphi$&#30456;&#20851;&#30340;&#26368;&#20339;&#24674;&#22797;&#26144;&#23556;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;$\bar{f}$ &#26680;&#26144;&#23556; $L^2(\mathcal{U},\mathbb{R}^n)$ &#23450;&#20041;&#19968;&#20010;$k$ &#31867;&#22411;&#30340;&#26368;&#23567;&#20108;&#20056;&#27169;&#22411;&#65292; &#28982;&#21518;&#29992; $\bar{\mathcal{G}}=\chi\circ\bar{f}\circ\psi$ &#26469;&#36817;&#20284;$\mathcal{G}^\dagger$&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#28041;&#21450;&#22810;&#20010;&#20363;&#23376;&#65292;&#21253;&#25324;&#24120;&#35265;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31639;&#23376;&#36817;&#20284;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#26680;&#26041;&#27861;&#37117;&#26159;&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general kernel-based framework for learning operators between Banach spaces along with a priori error analysis and comprehensive numerical comparisons with popular neural net (NN) approaches such as Deep Operator Net (DeepONet) [Lu et al.] and Fourier Neural Operator (FNO) [Li et al.]. We consider the setting where the input/output spaces of target operator $\mathcal{G}^\dagger\,:\, \mathcal{U}\to \mathcal{V}$ are reproducing kernel Hilbert spaces (RKHS), the data comes in the form of partial observations $\phi(u_i), \varphi(v_i)$ of input/output functions $v_i=\mathcal{G}^\dagger(u_i)$ ($i=1,\ldots,N$), and the measurement operators $\phi\,:\, \mathcal{U}\to \mathbb{R}^n$ and $\varphi\,:\, \mathcal{V} \to \mathbb{R}^m$ are linear. Writing $\psi\,:\, \mathbb{R}^n \to \mathcal{U}$ and $\chi\,:\, \mathbb{R}^m \to \mathcal{V}$ for the optimal recovery maps associated with $\phi$ and $\varphi$, we approximate $\mathcal{G}^\dagger$ with $\bar{\mathcal{G}}=\chi \circ \bar{f} \ci
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32467;&#30452;&#32928;&#30284;&#65288;CRC&#65289;&#24687;&#32905;&#20998;&#31867;&#25216;&#26415;&#36755;&#20986;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#31995;&#32479;&#21644;&#29420;&#29305;&#30340;CRC&#24687;&#32905;&#27169;&#25311;&#20307;&#30340;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#20854;&#36807;&#24230;&#33258;&#20449;&#30340;&#36755;&#20986;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;CRC&#24687;&#32905;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2304.13192</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#19982;&#32622;&#20449;&#24230;&#26657;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#32467;&#30452;&#32928;&#30284;&#24687;&#32905;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Towards Reliable Colorectal Cancer Polyps Classification via Vision Based Tactile Sensing and Confidence-Calibrated Neural Networks. (arXiv:2304.13192v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32467;&#30452;&#32928;&#30284;&#65288;CRC&#65289;&#24687;&#32905;&#20998;&#31867;&#25216;&#26415;&#36755;&#20986;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#31995;&#32479;&#21644;&#29420;&#29305;&#30340;CRC&#24687;&#32905;&#27169;&#25311;&#20307;&#30340;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#20854;&#36807;&#24230;&#33258;&#20449;&#30340;&#36755;&#20986;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;CRC&#24687;&#32905;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29616;&#26377;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32467;&#30452;&#32928;&#30284;&#65288;CRC&#65289;&#24687;&#32905;&#20998;&#31867;&#25216;&#26415;&#36755;&#20986;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32622;&#20449;&#24230;&#26657;&#20934;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#22411;&#30340;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#31995;&#32479;&#21644;&#29420;&#29305;&#30340;CRC&#24687;&#32905;&#27169;&#25311;&#20307;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20256;&#32479;&#30340;&#20934;&#30830;&#24230;&#21644;&#31934;&#24230;&#31561;&#25351;&#26631;&#19981;&#36275;&#20197;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#22312;&#22788;&#29702;&#25935;&#24863;CRC&#24687;&#32905;&#35786;&#26029;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#20854;&#36807;&#24230;&#33258;&#20449;&#30340;&#36755;&#20986;&#38382;&#39064;&#12290;&#20026;&#35780;&#20272;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807; reliability diagrams &#21644;&#20854;&#20182;&#32479;&#35745;&#25351;&#26631;&#65292;&#24341;&#20837;&#20102;&#22122;&#22768;&#21644;&#27169;&#31946;&#25928;&#26524;&#26469;&#27979;&#35797; VS-TS &#25152;&#33719;&#21462;&#30340;&#32441;&#29702;&#22270;&#20687;&#30340;&#38750;&#29702;&#24819;&#36755;&#20837;&#19979;&#30340;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, toward addressing the over-confident outputs of existing artificial intelligence-based colorectal cancer (CRC) polyp classification techniques, we propose a confidence-calibrated residual neural network. Utilizing a novel vision-based tactile sensing (VS-TS) system and unique CRC polyp phantoms, we demonstrate that traditional metrics such as accuracy and precision are not sufficient to encapsulate model performance for handling a sensitive CRC polyp diagnosis. To this end, we develop a residual neural network classifier and address its over-confident outputs for CRC polyps classification via the post-processing method of temperature scaling. To evaluate the proposed method, we introduce noise and blur to the obtained textural images of the VS-TS and test the model's reliability for non-ideal inputs through reliability diagrams and other statistical metrics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TABLET&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;20&#20010;&#19981;&#21516;&#30340;&#21253;&#21547;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#39044;&#27979;&#38382;&#39064;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#35780;&#20272;&#20102;&#25351;&#20196;&#22312;&#20445;&#30495;&#24230;&#21644;LLM&#22312;&#34920;&#26684;&#39044;&#27979;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13188</link><description>&lt;p&gt;
TABLET&#65306;&#22522;&#20110;&#25351;&#20196;&#23398;&#20064;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
TABLET: Learning From Instructions For Tabular Data. (arXiv:2304.13188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13188
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TABLET&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;20&#20010;&#19981;&#21516;&#30340;&#21253;&#21547;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#39044;&#27979;&#38382;&#39064;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#35780;&#20272;&#20102;&#25351;&#20196;&#22312;&#20445;&#30495;&#24230;&#21644;LLM&#22312;&#34920;&#26684;&#39044;&#27979;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#34920;&#26684;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#36890;&#24120;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#25935;&#24863;&#21644;&#25104;&#26412;&#39640;&#30340;&#39046;&#22495;&#65292;&#27604;&#22914;&#21307;&#23398;&#21644;&#37329;&#34701;&#12290;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25351;&#20196;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;LLM&#20013;&#30340;&#30693;&#35782;&#26469;&#35299;&#20915;&#34920;&#26684;&#39044;&#27979;&#38382;&#39064;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TABLET&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;20&#20010;&#19981;&#21516;&#30340;&#21253;&#21547;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#20123;&#25351;&#20196;&#22312;&#25514;&#36766;&#12289;&#32454;&#33410;&#21644;&#25216;&#26415;&#24615;&#26041;&#38754;&#21508;&#19981;&#30456;&#21516;&#12290;&#27492;&#22806;&#65292;TABLET&#36824;&#21253;&#25324;&#25351;&#20196;&#30340;&#36923;&#36753;&#21644;&#32467;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19978;&#19979;&#25991;&#25351;&#20196;&#30340;&#24110;&#21161;&#19979;&#65292;Flan-T5 11b&#30340;&#38646;&#31034;&#20363;F1&#24615;&#33021;&#24179;&#22343;&#25552;&#39640;&#20102;44&#65285;&#65292;&#22312;TABLET&#19978;&#65292;ChatGPT&#30340;&#25552;&#21319;&#20026;13&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25351;&#20196;&#20445;&#30495;&#24230;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;LLM&#36827;&#34892;&#34920;&#26684;&#39044;&#27979;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#36890;&#24120;&#20250;&#24573;&#30053;&#25351;&#20196;&#24182;&#20381;&#36182;&#20110;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring high-quality data is often a significant challenge in training machine learning (ML) models for tabular prediction, particularly in privacy-sensitive and costly domains like medicine and finance. Providing natural language instructions to large language models (LLMs) offers an alternative solution. However, it is unclear how effectively instructions leverage the knowledge in LLMs for solving tabular prediction problems. To address this gap, we introduce TABLET, a benchmark of 20 diverse tabular datasets annotated with instructions that vary in their phrasing, granularity, and technicality. Additionally, TABLET includes the instructions' logic and structured modifications to the instructions. We find in-context instructions increase zero-shot F1 performance for Flan-T5 11b by 44% on average and 13% for ChatGPT on TABLET. Also, we explore the limitations of using LLMs for tabular prediction in our benchmark by evaluating instruction faithfulness. We find LLMs often ignore instr
&lt;/p&gt;</description></item><item><title>&#21457;&#29616;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22343;&#21248;&#22320;&#25277;&#21462;&#36127;&#26679;&#26412;&#20250;&#24341;&#20837;&#38169;&#35823;&#30340;&#36127;&#38754;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32416;&#27491;&#38169;&#35823;&#36127;&#38754;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#38024;&#23545;&#22270;&#25991;&#27169;&#22411;&#30340;&#26679;&#26412;&#29305;&#24322;&#24615;&#21435;&#20559;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13181</link><description>&lt;p&gt;
&#38024;&#23545;&#22270;&#25991;&#27169;&#22411;&#30340;&#26679;&#26412;&#29305;&#24322;&#24615;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample-Specific Debiasing for Better Image-Text Models. (arXiv:2304.13181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13181
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22343;&#21248;&#22320;&#25277;&#21462;&#36127;&#26679;&#26412;&#20250;&#24341;&#20837;&#38169;&#35823;&#30340;&#36127;&#38754;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32416;&#27491;&#38169;&#35823;&#36127;&#38754;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#38024;&#23545;&#22270;&#25991;&#27169;&#22411;&#30340;&#26679;&#26412;&#29305;&#24322;&#24615;&#21435;&#20559;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22270;&#25991;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#35270;&#35273;&#23450;&#20301;&#21644;&#36328;&#27169;&#24577;&#26816;&#32034;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#35821;&#20041;&#19978;&#30456;&#20284;&#65288;&#27491;&#65289;&#21644;&#19981;&#30456;&#20284;&#65288;&#36127;&#65289;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#23545;&#27604;&#12290;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22343;&#21248;&#22320;&#25277;&#21462;&#36127;&#26679;&#26412;&#20250;&#24341;&#20837;&#38169;&#35823;&#30340;&#36127;&#38754;&#26679;&#26412;&#65292;&#21363;&#23558;&#21516;&#23646;&#19968;&#31867;&#30340;&#26679;&#26412;&#35270;&#20026;&#19981;&#30456;&#20284;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#20013;&#65292;&#28508;&#22312;&#30340;&#31867;&#21035;&#20998;&#24067;&#26159;&#19981;&#22343;&#21248;&#30340;&#65292;&#24847;&#21619;&#30528;&#38169;&#35823;&#30340;&#36127;&#38754;&#26679;&#26412;&#20986;&#29616;&#30340;&#27604;&#20363;&#39640;&#24230;&#19981;&#21516;&#12290;&#20026;&#20102;&#25552;&#39640;&#23398;&#24471;&#30340;&#34920;&#31034;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32416;&#27491;&#38169;&#35823;&#36127;&#38754;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#20351;&#29992;&#20272;&#35745;&#30340;&#26679;&#26412;&#29305;&#24322;&#24615;&#31867;&#21035;&#27010;&#29575;&#30340;&#21435;&#20559;&#23545;&#27604;&#23398;&#20064;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#22312;&#22270;&#20687;&#21644;&#37197;&#23545;&#30340;&#22270;&#25991;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#30340;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning on image-text data facilitates crucial medical applications, such as image classification, visual grounding, and cross-modal retrieval. One common approach involves contrasting semantically similar (positive) and dissimilar (negative) pairs of data points. Drawing negative samples uniformly from the training data set introduces false negatives, i.e., samples that are treated as dissimilar but belong to the same class. In healthcare data, the underlying class distribution is nonuniform, implying that false negatives occur at a highly variable rate. To improve the quality of learned representations, we develop a novel approach that corrects for false negatives. Our method can be viewed as a variant of debiased constrastive learning that uses estimated sample-specific class probabilities. We provide theoretical analysis of the objective function and demonstrate the proposed approach on both image and paired image-text data sets. Our experiments demo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; shard graph &#36827;&#34892;&#26426;&#22120;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#26368;&#23567;&#21270;&#36951;&#24536;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#22810;&#26679;&#25968;&#25454;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13169</link><description>&lt;p&gt;
SAFE: &#20351;&#29992; Shard Graphs &#36827;&#34892;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
SAFE: Machine Unlearning With Shard Graphs. (arXiv:2304.13169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; shard graph &#36827;&#34892;&#26426;&#22120;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#26368;&#23567;&#21270;&#36951;&#24536;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#22810;&#26679;&#25968;&#25454;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; Synergy Aware Forgetting Ensemble&#65288;SAFE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26368;&#23567;&#21270;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#28040;&#38500;&#35757;&#32451;&#26679;&#26412;&#24433;&#21709;&#30340;&#39044;&#26399;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#36866;&#24212;&#21508;&#31181;&#25968;&#25454;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;&#36825;&#20010;&#36807;&#31243;&#20063;&#34987;&#31216;&#20026;&#36873;&#25321;&#24615;&#36951;&#24536;&#25110;&#36951;&#24536;&#65292;&#36890;&#24120;&#26159;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#20998;&#25104;&#30862;&#29255;&#65292;&#23545;&#27599;&#20010;&#30862;&#29255;&#36827;&#34892;&#23436;&#20840;&#29420;&#31435;&#30340;&#27169;&#22411;&#35757;&#32451;&#65292;&#28982;&#21518;&#23558;&#25152;&#24471;&#27169;&#22411;&#21512;&#25104;&#26469;&#36827;&#34892;&#30340;&#12290;&#22686;&#21152;&#30862;&#29255;&#30340;&#25968;&#37327;&#21487;&#20197;&#38477;&#20302;&#36951;&#24536;&#30340;&#39044;&#26399;&#25104;&#26412;&#65292;&#20294;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#25512;&#29702;&#25104;&#26412;&#65292;&#24182;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#26368;&#32456;&#20934;&#30830;&#24615;&#65292;&#22240;&#20026;&#29420;&#31435;&#30340;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#22833;&#21435;&#20102;&#26679;&#26412;&#20043;&#38388;&#30340;&#21327;&#21516;&#20449;&#24687;&#12290;SAFE &#24341;&#20837;&#20102; shard graph &#30340;&#27010;&#24565;&#65292;&#23427;&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#20854;&#20182;&#30862;&#29255;&#20013;&#24341;&#20837;&#26377;&#38480;&#30340;&#20449;&#24687;&#65292;&#20197;&#29306;&#29298;&#19968;&#23450;&#30340;&#39044;&#26399;&#36951;&#24536;&#25104;&#26412;&#22686;&#21152;&#26126;&#26174;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20173;&#28982;&#23454;&#29616;&#23436;&#20840;&#28040;&#38500;&#27531;&#30041;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Synergy Aware Forgetting Ensemble (SAFE), a method to adapt large models on a diverse collection of data while minimizing the expected cost to remove the influence of training samples from the trained model. This process, also known as selective forgetting or unlearning, is often conducted by partitioning a dataset into shards, training fully independent models on each, then ensembling the resulting models. Increasing the number of shards reduces the expected cost to forget but at the same time it increases inference cost and reduces the final accuracy of the model since synergistic information between samples is lost during the independent model training. Rather than treating each shard as independent, SAFE introduces the notion of a shard graph, which allows incorporating limited information from other shards during training, trading off a modest increase in expected forgetting cost with a significant increase in accuracy, all while still attaining complete removal of resi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#35745;&#31639;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#32467;&#26500;&#21098;&#26525;&#65292;&#20351;&#20854;&#22312;&#26368;&#23567;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;20%&#20197;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.13164</link><description>&lt;p&gt;
&#36808;&#21521;&#35745;&#31639;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Compute-Optimal Transfer Learning. (arXiv:2304.13164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#35745;&#31639;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#32467;&#26500;&#21098;&#26525;&#65292;&#20351;&#20854;&#22312;&#26368;&#23567;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;20%&#20197;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#24471;&#36801;&#31227;&#23398;&#20064;&#39046;&#22495;&#21457;&#29983;&#20102;&#37325;&#22823;&#21464;&#38761;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#36866;&#24212;&#24615;&#12290;&#20294;&#26159;&#65292;&#24494;&#35843;&#25110;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#35201;&#27714;&#21487;&#33021;&#20250;&#38459;&#30861;&#23427;&#20204;&#30340;&#24191;&#27867;&#20351;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#35745;&#31639;&#25928;&#29575;&#19982;&#28176;&#36817;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#23398;&#20064;&#31639;&#27861;&#22312;&#35745;&#31639;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#23454;&#29616;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#32467;&#26500;&#21098;&#26525;&#65292;&#21487;&#20197;&#20351;&#23427;&#20204;&#22312;&#26368;&#23567;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#25552;&#20379;&#22810;&#31181;&#36801;&#31227;&#22330;&#26223;&#30340;Nevis'22&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20302;&#35745;&#31639;&#33539;&#22260;&#20869;&#65292;&#21098;&#26525;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#21367;&#31215;&#36807;&#28388;&#22120;&#21487;&#20197;&#24102;&#26469;&#36229;&#36807;20%&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of transfer learning is undergoing a significant shift with the introduction of large pretrained models which have demonstrated strong adaptability to a variety of downstream tasks. However, the high computational and memory requirements to finetune or use these models can be a hindrance to their widespread use. In this study, we present a solution to this issue by proposing a simple yet effective way to trade computational efficiency for asymptotic performance which we define as the performance a learning algorithm achieves as compute tends to infinity. Specifically, we argue that zero-shot structured pruning of pretrained models allows them to increase compute efficiency with minimal reduction in performance. We evaluate our method on the Nevis'22 continual learning benchmark that offers a diverse set of transfer scenarios. Our results show that pruning convolutional filters of pretrained models can lead to more than 20% performance improvement in low computational regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#31232;&#30095;&#32534;&#30721;&#30340;&#26041;&#27861;&#23545;&#20855;&#26377;&#30284;&#30151;&#20998;&#31867;&#30446;&#26631;&#30340;TCR&#34507;&#30333;&#24207;&#21015;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#31867;&#65292;&#20026;&#22522;&#20110;TCR&#30340;&#20813;&#30123;&#27835;&#30103;&#25552;&#20379;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.13145</link><description>&lt;p&gt;
T&#32454;&#32990;&#21463;&#20307;&#34507;&#30333;&#24207;&#21015;&#21644;&#31232;&#30095;&#32534;&#30721;&#65306;&#30284;&#30151;&#20998;&#31867;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
T Cell Receptor Protein Sequences and Sparse Coding: A Novel Approach to Cancer Classification. (arXiv:2304.13145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#31232;&#30095;&#32534;&#30721;&#30340;&#26041;&#27861;&#23545;&#20855;&#26377;&#30284;&#30151;&#20998;&#31867;&#30446;&#26631;&#30340;TCR&#34507;&#30333;&#24207;&#21015;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#31867;&#65292;&#20026;&#22522;&#20110;TCR&#30340;&#20813;&#30123;&#27835;&#30103;&#25552;&#20379;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#26159;&#19968;&#31181;&#20197;&#19981;&#21463;&#25511;&#21046;&#30340;&#32454;&#32990;&#29983;&#38271;&#21644;&#22686;&#27542;&#20026;&#29305;&#24449;&#30340;&#22797;&#26434;&#30142;&#30149;&#12290;T&#32454;&#32990;&#21463;&#20307;&#65288;TCR&#65289;&#26159;&#36866;&#24212;&#24615;&#20813;&#30123;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#34507;&#30333;&#36136;&#65292;&#23427;&#20204;&#23545;&#25239;&#21407;&#30340;&#29305;&#24322;&#24615;&#35782;&#21035;&#22312;&#20813;&#30123;&#21453;&#24212;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#23545;&#25239;&#30284;&#30151;&#12290;TCR&#30340;&#22810;&#26679;&#24615;&#21644;&#29305;&#24322;&#24615;&#20351;&#23427;&#20204;&#25104;&#20026;&#30596;&#20934;&#30284;&#32454;&#32990;&#30340;&#29702;&#24819;&#36873;&#25321;&#65292;&#36817;&#26399;&#27979;&#24207;&#25216;&#26415;&#30340;&#36827;&#27493;&#20351;&#24471;TCR&#24211;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#36825;&#23548;&#33268;&#21457;&#29616;&#20102;&#20855;&#26377;&#24378;&#26377;&#21147;&#30340;&#25239;&#30284;&#27963;&#24615;&#30340;TCR&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;TCR&#30340;&#20813;&#30123;&#27835;&#30103;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;&#31232;&#30095;&#32534;&#30721;&#23545;&#20855;&#26377;&#30284;&#30151;&#20998;&#31867;&#20316;&#20026;&#30446;&#26631;&#26631;&#31614;&#30340;TCR&#34507;&#30333;&#24207;&#21015;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#31867;&#30340;&#24212;&#29992;&#12290;&#31232;&#30095;&#32534;&#30721;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#20013;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#19968;&#32452;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#26469;&#34920;&#31034;&#25968;&#25454;&#65292;&#21487;&#20197;&#25429;&#25417;&#27688;&#22522;&#37240;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#35782;&#21035;&#24207;&#21015;&#20013;&#30340;&#24494;&#23567;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer is a complex disease characterized by uncontrolled cell growth and proliferation. T cell receptors (TCRs) are essential proteins for the adaptive immune system, and their specific recognition of antigens plays a crucial role in the immune response against diseases, including cancer. The diversity and specificity of TCRs make them ideal for targeting cancer cells, and recent advancements in sequencing technologies have enabled the comprehensive profiling of TCR repertoires. This has led to the discovery of TCRs with potent anti-cancer activity and the development of TCR-based immunotherapies. In this study, we investigate the use of sparse coding for the multi-class classification of TCR protein sequences with cancer categories as target labels. Sparse coding is a popular technique in machine learning that enables the representation of data with a set of informative features and can capture complex relationships between amino acids and identify subtle patterns in the sequence tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#33021;&#26681;&#25454;&#31227;&#21160;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#23545;&#26223;&#35266;&#36827;&#34892;&#20998;&#23618;&#65292;&#36890;&#36807;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#23454;&#29616;&#20102;&#23545;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#24314;&#27169;&#65292;&#36866;&#29992;&#20110;&#20998;&#31867;&#23621;&#27665;&#21306;&#21644;&#21830;&#19994;&#21306;&#31561;&#19981;&#21516;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.13143</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#26102;&#31354;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Temporal Analysis of Spatiotemporal Data. (arXiv:2304.13143v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#33021;&#26681;&#25454;&#31227;&#21160;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#23545;&#26223;&#35266;&#36827;&#34892;&#20998;&#23618;&#65292;&#36890;&#36807;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#23454;&#29616;&#20102;&#23545;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#24314;&#27169;&#65292;&#36866;&#29992;&#20110;&#20998;&#31867;&#23621;&#27665;&#21306;&#21644;&#21830;&#19994;&#21306;&#31561;&#19981;&#21516;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#27963;&#21160;&#30340;&#26102;&#38388;&#27169;&#24335;&#19982;&#29992;&#22320;&#31867;&#22411;&#20043;&#38388;&#23384;&#22312;&#30528;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#26681;&#25454;&#31227;&#21160;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#23545;&#26223;&#35266;&#36827;&#34892;&#20998;&#23618;&#12290;&#39318;&#20808;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#36716;&#25442;&#20026;&#39057;&#22495;&#65292;&#28982;&#21518;&#36890;&#36807;&#21387;&#32553;&#33258;&#32534;&#30721;&#22120;&#23558;&#20854;&#21387;&#32553;&#20026;&#20219;&#21153;&#26080;&#20851;&#30340;&#26102;&#38388;&#23884;&#20837;&#65292;&#35813;&#23884;&#20837;&#20445;&#30041;&#20102;&#26102;&#38388;&#24207;&#21015;&#20013;&#35266;&#23519;&#21040;&#30340;&#21608;&#26399;&#24615;&#26102;&#38388;&#27169;&#24335;&#12290;&#20687;&#32032;&#32423;&#30340;&#23884;&#20837;&#34987;&#36716;&#25442;&#20026;&#31867;&#20284;&#22270;&#20687;&#30340;&#36890;&#36947;&#65292;&#21487;&#20197;&#29992;&#20110;&#22522;&#20110;&#20219;&#21153;&#30340;&#19979;&#28216;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#24314;&#27169;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26102;&#38388;&#23884;&#20837;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#34920;&#24449;&#65292;&#24182;&#19988;&#23545;&#20110;&#20998;&#31867;&#23621;&#27665;&#21306;&#21644;&#21830;&#19994;&#21306;&#31561;&#19981;&#21516;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exists a correlation between geospatial activity temporal patterns and type of land use. A novel self-supervised approach is proposed to stratify landscape based on mobility activity time series. First, the time series signal is transformed to the frequency domain and then compressed into task-agnostic temporal embeddings by a contractive autoencoder, which preserves cyclic temporal patterns observed in time series. The pixel-wise embeddings are converted to image-like channels that can be used for task-based, multimodal modeling of downstream geospatial tasks using deep semantic segmentation. Experiments show that temporal embeddings are semantically meaningful representations of time series data and are effective across different tasks such as classifying residential area and commercial areas.
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#38024;&#23545;&#22686;&#26448;&#21046;&#36896;&#35797;&#20214;&#30340;&#34920;&#38754;&#31895;&#31961;&#24230;&#20351;&#29992;&#19977;&#31181;&#37327;&#23376;&#31639;&#27861;&#65288;QNN&#12289;Q-Forest&#21644;VQC&#65289;&#36827;&#34892;&#22238;&#24402;&#39044;&#27979;&#65292;&#20854;&#20013;Q-Forest&#31639;&#27861;&#34920;&#29616;&#26368;&#20248;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;MSE&#21644;MAE&#21644;&#36739;&#39640;&#30340;EVS&#12290;</title><link>http://arxiv.org/abs/2304.13142</link><description>&lt;p&gt;
&#38754;&#21521;&#22686;&#26448;&#21046;&#36896;&#35797;&#20214;&#34920;&#38754;&#31895;&#31961;&#24230;&#39044;&#27979;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning Approach for the Prediction of Surface Roughness in Additive Manufactured Specimens. (arXiv:2304.13142v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13142
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#38024;&#23545;&#22686;&#26448;&#21046;&#36896;&#35797;&#20214;&#30340;&#34920;&#38754;&#31895;&#31961;&#24230;&#20351;&#29992;&#19977;&#31181;&#37327;&#23376;&#31639;&#27861;&#65288;QNN&#12289;Q-Forest&#21644;VQC&#65289;&#36827;&#34892;&#22238;&#24402;&#39044;&#27979;&#65292;&#20854;&#20013;Q-Forest&#31639;&#27861;&#34920;&#29616;&#26368;&#20248;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;MSE&#21644;MAE&#21644;&#36739;&#39640;&#30340;EVS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#38754;&#31895;&#31961;&#24230;&#26159;&#24433;&#21709;&#22686;&#26448;&#21046;&#36896;&#38646;&#20214;&#24615;&#33021;&#21644;&#21151;&#33021;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#20934;&#30830;&#39044;&#27979;&#34920;&#38754;&#31895;&#31961;&#24230;&#23545;&#20110;&#20248;&#21270;&#21046;&#36896;&#36807;&#31243;&#21644;&#30830;&#20445;&#26368;&#32456;&#20135;&#21697;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#37327;&#23376;&#35745;&#31639;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#21644;&#21019;&#24314;&#31934;&#30830;&#39044;&#27979;&#27169;&#22411;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#38024;&#23545;&#22686;&#26448;&#21046;&#36896;&#35797;&#20214;&#30340;&#34920;&#38754;&#31895;&#31961;&#24230;&#36827;&#34892;&#20102;&#19977;&#31181;&#37327;&#23376;&#31639;&#27861;&#65292;&#21253;&#25324;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#12289;&#37327;&#23376;&#26862;&#26519;&#65288;Q-Forest&#65289;&#21644;&#21464;&#20998;&#37327;&#23376;&#20998;&#31867;&#22120;&#65288;VQC&#65289;&#30340;&#22238;&#24402;&#36866;&#24212;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#12289;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#21644;&#35299;&#37322;&#26041;&#24046;&#24471;&#20998;&#65288;EVS&#65289;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;Q-Forest&#31639;&#27861;&#36229;&#36234;&#20102;&#20854;&#20182;&#31639;&#27861;&#65292;MSE&#20026;56.905&#65292;MAE&#20026;7.479&#65292;EVS&#20026;0.2957&#12290;
&lt;/p&gt;
&lt;p&gt;
Surface roughness is a crucial factor influencing the performance and functionality of additive manufactured components. Accurate prediction of surface roughness is vital for optimizing manufacturing processes and ensuring the quality of the final product. Quantum computing has recently gained attention as a potential solution for tackling complex problems and creating precise predictive models. In this research paper, we conduct an in-depth comparison of three quantum algorithms i.e. the Quantum Neural Network (QNN), Quantum Forest (Q-Forest), and Variational Quantum Classifier (VQC) adapted for regression for predicting surface roughness in additive manufactured specimens for the first time. We assess the algorithms performance using Mean Squared Error (MSE), Mean Absolute Error (MAE), and Explained Variance Score (EVS) as evaluation metrics. Our findings show that the Q-Forest algorithm surpasses the other algorithms, achieving an MSE of 56.905, MAE of 7.479, and an EVS of 0.2957. I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;ESimCSE&#26080;&#30417;&#30563;&#27604;&#36739;&#23398;&#20064;&#21644;UDA&#21322;&#30417;&#30563;&#27604;&#36739;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#25216;&#26415;&#35299;&#20915;&#20102;&#22823;&#26631;&#31614;&#31995;&#32479;&#25991;&#26412;&#20998;&#31867;&#30340;&#22810;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#29575;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.13140</link><description>&lt;p&gt;
ESimCSE&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32852;&#21512;UDA&#21322;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#22823;&#26631;&#31614;&#31995;&#32479;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ESimCSE Unsupervised Contrastive Learning Jointly with UDA Semi-Supervised Learning for Large Label System Text Classification Mode. (arXiv:2304.13140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;ESimCSE&#26080;&#30417;&#30563;&#27604;&#36739;&#23398;&#20064;&#21644;UDA&#21322;&#30417;&#30563;&#27604;&#36739;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#25216;&#26415;&#35299;&#20915;&#20102;&#22823;&#26631;&#31614;&#31995;&#32479;&#25991;&#26412;&#20998;&#31867;&#30340;&#22810;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#25991;&#26412;&#20998;&#31867;&#38754;&#20020;&#30340;&#25361;&#25112;&#21253;&#25324;&#22810;&#20010;&#26631;&#31614;&#31995;&#32479;&#12289;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#21644;&#39640;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#32852;&#21512;&#35757;&#32451;&#25216;&#26415;&#65292;&#23558;ESimCSE&#26080;&#30417;&#30563;&#27604;&#36739;&#23398;&#20064;&#21644;UDA&#21322;&#30417;&#30563;&#27604;&#36739;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;ESimCSE&#27169;&#22411;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#39640;&#25928;&#22320;&#23398;&#20064;&#25991;&#26412;&#21521;&#37327;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#65307;&#32780;UDA&#21017;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;FGM&#21644;PGD&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;Ruesters&#19978;&#26377;8%&#21644;10%&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenges faced by text classification with large tag systems in natural language processing tasks include multiple tag systems, uneven data distribution, and high noise. To address these problems, the ESimCSE unsupervised comparative learning and UDA semi-supervised comparative learning models are combined through the use of joint training techniques in the models.The ESimCSE model efficiently learns text vector representations using unlabeled data to achieve better classification results, while UDA is trained using unlabeled data through semi-supervised learning methods to improve the prediction performance of the models and stability, and further improve the generalization ability of the model. In addition, adversarial training techniques FGM and PGD are used in the model training process to improve the robustness and reliability of the model. The experimental results show that there is an 8% and 10% accuracy improvement relative to Baseline on the public dataset Ruesters as we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#24471;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#22312;&#26356;&#22823;&#33539;&#22260;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#20915;&#31574;&#29615;&#22659;&#20013;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13138</link><description>&lt;p&gt;
&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#30340;&#26356;&#26032;&#31561;&#20215;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Update Equivalence Framework for Decision-Time Planning. (arXiv:2304.13138v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#24471;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#22312;&#26356;&#22823;&#33539;&#22260;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#20915;&#31574;&#29615;&#22659;&#20013;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26827;&#31867;&#28216;&#25103;&#31561;&#23436;&#20840;&#20449;&#24687;&#29615;&#22659;&#20013;&#65292;&#21363;&#26102;&#20462;&#27491;&#65288;&#25110;&#26500;&#24314;&#65289;&#31574;&#30053;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26159;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#30340;&#20851;&#38190;&#12290;&#19968;&#20123;&#30740;&#31350;&#23558;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#25193;&#23637;&#21040;&#26356;&#26222;&#36941;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25169;&#20811;&#20013;&#30340;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#32771;&#34385;&#38543;&#30528;&#38750;&#20844;&#20849;&#20449;&#24687;&#37327;&#30340;&#22686;&#21152;&#32780;&#24555;&#36895;&#22686;&#38271;&#30340;&#23376;&#28216;&#25103;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#38750;&#20844;&#20849;&#20449;&#24687;&#37327;&#36739;&#22823;&#26102;&#19981;&#36215;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#32780;&#19981;&#26159;&#23376;&#28216;&#25103;&#27010;&#24565;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#27169;&#25311;&#21516;&#27493;&#23398;&#20064;&#31639;&#27861;&#30340;&#26356;&#26032;&#12290;&#36825;&#20010;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#24341;&#20837;&#19968;&#31995;&#21015;&#21407;&#21017;&#19978;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#24182;&#20026;&#26032;&#30340;&#19968;&#20010;&#31995;&#21015;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of revising (or constructing) a policy immediately prior to execution -- known as decision-time planning -- is key to achieving superhuman performance in perfect-information settings like chess and Go. A recent line of work has extended decision-time planning to more general imperfect-information settings, leading to superhuman performance in poker. However, these methods requires considering subgames whose sizes grow quickly in the amount of non-public information, making them unhelpful when the amount of non-public information is large. Motivated by this issue, we introduce an alternative framework for decision-time planning that is not based on subgames but rather on the notion of update equivalence. In this framework, decision-time planning algorithms simulate updates of synchronous learning algorithms. This framework enables us to introduce a new family of principled decision-time planning algorithms that do not rely on public information, opening the door to sound and
&lt;/p&gt;</description></item><item><title>MEDNC&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;COVID-19&#33258;&#21160;&#35786;&#26029;&#26041;&#27861;&#65292;&#20351;&#29992;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#39044;&#27979;&#21644;&#35786;&#26029;COVID-19&#65292;&#20934;&#30830;&#24615;&#39640;&#36798;98.79%&#21644;99.82%&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#33021;&#36866;&#29992;&#20110;&#20854;&#20182;&#38382;&#39064;&#65292;&#22914;&#22823;&#33041;&#32959;&#30244;&#21644;&#34880;&#32454;&#32990;&#25968;&#25454;&#30340;&#39044;&#27979;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13135</link><description>&lt;p&gt;
MEDNC&#65306;&#22810;&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;COVID-19&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
MEDNC: Multi-ensemble deep neural network for COVID-19 diagnosis. (arXiv:2304.13135v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13135
&lt;/p&gt;
&lt;p&gt;
MEDNC&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;COVID-19&#33258;&#21160;&#35786;&#26029;&#26041;&#27861;&#65292;&#20351;&#29992;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#39044;&#27979;&#21644;&#35786;&#26029;COVID-19&#65292;&#20934;&#30830;&#24615;&#39640;&#36798;98.79%&#21644;99.82%&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#33021;&#36866;&#29992;&#20110;&#20854;&#20182;&#38382;&#39064;&#65292;&#22914;&#22823;&#33041;&#32959;&#30244;&#21644;&#34880;&#32454;&#32990;&#25968;&#25454;&#30340;&#39044;&#27979;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#22411;&#20896;&#29366;&#30149;&#27602;&#30149;2019(COVID-19)&#24050;&#32463;&#22312;&#20840;&#29699;&#34067;&#24310;&#20102;&#19977;&#24180;&#65292;&#20294;&#26159;&#35768;&#22810;&#22320;&#21306;&#30340;&#21307;&#30103;&#35774;&#26045;&#20173;&#28982;&#19981;&#36275;&#12290;&#38656;&#35201;&#24555;&#36895;&#30340;COVID-19&#35786;&#26029;&#26469;&#35782;&#21035;&#39640;&#21361;&#24739;&#32773;&#24182;&#26368;&#22823;&#21270;&#21033;&#29992;&#26377;&#38480;&#30340;&#21307;&#30103;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;MEDNC&#65292;&#29992;&#20110;&#20351;&#29992;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;(CT)&#22270;&#20687;&#36827;&#34892; COVID-19 &#30340;&#33258;&#21160;&#39044;&#27979;&#21644;&#35786;&#26029;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#20351;&#29992;&#20004;&#20010;&#20844;&#20849;&#21487;&#29992;&#30340;COVID-19&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#27169;&#22411;&#30340;&#26500;&#24314;&#28789;&#24863;&#28304;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;MEDNC&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;COVID-19&#24863;&#26579;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;98.79%&#21644;99.82%&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#22312;&#22823;&#33041;&#32959;&#30244;&#21644;&#34880;&#32454;&#32990;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;MEDNC&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20998;&#21035;&#36798;&#21040;&#20102;99.39%&#21644;99.28%&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#31181;COVID-19&#35782;&#21035;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#20248;&#21270;&#21307;&#30103;&#36164;&#28304;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coronavirus disease 2019 (COVID-19) has spread all over the world for three years, but medical facilities in many areas still aren't adequate. There is a need for rapid COVID-19 diagnosis to identify high-risk patients and maximize the use of limited medical resources. Motivated by this fact, we proposed the deep learning framework MEDNC for automatic prediction and diagnosis of COVID-19 using computed tomography (CT) images. Our model was trained using two publicly available sets of COVID-19 data. And it was built with the inspiration of transfer learning. Results indicated that the MEDNC greatly enhanced the detection of COVID-19 infections, reaching an accuracy of 98.79% and 99.82% respectively. We tested MEDNC on a brain tumor and a blood cell dataset to show that our model applies to a wide range of problems. The outcomes demonstrated that our proposed models attained an accuracy of 99.39% and 99.28%, respectively. This COVID-19 recognition tool could help optimize healthcare reso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#21521;&#38142;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;DC-GANs&#65289;&#65292;&#20351;&#29992;&#37051;&#22495;&#36807;&#31243;&#20316;&#20026;&#20851;&#38190;&#27493;&#39588;&#29983;&#25104;&#21516;&#26679;&#20998;&#24067;&#30340;&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.13131</link><description>&lt;p&gt;
&#26377;&#21521;&#38142;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Directed Chain Generative Adversarial Networks. (arXiv:2304.13131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#21521;&#38142;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;DC-GANs&#65289;&#65292;&#20351;&#29992;&#37051;&#22495;&#36807;&#31243;&#20316;&#20026;&#20851;&#38190;&#27493;&#39588;&#29983;&#25104;&#21516;&#26679;&#20998;&#24067;&#30340;&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#36890;&#24120;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#20363;&#22914;&#25551;&#36848;&#31038;&#21306;&#24847;&#35265;&#20998;&#27495;&#12289;&#31070;&#32463;&#20803;&#30340;&#38388;&#38548;&#20998;&#24067;&#20197;&#21450;&#25391;&#33633;&#22120;&#30340;&#22266;&#26377;&#39057;&#29575;&#30340;&#25968;&#25454;&#12290;&#29983;&#25104;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#24050;&#25104;&#20026;&#29616;&#26377;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#25361;&#25112;&#12290;&#20363;&#22914;&#65292;&#23558;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#35270;&#20026;&#26080;&#38480;&#32500;GAN&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#24050;&#32463;&#23637;&#31034;&#20102;&#25104;&#21151;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#29992;&#20110;&#29983;&#25104;&#21333;&#23792;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22120;&#8212;&#8212;&#26377;&#21521;&#38142;GAN&#65288;DC-GAN&#65289;&#65292;&#23427;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#26377;&#21521;&#38142;&#30340;&#37051;&#22495;&#36807;&#31243;&#25110;&#36755;&#20837;&#65289;&#25554;&#20837;&#20855;&#26377;&#20998;&#24067;&#32422;&#26463;&#30340;&#26377;&#21521;&#38142;SDE&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#20013;&#12290;DC-GAN&#21487;&#20197;&#29983;&#25104;&#19982;&#37051;&#22495;&#36807;&#31243;&#30456;&#21516;&#20998;&#24067;&#30340;&#26032;&#26102;&#38388;&#24207;&#21015;&#65292;&#32780;&#37051;&#22495;&#36807;&#31243;&#23558;&#25552;&#20379;&#23398;&#20064;&#21644;&#29983;&#25104;&#22810;&#27169;&#24577;&#20998;&#24067;&#25968;&#25454;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data can be multimodal distributed, e.g., data describing the opinion divergence in a community, the interspike interval distribution of neurons, and the oscillators natural frequencies. Generating multimodal distributed real-world data has become a challenge to existing generative adversarial networks (GANs). For example, neural stochastic differential equations (Neural SDEs), treated as infinite-dimensional GANs, have demonstrated successful performance mainly in generating unimodal time series data. In this paper, we propose a novel time series generator, named directed chain GANs (DC-GANs), which inserts a time series dataset (called a neighborhood process of the directed chain or input) into the drift and diffusion coefficients of the directed chain SDEs with distributional constraints. DC-GANs can generate new time series of the same distribution as the neighborhood process, and the neighborhood process will provide the key step in learning and generating multimodal di
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#26041;&#26696;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20107;&#20214;&#36880;&#20107;&#20214;&#22810;&#26222;&#21202;&#20462;&#27491;&#65292;&#23454;&#29616;&#23545;&#24555;&#36895;&#12289;&#28909;&#30340;&#24322;&#20301;&#21516;&#20301;&#32032;&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#28608;&#20809;&#20809;&#35889;&#23398;&#30740;&#31350;&#65292;&#22312;&#26497;&#31471;&#28201;&#24230;&#19979;&#20173;&#33021;&#23454;&#29616;kHz&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13120</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20107;&#20214;&#36880;&#20107;&#20214;&#22810;&#26222;&#21202;&#20462;&#27491;&#36827;&#34892;&#24555;&#36895;&#12289;&#28909;&#30340;&#24322;&#20301;&#21516;&#20301;&#32032;&#30340;&#39640;&#31934;&#24230;&#20809;&#35889;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Precision Spectroscopy of Fast, Hot Exotic Isotopes Using Machine Learning Assisted Event-by-Event Doppler Correction. (arXiv:2304.13120v1 [nucl-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13120
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#26041;&#26696;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20107;&#20214;&#36880;&#20107;&#20214;&#22810;&#26222;&#21202;&#20462;&#27491;&#65292;&#23454;&#29616;&#23545;&#24555;&#36895;&#12289;&#28909;&#30340;&#24322;&#20301;&#21516;&#20301;&#32032;&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#28608;&#20809;&#20809;&#35889;&#23398;&#30740;&#31350;&#65292;&#22312;&#26497;&#31471;&#28201;&#24230;&#19979;&#20173;&#33021;&#23454;&#29616;kHz&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#26041;&#26696;,&#29992;&#20110;&#22312;&#24555;&#36895;&#30340;&#24322;&#20301;&#21516;&#20301;&#32032;&#19978;&#36827;&#34892;&#25935;&#24863;&#12289;&#39640;&#31934;&#24230;&#30340;&#28608;&#20809;&#20809;&#35889;&#23398;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#30005;&#22330;&#20869;&#35825;&#23548;&#21407;&#23376;&#30340;&#36880;&#27493;&#20849;&#25391;&#30005;&#31163;,&#28982;&#21518;&#26816;&#27979;&#31163;&#23376;&#21644;&#30456;&#24212;&#30340;&#30005;&#23376;,&#21487;&#20197;&#36827;&#34892;&#26102;&#38388;&#21644;&#20301;&#32622;&#25935;&#24863;&#30340;&#32467;&#26524;&#31890;&#23376;&#30340;&#27979;&#37327;&#12290;&#20351;&#29992;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476; (MDN),&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#23545;&#21333;&#20010;&#21407;&#23376;&#30340;&#21021;&#22987;&#33021;&#37327;&#36827;&#34892;&#39044;&#27979;,&#20174;&#32780;&#22312;&#20107;&#20214;&#36880;&#20107;&#20214;&#30340;&#22522;&#30784;&#19978;&#24212;&#29992;&#22810;&#26222;&#21202;&#20462;&#27491;&#25152;&#35266;&#23519;&#21040;&#30340;&#36291;&#36801;&#39057;&#29575;&#12290;&#25105;&#20204;&#23545;&#35813;&#25552;&#35758;&#30340;&#23454;&#39564;&#26041;&#26696;&#36827;&#34892;&#25968;&#20540;&#27169;&#25311;,&#24182;&#34920;&#26126;&#21487;&#20197;&#23454;&#29616;kHz&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;,&#23545;&#22312;&#26497;&#31471;&#28201;&#24230; ($&gt; 10^8$ K) &#19979;&#20135;&#29983;&#30340;&#31163;&#23376;&#26463;&#36827;&#34892;&#65292;&#22312;&#33021;&#37327;&#20998;&#25955;&#26368;&#22823;&#21487;&#36798;10 keV&#21644;&#36895;&#24230;&#20998;&#24067;&#19981;&#22343;&#21248;&#30340;&#24773;&#20917;&#19979;&#12290;&#33021;&#22815;&#30452;&#25509;&#22312;&#39640;&#33021;&#26463;&#19978;&#36827;&#34892;&#39134;&#34892;&#35889;&#23398;&#30740;&#31350;,&#20026;&#30740;&#31350;&#30701;&#23551;&#21629;&#30340;&#31163;&#23376;&#30340;&#29289;&#29702;&#21644;&#21270;&#23398;&#24615;&#36136;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an experimental scheme for performing sensitive, high-precision laser spectroscopy studies on fast exotic isotopes. By inducing a step-wise resonant ionization of the atoms travelling inside an electric field and subsequently detecting the ion and the corresponding electron, time- and position-sensitive measurements of the resulting particles can be performed. Using a Mixture Density Network (MDN), we can leverage this information to predict the initial energy of individual atoms and thus apply a Doppler correction of the observed transition frequencies on an event-by-event basis. We conduct numerical simulations of the proposed experimental scheme and show that kHz-level uncertainties can be achieved for ion beams produced at extreme temperatures ($&gt; 10^8$ K), with energy spreads as large as $10$ keV and non-uniform velocity distributions. The ability to perform in-flight spectroscopy, directly on highly energetic beams, offers unique opportunities to studying short-lived i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#36827;&#34892;&#20809;&#23398;&#31995;&#32479;&#38750;&#32447;&#24615;&#36890;&#36947;&#34917;&#20607;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;Transformer&#30340;&#35760;&#24518;&#20851;&#27880;&#33021;&#21147;&#21644;&#24182;&#34892;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#34917;&#20607;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#20449;&#24687;&#25513;&#30721;&#65292;&#29992;&#20110;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13119</link><description>&lt;p&gt;
&#21033;&#29992;Transformer&#36827;&#34892;&#20809;&#23398;&#31995;&#32479;&#38750;&#32447;&#24615;&#36890;&#36947;&#34917;&#20607;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Transformers for Nonlinear Channel Compensation in Optical Systems. (arXiv:2304.13119v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#36827;&#34892;&#20809;&#23398;&#31995;&#32479;&#38750;&#32447;&#24615;&#36890;&#36947;&#34917;&#20607;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;Transformer&#30340;&#35760;&#24518;&#20851;&#27880;&#33021;&#21147;&#21644;&#24182;&#34892;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#34917;&#20607;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#20449;&#24687;&#25513;&#30721;&#65292;&#29992;&#20110;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#36890;&#36947;&#22343;&#34913;&#26041;&#27861;&#65292;&#29992;&#20110;&#30456;&#24178;&#38271;&#36317;&#31163;&#20256;&#36755;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#30452;&#25509;&#20851;&#27880;&#19968;&#31995;&#21015;&#31526;&#21495;&#20043;&#38388;&#30340;&#35760;&#24518;&#65292;&#22240;&#27492;Transformer&#21487;&#20197;&#19982;&#24182;&#34892;&#32467;&#26500;&#26377;&#25928;&#22320;&#37197;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#37096;&#20998;&#30340;Transformer&#23454;&#29616;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#22343;&#34913;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#19981;&#21516;&#36229;&#21442;&#25968;&#33539;&#22260;&#20869;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#22788;&#29702;&#31526;&#21495;&#22359;&#65292;&#24182;&#20180;&#32454;&#36873;&#25321;&#35201;&#19968;&#36215;&#22788;&#29702;&#30340;&#32534;&#30721;&#22120;&#36755;&#20986;&#23376;&#38598;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#34917;&#20607;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#25200;&#21160;&#29702;&#35770;&#30340;&#29289;&#29702;&#23398;&#20449;&#24687;&#25513;&#30721;&#65292;&#29992;&#20110;&#38477;&#20302;Transformer&#38750;&#32447;&#24615;&#22343;&#34913;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new nonlinear channel equalization method for the coherent long-haul transmission based on Transformers. We show that due to their capability to attend directly to the memory across a sequence of symbols, Transformers can be used effectively with a parallelized structure. We present an implementation of encoder part of Transformer for nonlinear equalization and analyze its performance over a wide range of different hyper-parameters. It is shown that by processing blocks of symbols at each iteration and carefully selecting subsets of the encoder's output to be processed together, an efficient nonlinear compensation can be achieved. We also propose the use of a physic-informed mask inspired by nonlinear perturbation theory for reducing the computational complexity of Transformer nonlinear equalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#26377;&#38480;CSI&#30340;&#24773;&#20917;&#19979;&#36805;&#36895;&#36827;&#34892;THz&#27874;&#26463;&#25628;&#32034;&#65292;&#20197;&#20811;&#26381;THz&#20449;&#21495;&#30340;&#20256;&#25773;&#34928;&#20943;&#12290;</title><link>http://arxiv.org/abs/2304.13109</link><description>&lt;p&gt;
&#26377;&#38480;CSI&#19979;&#30340;THz&#27874;&#26463;&#25628;&#32034;&#30340;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Deep Reinforcement Learning for THz-Beam Search with Limited CSI. (arXiv:2304.13109v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#26377;&#38480;CSI&#30340;&#24773;&#20917;&#19979;&#36805;&#36895;&#36827;&#34892;THz&#27874;&#26463;&#25628;&#32034;&#65292;&#20197;&#20811;&#26381;THz&#20449;&#21495;&#30340;&#20256;&#25773;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
THz&#36890;&#20449;&#26159;&#19979;&#19968;&#20195;&#26080;&#32447;&#32593;&#32476;&#20013;&#39640;&#25968;&#25454;&#29575;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#65292;&#28982;&#32780;&#20854;&#20005;&#37325;&#30340;&#20256;&#25773;&#34928;&#20943;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;FDRL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36816;&#33829;&#21830;&#32593;&#32476;&#20013;&#65292;&#30001;&#36793;&#32536;&#26381;&#21153;&#22120;&#21327;&#35843;&#30340;&#22810;&#20010;&#22522;&#31449;&#65288;BS&#65289;&#36805;&#36895;&#25191;&#34892;THz&#27874;&#26463;&#25628;&#32034;&#12290;&#25152;&#26377;BS&#37117;&#36827;&#34892;&#22522;&#20110;DDPG&#65288;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65289;&#30340;DRL&#20197;&#33719;&#24471;&#20855;&#26377;&#26377;&#38480;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;THz&#27874;&#26463;&#25104;&#24418;&#31574;&#30053;&#12290;&#20182;&#20204;&#20351;&#29992;&#38544;&#34255;&#20449;&#24687;&#26356;&#26032;&#20182;&#20204;&#30340;DDPG&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#36328;&#23567;&#21306;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
Terahertz (THz) communication with ultra-wide available spectrum is a promising technique that can achieve the stringent requirement of high data rate in the next-generation wireless networks, yet its severe propagation attenuation significantly hinders its implementation in practice. Finding beam directions for a large-scale antenna array to effectively overcome severe propagation attenuation of THz signals is a pressing need. This paper proposes a novel approach of federated deep reinforcement learning (FDRL) to swiftly perform THz-beam search for multiple base stations (BSs) coordinated by an edge server in a cellular network. All the BSs conduct deep deterministic policy gradient (DDPG)-based DRL to obtain THz beamforming policy with limited channel state information (CSI). They update their DDPG models with hidden information in order to mitigate inter-cell interference. We demonstrate that the cell network can achieve higher throughput as more THz CSI and hidden neurons of DDPG a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#25552;&#21462;&#20154;&#20307;&#31227;&#21160;&#21644;&#31354;&#38388;&#29305;&#24449;&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#26102;&#38388;-selective&#29305;&#24449;&#25552;&#21462;&#31639;&#27861;&#21306;&#20998;&#26377;&#30452;&#35273;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#21644;&#26080;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.13107</link><description>&lt;p&gt;
&#22522;&#20110;WiFi CSI&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#30340;&#26102;&#38388;&#36873;&#25321;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Time-Selective RNN for Device-Free Multi-Room Human Presence Detection Using WiFi CSI. (arXiv:2304.13107v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13107
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#25552;&#21462;&#20154;&#20307;&#31227;&#21160;&#21644;&#31354;&#38388;&#29305;&#24449;&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#26102;&#38388;-selective&#29305;&#24449;&#25552;&#21462;&#31639;&#27861;&#21306;&#20998;&#26377;&#30452;&#35273;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#21644;&#26080;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23384;&#22312;&#26816;&#27979;&#26159;&#21508;&#31181;&#24212;&#29992;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#21253;&#25324;&#23478;&#23621;&#33258;&#21160;&#21270;&#12289;&#23433;&#20840;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#37319;&#29992;&#22522;&#20110;&#25668;&#20687;&#26426;&#30340;&#31995;&#32479;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#20294;&#20250;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#21830;&#29992;WiFi&#25509;&#20837;&#28857;&#25552;&#20379;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;(CSI)&#26041;&#27861;&#65292;&#25552;&#20379;&#35814;&#32454;&#30340;&#20449;&#36947;&#29305;&#24449;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#36873;&#25321;&#24615;&#26465;&#20214;&#21452;&#29305;&#24449;&#25552;&#21462;&#36882;&#24402;&#32593;&#32476;(TCD-FERN)&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26088;&#22312;&#20351;&#29992;&#21160;&#24577;&#21644;&#38745;&#24577;(DaS)&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#22312;&#26465;&#20214;&#20154;&#20307;&#29305;&#24449;&#19979;&#25429;&#25417;&#37325;&#35201;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#25552;&#21462;&#20154;&#30340;&#31227;&#21160;&#21644;&#31354;&#38388;&#29305;&#24449;&#65292;&#24182;&#21306;&#20998;&#26377;&#30452;&#25509;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#21644;&#26080;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#20943;&#23569;&#25151;&#38388;&#38548;&#26029;&#36896;&#25104;&#30340;&#29305;&#24449;&#34928;&#20943;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110; LSTM &#30340; NCoV-DaS &#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human presence detection is a crucial technology for various applications, including home automation, security, and healthcare. While camera-based systems have traditionally been used for this purpose, they raise privacy concerns. To address this issue, recent research has explored the use of channel state information (CSI) approaches that can be extracted from commercial WiFi access points (APs) and provide detailed channel characteristics. In this thesis, we propose a device-free human presence detection system for multi-room scenarios using a time-selective conditional dual feature extract recurrent Network (TCD-FERN). Our system is designed to capture significant time features with the condition on current human features using a dynamic and static (DaS) data preprocessing technique to extract moving and spatial features of people and differentiate between line-of-sight (LoS) path blocking and non-blocking cases. To mitigate the feature attenuation problem caused by room partitions,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#20851;&#27880;&#26426;&#21046;&#21644;&#21452;&#21521;LSTM&#32593;&#32476;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13105</link><description>&lt;p&gt;
&#21033;&#29992;&#23460;&#20869;WiFi&#31995;&#32479;&#36827;&#34892;&#26080;&#35774;&#22791;&#31359;&#22681;&#23384;&#22312;&#26816;&#27979;&#30340;&#27880;&#24847;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System. (arXiv:2304.13105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#20851;&#27880;&#26426;&#21046;&#21644;&#21452;&#21521;LSTM&#32593;&#32476;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20934;&#30830;&#26816;&#27979;&#20154;&#21592;&#23384;&#22312;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#33021;&#28304;&#31649;&#29702;&#21644;&#23433;&#20840;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#30340;&#36890;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21517;&#20026;&#27880;&#24847;&#21147;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#65288;ALPD&#65289;&#65292;&#37319;&#29992;&#20851;&#27880;&#26426;&#21046;&#20174;CSI&#25968;&#25454;&#20013;&#33258;&#21160;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#23376;&#36733;&#27874;&#65292;&#24182;&#37319;&#29992;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#25429;&#25417;CSI&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#38745;&#24577;&#29305;&#24449;&#26469;&#25552;&#39640;&#38745;&#24577;&#29366;&#24577;&#19979;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#37096;&#32626;&#19968;&#23545;WiFi&#25509;&#20837;&#28857;&#65288;AP&#65289;&#26469;&#25910;&#38598;CSI&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;ALPD&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36827;&#19968;&#27493;&#19982;&#20960;&#20010;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ALPD&#31995;&#32479;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#21452;&#21521;&#20256;&#36755;&#25968;&#25454;&#19981;&#20250;&#24433;&#21709;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate detection of human presence in indoor environments is important for various applications, such as energy management and security. In this paper, we propose a novel system for human presence detection using the channel state information (CSI) of WiFi signals. Our system named attention-enhanced deep learning for presence detection (ALPD) employs an attention mechanism to automatically select informative subcarriers from the CSI data and a bidirectional long short-term memory (LSTM) network to capture temporal dependencies in CSI. Additionally, we utilize a static feature to improve the accuracy of human presence detection in static states. We evaluate the proposed ALPD system by deploying a pair of WiFi access points (APs) for collecting CSI dataset, which is further compared with several benchmarks. The results demonstrate that our ALPD system outperforms the benchmarks in terms of accuracy, especially in the presence of interference. Moreover, bidirectional transmission data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LSTM&#31070;&#32463;&#32593;&#32476;&#22312;&#24494;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#20013;&#21463;&#21040;&#22122;&#22768;&#27880;&#20837;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#20302;&#36890;&#28388;&#27874;&#22120;&#28040;&#38500;&#20102;&#25915;&#20987;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13104</link><description>&lt;p&gt;
&#22522;&#20110;LSTM&#30340;&#24494;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#23545;&#25239;&#27880;&#20837;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LSTM-based Load Forecasting Robustness Against Noise Injection Attack in Microgrid. (arXiv:2304.13104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LSTM&#31070;&#32463;&#32593;&#32476;&#22312;&#24494;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#20013;&#21463;&#21040;&#22122;&#22768;&#27880;&#20837;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#20302;&#36890;&#28388;&#27874;&#22120;&#28040;&#38500;&#20102;&#25915;&#20987;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LSTM&#31070;&#32463;&#32593;&#32476;&#22312;&#29702;&#24819;&#24494;&#30005;&#32593;&#20013;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#20013;&#23545;&#25239;&#27880;&#20837;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#20449;&#22122;&#27604;&#19979;&#36827;&#34892;&#30340;&#40657;&#30418;&#39640;&#26031;&#22122;&#22768;&#25915;&#20987;&#19979;LSTM&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20551;&#35774;&#25915;&#20987;&#32773;&#21482;&#33021;&#35775;&#38382;LSTM&#27169;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22122;&#22768;&#25915;&#20987;&#24433;&#21709;&#20102;LSTM&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#27491;&#24120;&#39044;&#27979;&#65292;&#36127;&#33655;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;0.047 MW&#65292;&#32780;&#23545;&#20110;&#20449;&#22122;&#27604;&#20026;6 dB&#30340;&#39640;&#26031;&#22122;&#22768;&#25554;&#20837;&#65292;&#35813;&#20540;&#22686;&#21152;&#21040;&#20102;0.097 MW&#12290;&#20026;&#20102;&#20351;LSTM&#27169;&#22411;&#23545;&#22122;&#22768;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#23558;&#26368;&#20339;&#25130;&#27490;&#39057;&#29575;&#30340;&#20302;&#36890;&#28388;&#27874;&#22120;&#24212;&#29992;&#20110;&#27169;&#22411;&#30340;&#36755;&#20837;&#20197;&#28040;&#38500;&#22122;&#22768;&#25915;&#20987;&#12290;&#35813;&#28388;&#27874;&#22120;&#22312;&#20302;&#20449;&#22122;&#27604;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#23545;&#20110;&#23567;&#22122;&#22768;&#30340;&#25928;&#26524;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the robustness of an LSTM neural network against noise injection attacks for electric load forecasting in an ideal microgrid. The performance of the LSTM model is investigated under a black-box Gaussian noise attack with different SNRs. It is assumed that attackers have just access to the input data of the LSTM model. The results show that the noise attack affects the performance of the LSTM model. The load prediction means absolute error (MAE) is 0.047 MW for a healthy prediction, while this value increases up to 0.097 MW for a Gaussian noise insertion with SNR= 6 dB. To robustify the LSTM model against noise attack, a low-pass filter with optimal cut-off frequency is applied at the model's input to remove the noise attack. The filter performs better in case of noise with lower SNR and is less promising for small noises.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20013;&#24515;&#26680;&#23545;&#40784;&#20998;&#26512;&#20102;&#20351;&#29992;&#26367;&#20195;&#26799;&#24230;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#20043;&#38388;&#30340;&#34920;&#31034;&#30456;&#20284;&#24615;&#65292;&#24182;&#21457;&#29616;SNN&#20013;&#30340;&#26102;&#38388;&#32500;&#24230;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.13098</link><description>&lt;p&gt;
&#29992;&#26367;&#20195;&#26799;&#24230;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#23398;&#20064;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Uncovering the Representation of Spiking Neural Networks Trained with Surrogate Gradient. (arXiv:2304.13098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20013;&#24515;&#26680;&#23545;&#40784;&#20998;&#26512;&#20102;&#20351;&#29992;&#26367;&#20195;&#26799;&#24230;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#20043;&#38388;&#30340;&#34920;&#31034;&#30456;&#20284;&#24615;&#65292;&#24182;&#21457;&#29616;SNN&#20013;&#30340;&#26102;&#38388;&#32500;&#24230;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30001;&#20110;&#20854;&#31867;&#29983;&#29289;&#29305;&#24615;&#21644;&#33021;&#37327;&#25928;&#29575;&#32780;&#34987;&#35748;&#20026;&#26159;&#19979;&#19968;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#20505;&#36873;&#32773;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#26367;&#20195;&#26799;&#24230;&#35757;&#32451;&#65292;SNN&#33021;&#22815;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#23454;&#29616;&#25509;&#36817;&#20110;&#26368;&#20808;&#36827;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;SNN&#30340;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#22914;&#65306;&#20351;&#29992;&#26367;&#20195;&#26799;&#24230;&#35757;&#32451;&#30340;SNN&#26159;&#21542;&#23398;&#20064;&#20102;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#34920;&#31034;&#23398;&#20064;&#65311;SNN&#20013;&#30340;&#26102;&#38388;&#32500;&#24230;&#26159;&#21542;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65311;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#20013;&#24515;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#36827;&#34892;SNN&#21644;ANN&#20043;&#38388;&#30340;&#34920;&#31034;&#30456;&#20284;&#24615;&#20998;&#26512;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#32593;&#32476;&#30340;&#31354;&#38388;&#32500;&#24230;&#65292;&#21253;&#25324;&#23485;&#24230;&#21644;&#28145;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;SNN&#23398;&#20064;&#20102;&#21608;&#26399;&#27169;&#24335;&#30340;&#27531;&#24046;&#36830;&#25509;&#65292;&#20174;&#32780;&#20351;SNN&#30340;&#34920;&#31034;&#31867;&#20284;&#20110;ANN&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;SNN&#20013;&#30340;&#26102;&#38388;&#32500;&#24230;&#25552;&#20379;&#20102;&#22312;ANN&#20013;&#19981;&#23384;&#22312;&#30340;&#29420;&#29305;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) are recognized as the candidate for the next-generation neural networks due to their bio-plausibility and energy efficiency. Recently, researchers have demonstrated that SNNs are able to achieve nearly state-of-the-art performance in image recognition tasks using surrogate gradient training. However, some essential questions exist pertaining to SNNs that are little studied: Do SNNs trained with surrogate gradient learn different representations from traditional Artificial Neural Networks (ANNs)? Does the time dimension in SNNs provide unique representation power? In this paper, we aim to answer these questions by conducting a representation similarity analysis between SNNs and ANNs using Centered Kernel Alignment (CKA). We start by analyzing the spatial dimension of the networks, including both the width and the depth. Furthermore, our analysis of residual connections shows that SNNs learn a periodic pattern, which rectifies the representations in SNNs to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HDRMAX&#29305;&#24449;&#38598;&#65292;&#21487;&#20197;&#25552;&#39640;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#31639;&#27861;&#23545;&#20110;&#19981;&#21516;&#20301;&#28145;&#35270;&#39057;&#30340;&#36136;&#37327;&#39044;&#27979;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;HDR&#35270;&#39057;&#19978;&#34920;&#29616;&#23588;&#20339;&#12290;</title><link>http://arxiv.org/abs/2304.13092</link><description>&lt;p&gt;
&#20351;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#23545;&#20301;&#28145;&#20855;&#26377;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Making Video Quality Assessment Models Robust to Bit Depth. (arXiv:2304.13092v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HDRMAX&#29305;&#24449;&#38598;&#65292;&#21487;&#20197;&#25552;&#39640;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#31639;&#27861;&#23545;&#20110;&#19981;&#21516;&#20301;&#28145;&#35270;&#39057;&#30340;&#36136;&#37327;&#39044;&#27979;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;HDR&#35270;&#39057;&#19978;&#34920;&#29616;&#23588;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#38598;&#65292;&#31216;&#20026;HDRMAX&#29305;&#24449;&#12290;&#24403;&#23558;&#20854;&#21253;&#21547;&#22312;&#20026;&#26631;&#20934;&#21160;&#24577;&#33539;&#22260;&#65288;SDR&#65289;&#35270;&#39057;&#35774;&#35745;&#30340;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;VQA&#65289;&#31639;&#27861;&#20013;&#26102;&#65292;&#23427;&#20204;&#20250;&#23545;&#39640;&#21160;&#24577;&#33539;&#22260;&#65288;HDR&#65289;&#35270;&#39057;&#30340;&#22833;&#30495;&#36827;&#34892;&#25935;&#24863;&#22788;&#29702;&#65292;&#36825;&#20123;&#22833;&#30495;&#26410;&#34987;&#36825;&#20123;&#31639;&#27861;&#20805;&#20998;&#32771;&#34385;&#12290;&#34429;&#28982;&#36825;&#20123;&#29305;&#24449;&#19981;&#20165;&#38480;&#20110;HDR&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#22686;&#24378;VQA&#27169;&#22411;&#23545;SDR&#20869;&#23481;&#30340;&#36136;&#37327;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;HDR&#19978;&#23588;&#20026;&#26377;&#25928;&#12290;HDRMAX&#29305;&#24449;&#36890;&#36807;&#22686;&#24378;&#33258;&#28982;&#35270;&#39057;&#32479;&#35745;&#27169;&#22411;&#65288;NVS&#65289;&#20013;&#20174;&#35270;&#35273;&#24433;&#21709;&#35270;&#39057;&#30340;&#26368;&#20142;&#21644;&#26368;&#26263;&#30340;&#23616;&#37096;&#20013;&#25552;&#21462;&#30340;&#24378;&#22823;&#20808;&#39564;&#20449;&#24687;&#30340;&#21487;&#27979;&#24615;&#26469;&#25429;&#25417;&#36890;&#24120;&#30001;&#29616;&#26377;VQA&#27169;&#22411;&#38590;&#20197;&#32771;&#34385;&#30340;&#22833;&#30495;&#12290;&#20316;&#20026;&#25105;&#20204;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#35777;&#26126;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;10&#20301;HDR&#25968;&#25454;&#24211;&#19978;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;VQA&#27169;&#22411;&#30340;&#34920;&#29616;&#24456;&#24046;&#65292;&#20294;&#24403;&#20351;&#29992;HDRMAX&#29305;&#24449;&#26102;&#65292;&#20854;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel feature set, which we call HDRMAX features, that when included into Video Quality Assessment (VQA) algorithms designed for Standard Dynamic Range (SDR) videos, sensitizes them to distortions of High Dynamic Range (HDR) videos that are inadequately accounted for by these algorithms. While these features are not specific to HDR, and also augment the equality prediction performances of VQA models on SDR content, they are especially effective on HDR. HDRMAX features modify powerful priors drawn from Natural Video Statistics (NVS) models by enhancing their measurability where they visually impact the brightest and darkest local portions of videos, thereby capturing distortions that are often poorly accounted for by existing VQA models. As a demonstration of the efficacy of our approach, we show that, while current state-of-the-art VQA models perform poorly on 10-bit HDR databases, their performances are greatly improved by the inclusion of HDRMAX features when tested on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#20013;&#30340;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#20391;&#20449;&#36947;&#20449;&#24687;&#36827;&#34892;&#20272;&#35745;&#20505;&#36873;&#39033;&#30340;&#35782;&#21035;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#36873;&#25321;&#26368;&#20339;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.13090</link><description>&lt;p&gt;
&#38754;&#21521;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#30340;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Model Extraction Attacks Against Reinforcement Learning Based Controllers. (arXiv:2304.13090v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#20013;&#30340;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#20391;&#20449;&#36947;&#20449;&#24687;&#36827;&#34892;&#20272;&#35745;&#20505;&#36873;&#39033;&#30340;&#35782;&#21035;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#36873;&#25321;&#26368;&#20339;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32593;&#32476;-&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#38382;&#39064;&#65292;&#25915;&#20987;&#32773;&#35797;&#22270;&#20272;&#35745;&#25110;&#33719;&#21462;&#31995;&#32479;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;&#25552;&#21462;&#25110;&#20272;&#35745;&#25511;&#21046;&#22120;&#21487;&#20197;&#32473;&#25915;&#20987;&#32773;&#24102;&#26469;&#26080;&#19982;&#20262;&#27604;&#30340;&#20248;&#21183;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#20182;&#20204;&#39044;&#27979;&#31995;&#32479;&#30340;&#26410;&#26469;&#25511;&#21046;&#21160;&#20316;&#24182;&#30456;&#24212;&#22320;&#35745;&#21010;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#25915;&#20987;&#32773;&#25191;&#34892;&#27492;&#31867;&#25915;&#20987;&#30340;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#24182;&#29992;&#20110;&#25511;&#21046;&#38543;&#26426;&#31995;&#32479;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25198;&#28436;&#25915;&#20987;&#32773;&#30340;&#35282;&#33394;&#65292;&#26088;&#22312;&#20272;&#35745;&#36825;&#31181;&#26410;&#30693;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#21363;&#31163;&#32447;&#38454;&#27573;&#65292;&#25915;&#20987;&#32773;&#20351;&#29992;&#20851;&#20110;RL&#22870;&#21169;&#20989;&#25968;&#21644;&#31995;&#32479;&#21160;&#24577;&#30340;&#20391;&#20449;&#36947;&#20449;&#24687;&#26469;&#35782;&#21035;&#19968;&#32452;&#20505;&#36873;&#30340;&#26410;&#30693;DNN&#20272;&#35745;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#20063;&#31216;&#20026;&#22312;&#32447;&#38454;&#27573;&#65292;&#25915;&#20987;&#32773;&#20351;&#29992;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#36873;&#25321;&#26368;&#20339;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the problem of model-extraction attacks in cyber-physical systems in which an attacker attempts to estimate (or extract) the feedback controller of the system. Extracting (or estimating) the controller provides an unmatched edge to attackers since it allows them to predict the future control actions of the system and plan their attack accordingly. Hence, it is important to understand the ability of the attackers to perform such an attack. In this paper, we focus on the setting when a Deep Neural Network (DNN) controller is trained using Reinforcement Learning (RL) algorithms and is used to control a stochastic system. We play the role of the attacker that aims to estimate such an unknown DNN controller, and we propose a two-phase algorithm. In the first phase, also called the offline phase, the attacker uses side-channel information about the RL-reward function and the system dynamics to identify a set of candidate estimates of the unknown DNN. In the second phase, also ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#21644;&#37325;&#24314;&#23398;&#20064;&#20004;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#35270;&#35273;transformers&#30340;&#30446;&#26631;&#23545;&#25152;&#23398;&#34920;&#31034;&#30340;&#24433;&#21709;&#21450;&#20854;&#36716;&#25442;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#29305;&#24449;&#26356;&#21033;&#20110;&#32447;&#24615;&#25506;&#27979;&#36716;&#31227;&#20998;&#31867;&#65292;&#36827;&#32780;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.13089</link><description>&lt;p&gt;
&#30446;&#26631;&#24456;&#37325;&#35201;&#65306;&#29702;&#35299;&#33258;&#30417;&#30563;&#30446;&#26631;&#23545;&#35270;&#35273;Transformer&#34920;&#31034;&#24418;&#24335;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations. (arXiv:2304.13089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#21644;&#37325;&#24314;&#23398;&#20064;&#20004;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#35270;&#35273;transformers&#30340;&#30446;&#26631;&#23545;&#25152;&#23398;&#34920;&#31034;&#30340;&#24433;&#21709;&#21450;&#20854;&#36716;&#25442;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#29305;&#24449;&#26356;&#21033;&#20110;&#32447;&#24615;&#25506;&#27979;&#36716;&#31227;&#20998;&#31867;&#65292;&#36827;&#32780;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;(SimCLR&#12289;MoCo&#12289;DINO&#31561;)&#21644;&#37325;&#24314;&#23398;&#20064;(BEiT&#12289;SimMIM&#12289;MAE&#31561;)&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#35270;&#35273;transformers&#30340;&#20004;&#31181;&#20027;&#35201;&#33539;&#20363;&#65292;&#20294;&#23427;&#20204;&#22312;&#36716;&#25442;&#24615;&#33021;&#19978;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#36825;&#20123;&#30446;&#26631;&#23545;&#25152;&#23398;&#34920;&#31034;&#30340;&#32467;&#26500;&#21644;&#21487;&#36716;&#31227;&#24615;&#30340;&#24433;&#21709;&#26469;&#35299;&#37322;&#36825;&#20123;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#37325;&#24314;&#23398;&#20064;&#29305;&#24449;&#19982;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#29305;&#24449;&#30456;&#27604;&#36739;&#26174;&#33879;&#30340;&#19981;&#21516;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#19981;&#21516;&#26550;&#26500;&#19979;&#20063;&#33021;&#36890;&#36807;&#31867;&#20284;&#30446;&#26631;&#26469;&#35757;&#32451;&#12290;&#36825;&#20123;&#24046;&#24322;&#26089;&#22312;&#32593;&#32476;&#30340;&#26089;&#26399;&#23601;&#20135;&#29983;&#20102;&#65292;&#24182;&#19988;&#20027;&#35201;&#21463;&#21040;&#27880;&#24847;&#21147;&#21644;&#24402;&#19968;&#21270;&#23618;&#30340;&#39537;&#21160;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32852;&#21512;&#23884;&#20837;&#29305;&#24449;&#20135;&#29983;&#26356;&#22909;&#30340;&#32447;&#24615;&#25506;&#27979;&#36716;&#31227;&#20998;&#31867;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#30446;&#26631;&#39537;&#21160;&#19981;&#21516;&#30340;&#20449;&#24687;&#20998;&#24067;&#21644;&#19981;&#21464;&#24615;&#22312;&#25152;&#23398;&#34920;&#31034;&#20013;&#12290;&#26412;&#25991;&#20998;&#26512;&#25552;&#20379;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#35774;&#35745;&#30340;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint-embedding based learning (e.g., SimCLR, MoCo, DINO) and reconstruction-based learning (e.g., BEiT, SimMIM, MAE) are the two leading paradigms for self-supervised learning of vision transformers, but they differ substantially in their transfer performance. Here, we aim to explain these differences by analyzing the impact of these objectives on the structure and transferability of the learned representations. Our analysis reveals that reconstruction-based learning features are significantly dissimilar to joint-embedding based learning features and that models trained with similar objectives learn similar features even across architectures. These differences arise early in the network and are primarily driven by attention and normalization layers. We find that joint-embedding features yield better linear probe transfer for classification because the different objectives drive different distributions of information and invariances in the learned representation. These differences expl
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#19982;&#32654;&#22269;&#20027;&#35201;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#30340;&#39046;&#23548;&#20154;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#20027;&#35201;&#30693;&#24773;&#20154;&#21512;&#20316;&#65292;&#21046;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#32452;&#32455;&#27835;&#29702;&#26694;&#26550;&#65292;&#21253;&#25324;&#20851;&#38190;&#25511;&#21046;&#28857;&#21644;&#20915;&#31574;&#26631;&#20934;&#65292;&#20026;&#21355;&#29983;&#31995;&#32479;&#39046;&#23548;&#20154;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#31574;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.13081</link><description>&lt;p&gt;
&#26032;&#20852;&#25216;&#26415;&#30340;&#32452;&#32455;&#27835;&#29702;&#65306;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Organizational Governance of Emerging Technologies: AI Adoption in Healthcare. (arXiv:2304.13081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#19982;&#32654;&#22269;&#20027;&#35201;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#30340;&#39046;&#23548;&#20154;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#20027;&#35201;&#30693;&#24773;&#20154;&#21512;&#20316;&#65292;&#21046;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#32452;&#32455;&#27835;&#29702;&#26694;&#26550;&#65292;&#21253;&#25324;&#20851;&#38190;&#25511;&#21046;&#28857;&#21644;&#20915;&#31574;&#26631;&#20934;&#65292;&#20026;&#21355;&#29983;&#31995;&#32479;&#39046;&#23548;&#20154;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#31574;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#33829;&#21644;&#20844;&#20849;&#37096;&#38376;&#30340;&#32467;&#26500;&#21644;&#35268;&#33539;&#31934;&#32454;&#21270;&#20102;&#26032;&#20852;&#25216;&#26415;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#23613;&#31649;&#20986;&#29616;&#20102;&#22823;&#37327;&#30340;AI&#37319;&#29992;&#26041;&#24335;&#65292;&#20294;&#26159;&#20854;&#20351;&#29992;&#21644;&#25972;&#21512;&#21608;&#22260;&#30340;&#32452;&#32455;&#27835;&#29702;&#24448;&#24448;&#34987;&#35748;&#20026;&#19981;&#21487;&#34892;&#12290;&#20581;&#24247;AI&#21512;&#20316;&#20249;&#20276;&#20851;&#31995;&#65288;HAIP&#65289;&#26088;&#22312;&#36890;&#36807;&#27492;&#30740;&#31350;&#26356;&#22909;&#22320;&#23450;&#20041;&#21307;&#30103;&#20445;&#20581;&#20013;AI&#31995;&#32479;&#30340;&#20805;&#20998;&#32452;&#32455;&#27835;&#29702;&#35201;&#27714;&#65292;&#24182;&#25903;&#25345;&#21355;&#29983;&#31995;&#32479;&#39046;&#23548;&#20154;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#35201;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#37319;&#29992;&#30340;&#26631;&#20934;&#22914;&#20309;&#26131;&#20110;&#20351;&#29992;&#21644;&#39640;&#25928;&#36816;&#20316;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#29305;&#23450;&#30340;&#21355;&#29983;&#31995;&#32479;&#20013;&#65292;&#32472;&#21046;&#20986;&#23454;&#38469;&#26426;&#26500;&#37319;&#29992;AI&#25216;&#26415;&#30340;&#20855;&#20307;&#20915;&#31574;&#28857;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#32654;&#22269;&#20027;&#35201;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#30340;&#39046;&#23548;&#20154;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#20027;&#35201;&#30693;&#24773;&#20154;&#21512;&#20316;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#31181;&#21512;&#20316;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#32452;&#32455;&#27835;&#29702;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20851;&#38190;&#25511;&#21046;&#28857;&#21644;&#20915;&#31574;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#24191;&#20102; Hopkins field &#20998;&#23618;&#32593;&#32476;&#65292;&#24182;&#20171;&#32461;&#20102; iMixer&#65292;MLP-Mixer &#27169;&#22411;&#30340;&#26032;&#27010;&#25324;&#65292;&#19981;&#21516;&#20110;&#26222;&#36890;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;iMixer &#28041;&#21450;&#21040;&#20174;&#36755;&#20986;&#21040;&#36755;&#20837;&#30340;&#20256;&#25773;&#30340; MLP &#23618;&#65292;&#34987;&#29305;&#24449;&#21270;&#20026;&#19968;&#20010;&#21487;&#36870;&#12289;&#38544;&#24335;&#12289;&#36845;&#20195;&#30340; mixing block&#12290;</title><link>http://arxiv.org/abs/2304.13061</link><description>&lt;p&gt;
iMixer: &#20998;&#23618;Hopfield&#32593;&#32476;&#26263;&#31034;&#20102;&#21487;&#36870;&#12289;&#38544;&#24335;&#21644;&#36845;&#20195;&#30340;MLP-Mixer
&lt;/p&gt;
&lt;p&gt;
iMixer: hierarchical Hopfield network implies an invertible, implicit and iterative MLP-Mixer. (arXiv:2304.13061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#24191;&#20102; Hopkins field &#20998;&#23618;&#32593;&#32476;&#65292;&#24182;&#20171;&#32461;&#20102; iMixer&#65292;MLP-Mixer &#27169;&#22411;&#30340;&#26032;&#27010;&#25324;&#65292;&#19981;&#21516;&#20110;&#26222;&#36890;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;iMixer &#28041;&#21450;&#21040;&#20174;&#36755;&#20986;&#21040;&#36755;&#20837;&#30340;&#20256;&#25773;&#30340; MLP &#23618;&#65292;&#34987;&#29305;&#24449;&#21270;&#20026;&#19968;&#20010;&#21487;&#36870;&#12289;&#38544;&#24335;&#12289;&#36845;&#20195;&#30340; mixing block&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#25104;&#21151;&#20419;&#20351;&#23547;&#25214;&#21487;&#20197;&#19982;&#20043;&#31454;&#20105;&#30340;&#35768;&#22810;&#26367;&#20195;&#27169;&#22411;&#65292;&#22914;MLP-Mixer&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#30340;&#24341;&#20837;&#20559;&#24046;&#36739;&#24369;&#65292;&#20294;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#19982;&#30740;&#31350;&#36739;&#22810;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#23218;&#32654;&#12290;&#26368;&#36817;&#23545;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102;&#26576;&#20123;&#22522;&#20110;&#33021;&#37327;&#30340;&#20851;&#32852;&#35760;&#24518;&#27169;&#22411;&#19982;Transformer&#25110;MLP-Mixer&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25581;&#31034;&#20102;Transformer&#31867;&#22411;&#26550;&#26500;&#35774;&#35745;&#30340;&#29702;&#35770;&#32972;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#23545;&#24212;&#20851;&#31995;&#25512;&#24191;&#21040;&#26368;&#36817;&#24341;&#20837;&#30340;&#20998;&#23618;Hopfield&#32593;&#32476;&#65292;&#24182;&#25214;&#21040;&#20102;iMixer&#65292;&#36825;&#26159;MLP-Mixer&#27169;&#22411;&#30340;&#26032;&#30340;&#27010;&#25324;&#12290;&#19982;&#26222;&#36890;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;iMixer&#28041;&#21450;&#20174;&#36755;&#20986;&#20391;&#21521;&#36755;&#20837;&#20391;&#20256;&#25773;&#30340;MLP&#23618;&#12290;&#25105;&#20204;&#23558;&#35813;&#27169;&#22359;&#29305;&#24449;&#21270;&#20026;&#21487;&#36870;&#12289;&#38544;&#24335;&#21644;&#36845;&#20195;&#28151;&#21512;&#27169;&#22359;&#30340;&#19968;&#20010;&#20363;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#20219;&#21153;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, the success of Transformers in computer vision has stimulated the discovery of many alternative models that compete with Transformers, such as the MLP-Mixer. Despite their weak induced bias, these models have achieved performance comparable to well-studied convolutional neural networks. Recent studies on modern Hopfield networks suggest the correspondence between certain energy-based associative memory models and Transformers or MLP-Mixer, and shed some light on the theoretical background of the Transformer-type architectures design. In this paper we generalize the correspondence to the recently introduced hierarchical Hopfield network, and find iMixer, a novel generalization of MLP-Mixer model. Unlike ordinary feedforward neural networks, iMixer involves MLP layers that propagate forward from the output side to the input side. We characterize the module as an example of invertible, implicit, and iterative mixing module. We evaluate the model performance with var
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22826;&#38451;&#33021;&#39537;&#21160;&#26234;&#33021;&#22403;&#22334;&#20998;&#31867;&#26742;&#65292;&#24182;&#36890;&#36807;&#30701;&#20449;&#36890;&#30693;&#21644;&#26426;&#22120;&#23398;&#20064;&#22270;&#20687;&#22788;&#29702;&#21151;&#33021;&#23454;&#29616;&#24223;&#29289;&#20998;&#31867;&#65292;&#25552;&#39640;&#24223;&#29289;&#31649;&#29702;&#25928;&#29575;&#65292;&#21516;&#26102;&#20351;&#32456;&#31471;&#29992;&#25143;&#26356;&#21152;&#20851;&#27880;&#29615;&#20445;&#12290;</title><link>http://arxiv.org/abs/2304.13040</link><description>&lt;p&gt;
GULP: &#22826;&#38451;&#33021;&#39537;&#21160;&#30340;&#26234;&#33021;&#22403;&#22334;&#20998;&#31867;&#26742;&#65292;&#20855;&#22791;&#30701;&#20449;&#36890;&#30693;&#21644;&#26426;&#22120;&#23398;&#20064;&#22270;&#20687;&#22788;&#29702;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
GULP: Solar-Powered Smart Garbage Segregation Bins with SMS Notification and Machine Learning Image Processing. (arXiv:2304.13040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22826;&#38451;&#33021;&#39537;&#21160;&#26234;&#33021;&#22403;&#22334;&#20998;&#31867;&#26742;&#65292;&#24182;&#36890;&#36807;&#30701;&#20449;&#36890;&#30693;&#21644;&#26426;&#22120;&#23398;&#20064;&#22270;&#20687;&#22788;&#29702;&#21151;&#33021;&#23454;&#29616;&#24223;&#29289;&#20998;&#31867;&#65292;&#25552;&#39640;&#24223;&#29289;&#31649;&#29702;&#25928;&#29575;&#65292;&#21516;&#26102;&#20351;&#32456;&#31471;&#29992;&#25143;&#26356;&#21152;&#20851;&#27880;&#29615;&#20445;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#26234;&#33021;&#22403;&#22334;&#26742;&#65292;&#23558;&#22266;&#20307;&#24223;&#29289;&#20998;&#31867;&#21040;&#20854;&#30456;&#24212;&#30340;&#23481;&#22120;&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;&#21487;&#20877;&#29983;&#30340;&#22826;&#38451;&#33021;&#28304;&#65292;&#20351;&#32456;&#31471;&#29992;&#25143;&#23545;&#24223;&#29289;&#31649;&#29702;&#36807;&#31243;&#26356;&#21152;&#24863;&#20852;&#36259;&#65292;&#24403;&#26234;&#33021;&#22403;&#22334;&#26742;&#38656;&#35201;&#21368;&#36733;&#26102;&#36890;&#30693;&#30456;&#20851;&#24037;&#20316;&#20154;&#21592;&#65292;&#40723;&#21169;&#20351;&#29992;&#29615;&#22659;&#21451;&#22909;&#22411;&#26234;&#33021;&#22403;&#22334;&#26742;&#12290;&#30740;&#31350;&#20154;&#21592;&#37319;&#29992;&#25935;&#25463;&#24320;&#21457;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20351;&#22242;&#38431;&#25104;&#21151;&#31649;&#29702;&#24037;&#20316;&#37327;&#65292;&#21019;&#24314;&#26368;&#39640;&#36136;&#37327;&#30340;&#20135;&#21697;&#65292;&#24182;&#22312;&#20998;&#37197;&#39044;&#31639;&#20869;&#23436;&#25104;&#24037;&#20316;&#12290;&#20845;&#20010;&#22522;&#26412;&#38454;&#27573;&#26159;&#35268;&#21010;&#12289;&#35774;&#35745;&#12289;&#24320;&#21457;&#12289;&#27979;&#35797;&#12289;&#21457;&#24067;&#21644;&#21453;&#39304;&#12290;&#36890;&#36807;ISO/IEC 25010&#35780;&#20272;&#25552;&#20379;&#30340;&#24635;&#20307;&#36136;&#37327;&#27979;&#35797;&#32467;&#26524;&#65292;&#24471;&#20986;&#31215;&#26497;&#30340;&#32467;&#35770;&#12290;&#25972;&#20307;&#24179;&#22343;&#20540;&#20026;4.55&#65292;&#21475;&#22836;&#35299;&#37322;&#20026;&#20248;&#31168;&#12290;&#27492;&#22806;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#36824;&#21487;&#20197;&#21033;&#29992;&#22826;&#38451;&#33021;&#28304;&#29420;&#31435;&#36816;&#34892;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#20854;&#26377;&#36259;&#30340;&#26426;&#21046;&#20139;&#21463;&#25972;&#20010;&#24223;&#29289;&#22788;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study intends to build a smartbin that segregates solid waste into its respective bins. To make the waste management process more interesting for the end-users; to notify the utility staff when the smart bin needs to be unloaded; to encourage an environment-friendly smart bin by utilizing renewable solar energy source. The researchers employed an Agile Development approach because it enables teams to manage their workloads successfully and create the highest-quality product while staying within their allocated budget. The six fundamental phases are planning, design, development, test, release, and feedback. The Overall quality testing result that was provided through the ISO/IEC 25010 evaluation which concludes a positive outcome. The overall average was 4.55, which is verbally interpreted as excellent. Additionally, the application can also independently run with its solar energy source. Users were able to enjoy the whole process of waste disposal through its interesting mechanis
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#26641;&#33683;&#27966;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#20462;&#21098;&#25216;&#26415;&#21644;&#27169;&#22411;&#21442;&#25968;&#32467;&#26500;&#20248;&#21270;&#65292;&#20197;&#36866;&#24212;&#20854;&#30828;&#20214;&#29305;&#28857;&#24182;&#25552;&#39640;&#33021;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.13039</link><description>&lt;p&gt;
&#38024;&#23545;&#26641;&#33683;&#27966;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Optimizing Deep Learning Models For Raspberry Pi. (arXiv:2304.13039v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13039
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26641;&#33683;&#27966;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#20462;&#21098;&#25216;&#26415;&#21644;&#27169;&#22411;&#21442;&#25968;&#32467;&#26500;&#20248;&#21270;&#65292;&#20197;&#36866;&#24212;&#20854;&#30828;&#20214;&#29305;&#28857;&#24182;&#25552;&#39640;&#33021;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20294;&#36825;&#31867;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#20351;&#24471;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#36816;&#34892;&#65288;&#22914;&#26641;&#33683;&#27966;&#65289;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20462;&#21098;&#65288;pruning&#65289;&#25216;&#26415;&#21644;&#20248;&#21270;&#27169;&#22411;&#20197;&#36866;&#21512;&#26641;&#33683;&#27966;&#31561;&#30828;&#20214;&#26550;&#26500;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#25110;&#35757;&#32451;&#21518;&#20462;&#21098;&#21487;&#20197;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#35753;&#27169;&#22411;&#26356;&#39640;&#25928;&#12290;&#20248;&#21270;&#27169;&#22411;&#21017;&#21253;&#25324;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#21644;&#32467;&#26500;&#36866;&#24212;&#26641;&#33683;&#27966;&#30828;&#20214;&#29305;&#28857;&#65292;&#20363;&#22914;&#26641;&#33683;&#27966;&#30340;CPU&#21644;GPU&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#35745;&#31639;&#37327;&#20197;&#23454;&#29616;&#33021;&#32791;&#30340;&#20248;&#21270;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have become increasingly popular for a wide range of applications, including computer vision, natural language processing, and speech recognition. However, these models typically require large amounts of computational resources, making them challenging to run on low-power devices such as the Raspberry Pi. One approach to addressing this challenge is to use pruning techniques to reduce the size of the deep learning models. Pruning involves removing unimportant weights and connections from the model, resulting in a smaller and more efficient model. Pruning can be done during training or after the model has been trained. Another approach is to optimize the deep learning models specifically for the Raspberry Pi architecture. This can include optimizing the model's architecture and parameters to take advantage of the Raspberry Pi's hardware capabilities, such as its CPU and GPU. Additionally, the model can be optimized for energy efficiency by minimizing the amount of c
&lt;/p&gt;</description></item><item><title>VeML&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#29256;&#26412;&#31649;&#29702;&#31995;&#32479;&#65292;&#22312;&#35299;&#20915;&#29983;&#21629;&#21608;&#26399;&#39640;&#25104;&#26412;&#38382;&#39064;&#12289;&#25968;&#25454;&#30456;&#20284;&#24615;&#35745;&#31639;&#21644;&#25968;&#25454;&#27169;&#24335;&#20998;&#26512;&#31561;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.13037</link><description>&lt;p&gt;
VeML&#65306;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;
&lt;/p&gt;
&lt;p&gt;
VeML: An End-to-End Machine Learning Lifecycle for Large-scale and High-dimensional Data. (arXiv:2304.13037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13037
&lt;/p&gt;
&lt;p&gt;
VeML&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#29256;&#26412;&#31649;&#29702;&#31995;&#32479;&#65292;&#22312;&#35299;&#20915;&#29983;&#21629;&#21608;&#26399;&#39640;&#25104;&#26412;&#38382;&#39064;&#12289;&#25968;&#25454;&#30456;&#20284;&#24615;&#35745;&#31639;&#21644;&#25968;&#25454;&#27169;&#24335;&#20998;&#26512;&#31561;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21253;&#21547;&#35768;&#22810;&#36845;&#20195;&#36807;&#31243;&#65292;&#20174;&#25968;&#25454;&#20934;&#22791;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#21040;&#27169;&#22411;&#35757;&#32451;&#65292;&#20877;&#21040;&#37096;&#32626;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#29992;&#20110;&#25512;&#29702;&#12290;&#24403;&#26500;&#24314;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#31471;&#21040;&#31471;&#29983;&#21629;&#21608;&#26399;&#26102;&#65292;&#24517;&#39035;&#35774;&#35745;&#21644;&#25191;&#34892;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#36825;&#20250;&#20135;&#29983;&#22823;&#37327;&#30340;&#29983;&#21629;&#21608;&#26399;&#29256;&#26412;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;VeML&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#29256;&#26412;&#31649;&#29702;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#35299;&#20915;&#20102;&#20854;&#20182;&#31995;&#32479;&#27809;&#26377;&#35299;&#20915;&#30340;&#20960;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#39640;&#25104;&#26412;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#35758;&#23558;&#22312;&#25105;&#20204;&#31995;&#32479;&#20013;&#31649;&#29702;&#30340;&#31867;&#20284;&#25968;&#25454;&#38598;&#30340;&#29983;&#21629;&#21608;&#26399;&#36716;&#31227;&#21040;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#24515;&#38598;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#12290;&#21478;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#24046;&#24322;&#32780;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#27169;&#24335;&#20998;&#26512;&#26041;&#27861;&#26469;&#26816;&#27979;&#20808;&#21069;&#20351;&#29992;&#30340;&#25968;&#25454;&#21644;&#26032;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20351;&#29992;&#25143;&#21487;&#20197;&#33258;&#23450;&#20041;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#24037;&#20316;&#27969;&#65292;&#24182;&#23558;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#19982;&#20854;API&#36830;&#25509;&#36215;&#26469;&#65292;&#20316;&#20026;&#29992;&#25143;&#36816;&#34892;&#33258;&#23450;&#20041;&#20195;&#30721;&#30340;&#26725;&#26753;&#12290; VeML&#24050;&#24212;&#29992;&#20110;&#22788;&#29702;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An end-to-end machine learning (ML) lifecycle consists of many iterative processes, from data preparation and ML model design to model training and then deploying the trained model for inference. When building an end-to-end lifecycle for an ML problem, many ML pipelines must be designed and executed that produce a huge number of lifecycle versions. Therefore, this paper introduces VeML, a Version management system dedicated to end-to-end ML Lifecycle. Our system tackles several crucial problems that other systems have not solved. First, we address the high cost of building an ML lifecycle, especially for large-scale and high-dimensional dataset. We solve this problem by proposing to transfer the lifecycle of similar datasets managed in our system to the new training data. We design an algorithm based on the core set to compute similarity for large-scale, high-dimensional data efficiently. Another critical issue is the model accuracy degradation by the difference between training data a
&lt;/p&gt;</description></item><item><title>SmartChoices &#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#29616;&#26377;&#36719;&#20214;&#31995;&#32479;&#36731;&#26494;&#12289;&#23433;&#20840;&#12289;&#26377;&#25928;&#22320;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13033</link><description>&lt;p&gt;
SmartChoices: &#23398;&#20064;&#23454;&#29616;&#22686;&#24378;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
SmartChoices: Augmenting Software with Learned Implementations. (arXiv:2304.13033v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13033
&lt;/p&gt;
&lt;p&gt;
SmartChoices &#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#29616;&#26377;&#36719;&#20214;&#31995;&#32479;&#36731;&#26494;&#12289;&#23433;&#20840;&#12289;&#26377;&#25928;&#22320;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22788;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#40644;&#37329;&#26102;&#20195;&#12290;&#24378;&#22823;&#30340;&#27169;&#22411;&#27491;&#22312;&#35757;&#32451;&#20013;&#65292;&#36828;&#27604;&#20165;&#20351;&#29992;&#20256;&#32479;&#36719;&#20214;&#24037;&#31243;&#26041;&#27861;&#26356;&#22909;&#22320;&#25191;&#34892;&#35768;&#22810;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#24320;&#21457;&#24182;&#37096;&#32626;&#21040;&#29616;&#26377;&#36719;&#20214;&#31995;&#32479;&#20013;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SmartChoices&#65292;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#36731;&#26494;&#12289;&#23433;&#20840;&#12289;&#26377;&#25928;&#22320;&#32467;&#21512;&#21040;&#25104;&#29087;&#36719;&#20214;&#22534;&#26632;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#24635;&#20307;&#35774;&#35745;&#29702;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992; SmartChoices &#22312;&#22823;&#22411;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are living in a golden age of machine learning. Powerful models are being trained to perform many tasks far better than is possible using traditional software engineering approaches alone. However, developing and deploying those models in existing software systems remains difficult. In this paper we present SmartChoices, a novel approach to incorporating machine learning into mature software stacks easily, safely, and effectively. We explain the overall design philosophy and present case studies using SmartChoices within large scale industrial systems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28304;&#20195;&#30721;&#35299;&#26512;&#25104;&#27969;&#22686;&#24378;&#25277;&#35937;&#35821;&#27861;&#26641;&#22270;&#24418;&#24335;&#65292;&#26500;&#36896;&#21508;&#31181;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#22270;&#23884;&#20837;&#65292;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#39640;&#25928;&#21033;&#29992;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2304.13032</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#27880;&#37322;&#22270;&#24418;&#25968;&#25454;&#24182;&#24212;&#29992;&#20110;&#36719;&#20214;&#20195;&#30721;&#24615;&#33021;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Unified Active Learning Framework for Annotating Graph Data with Application to Software Source Code Performance Prediction. (arXiv:2304.13032v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13032
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28304;&#20195;&#30721;&#35299;&#26512;&#25104;&#27969;&#22686;&#24378;&#25277;&#35937;&#35821;&#27861;&#26641;&#22270;&#24418;&#24335;&#65292;&#26500;&#36896;&#21508;&#31181;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#22270;&#23884;&#20837;&#65292;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#39640;&#25928;&#21033;&#29992;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#31243;&#24207;&#65292;&#21253;&#25324;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#24037;&#31243;&#65292;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#21644;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20123;&#21487;&#33021;&#20107;&#20808;&#24182;&#19981;&#21487;&#29992;&#12290;&#33719;&#21462;&#27880;&#37322;&#36890;&#24120;&#38656;&#35201;&#26174;&#30528;&#30340;&#26102;&#38388;&#12289;&#31934;&#21147;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20351;&#24471;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#19987;&#38376;&#38024;&#23545;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;&#23558;&#28304;&#20195;&#30721;&#35299;&#26512;&#25104;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#24320;&#22987;&#65292;&#28982;&#21518;&#29992;&#25968;&#25454;&#21644;&#25511;&#21046;&#27969;&#36793;&#26469;&#22686;&#24378;&#23427;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#28304;&#20195;&#30721;&#30340;&#26641;&#24418;&#34920;&#31034;&#36716;&#25442;&#20026;&#27969;&#22686;&#24378;&#25277;&#35937;&#35821;&#27861;&#26641;&#22270;&#65288;FA-AST&#65289;&#34920;&#31034;&#27861;&#12290;&#22522;&#20110;&#22270;&#24418;&#34920;&#31034;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#22270;&#23884;&#20837;&#65288;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#65289;&#26500;&#36896;&#25104;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#12290;&#37492;&#20110;&#36825;&#26679;&#30340;&#23884;&#20837;&#65292;&#35813;&#26694;&#26550;&#21464;&#24471;&#20219;&#21153;&#19981;&#21487;&#30693;&#65292;&#22240;&#20026;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#22238;&#24402;&#26041;&#27861;&#21644;&#36866;&#29992;&#20110;&#22238;&#24402;&#30340;&#26597;&#35810;&#31574;&#30053;&#26469;&#25191;&#34892;&#20027;&#21160;&#23398;&#20064;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#29992;&#20110;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26597;&#35810;&#31574;&#30053;&#21644;&#22238;&#24402;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most machine learning and data analytics applications, including performance engineering in software systems, require a large number of annotations and labelled data, which might not be available in advance. Acquiring annotations often requires significant time, effort, and computational resources, making it challenging. We develop a unified active learning framework, specializing in software performance prediction, to address this task. We begin by parsing the source code to an Abstract Syntax Tree (AST) and augmenting it with data and control flow edges. Then, we convert the tree representation of the source code to a Flow Augmented-AST graph (FA-AST) representation. Based on the graph representation, we construct various graph embeddings (unsupervised and supervised) into a latent space. Given such an embedding, the framework becomes task agnostic since active learning can be performed using any regression method and query strategy suited for regression. Within this framework, we in
&lt;/p&gt;</description></item><item><title>Awesome-META+&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#38598;&#25104;&#21644;&#23398;&#20064;&#24179;&#21488;&#65292;&#26088;&#22312;&#25552;&#20379;&#23436;&#25972;&#21487;&#38752;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#24212;&#29992;&#21644;&#38754;&#21521;&#21021;&#23398;&#32773;&#30340;&#23398;&#20064;&#26448;&#26009;&#65292;&#36827;&#32780;&#20419;&#36827;&#20803;&#23398;&#20064;&#30340;&#21457;&#23637;&#24182;&#23558;&#20854;&#20174;&#23567;&#20247;&#39046;&#22495;&#36716;&#21270;&#20026;&#20027;&#27969;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.12921</link><description>&lt;p&gt;
Awesome-META+: &#20803;&#23398;&#20064;&#30740;&#31350;&#19982;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Awesome-META+: Meta-Learning Research and Learning Platform. (arXiv:2304.12921v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12921
&lt;/p&gt;
&lt;p&gt;
Awesome-META+&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#38598;&#25104;&#21644;&#23398;&#20064;&#24179;&#21488;&#65292;&#26088;&#22312;&#25552;&#20379;&#23436;&#25972;&#21487;&#38752;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#24212;&#29992;&#21644;&#38754;&#21521;&#21021;&#23398;&#32773;&#30340;&#23398;&#20064;&#26448;&#26009;&#65292;&#36827;&#32780;&#20419;&#36827;&#20803;&#23398;&#20064;&#30340;&#21457;&#23637;&#24182;&#23558;&#20854;&#20174;&#23567;&#20247;&#39046;&#22495;&#36716;&#21270;&#20026;&#20027;&#27969;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#22312;&#32463;&#27982;&#12289;&#20135;&#19994;&#12289;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#20294;&#36824;&#23384;&#22312;&#35832;&#22810;&#38480;&#21046;&#12290;&#20803;&#23398;&#20064;&#65292;&#20063;&#31216;&#20026;&#8220;&#23398;&#20064;&#22914;&#20309;&#23398;&#20064;&#8221;&#65292;&#20026;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#31361;&#30772;&#30446;&#21069;&#29942;&#39048;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#20803;&#23398;&#20064;&#36215;&#27493;&#36739;&#26202;&#65292;&#30456;&#27604;CV&#12289;NLP&#31561;&#39046;&#22495;&#65292;&#39033;&#30446;&#25968;&#37327;&#36739;&#23569;&#12290;&#27599;&#27425;&#37096;&#32626;&#37117;&#38656;&#35201;&#22823;&#37327;&#30340;&#32463;&#39564;&#21435;&#37197;&#32622;&#29615;&#22659;&#12289;&#35843;&#35797;&#20195;&#30721;&#29978;&#33267;&#37325;&#20889;&#65292;&#32780;&#19988;&#26694;&#26550;&#20043;&#38388;&#30456;&#23545;&#23396;&#31435;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#38024;&#23545;&#20803;&#23398;&#20064;&#30340;&#19987;&#38376;&#24179;&#21488;&#21644;&#38754;&#21521;&#21021;&#23398;&#32773;&#30340;&#23398;&#20064;&#26448;&#26009;&#30456;&#23545;&#36739;&#23569;&#65292;&#38376;&#27099;&#30456;&#23545;&#36739;&#39640;&#12290;&#22522;&#20110;&#27492;&#65292;Awesome-META+&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#38598;&#25104;&#21644;&#23398;&#20064;&#24179;&#21488;&#65292;&#26088;&#22312;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#24182;&#25552;&#20379;&#23436;&#25972;&#21487;&#38752;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#24212;&#29992;&#21644;&#23398;&#20064;&#24179;&#21488;&#12290;&#35813;&#39033;&#30446;&#26088;&#22312;&#20419;&#36827;&#20803;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#24182;&#23558;&#20854;&#20174;&#19968;&#20010;&#23567;&#20247;&#39046;&#22495;&#36716;&#21270;&#20026;&#19968;&#20010;&#20027;&#27969;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence technology has already had a profound impact in various fields such as economy, industry, and education, but still limited. Meta-learning, also known as "learning to learn", provides an opportunity for general artificial intelligence, which can break through the current AI bottleneck. However, meta learning started late and there are fewer projects compare with CV, NLP etc. Each deployment requires a lot of experience to configure the environment, debug code or even rewrite, and the frameworks are isolated. Moreover, there are currently few platforms that focus exclusively on meta-learning, or provide learning materials for novices, for which the threshold is relatively high. Based on this, Awesome-META+, a meta-learning framework integration and learning platform is proposed to solve the above problems and provide a complete and reliable meta-learning framework application and learning platform. The project aims to promote the development of meta-learning and t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;LyaDEQ&#30340;&#40065;&#26834;DEQ&#27169;&#22411;&#65292;&#36890;&#36807;Lyapunov&#29702;&#35770;&#25552;&#20379;&#20102;&#20445;&#35777;&#30340;&#31283;&#23450;&#24615;&#20197;&#25269;&#25239;&#24494;&#23567;&#30340;&#21021;&#22987;&#25200;&#21160;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22266;&#23450;&#28857;&#20043;&#38388;&#21152;&#20837;&#20840;&#36830;&#25509;&#23618;&#20197;&#36991;&#20813;&#19981;&#33391;&#23545;&#25239;&#24615;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2304.12707</link><description>&lt;p&gt;
&#23398;&#20064;&#40065;&#26834;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Robust Deep Equilibrium Models. (arXiv:2304.12707v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;LyaDEQ&#30340;&#40065;&#26834;DEQ&#27169;&#22411;&#65292;&#36890;&#36807;Lyapunov&#29702;&#35770;&#25552;&#20379;&#20102;&#20445;&#35777;&#30340;&#31283;&#23450;&#24615;&#20197;&#25269;&#25239;&#24494;&#23567;&#30340;&#21021;&#22987;&#25200;&#21160;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22266;&#23450;&#28857;&#20043;&#38388;&#21152;&#20837;&#20840;&#36830;&#25509;&#23618;&#20197;&#36991;&#20813;&#19981;&#33391;&#23545;&#25239;&#24615;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24179;&#34913;(DEQ)&#27169;&#22411;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38544;&#24335;&#23618;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#35299;&#20915;&#21333;&#20010;&#38750;&#32447;&#24615;&#23618;&#30340;&#22266;&#23450;&#28857;&#26469;&#25918;&#24323;&#20102;&#20256;&#32479;&#28145;&#24230;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#24456;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#22266;&#23450;&#28857;&#30340;&#31283;&#23450;&#24615;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#65292;&#23558;Lyapunov&#29702;&#35770;&#24212;&#29992;&#20110;&#21478;&#19968;&#31181;&#31867;&#22411;&#30340;&#38544;&#24335;&#23618;&#27169;&#22411;&#8212;&#8212;&#31070;&#32463;ODE&#65292;&#21487;&#20197;&#36171;&#20104;&#20854;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;DEQ&#27169;&#22411;&#35270;&#20026;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LyaDEQ&#30340;&#40065;&#26834;DEQ&#27169;&#22411;&#65292;&#36890;&#36807;Lyapunov&#29702;&#35770;&#25552;&#20379;&#20102;&#20445;&#35777;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#30830;&#20445;DEQ&#27169;&#22411;&#30340;&#22266;&#23450;&#28857;&#26159;Lyapunov&#31283;&#23450;&#30340;&#65292;&#36825;&#20351;&#24471;LyaDEQ&#27169;&#22411;&#33021;&#22815;&#25269;&#25239;&#24494;&#23567;&#30340;&#21021;&#22987;&#25200;&#21160;&#12290;&#20026;&#20102;&#36991;&#20813;&#30001;&#20110;Lyapunov&#31283;&#23450;&#30340;&#22266;&#23450;&#28857;&#24444;&#27492;&#38752;&#36817;&#32780;&#23548;&#33268;&#30340;&#19981;&#33391;&#23545;&#25239;&#24615;&#38450;&#24481;&#65292;&#25105;&#20204;&#22312;Lyapunov&#31283;&#23450;&#24615;&#27169;&#22359;&#20043;&#21518;&#21152;&#20837;&#20102;&#19968;&#20010;&#27491;&#20132;&#30340;&#20840;&#36830;&#25509;&#23618;&#65292;&#20197;&#20998;&#31163;&#19981;&#21516;&#30340;&#22266;&#23450;&#28857;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;LyaDEQ&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equilibrium (DEQ) models have emerged as a promising class of implicit layer models in deep learning, which abandon traditional depth by solving for the fixed points of a single nonlinear layer. Despite their success, the stability of the fixed points for these models remains poorly understood. Recently, Lyapunov theory has been applied to Neural ODEs, another type of implicit layer model, to confer adversarial robustness. By considering DEQ models as nonlinear dynamic systems, we propose a robust DEQ model named LyaDEQ with guaranteed provable stability via Lyapunov theory. The crux of our method is ensuring the fixed points of the DEQ models are Lyapunov stable, which enables the LyaDEQ models to resist the minor initial perturbations. To avoid poor adversarial defense due to Lyapunov-stable fixed points being located near each other, we add an orthogonal fully connected layer after the Lyapunov stability module to separate different fixed points. We evaluate LyaDEQ models on se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;(PI-INN)&#35299;&#20915;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#25277;&#26679;&#21644;&#20934;&#30830;&#30340;&#23494;&#24230;&#35780;&#20272;&#12290;&#30740;&#31350;&#36890;&#36807;&#27531;&#24046;&#39033;&#21644;&#29420;&#31435;&#24615;&#25439;&#22833;&#39033;&#30830;&#20445;&#20102;INN&#36755;&#20986;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#65292;&#24182;&#22312;&#22810;&#39033;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12541</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Efficient Bayesian inference using physics-informed invertible neural networks for inverse problems. (arXiv:2304.12541v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;(PI-INN)&#35299;&#20915;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#25277;&#26679;&#21644;&#20934;&#30830;&#30340;&#23494;&#24230;&#35780;&#20272;&#12290;&#30740;&#31350;&#36890;&#36807;&#27531;&#24046;&#39033;&#21644;&#29420;&#31435;&#24615;&#25439;&#22833;&#39033;&#30830;&#20445;&#20102;INN&#36755;&#20986;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#65292;&#24182;&#22312;&#22810;&#39033;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;(PI-INN)&#35299;&#20915;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;PI-INN&#30340;&#32467;&#26500;&#21253;&#25324;&#20004;&#20010;&#23376;&#32593;&#32476;&#65306;&#19968;&#20010;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;(INN)&#21644;&#19968;&#20010;&#31070;&#32463;&#22522;&#30784;&#32593;&#32476;(NB-Net)&#12290;&#36890;&#36807;NB-Net&#24110;&#21161;&#24314;&#31435;&#21442;&#25968;&#36755;&#20837;&#21644;INN&#36755;&#20986;&#20043;&#38388;&#30340;&#21487;&#36870;&#26144;&#23556;&#65292;&#20197;&#25552;&#20379;&#21487;&#34892;&#30340;&#21518;&#39564;&#20998;&#24067;&#20272;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#25277;&#26679;&#21644;&#20934;&#30830;&#30340;&#23494;&#24230;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;PI-INN&#30340;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#20004;&#20010;&#37096;&#20998;&#65306;&#19968;&#37096;&#20998;&#26159;&#22522;&#20110;&#27531;&#24046;&#30340;&#29289;&#29702;&#20449;&#24687;&#25439;&#22833;&#39033;&#65292;&#21478;&#19968;&#37096;&#20998;&#26159;&#26032;&#30340;&#29420;&#31435;&#24615;&#25439;&#22833;&#39033;&#12290;&#25552;&#20986;&#30340;&#29420;&#31435;&#24615;&#25439;&#22833;&#39033;&#21487;&#20197;&#39640;&#26031;&#21270;&#38543;&#26426;&#28508;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#20272;&#35745;&#30340;&#23494;&#24230;&#20989;&#25968;&#65292;&#30830;&#20445;INN&#36755;&#20986;&#30340;&#20004;&#20010;&#37096;&#20998;&#20043;&#38388;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#12290;&#36890;&#36807;&#36827;&#34892;&#21453;&#21521;&#36816;&#21160;&#23398;&#21644;&#21453;&#21521;&#25193;&#25955;&#31561;&#22810;&#39033;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;PI-INN&#30340;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the paper, we propose a novel approach for solving Bayesian inverse problems with physics-informed invertible neural networks (PI-INN). The architecture of PI-INN consists of two sub-networks: an invertible neural network (INN) and a neural basis network (NB-Net). The invertible map between the parametric input and the INN output with the aid of NB-Net is constructed to provide a tractable estimation of the posterior distribution, which enables efficient sampling and accurate density evaluation. Furthermore, the loss function of PI-INN includes two components: a residual-based physics-informed loss term and a new independence loss term. The presented independence loss term can Gaussianize the random latent variables and ensure statistical independence between two parts of INN output by effectively utilizing the estimated density function. Several numerical experiments are presented to demonstrate the efficiency and accuracy of the proposed PI-INN, including inverse kinematics, inver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#28966;&#28857;&#29228;&#34411;ThreatCrawl&#65292;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#26469;&#31579;&#36873;&#20986;&#26368;&#21487;&#33021;&#21253;&#21547;&#26377;&#20215;&#20540;CTI&#20449;&#24687;&#30340;&#32593;&#39029;&#12290;</title><link>http://arxiv.org/abs/2304.11960</link><description>&lt;p&gt;
ThreatCrawl&#65306;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23433;&#20840;&#28966;&#28857;&#29228;&#34411;
&lt;/p&gt;
&lt;p&gt;
ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain. (arXiv:2304.11960v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#28966;&#28857;&#29228;&#34411;ThreatCrawl&#65292;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#26469;&#31579;&#36873;&#20986;&#26368;&#21487;&#33021;&#21253;&#21547;&#26377;&#20215;&#20540;CTI&#20449;&#24687;&#30340;&#32593;&#39029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20844;&#24320;&#33719;&#21462;&#30340;&#20449;&#24687;&#23545;&#20110;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#65288;CTI&#65289;&#26469;&#35828;&#21253;&#21547;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#36825;&#21487;&#20197;&#29992;&#20110;&#39044;&#38450;&#24050;&#32463;&#22312;&#20854;&#20182;&#31995;&#32479;&#19978;&#21457;&#29983;&#30340;&#25915;&#20987;&#12290;&#20294;&#26159;&#65292;&#34429;&#28982;&#26377;&#19981;&#21516;&#30340;&#26631;&#20934;&#26469;&#20132;&#27969;&#36825;&#20123;&#20449;&#24687;&#65292;&#20294;&#24456;&#22810;&#20449;&#24687;&#26159;&#20197;&#38750;&#26631;&#20934;&#21270;&#30340;&#26041;&#24335;&#22312;&#25991;&#31456;&#25110;&#21338;&#23458;&#24086;&#23376;&#20013;&#20849;&#20139;&#30340;&#12290;&#25163;&#21160;&#27983;&#35272;&#22810;&#20010;&#22312;&#32447;&#38376;&#25143;&#21644;&#26032;&#38395;&#39029;&#38754;&#20197;&#21457;&#29616;&#26032;&#23041;&#32961;&#24182;&#25552;&#21462;&#23427;&#20204;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#36825;&#20010;&#25195;&#25551;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#22810;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#23041;&#32961;&#25351;&#31034;&#22120;&#65288;IOCs&#65289;&#30340;&#25552;&#21462;&#22120;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#24050;&#32463;&#35299;&#20915;&#20102;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#20294;&#24456;&#23569;&#32771;&#34385;&#25628;&#32034;&#36825;&#20123;&#25991;&#26723;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28966;&#28857;&#29228;&#34411;ThreatCrawl&#65292;&#23427;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65288;BERT&#65289;&#25628;&#32034;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#30456;&#20851;&#25991;&#26723;&#12290;ThreatCrawl&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#26469;&#35782;&#21035;&#30456;&#20851;&#32593;&#31449;&#21644;&#32593;&#39029;&#65292;&#28982;&#21518;&#24212;&#29992;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#22120;&#26469;&#20248;&#20808;&#32771;&#34385;&#26368;&#21487;&#33021;&#21253;&#21547;&#26377;&#20215;&#20540;CTI&#20449;&#24687;&#30340;&#32593;&#39029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Publicly available information contains valuable information for Cyber Threat Intelligence (CTI). This can be used to prevent attacks that have already taken place on other systems. Ideally, only the initial attack succeeds and all subsequent ones are detected and stopped. But while there are different standards to exchange this information, a lot of it is shared in articles or blog posts in non-standardized ways. Manually scanning through multiple online portals and news pages to discover new threats and extracting them is a time-consuming task. To automize parts of this scanning process, multiple papers propose extractors that use Natural Language Processing (NLP) to extract Indicators of Compromise (IOCs) from documents. However, while this already solves the problem of extracting the information out of documents, the search for these documents is rarely considered. In this paper, a new focused crawler is proposed called ThreatCrawl, which uses Bidirectional Encoder Representations 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#32858;&#31867;&#31639;&#27861;&#21644;&#30452;&#25509;&#24212;&#29992;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#24212;&#20808;&#37319;&#29992;&#30452;&#25509;&#27604;&#20363;&#26041;&#27861;&#25214;&#21040;&#27491;&#30830;&#30340;&#32422;&#26463;&#32500;&#24230;&#38598;&#65292;&#20877;&#29992;&#32858;&#31867;&#26469;&#32454;&#21270;&#32467;&#26524;&#65292;&#21516;&#26102;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#20248;&#20110;&#26631;&#20934;&#30340;k-means++&#32858;&#31867;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.11901</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#20998;&#31867;&#21608;&#26399;&#24615;&#36229;&#32467;&#26500;&#20013;&#27874;&#30340;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Machine Learning to Classify the Confinement of Waves in Periodic Superstructures. (arXiv:2304.11901v2 [physics.optics] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11901
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#32858;&#31867;&#31639;&#27861;&#21644;&#30452;&#25509;&#24212;&#29992;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#24212;&#20808;&#37319;&#29992;&#30452;&#25509;&#27604;&#20363;&#26041;&#27861;&#25214;&#21040;&#27491;&#30830;&#30340;&#32422;&#26463;&#32500;&#24230;&#38598;&#65292;&#20877;&#29992;&#32858;&#31867;&#26469;&#32454;&#21270;&#32467;&#26524;&#65292;&#21516;&#26102;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#20248;&#20110;&#26631;&#20934;&#30340;k-means++&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#20102;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;&#27874;&#32422;&#26463;&#20998;&#26512;&#30340;&#27604;&#20363;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;[1]&#12290;&#25105;&#20204;&#37319;&#29992;&#26631;&#20934;&#30340;k-means ++&#31639;&#27861;&#20197;&#21450;&#25105;&#20204;&#33258;&#24049;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#31751;&#26377;&#25928;&#24615;&#25351;&#25968;&#20316;&#20026;&#19968;&#31181;&#25163;&#27573;&#26469;&#25214;&#21040;&#27491;&#30830;&#30340;&#32422;&#26463;&#32500;&#24230;&#21495;&#65292;&#20197;&#29992;&#20316;&#32858;&#31867;&#31639;&#27861;&#30340;&#36755;&#20837;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#32858;&#31867;&#31639;&#27861;&#30456;&#23545;&#20110;&#19981;&#20351;&#29992;&#32858;&#31867;&#30340;&#30452;&#25509;&#24212;&#29992;&#27604;&#20363;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#32858;&#31867;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20855;&#29289;&#29702;&#24847;&#20041;&#30340;&#32467;&#26524;&#65292;&#20294;&#21487;&#33021;&#38590;&#20197;&#35782;&#21035;&#27491;&#30830;&#30340;&#32422;&#26463;&#32500;&#24230;&#38598;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#26368;&#20934;&#30830;&#30340;&#32467;&#26524;&#26159;&#20808;&#24212;&#29992;&#30452;&#25509;&#27604;&#20363;&#26041;&#27861;&#25214;&#21040;&#27491;&#30830;&#30340;&#32422;&#26463;&#32500;&#24230;&#38598;&#65292;&#28982;&#21518;&#20877;&#37319;&#29992;&#32858;&#31867;&#26469;&#32454;&#21270;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#20248;&#20110;&#26631;&#20934;&#30340;k-means ++&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
We employ unsupervised machine learning to enhance the accuracy of our recently presented scaling method for wave confinement analysis [1]. We employ the standard k-means++ algorithm as well as our own model-based algorithm. We investigate cluster validity indices as a means to find the correct number of confinement dimensionalities to be used as an input to the clustering algorithms. Subsequently, we analyze the performance of the two clustering algorithms when compared to the direct application of the scaling method without clustering. We find that the clustering approach provides more physically meaningful results, but may struggle with identifying the correct set of confinement dimensionalities. We conclude that the most accurate outcome is obtained by first applying the direct scaling to find the correct set of confinement dimensionalities and subsequently employing clustering to refine the results. Moreover, our model-based algorithm outperforms the standard k-means++ clustering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#24449;&#30456;&#20851;&#24615;&#22312;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#35823;&#23548;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.11597</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning Partial Correlation based Deep Visual Representation for Image Classification. (arXiv:2304.11597v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#24449;&#30456;&#20851;&#24615;&#22312;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#35823;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35270;&#35273;&#34920;&#31034;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23545;&#21367;&#31215;&#29305;&#24449;&#26144;&#23556;&#20013;&#19981;&#21516;&#36890;&#36947;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#23384;&#22312;&#21478;&#19968;&#20010;&#36890;&#36947;&#19982;&#24863;&#20852;&#36259;&#30340;&#20004;&#20010;&#36890;&#36947;&#30456;&#20851;&#65292;&#21017;&#25104;&#23545;&#30456;&#20851;&#24615;&#23558;&#21464;&#24471;&#35823;&#23548;&#20154;&#65292;&#23548;&#33268;&#8220;&#28151;&#28102;&#8221;&#25928;&#24212;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#24212;&#35813;&#20272;&#35745;&#8220;&#20559;&#30456;&#20851;&#8221;&#65292;&#20197;&#28040;&#38500;&#28151;&#28102;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#21487;&#38752;&#22320;&#20272;&#35745;&#20559;&#30456;&#20851;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#31232;&#30095;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#65288;SICE&#65289;&#12290;&#22914;&#20309;&#23558;&#27492;&#36807;&#31243;&#34701;&#20837;CNN&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;SICE&#21046;&#23450;&#20026;CNN&#30340;&#19968;&#20010;&#26032;&#32467;&#26500;&#23618;&#12290;&#20026;&#30830;&#20445;&#31471;&#21040;&#31471;&#30340;&#21487;&#35757;&#32451;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#22312;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#25773;&#27493;&#39588;&#20013;&#35299;&#20915;&#19978;&#36848;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#33719;&#24471;&#20102;&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual representation based on covariance matrix has demonstrates its efficacy for image classification by characterising the pairwise correlation of different channels in convolutional feature maps. However, pairwise correlation will become misleading once there is another channel correlating with both channels of interest, resulting in the ``confounding'' effect. For this case, ``partial correlation'' which removes the confounding effect shall be estimated instead. Nevertheless, reliably estimating partial correlation requires to solve a symmetric positive definite matrix optimisation, known as sparse inverse covariance estimation (SICE). How to incorporate this process into CNN remains an open issue. In this work, we formulate SICE as a novel structured layer of CNN. To ensure end-to-end trainability, we develop an iterative method to solve the above matrix optimisation during forward and backward propagation steps. Our work obtains a partial correlation based deep visual representa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#30340;&#20445;&#24207;&#26426;&#21046;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25351;&#25968;&#26063;&#20998;&#24067;&#20197;&#25552;&#39640;&#21516;&#34892;&#35780;&#23457;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20316;&#32773;&#30340;&#21516;&#34892;&#35780;&#20998;&#21487;&#20197;&#36739;&#20934;&#30830;&#22320;&#22312;&#19981;&#38656;&#35201;&#30693;&#36947;&#20855;&#20307;&#20998;&#24067;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2304.11160</link><description>&lt;p&gt;
&#21033;&#29992;&#20445;&#24207;&#26426;&#21046;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;
&lt;/p&gt;
&lt;p&gt;
The Isotonic Mechanism for Exponential Family Estimation. (arXiv:2304.11160v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#30340;&#20445;&#24207;&#26426;&#21046;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25351;&#25968;&#26063;&#20998;&#24067;&#20197;&#25552;&#39640;&#21516;&#34892;&#35780;&#23457;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20316;&#32773;&#30340;&#21516;&#34892;&#35780;&#20998;&#21487;&#20197;&#36739;&#20934;&#30830;&#22320;&#22312;&#19981;&#38656;&#35201;&#30693;&#36947;&#20855;&#20307;&#20998;&#24067;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#25193;&#23637;&#20445;&#24207;&#26426;&#21046;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25351;&#25968;&#26063;&#20998;&#24067;&#20197;&#25552;&#39640;&#21516;&#34892;&#35780;&#23457;&#30340;&#36136;&#37327;&#12290;&#35813;&#26426;&#21046;&#21487;&#29983;&#25104;&#19982;&#21407;&#22987;&#35780;&#20998;&#25509;&#36817;&#30340;&#35843;&#25972;&#20998;&#25968;&#65292;&#24182;&#31526;&#21512;&#20316;&#32773;&#25351;&#23450;&#30340;&#25490;&#21517;&#35201;&#27714;&#65292;&#24471;&#21040;&#24191;&#27867;&#30340;&#25351;&#25968;&#26063;&#20998;&#24067;&#24212;&#29992;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#30693;&#36947;&#20855;&#20307;&#30340;&#20998;&#24067;&#24418;&#24335;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19968;&#23450;&#30340;&#25351;&#25968;&#26063;&#20998;&#24067;&#19979;&#65292;&#22914;&#26524;&#20316;&#32773;&#30340;&#25928;&#29992;&#20989;&#25968;&#37319;&#29992;&#31616;&#21333;&#30340;&#20984;&#21487;&#21152;&#20989;&#25968;&#65292;&#21017;&#28608;&#21169;&#20316;&#32773;&#25552;&#20379;&#20934;&#30830;&#30340;&#25490;&#21517;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2023, the International Conference on Machine Learning (ICML) required authors with multiple submissions to rank their submissions based on perceived quality. In this paper, we aim to employ these author-specified rankings to enhance peer review in machine learning and artificial intelligence conferences by extending the Isotonic Mechanism (Su, 2021, 2022) to exponential family distributions. This mechanism generates adjusted scores closely align with the original scores while adhering to author-specified rankings. Despite its applicability to a broad spectrum of exponential family distributions, this mechanism's implementation does not necessitate knowledge of the specific distribution form. We demonstrate that an author is incentivized to provide accurate rankings when her utility takes the form of a convex additive function of the adjusted review scores. For a certain subclass of exponential family distributions, we prove that the author reports truthfully only if the question in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11127</link><description>&lt;p&gt;
&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65306;&#29702;&#35299;&#20854;&#31639;&#27861;&#32452;&#25104;&#37096;&#20998;&#21450;&#20854;&#22312;&#25552;&#39640;&#23454;&#35777;&#34920;&#29616;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance. (arXiv:2304.11127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#22495;&#20013;&#26368;&#36817;&#30340;&#36827;&#23637;&#35201;&#27714;&#26356;&#21152;&#22797;&#26434;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#23454;&#39564;&#36890;&#24120;&#26377;&#35768;&#22810;&#21442;&#25968;&#65292;&#38656;&#35201;&#21442;&#25968;&#35843;&#25972;&#12290;Tree-structured Parzen estimator (TPE) &#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#26368;&#36817;&#30340;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23613;&#31649;&#23427;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#25511;&#21046;&#21442;&#25968;&#30340;&#35282;&#33394;&#21644;&#31639;&#27861;&#30452;&#35273;&#23578;&#26410;&#24471;&#21040;&#35752;&#35770;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#30830;&#23450;&#27599;&#20010;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#20197;&#21450;&#23427;&#20204;&#23545;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#23558;&#20174;&#21078;&#26512;&#30740;&#31350;&#20013;&#24471;&#20986;&#30340;&#25512;&#33616;&#35774;&#32622;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#25512;&#33616;&#35774;&#32622;&#25552;&#39640;&#20102;TPE&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;TPE&#23454;&#29616;&#21487;&#22312;https://github.com/nabenabe0928/tpe/tree/single-opt&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;SAM&#22312;&#21508;&#31181;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#28857;&#25552;&#31034;&#19979;&#20854;&#34920;&#29616;&#39640;&#24230;&#21464;&#21270;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2304.10517</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#65306;&#19968;&#39033;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model for Medical Image Analysis: an Experimental Study. (arXiv:2304.10517v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;SAM&#22312;&#21508;&#31181;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#28857;&#25552;&#31034;&#19979;&#20854;&#34920;&#29616;&#39640;&#24230;&#21464;&#21270;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#27880;&#37322;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#33719;&#21462;&#25104;&#26412;&#65292;&#35757;&#32451;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;Segment Anything Model&#65288;SAM&#65289;&#26159;&#19968;&#31181;&#22522;&#30784;&#27169;&#22411;&#65292;&#32463;&#36807;&#36229;&#36807;10&#20159;&#20010;&#27880;&#37322;&#30340;&#35757;&#32451;&#65292;&#20027;&#35201;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;&#65292;&#26088;&#22312;&#33021;&#22815;&#20197;&#20132;&#20114;&#26041;&#24335;&#20998;&#21106;&#29992;&#25143;&#23450;&#20041;&#30340;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#23613;&#31649;SAM&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#19981;&#28165;&#26970;&#35813;&#27169;&#22411;&#22312;&#36716;&#25442;&#21040;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#26102;&#20250;&#21463;&#21040;&#22810;&#22823;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;SAM&#22312;&#21508;&#31181;&#27169;&#24577;&#21644;&#35299;&#21078;&#23398;&#30340;11&#20010;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#26041;&#27861;&#29983;&#25104;&#28857;&#25552;&#31034;&#26469;&#27169;&#25311;&#20132;&#20114;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAM&#22522;&#20110;&#21333;&#28857;&#25552;&#31034;&#30340;&#34920;&#29616;&#22312;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26041;&#38754;&#39640;&#24230;&#21464;&#21270;&#65292;&#21363;&#20174;&#33034;&#26609;MRI&#25968;&#25454;&#38598;&#30340;0.1135&#21040;&#39627;&#20851;&#33410;X&#23556;&#32447;&#25968;&#25454;&#38598;&#30340;0.8650&#12290;
&lt;/p&gt;
&lt;p&gt;
Training segmentation models for medical images continues to be challenging due to the limited availability and acquisition expense of data annotations. Segment Anything Model (SAM) is a foundation model trained on over 1 billion annotations, predominantly for natural images, that is intended to be able to segment the user-defined object of interest in an interactive manner. Despite its impressive performance on natural images, it is unclear how the model is affected when shifting to medical image domains. Here, we perform an extensive evaluation of SAM's ability to segment medical images on a collection of 11 medical imaging datasets from various modalities and anatomies. In our experiments, we generated point prompts using a standard method that simulates interactive segmentation. Experimental results show that SAM's performance based on single prompts highly varies depending on the task and the dataset, i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset, eva
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174; softmax &#21333;&#20803;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#26426;&#21046; inspired &#30340; softmax &#22238;&#24402;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#29992;&#20110;&#25511;&#21046;&#28508;&#22312;&#20989;&#25968;&#30340;&#36827;&#23637;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10411</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#30340; softmax &#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Attention Scheme Inspired Softmax Regression. (arXiv:2304.10411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174; softmax &#21333;&#20803;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#26426;&#21046; inspired &#30340; softmax &#22238;&#24402;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#29992;&#20110;&#25511;&#21046;&#28508;&#22312;&#20989;&#25968;&#30340;&#36827;&#23637;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#32473;&#20154;&#31867;&#31038;&#20250;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#21464;&#38761;&#12290;LLMs &#30340;&#20851;&#38190;&#35745;&#31639;&#20043;&#19968;&#26159; softmax &#21333;&#20803;&#12290;&#36825;&#20010;&#25805;&#20316;&#22312; LLMs &#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#27169;&#22411;&#22312;&#32473;&#23450;&#36755;&#20837;&#21333;&#35789;&#24207;&#21015;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#21487;&#33021;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#25110;&#30701;&#35821;&#30340;&#20998;&#24067;&#12290;&#36825;&#20010;&#20998;&#24067;&#28982;&#21518;&#29992;&#26469;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#25110;&#30701;&#35821;&#65292;&#22522;&#20110;&#27169;&#22411;&#20998;&#37197;&#30340;&#27010;&#29575;&#12290;softmax &#21333;&#20803;&#22312;&#35757;&#32451; LLMs &#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#27169;&#22411;&#36890;&#36807;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#22312;&#20984;&#20248;&#21270;&#39046;&#22495;&#65292;&#20363;&#22914;&#20351;&#29992;&#20013;&#24515;&#36335;&#24452;&#27861;&#35299;&#20915;&#32447;&#24615;&#35268;&#21010;&#65292;softmax &#20989;&#25968;&#24050;&#32463;&#25104;&#20026;&#25511;&#21046;&#28508;&#22312;&#20989;&#25968;&#30340;&#36827;&#23637;&#21644;&#31283;&#23450;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174; softmax &#21333;&#20803;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010; softmax &#22238;&#24402;&#38382;&#39064;&#12290;&#24418;&#24335;&#19978;&#35762;&#65292;&#32473;&#23450;&#19968;&#20010;&#30697;&#38453; $A \in \mathbb{R}^{n \times d}$ &#21644;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made transformed changes for human society. One of the key computation in LLMs is the softmax unit. This operation is important in LLMs because it allows the model to generate a distribution over possible next words or phrases, given a sequence of input words. This distribution is then used to select the most likely next word or phrase, based on the probabilities assigned by the model. The softmax unit plays a crucial role in training LLMs, as it allows the model to learn from the data by adjusting the weights and biases of the neural network.  In the area of convex optimization such as using central path method to solve linear programming. The softmax function has been used a crucial tool for controlling the progress and stability of potential function [Cohen, Lee and Song STOC 2019, Brand SODA 2020].  In this work, inspired the softmax unit, we define a softmax regression problem. Formally speaking, given a matrix $A \in \mathbb{R}^{n \times d}$ and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30740;&#31350;&#20102;Nextdoor&#31038;&#20132;&#32593;&#32476;&#65292;&#21457;&#29616;&#19981;&#21516;&#25910;&#20837;&#27700;&#24179;&#30340;&#31038;&#21306;&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#22312;&#32447;&#34892;&#20026;&#19981;&#21516;&#65292;&#26356;&#23500;&#35029;&#30340;&#31038;&#21306;&#24773;&#24863;&#26356;&#31215;&#26497;&#65292;&#26356;&#22810;&#22320;&#35752;&#35770;&#29359;&#32618;&#65292;&#23613;&#31649;&#23454;&#38469;&#29359;&#32618;&#29575;&#35201;&#20302;&#24471;&#22810;&#65292;&#21516;&#26102;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#33021;&#22815;&#39044;&#27979;&#25910;&#20837;&#21644;&#19981;&#24179;&#31561;&#12290;</title><link>http://arxiv.org/abs/2304.05232</link><description>&lt;p&gt;
&#37051;&#37324;&#29356;&#19982;&#34903;&#22836;&#27969;&#28010;&#27721;&#65306;Nextdoor&#31038;&#20132;&#32593;&#32476;&#20013;&#30495;&#23454;&#19990;&#30028;&#19981;&#24179;&#31561;&#30340;&#22312;&#32447;&#20307;&#29616;
&lt;/p&gt;
&lt;p&gt;
Lady and the Tramp Nextdoor: Online Manifestations of Real-World Inequalities in the Nextdoor Social Network. (arXiv:2304.05232v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30740;&#31350;&#20102;Nextdoor&#31038;&#20132;&#32593;&#32476;&#65292;&#21457;&#29616;&#19981;&#21516;&#25910;&#20837;&#27700;&#24179;&#30340;&#31038;&#21306;&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#22312;&#32447;&#34892;&#20026;&#19981;&#21516;&#65292;&#26356;&#23500;&#35029;&#30340;&#31038;&#21306;&#24773;&#24863;&#26356;&#31215;&#26497;&#65292;&#26356;&#22810;&#22320;&#35752;&#35770;&#29359;&#32618;&#65292;&#23613;&#31649;&#23454;&#38469;&#29359;&#32618;&#29575;&#35201;&#20302;&#24471;&#22810;&#65292;&#21516;&#26102;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#33021;&#22815;&#39044;&#27979;&#25910;&#20837;&#21644;&#19981;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#20837;&#24433;&#21709;&#30528;&#29983;&#27963;&#20013;&#24456;&#22810;&#36873;&#25321;&#65292;&#20174;&#20581;&#24247;&#21040;&#25945;&#32946;&#12290;&#35768;&#22810;&#30740;&#31350;&#37117;&#21033;&#29992;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#30340;&#25968;&#25454;&#30740;&#31350;&#27492;&#38382;&#39064;&#12290;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#21453;&#30340;&#38382;&#39064;&#65306;&#19981;&#21516;&#25910;&#20837;&#27700;&#24179;&#26159;&#21542;&#23548;&#33268;&#19981;&#21516;&#30340;&#22312;&#32447;&#34892;&#20026;&#65311;&#25105;&#20204;&#35777;&#26126;&#20102;&#30830;&#23454;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;Nextdoor&#31038;&#20132;&#32593;&#32476;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#22312;&#32654;&#22269;&#30340;64,283&#20010;&#31038;&#21306;&#21644;&#33521;&#22269;&#30340;3,325&#20010;&#31038;&#21306;&#25910;&#38598;&#20102;2.6&#19975;&#31687;&#24086;&#23376;&#65292;&#20197;&#30740;&#31350;&#22312;&#32447;&#35805;&#35821;&#26159;&#21542;&#21453;&#26144;&#20102;&#31038;&#21306;&#30340;&#25910;&#20837;&#21644;&#25910;&#20837;&#19981;&#24179;&#31561;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#25910;&#20837;&#30340;&#31038;&#21306;&#30340;&#24086;&#23376;&#30830;&#23454;&#19981;&#21516;&#65292;&#27604;&#22914;&#26356;&#23500;&#35029;&#30340;&#31038;&#21306;&#24773;&#24863;&#26356;&#31215;&#26497;&#65292;&#26356;&#22810;&#22320;&#35752;&#35770;&#29359;&#32618;&#65292;&#23613;&#31649;&#23454;&#38469;&#29359;&#32618;&#29575;&#35201;&#20302;&#24471;&#22810;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#21487;&#39044;&#27979;&#25910;&#20837;&#21644;&#19981;&#24179;&#31561;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#25910;&#20837;&#65288;R-Square=0.841&#65289;&#21644;&#19981;&#24179;&#31561;&#65288;R-Sq&#8230;
&lt;/p&gt;
&lt;p&gt;
From health to education, income impacts a huge range of life choices. Many papers have leveraged data from online social networks to study precisely this. In this paper, we ask the opposite question: do different levels of income result in different online behaviors? We demonstrate it does. We present the first large-scale study of Nextdoor, a popular location-based social network. We collect 2.6 Million posts from 64,283 neighborhoods in the United States and 3,325 neighborhoods in the United Kingdom, to examine whether online discourse reflects the income and income inequality of a neighborhood. We show that posts from neighborhoods with different income indeed differ, e.g. richer neighborhoods have a more positive sentiment and discuss crimes more, even though their actual crime rates are much lower. We then show that user-generated content can predict both income and inequality. We train multiple machine learning models and predict both income (R-Square=0.841) and inequality (R-Sq
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Perp-Neg&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24471;&#20998;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#26469;&#35299;&#20915;&#30446;&#21069;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#29992;&#25143;&#33021;&#22815;&#32534;&#36753;&#25481;&#21021;&#22987;&#29983;&#25104;&#22270;&#20687;&#20013;&#19981;&#24819;&#35201;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;Perp-Neg&#30340;3D&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#65292;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;3D&#24212;&#29992;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.04968</link><description>&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#36127;&#25552;&#31034;&#31639;&#27861;&#65306;&#23558;2D&#25193;&#25955;&#36716;&#21270;&#20026;3D&#65292;&#32531;&#35299;&#8220;&#25196;&#23612;&#26031;&#38382;&#39064;&#8221;&#31561;&#31561;
&lt;/p&gt;
&lt;p&gt;
Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond. (arXiv:2304.04968v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Perp-Neg&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24471;&#20998;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#26469;&#35299;&#20915;&#30446;&#21069;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#29992;&#25143;&#33021;&#22815;&#32534;&#36753;&#25481;&#21021;&#22987;&#29983;&#25104;&#22270;&#20687;&#20013;&#19981;&#24819;&#35201;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;Perp-Neg&#30340;3D&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#65292;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;3D&#24212;&#29992;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#20174;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#26377;&#26102;&#26356;&#20542;&#21521;&#20110;&#29983;&#25104;&#31867;&#20284;&#20110;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#30340;&#22270;&#20687;&#65292;&#32780;&#19981;&#26159;&#25552;&#20379;&#30340;&#25991;&#26412;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;2D&#21644;3D&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#36127;&#38754;&#25552;&#31034;&#65292;&#20294;&#21457;&#29616;&#24403;&#21069;&#30340;&#23454;&#29616;&#26080;&#27861;&#20135;&#29983;&#26399;&#26395;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#24403;&#20027;&#25552;&#31034;&#21644;&#36127;&#38754;&#25552;&#31034;&#20043;&#38388;&#23384;&#22312;&#37325;&#21472;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;Perp-Neg&#65292;&#19968;&#31181;&#21033;&#29992;&#24471;&#20998;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#26469;&#35299;&#20915;&#24403;&#21069;&#36127;&#24615;&#25552;&#31034;&#31639;&#27861;&#32570;&#28857;&#30340;&#26032;&#31639;&#27861;&#12290;Perp-Neg&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;Perp-Neg&#36890;&#36807;&#22312;2D&#24773;&#20917;&#19979;&#20351;&#29992;&#25143;&#33021;&#22815;&#32534;&#36753;&#25481;&#21021;&#22987;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#19981;&#24819;&#35201;&#30340;&#27010;&#24565;&#65292;&#25552;&#20379;&#20102;&#29983;&#25104;&#22270;&#20687;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25193;&#23637;&#25105;&#20204;&#30340;&#31639;&#27861;&#21040;3D&#24212;&#29992;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Perp-Neg&#30340;3D&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although text-to-image diffusion models have made significant strides in generating images from text, they are sometimes more inclined to generate images like the data on which the model was trained rather than the provided text. This limitation has hindered their usage in both 2D and 3D applications. To address this problem, we explored the use of negative prompts but found that the current implementation fails to produce desired results, particularly when there is an overlap between the main and negative prompts. To overcome this issue, we propose Perp-Neg, a new algorithm that leverages the geometrical properties of the score space to address the shortcomings of the current negative prompts algorithm. Perp-Neg does not require any training or fine-tuning of the model. Moreover, we experimentally demonstrate that Perp-Neg provides greater flexibility in generating images by enabling users to edit out unwanted concepts from the initially generated images in 2D cases. Furthermore, to e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#31227;&#21160;&#20581;&#24247;&#25968;&#25454;&#26469;&#27604;&#36739;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#39044;&#27979;&#20154;&#20307;&#20581;&#24247;&#34892;&#20026;&#21644;&#20581;&#36523;&#24773;&#20917;&#30340;&#26041;&#27861;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;XGBoost&#31639;&#27861;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04839</link><description>&lt;p&gt;
MHfit&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#36816;&#21160;&#21592;&#20581;&#24247;&#29366;&#20917;&#30340;&#31227;&#21160;&#20581;&#24247;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
MHfit: Mobile Health Data for Predicting Athletics Fitness Using Machine Learning. (arXiv:2304.04839v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#31227;&#21160;&#20581;&#24247;&#25968;&#25454;&#26469;&#27604;&#36739;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#39044;&#27979;&#20154;&#20307;&#20581;&#24247;&#34892;&#20026;&#21644;&#20581;&#36523;&#24773;&#20917;&#30340;&#26041;&#27861;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;XGBoost&#31639;&#27861;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#30005;&#35805;&#21644;&#20854;&#20182;&#30005;&#23376;&#35774;&#22791;&#24050;&#32463;&#24110;&#21161;&#20154;&#20204;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25910;&#38598;&#25968;&#25454;&#12290;&#26412;&#25991;&#23558;&#29305;&#21035;&#20851;&#27880;&#31227;&#21160;&#20581;&#24247;&#25968;&#25454;&#12290;&#31227;&#21160;&#20581;&#24247;&#25968;&#25454;&#20351;&#29992;&#31227;&#21160;&#35774;&#22791;&#23454;&#26102;&#25910;&#38598;&#20020;&#24202;&#20581;&#24247;&#25968;&#25454;&#24182;&#36319;&#36394;&#24739;&#32773;&#29983;&#21629;&#20307;&#24449;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#20174;&#31227;&#21160;&#35774;&#22791;&#21644;&#24739;&#32773;&#36523;&#19978;&#30340;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#27604;&#36739;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#21644;&#20581;&#24247;&#65292;&#24182;&#20026;&#23567;&#22411;&#25110;&#22823;&#22411;&#36816;&#21160;&#38431;&#25552;&#20379;&#20851;&#20110;&#26576;&#20010;&#36816;&#21160;&#21592;&#26159;&#21542;&#36866;&#21512;&#21442;&#19982;&#29305;&#23450;&#27604;&#36187;&#30340;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#20174;&#19968;&#39033;&#31867;&#20284;&#30340;&#31227;&#21160;&#20581;&#24247;&#30740;&#31350;&#20013;&#33719;&#24471;&#20102;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#32972;&#26223;&#30340;10&#21517;&#24535;&#24895;&#32773;&#30340;&#29983;&#21629;&#20307;&#24449;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#12290;&#20182;&#20204;&#24517;&#39035;&#22312;&#36523;&#20307;&#19978;&#25918;&#32622;&#20256;&#24863;&#22120;&#24182;&#36827;&#34892;&#22810;&#39033;&#20307;&#21147;&#27963;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;5&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;XGBoost&#65292;&#26420;&#32032;&#36125;&#21494;&#26031;&#65292;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#36923;&#36753;&#22238;&#24402;&#65289;&#26469;&#20998;&#26512;&#21644;&#39044;&#27979;&#20154;&#31867;&#20581;&#24247;&#34892;&#20026;&#21644;&#20581;&#36523;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;XGBoost&#31639;&#27861;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile phones and other electronic gadgets or devices have aided in collecting data without the need for data entry. This paper will specifically focus on Mobile health data. Mobile health data use mobile devices to gather clinical health data and track patient vitals in real-time. Our study is aimed to give decisions for small or big sports teams on whether one athlete good fit or not for a particular game with the compare several machine learning algorithms to predict human behavior and health using the data collected from mobile devices and sensors placed on patients. In this study, we have obtained the dataset from a similar study done on mhealth. The dataset contains vital signs recordings of ten volunteers from different backgrounds. They had to perform several physical activities with a sensor placed on their bodies. Our study used 5 machine learning algorithms (XGBoost, Naive Bayes, Decision Tree, Random Forest, and Logistic Regression) to analyze and predict human health behav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DiffMimic&#65292;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#39640;&#25928;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#26377;&#26356;&#24555;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#21516;&#26102;&#36890;&#36807;&#28436;&#31034;&#37325;&#25773;&#26426;&#21046;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.03274</link><description>&lt;p&gt;
DiffMimic: &#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#39640;&#25928;&#36816;&#21160;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
DiffMimic: Efficient Motion Mimicking with Differentiable Physics. (arXiv:2304.03274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DiffMimic&#65292;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#39640;&#25928;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#26377;&#26356;&#24555;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#21516;&#26102;&#36890;&#36807;&#28436;&#31034;&#37325;&#25773;&#26426;&#21046;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#27169;&#20223;&#26159;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#21160;&#30011;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#37117;&#24314;&#31435;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20043;&#19978;&#65292;&#23384;&#22312;&#37325;&#24230;&#22870;&#21169;&#24037;&#31243;&#12289;&#39640;&#26041;&#24046;&#21644;&#38590;&#20197;&#25506;&#32034;&#30340;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#27169;&#25311;&#22120;&#65288;DPS&#65289;&#30340;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#65292;&#21517;&#20026;DiffMimic&#65292;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#21644;&#22522;&#20110;&#30495;&#23454;&#29289;&#29702;&#20808;&#39564;&#23398;&#20064;&#31283;&#23450;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#26356;&#24555;&#21644;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#28436;&#31034;&#37325;&#25773;&#26426;&#21046;&#65292;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#20869;&#23454;&#29616;&#31283;&#23450;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion mimicking is a foundational task in physics-based character animation. However, most existing motion mimicking methods are built upon reinforcement learning (RL) and suffer from heavy reward engineering, high variance, and slow convergence with hard explorations. Specifically, they usually take tens of hours or even days of training to mimic a simple motion sequence, resulting in poor scalability. In this work, we leverage differentiable physics simulators (DPS) and propose an efficient motion mimicking method dubbed DiffMimic. Our key insight is that DPS casts a complex policy learning task to a much simpler state matching problem. In particular, DPS learns a stable policy by analytical gradients with ground-truth physical priors hence leading to significantly faster and stabler convergence than RL-based methods. Moreover, to escape from local optima, we utilize a Demonstration Replay mechanism to enable stable gradient backpropagation in a long horizon. Extensive experiments o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#22411;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#22240;&#26524;&#35786;&#26029;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#30699;&#27491;&#26377;&#38382;&#39064;&#30340;&#36755;&#20837;/&#36755;&#20986;&#34892;&#20026;&#23376;&#38598;&#65292;&#35782;&#21035;&#30495;&#27491;&#30340;&#23646;&#24615;&#36829;&#35268;&#21407;&#22240;&#24182;&#20462;&#22797;&#12290;</title><link>http://arxiv.org/abs/2304.02813</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#22411;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#22240;&#26524;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Causal Repair of Learning-enabled Cyber-physical Systems. (arXiv:2304.02813v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#22411;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#22240;&#26524;&#35786;&#26029;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#30699;&#27491;&#26377;&#38382;&#39064;&#30340;&#36755;&#20837;/&#36755;&#20986;&#34892;&#20026;&#23376;&#38598;&#65292;&#35782;&#21035;&#30495;&#27491;&#30340;&#23646;&#24615;&#36829;&#35268;&#21407;&#22240;&#24182;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#22240;&#26524;&#27169;&#22411;&#20351;&#29992;&#39046;&#22495;&#30693;&#35782;&#29983;&#25104;&#23548;&#33268;&#32467;&#26524;&#30340;&#20107;&#20214;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#35786;&#26029;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#20855;&#26377;&#23398;&#20064;&#22686;&#24378;&#32452;&#20214;&#65288;LEC&#65289;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#65288;CPS&#65289;&#20013;&#35786;&#26029;&#21644;&#20462;&#22797;&#36816;&#34892;&#26102;&#23646;&#24615;&#36829;&#35268;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;LEC&#30340;&#39640;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#65288;&#20363;&#22914;CPS&#21160;&#24577;&#65289;&#32534;&#30721;&#25104;&#21487;&#29983;&#25104;&#26377;&#29992;&#20462;&#22797;&#24314;&#35758;&#30340;&#21487;&#25193;&#23637;&#23454;&#38469;&#22240;&#26524;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#35786;&#26029;&#38598;&#20013;&#20110;LEC&#30340;&#36755;&#20837;/&#36755;&#20986;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;LEC&#30340;&#21738;&#20010;&#36755;&#20837;/&#36755;&#20986;&#34892;&#20026;&#23376;&#38598;&#26159;&#23548;&#33268;&#23646;&#24615;&#36829;&#35268;&#30340;&#30495;&#27491;&#21407;&#22240;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#21103;&#20135;&#21697;&#26159;&#30699;&#27491;&#35782;&#21035;&#20986;&#26377;&#38382;&#39064;&#30340;&#34892;&#20026;&#26469;&#20462;&#22797;&#36816;&#34892;&#26102;&#23646;&#24615;&#30340;LEC&#30340;&#21453;&#20107;&#23454;&#29256;&#26412;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#27493;&#35786;&#26029;&#27969;&#31243;&#65306;&#65288;1&#65289;&#26500;&#24314;Halpern-Pearl&#22240;&#26524;&#27169;&#22411;&#20197;&#21453;&#26144;&#23646;&#24615;&#32467;&#26524;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models of actual causality leverage domain knowledge to generate convincing diagnoses of events that caused an outcome. It is promising to apply these models to diagnose and repair run-time property violations in cyber-physical systems (CPS) with learning-enabled components (LEC). However, given the high diversity and complexity of LECs, it is challenging to encode domain knowledge (e.g., the CPS dynamics) in a scalable actual causality model that could generate useful repair suggestions. In this paper, we focus causal diagnosis on the input/output behaviors of LECs. Specifically, we aim to identify which subset of I/O behaviors of the LEC is an actual cause for a property violation. An important by-product is a counterfactual version of the LEC that repairs the run-time property by fixing the identified problematic behaviors. Based on this insights, we design a two-step diagnostic pipeline: (1) construct and Halpern-Pearl causality model that reflects the dependency of property outcom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27744;&#21270;&#31639;&#23376;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26631;&#20934;&#26469;&#36873;&#25321;&#25110;&#35774;&#35745;&#27744;&#21270;&#31639;&#23376;&#12290;</title><link>http://arxiv.org/abs/2304.01575</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#27744;&#21270;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The expressive power of pooling in Graph Neural Networks. (arXiv:2304.01575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27744;&#21270;&#31639;&#23376;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26631;&#20934;&#26469;&#36873;&#25321;&#25110;&#35774;&#35745;&#27744;&#21270;&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#65292;&#20998;&#23618;&#27744;&#21270;&#31639;&#23376;&#36890;&#36807;&#21019;&#24314;&#22270;&#32467;&#26500;&#21644;&#20854;&#39030;&#28857;&#29305;&#24449;&#30340;&#26412;&#22320;&#25688;&#35201;&#26469;&#29983;&#25104;&#36755;&#20837;&#25968;&#25454;&#30340;&#26356;&#31895;&#31961;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#24050;&#32463;&#33268;&#21147;&#20110;&#30740;&#31350;GNN&#20013;&#28040;&#24687;&#20256;&#36882;&#65288;MP&#65289;&#23618;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#20851;&#20110;&#27744;&#21270;&#31639;&#23376;&#22914;&#20309;&#24433;&#21709;GNN&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#26368;&#36817;&#22312;&#26377;&#25928;&#27744;&#21270;&#31639;&#23376;&#30340;&#35774;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26631;&#20934;&#26469;&#27604;&#36739;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#36275;&#22815;&#30340;&#26465;&#20214;&#20351;&#27744;&#21270;&#31639;&#23376;&#22312;&#20854;&#20043;&#21069;&#30340;MP&#23618;&#20013;&#23436;&#20840;&#20445;&#30041;&#34920;&#36798;&#33021;&#21147;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#36825;&#20123;&#26465;&#20214;&#20316;&#20026;&#36873;&#25321;&#29616;&#26377;&#27744;&#21270;&#31639;&#23376;&#25110;&#35774;&#35745;&#26032;&#30340;&#27744;&#21270;&#31639;&#23376;&#30340;&#36890;&#29992;&#21644;&#29702;&#35770;&#22522;&#30784;&#30340;&#26631;&#20934;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#20960;&#20010;&#29616;&#26377;&#30340;&#27744;&#21270;&#31639;&#23376;&#65292;&#24182;&#30830;&#23450;&#20102;&#37027;&#20123;&#19981;&#33021;&#28385;&#36275;&#34920;&#36798;&#24615;&#20551;&#35774;&#30340;&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Graph Neural Networks (GNNs), hierarchical pooling operators generate a coarser representation of the input data by creating local summaries of the graph structure and its vertex features. Considerable attention has been devoted to studying the expressive power of message-passing (MP) layers in GNNs, while a study on how pooling operators affect the expressivity of a GNN is still lacking. Additionally, despite the recent advances in the design of effective pooling operators, there is not a principled criterion to compare them. Our work aims to fill this gap by providing sufficient conditions for a pooling operator to fully preserve the expressive power of the MP layers before it. These conditions serve as a universal and theoretically-grounded criterion for choosing among existing pooling operators or designing new ones. Based on our theoretical findings, we reviewed several existing pooling operators and identified those that fail to satisfy the expressiveness assumptions. Finally,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#22823;&#22411;&#32593;&#32476;&#30340;&#30456;&#37051;&#30697;&#38453;&#65292;&#24182;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#25512;&#26029;&#38382;&#39064;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2303.18059</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#25512;&#26029;&#32593;&#32476;&#32467;&#26500;&#30340;&#31070;&#32463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inferring networks from time series: a neural approach. (arXiv:2303.18059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#22823;&#22411;&#32593;&#32476;&#30340;&#30456;&#37051;&#30697;&#38453;&#65292;&#24182;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#25512;&#26029;&#38382;&#39064;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#32467;&#26500;&#26159;&#35768;&#22810;&#22797;&#26434;&#29616;&#35937;&#30340;&#21160;&#24577;&#22522;&#30784;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#12289;&#39135;&#29289;&#38142;&#12289;&#30005;&#21147;&#32593;&#32476;&#21644;&#31038;&#20132;&#23186;&#20307;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32593;&#32476;&#32467;&#26500;&#36890;&#24120;&#26080;&#27861;&#30452;&#25509;&#35266;&#27979;&#21040;&#65292;&#22240;&#27492;&#24517;&#39035;&#20174;&#20854;&#32039;&#24613;&#21160;&#24577;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#23427;&#20204;&#30340;&#30456;&#20114;&#36830;&#25509;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#22823;&#22411;&#32593;&#32476;&#30456;&#37051;&#30697;&#38453;&#12290;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#21453;&#26144;&#20102;&#25512;&#26029;&#38382;&#39064;&#30340;&#38750;&#20984;&#24615;&#21644;&#25968;&#25454;&#19978;&#30340;&#22122;&#22768;&#12290;&#36825;&#26159;&#26377;&#29992;&#30340;&#65292;&#22240;&#20026;&#32593;&#32476;&#25512;&#26029;&#38382;&#39064;&#36890;&#24120;&#26159;&#27424;&#23450;&#30340;&#65292;&#24182;&#19988;&#22312;&#32593;&#32476;&#25512;&#26029;&#26041;&#27861;&#20013;&#32570;&#20047;&#36825;&#20010;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#35266;&#27979;&#20854;&#21709;&#24212;&#26029;&#30005;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#33521;&#22269;&#30005;&#21147;&#32593;&#32476;&#30340;&#32447;&#36335;&#25925;&#38556;&#20301;&#32622;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network structures underlie the dynamics of many complex phenomena, from gene regulation and foodwebs to power grids and social media. Yet, as they often cannot be observed directly, their connectivities must be inferred from observations of their emergent dynamics. In this work we present a powerful and fast computational method to infer large network adjacency matrices from time series data using a neural network. Using a neural network provides uncertainty quantification on the prediction in a manner that reflects both the non-convexity of the inference problem as well as the noise on the data. This is useful since network inference problems are typically underdetermined, and a feature that has hitherto been lacking from network inference methods. We demonstrate our method's capabilities by inferring line failure locations in the British power grid from observations of its response to a power cut. Since the problem is underdetermined, many classical statistical tools (e.g. regressio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;Wyner&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#20132;&#26367;&#26368;&#23567;&#21270;&#27714;&#35299;&#22120;&#65292;&#21253;&#25324;&#21464;&#20998;&#24418;&#24335;&#21644;&#34920;&#29616;&#24418;&#24335;&#65292;&#36890;&#36807;&#22810;&#37325;&#20056;&#25968;&#20132;&#26367;&#26041;&#21521;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#30001;&#27492;&#36896;&#25104;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.15866</link><description>&lt;p&gt;
Wyner&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#20132;&#26367;&#26368;&#23567;&#21270;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient Alternating Minimization Solvers for Wyner Multi-View Unsupervised Learning. (arXiv:2303.15866v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;Wyner&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#20132;&#26367;&#26368;&#23567;&#21270;&#27714;&#35299;&#22120;&#65292;&#21253;&#25324;&#21464;&#20998;&#24418;&#24335;&#21644;&#34920;&#29616;&#24418;&#24335;&#65292;&#36890;&#36807;&#22810;&#37325;&#20056;&#25968;&#20132;&#26367;&#26041;&#21521;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#30001;&#27492;&#36896;&#25104;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;Wyner&#36890;&#29992;&#20449;&#24687;&#26694;&#26550;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#22810;&#35270;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20844;&#24335;&#65292;&#20351;&#24471;&#21487;&#20197;&#22522;&#20110;&#20132;&#26367;&#26368;&#23567;&#21270;&#21407;&#21017;&#24320;&#21457;&#35745;&#31639;&#39640;&#25928;&#30340;&#27714;&#35299;&#22120;&#12290;&#31532;&#19968;&#31181;&#20844;&#24335;&#34987;&#31216;&#20026;&#8220;&#21464;&#20998;&#24418;&#24335;&#8221;&#65292;&#20854;&#22797;&#26434;&#24230;&#38543;&#30528;&#35270;&#22270;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#32447;&#24615;&#22686;&#38271;&#65292;&#24182;&#22522;&#20110;&#21464;&#20998;&#25512;&#29702;&#32039;&#26463;&#32538;&#21644;&#25289;&#26684;&#26391;&#26085;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#31532;&#20108;&#31181;&#20844;&#24335;&#65292;&#21363;&#8220;&#34920;&#29616;&#24418;&#24335;&#8221;&#65292;&#34987;&#35777;&#26126;&#21253;&#25324;&#24050;&#30693;&#32467;&#26524;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#21046;&#29256;&#26412;&#30340;&#22810;&#37325;&#20056;&#25968;&#20132;&#26367;&#26041;&#21521;&#31639;&#27861;&#65288;ADMM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30001;&#27492;&#20135;&#29983;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#27714;&#35299;&#22120;&#30340;&#25910;&#25947;&#24615;&#22312;&#26576;&#20123;&#30456;&#20851;&#33539;&#22260;&#20869;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we adopt Wyner common information framework for unsupervised multi-view representation learning. Within this framework, we propose two novel formulations that enable the development of computational efficient solvers based on the alternating minimization principle. The first formulation, referred to as the {\em variational form}, enjoys a linearly growing complexity with the number of views and is based on a variational-inference tight surrogate bound coupled with a Lagrangian optimization objective function. The second formulation, i.e., the {\em representational form}, is shown to include known results as special cases. Here, we develop a tailored version from the alternating direction method of multipliers (ADMM) algorithm for solving the resulting non-convex optimization problem. In the two cases, the convergence of the proposed solvers is established in certain relevant regimes. Furthermore, our empirical results demonstrate the effectiveness of the proposed methods 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#24179;&#27969;-&#25193;&#25955;-&#21560;&#38468;&#31995;&#32479;&#30340;&#39640;&#25928;&#28151;&#21512;&#27169;&#22411;&#21644;&#21457;&#29616;&#21560;&#38468;&#25668;&#21462;&#27169;&#22411;&#30340;&#21560;&#38468;&#21160;&#21147;&#23398;&#23450;&#24459;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.13555</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#24179;&#27969;-&#25193;&#25955;-&#21560;&#38468;&#31995;&#32479;&#30340;&#39640;&#25928;&#28151;&#21512;&#24314;&#27169;&#21644;&#21560;&#38468;&#27169;&#22411;&#25506;&#32034;&#65306;&#31995;&#32479;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient hybrid modeling and sorption model discovery for non-linear advection-diffusion-sorption systems: A systematic scientific machine learning approach. (arXiv:2303.13555v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#24179;&#27969;-&#25193;&#25955;-&#21560;&#38468;&#31995;&#32479;&#30340;&#39640;&#25928;&#28151;&#21512;&#27169;&#22411;&#21644;&#21457;&#29616;&#21560;&#38468;&#25668;&#21462;&#27169;&#22411;&#30340;&#21560;&#38468;&#21160;&#21147;&#23398;&#23450;&#24459;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#24179;&#27969;-&#25193;&#25955;-&#21560;&#38468;&#31995;&#32479;&#30340;&#39640;&#25928;&#28151;&#21512;&#27169;&#22411;&#21644;&#21457;&#29616;&#21560;&#38468;&#25668;&#21462;&#27169;&#22411;&#12290;&#23427;&#28436;&#31034;&#20102;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#12289;&#20276;&#38543;&#28789;&#25935;&#24230;&#20998;&#26512;&#21644;JIT&#32534;&#35793;&#30340;&#21521;&#37327;&#38597;&#21508;&#24067;&#20056;&#31215;&#65292;&#32467;&#21512;&#31354;&#38388;&#31163;&#25955;&#21644;&#33258;&#36866;&#24212;&#31215;&#20998;&#22120;&#26469;&#35757;&#32451;&#36825;&#20123;&#22797;&#26434;&#31995;&#32479;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#31232;&#30095;&#21644;&#31526;&#21495;&#22238;&#24402;&#34987;&#29992;&#26469;&#35782;&#21035;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#32570;&#22833;&#30340;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#22312;&#19968;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22266;&#23450;&#24202;&#21560;&#38468;&#30340;&#22122;&#22768;&#31361;&#30772;&#26354;&#32447;&#35266;&#27979;&#32467;&#26524;&#65292;&#24471;&#20986;&#20102;&#25311;&#21512;&#33391;&#22909;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#35813;&#30740;&#31350;&#25104;&#21151;&#22320;&#21033;&#29992;&#31232;&#30095;&#21644;&#31526;&#21495;&#22238;&#24402;&#37325;&#24314;&#20102;&#21560;&#38468;&#25668;&#21462;&#21160;&#21147;&#23398;&#65292;&#24182;&#21033;&#29992;&#30830;&#23450;&#30340;&#22810;&#39033;&#24335;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#31361;&#30772;&#26354;&#32447;&#65292;&#31361;&#26174;&#20102;&#35813;&#26694;&#26550;&#21457;&#29616;&#21560;&#38468;&#21160;&#21147;&#23398;&#23450;&#24459;&#32467;&#26500;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a systematic machine learning approach for creating efficient hybrid models and discovering sorption uptake models in non-linear advection-diffusion-sorption systems. It demonstrates an effective method to train these complex systems using gradientbased optimizers, adjoint sensitivity analysis, and JIT-compiled vector Jacobian products, combined with spatial discretization and adaptive integrators. Sparse and symbolic regression were employed to identify missing functions in the artificial neural network. The robustness of the proposed method was tested on an in-silico data set of noisy breakthrough curve observations of fixed-bed adsorption, resulting in a well-fitted hybrid model. The study successfully reconstructed sorption uptake kinetics using sparse and symbolic regression, and accurately predicted breakthrough curves using identified polynomials, highlighting the potential of the proposed framework for discovering sorption kinetic law structures.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23637;&#31034;&#65292;&#19968;&#20010;&#25317;&#26377;&#20856;&#22411;API&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#25163;&#21487;&#20197;&#20197;&#26497;&#20302;&#30340;&#37329;&#38065;&#25104;&#26412;&#31363;&#21462;GPT-2&#21644;GPT-3&#31561;LM&#30340;&#35299;&#30721;&#31639;&#27861;&#30340;&#31867;&#22411;&#21644;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.04729</link><description>&lt;p&gt;
&#35770;&#30423;&#29992;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#31639;&#27861;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
On the Risks of Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23637;&#31034;&#65292;&#19968;&#20010;&#25317;&#26377;&#20856;&#22411;API&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#25163;&#21487;&#20197;&#20197;&#26497;&#20302;&#30340;&#37329;&#38065;&#25104;&#26412;&#31363;&#21462;GPT-2&#21644;GPT-3&#31561;LM&#30340;&#35299;&#30721;&#31639;&#27861;&#30340;&#31867;&#22411;&#21644;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#29983;&#25104;&#25991;&#26412;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#21644;&#35843;&#25972;&#35299;&#30721;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#30830;&#23450;&#22914;&#20309;&#20174;LM&#29983;&#25104;&#30340;&#20869;&#37096;&#27010;&#29575;&#20998;&#24067;&#20013;&#29983;&#25104;&#25991;&#26412;&#12290;&#36873;&#25321;&#35299;&#30721;&#31639;&#27861;&#24182;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#30340;&#36807;&#31243;&#38656;&#35201;&#26174;&#33879;&#30340;&#26102;&#38388;&#12289;&#25163;&#21160;&#24037;&#20316;&#21644;&#35745;&#31639;&#65292;&#36824;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#35299;&#30721;&#31639;&#27861;&#30340;&#36523;&#20221;&#21644;&#36229;&#21442;&#25968;&#34987;&#35748;&#20026;&#26159;&#26497;&#20854;&#26377;&#20215;&#20540;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#19968;&#20010;&#25317;&#26377;&#20856;&#22411;API&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#25163;&#21487;&#20197;&#20197;&#26497;&#20302;&#30340;&#37329;&#38065;&#25104;&#26412;&#31363;&#21462;&#20854;&#35299;&#30721;&#31639;&#27861;&#30340;&#31867;&#22411;&#21644;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;API&#30340;&#27969;&#34892;LM&#26377;&#25928;&#65292;&#21253;&#25324;GPT-2&#21644;GPT-3&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21482;&#38656;&#33457;&#36153;&#20960;&#32654;&#20803;&#65292;&#20363;&#22914;0.8&#32654;&#20803;&#12289;1&#32654;&#20803;&#12289;4&#32654;&#20803;&#21644;40&#32654;&#20803;&#65292;&#23601;&#21487;&#20197;&#30423;&#21462;&#27492;&#31867;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., $\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23481;&#26131;&#23398;&#20064;&#30340;&#26679;&#26412;&#23545;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09195</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#30340;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65306;&#26131;&#20110;&#23398;&#20064;&#30340;&#26679;&#26412;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most. (arXiv:2302.09195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23481;&#26131;&#23398;&#20064;&#30340;&#26679;&#26412;&#23545;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20174;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#12290;&#38543;&#30528;&#25968;&#25454;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#65292;&#35782;&#21035;&#23545;&#23398;&#20064;&#27492;&#31867;&#34920;&#31034;&#26368;&#26377;&#29992;&#30340;&#31034;&#20363;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;SSL&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;SSL&#30340;&#20215;&#20540;&#22914;&#20309;&#37327;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#22312;&#26399;&#26395;&#24847;&#20041;&#19979;&#65292;&#23545;&#27604;SSL&#20013;&#23545;&#23398;&#20064;&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#31034;&#20363;&#26159;&#20855;&#26377;&#26368;&#30456;&#20284;&#25968;&#25454;&#22686;&#24378;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#23376;&#38598;&#30340;SSL&#30340;&#24191;&#20041;&#24615;&#33021;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23545;SSL&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#23376;&#38598;&#26159;&#23545;&#30417;&#30563;&#23398;&#20064;&#20570;&#20986;&#26368;&#23567;&#36129;&#29486;&#30340;&#23376;&#38598;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23376;&#38598;&#22312;CIFAR100&#12289;CIFAR&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#23376;&#38598;3%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required for learning high-quality representations. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of SSL on such subsets. Empirically, we discover, perhaps surprisingly, the subsets that contribute the most to SSL are those that contribute the least to supervised learning. Through extensive experiments, we show that our subsets outperform random subsets by more than 3% on CIFAR100, CIFAR
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#26469;&#23454;&#29616;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#30693;&#35782;&#36801;&#31227;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#38754;&#21521;&#22810;&#26679;&#24615;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.08635</link><description>&lt;p&gt;
&#38754;&#21521;&#22810;&#26679;&#24615;&#39044;&#27979;&#30340;&#29983;&#25104;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative Causal Representation Learning for Out-of-Distribution Motion Forecasting. (arXiv:2302.08635v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08635
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#26469;&#23454;&#29616;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#30693;&#35782;&#36801;&#31227;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#38754;&#21521;&#22810;&#26679;&#24615;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#26679;&#26412;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#20294;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#24456;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#23454;&#29616;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#30693;&#35782;&#36801;&#31227;&#30340;&#29983;&#25104;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#22312;&#20154;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional supervised learning methods typically assume i.i.d samples and are found to be sensitive to out-of-distribution (OOD) data. We propose Generative Causal Representation Learning (GCRL) which leverages causality to facilitate knowledge transfer under distribution shifts. While we evaluate the effectiveness of our proposed method in human trajectory prediction models, GCRL can be applied to other domains as well. First, we propose a novel causal model that explains the generative factors in motion forecasting datasets using features that are common across all environments and with features that are specific to each environment. Selection variables are used to determine which parts of the model can be directly transferred to a new environment without fine-tuning. Second, we propose an end-to-end variational learning paradigm to learn the causal mechanisms that generate observations from features. GCRL is supported by strong theoretical results that imply identifiability of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#29289;&#36136;&#24863;&#30693;&#21452;&#32819;&#38899;&#39057;&#20256;&#25773;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#28210;&#26579;&#38899;&#39057;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#22788;&#29702;&#37325;&#26500;&#19977;&#32500;&#27169;&#22411;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#29983;&#25104;&#19982;&#30495;&#23454;&#29615;&#22659;&#30456;&#31526;&#30340;&#22768;&#23398;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2302.02809</link><description>&lt;p&gt;
Listen2Scene&#65306;&#20132;&#20114;&#24335;&#29289;&#36136;&#24863;&#30693;&#21452;&#32819;&#38899;&#39057;&#20256;&#25773;&#37325;&#26500;&#19977;&#32500;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Listen2Scene: Interactive material-aware binaural soundbpropagation for reconstructed 3D scenes. (arXiv:2302.02809v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#29289;&#36136;&#24863;&#30693;&#21452;&#32819;&#38899;&#39057;&#20256;&#25773;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#28210;&#26579;&#38899;&#39057;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#22788;&#29702;&#37325;&#26500;&#19977;&#32500;&#27169;&#22411;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#29983;&#25104;&#19982;&#30495;&#23454;&#29615;&#22659;&#30456;&#31526;&#30340;&#22768;&#23398;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#21644;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#24212;&#29992;&#30340;&#31471;&#21040;&#31471;&#21452;&#32819;&#38899;&#39057;&#28210;&#26579;&#26041;&#27861;&#65288;Listen2Scene&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21452;&#32819;&#22768;&#23398;&#20256;&#25773;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#30495;&#23454;&#29615;&#22659;&#30340;3D&#27169;&#22411;&#30340;&#22768;&#23398;&#25928;&#26524;&#12290;&#20219;&#20309;&#28165;&#27905;&#38899;&#39057;&#25110;&#24178;&#38899;&#39057;&#37117;&#21487;&#20197;&#19982;&#29983;&#25104;&#30340;&#22768;&#23398;&#25928;&#26524;&#21367;&#31215;&#65292;&#20197;&#28210;&#26579;&#19982;&#30495;&#23454;&#29615;&#22659;&#30456;&#23545;&#24212;&#30340;&#38899;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;3D&#22330;&#26223;&#30340;&#26448;&#26009;&#21644;&#25299;&#25169;&#20449;&#24687;&#29983;&#25104;&#22330;&#26223;&#28508;&#22312;&#21521;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGAN&#65289;&#20174;&#22330;&#26223;&#28508;&#22312;&#21521;&#37327;&#29983;&#25104;&#22768;&#23398;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#37325;&#26500;&#30340;&#19977;&#32500;&#32593;&#26684;&#27169;&#22411;&#20013;&#30340;&#23380;&#27934;&#25110;&#20854;&#20182;&#20266;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#29992;&#20110;&#29983;&#25104;&#22120;&#32593;&#32476;&#20197;&#25972;&#21512;&#31354;&#38388;&#38899;&#39057;&#25928;&#26524;&#12290;&#32473;&#23450;&#28304;&#21644;&#21548;&#32773;&#20301;&#32622;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#21452;&#32819;&#22768;&#38899;&#20256;&#25773;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#30495;&#23454;&#29615;&#22659;&#31934;&#24230;&#30456;&#31526;&#30340;&#22768;&#23398;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an end-to-end binaural audio rendering approach (Listen2Scene) for virtual reality (VR) and augmented reality (AR) applications. We propose a novel neural-network-based binaural sound propagation method to generate acoustic effects for 3D models of real environments. Any clean audio or dry audio can be convolved with the generated acoustic effects to render audio corresponding to the real environment. We propose a graph neural network that uses both the material and the topology information of the 3D scenes and generates a scene latent vector. Moreover, we use a conditional generative adversarial network (CGAN) to generate acoustic effects from the scene latent vector. Our network is able to handle holes or other artifacts in the reconstructed 3D mesh model. We present an efficient cost function to the generator network to incorporate spatial audio effects. Given the source and the listener position, our learning-based binaural sound propagation approach can generate an acou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20174;&#22810;&#22495;&#25968;&#25454;&#20013;&#29983;&#25104;&#30340;&#22495;&#32034;&#24341;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#27934;&#23519;&#35270;&#35282;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02561</link><description>&lt;p&gt;
&#22495;&#32034;&#24341;&#21464;&#20998;&#36125;&#21494;&#26031;&#65306;&#21487;&#35299;&#37322;&#30340;&#22495;&#32034;&#24341;&#29992;&#20110;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation. (arXiv:2302.02561v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20174;&#22810;&#22495;&#25968;&#25454;&#20013;&#29983;&#25104;&#30340;&#22495;&#32034;&#24341;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#27934;&#23519;&#35270;&#35282;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22495;&#32034;&#24341;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22495;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#24635;&#26159;&#26377;&#36825;&#26679;&#30340;&#22495;&#32034;&#24341;&#21487;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#27010;&#29575;&#35282;&#24230;&#25552;&#20379;&#20102;&#22495;&#32034;&#24341;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#20174;&#22810;&#22495;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#22495;&#32034;&#24341;&#65292;&#20174;&#32780;&#25552;&#20379;&#39069;&#22806;&#30340;&#22495;&#20851;&#31995;&#27934;&#23519;&#65292;&#24182;&#25552;&#39640;&#22495;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23545;&#25239;&#24615;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#22312;&#24179;&#34913;&#28857;&#22788;&#25214;&#21040;&#20102;&#26368;&#20248;&#30340;&#22495;&#32034;&#24341;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#22495;&#32034;&#24341;&#65292;&#20351;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20248;&#20110;&#29616;&#26377;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Wang-ML-Lab/VDI&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have shown that leveraging domain index can significantly boost domain adaptation performance (arXiv:2007.01807, arXiv:2202.03628). However, such domain indices are not always available. To address this challenge, we first provide a formal definition of domain index from the probabilistic perspective, and then propose an adversarial variational Bayesian framework that infers domain indices from multi-domain data, thereby providing additional insight on domain relations and improving domain adaptation performance. Our theoretical analysis shows that our adversarial variational Bayesian framework finds the optimal domain index at equilibrium. Empirical results on both synthetic and real data verify that our model can produce interpretable domain indices which enable us to achieve superior performance compared to state-of-the-art domain adaptation methods. Code is available at https://github.com/Wang-ML-Lab/VDI.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#24046;&#20998;&#31169;&#26377;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#39640;&#32500;&#21327;&#26041;&#24046;&#24863;&#30693;&#22343;&#20540;&#20272;&#35745;&#12290;&#22312;Mahalanobis&#35823;&#24046;&#24230;&#37327;&#20013;&#65292;&#20063;&#23601;&#26159;&#30456;&#23545;&#20110;&#21327;&#26041;&#24046;&#30340;&#24179;&#22343;&#35823;&#24046;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#24471;$\hat \mu$&#26356;&#25509;&#36817;$\mu$&#12290;</title><link>http://arxiv.org/abs/2301.12250</link><description>&lt;p&gt;
&#39640;&#32500;&#27425;&#39640;&#26031;&#20998;&#24067;&#30340;&#24555;&#36895;&#65292;&#26679;&#26412;&#26377;&#25928;&#65292;&#20223;&#23556;&#19981;&#21464;&#31169;&#26377;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fast, Sample-Efficient, Affine-Invariant Private Mean and Covariance Estimation for Subgaussian Distributions. (arXiv:2301.12250v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#24046;&#20998;&#31169;&#26377;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#39640;&#32500;&#21327;&#26041;&#24046;&#24863;&#30693;&#22343;&#20540;&#20272;&#35745;&#12290;&#22312;Mahalanobis&#35823;&#24046;&#24230;&#37327;&#20013;&#65292;&#20063;&#23601;&#26159;&#30456;&#23545;&#20110;&#21327;&#26041;&#24046;&#30340;&#24179;&#22343;&#35823;&#24046;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#24471;$\hat \mu$&#26356;&#25509;&#36817;$\mu$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#24046;&#20998;&#31169;&#26377;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#39640;&#32500;&#21327;&#26041;&#24046;&#24863;&#30693;&#22343;&#20540;&#20272;&#35745;&#12290;&#20197;&#21069;&#24050;&#30693;&#21482;&#26377;&#25351;&#25968;&#26102;&#38388;&#20272;&#35745;&#22120;&#25165;&#33021;&#23454;&#29616;&#27492;&#20445;&#35777;&#12290;&#32473;&#23450;&#20174;&#20855;&#26377;&#26410;&#30693;&#22343;&#20540; $&#956;$ &#21644;&#21327;&#26041;&#24046; $&#931;$ &#30340;&#65288;&#20122;&#65289;&#39640;&#26031;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;$n$&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#30340; $(\varepsilon,\delta)$-&#24046;&#20998;&#24335;&#31169;&#26377;&#20272;&#35745;&#22120;&#29983;&#25104;$\tilde{\mu}$&#65292;&#20351;&#24471;&#21482;&#35201; $n \gtrsim \tfrac d {\alpha^2} + \tfrac{d \sqrt{\log 1/\delta}}{\alpha \varepsilon}+\frac{d\log 1/\delta}{\varepsilon}$&#65292;&#23601;&#28385;&#36275; $\|\mu - \tilde{\mu}\|_{\Sigma} \leq \alpha$&#12290;Mahalanobis&#35823;&#24046;&#24230;&#37327; $\|\mu - \hat{\mu}\|_{\Sigma}$ &#34913;&#37327;&#20102;$\hat \mu$&#19982;$\mu$&#22312;$\Sigma$&#30456;&#23545;&#36317;&#31163;; &#23427;&#34920;&#24449;&#20102;&#26679;&#26412;&#24179;&#22343;&#20540;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#20026;$\tilde{O}(nd^{\omega - 1} + nd/\varepsilon)$&#65292;&#20854;&#20013;$\omega &lt; 2.38$&#26159;&#30697;&#38453;&#20056;&#27861;&#25351;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992; Brown&#12289;Gaboardi&#12289;Smith&#12289;Ullman &#21644; Zakynthiadaki[BGSUZ18] &#30340;&#25351;&#25968;&#26102;&#38388;&#26041;&#27861;&#26469;&#35745;&#31639;&#38382;&#39064;&#30340;&#26368;&#20248;&#20272;&#35745;&#30340;&#36275;&#22815;&#32479;&#35745;&#37327;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36890;&#36807;&#38543;&#26426;&#32447;&#24615;&#20195;&#25968;&#26500;&#36896;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a fast, differentially private algorithm for high-dimensional covariance-aware mean estimation with nearly optimal sample complexity. Only exponential-time estimators were previously known to achieve this guarantee. Given $n$ samples from a (sub-)Gaussian distribution with unknown mean $\mu$ and covariance $\Sigma$, our $(\varepsilon,\delta)$-differentially private estimator produces $\tilde{\mu}$ such that $\|\mu - \tilde{\mu}\|_{\Sigma} \leq \alpha$ as long as $n \gtrsim \tfrac d {\alpha^2} + \tfrac{d \sqrt{\log 1/\delta}}{\alpha \varepsilon}+\frac{d\log 1/\delta}{\varepsilon}$. The Mahalanobis error metric $\|\mu - \hat{\mu}\|_{\Sigma}$ measures the distance between $\hat \mu$ and $\mu$ relative to $\Sigma$; it characterizes the error of the sample mean. Our algorithm runs in time $\tilde{O}(nd^{\omega - 1} + nd/\varepsilon)$, where $\omega &lt; 2.38$ is the matrix multiplication exponent.  We adapt an exponential-time approach of Brown, Gaboardi, Smith, Ullman, and Zakynthi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#37319;&#29992;&#21069;&#32512;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11719</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#32435;&#20837;&#25991;&#26723;&#25688;&#35201;&#29983;&#25104;&#20013;&#65306;&#22522;&#20110;GPT-2&#30340;&#21069;&#32512;&#35843;&#25972;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#37319;&#29992;&#21069;&#32512;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#22312;&#25991;&#26723;&#25688;&#35201;&#25216;&#26415;&#24471;&#21040;&#20102;&#24456;&#22823;&#30340;&#21457;&#23637;&#65292;&#20294;&#26159;&#29983;&#25104;&#30340;&#25688;&#35201;&#21644;&#21407;&#22987;&#25991;&#26412;&#20043;&#38388;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#20173;&#28982;&#26102;&#26377;&#21457;&#29983;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#37319;&#29992;&#25552;&#31034;&#26469;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20855;&#20307;&#30740;&#31350;&#20102;&#21069;&#32512;&#35843;&#25972;&#65292;&#23427;&#20351;&#29992;&#19968;&#32452;&#21487;&#35757;&#32451;&#30340;&#36830;&#32493;&#21069;&#32512;&#25552;&#31034;&#21644;&#31163;&#25955;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#24110;&#21161;&#25688;&#35201;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#35757;&#32451;&#30340;&#21069;&#32512;&#21487;&#20197;&#24110;&#21161;&#25688;&#35201;&#27169;&#22411;&#20934;&#30830;&#22320;&#20174;&#31163;&#25955;&#25552;&#31034;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#20174;&#32780;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#36825;&#20123;&#25688;&#35201;&#22312;&#20107;&#23454;&#19978;&#19982;&#31163;&#25955;&#25552;&#31034;&#19968;&#33268;&#12290;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;ROUGE&#25913;&#36827;&#34920;&#26126;&#65292;&#23558;&#20107;&#23454;&#30693;&#35782;&#26126;&#30830;&#22320;&#28155;&#21152;&#21040;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#20013;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24212;&#29992;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great development of document summarisation techniques nowadays, factual inconsistencies between the generated summaries and the original texts still occur from time to time. This study explores the possibility of adopting prompts to incorporate factual knowledge into generated summaries. We specifically study prefix-tuning that uses a set of trainable continuous prefix prompts together with discrete natural language prompts to aid summary generation. Experimental results demonstrate that the trainable prefixes can help the summarisation model extract information from discrete prompts precisely, thus generating knowledge-preserving summaries that are factually consistent with the discrete prompts. The ROUGE improvements of the generated summaries indicate that explicitly adding factual knowledge into the summarisation process could boost the overall performance, showing great potential for applying it to other natural language processing tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#39044;&#27979;&#31639;&#27861;&#20013;&#22810;&#32452;&#20844;&#24179;&#24615;&#21644;&#20266;&#38543;&#26426;&#24615;&#30340;&#32852;&#31995;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#22810;&#26657;&#20934;&#31639;&#27861;&#21644;&#23454;&#20540;&#20989;&#25968;&#26680;&#24341;&#29702;&#35777;&#26126;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.08837</link><description>&lt;p&gt;
&#20174;&#20266;&#38543;&#26426;&#24615;&#21040;&#22810;&#32452;&#20844;&#24179;&#24615;&#20877;&#21040;&#22238;&#26469;
&lt;/p&gt;
&lt;p&gt;
From Pseudorandomness to Multi-Group Fairness and Back. (arXiv:2301.08837v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#39044;&#27979;&#31639;&#27861;&#20013;&#22810;&#32452;&#20844;&#24179;&#24615;&#21644;&#20266;&#38543;&#26426;&#24615;&#30340;&#32852;&#31995;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#22810;&#26657;&#20934;&#31639;&#27861;&#21644;&#23454;&#20540;&#20989;&#25968;&#26680;&#24341;&#29702;&#35777;&#26126;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#27979;&#31639;&#27861;&#20013;&#22810;&#32452;&#20844;&#24179;&#24615;&#21644;&#27844;&#38706;-&#38887;&#24615;&#21644;&#22270;&#24418;&#35268;&#21017;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22312;&#19968;&#20123;&#21442;&#25968;&#33539;&#22260;&#20869;&#25552;&#20379;&#20102;&#26032;&#30340;&#22810;&#26657;&#20934;&#21644;&#23454;&#20540;&#20989;&#25968;&#26680;&#24341;&#29702;&#35777;&#26126;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We identify and explore connections between the recent literature on multi-group fairness for prediction algorithms and the pseudorandomness notions of leakage-resilience and graph regularity. We frame our investigation using new, statistical distance-based variants of multicalibration that are closely related to the concept of outcome indistinguishability. Adopting this perspective leads us naturally not only to our graph theoretic results, but also to new, more efficient algorithms for multicalibration in certain parameter regimes and a novel proof of a hardcore lemma for real-valued functions.
&lt;/p&gt;</description></item><item><title>SEQUENT&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#37327;&#23376;&#22686;&#24378;&#35757;&#32451;&#23454;&#29616;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36861;&#28335;&#24615;&#65292;&#21487;&#20197;&#36861;&#36394;&#21040;&#25152;&#36873;&#30340;&#30005;&#36335;&#26550;&#26500;&#21644;&#21442;&#25968;&#21270;&#23545;&#27169;&#22411;&#30340;&#36129;&#29486;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26080;&#27861;&#20005;&#26684;&#20998;&#31163;&#32463;&#20856;&#21644;&#37327;&#23376;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.02601</link><description>&lt;p&gt;
SEQUENT: &#24207;&#21015;&#37327;&#23376;&#22686;&#24378;&#35757;&#32451;&#23454;&#29616;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21487;&#36861;&#28335;&#24615;
&lt;/p&gt;
&lt;p&gt;
SEQUENT: Towards Traceable Quantum Machine Learning using Sequential Quantum Enhanced Training. (arXiv:2301.02601v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02601
&lt;/p&gt;
&lt;p&gt;
SEQUENT&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#37327;&#23376;&#22686;&#24378;&#35757;&#32451;&#23454;&#29616;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36861;&#28335;&#24615;&#65292;&#21487;&#20197;&#36861;&#36394;&#21040;&#25152;&#36873;&#30340;&#30005;&#36335;&#26550;&#26500;&#21644;&#21442;&#25968;&#21270;&#23545;&#27169;&#22411;&#30340;&#36129;&#29486;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26080;&#27861;&#20005;&#26684;&#20998;&#31163;&#32463;&#20856;&#21644;&#37327;&#23376;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#23558;&#37327;&#23376;&#35745;&#31639;&#31561;&#26032;&#30340;&#35745;&#31639;&#33539;&#24335;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#32500;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30446;&#21069;&#23578;&#26080;&#27861;&#20351;&#29992;&#32431;&#37327;&#23376;&#30828;&#20214;&#36827;&#34892;&#27714;&#35299;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20351;&#29992;&#32463;&#20856;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#28151;&#21512;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#20173;&#38656;&#35201;&#25506;&#32034;&#26377;&#30410;&#30340;&#30005;&#36335;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#36319;&#36394;&#25152;&#36873;&#30005;&#36335;&#26550;&#26500;&#21644;&#21442;&#25968;&#21270;&#30340;&#24433;&#21709;&#23545;&#20110;&#24320;&#21457;&#26377;&#30410;&#30340;&#28151;&#21512;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#21253;&#25324;&#21516;&#26102;&#35757;&#32451;&#20004;&#20010;&#37096;&#20998;&#30340;&#36807;&#31243;&#65292;&#22240;&#27492;&#19981;&#33021;&#20005;&#26684;&#20998;&#31163;&#32463;&#20856;&#21644;&#37327;&#23376;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26550;&#26500;&#21487;&#33021;&#20250;&#20135;&#29983;&#20855;&#26377;&#21331;&#36234;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#37319;&#29992;&#26368;&#23569;&#37327;&#30340;&#37327;&#23376;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying new computing paradigms like quantum computing to the field of machine learning has recently gained attention. However, as high-dimensional real-world applications are not yet feasible to be solved using purely quantum hardware, hybrid methods using both classical and quantum machine learning paradigms have been proposed. For instance, transfer learning methods have been shown to be successfully applicable to hybrid image classification tasks. Nevertheless, beneficial circuit architectures still need to be explored. Therefore, tracing the impact of the chosen circuit architecture and parameterization is crucial for the development of beneficially applicable hybrid methods. However, current methods include processes where both parts are trained concurrently, therefore not allowing for a strict separability of classical and quantum impact. Thus, those architectures might produce models that yield a superior prediction accuracy whilst employing the least possible quantum impact. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#32479;&#35745;&#27714;&#35299;&#22120;&#65288;DSS$^2$&#65289;&#65292;&#24212;&#29992;&#20110;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#12290;DSS$^2$&#21033;&#29992;&#36229;&#22270;&#21644;&#33410;&#28857;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26356;&#26032;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;DSS$^2$&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.01835</link><description>&lt;p&gt;
&#28145;&#24230;&#32479;&#35745;&#27714;&#35299;&#22120;&#29992;&#20110;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep Statistical Solver for Distribution System State Estimation. (arXiv:2301.01835v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#32479;&#35745;&#27714;&#35299;&#22120;&#65288;DSS$^2$&#65289;&#65292;&#24212;&#29992;&#20110;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#12290;DSS$^2$&#21033;&#29992;&#36229;&#22270;&#21644;&#33410;&#28857;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26356;&#26032;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;DSS$^2$&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20934;&#30830;&#30340;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#65288;DSSE&#65289;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#35266;&#27979;&#24615;&#19981;&#36275;&#21644;&#37197;&#30005;&#31995;&#32479;&#23494;&#24230;&#39640;&#12290;&#34429;&#28982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#26041;&#26696;&#21487;&#33021;&#26159;&#19968;&#31181;&#36873;&#25321;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#65292;&#23427;&#20204;&#22312;DSSE&#20013;&#21463;&#21040;&#24433;&#21709;&#12290;&#23454;&#38469;&#19978;&#65292;&#37197;&#30005;&#31995;&#32479;&#20013;&#30340;&#27979;&#37327;&#24448;&#24448;&#26159;&#22024;&#26434;&#12289;&#25439;&#22351;&#21644;&#19981;&#21487;&#29992;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#29992;&#20110;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#30340;&#28145;&#24230;&#32479;&#35745;&#27714;&#35299;&#22120;&#65288;DSS$^2$&#65289;&#65292;&#23427;&#32771;&#34385;&#21040;&#20102;&#37197;&#30005;&#31995;&#32479;&#30340;&#32593;&#32476;&#32467;&#26500;&#21644;&#29289;&#29702;&#25511;&#21046;&#21151;&#29575;&#27969;&#26041;&#31243;&#12290;DSS$^2$&#21033;&#29992;&#36229;&#22270;&#26469;&#34920;&#31034;&#37197;&#30005;&#31995;&#32479;&#30340;&#24322;&#26500;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#20197;&#33410;&#28857;&#20026;&#20013;&#24515;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26356;&#26032;&#20854;&#28508;&#22312;&#34920;&#31034;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20197;&#23398;&#20064;&#20248;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;DSS$^2$&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DSS$^2$&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#20272;&#35745;&#31934;&#24230;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implementing accurate Distribution System State Estimation (DSSE) faces several challenges, among which the lack of observability and the high density of the distribution system. While data-driven alternatives based on Machine Learning models could be a choice, they suffer in DSSE because of the lack of labeled data. In fact, measurements in the distribution system are often noisy, corrupted, and unavailable. To address these issues, we propose the Deep Statistical Solver for Distribution System State Estimation (DSS$^2$), a deep learning model based on graph neural networks (GNNs) that accounts for the network structure of the distribution system and for the physical governing power flow equations. DSS$^2$ leverages hypergraphs to represent the heterogeneous components of the distribution systems and updates their latent representations via a node-centric message-passing scheme. A weakly supervised learning approach is put forth to train the DSS$^2$ in a learning-to-optimize fashion w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#21046;&#24230;&#21464;&#21270;&#19979;&#23545;&#26102;&#24207;&#38754;&#26495;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#36827;&#34892;&#25490;&#21517;&#12290;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#24182;&#32467;&#21512;dropout&#25216;&#26415;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#21160;&#24577;&#29305;&#24449;&#20013;&#21644;&#21017;&#26159;&#19968;&#31181;&#39640;&#25928;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#23601;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2301.00790</link><description>&lt;p&gt;
&#38754;&#21521;&#26102;&#24207;&#34920;&#26684;&#25968;&#25454;&#30340;&#21160;&#24577;&#29305;&#24449;&#24037;&#31243;&#21644;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#22312;&#21046;&#24230;&#21464;&#21270;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dynamic Feature Engineering and model selection methods for temporal tabular datasets with regime changes. (arXiv:2301.00790v2 [q-fin.CP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#21046;&#24230;&#21464;&#21270;&#19979;&#23545;&#26102;&#24207;&#38754;&#26495;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#36827;&#34892;&#25490;&#21517;&#12290;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#24182;&#32467;&#21512;dropout&#25216;&#26415;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#21160;&#24577;&#29305;&#24449;&#20013;&#21644;&#21017;&#26159;&#19968;&#31181;&#39640;&#25928;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#23601;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20005;&#37325;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#26102;&#24207;&#38754;&#26495;&#25968;&#25454;&#38598;&#26159;&#22256;&#38590;&#30340;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#30340;&#27169;&#22411;&#22312;&#21046;&#24230;&#21464;&#21270;&#19979;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#21046;&#24230;&#21464;&#21270;&#19979;&#23545;&#26102;&#24207;&#38754;&#26495;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#36827;&#34892;&#25490;&#21517;&#12290;&#31649;&#36947;&#35780;&#20272;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#21644;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#31616;&#21333;&#29305;&#24449;&#24037;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;dropout&#30340;GBDT&#27169;&#22411;&#20855;&#26377;&#39640;&#24615;&#33021;&#12289;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19988;&#30456;&#23545;&#22797;&#26434;&#24230;&#36739;&#20302;&#12289;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#22312;&#39044;&#27979;&#21518;&#22788;&#29702;&#20013;&#29992;&#20110;&#22686;&#24378;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#29305;&#24449;&#20013;&#21644;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#23601;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39640;&#25928;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of deep learning algorithms to temporal panel datasets is difficult due to heavy non-stationarities which can lead to over-fitted models that under-perform under regime changes. In this work we propose a new machine learning pipeline for ranking predictions on temporal panel datasets which is robust under regime changes of data. Different machine-learning models, including Gradient Boosting Decision Trees (GBDTs) and Neural Networks with and without simple feature engineering are evaluated in the pipeline with different settings. We find that GBDT models with dropout display high performance, robustness and generalisability with relatively low complexity and reduced computational cost. We then show that online learning techniques can be used in post-prediction processing to enhance the results. In particular, dynamic feature neutralisation, an efficient procedure that requires no retraining of models and can be applied post-prediction to any machine learning model, impr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;CSI&#30340;&#23460;&#20869;&#23384;&#22312;&#26816;&#27979;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#26102;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10802</link><description>&lt;p&gt;
BTS&#65306;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#23460;&#20869;&#20004;&#25151;&#38388;&#23384;&#22312;&#26816;&#27979;&#20013;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
BTS: Bifold Teacher-Student in Semi-Supervised Learning for Indoor Two-Room Presence Detection Under Time-Varying CSI. (arXiv:2212.10802v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;CSI&#30340;&#23460;&#20869;&#23384;&#22312;&#26816;&#27979;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#26102;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#23460;&#20869;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;CSI&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#23481;&#26131;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#22914;&#29289;&#20307;&#31227;&#21160;&#12289;&#22823;&#27668;&#22240;&#32032;&#21644;&#26426;&#22120;&#37325;&#21551;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#39044;&#27979;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#32791;&#26102;&#30340;&#26631;&#27880;&#26469;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#35774;&#35745;&#19968;&#20010;&#36830;&#32493;&#30417;&#25511;&#30340;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24605;&#20102;&#19968;&#31181;&#21452;&#25240;&#21472;&#24072;&#29983;&#65288;BTS&#65289;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#23384;&#22312;&#20110;&#31995;&#32479;&#20013;&#30340;&#23384;&#22312;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;&#21407;&#22987;&#23545;&#20598;&#24072;&#29983;&#32593;&#32476;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;CSI&#20013;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#22686;&#24378;&#30340;&#24809;&#32602;&#25439;&#22833;&#20989;&#25968;&#21033;&#29992;&#29109;&#21644;&#36317;&#31163;&#27979;&#37327;&#26469;&#21306;&#20998;&#28145;&#23618;&#29305;&#24449;&#65292;&#38477;&#20302;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, indoor human presence detection based on supervised learning (SL) and channel state information (CSI) has attracted much attention. However, the existing studies that rely on spatial information of CSI are susceptible to environmental changes, such as object movement, atmospheric factors, and machine rebooting, which degrade prediction accuracy. Moreover, SL-based methods require time-consuming labeling for retraining models. Therefore, it is imperative to design a continuously monitored model life-cycle using a semi-supervised learning (SSL) based scheme. In this paper, we conceive a bifold teacher-student (BTS) learning approach for presence detection systems that combines SSL by utilizing partially labeled and unlabeled datasets. The proposed primal-dual teacher-student network intelligently learns spatial and temporal features from labeled and unlabeled CSI. Additionally, the enhanced penalized loss function leverages entropy and distance measures to distinguish dr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#21450;&#20854;&#32452;&#25104;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#38598;&#25104;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#21333;&#20010;&#23398;&#20064;&#22120;&#21644;&#38598;&#25104;&#36229;&#21442;&#25968;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20844;&#24179;&#24615;&#22312;&#38598;&#25104;&#20013;&#30340;&#32452;&#25104;&#26041;&#24335;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#38598;&#25104;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.04593</link><description>&lt;p&gt;
&#25506;&#32034;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#21450;&#20854;&#32452;&#25104;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Fairness and its Composition in Ensemble Machine Learning. (arXiv:2212.04593v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#21450;&#20854;&#32452;&#25104;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#38598;&#25104;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#21333;&#20010;&#23398;&#20064;&#22120;&#21644;&#38598;&#25104;&#36229;&#21442;&#25968;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20844;&#24179;&#24615;&#22312;&#38598;&#25104;&#20013;&#30340;&#32452;&#25104;&#26041;&#24335;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#38598;&#25104;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#22312;&#29616;&#20195;&#31038;&#20250;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#23384;&#22312;&#21487;&#33021;&#22522;&#20110;&#31181;&#26063;&#12289;&#24615;&#21035;&#12289;&#24180;&#40836;&#31561;&#22240;&#32032;&#24433;&#21709;&#23569;&#25968;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#35768;&#22810;&#30740;&#31350;&#25552;&#20986;&#20102;&#34913;&#37327;&#21644;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#20294;&#29616;&#23454;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#30001;&#22810;&#20010;&#29420;&#31435;&#25110;&#30456;&#20851;&#32852;&#30340;&#23398;&#20064;&#22120;&#32452;&#25104;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#65289;&#65292;&#20854;&#20013;&#30340;&#20844;&#24179;&#24615;&#32452;&#21512;&#26041;&#24335;&#38750;&#24120;&#22797;&#26434;&#12290;&#20844;&#24179;&#24615;&#22914;&#20309;&#22312;&#38598;&#25104;&#20013;&#32452;&#21512;&#65311;&#23398;&#20064;&#22120;&#23545;&#38598;&#25104;&#26368;&#32456;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#26159;&#20160;&#20040;&#65311;&#20844;&#24179;&#30340;&#23398;&#20064;&#22120;&#26159;&#21542;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#38598;&#25104;&#65311;&#27492;&#22806;&#65292;&#30740;&#31350;&#34920;&#26126;&#36229;&#21442;&#25968;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#20135;&#29983;&#24433;&#21709;&#12290;&#38598;&#25104;&#36229;&#21442;&#25968;&#26356;&#20026;&#22797;&#26434;&#65292;&#22240;&#20026;&#23427;&#20204;&#24433;&#21709;&#23398;&#20064;&#22120;&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#38598;&#25104;&#20013;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;&#28145;&#20837;&#20102;&#35299;&#38598;&#25104;&#36229;&#21442;&#25968;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#23558;&#26377;&#21161;&#20110;&#31243;&#24207;&#21592;&#35774;&#35745;&#20844;&#24179;&#30340;&#38598;&#25104;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#21450;&#20854;&#32452;&#25104;&#26041;&#24335;&#65292;&#24182;&#22522;&#20110;&#26465;&#20214;&#20154;&#21475;&#24179;&#31561;&#30340;&#27010;&#24565;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#38598;&#25104;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#21333;&#20010;&#23398;&#20064;&#22120;&#21644;&#38598;&#25104;&#36229;&#21442;&#25968;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20844;&#24179;&#24615;&#22312;&#38598;&#25104;&#20013;&#30340;&#32452;&#25104;&#26041;&#24335;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#38598;&#25104;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) software has been widely adopted in modern society, with reported fairness implications for minority groups based on race, sex, age, etc. Many recent works have proposed methods to measure and mitigate algorithmic bias in ML models. The existing approaches focus on single classifier-based ML models. However, real-world ML models are often composed of multiple independent or dependent learners in an ensemble (e.g., Random Forest), where the fairness composes in a non-trivial way. How does fairness compose in ensembles? What are the fairness impacts of the learners on the ultimate fairness of the ensemble? Can fair learners result in an unfair ensemble? Furthermore, studies have shown that hyperparameters influence the fairness of ML models. Ensemble hyperparameters are more complex since they affect how learners are combined in different categories of ensembles. Understanding the impact of ensemble hyperparameters on fairness will help programmers design fair ensem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;VISEM-Tracking&#65292;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#19987;&#23478;&#20998;&#26512;&#30340;&#31934;&#23376;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#20197;&#20379;&#26131;&#20110;&#35775;&#38382;&#21644;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.02842</link><description>&lt;p&gt;
VISEM-Tracking&#65292;&#19968;&#20221;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VISEM-Tracking, a human spermatozoa tracking dataset. (arXiv:2212.02842v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;VISEM-Tracking&#65292;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#19987;&#23478;&#20998;&#26512;&#30340;&#31934;&#23376;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#20197;&#20379;&#26131;&#20110;&#35775;&#38382;&#21644;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#23376;&#36816;&#21160;&#30340;&#25163;&#21160;&#35780;&#20272;&#38656;&#35201;&#26174;&#24494;&#38236;&#35266;&#23519;&#65292;&#30001;&#20110;&#25152;&#35266;&#23519;&#30340;&#31934;&#23376;&#22312;&#35270;&#37326;&#20013;&#30340;&#24555;&#36895;&#31227;&#21160;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#27491;&#30830;&#30340;&#32467;&#26524;&#65292;&#25163;&#21160;&#35780;&#20272;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#22521;&#35757;&#12290;&#22240;&#27492;&#65292;&#22312;&#35786;&#25152;&#20013;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#31934;&#23376;&#20998;&#26512;&#65288;CASA&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#29992;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#26469;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;VISEM-Tracking&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20&#20010;30&#31186;&#30340;&#35270;&#39057;&#35760;&#24405;&#65288;&#21253;&#25324;29,196&#24103;&#65289;&#30340;&#28287;&#24615;&#31934;&#23376;&#21046;&#22791;&#29289;&#65292;&#20855;&#22791;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#35813;&#39046;&#22495;&#30340;&#19987;&#23478;&#20998;&#26512;&#30340;&#19968;&#32452;&#31934;&#23376;&#29305;&#24449;&#12290;&#38500;&#20102;&#24050;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#21098;&#36753;&#65292;&#20197;&#20415;&#36890;&#36807;&#33258;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#36731;&#26494;&#35775;&#38382;&#21644;&#20998;&#26512;&#25968;&#25454;&#12290;&#20316;&#20026;&#26412;&#25991;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#32447;&#31934;&#23376;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A manual assessment of sperm motility requires microscopy observation, which is challenging due to the fast-moving spermatozoa in the field of view. To obtain correct results, manual evaluation requires extensive training. Therefore, computer-assisted sperm analysis (CASA) has become increasingly used in clinics. Despite this, more data is needed to train supervised machine learning approaches in order to improve accuracy and reliability in the assessment of sperm motility and kinematics. In this regard, we provide a dataset called VISEM-Tracking with 20 video recordings of 30 seconds (comprising 29,196 frames) of wet sperm preparations with manually annotated bounding-box coordinates and a set of sperm characteristics analyzed by experts in the domain. In addition to the annotated data, we provide unlabeled video clips for easy-to-use access and analysis of the data via methods such as selfor unsupervised learning. As part of this paper, we present baseline sperm detection performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CrossSplit&#30340;&#26032;&#35757;&#32451;&#31243;&#24207;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#21449;&#20998;&#21106;&#30340;&#26631;&#31614;&#20462;&#27491;&#21644;&#21322;&#30417;&#30563;&#35757;&#32451;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#32531;&#35299;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#26631;&#31614;&#22122;&#22768;&#35760;&#24518;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.01674</link><description>&lt;p&gt;
CrossSplit: &#36890;&#36807;&#25968;&#25454;&#20998;&#21106;&#32531;&#35299;&#26631;&#31614;&#22122;&#22768;&#35760;&#24518;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
CrossSplit: Mitigating Label Noise Memorization through Data Splitting. (arXiv:2212.01674v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CrossSplit&#30340;&#26032;&#35757;&#32451;&#31243;&#24207;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#21449;&#20998;&#21106;&#30340;&#26631;&#31614;&#20462;&#27491;&#21644;&#21322;&#30417;&#30563;&#35757;&#32451;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#32531;&#35299;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#26631;&#31614;&#22122;&#22768;&#35760;&#24518;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#24773;&#20917;&#19979;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#40065;&#26834;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#29616;&#26377;&#30340;&#26631;&#31614;&#20462;&#27491;&#21644;&#20849;&#21516;&#25945;&#23398;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31243;&#24207;&#8212;&#8212;CrossSplit&#65292;&#20197;&#32531;&#35299;&#22122;&#22768;&#26631;&#31614;&#30340;&#35760;&#24518;&#38382;&#39064;&#12290;CrossSplit&#20351;&#29992;&#22312;&#20004;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#19981;&#30456;&#20132;&#37096;&#20998;&#19978;&#35757;&#32451;&#30340;&#19968;&#23545;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#32452;&#21512;&#20102;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;(i)&#20132;&#21449;&#20998;&#21106;&#26631;&#31614;&#20462;&#27491;&#65306;&#30001;&#20110;&#22312;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#19981;&#33021;&#35760;&#24518;&#26469;&#33258;&#20854;&#20182;&#37096;&#20998;&#30340;&#31034;&#20363;-&#26631;&#31614;&#23545;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#23545;&#31561;&#32593;&#32476;&#30340;&#39044;&#27979;&#24179;&#28369;&#35843;&#25972;&#27599;&#20010;&#32593;&#32476;&#21576;&#29616;&#30340;&#35757;&#32451;&#26631;&#31614;&#65307;(ii)&#20132;&#21449;&#20998;&#21106;&#21322;&#30417;&#30563;&#35757;&#32451;&#65306;&#22312;&#19968;&#20010;&#37096;&#20998;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#32593;&#32476;&#20063;&#20351;&#29992;&#21478;&#19968;&#20010;&#37096;&#20998;&#30340;&#26410;&#26631;&#35760;&#36755;&#20837;&#12290;&#22312;CIFAR-10&#12289;CIFAR-100&#12289;Tiny-ImageNet&#21644;mini-WebVision&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#22122;&#22768;&#21644;&#24178;&#25200;&#19979;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We approach the problem of improving robustness of deep learning algorithms in the presence of label noise. Building upon existing label correction and co-teaching methods, we propose a novel training procedure to mitigate the memorization of noisy labels, called CrossSplit, which uses a pair of neural networks trained on two disjoint parts of the labelled dataset. CrossSplit combines two main ingredients: (i) Cross-split label correction. The idea is that, since the model trained on one part of the data cannot memorize example-label pairs from the other part, the training labels presented to each network can be smoothly adjusted by using the predictions of its peer network; (ii) Cross-split semi-supervised training. A network trained on one part of the data also uses the unlabeled inputs of the other part. Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and mini-WebVision datasets demonstrate that our method can outperform the current state-of-the-art in a wide range of no
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;QR&#20998;&#35299;&#30340;&#26102;&#38388;&#20559;&#31227;&#36873;&#25321;&#25216;&#26415;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#33988;&#27700;&#27744;&#30697;&#38453;&#31209;&#26469;&#25552;&#39640;&#24615;&#33021;&#20934;&#30830;&#24615;&#65292;&#22312;&#27169;&#25311;&#30828;&#20214;&#33988;&#27700;&#27744;&#35745;&#31639;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.17095</link><description>&lt;p&gt;
&#22522;&#20110;QR&#20998;&#35299;&#30340;&#26102;&#38388;&#20559;&#31227;&#36873;&#25321;&#25216;&#26415;&#22312;&#33988;&#27700;&#27744;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Time-shift selection for reservoir computing using a rank-revealing QR algorithm. (arXiv:2211.17095v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;QR&#20998;&#35299;&#30340;&#26102;&#38388;&#20559;&#31227;&#36873;&#25321;&#25216;&#26415;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#33988;&#27700;&#27744;&#30697;&#38453;&#31209;&#26469;&#25552;&#39640;&#24615;&#33021;&#20934;&#30830;&#24615;&#65292;&#22312;&#27169;&#25311;&#30828;&#20214;&#33988;&#27700;&#27744;&#35745;&#31639;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33988;&#27700;&#27744;&#35745;&#31639;&#26159;&#19968;&#31181;&#20165;&#23545;&#36755;&#20986;&#23618;&#36827;&#34892;&#35757;&#32451;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#24050;&#32463;&#22312;&#38750;&#32447;&#24615;&#31995;&#32479;&#39044;&#27979;&#21644;&#25511;&#21046;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#33988;&#27700;&#27744;&#20013;&#21152;&#20837;&#26102;&#38388;&#20559;&#31227;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;QR&#20998;&#35299;&#31639;&#27861;&#26368;&#22823;&#21270;&#33988;&#27700;&#27744;&#30697;&#38453;&#31209;&#26469;&#36873;&#25321;&#26102;&#38388;&#20559;&#31227;&#30340;&#25216;&#26415;&#12290;&#35813;&#25216;&#26415;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#65292;&#19981;&#38656;&#35201;&#31995;&#32479;&#27169;&#22411;&#65292;&#22240;&#27492;&#30452;&#25509;&#36866;&#29992;&#20110;&#27169;&#25311;&#30828;&#20214;&#33988;&#27700;&#27744;&#35745;&#31639;&#12290;&#25105;&#20204;&#23558;&#26102;&#38388;&#20559;&#31227;&#36873;&#25321;&#25216;&#26415;&#24212;&#29992;&#20110;&#20004;&#31181;&#31867;&#22411;&#30340;&#33988;&#27700;&#27744;&#35745;&#31639;&#65306;&#22522;&#20110;&#20809;&#30005;&#25391;&#33633;&#22120;&#21644;&#20256;&#32479;&#36882;&#24402;&#32593;&#32476;&#30340;$tanh$&#28608;&#27963;&#20989;&#25968;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#38543;&#26426;&#36873;&#25321;&#26102;&#38388;&#20559;&#31227;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir computing, a recurrent neural network paradigm in which only the output layer is trained, has demonstrated remarkable performance on tasks such as prediction and control of nonlinear systems. Recently, it was demonstrated that adding time-shifts to the signals generated by a reservoir can provide large improvements in performance accuracy. In this work, we present a technique to choose the time-shifts by maximizing the rank of the reservoir matrix using a rank-revealing QR algorithm. This technique, which is not task dependent, does not require a model of the system, and therefore is directly applicable to analog hardware reservoir computers. We demonstrate our time-shift selection technique on two types of reservoir computer: one based on an opto-electronic oscillator and the traditional recurrent network with a $tanh$ activation function. We find that our technique provides improved accuracy over random time-shift selection in essentially all cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#37492;&#21035;&#22120;&#24341;&#23548;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35780;&#20998;&#35757;&#32451;&#20043;&#21518;&#35757;&#32451;&#37492;&#21035;&#22120;&#65292;&#20351;&#27169;&#22411;&#35780;&#20272;&#26356;&#21152;&#20934;&#30830;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#22312; ImageNet 256x256 &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102; FID 1.83 &#21644;&#21484;&#22238;&#29575; 0.64 &#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#39564;&#35777;&#25968;&#25454;&#30340; FID &#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.17091</link><description>&lt;p&gt;
&#21033;&#29992;&#37492;&#21035;&#22120;&#24341;&#23548;&#22312;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#23436;&#21892;&#29983;&#25104;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models. (arXiv:2211.17091v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#37492;&#21035;&#22120;&#24341;&#23548;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35780;&#20998;&#35757;&#32451;&#20043;&#21518;&#35757;&#32451;&#37492;&#21035;&#22120;&#65292;&#20351;&#27169;&#22411;&#35780;&#20272;&#26356;&#21152;&#20934;&#30830;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#22312; ImageNet 256x256 &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102; FID 1.83 &#21644;&#21484;&#22238;&#29575; 0.64 &#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#39564;&#35777;&#25968;&#25454;&#30340; FID &#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#8220;&#37492;&#21035;&#22120;&#24341;&#23548;&#8221;&#26041;&#27861;&#26088;&#22312;&#25913;&#21892;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#37492;&#21035;&#22120;&#65292;&#26126;&#30830;&#22320;&#30417;&#30563;&#21435;&#22122;&#26679;&#26412;&#36335;&#24452;&#26159;&#21542;&#30495;&#23454;&#12290;&#19982; GAN &#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#32852;&#21512;&#35757;&#32451;&#35780;&#20998;&#21644;&#37492;&#21035;&#22120;&#32593;&#32476;&#12290;&#30456;&#21453;&#65292;&#22312;&#35780;&#20998;&#35757;&#32451;&#20043;&#21518;&#35757;&#32451;&#37492;&#21035;&#22120;&#65292;&#20351;&#37492;&#21035;&#22120;&#35757;&#32451;&#31283;&#23450;&#19988;&#24555;&#36895;&#25910;&#25947;&#12290;&#22312;&#26679;&#26412;&#29983;&#25104;&#20013;&#65292;&#25105;&#20204;&#21521;&#39044;&#35757;&#32451;&#30340;&#35780;&#20998;&#28155;&#21152;&#19968;&#20010;&#36741;&#21161;&#39033;&#20197;&#27450;&#39575;&#37492;&#21035;&#22120;&#12290;&#35813;&#39033;&#23558;&#27169;&#22411;&#35780;&#20998;&#30699;&#27491;&#20026;&#26368;&#20248;&#37492;&#21035;&#22120;&#22788;&#30340;&#25968;&#25454;&#35780;&#20998;&#65292;&#36825;&#24847;&#21619;&#30528;&#37492;&#21035;&#22120;&#20197;&#34917;&#20805;&#30340;&#26041;&#24335;&#24110;&#21161;&#26356;&#22909;&#22320;&#35780;&#20272;&#20998;&#25968;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#22312; ImageNet 256x256 &#19978;&#23454;&#29616;&#20102; FID 1.83 &#21644;&#21484;&#22238;&#29575; 0.64 &#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#39564;&#35777;&#25968;&#25454;&#30340; FID&#65288;1.68&#65289;&#21644;&#21484;&#22238;&#29575;&#65288;0.66&#65289;&#12290;&#25105;&#20204;&#22312; https://github.com/alsdudrla10/DG &#19978;&#20844;&#24320;&#20102;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.
&lt;/p&gt;</description></item><item><title>SimVP&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#24378;&#22823;&#30340;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#22522;&#20934;&#27169;&#22411;&#65292;&#23436;&#20840;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#19988;&#27809;&#26377;&#24490;&#29615;&#32467;&#26500;&#65292;&#24182;&#19988;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;SimVP&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#33021;&#21462;&#24471;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.12509</link><description>&lt;p&gt;
SimVP: &#36739;&#20026;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
SimVP: Towards Simple yet Powerful Spatiotemporal Predictive Learning. (arXiv:2211.12509v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12509
&lt;/p&gt;
&lt;p&gt;
SimVP&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#24378;&#22823;&#30340;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#22522;&#20934;&#27169;&#22411;&#65292;&#23436;&#20840;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#19988;&#27809;&#26377;&#24490;&#29615;&#32467;&#26500;&#65292;&#24182;&#19988;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;SimVP&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#33021;&#21462;&#24471;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#22312;&#36741;&#21161;&#36755;&#20837;&#12289;&#22797;&#26434;&#31070;&#32463;&#32467;&#26500;&#21644;&#31934;&#32454;&#21270;&#35757;&#32451;&#31574;&#30053;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20027;&#27969;&#26041;&#27861;&#30340;&#31995;&#32479;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#36825;&#21487;&#33021;&#20250;&#22952;&#30861;&#23427;&#20204;&#30340;&#20415;&#25463;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;SimVP&#65292;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#19988;&#27809;&#26377;&#24490;&#29615;&#32467;&#26500;&#12289;&#20197;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20026;&#35757;&#32451;&#30446;&#26631;&#30340;&#31616;&#21333;&#26102;&#31354;&#39044;&#27979;&#22522;&#20934;&#27169;&#22411;&#12290;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#25216;&#24039;&#21644;&#31574;&#30053;&#65292;SimVP&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#33021;&#21462;&#24471;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#20174;SimVP&#20013;&#25552;&#21462;&#20102;&#20855;&#26377;&#38376;&#24335;&#26102;&#31354;&#27880;&#24847;&#21147;&#32763;&#35793;&#22120;&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SimVP&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290; &#32780;&#19988;&#35757;&#32451;&#25104;&#26412;&#26174;&#33879;&#38477;&#20302;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed remarkable advances in spatiotemporal predictive learning, incorporating auxiliary inputs, elaborate neural architectures, and sophisticated training strategies. Although impressive, the system complexity of mainstream methods is increasing as well, which may hinder the convenient applications. This paper proposes SimVP, a simple spatiotemporal predictive baseline model that is completely built upon convolutional networks without recurrent architectures and trained by common mean squared error loss in an end-to-end fashion. Without introducing any extra tricks and strategies, SimVP can achieve superior performance on various benchmark datasets. To further improve the performance, we derive variants with the gated spatiotemporal attention translator from SimVP that can achieve better performance. We demonstrate that SimVP has strong generalization and extensibility on real-world datasets through extensive experiments. The significant reduction in training cos
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;COVID-19&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#25968;&#25454;&#20998;&#26512;&#21644;&#24739;&#32773;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#40657;&#30418;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#38469;&#38544;&#31169;&#30340;&#35780;&#20272;&#65292;&#32467;&#35770;&#34920;&#26126;&#25152;&#38656;&#30340;&#38544;&#31169;&#31561;&#32423;&#21487;&#33021;&#22240;&#21463;&#21040;&#23454;&#38469;&#23041;&#32961;&#30340;&#20219;&#21153;&#32780;&#24322;&#12290;</title><link>http://arxiv.org/abs/2211.11434</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30340;&#38544;&#31169;&#65306;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;COVID-19&#26816;&#27979;&#30340;&#31169;&#26377;&#21270;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version). (arXiv:2211.11434v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;COVID-19&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#25968;&#25454;&#20998;&#26512;&#21644;&#24739;&#32773;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#40657;&#30418;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#38469;&#38544;&#31169;&#30340;&#35780;&#20272;&#65292;&#32467;&#35770;&#34920;&#26126;&#25152;&#38656;&#30340;&#38544;&#31169;&#31561;&#32423;&#21487;&#33021;&#22240;&#21463;&#21040;&#23454;&#38469;&#23041;&#32961;&#30340;&#20219;&#21153;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21487;&#20197;&#36890;&#36807;&#20351;&#22823;&#37327;&#22270;&#20687;&#24555;&#36895;&#31579;&#36873;&#26469;&#24110;&#21161;&#25239;&#20987;COVID-19&#31561;&#20840;&#29699;&#22823;&#27969;&#34892;&#30149;&#12290;&#20026;&#20102;&#22312;&#20445;&#25252;&#24739;&#32773;&#38544;&#31169;&#30340;&#21516;&#26102;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35201;&#27714;&#30340;ML&#27169;&#22411;&#12290;&#20197;&#24448;&#25506;&#32034;&#31169;&#26377;COVID-19&#27169;&#22411;&#30340;&#30740;&#31350;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#22522;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#36739;&#24369;&#25110;&#19981;&#26126;&#30830;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#19988;&#27809;&#26377;&#30740;&#31350;&#23454;&#38469;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#25913;&#36827;&#25514;&#26045;&#20197;&#35299;&#20915;&#36825;&#20123;&#31354;&#32570;&#12290;&#25105;&#20204;&#32771;&#34385;&#22825;&#29983;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#24182;&#26356;&#24191;&#27867;&#22320;&#35780;&#20272;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#20197;&#21450;&#26356;&#20005;&#26684;&#30340;&#38544;&#31169;&#39044;&#31639;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#24471;&#21040;&#40657;&#30418;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#30340;&#23454;&#38469;&#38544;&#31169;&#20272;&#35745;&#25903;&#25345;&#12290;&#24341;&#20837;&#30340;DP&#24212;&#26377;&#21161;&#20110;&#38480;&#21046;MIAs&#24102;&#26469;&#30340;&#27844;&#28431;&#23041;&#32961;&#65292;&#32780;&#25105;&#20204;&#30340;&#23454;&#38469;&#20998;&#26512;&#26159;&#31532;&#19968;&#20010;&#22312;COVID-19&#20998;&#31867;&#20219;&#21153;&#19978;&#27979;&#35797;&#36825;&#20010;&#20551;&#35774;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) can help fight pandemics like COVID-19 by enabling rapid screening of large volumes of images. To perform data analysis while maintaining patient privacy, we create ML models that satisfy Differential Privacy (DP). Previous works exploring private COVID-19 models are in part based on small datasets, provide weaker or unclear privacy guarantees, and do not investigate practical privacy. We suggest improvements to address these open gaps. We account for inherent class imbalances and evaluate the utility-privacy trade-off more extensively and over stricter privacy budgets. Our evaluation is supported by empirically estimating practical privacy through black-box Membership Inference Attacks (MIAs). The introduced DP should help limit leakage threats posed by MIAs, and our practical analysis is the first to test this hypothesis on the COVID-19 classification task. Our results indicate that needed privacy levels might differ based on the task-dependent practical threat 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRONOS&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#22522;&#20110;Wi-Fi CSI&#23454;&#29616;&#26080;&#20154;&#35774;&#22791;NLoS&#20154;&#20307;&#26816;&#27979;&#65292;&#21487;&#20197;&#21306;&#20998;&#25151;&#38388;&#20013;&#30340;&#31227;&#21160;&#20154;&#21592;&#21644;&#31354;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;NLoS&#26465;&#20214;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#25151;&#38388;&#20013;&#30340;&#20154;&#29289;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2211.10354</link><description>&lt;p&gt;
CRONOS&#65306;&#22522;&#20110;Wi-Fi CSI&#30340;&#26080;&#20154;&#35774;&#22791;NLoS&#20154;&#20307;&#26816;&#27979;&#30340;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CRONOS: Colorization and Contrastive Learning for Device-Free NLoS Human Presence Detection using Wi-Fi CSI. (arXiv:2211.10354v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRONOS&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#22522;&#20110;Wi-Fi CSI&#23454;&#29616;&#26080;&#20154;&#35774;&#22791;NLoS&#20154;&#20307;&#26816;&#27979;&#65292;&#21487;&#20197;&#21306;&#20998;&#25151;&#38388;&#20013;&#30340;&#31227;&#21160;&#20154;&#21592;&#21644;&#31354;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;NLoS&#26465;&#20214;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#25151;&#38388;&#20013;&#30340;&#20154;&#29289;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#20840;&#38754;&#26234;&#33021;&#21270;&#26381;&#21153;&#21644;&#24212;&#29992;&#30340;&#38656;&#27714;&#36805;&#36895;&#22686;&#38271;&#12290;&#36890;&#36807;&#20256;&#24863;&#22120;&#25110;&#25668;&#20687;&#22836;&#36827;&#34892;&#26080;&#20154;&#26816;&#27979;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#20197;&#21450;&#23545;&#38745;&#27490;&#20154;&#21592;&#30340;&#38169;&#35823;&#26816;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#21830;&#29992;Wi-Fi&#35774;&#22791;&#25429;&#33719;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;(CSI)&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20449;&#21495;&#29305;&#24449;&#65292;&#20197;&#36827;&#34892;&#20934;&#30830;&#30340;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#38750;&#30452;&#35270;(NLoS)&#21644;&#38745;&#24577;&#22330;&#26223;&#19979;&#23384;&#22312;&#20998;&#31867;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#24403;&#19968;&#20010;&#20154;&#38745;&#27490;&#31449;&#22312;&#25151;&#38388;&#35282;&#33853;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRONOS(&#22522;&#20110;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#24230;&#23398;&#20064;&#22686;&#24378;&#30340;NLoS&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;)&#30340;&#31995;&#32479;&#65292;&#23427;&#29983;&#25104;&#21160;&#24577;&#30340;&#22797;&#21457;&#22270;(RPs)&#21644;&#39068;&#33394;&#32534;&#30721;&#30340;CSI&#27604;&#29575;&#20197;&#21306;&#20998;&#25151;&#38388;&#20013;&#30340;&#31227;&#21160;&#20154;&#21592;&#21644;&#31354;&#32622;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#26816;&#32034;&#23454;&#36136;&#24615;&#30340;&#34920;&#24449;&#65292;&#20854;&#20013;&#21672;&#35810;&#25439;&#22833;&#34987;&#21046;&#23450;&#20026;&#21306;&#20998;&#21516;&#31867;&#21644;&#24322;&#31867;&#30340;&#23884;&#20837;&#28857;&#20043;&#38388;&#36317;&#31163;&#24230;&#37327;&#30340;&#25439;&#22833;&#12290;&#22312;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;NLoS&#26465;&#20214;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#25151;&#38388;&#20013;&#30340;&#20154;&#29289;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the demand for pervasive smart services and applications has increased rapidly. Device-free human detection through sensors or cameras has been widely adopted, but it comes with privacy issues as well as misdetection for motionless people. To address these drawbacks, channel state information (CSI) captured from commercialized Wi-Fi devices provides rich signal features for accurate detection. However, existing systems suffer from inaccurate classification under a non-line-of-sight (NLoS) and stationary scenario, such as when a person is standing still in a room corner. In this work, we propose a system called CRONOS (Colorization and Contrastive Learning Enhanced NLoS Human Presence Detection), which generates dynamic recurrence plots (RPs) and color-coded CSI ratios to distinguish mobile people from vacancy in a room, respectively. We also incorporate supervised contrastive learning to retrieve substantial representations, where consultation loss is formulated to dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#26426;&#21046;&#30456;&#20284;&#24615;&#30340;&#23450;&#20041;&#65292;&#24182;&#28436;&#31034;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#32570;&#20047;&#32447;&#24615;&#36830;&#36890;&#24615;&#24847;&#21619;&#30528;&#23427;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#21046;&#26469;&#20570;&#20986;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#36830;&#36890;&#24615;&#30340;&#24494;&#35843;&#65288;CBFT&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30446;&#26631;&#20462;&#25913;&#27169;&#22411;&#26426;&#21046;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2211.08422</link><description>&lt;p&gt;
&#26426;&#21046;&#27169;&#24335;&#36830;&#36890;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mechanistic Mode Connectivity. (arXiv:2211.08422v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#26426;&#21046;&#30456;&#20284;&#24615;&#30340;&#23450;&#20041;&#65292;&#24182;&#28436;&#31034;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#32570;&#20047;&#32447;&#24615;&#36830;&#36890;&#24615;&#24847;&#21619;&#30528;&#23427;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#21046;&#26469;&#20570;&#20986;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#36830;&#36890;&#24615;&#30340;&#24494;&#35843;&#65288;CBFT&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30446;&#26631;&#20462;&#25913;&#27169;&#22411;&#26426;&#21046;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26223;&#35266;&#65292;&#21363;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#26816;&#32034;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#23567;&#21270;&#22120;&#36890;&#36807;&#20302;&#25439;&#22833;&#30340;&#31616;&#21333;&#36335;&#24452;&#30456;&#20114;&#36830;&#25509;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#21046;&#30456;&#20284;&#24615;&#30340;&#23450;&#20041;&#65292;&#21363;&#19982;&#36755;&#20837;&#36716;&#25442;&#30340;&#20849;&#20139;&#19981;&#21464;&#24615;&#65292;&#24182;&#28436;&#31034;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#32570;&#20047;&#32447;&#24615;&#36830;&#36890;&#24615;&#24847;&#21619;&#30528;&#23427;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#21046;&#26469;&#20570;&#20986;&#39044;&#27979;&#12290;&#19982;&#23454;&#36341;&#30456;&#20851;&#30340;&#26159;&#65292;&#36825;&#20010;&#32467;&#26524;&#24110;&#21161;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#30340;&#31616;&#21333;&#24494;&#35843;&#21487;&#33021;&#26080;&#27861;&#25913;&#21464;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#20363;&#22914;&#65292;&#24494;&#35843;&#21487;&#33021;&#26080;&#27861;&#28040;&#38500;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25512;&#21160;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#36830;&#36890;&#24615;&#30340;&#24494;&#35843;&#65288;CBFT&#65289;&#30340;&#30446;&#26631;&#21464;&#21270;&#27169;&#22411;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#20960;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#20854;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study neural network loss landscapes through the lens of mode connectivity, the observation that minimizers of neural networks retrieved via training on a dataset are connected via simple paths of low loss. Specifically, we ask the following question: are minimizers that rely on different mechanisms for making their predictions connected via simple paths of low loss? We provide a definition of mechanistic similarity as shared invariances to input transformations and demonstrate that lack of linear connectivity between two models implies they use dissimilar mechanisms for making their predictions. Relevant to practice, this result helps us demonstrate that naive fine-tuning on a downstream dataset can fail to alter a model's mechanisms, e.g., fine-tuning can fail to eliminate a model's reliance on spurious attributes. Our analysis also motivates a method for targeted alteration of a model's mechanisms, named connectivity-based fine-tuning (CBFT), which we analyze using several synthe
&lt;/p&gt;</description></item><item><title>RecD &#26159;&#19968;&#31181;&#20026; DLRM &#35757;&#32451;&#25552;&#20379;&#21435;&#37325;&#21151;&#33021;&#30340;&#31471;&#21040;&#31471;&#22522;&#30784;&#35774;&#26045;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#29305;&#24449;&#37325;&#22797;&#36896;&#25104;&#30340;&#28023;&#37327;&#23384;&#20648;&#12289;&#39044;&#22788;&#29702;&#21644;&#35757;&#32451;&#24320;&#38144;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#24352;&#37327;&#26684;&#24335; InverseKeyedJaggedTensors (IKJTs) &#26469;&#21435;&#38500;&#29305;&#24449;&#20540;&#30340;&#37325;&#22797;&#65292;&#20351; DLRM &#27169;&#22411;&#26550;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#25968;&#25454;&#30340;&#37325;&#22797;&#24615;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.05239</link><description>&lt;p&gt;
RecD&#65306;&#20026;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#35757;&#32451;&#22522;&#30784;&#35774;&#26045;&#25552;&#20379;&#21435;&#37325;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure. (arXiv:2211.05239v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05239
&lt;/p&gt;
&lt;p&gt;
RecD &#26159;&#19968;&#31181;&#20026; DLRM &#35757;&#32451;&#25552;&#20379;&#21435;&#37325;&#21151;&#33021;&#30340;&#31471;&#21040;&#31471;&#22522;&#30784;&#35774;&#26045;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#29305;&#24449;&#37325;&#22797;&#36896;&#25104;&#30340;&#28023;&#37327;&#23384;&#20648;&#12289;&#39044;&#22788;&#29702;&#21644;&#35757;&#32451;&#24320;&#38144;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#24352;&#37327;&#26684;&#24335; InverseKeyedJaggedTensors (IKJTs) &#26469;&#21435;&#38500;&#29305;&#24449;&#20540;&#30340;&#37325;&#22797;&#65292;&#20351; DLRM &#27169;&#22411;&#26550;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#25968;&#25454;&#30340;&#37325;&#22797;&#24615;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; RecD&#65288;&#25512;&#33616;&#21435;&#37325;&#65289;&#65292;&#23427;&#26159;&#19968;&#32452;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411; (DLRM) &#35757;&#32451;&#27969;&#31243;&#30340;&#31471;&#21040;&#31471;&#22522;&#30784;&#35774;&#26045;&#20248;&#21270;&#12290;RecD&#35299;&#20915;&#20102;&#30001;&#20110;&#29305;&#24449;&#37325;&#22797;&#36896;&#25104;&#30340;&#28023;&#37327;&#23384;&#20648;&#12289;&#39044;&#22788;&#29702;&#21644;&#35757;&#32451;&#24320;&#38144;&#65292;&#36825;&#26159;&#22823;&#35268;&#27169; DLRM &#35757;&#32451;&#25968;&#25454;&#38598;&#20869;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026; DLRM &#25968;&#25454;&#38598;&#26159;&#20174;&#20132;&#20114;&#20013;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; RecD &#22914;&#20309;&#21033;&#29992;&#27492;&#23646;&#24615;&#26469;&#20248;&#21270;&#29983;&#20135;&#25968;&#25454;&#30340;&#27969;&#31243;&#65292;&#20943;&#23569;&#25968;&#25454;&#38598;&#23384;&#20648;&#21644;&#39044;&#22788;&#29702;&#38656;&#27714;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#22312;&#35757;&#32451;&#25209;&#27425;&#20013;&#37325;&#22797;&#12290;RecD &#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#26684;&#24335; InverseKeyedJaggedTensors (IKJTs)&#65292;&#20197;&#22312;&#27599;&#20010;&#25209;&#27425;&#20013;&#21435;&#38500;&#29305;&#24449;&#20540;&#30340;&#37325;&#22797;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; DLRM &#27169;&#22411;&#26550;&#26500;&#22914;&#20309;&#21033;&#29992; IKJTs &#26469;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. Re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#31227;&#38500;&#21644;&#25233;&#21046;&#19981;&#24403;&#30340;&#22270;&#20687;&#37096;&#20998;&#65292;&#20174;&#32780;&#32531;&#35299;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22240;&#19981;&#24403;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2211.05105</link><description>&lt;p&gt;
&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#65306;&#32531;&#35299;&#25193;&#25955;&#27169;&#22411;&#20013;&#19981;&#24403;&#36864;&#21270;
&lt;/p&gt;
&lt;p&gt;
Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models. (arXiv:2211.05105v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#31227;&#38500;&#21644;&#25233;&#21046;&#19981;&#24403;&#30340;&#22270;&#20687;&#37096;&#20998;&#65292;&#20174;&#32780;&#32531;&#35299;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22240;&#19981;&#24403;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#26524;&#65292;&#24182;&#22240;&#27492;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#30001;&#20110;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#38543;&#26426;&#20174;&#20114;&#32852;&#32593;&#19978;&#25235;&#21462;&#30340;&#25968;&#21313;&#20159;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#23427;&#20204;&#20063;&#38754;&#20020;&#26469;&#33258;&#36864;&#21270;&#21644;&#20559;&#35265;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#12290;&#21453;&#36807;&#26469;&#65292;&#23427;&#20204;&#29978;&#33267;&#21487;&#33021;&#24378;&#21270;&#36825;&#20123;&#20559;&#35265;&#12290;&#20026;&#20102;&#24110;&#21161;&#24212;&#23545;&#36825;&#20123;&#19981;&#33391;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#65288;SLD&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#34913;&#37327;&#30001;&#20110;&#26410;&#36807;&#28388;&#21644;&#19981;&#24179;&#34913;&#30340;&#35757;&#32451;&#38598;&#32780;&#24341;&#36215;&#30340;&#19981;&#24403;&#36864;&#21270;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#20687;&#29983;&#25104;&#27979;&#35797;&#24179;&#21488;&#8212;&#8212;&#21253;&#21547;&#19987;&#38376;&#30340;&#12289;&#35206;&#30422;&#35064;&#38706;&#21644;&#26292;&#21147;&#31561;&#27010;&#24565;&#30340;&#23454;&#38469;&#22270;&#20687;&#21040;&#25991;&#26412;&#25552;&#31034;&#30340;&#19981;&#24403;&#22270;&#20687;&#25552;&#31034;&#65288;I2P&#65289;&#12290;&#27491;&#22914;&#25105;&#20204;&#35814;&#23613;&#30340;&#23454;&#35777;&#35780;&#20272;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;&#24341;&#20837;&#30340;SLD&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#31227;&#38500;&#21644;&#25233;&#21046;&#20102;&#19981;&#24403;&#30340;&#22270;&#20687;&#37096;&#20998;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#24182;&#19988;&#23545;&#22270;&#20687;&#36136;&#37327;&#27809;&#26377;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#39033;&#22810;&#20013;&#24515;&#30740;&#31350;&#65292;&#26088;&#22312;&#35780;&#20272;MRI&#25968;&#25454;&#21327;&#35843;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25928;&#29992;;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#35843;&#21644;&#22120;&#21464;&#21387;&#22120;&#8221;&#26041;&#27861;&#65292;&#22312;&#19981;&#27844;&#38706;&#20449;&#24687;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#21327;&#35843;&#12290;</title><link>http://arxiv.org/abs/2211.04125</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26102;&#20195;MRI&#25968;&#25454;&#21327;&#35843;&#30340;&#26377;&#25928;&#24615;&#65306;&#19968;&#39033;&#36328;36&#20010;&#25968;&#25454;&#38598;&#30340;&#22810;&#20013;&#24515;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Efficacy of MRI data harmonization in the age of machine learning. A multicenter study across 36 datasets. (arXiv:2211.04125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#39033;&#22810;&#20013;&#24515;&#30740;&#31350;&#65292;&#26088;&#22312;&#35780;&#20272;MRI&#25968;&#25454;&#21327;&#35843;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25928;&#29992;;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#35843;&#21644;&#22120;&#21464;&#21387;&#22120;&#8221;&#26041;&#27861;&#65292;&#22312;&#19981;&#27844;&#38706;&#20449;&#24687;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22810;&#20010;&#32593;&#31449;&#27719;&#38598;&#20844;&#24320;&#21487;&#29992;&#30340;MRI&#25968;&#25454;&#21487;&#20197;&#32452;&#35013;&#22823;&#37327;&#21463;&#35797;&#23545;&#35937;&#65292;&#22686;&#21152;&#32479;&#35745;&#21151;&#29575;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20419;&#36827;&#25968;&#25454;&#37325;&#29992;&#12290;&#22810;&#20013;&#24515;&#25968;&#25454;&#30340;&#21327;&#35843;&#26159;&#20943;&#23569;&#25968;&#25454;&#20013;&#19982;&#38750;&#29983;&#29289;&#26469;&#28304;&#30340;&#21464;&#24322;&#24230;&#37327;&#30340;&#28151;&#26434;&#25928;&#24212;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#23558;&#21327;&#35843;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20043;&#21069;&#30340;&#25972;&#20010;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#25968;&#25454;&#27844;&#28431;&#65292;&#22240;&#20026;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#20449;&#24687;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#26500;&#24314;&#24182;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36807;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;1&#65289;&#25968;&#25454;&#21327;&#35843;&#30340;&#26377;&#25928;&#24615;&#27979;&#37327;&#26041;&#27861;&#65307;2&#65289;&#8220;&#35843;&#21644;&#22120;&#21464;&#21387;&#22120;&#8221;&#65292;&#21363;ComBat&#21327;&#35843;&#26041;&#27861;&#30340;&#19968;&#20010;&#23454;&#29616;&#65292;&#23427;&#20801;&#35768;&#23558;&#20854;&#23553;&#35013;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#65292;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#26469;&#33258;36&#20010;&#32593;&#31449;&#30340;1740&#21517;&#20581;&#24247;&#21463;&#35797;&#32773;&#30340;&#22823;&#33041;T1&#21152;&#26435;MRI&#25968;&#25454;&#26469;&#27979;&#35797;&#36825;&#20123;&#24037;&#20855;&#12290;&#32463;&#36807;&#21327;&#35843;&#21518;&#65292;&#32593;&#31449;&#25928;&#24212;&#34987;&#21024;&#38500;&#25110;&#20943;&#23569;&#20102;&#65292;
&lt;/p&gt;
&lt;p&gt;
Pooling publicly-available MRI data from multiple sites allows to assemble extensive groups of subjects, increase statistical power, and promote data reuse with machine learning techniques. The harmonization of multicenter data is necessary to reduce the confounding effect associated with non-biological sources of variability in the data. However, when applied to the entire dataset before machine learning, the harmonization leads to data leakage, because information outside the training set may affect model building, and potentially falsely overestimate performance. We propose a 1) measurement of the efficacy of data harmonization; 2) harmonizer transformer, i.e., an implementation of the ComBat harmonization allowing its encapsulation among the preprocessing steps of a machine learning pipeline, avoiding data leakage. We tested these tools using brain T1-weighted MRI data from 1740 healthy subjects acquired at 36 sites. After harmonization, the site effect was removed or reduced, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FingerFlex&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;ECoG&#20449;&#21495;&#19978;&#23454;&#29616;&#25163;&#25351;&#36816;&#21160;&#22238;&#24402;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#20026;&#24320;&#21457;&#39640;&#31934;&#24230;&#30382;&#23618;&#36816;&#21160;&#33041;&#26426;&#25509;&#21475;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2211.01960</link><description>&lt;p&gt;
FingerFlex&#65306;&#20174;ECoG&#20449;&#21495;&#25512;&#26029;&#25163;&#25351;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
FingerFlex: Inferring Finger Trajectories from ECoG signals. (arXiv:2211.01960v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FingerFlex&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;ECoG&#20449;&#21495;&#19978;&#23454;&#29616;&#25163;&#25351;&#36816;&#21160;&#22238;&#24402;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#20026;&#24320;&#21457;&#39640;&#31934;&#24230;&#30382;&#23618;&#36816;&#21160;&#33041;&#26426;&#25509;&#21475;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#30340;&#21457;&#23637;&#20851;&#38190;&#22312;&#20110;&#31070;&#32463;&#26102;&#38388;&#24207;&#21015;&#35299;&#30721;&#31639;&#27861;&#12290;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#30340;&#26368;&#26032;&#36827;&#23637;&#20801;&#35768;&#33258;&#21160;&#36873;&#25321;&#29305;&#24449;&#20197;&#36924;&#36817;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FingerFlex&#27169;&#22411;&#8212;&#8212;&#19968;&#31181;&#21367;&#31215;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#30005;&#23548;&#23618;&#22270;&#65288;ECoG&#65289;&#33041;&#25968;&#25454;&#19978;&#36827;&#34892;&#25163;&#25351;&#36816;&#21160;&#22238;&#24402;&#12290;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;BCI&#31454;&#36187;IV&#25968;&#25454;&#38598;4&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#30495;&#23454;&#21644;&#39044;&#27979;&#36712;&#36857;&#20043;&#38388;&#30340;&#30456;&#20851;&#31995;&#25968;&#39640;&#36798;0.74&#12290;&#35813;&#26041;&#27861;&#20026;&#24320;&#21457;&#23436;&#20840;&#21151;&#33021;&#30340;&#39640;&#31934;&#24230;&#30382;&#23618;&#36816;&#21160;&#33041;&#26426;&#25509;&#21475;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motor brain-computer interface (BCI) development relies critically on neural time series decoding algorithms. Recent advances in deep learning architectures allow for automatic feature selection to approximate higher-order dependencies in data. This article presents the FingerFlex model - a convolutional encoder-decoder architecture adapted for finger movement regression on electrocorticographic (ECoG) brain data. State-of-the-art performance was achieved on a publicly available BCI competition IV dataset 4 with a correlation coefficient between true and predicted trajectories up to 0.74. The presented method provides the opportunity for developing fully-functional high-precision cortical motor brain-computer interfaces.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20581;&#24247;&#32452;&#32455;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#65292;&#20351;&#34920;&#31034;&#36866;&#24212;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#23454;&#29616;&#23545;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2210.07675</link><description>&lt;p&gt;
&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#20197;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65306;&#22312;&#33647;&#29289;&#24320;&#21457;&#20013;&#21457;&#29616;&#32452;&#32455;&#23398;&#25913;&#21464;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20581;&#24247;&#32452;&#32455;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#65292;&#20351;&#34920;&#31034;&#36866;&#24212;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#23454;&#29616;&#23545;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#30340;&#31995;&#32479;&#12290;&#22312;&#32452;&#32455;&#23398;&#20013;&#65292;&#27491;&#24120;&#26679;&#26412;&#36890;&#24120;&#26159;&#22823;&#37327;&#23384;&#22312;&#30340;&#65292;&#32780;&#24322;&#24120;&#65288;&#30149;&#29702;&#65289;&#24773;&#20917;&#36890;&#24120;&#24456;&#23569;&#25110;&#19981;&#21487;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22312;&#20581;&#24247;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#21333;&#31867;&#20998;&#31867;&#22120;&#21487;&#20197;&#26816;&#27979;&#21040;&#20998;&#24067;&#22806;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#19982;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22270;&#20687;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#21069;&#24050;&#32463;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#12290;&#20294;&#26159;&#65292;&#39044;&#35757;&#32451;&#30340;&#29616;&#25104;CNN&#34920;&#31034;&#21487;&#33021;&#23545;&#32452;&#32455;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#19981;&#25935;&#24863;&#65292;&#32780;&#20581;&#24247;&#32452;&#32455;&#30340;&#33258;&#28982;&#21464;&#24322;&#21487;&#33021;&#23548;&#33268;&#36828;&#31163;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#20351;&#34920;&#31034;&#36866;&#24212;&#20581;&#24247;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#36741;&#21161;&#20219;&#21153;&#19978;&#35757;&#32451;CNN&#65292;&#35813;&#20219;&#21153;&#21306;&#20998;&#19981;&#21516;&#29289;&#31181;&#12289;&#22120;&#23448;&#21644;&#26579;&#33394;&#35797;&#21058;&#30340;&#20581;&#24247;&#32452;&#32455;&#12290;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#27880;&#24037;&#20316;&#37327;&#65292;&#22240;&#20026;&#20581;&#24247;&#26679;&#26412;&#21487;&#20197;&#33258;&#21160;&#33719;&#24471;&#19978;&#36848;&#26631;&#31614;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#24378;&#21046;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
We present a system for anomaly detection in histopathological images. In histology, normal samples are usually abundant, whereas anomalous (pathological) cases are scarce or not available. Under such settings, one-class classifiers trained on healthy data can detect out-of-distribution anomalous samples. Such approaches combined with pre-trained Convolutional Neural Network (CNN) representations of images were previously employed for anomaly detection (AD). However, pre-trained off-the-shelf CNN representations may not be sensitive to abnormal conditions in tissues, while natural variations of healthy tissue may result in distant representations. To adapt representations to relevant details in healthy tissue we propose training a CNN on an auxiliary task that discriminates healthy tissue of different species, organs, and staining reagents. Almost no additional labeling workload is required, since healthy samples come automatically with aforementioned labels. During training we enforce
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;bi-stride&#30340;&#26032;&#22411;&#27744;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;&#20108;&#37096;&#22270;&#20915;&#31574;&#23454;&#29616;&#39640;&#25928;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#29289;&#29702;&#20223;&#30495;&#65292;&#26080;&#38656;&#25163;&#21160;&#32472;&#21046;&#31895;&#32593;&#26684;&#65292;&#24182;&#36991;&#20813;&#20102;&#31354;&#38388;&#25509;&#36817;&#24615;&#24102;&#26469;&#30340;&#38169;&#35823;&#36793;&#32536;&#12290;</title><link>http://arxiv.org/abs/2210.02573</link><description>&lt;p&gt;
&#29992;BSMS-GNN&#39640;&#25928;&#23398;&#20064;&#22522;&#20110;&#32593;&#26684;&#30340;&#29289;&#29702;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of Mesh-Based Physical Simulation with BSMS-GNN. (arXiv:2210.02573v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;bi-stride&#30340;&#26032;&#22411;&#27744;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;&#20108;&#37096;&#22270;&#20915;&#31574;&#23454;&#29616;&#39640;&#25928;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#29289;&#29702;&#20223;&#30495;&#65292;&#26080;&#38656;&#25163;&#21160;&#32472;&#21046;&#31895;&#32593;&#26684;&#65292;&#24182;&#36991;&#20813;&#20102;&#31354;&#38388;&#25509;&#36817;&#24615;&#24102;&#26469;&#30340;&#38169;&#35823;&#36793;&#32536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24179;&#38754;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#22534;&#21472;&#30340;&#28040;&#24687;&#20256;&#36882;&#65288;MPs&#65289;&#23398;&#20064;&#22823;&#35268;&#27169;&#32593;&#26684;&#19978;&#30340;&#29289;&#29702;&#20223;&#30495;&#20855;&#26377;&#19982;&#33410;&#28857;&#25968;&#37327;&#30456;&#20851;&#30340;&#32553;&#25918;&#22797;&#26434;&#24230;&#21644;&#36807;&#24230;&#24179;&#28369;&#30340;&#25361;&#25112;&#12290;&#24341;&#20837;&#8220;&#22810;&#23610;&#24230;&#8221;&#32467;&#26500;&#21040;GNNs&#20197;&#36827;&#34892;&#29289;&#29702;&#20223;&#30495;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21463;&#21040;&#20381;&#36182;&#20110;&#32321;&#29712;&#30340;&#32472;&#21046;&#31895;&#32593;&#26684;&#25110;&#22522;&#20110;&#31354;&#38388;&#37051;&#36817;&#24615;&#26500;&#24314;&#31895;&#30053;&#32423;&#21035;&#30340;&#38480;&#21046;&#65292;&#36825;&#21487;&#33021;&#22312;&#20960;&#20309;&#36793;&#30028;&#22788;&#24341;&#20837;&#38169;&#35823;&#30340;&#36793;&#32536;&#12290;&#21463;&#21040;&#20108;&#37096;&#22270;&#20915;&#31574;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27719;&#38598;&#31574;&#30053;&#8212;&#8212;&#21452;&#27493;&#36328;&#65288;bi-stride&#65289;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#12290;&#24403;&#36827;&#34892;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#26102;&#65292;&#21452;&#27493;&#36328;&#22312;&#27599;&#20010;&#20854;&#20182;&#21069;&#27839;&#30340;&#33410;&#28857;&#19978;&#36827;&#34892;&#27744;&#21270;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#32472;&#21046;&#36739;&#31895;&#30340;&#32593;&#26684;&#65292;&#24182;&#36890;&#36807;&#31354;&#38388;&#37051;&#36817;&#24615;&#36991;&#20813;&#38169;&#35823;&#30340;&#36793;&#32536;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#23454;&#29616;&#20102;&#27599;&#23618;&#30340;&#21333;&#19968;MP&#26041;&#26696;&#21644;&#26080;&#21442;&#25968;&#30340;&#27744;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning the physical simulation on large-scale meshes with flat Graph Neural Networks (GNNs) and stacking Message Passings (MPs) is challenging due to the scaling complexity w.r.t. the number of nodes and over-smoothing. There has been growing interest in the community to introduce \textit{multi-scale} structures to GNNs for physical simulation. However, current state-of-the-art methods are limited by their reliance on the labor-intensive drawing of coarser meshes or building coarser levels based on spatial proximity, which can introduce wrong edges across geometry boundaries. Inspired by the bipartite graph determination, we propose a novel pooling strategy, \textit{bi-stride} to tackle the aforementioned limitations. Bi-stride pools nodes on every other frontier of the breadth-first search (BFS), without the need for the manual drawing of coarser meshes and avoiding the wrong edges by spatial proximity. Additionally, it enables a one-MP scheme per level and non-parametrized pooling 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FADE&#30340;&#32852;&#37030;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#23545;&#25239;&#35757;&#32451;&#65292;&#26694;&#26550;&#23558;&#25972;&#20010;&#27169;&#22411;&#24046;&#20998;&#20998;&#35299;&#25104;&#23567;&#27169;&#22359;&#26469;&#36866;&#24212;&#35774;&#22791;&#30340;&#36164;&#28304;&#39044;&#31639;&#65292;&#24182;&#25552;&#20986;&#36741;&#21161;&#26435;&#37325;&#34928;&#20943;&#26041;&#27861;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#24230;-&#40065;&#26834;&#24615;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2209.03839</link><description>&lt;p&gt;
FADE&#65306;&#23454;&#29616;&#24322;&#26500;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#35774;&#22791;&#19978;&#32852;&#37030;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FADE: Enabling Federated Adversarial Training on Heterogeneous Resource-Constrained Edge Devices. (arXiv:2209.03839v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03839
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FADE&#30340;&#32852;&#37030;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#23545;&#25239;&#35757;&#32451;&#65292;&#26694;&#26550;&#23558;&#25972;&#20010;&#27169;&#22411;&#24046;&#20998;&#20998;&#35299;&#25104;&#23567;&#27169;&#22359;&#26469;&#36866;&#24212;&#35774;&#22791;&#30340;&#36164;&#28304;&#39044;&#31639;&#65292;&#24182;&#25552;&#20986;&#36741;&#21161;&#26435;&#37325;&#34928;&#20943;&#26041;&#27861;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#24230;-&#40065;&#26834;&#24615;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23545;&#25239;&#35757;&#32451;&#33021;&#22815;&#26377;&#25928;&#23558;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#24341;&#20837;&#20445;&#25252;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20869;&#23384;&#23481;&#37327;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#39640;&#38656;&#27714;&#65292;&#22823;&#35268;&#27169;&#32852;&#37030;&#23545;&#25239;&#35757;&#32451;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#26080;&#27861;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;FADE&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#24322;&#26500;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#23545;&#25239;&#35757;&#32451;&#12290;FADE&#23558;&#25972;&#20010;&#27169;&#22411;&#24046;&#20998;&#20998;&#35299;&#25104;&#23567;&#27169;&#22359;&#20197;&#36866;&#24212;&#27599;&#20010;&#35774;&#22791;&#30340;&#36164;&#28304;&#39044;&#31639;&#65292;&#27599;&#20010;&#35774;&#22791;&#27599;&#27425;&#36890;&#20449;&#21482;&#38656;&#35201;&#23545;&#21333;&#20010;&#27169;&#22359;&#25191;&#34892;&#23545;&#25239;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36741;&#21161;&#26435;&#37325;&#34928;&#20943;&#26041;&#27861;&#65292;&#22312;FADE&#20013;&#32531;&#35299;&#30446;&#26631;&#19981;&#19968;&#33268;&#24615;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24230;-&#40065;&#26834;&#24615;&#24179;&#34913;&#12290;FADE&#22312;&#25910;&#25947;&#21644;&#23545;&#25239;&#30340;&#25910;&#25947;&#29575;&#26041;&#38754;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated adversarial training can effectively complement adversarial robustness into the privacy-preserving federated learning systems. However, the high demand for memory capacity and computing power makes large-scale federated adversarial training infeasible on resource-constrained edge devices. Few previous studies in federated adversarial training have tried to tackle both memory and computational constraints simultaneously. In this paper, we propose a new framework named Federated Adversarial Decoupled Learning (FADE) to enable AT on heterogeneous resource-constrained edge devices. FADE differentially decouples the entire model into small modules to fit into the resource budget of each device, and each device only needs to perform AT on a single module in each communication round. We also propose an auxiliary weight decay to alleviate objective inconsistency and achieve better accuracy-robustness balance in FADE. FADE offers theoretical guarantees for convergence and adversarial 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#39640;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#35821;&#35328;&#27169;&#22411;TransPolymer&#65292;&#23427;&#21033;&#29992;&#21270;&#23398;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#20998;&#23376;&#20998;&#35789;&#22120;&#23398;&#20064;&#39640;&#20998;&#23376;&#24207;&#21015;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.01307</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39640;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#35821;&#35328;&#27169;&#22411;TransPolymer
&lt;/p&gt;
&lt;p&gt;
TransPolymer: a Transformer-based language model for polymer property predictions. (arXiv:2209.01307v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#39640;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#35821;&#35328;&#27169;&#22411;TransPolymer&#65292;&#23427;&#21033;&#29992;&#21270;&#23398;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#20998;&#23376;&#20998;&#35789;&#22120;&#23398;&#20064;&#39640;&#20998;&#23376;&#24207;&#21015;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#20998;&#23376;&#35774;&#35745;&#20013;&#65292;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#39044;&#27979;&#39640;&#20998;&#23376;&#30340;&#23646;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20256;&#32479;&#19978;&#65292;&#38656;&#35201;&#36827;&#34892;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#23454;&#39564;&#25110;&#27169;&#25311;&#25165;&#33021;&#35780;&#20272;&#39640;&#20998;&#23376;&#30340;&#21151;&#33021;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25903;&#25345;&#19979;&#65292;Transformer&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#20998;&#23376;&#31185;&#23398;&#20013;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;TransPolymer&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#39640;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21270;&#23398;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#20998;&#23376;&#20998;&#35789;&#22120;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#39640;&#20998;&#23376;&#24207;&#21015;&#30340;&#34920;&#31034;&#12290;&#22312;&#21313;&#20010;&#39640;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20005;&#26684;&#23454;&#39564;&#34920;&#26126;&#20102;TransPolymer&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TransPolymer&#20174;&#22823;&#22411;&#26080;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#20013;&#21463;&#30410;&#12290;&#23454;&#39564;&#32467;&#26524;&#36827;&#19968;&#27493;&#34920;&#26126;&#20102;&#33258;&#27880;&#24847;&#21147;&#22312;&#24314;&#27169;&#39640;&#20998;&#23376;&#24207;&#21015;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and efficient prediction of polymer properties is of great significance in polymer design. Conventionally, expensive and time-consuming experiments or simulations are required to evaluate polymer functions. Recently, Transformer models, equipped with self-attention mechanisms, have exhibited superior performance in natural language processing. However, such methods have not been investigated in polymer sciences. Herein, we report TransPolymer, a Transformer-based language model for polymer property prediction. Our proposed polymer tokenizer with chemical awareness enables learning representations from polymer sequences. Rigorous experiments on ten polymer property prediction benchmarks demonstrate the superior performance of TransPolymer. Moreover, we show that TransPolymer benefits from pretraining on large unlabeled dataset via Masked Language Modeling. Experimental results further manifest the important role of self-attention in modeling polymer sequences. We highlight this
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#21644;&#27010;&#29575;&#24322;&#24120;&#24471;&#20998;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#19982;&#35757;&#32451;&#25968;&#25454;&#26377;&#20559;&#24046;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#33258;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#22120;&#35757;&#32451;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#24046;&#20998;&#24067;&#26469;&#25552;&#39640;&#26816;&#27979;&#25928;&#26524;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2208.14024</link><description>&lt;p&gt;
&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#21644;&#23545;&#27604;&#25968;&#25454;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#27491;&#24046;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Positive Difference Distribution for Image Outlier Detection using Normalizing Flows and Contrastive Data. (arXiv:2208.14024v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#21644;&#27010;&#29575;&#24322;&#24120;&#24471;&#20998;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#19982;&#35757;&#32451;&#25968;&#25454;&#26377;&#20559;&#24046;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#33258;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#22120;&#35757;&#32451;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#24046;&#20998;&#24067;&#26469;&#25552;&#39640;&#26816;&#27979;&#25928;&#26524;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#19982;&#35757;&#32451;&#25968;&#25454;&#26377;&#20559;&#24046;&#30340;&#27979;&#35797;&#25968;&#25454;&#26159;&#23433;&#20840;&#21644;&#31283;&#20581;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#20013;&#24515;&#38382;&#39064;&#12290;&#36890;&#36807;&#26631;&#20934;&#23545;&#25968;&#20284;&#28982;&#35757;&#32451;&#65292;&#20363;&#22914;&#24402;&#19968;&#21270;&#27969;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#20316;&#20026;&#24322;&#24120;&#24471;&#20998;&#25928;&#26524;&#19981;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#21644;&#27010;&#29575;&#24322;&#24120;&#24471;&#20998;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#22120;&#23545;&#36741;&#21161;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#21487;&#33021;&#24615;&#21644;&#26368;&#23567;&#21270;&#22312;&#23545;&#27604;&#25968;&#25454;&#19978;&#30340;&#21487;&#33021;&#24615;&#35757;&#32451;&#25552;&#21462;&#29305;&#24449;&#30340;&#24402;&#19968;&#21270;&#27969;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#30456;&#24403;&#20110;&#23398;&#20064;&#20998;&#24067;&#25968;&#25454;&#21644;&#23545;&#27604;&#25968;&#25454;&#20043;&#38388;&#30340;&#27491;&#24120;&#21270;&#27491;&#24046;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#20284;&#28982;&#12289;&#20284;&#28982;&#27604;&#21644;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting test data deviating from training data is a central problem for safe and robust machine learning. Likelihoods learned by a generative model, e.g., a normalizing flow via standard log-likelihood training, perform poorly as an outlier score. We propose to use an unlabelled auxiliary dataset and a probabilistic outlier score for outlier detection. We use a self-supervised feature extractor trained on the auxiliary dataset and train a normalizing flow on the extracted features by maximizing the likelihood on in-distribution data and minimizing the likelihood on the contrastive dataset. We show that this is equivalent to learning the normalized positive difference between the in-distribution and the contrastive feature density. We conduct experiments on benchmark datasets and compare to the likelihood, the likelihood ratio and state-of-the-art anomaly detection methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SOVR&#30340;&#23545;&#25239;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#32858;&#28966;&#37325;&#35201;&#26679;&#26412;&#65292;&#22686;&#21152;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#23545;&#25968;&#20960;&#29575;&#38388;&#38548;&#65292;&#20174;&#32780;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#23545;&#25239;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.10283</link><description>&lt;p&gt;
&#19968;&#23545;&#20854;&#20313;&#25439;&#22833;&#20989;&#25968;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#32858;&#28966;&#37325;&#35201;&#26679;&#26412;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
One-vs-the-Rest Loss to Focus on Important Samples in Adversarial Training. (arXiv:2207.10283v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SOVR&#30340;&#23545;&#25239;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#32858;&#28966;&#37325;&#35201;&#26679;&#26412;&#65292;&#22686;&#21152;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#23545;&#25968;&#20960;&#29575;&#38388;&#38548;&#65292;&#20174;&#32780;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#23545;&#25239;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#12290;&#30001;&#20110;&#23545;&#25239;&#35757;&#32451;&#23384;&#22312;&#22256;&#38590;&#65292;&#22914;&#38656;&#35201;&#39640;&#27169;&#22411;&#23481;&#37327;&#65292;&#36890;&#36807;&#21152;&#26435;&#20132;&#21449;&#29109;&#25439;&#22833;&#20851;&#27880;&#37325;&#35201;&#25968;&#25454;&#28857;&#24050;&#24341;&#36215;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#22797;&#26434;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22914;Auto-Attack&#12290;&#26412;&#25991;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#20204;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#21407;&#22240;&#26159;&#30495;&#23454;&#26631;&#31614;&#21644;&#20854;&#20182;&#26631;&#31614;&#20043;&#38388;&#30340;&#23545;&#25968;&#20960;&#29575;&#20043;&#38388;&#30340;&#36739;&#23567;&#38388;&#38548;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#26159;&#26681;&#25454;&#23545;&#25968;&#20960;&#29575;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#20998;&#31867;&#30340;&#65292;&#25152;&#20197;&#23545;&#25968;&#20960;&#29575;&#30340;&#38388;&#38548;&#24212;&#35813;&#36275;&#22815;&#22823;&#65292;&#20197;&#36991;&#20813;&#25915;&#20987;&#32763;&#36716;&#26368;&#22823;&#30340;&#23545;&#25968;&#20960;&#29575;&#12290;&#37325;&#35201;&#24615;&#24863;&#30693;&#26041;&#27861;&#19981;&#20250;&#22686;&#21152;&#37325;&#35201;&#26679;&#26412;&#30340;&#23545;&#25968;&#20960;&#29575;&#38388;&#38548;&#65292;&#20294;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#30456;&#27604;&#20250;&#20943;&#23569;&#36739;&#19981;&#37325;&#35201;&#26679;&#26412;&#30340;&#23545;&#25968;&#20960;&#29575;&#38388;&#38548;&#12290;&#20026;&#20102;&#22686;&#21152;&#37325;&#35201;&#26679;&#26412;&#30340;&#23545;&#25968;&#20960;&#29575;&#38388;&#38548;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20999;&#25442;&#19968;&#23545;&#20854;&#20313;&#65288;SOVR&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#22312;&#20855;&#26377;&#36739;&#23567;&#23545;&#25968;&#20960;&#29575;&#38388;&#38548;&#30340;&#37325;&#35201;&#26679;&#26412;&#20013;&#20174;&#20132;&#21449;&#29109;&#20999;&#25442;&#21040;&#19968;&#23545;&#20854;&#20313;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#20998;&#26512;&#12289;&#28040;&#34701;&#30740;&#31350;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;SOVR&#23545;&#25239;&#25239;&#20987;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new loss function for adversarial training. Since adversarial training has difficulties, e.g., necessity of high model capacity, focusing on important data points by weighting cross-entropy loss has attracted much attention. However, they are vulnerable to sophisticated attacks, e.g., Auto-Attack. This paper experimentally reveals that the cause of their vulnerability is their small margins between logits for the true label and the other labels. Since neural networks classify the data points based on the logits, logit margins should be large enough to avoid flipping the largest logit by the attacks. Importance-aware methods do not increase logit margins of important samples but decrease those of less-important samples compared with cross-entropy loss. To increase logit margins of important samples, we propose switching one-vs-the-rest loss (SOVR), which switches from cross-entropy to one-vs-the-rest loss for important samples that have small logit margins. We prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PIXEL&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#23558;&#25991;&#26412;&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#25193;&#23637;&#25903;&#25345;&#30340;&#35821;&#35328;&#25968;&#37327;&#26102;&#20986;&#29616;&#30340;&#35789;&#27719;&#29942;&#39048;&#38382;&#39064;&#65292;&#19988;&#22312;&#24418;&#24577;&#23398;&#21644;&#35821;&#20041;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;BERT&#12290;</title><link>http://arxiv.org/abs/2207.06991</link><description>&lt;p&gt;
&#20351;&#29992;&#20687;&#32032;&#30340;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Language Modelling with Pixels. (arXiv:2207.06991v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PIXEL&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#23558;&#25991;&#26412;&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#25193;&#23637;&#25903;&#25345;&#30340;&#35821;&#35328;&#25968;&#37327;&#26102;&#20986;&#29616;&#30340;&#35789;&#27719;&#29942;&#39048;&#38382;&#39064;&#65292;&#19988;&#22312;&#24418;&#24577;&#23398;&#21644;&#35821;&#20041;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;BERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#26377;&#38480;&#30340;&#36755;&#20837;&#38598;&#21512;&#19978;&#36827;&#34892;&#23450;&#20041;&#65292;&#36825;&#23548;&#33268;&#22312;&#23581;&#35797;&#25193;&#23637;&#25903;&#25345;&#30340;&#35821;&#35328;&#25968;&#37327;&#26102;&#20986;&#29616;&#35789;&#27719;&#29942;&#39048;&#12290;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#20250;&#23548;&#33268;&#22312;&#23884;&#20837;&#30697;&#38453;&#20013;&#25152;&#33021;&#34920;&#31034;&#30340;&#20869;&#23481;&#19982;&#36755;&#20986;&#23618;&#30340;&#35745;&#31639;&#38382;&#39064;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21517;&#20026;PIXEL&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#12290;PIXEL&#26159;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#25991;&#26412;&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#20174;&#32780;&#21487;&#20197;&#22522;&#20110;&#25340;&#20889;&#30456;&#20284;&#24615;&#25110;&#20687;&#32032;&#30340;&#20849;&#21516;&#28608;&#27963;&#26469;&#36328;&#35821;&#35328;&#20256;&#36882;&#34920;&#31034;&#12290;PIXEL&#35757;&#32451;&#26102;&#19981;&#26159;&#39044;&#27979;&#26631;&#35760;&#20998;&#24067;&#65292;&#32780;&#26159;&#37325;&#26500;&#34987;&#23631;&#34109;&#30340;&#22359;&#30340;&#20687;&#32032;&#12290;&#25105;&#20204;&#23545;&#19982;BART&#30456;&#21516;&#30340;&#33521;&#35821;&#25968;&#25454;&#36827;&#34892;&#20102;86M&#21442;&#25968;PIXEL&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#24418;&#24577;&#23398;&#21644;&#35821;&#20041;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#65292;&#21253;&#25324;&#21508;&#31181;&#38750;&#25289;&#19969;&#25991;&#23383;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35821;&#27861;&#21644;&#35821;&#20041;&#22788;&#29702;&#26041;&#38754;&#65292;PIXEL&#30340;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;BERT&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#29289;&#29702;&#31995;&#32479;&#20013;&#35782;&#21035;&#31616;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#20351;&#29992;&#19981;&#21464;&#21494;&#29255;&#26159;&#20174;&#29616;&#26377;&#25968;&#25454;&#20013;&#35782;&#21035;ROM&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#20854;&#20313;&#26041;&#27861;&#21482;&#33021;&#35782;&#21035;&#23436;&#25972;&#27169;&#22411;&#12290;&#30740;&#31350;&#34920;&#26126;&#25214;&#21040;&#19981;&#21464;&#21494;&#29255;&#38656;&#35201;&#36817;&#20284;&#39640;&#32500;&#20989;&#25968;&#65292;&#20351;&#29992;&#24102;&#26377;&#21387;&#32553;&#24352;&#37327;&#31995;&#25968;&#30340;&#22810;&#39033;&#24335;&#21363;&#21487;&#12290;</title><link>http://arxiv.org/abs/2206.12269</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#21464;&#21494;&#29255;&#12289;&#27969;&#24418;&#21644;&#33258;&#32534;&#30721;&#22120;&#30340;&#25968;&#25454;&#39537;&#21160;&#31616;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Data-driven reduced order models using invariant foliations, manifolds and autoencoders. (arXiv:2206.12269v3 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#29289;&#29702;&#31995;&#32479;&#20013;&#35782;&#21035;&#31616;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#20351;&#29992;&#19981;&#21464;&#21494;&#29255;&#26159;&#20174;&#29616;&#26377;&#25968;&#25454;&#20013;&#35782;&#21035;ROM&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#20854;&#20313;&#26041;&#27861;&#21482;&#33021;&#35782;&#21035;&#23436;&#25972;&#27169;&#22411;&#12290;&#30740;&#31350;&#34920;&#26126;&#25214;&#21040;&#19981;&#21464;&#21494;&#29255;&#38656;&#35201;&#36817;&#20284;&#39640;&#32500;&#20989;&#25968;&#65292;&#20351;&#29992;&#24102;&#26377;&#21387;&#32553;&#24352;&#37327;&#31995;&#25968;&#30340;&#22810;&#39033;&#24335;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#29289;&#29702;&#31995;&#32479;&#20013;&#35782;&#21035;&#20986;&#31616;&#21270;&#27169;&#22411;&#65288;ROM&#65289;&#12290;ROM&#25429;&#25417;&#20102;&#35266;&#23519;&#21040;&#30340;&#21160;&#24577;&#30340;&#19981;&#21464;&#23376;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29289;&#29702;&#31995;&#32479;&#19982;&#25968;&#23398;&#27169;&#22411;&#26377;&#22235;&#31181;&#20851;&#31995;&#65306;&#19981;&#21464;&#21494;&#29255;&#12289;&#19981;&#21464;&#27969;&#24418;&#12289;&#33258;&#32534;&#30721;&#22120;&#21644;&#26080;&#26041;&#31243;&#27169;&#22411;&#12290;&#35782;&#21035;&#19981;&#21464;&#27969;&#24418;&#21644;&#26080;&#26041;&#31243;&#27169;&#22411;&#38656;&#35201;&#23545;&#31995;&#32479;&#36827;&#34892;&#38381;&#29615;&#25805;&#20316;&#12290;&#19981;&#21464;&#21494;&#29255;&#21644;&#33258;&#32534;&#30721;&#22120;&#20063;&#21487;&#20197;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;&#21482;&#26377;&#19981;&#21464;&#21494;&#29255;&#21644;&#19981;&#21464;&#27969;&#24418;&#21487;&#20197;&#35782;&#21035;ROM&#65292;&#20854;&#20313;&#21482;&#33021;&#35782;&#21035;&#23436;&#25972;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#20174;&#29616;&#26377;&#25968;&#25454;&#20013;&#35782;&#21035;ROM&#30340;&#24120;&#35265;&#24773;&#20917;&#21482;&#33021;&#20351;&#29992;&#19981;&#21464;&#21494;&#29255;&#12290;&#25214;&#21040;&#19981;&#21464;&#21494;&#29255;&#38656;&#35201;&#36817;&#20284;&#39640;&#32500;&#20989;&#25968;&#12290;&#22312;&#20989;&#25968;&#36924;&#36817;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#21387;&#32553;&#24352;&#37327;&#31995;&#25968;&#30340;&#22810;&#39033;&#24335;&#65292;&#20854;&#22797;&#26434;&#24615;&#38543;&#32500;&#25968;&#22686;&#21152;&#32780;&#32447;&#24615;&#22686;&#21152;&#12290;&#19981;&#21464;&#27969;&#24418;&#20063;&#21487;&#20197;&#34987;&#25214;&#21040;&#20316;&#20026;&#20462;&#27491;&#20294;&#19981;&#23436;&#20840;&#21051;&#30011;ROM&#30340;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores how to identify a reduced order model (ROM) from a physical system. A ROM captures an invariant subset of the observed dynamics. We find that there are four ways a physical system can be related to a mathematical model: invariant foliations, invariant manifolds, autoencoders and equation-free models. Identification of invariant manifolds and equation-free models require closed-loop manipulation of the system. Invariant foliations and autoencoders can also use off-line data. Only invariant foliations and invariant manifolds can identify ROMs, the rest identify complete models. Therefore, the common case of identifying a ROM from existing data can only be achieved using invariant foliations.  Finding an invariant foliation requires approximating high-dimensional functions. For function approximation, we use polynomials with compressed tensor coefficients, whose complexity increases linearly with increasing dimensions. An invariant manifold can also be found as the fix
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiffOpt.jl&#65292;&#19968;&#20010;&#22522;&#20110;MathOptInterface&#26500;&#24314;&#30340;Julia&#24211;&#65292;&#23427;&#21487;&#20197;&#23545;&#20219;&#20309;&#27169;&#22411;&#30340;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#27714;&#35299;&#65292;&#19981;&#20165;&#38480;&#20110;&#20984;&#38181;&#35268;&#21010;&#21644;&#20108;&#27425;&#35268;&#21010;&#26631;&#20934;&#24418;&#24335;&#12290;&#20351;&#29992;&#35813;&#24211;, &#21487;&#20197;&#23454;&#29616;&#28789;&#27963;&#39640;&#25928;&#22320;&#27714;&#35299;&#19968;&#31995;&#21015;&#27169;&#22411;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20026;&#22522;&#20110;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2206.06135</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#21464;&#25442;&#23454;&#29616;&#28789;&#27963;&#21487;&#23548;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Flexible Differentiable Optimization via Model Transformations. (arXiv:2206.06135v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffOpt.jl&#65292;&#19968;&#20010;&#22522;&#20110;MathOptInterface&#26500;&#24314;&#30340;Julia&#24211;&#65292;&#23427;&#21487;&#20197;&#23545;&#20219;&#20309;&#27169;&#22411;&#30340;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#27714;&#35299;&#65292;&#19981;&#20165;&#38480;&#20110;&#20984;&#38181;&#35268;&#21010;&#21644;&#20108;&#27425;&#35268;&#21010;&#26631;&#20934;&#24418;&#24335;&#12290;&#20351;&#29992;&#35813;&#24211;, &#21487;&#20197;&#23454;&#29616;&#28789;&#27963;&#39640;&#25928;&#22320;&#27714;&#35299;&#19968;&#31995;&#21015;&#27169;&#22411;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20026;&#22522;&#20110;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffOpt.jl&#65292;&#36825;&#26159;&#19968;&#20010;Julia&#24211;&#65292;&#23427;&#21487;&#20197;&#23545;&#21253;&#21547;&#30446;&#26631;&#20540;&#21644;/&#25110;&#32422;&#26463;&#26465;&#20214;&#30340;&#20219;&#24847;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#65292;&#24182;&#36827;&#34892;&#24494;&#20998;&#12290;&#36825;&#20010;&#24211;&#22522;&#20110;MathOptInterface&#26500;&#24314;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#30340;&#27714;&#35299;&#22120;&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#19982;&#20687;JuMP&#36825;&#26679;&#30340;&#24314;&#27169;&#35821;&#35328;&#32452;&#21512;&#24471;&#24456;&#22909;&#12290;DiffOpt&#25552;&#20379;&#21069;&#21521;&#21644;&#21453;&#21521;&#24494;&#20998;&#27169;&#24335;&#65292;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#21453;&#21521;&#20256;&#25773;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#31561;&#22810;&#31181;&#29992;&#36884;&#65292;&#36890;&#36807;&#23558;&#32422;&#26463;&#20248;&#21270;&#19982;&#31471;&#21040;&#31471;&#21487;&#23548;&#24615;&#32534;&#31243;&#30456;&#32467;&#21512;&#12290;DiffOpt&#26159;&#22522;&#20110;&#20004;&#20010;&#24050;&#30693;&#30340;&#35268;&#21017;&#26469;&#27714;&#35299;&#30340;&#65292;&#20998;&#21035;&#26159;&#20984;&#38181;&#35268;&#21010;&#21644;&#20108;&#27425;&#35268;&#21010;&#26631;&#20934;&#24418;&#24335;&#30340;&#24494;&#20998;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#36716;&#25442;&#36827;&#34892;&#24494;&#20998;&#65292;&#22240;&#27492;&#29992;&#25143;&#19981;&#20165;&#38480;&#20110;&#36825;&#20123;&#24418;&#24335;&#65292;&#36824;&#21487;&#20197;&#38024;&#23545;&#21487;&#20197;&#36716;&#25442;&#20026;&#36825;&#20123;&#26631;&#20934;&#24418;&#24335;&#30340;&#20219;&#20309;&#27169;&#22411;&#30340;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#12290;&#36825;&#29305;&#21035;&#21253;&#25324;&#28151;&#21512;&#20223;&#23556;&#38181;&#32422;&#26463;&#21644;&#20984;&#22235;&#27425;&#32422;&#26463;&#30340;&#31243;&#24207;&#65292;&#36825;&#20123;&#31243;&#24207;&#26080;&#27861;&#30452;&#25509;&#24314;&#27169;&#20026;&#26631;&#20934;&#38181;&#24418;&#31243;&#24207;&#12290;&#22240;&#27492;&#65292;DiffOpt&#20351;&#24471;&#21487;&#20197;&#28789;&#27963;&#39640;&#25928;&#22320;&#27714;&#35299;&#19968;&#31995;&#21015;&#27169;&#22411;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20026;&#22522;&#20110;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DiffOpt.jl, a Julia library to differentiate through the solution of optimization problems with respect to arbitrary parameters present in the objective and/or constraints. The library builds upon MathOptInterface, thus leveraging the rich ecosystem of solvers and composing well with modeling languages like JuMP. DiffOpt offers both forward and reverse differentiation modes, enabling multiple use cases from hyperparameter optimization to backpropagation and sensitivity analysis, bridging constrained optimization with end-to-end differentiable programming. DiffOpt is built on two known rules for differentiating quadratic programming and conic programming standard forms. However, thanks ability to differentiate through model transformation, the user is not limited to these forms and can differentiate with respect to the parameters of any model that can be reformulated into these standard forms. This notably includes programs mixing affine conic constraints and convex quadrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#20013;&#30340;&#24377;&#24615;&#36317;&#31163;&#24230;&#37327;&#31639;&#27861;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;&#24120;&#29992;&#30340;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#20043;&#19968;-DTW&#65292;&#22312;&#19982;k-means&#32467;&#21512;&#26102;&#34920;&#29616;&#19981;&#22914;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#12290;&#20351;&#29992;k-medoids&#21487;&#20197;&#25913;&#21892;&#32858;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.15181</link><description>&lt;p&gt;
&#24377;&#24615;&#36317;&#31163;&#20989;&#25968;&#22312;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#20013;&#30340;&#32508;&#36848;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Review and Evaluation of Elastic Distance Functions for Time Series Clustering. (arXiv:2205.15181v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#20013;&#30340;&#24377;&#24615;&#36317;&#31163;&#24230;&#37327;&#31639;&#27861;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;&#24120;&#29992;&#30340;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#20043;&#19968;-DTW&#65292;&#22312;&#19982;k-means&#32467;&#21512;&#26102;&#34920;&#29616;&#19981;&#22914;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#12290;&#20351;&#29992;k-medoids&#21487;&#20197;&#25913;&#21892;&#32858;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26159;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#32452;&#30340;&#34892;&#20026;&#65292;&#19981;&#38656;&#35201;&#26631;&#31614;&#12290;&#32858;&#31867;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#31639;&#27861;&#21487;&#20197;&#20998;&#20026;&#20004;&#32452;&#65306;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#36317;&#31163;&#24230;&#37327;&#30340;&#31639;&#27861;&#21644;&#20174;&#26102;&#38388;&#24207;&#21015;&#25512;&#23548;&#29305;&#24449;&#30340;&#31639;&#27861;&#12290;&#20004;&#31181;&#26041;&#27861;&#36890;&#24120;&#37117;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#27604;&#22914;$k$-means&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22522;&#20110;&#24377;&#24615;&#36317;&#31163;&#24230;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#36317;&#31163;&#31639;&#27861;&#65292;&#21363;&#22312;&#27979;&#37327;&#36317;&#31163;&#26102;&#25191;&#34892;&#26576;&#31181;&#23454;&#38469;&#23545;&#20854;&#25805;&#20316;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20061;&#31181;&#24120;&#29992;&#30340;&#24377;&#24615;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;k-means&#21644;k-medoids&#32858;&#31867;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20196;&#20154;&#24847;&#22806;&#12290;&#26368;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;-&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;(DTW)&#65292;&#22312;&#19982;k-means&#32467;&#21512;&#26102;&#27604;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#34920;&#29616;&#26356;&#24046;&#65292;&#21363;&#20351;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#25928;&#26524;&#20063;&#19981;&#22909;&#12290;&#20351;&#29992;k-medoids&#32780;&#19981;&#26159;k-means&#23545;&#25152;&#26377;&#20061;&#20010;&#36317;&#31163;&#24230;&#37327;&#37117;&#25913;&#21892;&#20102;&#32858;&#31867;&#25928;&#26524;&#12290;DTW&#19982;k-medoids&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#19968;&#33324;&#32780;&#35328;&#65292;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;&#32858;&#31867;&#30340;&#31639;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series clustering is the act of grouping time series data without recourse to a label. Algorithms that cluster time series can be classified into two groups: those that employ a time series specific distance measure; and those that derive features from time series. Both approaches usually rely on traditional clustering algorithms such as $k$-means. Our focus is on distance based time series that employ elastic distance measures, i.e. distances that perform some kind of realignment whilst measuring distance. We describe nine commonly used elastic distance measures and compare their performance with k-means and k-medoids clustering. Our findings are surprising. The most popular technique, dynamic time warping (DTW), performs worse than Euclidean distance with k-means, and even when tuned, is no better. Using k-medoids rather than k-means improved the clusterings for all nine distance measures. DTW is not significantly better than Euclidean distance with k-medoids. Generally, distanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#35268;&#21017;&#25628;&#32034;&#25216;&#26415;&#65292;&#26089;&#26399;&#35782;&#21035;&#26377;&#21487;&#33021;&#25104;&#20026;&#38271;&#26399;&#25110;&#24930;&#24615;&#26080;&#23478;&#21487;&#24402;&#32773;&#25910;&#23481;&#25152;&#30340;&#39640;&#39118;&#38505;&#20154;&#32676;&#12290;&#22312;&#23454;&#26102;&#20132;&#20184;&#25903;&#25345;&#24615;&#20303;&#25151;&#35745;&#21010;&#30340;&#26694;&#26550;&#20869;&#65292;&#24212;&#29992;&#26412;&#25991;&#26041;&#27861;&#20351;&#24471;&#26089;&#26399;&#35782;&#21035;&#22788;&#20110;&#24930;&#24615;&#26080;&#23478;&#21487;&#24402;&#32773;&#39118;&#38505;&#30340;&#23458;&#25143;&#30340;&#20013;&#20301;&#26102;&#38388;&#20174;297&#22825;&#38477;&#33267;162&#22825;&#12290;</title><link>http://arxiv.org/abs/2205.09883</link><description>&lt;p&gt;
&#19968;&#20010;&#35268;&#21017;&#25628;&#32034;&#26694;&#26550;&#29992;&#20110;&#26089;&#26399;&#35782;&#21035;&#24930;&#24615;&#32039;&#24613;&#26080;&#23478;&#21487;&#24402;&#32773;
&lt;/p&gt;
&lt;p&gt;
A Rule Search Framework for the Early Identification of Chronic Emergency Homeless Shelter Clients. (arXiv:2205.09883v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#35268;&#21017;&#25628;&#32034;&#25216;&#26415;&#65292;&#26089;&#26399;&#35782;&#21035;&#26377;&#21487;&#33021;&#25104;&#20026;&#38271;&#26399;&#25110;&#24930;&#24615;&#26080;&#23478;&#21487;&#24402;&#32773;&#25910;&#23481;&#25152;&#30340;&#39640;&#39118;&#38505;&#20154;&#32676;&#12290;&#22312;&#23454;&#26102;&#20132;&#20184;&#25903;&#25345;&#24615;&#20303;&#25151;&#35745;&#21010;&#30340;&#26694;&#26550;&#20869;&#65292;&#24212;&#29992;&#26412;&#25991;&#26041;&#27861;&#20351;&#24471;&#26089;&#26399;&#35782;&#21035;&#22788;&#20110;&#24930;&#24615;&#26080;&#23478;&#21487;&#24402;&#32773;&#39118;&#38505;&#30340;&#23458;&#25143;&#30340;&#20013;&#20301;&#26102;&#38388;&#20174;297&#22825;&#38477;&#33267;162&#22825;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#35268;&#21017;&#25628;&#32034;&#25216;&#26415;&#65292;&#26089;&#26399;&#35782;&#21035;&#37027;&#20123;&#26377;&#21487;&#33021;&#25104;&#20026;&#38271;&#26399;&#25110;&#24930;&#24615;&#20303;&#22312;&#26080;&#23478;&#21487;&#24402;&#32773;&#25910;&#23481;&#25152;&#30340;&#39640;&#39118;&#38505;&#20154;&#32676;&#12290;&#20351;&#29992;&#19968;&#23478;&#21271;&#32654;&#20027;&#35201;&#25910;&#23481;&#25152;&#26381;&#21153;&#19982;&#36229;&#36807;40,000&#20154;&#30340;12&#24180;&#20132;&#20114;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#20248;&#21270;&#20462;&#21098;&#26080;&#24207;&#25628;&#32034;(OPUS)&#31639;&#27861;&#65292;&#24320;&#21457;&#30452;&#35266;&#32780;&#26377;&#25928;&#30340;&#35268;&#21017;&#12290;&#22312;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#20132;&#20184;&#25903;&#25345;&#24615;&#20303;&#25151;&#35745;&#21010;&#30340;&#26694;&#26550;&#20869;&#65292;&#23545;&#36825;&#20123;&#35268;&#21017;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24212;&#29992;&#26412;&#25991;&#26041;&#27861;&#20351;&#24471;&#26089;&#26399;&#35782;&#21035;&#22788;&#20110;&#24930;&#24615;&#26080;&#23478;&#21487;&#24402;&#32773;&#39118;&#38505;&#30340;&#23458;&#25143;&#30340;&#20013;&#20301;&#26102;&#38388;&#20174;297&#22825;&#38477;&#33267;162&#22825;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper uses rule search techniques for the early identification of emergency homeless shelter clients who are at risk of becoming long term or chronic shelter users. Using a data set from a major North American shelter containing 12 years of service interactions with over 40,000 individuals, the optimized pruning for unordered search (OPUS) algorithm is used to develop rules that are both intuitive and effective. The rules are evaluated within a framework compatible with the real-time delivery of a housing program meant to transition high risk clients to supportive housing. Results demonstrate that the median time to identification of clients at risk of chronic shelter use drops from 297 days to 162 days when the methods in this paper are applied.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DECONET&#30340;&#26032;&#22411;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#65292;&#21487;&#20197;&#29992;&#20110;&#22522;&#20110;&#20998;&#26512;&#31232;&#30095;&#24615;&#30340;&#21387;&#32553;&#24863;&#30693;&#65292;&#33021;&#26377;&#25928;&#22320;&#37325;&#26500;&#21521;&#37327;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#23637;&#24320;&#32593;&#32476;&#65292;&#22312;&#20272;&#35745;&#20102;&#20854;&#27867;&#21270;&#35823;&#24046;&#30340;&#22522;&#30784;&#19978;&#24471;&#20986;&#30456;&#20851;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2205.07050</link><description>&lt;p&gt;
DECONET&#65306;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;&#31232;&#30095;&#24615;&#21387;&#32553;&#24863;&#30693;&#30340;&#23637;&#24320;&#32593;&#32476;&#21450;&#20854;&#27867;&#21270;&#35823;&#24046;&#30028;
&lt;/p&gt;
&lt;p&gt;
DECONET: an Unfolding Network for Analysis-based Compressed Sensing with Generalization Error Bounds. (arXiv:2205.07050v6 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DECONET&#30340;&#26032;&#22411;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#65292;&#21487;&#20197;&#29992;&#20110;&#22522;&#20110;&#20998;&#26512;&#31232;&#30095;&#24615;&#30340;&#21387;&#32553;&#24863;&#30693;&#65292;&#33021;&#26377;&#25928;&#22320;&#37325;&#26500;&#21521;&#37327;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#23637;&#24320;&#32593;&#32476;&#65292;&#22312;&#20272;&#35745;&#20102;&#20854;&#27867;&#21270;&#35823;&#24046;&#30340;&#22522;&#30784;&#19978;&#24471;&#20986;&#30456;&#20851;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;DECONET&#65292;&#29992;&#20110;&#22522;&#20110;&#20998;&#26512;&#31232;&#30095;&#24615;&#30340;&#21387;&#32553;&#24863;&#30693;&#12290;DECONET&#32852;&#21512;&#23398;&#20064;&#19968;&#20010;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#20174;&#19981;&#23436;&#25972;&#12289;&#22122;&#22768;&#27979;&#37327;&#20013;&#37325;&#26500;&#21521;&#37327;&#65292;&#20197;&#21450;&#19968;&#20010;&#20887;&#20313;&#30340;&#31232;&#30095;&#20998;&#26512;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#22312;DECONET&#30340;&#21508;&#20010;&#23618;&#20043;&#38388;&#20849;&#20139;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;DECONET&#30340;&#20551;&#35774;&#31867;&#24182;&#20272;&#35745;&#20854;&#30456;&#20851;&#30340;Rademacher&#22797;&#26434;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#20272;&#35745;&#32467;&#26524;&#20026;DECONET&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#19978;&#38480;&#65292;&#29992;&#20110;&#35780;&#20272;DECONET&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#23637;&#24320;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#65292;&#24182;&#19988;&#20854;&#34892;&#20026;&#31526;&#21512;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new deep unfolding network for analysis-sparsity-based Compressed Sensing. The proposed network coined Decoding Network (DECONET) jointly learns a decoder that reconstructs vectors from their incomplete, noisy measurements and a redundant sparsifying analysis operator, which is shared across the layers of DECONET. Moreover, we formulate the hypothesis class of DECONET and estimate its associated Rademacher complexity. Then, we use this estimate to deliver meaningful upper bounds for the generalization error of DECONET. Finally, the validity of our theoretical results is assessed and comparisons to state-of-the-art unfolding networks are made, on both synthetic and real-world datasets. Experimental results indicate that our proposed network outperforms the baselines, consistently for all datasets, and its behaviour complies with our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20986;&#19968;&#31181;&#26816;&#27979;&#24613;&#35786;&#31185;&#20998;&#35786;&#21069;&#36133;&#34880;&#30151;&#30340;&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#26631;&#20934;&#36133;&#34880;&#30151;&#31579;&#26597;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.07657</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#24613;&#35786;&#31185;&#20998;&#35786;&#20013;&#26816;&#27979;&#36133;&#34880;&#30151;
&lt;/p&gt;
&lt;p&gt;
Detection of sepsis during emergency department triage using machine learning. (arXiv:2204.07657v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20986;&#19968;&#31181;&#26816;&#27979;&#24613;&#35786;&#31185;&#20998;&#35786;&#21069;&#36133;&#34880;&#30151;&#30340;&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#26631;&#20934;&#36133;&#34880;&#30151;&#31579;&#26597;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#22120;&#23448;&#21151;&#33021;&#38556;&#30861;&#30340;&#36133;&#34880;&#30151;&#26159;&#20840;&#29699;&#27515;&#20129;&#21644;&#21361;&#37325;&#30142;&#30149;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#27604;&#36739;&#26631;&#20934;&#36133;&#34880;&#30151;&#31579;&#26597;&#31639;&#27861;&#21644;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20998;&#35786;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#24613;&#35786;&#31185;&#20998;&#35786;&#21069;&#65288;&#26410;&#20351;&#29992;&#23454;&#39564;&#23460;&#35786;&#26029;&#65289;&#30340;&#36133;&#34880;&#30151;&#26816;&#27979;&#24615;&#33021;&#12290;&#30740;&#31350;&#24471;&#20986;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340; AUC &#20026; 0.9423&#65292;&#25935;&#24863;&#24615;&#20026; 71.09%&#12290;
&lt;/p&gt;
&lt;p&gt;
Sepsis is a life-threatening condition with organ dysfunction and is a leading cause of death and critical illness worldwide. Even a few hours of delay in the treatment of sepsis results in increased mortality. Early detection of sepsis during emergency department triage would allow early initiation of lab analysis, antibiotic administration, and other sepsis treatment protocols. The purpose of this study was to compare sepsis detection performance at ED triage (prior to the use of laboratory diagnostics) of the standard sepsis screening algorithm (SIRS with source of infection) and a machine learning algorithm trained on EHR triage data. A machine learning model (KATE Sepsis) was developed using patient encounters with triage data from 16participating hospitals. KATE Sepsis and standard screening were retrospectively evaluated on the adult population of 512,949 medical records. KATE Sepsis demonstrates an AUC of 0.9423 (0.9401 - 0.9441) with sensitivity of 71.09% (70.12% - 71.98%) and
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;&#24050;&#32463;&#23384;&#22312;&#30340;GNN&#65292;&#24182;&#26681;&#25454;&#20854;&#22788;&#29702;&#19981;&#21516;&#22270;&#31867;&#22411;&#21644;&#23646;&#24615;&#30340;&#33021;&#21147;&#23545;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2204.03080</link><description>&lt;p&gt;
&#38754;&#21521;&#19981;&#21516;&#22270;&#31867;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks Designed for Different Graph Types: A Survey. (arXiv:2204.03080v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;&#24050;&#32463;&#23384;&#22312;&#30340;GNN&#65292;&#24182;&#26681;&#25454;&#20854;&#22788;&#29702;&#19981;&#21516;&#22270;&#31867;&#22411;&#21644;&#23646;&#24615;&#30340;&#33021;&#21147;&#23545;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22312;&#33258;&#28982;&#30028;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#22240;&#27492;&#21487;&#20197;&#20316;&#20026;&#35768;&#22810;&#23454;&#38469;&#20294;&#20063;&#26159;&#29702;&#35770;&#38382;&#39064;&#30340;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#23427;&#20204;&#21487;&#20197;&#23450;&#20041;&#20026;&#36866;&#21512;&#21453;&#26144;&#25152;&#34920;&#31034;&#38382;&#39064;&#30340;&#21508;&#20010;&#19978;&#19979;&#25991;&#30340;&#35768;&#22810;&#19981;&#21516;&#31867;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#22522;&#20110;&#22270;&#25968;&#25454;&#30340;&#23574;&#31471;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#24456;&#24180;&#36731;&#65292;&#26032;&#27169;&#22411;&#30340;&#24320;&#21457;&#36895;&#24230;&#24456;&#24555;&#65292;&#20294;&#35768;&#22810;&#26368;&#36817;&#30340;&#35843;&#26597;&#24050;&#32463;&#21457;&#24067;&#65292;&#20197;&#36319;&#36394;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#36824;&#27809;&#26377;&#25910;&#38598;&#21040;&#21738;&#20010;GNN&#21487;&#20197;&#22788;&#29702;&#20160;&#20040;&#31867;&#22411;&#30340;&#22270;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#24050;&#32463;&#23384;&#22312;&#30340;GNN&#65292;&#19982;&#20197;&#21069;&#30340;&#35843;&#26597;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#26681;&#25454;&#23427;&#20204;&#22788;&#29702;&#19981;&#21516;&#22270;&#31867;&#22411;&#21644;&#23646;&#24615;&#30340;&#33021;&#21147;&#23545;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#32771;&#34385;&#25805;&#20316;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#26500;&#25104;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#22270;&#30340;GNN&#65292;&#20855;&#26377;&#25110;&#19981;&#20855;&#26377;&#33410;&#28857;&#25110;&#36793;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21306;&#20998;&#31163;&#25955;&#26102;&#38388;&#25110;&#36830;&#32493;&#26102;&#38388;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are ubiquitous in nature and can therefore serve as models for many practical but also theoretical problems. For this purpose, they can be defined as many different types which suitably reflect the individual contexts of the represented problem. To address cutting-edge problems based on graph data, the research field of Graph Neural Networks (GNNs) has emerged. Despite the field's youth and the speed at which new models are developed, many recent surveys have been published to keep track of them. Nevertheless, it has not yet been gathered which GNN can process what kind of graph types. In this survey, we give a detailed overview of already existing GNNs and, unlike previous surveys, categorize them according to their ability to handle different graph types and properties. We consider GNNs operating on static and dynamic graphs of different structural constitutions, with or without node or edge attributes. Moreover, we distinguish between GNN models for discrete-time or continuou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28789;&#27963;&#35774;&#35745;&#30340;&#22810;&#37327;&#23376;&#27604;&#29305;&#38376;&#25805;&#20316;&#30340;&#37327;&#23376;&#21464;&#20998;&#25351;&#20196;&#38598;(QuVIS)&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#26102;&#38388;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#31934;&#30830;&#30340;&#37327;&#23376;&#35745;&#31639;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20986;&#20102;&#26356;&#20302;&#30340;&#35823;&#24046;&#31215;&#32047;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2203.15574</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#20998;&#25351;&#20196;&#38598;&#30340;&#37327;&#23376;&#32534;&#35793;&#20248;&#21270;&#65306;&#23454;&#29616;&#39640;&#31934;&#24230;&#19982;&#24555;&#36895;&#37327;&#23376;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Quantum compiling with variational instruction set for accurate and fast quantum computing. (arXiv:2203.15574v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28789;&#27963;&#35774;&#35745;&#30340;&#22810;&#37327;&#23376;&#27604;&#29305;&#38376;&#25805;&#20316;&#30340;&#37327;&#23376;&#21464;&#20998;&#25351;&#20196;&#38598;(QuVIS)&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#26102;&#38388;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#31934;&#30830;&#30340;&#37327;&#23376;&#35745;&#31639;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20986;&#20102;&#26356;&#20302;&#30340;&#35823;&#24046;&#31215;&#32047;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#25351;&#20196;&#38598;(QIS)&#23450;&#20041;&#20026;&#22312;&#25511;&#21046;&#37327;&#23376;&#27604;&#29305;&#29366;&#24577;&#19979;&#21487;&#20197;&#29289;&#29702;&#23454;&#29616;&#30340;&#19968;&#31995;&#21015;&#37327;&#23376;&#38376;&#25805;&#20316;&#65292;&#20854;&#22312;&#37327;&#23376;&#35745;&#31639;&#20013;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28789;&#27963;&#35774;&#35745;&#30340;&#22810;&#37327;&#23376;&#27604;&#29305;&#38376;&#25805;&#20316;&#30340;&#37327;&#23376;&#21464;&#20998;&#25351;&#20196;&#38598;(QuVIS)&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#26102;&#38388;&#20248;&#21270;&#31639;&#27861;&#26469;&#21464;&#20998;&#23454;&#29616;&#37327;&#23376;&#27604;&#29305;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#21644;&#31934;&#30830;&#30340;&#37327;&#23376;&#35745;&#31639;&#12290;&#19982;&#26631;&#20934;QIS &#22914;&#37327;&#23376;&#24494;&#25351;&#20196;&#38598;(QuMIS)&#30456;&#27604;&#65292;QuVIS &#29992;&#20110;&#22810;&#37327;&#23376;&#27604;&#29305;&#20132;&#25442;&#21644;&#37327;&#23376;&#20613;&#37324;&#21494;&#21464;&#25442;&#31561;&#38376;&#25805;&#20316;&#20855;&#26377;&#26356;&#20302;&#30340;&#35823;&#24046;&#31215;&#32047;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#22312;&#30456;&#21516;&#37327;&#23376;&#30828;&#20214;&#35201;&#27714;&#19979;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#21487;&#22823;&#24133;&#25552;&#21319;&#37327;&#23376;&#35745;&#31639;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quantum instruction set (QIS) is defined as the quantum gates that are physically realizable by controlling the qubits in a quantum hardware. Compiling quantum circuits into the product of the gates in a properly-defined QIS is a fundamental step in quantum computing. We here propose the \R{quantum variational instruction set (QuVIS)} formed by flexibly-designed multi-qubit gates for higher speed and accuracy of quantum computing. The controlling of qubits for realizing the gates in a QuVIS are variationally achieved using the fine-grained time optimization algorithm. Significant reductions on both the error accumulation and time cost are demonstrated in realizing the swaps of multiple qubits and quantum Fourier transformations, compared with the compiling by the standard QIS such as \RR{the quantum microinstruction set} (QuMIS, formed by several one- and two-qubit gates including the one-qubit rotations and controlled-NOT gate). With the same requirement on quantum hardware, the t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#20316;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31361;&#35302;&#30340;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#23454;&#29616;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#20851;&#38190;&#26399;&#30456;&#20851;&#30340;&#31070;&#32463;&#32010;&#20081;&#21644;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#23545;&#31361;&#35302;&#28608;&#27963;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2203.11740</link><description>&lt;p&gt;
&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#23545;&#20851;&#38190;&#26399;&#30340;&#31070;&#32463;&#21487;&#22609;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#23454;&#29616;&#31361;&#35302;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#12290;&#65288;arXiv: 2203.11740v12 [cs.NE] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation. (arXiv:2203.11740v12 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#20316;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31361;&#35302;&#30340;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#23454;&#29616;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#20851;&#38190;&#26399;&#30456;&#20851;&#30340;&#31070;&#32463;&#32010;&#20081;&#21644;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#23545;&#31361;&#35302;&#28608;&#27963;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#31361;&#35302;&#20849;&#20139;&#36830;&#25509;&#26435;&#37325;&#20043;&#22806;&#65292;PNN&#36824;&#21253;&#25324;&#31361;&#35302;&#26377;&#25928;&#33539;&#22260;&#30340;&#26435;&#37325;[14-25]&#12290;PNN&#32771;&#34385;&#31361;&#35302;&#24378;&#24230;&#24179;&#34913;&#22312;&#31361;&#35302;&#21534;&#22124;&#30340;&#21160;&#24577;&#21644;&#38271;&#24230;&#24120;&#25968;&#20043;&#21644;&#30340;&#38745;&#24577;&#20013;[14]&#65292;&#24182;&#21253;&#21547;&#20102;&#40060;&#32676;&#34892;&#20026;&#30340;&#20808;&#23548;&#34892;&#20026;&#12290;&#31361;&#35302;&#24418;&#25104;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#20250;&#25233;&#21046;&#26641;&#31361;&#29983;&#25104;[15]&#12290;&#31867;&#20284;&#20110;Spring Boot&#20013;&#30340;&#24378;&#21046;&#38887;&#24615;&#65292;&#21453;&#21521;&#22238;&#36335;&#30340;&#35760;&#24518;&#25345;&#20037;&#24230;&#26799;&#24230;&#20063;&#23384;&#22312;&#12290;&#30456;&#23545;&#36739;&#22909;&#21644;&#36739;&#24046;&#30340;&#26799;&#24230;&#20449;&#24687;&#23384;&#20648;&#22312;&#31867;&#20284;&#20110;&#33041;&#35126;&#30340;&#35760;&#24518;&#30165;&#36857;&#32454;&#32990;&#20013;&#65292;&#22312;&#21453;&#21521;&#22238;&#36335;&#30340;&#31361;&#35302;&#24418;&#25104;&#20013;&#12290;&#20105;&#35758;&#35748;&#20026;&#20154;&#31867;&#28023;&#39532;&#31070;&#32463;&#20803;&#30340;&#20877;&#29983;&#33021;&#21147;&#26159;&#21542;&#25345;&#32493;&#21040;&#32769;&#24180;&#65292;&#24182;&#21487;&#33021;&#22312;&#21518;&#26399;&#36845;&#20195;&#20013;&#24418;&#25104;&#26032;&#30340;&#26356;&#38271;&#30340;&#22238;&#36335;[17,18]&#12290;&#20851;&#38381;&#20851;&#38190;&#26399;&#20250;&#23548;&#33268;&#31070;&#32463;&#32010;&#20081;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;[19]&#12290;&#32771;&#34385;&#21040;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#28608;&#27963;&#31361;&#35302;&#12290;
&lt;/p&gt;
&lt;p&gt;
In addition to the weights of synaptic shared connections, PNN includes weights of synaptic effective ranges [14-25]. PNN considers synaptic strength balance in dynamic of phagocytosing of synapses and static of constant sum of synapses length [14], and includes the lead behavior of the school of fish. Synapse formation will inhibit dendrites generation in experiments and simulations [15]. The memory persistence gradient of retrograde circuit similar to the Enforcing Resilience in a Spring Boot. The relatively good and inferior gradient information stored in memory engram cells in synapse formation of retrograde circuit like the folds in brain [16]. The controversy was claimed if human hippocampal neurogenesis persists throughout aging, may have a new and longer circuit in late iteration [17,18]. Closing the critical period will cause neurological disorder in experiments and simulations [19]. Considering both negative and positive memories persistence help activate synapse better than 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35889;&#26465;&#20214;&#65292;&#29992;&#20110;&#25552;&#20379;&#20855;&#26377;&#24130;&#24459;&#20248;&#21270;&#36712;&#36857;&#30340;&#38382;&#39064;&#30340;&#26356;&#32039;&#23494;&#19978;&#30028;&#65292;&#28436;&#31034;&#20102;&#22914;&#20309;&#32479;&#19968;&#33719;&#24471;&#26368;&#20248;&#21152;&#36895;&#26041;&#27861;&#21450;&#20854;&#35745;&#21010;&#21644;&#25910;&#25947;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2202.00992</link><description>&lt;p&gt;
&#22522;&#20110;&#24130;&#24459;&#35889;&#26465;&#20214;&#19979;&#30340;&#20248;&#21270;&#25910;&#25947;&#29575;&#32039;&#23494;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions. (arXiv:2202.00992v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35889;&#26465;&#20214;&#65292;&#29992;&#20110;&#25552;&#20379;&#20855;&#26377;&#24130;&#24459;&#20248;&#21270;&#36712;&#36857;&#30340;&#38382;&#39064;&#30340;&#26356;&#32039;&#23494;&#19978;&#30028;&#65292;&#28436;&#31034;&#20102;&#22914;&#20309;&#32479;&#19968;&#33719;&#24471;&#26368;&#20248;&#21152;&#36895;&#26041;&#27861;&#21450;&#20854;&#35745;&#21010;&#21644;&#25910;&#25947;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20108;&#27425;&#38382;&#39064;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;&#21462;&#20915;&#20110;&#35889;&#30340;&#20302;&#33021;&#37096;&#20998;&#12290;&#23545;&#20110;&#22823;&#22411;&#65288;&#26377;&#25928;&#26080;&#38480;&#32500;&#65289;&#38382;&#39064;&#65292;&#36825;&#37096;&#20998;&#35889;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#24130;&#24459;&#20998;&#24067;&#33258;&#28982;&#34920;&#31034;&#25110;&#36817;&#20284;&#65292;&#23548;&#33268;&#26799;&#24230;&#31639;&#27861;&#30340;&#36845;&#20195;&#35299;&#34920;&#29616;&#20986;&#24130;&#24459;&#25910;&#25947;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35889;&#26465;&#20214;&#65292;&#29992;&#20110;&#25552;&#20379;&#20855;&#26377;&#24130;&#24459;&#20248;&#21270;&#36712;&#36857;&#30340;&#38382;&#39064;&#30340;&#26356;&#32039;&#23494;&#19978;&#30028;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26465;&#20214;&#26469;&#24314;&#31435;&#19968;&#24352;&#24191;&#27867;&#20248;&#21270;&#31639;&#27861;&#30340;&#19978;&#19979;&#30028;&#23436;&#25972;&#22270;&#20687;&#8212;&#8212;&#26799;&#24230;&#19979;&#38477;&#12289;&#26368;&#38497;&#19979;&#38477;&#12289;&#37325;&#29699;&#12289;&#20849;&#36717;&#26799;&#24230;&#8212;&#8212;&#24182;&#24378;&#35843;&#20102;&#23398;&#20064;&#29575;&#21644;&#21160;&#37327;&#30340;&#22522;&#26412;&#35745;&#21010;&#12290;&#29305;&#21035;&#30340;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#32479;&#19968;&#33719;&#24471;&#26368;&#20248;&#21152;&#36895;&#26041;&#27861;&#21450;&#20854;&#35745;&#21010;&#21644;&#25910;&#25947;&#19978;&#30028;&#65292;&#23545;&#20110;&#32473;&#23450;&#35889;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#20110;&#39318;&#20010;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance of optimization on quadratic problems sensitively depends on the low-lying part of the spectrum. For large (effectively infinite-dimensional) problems, this part of the spectrum can often be naturally represented or approximated by power law distributions, resulting in power law convergence rates for iterative solutions of these problems by gradient-based algorithms. In this paper, we propose a new spectral condition providing tighter upper bounds for problems with power law optimization trajectories. We use this condition to build a complete picture of upper and lower bounds for a wide range of optimization algorithms -- Gradient Descent, Steepest Descent, Heavy Ball, and Conjugate Gradients -- with an emphasis on the underlying schedules of learning rate and momentum. In particular, we demonstrate how an optimally accelerated method, its schedule, and convergence upper bound can be obtained in a unified manner for a given shape of the spectrum. Also, we provide first proo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#39057;&#27604;&#29305;&#27969;&#20803;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#30340;&#21387;&#32553;&#35270;&#39057;&#36136;&#37327;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#26356;&#39640;&#21534;&#21520;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#21387;&#32553;&#35270;&#39057;&#30340;&#36824;&#21407;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2202.00011</link><description>&lt;p&gt;
&#21033;&#29992;&#27604;&#29305;&#27969;&#20803;&#25968;&#25454;&#24555;&#36895;&#12289;&#20934;&#30830;&#12289;&#27867;&#21270;&#22320;&#21387;&#32553;&#35270;&#39057;&#36136;&#37327;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Leveraging Bitstream Metadata for Fast, Accurate, Generalized Compressed Video Quality Enhancement. (arXiv:2202.00011v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#39057;&#27604;&#29305;&#27969;&#20803;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#30340;&#21387;&#32553;&#35270;&#39057;&#36136;&#37327;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#26356;&#39640;&#21534;&#21520;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#21387;&#32553;&#35270;&#39057;&#30340;&#36824;&#21407;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#21387;&#32553;&#26159;&#29616;&#20195;&#20114;&#32852;&#32593;&#30340;&#26680;&#24515;&#21151;&#33021;&#65292;&#28085;&#30422;&#20174;&#31038;&#20132;&#23186;&#20307;&#21040;&#35270;&#39057;&#20250;&#35758;&#31561;&#25216;&#26415;&#12290;&#34429;&#28982;&#35270;&#39057;&#21387;&#32553;&#19981;&#26029;&#25104;&#29087;&#65292;&#20294;&#22312;&#35768;&#22810;&#21387;&#32553;&#35774;&#32622;&#20013;&#65292;&#36136;&#37327;&#25439;&#22833;&#20173;&#28982;&#24456;&#26126;&#26174;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#32622;&#23545;&#20110;&#22312;&#24102;&#23485;&#21463;&#38480;&#25110;&#19981;&#31283;&#23450;&#30340;&#36830;&#25509;&#19978;&#39640;&#25928;&#20256;&#36755;&#35270;&#39057;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#21033;&#29992;&#23884;&#20837;&#35270;&#39057;&#27604;&#29305;&#27969;&#20013;&#30340;&#24213;&#23618;&#32467;&#26500;&#21644;&#21160;&#24577;&#20449;&#24687;&#65292;&#20026;&#21387;&#32553;&#35270;&#39057;&#24674;&#22797;&#32454;&#33410;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#20110;&#20197;&#21069;&#30340;&#21387;&#32553;&#26657;&#27491;&#26041;&#27861;&#25552;&#39640;&#20102;&#24674;&#22797;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#23454;&#29616;&#26356;&#39640;&#21534;&#21520;&#37327;&#30340;&#21516;&#26102;&#65292;&#19982;&#26368;&#36817;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#39057;&#21387;&#32553;&#26041;&#27861;&#22312;&#36895;&#29575;-&#22833;&#30495;&#27604;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#19978;&#36827;&#34892;&#37327;&#21270;&#25968;&#25454;&#26465;&#20214;&#21270;&#65292;&#36825;&#21487;&#36731;&#26494;&#22320;&#20174;&#27604;&#29305;&#27969;&#20013;&#33719;&#21462;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#21333;&#19968;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#19981;&#21516;&#30340;&#21387;&#32553;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video compression is a central feature of the modern internet powering technologies from social media to video conferencing. While video compression continues to mature, for many compression settings, quality loss is still noticeable. These settings nevertheless have important applications to the efficient transmission of videos over bandwidth constrained or otherwise unstable connections. In this work, we develop a deep learning architecture capable of restoring detail to compressed videos which leverages the underlying structure and motion information embedded in the video bitstream. We show that this improves restoration accuracy compared to prior compression correction methods and is competitive when compared with recent deep-learning-based video compression methods on rate-distortion while achieving higher throughput. Furthermore, we condition our model on quantization data which is readily available in the bitstream. This allows our single model to handle a variety of different c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37325;&#21551;&#24335;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#65288;AGD&#65289;&#21644;&#37325;&#21551;&#24335;&#37325;&#29699;&#65288;HB&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22522;&#26412;&#35777;&#26126;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#22312;$O(\epsilon^{-7/4})$&#20010;&#26799;&#24230;&#35780;&#20272;&#20869;&#36798;&#21040;$\epsilon$&#36817;&#20284;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#22797;&#26434;&#24230;&#27809;&#26377;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#65292;&#24182;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20339;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2201.11411</link><description>&lt;p&gt;
&#37325;&#21551;&#30340;&#38750;&#20984;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#65306;&#22312;$O(\epsilon^{-7/4})$&#22797;&#26434;&#24230;&#20013;&#19981;&#20877;&#38656;&#35201;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;
&lt;/p&gt;
&lt;p&gt;
Restarted Nonconvex Accelerated Gradient Descent: No More Polylogarithmic Factor in the $O(\epsilon^{-7/4})$ Complexity. (arXiv:2201.11411v4 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37325;&#21551;&#24335;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#65288;AGD&#65289;&#21644;&#37325;&#21551;&#24335;&#37325;&#29699;&#65288;HB&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22522;&#26412;&#35777;&#26126;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#22312;$O(\epsilon^{-7/4})$&#20010;&#26799;&#24230;&#35780;&#20272;&#20869;&#36798;&#21040;$\epsilon$&#36817;&#20284;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#22797;&#26434;&#24230;&#27809;&#26377;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#65292;&#24182;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20339;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20855;&#26377;Lipschitz&#36830;&#32493;&#26799;&#24230;&#21644;Hessian&#30340;&#38750;&#20984;&#20248;&#21270;&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#65292;&#37325;&#21551;&#24335;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#65288;AGD&#65289;&#21644;&#37325;&#21551;&#24335;&#37325;&#29699;&#65288;HB&#65289;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22522;&#26412;&#35777;&#26126;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;$O(\epsilon^{-7/4})$&#20010;&#26799;&#24230;&#35780;&#20272;&#20869;&#36798;&#21040;$\epsilon$&#36817;&#20284;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#30340;&#22797;&#26434;&#24230;&#27809;&#26377;&#38544;&#34255;&#20219;&#20309;&#30340;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#65292;&#24182;&#22240;&#27492;&#27604;&#24050;&#30693;&#30340;&#26368;&#20339;&#22797;&#26434;&#24230;&#25552;&#39640;&#20102;$O(\log \frac{1}{\epsilon})$&#30340;&#22240;&#23376;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#38750;&#24120;&#31616;&#21333;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#26159;&#30001;Nesterov&#30340;&#32463;&#20856;AGD&#25110;Polyak&#30340;HB&#36845;&#20195;&#20197;&#21450;&#37325;&#21551;&#26426;&#21046;&#32452;&#25104;&#12290;&#19982;&#29616;&#26377;&#30340;&#20998;&#26512;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#22522;&#26412;&#35777;&#26126;&#20351;&#29992;&#20102;&#19981;&#37027;&#20040;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#24182;&#19988;&#19981;&#28041;&#21450;&#24378;&#20984;&#24615;&#30340;&#20998;&#26512;&#25110;&#27491;&#21017;&#21270;&#20195;&#29702;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#20316;&#20026;&#23376;&#20363;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies accelerated gradient methods for nonconvex optimization with Lipschitz continuous gradient and Hessian. We propose two simple accelerated gradient methods, restarted accelerated gradient descent (AGD) and restarted heavy ball (HB) method, and establish that our methods achieve an $\epsilon$-approximate first-order stationary point within $O(\epsilon^{-7/4})$ number of gradient evaluations by elementary proofs. Theoretically, our complexity does not hide any polylogarithmic factors, and thus it improves over the best known one by the $O(\log\frac{1}{\epsilon})$ factor. Our algorithms are simple in the sense that they only consist of Nesterov's classical AGD or Polyak's HB iterations, as well as a restart mechanism. They do not invoke negative curvature exploitation or minimization of regularized surrogate functions as the subroutines. In contrast with existing analysis, our elementary proofs use less advanced techniques and do not invoke the analysis of strongly conve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#31614;&#21517;&#26041;&#27861;&#22312;&#23398;&#20064;&#31895;&#31961;&#21160;&#24577;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#27604;&#25130;&#26029;&#31614;&#21517;&#26041;&#27861;&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26356;&#20855;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2201.00384</link><description>&lt;p&gt;
&#38543;&#26426;&#31614;&#21517;&#20316;&#20026;&#23398;&#20064;&#31895;&#31961;&#21160;&#24577;&#30340;&#20648;&#22791;&#24211;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the effectiveness of Randomized Signatures as Reservoir for Learning Rough Dynamics. (arXiv:2201.00384v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.00384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#31614;&#21517;&#26041;&#27861;&#22312;&#23398;&#20064;&#31895;&#31961;&#21160;&#24577;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#27604;&#25130;&#26029;&#31614;&#21517;&#26041;&#27861;&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#37329;&#34701;&#12289;&#29289;&#29702;&#21644;&#24037;&#31243;&#29616;&#35937;&#37117;&#30001;&#39640;&#24230;&#19981;&#35268;&#21017;&#65288;&#38543;&#26426;&#65289;&#36755;&#20837;&#39537;&#21160;&#30340;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#12290;&#30001;&#20110;&#31895;&#36335;&#24452;&#29702;&#35770;&#65292;&#24182;&#21033;&#29992;&#25152;&#35859;&#30340;&#31614;&#21517;&#21464;&#25442;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25191;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#24050;&#32463;&#20986;&#29616;&#12290;&#35813;&#31639;&#27861;&#20139;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20294;&#24456;&#38590;&#25193;&#23637;&#21040;&#39640;&#32500;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#25512;&#23548;&#30340;&#38543;&#26426;&#25237;&#24433;&#21464;&#37327;&#65292;&#31216;&#20026;&#38543;&#26426;&#31614;&#21517;&#12290;&#25105;&#20204;&#23545;&#38543;&#26426;&#31614;&#21517;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#26088;&#22312;&#23637;&#31034;&#36825;&#31181;&#20648;&#22791;&#24211;&#23545;&#31038;&#21306;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#27169;&#22411;&#22797;&#26434;&#24615;&#12289;&#35757;&#32451;&#26102;&#38388;&#12289;&#20934;&#30830;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#25968;&#25454;&#38656;&#27714;&#26041;&#38754;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#25130;&#26029;&#31614;&#21517;&#26041;&#27861;&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many finance, physics, and engineering phenomena are modeled by continuous-time dynamical systems driven by highly irregular (stochastic) inputs. A powerful tool to perform time series analysis in this context is rooted in rough path theory and leverages the so-called Signature Transform. This algorithm enjoys strong theoretical guarantees but is hard to scale to high-dimensional data. In this paper, we study a recently derived random projection variant called Randomized Signature, obtained using the Johnson-Lindenstrauss Lemma. We provide an in-depth experimental evaluation of the effectiveness of the Randomized Signature approach, in an attempt to showcase the advantages of this reservoir to the community. Specifically, we find that this method is preferable to the truncated Signature approach and alternative deep learning techniques in terms of model complexity, training time, accuracy, robustness, and data hungriness.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#35299;&#37322;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#36807;&#24230;&#36924;&#36817;&#25968;&#20540;Fixpoint&#36845;&#20195;&#22120;&#12290;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#22495;&#65292;CH-Zonotope&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20256;&#25773;&#21644;&#21253;&#21547;&#26816;&#26597;&#12290;&#35813;&#26694;&#26550;&#22312;&#30740;&#31350;monDEQ&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#39564;&#35777;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2110.08260</link><description>&lt;p&gt;
&#25277;&#35937;&#35299;&#37322;&#19979;&#30340;Fixpoint&#36845;&#20195;&#22120;&#21450;&#20854;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Abstract Interpretation of Fixpoint Iterators with Applications to Neural Networks. (arXiv:2110.08260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08260
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#35299;&#37322;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#36807;&#24230;&#36924;&#36817;&#25968;&#20540;Fixpoint&#36845;&#20195;&#22120;&#12290;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#22495;&#65292;CH-Zonotope&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20256;&#25773;&#21644;&#21253;&#21547;&#26816;&#26597;&#12290;&#35813;&#26694;&#26550;&#22312;&#30740;&#31350;monDEQ&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#39564;&#35777;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25277;&#35937;&#35299;&#37322;&#26694;&#26550;&#65292;&#20197;&#31934;&#30830;&#22320;&#36807;&#24230;&#36924;&#36817;&#25968;&#20540;Fixpoint&#36845;&#20195;&#22120;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19982;&#36890;&#24120;&#29992;&#20110;&#36924;&#36817;&#25152;&#26377;&#21487;&#36798;&#31243;&#24207;&#29366;&#24577;&#30340;&#26631;&#20934;&#25277;&#35937;&#35299;&#37322;&#65288;AI&#65289;&#19981;&#21516;&#65292;&#20154;&#20204;&#21482;&#38656;&#35201;&#25277;&#35937;&#20855;&#20307;&#30340;Fixpoints&#65292;&#21363;&#26368;&#32456;&#30340;&#31243;&#24207;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38024;&#23545;&#20855;&#20307;&#25910;&#25947;&#24615;&#21644;&#21807;&#19968;&#24615;&#20445;&#35777;&#30340;&#25968;&#20540;Fixpoint&#36845;&#20195;&#22120;&#65292;&#22522;&#20110;&#20004;&#20010;&#20027;&#35201;&#30340;&#25216;&#26415;&#36129;&#29486;&#65306;&#65288;i&#65289;&#29702;&#35770;&#19978;&#30340;&#27934;&#23519;&#21147;&#65292;&#20801;&#35768;&#25105;&#20204;&#35745;&#31639;&#20986;&#19981;&#20351;&#29992;&#20132;&#27719;&#28857;&#30340;&#22768;&#38899;&#21644;&#31934;&#30830;&#30340;Fixpoint&#25277;&#35937;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#22495;&#65292;CH- Zonotope&#65292;&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#20801;&#35768;&#26377;&#25928;&#30340;&#20256;&#25773;&#21644;&#21253;&#21547;&#26816;&#26597;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;CRAFT&#30340;&#24037;&#20855;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20110;Fixpoint&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;monDEQ&#65289;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;CRAFT&#36229;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new abstract interpretation framework for the precise over-approximation of numerical fixpoint iterators. Our key observation is that unlike in standard abstract interpretation (AI), typically used to over-approximate all reachable program states, in this setting, one only needs to abstract the concrete fixpoints, i.e., the final program states. Our framework targets numerical fixpoint iterators with convergence and uniqueness guarantees in the concrete and is based on two major technical contributions: (i) theoretical insights which allow us to compute sound and precise fixpoint abstractions without using joins, and (ii) a new abstract domain, CH-Zonotope, which admits efficient propagation and inclusion checks while retaining high precision. We implement our framework in a tool called CRAFT and evaluate it on a novel fixpoint-based neural network architecture (monDEQ) that is particularly challenging to verify. Our extensive evaluation demonstrates that CRAFT exceeds the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#26032;&#20896;&#30149;&#20363;&#25968;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#36882;&#24402;&#36716;&#31227;&#38598;&#25104;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21360;&#24230;&#22810;&#26085;COVID-19&#30123;&#24773;&#36235;&#21183;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2108.09131</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#24402;&#36716;&#31227;&#38598;&#25104;&#23398;&#20064;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21360;&#24230;&#22810;&#26085;COVID-19&#30123;&#24773;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Transfer-Recursive-Ensemble Learning for Multi-Day COVID-19 Prediction in India using Recurrent Neural Networks. (arXiv:2108.09131v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.09131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#26032;&#20896;&#30149;&#20363;&#25968;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#36882;&#24402;&#36716;&#31227;&#38598;&#25104;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21360;&#24230;&#22810;&#26085;COVID-19&#30123;&#24773;&#36235;&#21183;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;COVID-19&#22823;&#27969;&#34892;&#32473;&#21360;&#24230;&#30340;&#21307;&#30103;&#22522;&#30784;&#35774;&#26045;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#31532;&#20108;&#27874;&#30123;&#24773;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24863;&#26579;&#65292;&#21307;&#38498;&#36127;&#25285;&#36807;&#37325;&#65292;&#29289;&#36164;&#21644;&#27687;&#27668;&#30701;&#32570;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25552;&#21069;&#39044;&#27979;COVID-19&#30149;&#20363;&#25968;&#37327;&#21487;&#33021;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#36164;&#28304;&#21644;&#29289;&#36164;&#12290;&#26412;&#25991;&#28041;&#21450;&#22810;&#26085;&#39044;&#27979;&#26032;&#30340;COVID-19&#30149;&#20363;&#12289;&#26032;&#30340;&#27515;&#20129;&#20154;&#25968;&#21644;&#24635;&#27963;&#21160;&#30149;&#20363;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#32593;&#32476;&#20316;&#20026;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#24314;&#31435;&#22235;&#20010;&#39044;&#20808;&#35757;&#32451;&#22312;&#19981;&#21516;&#22269;&#23478;&#65288;&#32654;&#22269;&#12289;&#24052;&#35199;&#12289;&#35199;&#29677;&#29273;&#21644;&#23391;&#21152;&#25289;&#22269;&#65289;&#25968;&#25454;&#19978;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#21360;&#24230;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;&#30001;&#20110;&#25152;&#36873;&#25321;&#30340;&#22235;&#20010;&#22269;&#23478;&#32463;&#21382;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#24863;&#26579;&#26354;&#32447;&#65292;&#22240;&#27492;&#39044;&#35757;&#32451;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#32771;&#34385;&#21040;&#20102;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current COVID-19 pandemic has put a huge challenge on the Indian health infrastructure. With more and more people getting affected during the second wave, the hospitals were over-burdened, running out of supplies and oxygen. In this scenario, prediction of the number of COVID-19 cases beforehand might have helped in the better utilization of limited resources and supplies. This manuscript deals with the prediction of new COVID-19 cases, new deaths and total active cases for multiple days in advance. The proposed method uses gated recurrent unit networks as the main predicting model. A study is conducted by building four models that are pre-trained on the data from four different countries (United States of America, Brazil, Spain and Bangladesh) and are fine-tuned or retrained on India's data. Since the four countries chosen have experienced different types of infection curves, the pre-training provides a transfer learning to the models incorporating diverse situations into account.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21483;&#20570;ZEROS&#30340;&#36830;&#25509;&#27010;&#24565;&#26469;&#35780;&#20272;DARTS&#20013;&#30340;&#25805;&#20316;&#37325;&#35201;&#24615;&#65292;&#20351;&#24471;&#25972;&#20010;&#26550;&#26500;&#30340;&#25628;&#32034;&#36807;&#31243;&#26356;&#39640;&#25928;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;NTK&#29702;&#35770;&#30340;&#20840;&#26032;&#26694;&#26550;FreeDARTS&#12290;</title><link>http://arxiv.org/abs/2106.11542</link><description>&lt;p&gt;
&#22522;&#20110;&#36830;&#25509;&#25935;&#24863;&#24615;&#30340;&#35757;&#32451;&#20813;&#36153;DARTS&#65306;&#20174;&#26550;&#26500;&#32423;&#35780;&#20998;&#21040;&#25805;&#20316;&#32423;&#25935;&#24863;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Connection Sensitivity Matters for Training-free DARTS: From Architecture-Level Scoring to Operation-Level Sensitivity Analysis. (arXiv:2106.11542v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21483;&#20570;ZEROS&#30340;&#36830;&#25509;&#27010;&#24565;&#26469;&#35780;&#20272;DARTS&#20013;&#30340;&#25805;&#20316;&#37325;&#35201;&#24615;&#65292;&#20351;&#24471;&#25972;&#20010;&#26550;&#26500;&#30340;&#25628;&#32034;&#36807;&#31243;&#26356;&#39640;&#25928;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;NTK&#29702;&#35770;&#30340;&#20840;&#26032;&#26694;&#26550;FreeDARTS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#35757;&#32451;&#20813;&#36153;NAS&#26041;&#27861;&#25918;&#24323;&#20102;&#35757;&#32451;&#38454;&#27573;&#24182;&#35774;&#35745;&#20102;&#21508;&#31181;&#38646;&#25104;&#26412;&#20195;&#29702;&#20316;&#20026;&#35780;&#20998;&#65292;&#20197;&#35782;&#21035;&#20986;&#20248;&#31168;&#30340;&#26550;&#26500;&#65292;&#24341;&#36215;&#20102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#26497;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#36890;&#36807;&#35757;&#32451;&#20813;&#36153;&#30340;&#26041;&#24335;&#36866;&#24403;&#22320;&#27979;&#37327;DARTS&#20013;&#30340;&#25805;&#20316;&#37325;&#35201;&#24615;&#65292;&#36991;&#20813;&#21442;&#25968;&#23494;&#38598;&#30340;&#20559;&#24046;&#65311;&#25105;&#20204;&#36890;&#36807;&#36793;&#32536;&#36830;&#36890;&#24615;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32943;&#23450;&#30340;&#31572;&#26696;&#65292;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#36830;&#25509;&#27010;&#24565;&#8220;ZERo-cost Operation Sensitivity (ZEROS)&#8221;&#65292;&#26469;&#35780;&#20998;DARTS&#20013;&#20505;&#36873;&#25805;&#20316;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#36845;&#20195;&#21644;&#25968;&#25454;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;ZEROS&#36827;&#34892;NAS&#65292;&#25105;&#20204;&#30340;&#26032;&#23581;&#35797;&#23548;&#33268;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;training free differentiable architecture search (FreeDARTS)&#8221;&#30340;&#26694;&#26550;&#12290;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#36830;&#25509;&#35780;&#20998;&#19982;&#27867;&#21270;&#19979;&#38477;&#21576;&#36127;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently proposed training-free NAS methods abandon the training phase and design various zero-cost proxies as scores to identify excellent architectures, arousing extreme computational efficiency for neural architecture search. In this paper, we raise an interesting problem: can we properly measure the operation importance in DARTS through a training-free way, with avoiding the parameter-intensive bias? We investigate this question through the lens of edge connectivity, and provide an affirmative answer by defining a connectivity concept, ZERo-cost Operation Sensitivity (ZEROS), to score the importance of candidate operations in DARTS at initialization. By devising an iterative and data-agnostic manner in utilizing ZEROS for NAS, our novel trial leads to a framework called training free differentiable architecture search (FreeDARTS). Based on the theory of Neural Tangent Kernel (NTK), we show the proposed connectivity score provably negatively correlated with the generalization bo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#19982;&#22810;&#20803;TSC(MTSC)&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;&#20316;&#32773;&#27979;&#35797;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#12289;&#24418;&#29366;&#21644;&#21333;&#35789;&#34955;&#26041;&#27861;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2007.13156</link><description>&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Multivariate Time Series Classification Algorithms. (arXiv:2007.13156v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.13156
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#19982;&#22810;&#20803;TSC(MTSC)&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;&#20316;&#32773;&#27979;&#35797;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#12289;&#24418;&#29366;&#21644;&#21333;&#35789;&#34955;&#26041;&#27861;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#28041;&#21450;&#20174;&#26377;&#24207;&#30340;&#23454;&#20540;&#23646;&#24615;&#20013;&#26500;&#24314;&#29992;&#20110;&#31163;&#25955;&#30446;&#26631;&#21464;&#37327;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;&#19968;&#32452;&#26032;&#30340;TSC&#31639;&#27861;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20043;&#21069;&#30340;&#25216;&#26415;&#27700;&#24179;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#21333;&#21464;&#37327;TSC&#65292;&#21363;&#27599;&#20010;&#26696;&#20363;&#37117;&#26377;&#19968;&#20010;&#21333;&#19968;&#24207;&#21015;&#21644;&#19968;&#20010;&#31867;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;&#20294;&#23454;&#38469;&#19978;&#65292;&#26356;&#24120;&#35265;&#30340;&#26159;&#36935;&#21040;&#22810;&#20803;TSC(MTSC)&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#24207;&#21015;&#19982;&#21333;&#19968;&#26631;&#31614;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;MTSC&#30340;&#32771;&#34385;&#36828;&#19981;&#22914;&#21333;&#21464;&#37327;&#24773;&#20917;&#37027;&#20040;&#22810;&#12290;2018&#24180;&#25512;&#20986;&#20102;30&#20010;MTSC&#38382;&#39064;&#30340;UEA&#23384;&#26723;&#24211;&#65292;&#20351;&#31639;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#26356;&#23481;&#26131;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#12289;&#24418;&#29366;&#21644;&#21333;&#35789;&#34955;&#26041;&#27861;&#25552;&#20986;&#30340;&#23450;&#21046;MTSC&#31639;&#27861;&#12290; MTSC&#30340;&#26368;&#31616;&#21333;&#26041;&#27861;&#26159;&#36890;&#36807;&#22810;&#20803;&#32500;&#24230;&#19978;&#30340;&#38598;&#25104;&#21333;&#21464;&#37327;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#23450;&#21046;&#31639;&#27861;&#19982;&#36825;&#20123;&#32500;&#24230;&#26080;&#20851;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time Series Classification (TSC) involved building predictive models for a discrete target variable from ordered, real valued, attributes. Over recent years, a new set of TSC algorithms have been developed which have made significant improvement over the previous state of the art. The main focus has been on univariate TSC, i.e. the problem where each case has a single series and a class label. In reality, it is more common to encounter multivariate TSC (MTSC) problems where multiple series are associated with a single label. Despite this, much less consideration has been given to MTSC than the univariate case. The UEA archive of 30 MTSC problems released in 2018 has made comparison of algorithms easier. We review recently proposed bespoke MTSC algorithms based on deep learning, shapelets and bag of words approaches. The simplest approach to MTSC is to ensemble univariate classifiers over the multivariate dimensions. We compare the bespoke algorithms to these dimension independent appro
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#24322;&#26500;&#20803;&#38598;&#25104;&#31639;&#27861; HIVE-COTE &#30340;&#26368;&#26032;&#31283;&#23450;&#29256;&#26412; 1.0&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#25351;&#21335;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#21644;&#36164;&#28304;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#19982;&#19977;&#31181;&#36817;&#26399;&#25552;&#20986;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2004.06069</link><description>&lt;p&gt;
&#20004;&#20010;&#24037;&#20855;&#31665;&#30340;&#25925;&#20107;&#8212;&#8212;&#31532;&#19977;&#27425;&#25253;&#21578;&#65306;&#20851;&#20110;HIVE-COTE v1.0&#30340;&#20351;&#29992;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
A tale of two toolkits, report the third: on the usage and performance of HIVE-COTE v1.0. (arXiv:2004.06069v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.06069
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#24322;&#26500;&#20803;&#38598;&#25104;&#31639;&#27861; HIVE-COTE &#30340;&#26368;&#26032;&#31283;&#23450;&#29256;&#26412; 1.0&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#25351;&#21335;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#21644;&#36164;&#28304;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#19982;&#19977;&#31181;&#36817;&#26399;&#25552;&#20986;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#36716;&#25442;&#38598;&#21512;&#27861;&#65288;HIVE-COTE&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#24322;&#26500;&#20803;&#38598;&#25104;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26368;&#21021;&#20110;2016&#24180;&#25552;&#20986;&#65292;&#32463;&#21382;&#20102;&#19968;&#20123;&#23567;&#30340;&#25913;&#21464;&#65292;&#24182;&#22312;&#20004;&#20010;&#24320;&#28304;&#20195;&#30721;&#24211;&#20013;&#25512;&#20986;&#20102;&#21487;&#37197;&#32622;&#12289;&#21487;&#25193;&#23637;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#29256;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#26032;&#31283;&#23450;&#30340; HIVE-COTE &#29256;&#26412; 1.0 &#30340;&#27010;&#36848;&#65292;&#24182;&#38416;&#36848;&#20102;&#23427;&#19982;&#26368;&#21021;&#29256;&#26412;&#30340;&#21306;&#21035;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#35813;&#20998;&#31867;&#22120;&#30340;&#25351;&#21335;&#65292;&#24182;&#23545;&#20854;&#39044;&#27979;&#24615;&#33021;&#21644;&#36164;&#28304;&#20351;&#29992;&#24773;&#20917;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992; aeon &#24037;&#20855;&#21253;&#27604;&#36739;&#20102; HIVE-COTE &#19982;&#19977;&#31181;&#36817;&#26399;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE) is a heterogeneous meta ensemble for time series classification. Since it was first proposed in 2016, the algorithm has undergone some minor changes and there is now a configurable, scalable and easy to use version available in two open source repositories. We present an overview of the latest stable HIVE-COTE, version 1.0, and describe how it differs to the original. We provide a walkthrough guide of how to use the classifier, and conduct extensive experimental evaluation of its predictive performance and resource usage. We compare the performance of HIVE-COTE to three recently proposed algorithms using the aeon toolkit.
&lt;/p&gt;</description></item></channel></rss>