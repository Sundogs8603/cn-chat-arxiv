<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;EPAC&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26102;&#27169;&#22411;&#30340;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36817;&#20284;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.14496</link><description>&lt;p&gt;
&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Explanation Constraints. (arXiv:2303.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;EPAC&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26102;&#27169;&#22411;&#30340;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36817;&#20284;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#30417;&#30563;&#23398;&#20064;&#20551;&#35774;&#23384;&#22312;&#26631;&#27880;&#25968;&#25454;&#65292;&#20294;&#25105;&#20204;&#21487;&#33021;&#26377;&#20851;&#20110;&#27169;&#22411;&#24212;&#22914;&#20309;&#36816;&#34892;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#26412;&#25991;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#20174;&#35299;&#37322;&#32422;&#26463;&#20013;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23398;&#20064;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#35299;&#37322;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#39033;&#20851;&#38190;&#36129;&#29486;&#26159;&#36890;&#36807;&#23450;&#20041;&#25105;&#20204;&#31216;&#20043;&#20026;EPAC&#27169;&#22411;&#65288;&#22312;&#26032;&#25968;&#25454;&#26399;&#26395;&#20013;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#30340;&#27169;&#22411;&#65289;&#26469;&#22238;&#31572;&#21738;&#20123;&#27169;&#22411;&#20250;&#21463;&#30410;&#20110;&#35299;&#37322;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;&#23398;&#20064;&#29702;&#35770;&#24037;&#20855;&#20998;&#26512;&#20102;&#36825;&#31867;&#27169;&#22411;&#12290;&#31532;&#20108;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#23545;&#20110;&#30001;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#20449;&#24687;&#32473;&#20986;&#30340;&#35268;&#33539;&#35299;&#37322;&#30340;&#38480;&#21046;&#65288;&#20197;&#20854;Rademacher&#22797;&#26434;&#24230;&#20026;&#34913;&#37327;&#26631;&#20934;&#65289;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#21464;&#20998;&#36817;&#20284;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
While supervised learning assumes the presence of labeled data, we may have prior information about how models should behave. In this paper, we formalize this notion as learning from explanation constraints and provide a learning theoretic framework to analyze how such explanations can improve the learning of our models. For what models would explanations be helpful? Our first key contribution addresses this question via the definition of what we call EPAC models (models that satisfy these constraints in expectation over new data), and we analyze this class of models using standard learning theoretic tools. Our second key contribution is to characterize these restrictions (in terms of their Rademacher complexities) for a canonical class of explanations given by gradient information for linear models and two layer neural networks. Finally, we provide an algorithmic solution for our framework, via a variational approximation that achieves better performance and satisfies these constraint
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#38754;&#21521;&#22478;&#24066;&#35745;&#31639;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#23398;&#20064;&#39046;&#22495;&#30340;&#21457;&#23637;&#29616;&#29366;&#65292;&#21253;&#25324;&#20854;&#26694;&#26550;&#12289;&#23454;&#29616;&#26041;&#27861;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#20197;&#21450;&#24403;&#21069;&#30340;&#30740;&#31350;&#28909;&#28857;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.14483</link><description>&lt;p&gt;
&#38754;&#21521;&#22478;&#24066;&#35745;&#31639;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey. (arXiv:2303.14483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#38754;&#21521;&#22478;&#24066;&#35745;&#31639;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#23398;&#20064;&#39046;&#22495;&#30340;&#21457;&#23637;&#29616;&#29366;&#65292;&#21253;&#25324;&#20854;&#26694;&#26550;&#12289;&#23454;&#29616;&#26041;&#27861;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#20197;&#21450;&#24403;&#21069;&#30340;&#30740;&#31350;&#28909;&#28857;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20808;&#36827;&#20256;&#24863;&#22120;&#21644;&#22823;&#22411;&#25968;&#25454;&#24211;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#22478;&#24066;&#31995;&#32479;&#26102;&#31354;&#25968;&#25454;&#34987;&#35760;&#24405;&#21644;&#23384;&#20648;&#12290;&#36825;&#20123;&#25968;&#25454;&#30340;&#28436;&#21270;&#27169;&#24335;&#30340;&#39044;&#27979;&#23398;&#20064;&#26159;&#22478;&#24066;&#35745;&#31639;&#20013;&#22522;&#26412;&#20294;&#37325;&#35201;&#30340;&#24490;&#29615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#22478;&#24066;&#26234;&#33021;&#31649;&#29702;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#22312;&#20132;&#36890;&#12289;&#29615;&#22659;&#12289;&#23433;&#20840;&#12289;&#20844;&#20849;&#21355;&#29983;&#31561;&#39046;&#22495;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#32479;&#35745;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#25429;&#25417;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#30340;&#22797;&#26434;&#30456;&#20851;&#24615;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNN&#65289;&#30340;&#26694;&#26550;&#12290;STGNN&#36890;&#36807;&#38598;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#21508;&#31181;&#26102;&#38388;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#22797;&#26434;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#25552;&#21462;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#23398;&#20064;&#20219;&#21153;&#65292;&#26377;&#25928;&#35774;&#35745;&#31354;&#38388;&#20381;&#36182;&#23398;&#20064;&#27169;&#22359;&#12289;&#26102;&#38388;&#20381;&#36182;&#23398;&#20064;&#27169;&#22359;&#12289;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#27861;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of sophisticated sensors and large database technologies, more and more spatio-temporal data in urban systems are recorded and stored. Predictive learning for the evolution patterns of these spatio-temporal data is a basic but important loop in urban computing, which can better support urban intelligent management decisions, especially in the fields of transportation, environment, security, public health, etc. Since traditional statistical learning and deep learning methods can hardly capture the complex correlations in the urban spatio-temporal data, the framework of spatio-temporal graph neural network (STGNN) has been proposed in recent years. STGNNs enable the extraction of complex spatio-temporal dependencies by integrating graph neural networks (GNNs) and various temporal learning methods. However, for different predictive learning tasks, it is a challenging problem to effectively design the spatial dependencies learning modules, temporal dependencies learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#30740;&#31350;&#20171;&#32461;&#20102;&#30740;&#21457;&#20986;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#36827;&#23637;, &#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#26597;&#25214;&#24182;&#35821;&#20041;&#20998;&#26512;&#30456;&#20851;&#25991;&#29486;,&#20197;&#21327;&#21161;&#22303;&#33879;&#20154;&#31867;&#23547;&#25214;&#20854;&#34987;&#30423;&#31363;&#12289;&#25424;&#36192;&#12289;&#20986;&#21806;&#25110;&#22312;&#26426;&#26500;&#20043;&#38388;&#20132;&#25442;&#30340;&#36951;&#39608;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.14475</link><description>&lt;p&gt;
&#20449;&#24687;&#23398;&#20064;&#12289;&#20013;&#24515;&#24615;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#30456;&#20851;&#25991;&#29486;&#26816;&#27979;&#12289;&#22303;&#33879;&#20154;&#31867;&#36951;&#39608;&#24402;&#36824;
&lt;/p&gt;
&lt;p&gt;
Informed Machine Learning, Centrality, CNN, Relevant Document Detection, Repatriation of Indigenous Human Remains. (arXiv:2303.14475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#30740;&#31350;&#20171;&#32461;&#20102;&#30740;&#21457;&#20986;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#36827;&#23637;, &#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#26597;&#25214;&#24182;&#35821;&#20041;&#20998;&#26512;&#30456;&#20851;&#25991;&#29486;,&#20197;&#21327;&#21161;&#22303;&#33879;&#20154;&#31867;&#23547;&#25214;&#20854;&#34987;&#30423;&#31363;&#12289;&#25424;&#36192;&#12289;&#20986;&#21806;&#25110;&#22312;&#26426;&#26500;&#20043;&#38388;&#20132;&#25442;&#30340;&#36951;&#39608;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28595;&#22823;&#21033;&#20122;&#21644;&#20854;&#20182;&#21407;&#20303;&#27665;&#38754;&#20020;&#30340;&#32039;&#36843;&#38382;&#39064;&#20043;&#19968;&#26159;&#23558;&#20182;&#20204;&#31062;&#20808;&#30340;&#23608;&#20307;&#36951;&#39608;&#24402;&#36824;&#21040;&#35199;&#26041;&#31185;&#23398;&#26426;&#26500;&#12290;&#25104;&#21151;&#23558;&#36825;&#20123;&#36951;&#39608;&#36820;&#36824;&#21040;&#20854;&#31038;&#21306;&#20197;&#37325;&#26032;&#23433;&#33900;&#65292;&#20027;&#35201;&#21462;&#20915;&#20110;&#22312;1790&#24180;&#33267;1970&#24180;&#26399;&#38388;&#21457;&#34920;&#30340;&#31185;&#23398;&#21644;&#20854;&#20182;&#25991;&#29486;&#20013;&#25214;&#21040;&#35760;&#24405;&#23427;&#20204;&#34987;&#30423;&#31363;&#12289;&#25424;&#36192;&#12289;&#20986;&#21806;&#25110;&#22312;&#26426;&#26500;&#20043;&#38388;&#20132;&#25442;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25253;&#36947;&#20102;&#30001;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#22312;&#8220;&#30740;&#31350;&#12289;&#21644;&#35299;&#12289;&#26356;&#26032;&#8221;&#32593;&#32476;&#65288;RRR&#65289;&#20013;&#36827;&#34892;&#30340;&#21327;&#20316;&#30740;&#31350;&#65292;&#20197;&#24320;&#21457;&#21644;&#24212;&#29992;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#26469;&#30830;&#23450;&#36825;&#20123;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#36804;&#20170;&#20026;&#27490;&#24320;&#21457;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#33258;&#21160;&#21270;&#26597;&#25214;&#21644;&#35821;&#20041;&#20998;&#26512;&#30456;&#20851;&#25991;&#26412;&#30340;&#24037;&#20316;&#12290;&#20998;&#31867;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#22312;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#31934;&#24230;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the pressing issues facing Australian and other First Nations peoples is the repatriation of the bodily remains of their ancestors, which are currently held in Western scientific institutions. The success of securing the return of these remains to their communities for reburial depends largely on locating information within scientific and other literature published between 1790 and 1970 documenting their theft, donation, sale, or exchange between institutions. This article reports on collaborative research by data scientists and social science researchers in the Research, Reconcile, Renew Network (RRR) to develop and apply text mining techniques to identify this vital information. We describe our work to date on developing a machine learning-based solution to automate the process of finding and semantically analysing relevant texts. Classification models, particularly deep learning-based models, are known to have low accuracy when trained with small amounts of labelled (i.e. rele
&lt;/p&gt;</description></item><item><title>3Mformer&#26159;&#19968;&#31181;&#22810;&#38454;&#22810;&#27169;&#21464;&#24418;&#22120;&#65292;&#36890;&#36807;&#24418;&#25104;&#36229;&#22270;&#25429;&#25417;&#36523;&#20307;&#20851;&#33410;&#32452;&#30340;&#39640;&#38454;&#36816;&#21160;&#27169;&#24335;&#65292;&#20351;&#24471;&#22312;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;</title><link>http://arxiv.org/abs/2303.14474</link><description>&lt;p&gt;
3Mformer: &#22810;&#38454;&#22810;&#27169;&#21464;&#24418;&#22120;&#29992;&#20110;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
3Mformer: Multi-order Multi-mode Transformer for Skeletal Action Recognition. (arXiv:2303.14474v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14474
&lt;/p&gt;
&lt;p&gt;
3Mformer&#26159;&#19968;&#31181;&#22810;&#38454;&#22810;&#27169;&#21464;&#24418;&#22120;&#65292;&#36890;&#36807;&#24418;&#25104;&#36229;&#22270;&#25429;&#25417;&#36523;&#20307;&#20851;&#33410;&#32452;&#30340;&#39640;&#38454;&#36816;&#21160;&#27169;&#24335;&#65292;&#20351;&#24471;&#22312;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#27169;&#22411;&#20351;&#29992;GCN&#36890;&#36807;&#36830;&#25509;&#30340;3D&#20154;&#20307;&#20851;&#33410;&#20195;&#34920;&#20154;&#20307;&#12290;GCNs&#32858;&#21512;&#19968;&#21040;&#23569;&#37327;&#36339;&#36291;&#22270;&#37051;&#22495;&#65292;&#24182;&#24573;&#30053;&#26410;&#36830;&#25509;&#36523;&#20307;&#20851;&#33410;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#24418;&#25104;&#36229;&#22270;&#26469;&#27169;&#25311;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36229;&#36793;&#65288;&#20363;&#22914;&#65292;&#31532;&#19977;&#21644;&#31532;&#22235;&#38454;&#36229;&#36793;&#25429;&#33719;&#19977;&#20010;&#21644;&#22235;&#20010;&#33410;&#28857;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#25429;&#33719;&#36523;&#20307;&#20851;&#33410;&#32452;&#30340;&#39640;&#38454;&#36816;&#21160;&#27169;&#24335;&#12290;&#25105;&#20204;&#23558;&#21160;&#20316;&#24207;&#21015;&#20998;&#25104;&#26102;&#38388;&#22359;&#65292;Higher-order Transformer (HoT)&#22522;&#20110;&#65288;i&#65289;&#36523;&#20307;&#20851;&#33410;&#65292;&#65288;ii&#65289;&#36523;&#20307;&#20851;&#33410;&#30340;&#25104;&#23545;&#38142;&#25509;&#21644;&#65288;iii&#65289;&#39592;&#26550;&#36523;&#20307;&#20851;&#33410;&#30340;&#39640;&#38454;&#36229;&#36793;&#65292;&#20135;&#29983;&#27599;&#20010;&#26102;&#38388;&#22359;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#26032;&#22411;Multi-order Multi-mode Transformer(3Mformer)&#32467;&#21512;&#36825;&#20123;&#36229;&#32423;&#36793;&#30340;HoT&#23884;&#20837;&#65292;&#20854;&#20013;&#20004;&#20010;&#27169;&#22359;&#30340;&#39034;&#24207;&#21487;&#20197;&#20132;&#25442;&#65292;&#20197;&#23454;&#29616;&#22522;&#20110;'channel-temporal block'&#65292;'order-channel-body joint'&#65292;'channel-hyper-edge-order'&#30340;&#32806;&#21512;&#27169;&#24335;&#20196;&#29260;&#19978;&#30340;&#32806;&#21512;&#27169;&#24335;&#27880;&#24847;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;3Mformer&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many skeletal action recognition models use GCNs to represent the human body by 3D body joints connected body parts. GCNs aggregate one- or few-hop graph neighbourhoods, and ignore the dependency between not linked body joints. We propose to form hypergraph to model hyper-edges between graph nodes (e.g., third- and fourth-order hyper-edges capture three and four nodes) which help capture higher-order motion patterns of groups of body joints. We split action sequences into temporal blocks, Higher-order Transformer (HoT) produces embeddings of each temporal block based on (i) the body joints, (ii) pairwise links of body joints and (iii) higher-order hyper-edges of skeleton body joints. We combine such HoT embeddings of hyper-edges of orders 1, ..., r by a novel Multi-order Multi-mode Transformer (3Mformer) with two modules whose order can be exchanged to achieve coupled-mode attention on coupled-mode tokens based on 'channel-temporal block', 'order-channel-body joint', 'channel-hyper-edg
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#22238;&#24402;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#27169;&#22411;&#65292;&#33021;&#22815;&#24314;&#27169;&#39640;&#24230;&#30456;&#20851;&#30340;&#38750;&#39640;&#26031;&#39044;&#27979;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.14468</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Conditional Neural Processes. (arXiv:2303.14468v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#22238;&#24402;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#27169;&#22411;&#65292;&#33021;&#22815;&#24314;&#27169;&#39640;&#24230;&#30456;&#20851;&#30340;&#38750;&#39640;&#26031;&#39044;&#27979;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26465;&#20214;&#30340;&#31070;&#32463;&#36807;&#31243;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#20135;&#29983;&#33391;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#26368;&#22823;&#20284;&#28982;&#36807;&#31243;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21464;&#26377;&#26465;&#20214;&#30340;&#31070;&#32463;&#36807;&#31243;&#22312;&#27979;&#35797;&#26102;&#37096;&#32626;&#26041;&#24335;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#24314;&#27169;&#39640;&#24230;&#30456;&#20851;&#30340;&#38750;&#39640;&#26031;&#39044;&#27979;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional neural processes (CNPs; Garnelo et al., 2018a) are attractive meta-learning models which produce well-calibrated predictions and are trainable via a simple maximum likelihood procedure. Although CNPs have many advantages, they are unable to model dependencies in their predictions. Various works propose solutions to this, but these come at the cost of either requiring approximate inference or being limited to Gaussian predictions. In this work, we instead propose to change how CNPs are deployed at test time, without any modifications to the model or training procedure. Instead of making predictions independently for every target point, we autoregressively define a joint predictive distribution using the chain rule of probability, taking inspiration from the neural autoregressive density estimator (NADE) literature. We show that this simple procedure allows factorised Gaussian CNPs to model highly dependent, non-Gaussian predictive distributions. Perhaps surprisingly, in an e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;Tsetlin Machine&#36827;&#34892;&#27491;&#24335;&#39564;&#35777;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12289;&#31561;&#20215;&#24615;&#21644;&#30456;&#20284;&#24615;&#26041;&#38754;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36824;&#23558;&#26032;&#30340;&#27169;&#22411;&#30456;&#20284;&#24615;&#27010;&#24565;&#24212;&#29992;&#20110;TsMs&#12290;</title><link>http://arxiv.org/abs/2303.14464</link><description>&lt;p&gt;
Tsetlin&#26426;&#22120;&#30340;&#24615;&#36136;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Verifying Properties of Tsetlin Machines. (arXiv:2303.14464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;Tsetlin Machine&#36827;&#34892;&#27491;&#24335;&#39564;&#35777;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12289;&#31561;&#20215;&#24615;&#21644;&#30456;&#20284;&#24615;&#26041;&#38754;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36824;&#23558;&#26032;&#30340;&#27169;&#22411;&#30456;&#20284;&#24615;&#27010;&#24565;&#24212;&#29992;&#20110;TsMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tsetlin&#26426;&#22120;&#65288;TsMs&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#19988;&#26131;&#20110;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;TsMs&#31934;&#30830;&#32534;&#30721;&#20026;&#21629;&#39064;&#36923;&#36753;&#24182;&#20351;&#29992;SAT&#27714;&#35299;&#22120;&#27491;&#24335;&#39564;&#35777;&#20102;TsMs&#30340;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#20284;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#27010;&#24565;&#24212;&#29992;&#20110;&#26816;&#26597;TsMs&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#25991;&#29486;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#31561;&#20215;&#24615;&#27010;&#24565;&#65292;&#24182;&#20026;TsMs&#37325;&#26032;&#35843;&#25972;&#20102;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#32534;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#20026;TsMs&#30340;&#24615;&#36136;&#8212;&#8212;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#31561;&#20215;&#24615;&#21644;&#30456;&#20284;&#24615;&#25552;&#20379;&#32467;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20998;&#21035;&#24212;&#29992;MNIST&#21644;IMDB&#25968;&#25454;&#38598;&#36827;&#34892;&#22270;&#20687;&#21644;&#24773;&#24863;&#20998;&#31867;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;TsMs&#26816;&#26597;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#32467;&#26524;&#19982;&#25991;&#29486;&#20013;&#22312;MNIST&#19978;&#20351;&#29992;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#30340;&#32467;&#26524;&#36827;&#34892;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tsetlin Machines (TsMs) are a promising and interpretable machine learning method which can be applied for various classification tasks. We present an exact encoding of TsMs into propositional logic and formally verify properties of TsMs using a SAT solver. In particular, we introduce in this work a notion of similarity of machine learning models and apply our notion to check for similarity of TsMs. We also consider notions of robustness and equivalence from the literature and adapt them for TsMs. Then, we show the correctness of our encoding and provide results for the properties: adversarial robustness, equivalence, and similarity of TsMs. In our experiments, we employ the MNIST and IMDB datasets for (respectively) image and sentiment classification. We discuss the results for verifying robustness obtained with TsMs with those in the literature obtained with Binarized Neural Networks on MNIST.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CFA&#26694;&#26550;&#65292;&#38024;&#23545;&#27599;&#20010;&#31867;&#21035;&#33258;&#21160;&#23450;&#21046;&#29305;&#23450;&#30340;&#23545;&#25239;&#35757;&#32451;&#37197;&#32622;&#65292;&#25552;&#39640;&#20102;DNN&#23545;&#25239;&#35757;&#32451;&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#24635;&#20307;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14460</link><description>&lt;p&gt;
CFA: &#31867;&#21035;&#38388;&#26657;&#20934;&#30340;&#20844;&#24179;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CFA: Class-wise Calibrated Fair Adversarial Training. (arXiv:2303.14460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CFA&#26694;&#26550;&#65292;&#38024;&#23545;&#27599;&#20010;&#31867;&#21035;&#33258;&#21160;&#23450;&#21046;&#29305;&#23450;&#30340;&#23545;&#25239;&#35757;&#32451;&#37197;&#32622;&#65292;&#25552;&#39640;&#20102;DNN&#23545;&#25239;&#35757;&#32451;&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#24635;&#20307;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#24050;&#32463;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#22686;&#24378;&#25972;&#20010;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#19978;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#21516;&#31561;&#23545;&#24453;&#27599;&#20010;&#31867;&#21035;&#12290;&#34429;&#28982;&#25581;&#31034;&#20102;&#31867;&#21035;&#38388;&#40065;&#26834;&#24615;&#30340;&#24046;&#24322;&#65292;&#20294;&#24456;&#23569;&#26377;&#24037;&#20316;&#22312;&#19981;&#29306;&#29298;&#24635;&#20307;&#40065;&#26834;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#31867;&#21035;&#32423;&#21035;&#19978;&#20351;&#23545;&#25239;&#35757;&#32451;&#20844;&#24179;&#12290;&#26412;&#25991;&#26159;&#31532;&#19968;&#20010;&#22312;&#29702;&#35770;&#21644;&#32463;&#39564;&#19978;&#30740;&#31350;&#19981;&#21516;&#31867;&#21035;&#23545;&#23545;&#25239;&#24615;&#37197;&#32622;&#21916;&#22909;&#30340;&#65292;&#21253;&#25324;&#25200;&#21160;&#24133;&#24230;&#12289;&#27491;&#21017;&#21270;&#21644;&#26435;&#37325;&#24179;&#22343;&#20540;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CFA&#30340;&#31867;&#21035;&#38388;&#26657;&#20934;&#30340;&#20844;&#24179;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#23450;&#21046;&#27599;&#20010;&#31867;&#21035;&#30340;&#29305;&#23450;&#35757;&#32451;&#37197;&#32622;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;CFA&#21487;&#20197;&#25552;&#39640;DNNs&#30340;&#23545;&#25239;&#35757;&#32451;&#20844;&#24179;&#24615;&#65292;&#32780;&#19981;&#29306;&#29298;&#24635;&#20307;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training has been widely acknowledged as the most effective method to improve the adversarial robustness against adversarial examples for Deep Neural Networks (DNNs). So far, most existing works focus on enhancing the overall model robustness, treating each class equally in both the training and testing phases. Although revealing the disparity in robustness among classes, few works try to make adversarial training fair at the class level without sacrificing overall robustness. In this paper, we are the first to theoretically and empirically investigate the preference of different classes for adversarial configurations, including perturbation margin, regularization, and weight averaging. Motivated by this, we further propose a \textbf{C}lass-wise calibrated \textbf{F}air \textbf{A}dversarial training framework, named CFA, which customizes specific training configurations for each class automatically. Experiments on benchmark datasets demonstrate that our proposed CFA can imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#22312;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#20013;&#21033;&#29992;&#26080;&#26631;&#27880;&#25968;&#25454;&#35299;&#20915;&#26080;&#20840;&#26631;&#31614;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#24635;&#32467;&#20102;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#21644;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.14453</link><description>&lt;p&gt;
&#26080;&#20840;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Federated Learning without Full Labels: A Survey. (arXiv:2303.14453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#22312;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#20013;&#21033;&#29992;&#26080;&#26631;&#27880;&#25968;&#25454;&#35299;&#20915;&#26080;&#20840;&#26631;&#31614;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#24635;&#32467;&#20102;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#21644;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#24050;&#25104;&#20026;&#23454;&#38469;&#22823;&#25968;&#25454;&#24212;&#29992;&#20013;&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#32852;&#37030;&#23398;&#20064;&#25104;&#20026;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#21033;&#29992;&#20998;&#25955;&#19988;&#38544;&#31169;&#30340;&#25968;&#25454;&#26500;&#24314;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20027;&#35201;&#35299;&#20915;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#36825;&#35201;&#27714;&#25968;&#25454;&#24517;&#39035;&#26159;&#20840;&#26631;&#31614;&#30340;&#12290;&#20294;&#23454;&#38469;&#19978;&#65292;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#24471;&#65292;&#22240;&#20026;&#21442;&#19982;&#32773;&#21487;&#33021;&#32570;&#20047;&#36275;&#22815;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#25110;&#32773;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#21160;&#26426;&#21644;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#26080;&#20840;&#26631;&#31614;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#26080;&#26631;&#27880;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data privacy has become an increasingly important concern in real-world big data applications such as machine learning. To address the problem, federated learning (FL) has been a promising solution to building effective machine learning models from decentralized and private data. Existing federated learning algorithms mainly tackle the supervised learning problem, where data are assumed to be fully labeled. However, in practice, fully labeled data is often hard to obtain, as the participants may not have sufficient domain expertise, or they lack the motivation and tools to label data. Therefore, the problem of federated learning without full labels is important in real-world FL applications. In this paper, we discuss how the problem can be solved with machine learning techniques that leverage unlabeled data. We present a survey of methods that combine FL with semi-supervised learning, self-supervised learning, and transfer learning methods. We also summarize the datasets used to evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#30772;&#35299;&#20102;&#33258;&#21160;&#35770;&#25991;&#23457;&#31295;&#20154;&#20998;&#37197;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#23545;&#35770;&#25991;&#30340;&#19981;&#26126;&#26174;&#26356;&#25913;&#65292;&#36827;&#32780;&#21487;&#20197;&#25104;&#21151;&#36873;&#25321;&#21644;&#21024;&#38500;&#23457;&#31295;&#20154;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#36798;80&#65285;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2303.14443</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#30772;&#35299;&#33258;&#21160;&#35770;&#25991;&#23457;&#31295;&#20154;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
No more Reviewer #2: Subverting Automatic Paper-Reviewer Assignment using Adversarial Learning. (arXiv:2303.14443v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#30772;&#35299;&#20102;&#33258;&#21160;&#35770;&#25991;&#23457;&#31295;&#20154;&#20998;&#37197;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#23545;&#35770;&#25991;&#30340;&#19981;&#26126;&#26174;&#26356;&#25913;&#65292;&#36827;&#32780;&#21487;&#20197;&#25104;&#21151;&#36873;&#25321;&#21644;&#21024;&#38500;&#23457;&#31295;&#20154;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#36798;80&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#31185;&#30340;&#23398;&#26415;&#20250;&#35758;&#25152;&#25552;&#20132;&#30340;&#35770;&#25991;&#25968;&#37327;&#36880;&#28176;&#22686;&#21152;&#65292;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#22686;&#38271;&#65292;&#22312;&#23457;&#26597;&#36807;&#31243;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#33258;&#21160;&#35770;&#25991;&#23457;&#31295;&#20154;&#20998;&#37197;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#20351;&#29992;&#32479;&#35745;&#20027;&#39064;&#27169;&#22411;&#26469;&#34920;&#24449;&#25552;&#20132;&#20869;&#23481;&#65292;&#24182;&#33258;&#21160;&#20998;&#37197;&#32473;&#23457;&#31295;&#20154;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36825;&#31181;&#33258;&#21160;&#21270;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#65292;&#20351;&#29305;&#23450;&#35770;&#25991;&#21487;&#20197;&#35823;&#23548;&#35780;&#23457;&#24182;&#36873;&#25321;&#33258;&#24049;&#30340;&#35780;&#23457;&#20154;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#22522;&#20110;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#20132;&#26367;&#20351;&#29992;&#29305;&#24449;&#31354;&#38388;&#21644;&#38382;&#39064;&#31354;&#38388;&#26469;&#23454;&#29616;&#23545;&#35770;&#25991;&#30340;&#19981;&#26126;&#26174;&#26356;&#25913;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;&#23433;&#20840;&#20250;&#35758;&#65288;IEEE S&amp;P&#65289;&#30340;&#35770;&#25991;&#23457;&#31295;&#20154;&#20998;&#37197;&#65292;&#20854;&#20013;&#26377;165&#21517;&#23457;&#31295;&#20154;&#22312;&#39033;&#30446;&#22996;&#21592;&#20250;&#23457;&#26680;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#25104;&#21151;&#36873;&#25321;&#24182;&#21024;&#38500;&#23457;&#31295;&#20154;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#20998;&#37197;&#31995;&#32479;&#30340;&#21442;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#25915;&#20987;&#21487;&#20197;&#22312;&#20445;&#25345;&#19981;&#34987;&#21457;&#29616;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36229;&#36807;80&#65285;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The number of papers submitted to academic conferences is steadily rising in many scientific disciplines. To handle this growth, systems for automatic paper-reviewer assignments are increasingly used during the reviewing process. These systems use statistical topic models to characterize the content of submissions and automate the assignment to reviewers. In this paper, we show that this automation can be manipulated using adversarial learning. We propose an attack that adapts a given paper so that it misleads the assignment and selects its own reviewers. Our attack is based on a novel optimization strategy that alternates between the feature space and problem space to realize unobtrusive changes to the paper. To evaluate the feasibility of our attack, we simulate the paper-reviewer assignment of an actual security conference (IEEE S&amp;P) with 165 reviewers on the program committee. Our results show that we can successfully select and remove reviewers without access to the assignment sys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;Green-Kubo&#26041;&#27861;&#24212;&#29992;&#20110;&#21322;&#23616;&#22495;&#26426;&#22120;&#23398;&#20064;&#21183;&#65292;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#25512;&#23548;&#20986;&#36866;&#24212;&#30340;&#28909;&#36890;&#37327;&#20844;&#24335;&#65292;&#25104;&#21151;&#35745;&#31639;&#20102;&#20108;&#27687;&#21270;&#38150;&#30340;&#28909;&#23548;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.14434</link><description>&lt;p&gt;
&#21322;&#23616;&#22495;&#26426;&#22120;&#23398;&#20064;&#21183;&#30340;&#28909;&#36890;&#37327;
&lt;/p&gt;
&lt;p&gt;
Heat flux for semi-local machine-learning potentials. (arXiv:2303.14434v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;Green-Kubo&#26041;&#27861;&#24212;&#29992;&#20110;&#21322;&#23616;&#22495;&#26426;&#22120;&#23398;&#20064;&#21183;&#65292;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#25512;&#23548;&#20986;&#36866;&#24212;&#30340;&#28909;&#36890;&#37327;&#20844;&#24335;&#65292;&#25104;&#21151;&#35745;&#31639;&#20102;&#20108;&#27687;&#21270;&#38150;&#30340;&#28909;&#23548;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Green-Kubo (GK)&#26041;&#27861;&#26159;&#26448;&#26009;&#28909;&#20256;&#36755;&#27169;&#25311;&#30340;&#20005;&#26684;&#26694;&#26550;&#65292;&#20294;&#23427;&#38656;&#35201;&#20934;&#30830;&#25551;&#36848;&#21183;&#33021;&#34920;&#38754;&#19988;&#25910;&#25947;&#32479;&#35745;&#37327;&#12290;&#26426;&#22120;&#23398;&#20064;&#21183;&#21487;&#20197;&#36798;&#21040;&#31532;&#19968;&#24615;&#21407;&#29702;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#19968;&#23567;&#37096;&#20998;&#25104;&#26412;&#20869;&#36229;&#36234;&#20854;&#27169;&#25311;&#26102;&#38388;&#21644;&#38271;&#24230;&#23610;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#23558;GK&#26041;&#27861;&#24212;&#29992;&#20110;&#26368;&#36817;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#22120;&#23398;&#20064;&#21183;&#31867;&#65292;&#35813;&#26041;&#27861;&#36845;&#20195;&#22320;&#32771;&#34385;&#21021;&#22987;&#20132;&#20114;&#25130;&#26029;&#20197;&#22806;&#30340;&#21322;&#23616;&#22495;&#20132;&#20114;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#36866;&#24212;&#30340;&#28909;&#36890;&#37327;&#20844;&#24335;&#65292;&#21487;&#20197;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#35745;&#31639;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#20108;&#27687;&#21270;&#38150;&#30340;&#28909;&#23548;&#29575;&#39564;&#35777;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Green-Kubo (GK) method is a rigorous framework for heat transport simulations in materials. However, it requires an accurate description of the potential-energy surface and carefully converged statistics. Machine-learning potentials can achieve the accuracy of first-principles simulations while allowing to reach well beyond their simulation time and length scales at a fraction of the cost. In this paper, we explain how to apply the GK approach to the recent class of message-passing machine-learning potentials, which iteratively consider semi-local interactions beyond the initial interaction cutoff. We derive an adapted heat flux formulation that can be implemented using automatic differentiation without compromising computational efficiency. The approach is demonstrated and validated by calculating the thermal conductivity of zirconium dioxide across temperatures.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#25968;&#25454;&#27744;&#30340;&#12289;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#21253;&#25324;&#27169;&#31946;&#30340;&#12289;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#24067;&#22806;&#26679;&#26412;&#20197;&#21450;&#20998;&#24067;&#20869;&#26679;&#26412;&#30340;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.14433</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#29616;&#23454;&#25968;&#25454;&#27744;&#20551;&#35774;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Active Learning with Contrastive Learning Under Realistic Data Pool Assumptions. (arXiv:2303.14433v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14433
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#25968;&#25454;&#27744;&#30340;&#12289;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#21253;&#25324;&#27169;&#31946;&#30340;&#12289;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#24067;&#22806;&#26679;&#26412;&#20197;&#21450;&#20998;&#24067;&#20869;&#26679;&#26412;&#30340;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#27744;&#20013;&#35782;&#21035;&#20986;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#24555;&#36895;&#36798;&#21040;&#25152;&#38656;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#23545;&#20110;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#25165;&#33021;&#36798;&#21040;&#39640;&#24615;&#33021;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23588;&#20026;&#26377;&#30410;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#22312;&#19968;&#20010;&#29702;&#24819;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#21482;&#26377;&#19982;&#30446;&#26631;&#20219;&#21153;&#30456;&#20851;&#30340;&#26679;&#26412;&#65292;&#21363;&#20998;&#24067;&#20869;&#26679;&#26412;&#65292;&#23384;&#22312;&#20110;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#27744;&#20013;&#12290;&#28982;&#32780;&#65292;&#20174;&#37326;&#22806;&#25910;&#38598;&#30340;&#25968;&#25454;&#27744;&#24456;&#21487;&#33021;&#21253;&#21547;&#23436;&#20840;&#19982;&#30446;&#26631;&#20219;&#21153;&#26080;&#20851;&#30340;&#26679;&#26412;&#21644;/&#25110;&#23545;&#20110;&#29978;&#33267;&#23545;&#20110;&#31070;&#35861;&#26469;&#35828;&#37117;&#26080;&#27861;&#20998;&#37197;&#21333;&#20010;&#31867;&#26631;&#31614;&#30340;&#36807;&#20110;&#27169;&#31946;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#20551;&#35774;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#27744;&#21253;&#21547;&#26469;&#33258;&#21508;&#31181;&#20998;&#24067;&#30340;&#26679;&#26412;&#26356;&#21152;&#29616;&#23454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21253;&#25324;&#27169;&#31946;&#30340;&#12289;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#24067;&#22806;&#26679;&#26412;&#20197;&#21450;&#20998;&#24067;&#20869;&#26679;&#26412;&#30340;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#33719;&#21462;&#26435;&#23041;&#20449;&#24687;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning aims to identify the most informative data from an unlabeled data pool that enables a model to reach the desired accuracy rapidly. This benefits especially deep neural networks which generally require a huge number of labeled samples to achieve high performance. Most existing active learning methods have been evaluated in an ideal setting where only samples relevant to the target task, i.e., in-distribution samples, exist in an unlabeled data pool. A data pool gathered from the wild, however, is likely to include samples that are irrelevant to the target task at all and/or too ambiguous to assign a single class label even for the oracle. We argue that assuming an unlabeled data pool consisting of samples from various distributions is more realistic. In this work, we introduce new active learning benchmarks that include ambiguous, task-irrelevant out-of-distribution as well as in-distribution samples. We also propose an active learning method designed to acquire informat
&lt;/p&gt;</description></item><item><title>Beta-VAE&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#21463;&#28508;&#22312;&#21464;&#37327;&#24635;&#37327;&#24433;&#21709;&#65306;&#20351;&#29992;&#23569;&#37327;&#28508;&#22312;&#21464;&#37327;&#26102;&#34920;&#29616;&#20026;PCA&#65292;&#20351;&#29992;&#22823;&#37327;&#28508;&#22312;&#21464;&#37327;&#26102;&#34920;&#29616;&#20026;ICA&#12290;</title><link>http://arxiv.org/abs/2303.14430</link><description>&lt;p&gt;
Beta-VAE&#26377;&#20004;&#31181;&#34920;&#29616;&#24418;&#24335;&#65306;PCA&#25110;ICA&#65311;
&lt;/p&gt;
&lt;p&gt;
Beta-VAE has 2 Behaviors: PCA or ICA?. (arXiv:2303.14430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14430
&lt;/p&gt;
&lt;p&gt;
Beta-VAE&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#21463;&#28508;&#22312;&#21464;&#37327;&#24635;&#37327;&#24433;&#21709;&#65306;&#20351;&#29992;&#23569;&#37327;&#28508;&#22312;&#21464;&#37327;&#26102;&#34920;&#29616;&#20026;PCA&#65292;&#20351;&#29992;&#22823;&#37327;&#28508;&#22312;&#21464;&#37327;&#26102;&#34920;&#29616;&#20026;ICA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Beta-VAE&#26159;&#19968;&#31181;&#38750;&#24120;&#32463;&#20856;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#25193;&#23637;&#29942;&#39048;&#30340;&#20351;&#29992;&#21487;&#20351;&#20449;&#24687;&#36880;&#28176;&#36827;&#20837;&#35299;&#30721;&#22120;&#65292;&#36825;&#26159;&#34920;&#31034;&#35299;&#32544;&#20197;&#21450;&#39640;&#36136;&#37327;&#37325;&#24314;&#30340;&#20851;&#38190;&#12290;&#22312;&#26368;&#36817;&#23545;&#36825;&#31181;&#36855;&#20154;&#32467;&#26500;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#28508;&#22312;&#21464;&#37327;&#30340;&#24635;&#37327;&#21487;&#20197;&#24433;&#21709;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65306;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#28508;&#22312;&#21464;&#37327;&#26102;&#65292;&#32593;&#32476;&#20542;&#21521;&#20110;&#23398;&#20064;&#26368;&#37325;&#35201;&#25110;&#20027;&#35201;&#30340;&#21464;&#37327;&#65292;&#34920;&#29616;&#24471;&#20687;&#19968;&#20010;PCA; &#20351;&#29992;&#38750;&#24120;&#22823;&#37327;&#30340;&#28508;&#22312;&#21464;&#37327;&#26102;&#65292;&#21464;&#37327;&#20542;&#21521;&#20110;&#26356;&#21152;&#35299;&#32544;&#65292;&#34920;&#29616;&#20986;&#31867;&#20284;ICA&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#20026;&#33719;&#21462;&#26368;&#22823;&#20449;&#24687;&#24102;&#23485;&#32780;&#36827;&#34892;&#30340;&#31454;&#20105;&#21487;&#33021;&#23548;&#33268;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beta-VAE is a very classical model for disentangled representation learning, the use of an expanding bottleneck that allow information into the decoder gradually is key to representation disentanglement as well as high-quality reconstruction. During recent experiments on such fascinating structure, we discovered that the total amount of latent variables can affect the representation learnt by the network: with very few latent variables, the network tend to learn the most important or principal variables, acting like a PCA; with very large numbers of latent variables, the variables tend to be more disentangled, and act like an ICA. Our assumption is that the competition between latent variables while trying to gain the most information bandwidth can lead to this phenomenon.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#36830;&#32493;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#21452;&#27169;&#24577;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#21160;&#24577;&#22686;&#21152;&#21442;&#25968;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#20849;&#20139;&#20449;&#24687;&#21644;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.14423</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#20851;&#27880;&#21464;&#25442;&#22120;&#26550;&#26500;&#30340;&#30693;&#35782;&#33976;&#39311;&#36830;&#32493;&#23398;&#20064;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation. (arXiv:2303.14423v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#36830;&#32493;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#21452;&#27169;&#24577;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#21160;&#24577;&#22686;&#21152;&#21442;&#25968;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#20849;&#20139;&#20449;&#24687;&#21644;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#32780;&#35328;&#65292;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#21644;&#35745;&#31639;&#36127;&#36733;&#25104;&#20026;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#32780;&#36830;&#32493;&#23398;&#20064; (CL) &#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#34917;&#25937;&#25514;&#26045;&#65292;&#36890;&#36807;&#20351;&#39034;&#24207;&#21040;&#36798;&#30340;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#24494;&#35843;&#25152;&#26377;&#32593;&#32476;&#26435;&#37325;&#30340;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340; CL &#31639;&#27861;&#20027;&#35201;&#32771;&#34385;&#23398;&#20064;&#21333;&#27169;&#24577;&#30340;&#20165;&#35270;&#35273;&#25110;&#20165;&#35821;&#35328;&#20219;&#21153;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340; CL &#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#21452;&#27169;&#24577;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#36890;&#36807;&#21160;&#24577;&#22686;&#21152;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#25968;&#37327;&#24182;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#12290;&#26032;&#28155;&#21152;&#30340;&#21442;&#25968;&#29992;&#20110;&#20026;&#27599;&#20010;&#20219;&#21153;&#19987;&#38376;&#23450;&#21046;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#20219;&#21153;&#20043;&#38388;&#21487;&#20197;&#20849;&#20139;&#20449;&#24687;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#25193;&#23637;&#21040;&#22823;&#37327;&#20219;&#21153;&#30340;&#23398;&#20064;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#24456;&#23569;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The size and the computational load of fine-tuning large-scale pre-trained neural network are becoming two major obstacles in adopting machine learning in many applications. Continual learning (CL) can serve as a remedy through enabling knowledge-transfer across sequentially arriving tasks which relaxes the need to fine-tune all network weights from scratch. However, existing CL algorithms primarily consider learning unimodal vision-only or language-only tasks. We develop a transformer-based CL architecture for learning bimodal vision-and-language tasks based on increasing the number of the learnable parameters dynamically and using knowledge distillation. The new additional parameters are used to specialize the network for each task. Our approach enables sharing information between the tasks while addressing the challenge of catastrophic forgetting. Our approach is scalable learning to a large number of tasks because it requires little memory and time overhead. Our model reaches state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#24863;&#30693;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#20849;&#20139;&#27773;&#36710;&#31449;&#28857;&#30340;&#24179;&#22343;&#26376;&#24230;&#38656;&#27714;&#65292;&#21033;&#29992;&#20102;&#20016;&#23500;&#30340;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.14421</link><description>&lt;p&gt;
&#31354;&#38388;&#24863;&#30693;&#30340;&#20849;&#20139;&#27773;&#36710;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatially-Aware Car-Sharing Demand Prediction. (arXiv:2303.14421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#24863;&#30693;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#20849;&#20139;&#27773;&#36710;&#31449;&#28857;&#30340;&#24179;&#22343;&#26376;&#24230;&#38656;&#27714;&#65292;&#21033;&#29992;&#20102;&#20016;&#23500;&#30340;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20849;&#20139;&#27773;&#36710;&#26381;&#21153;&#20316;&#20026;&#31169;&#20154;&#20010;&#20154;&#20986;&#34892;&#30340;&#21487;&#34892;&#26367;&#20195;&#21697;&#20986;&#29616;&#65292;&#25215;&#35834;&#26356;&#21487;&#25345;&#32493;&#12289;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#26356;&#39640;&#65292;&#20294;&#20173;&#28982;&#31561;&#21516;&#20110;&#31169;&#20154;&#20986;&#34892;&#12290;&#20851;&#20110;&#30701;&#26399;&#39044;&#27979;&#21644;&#20248;&#21270;&#26041;&#27861;&#30340;&#30740;&#31350;&#24050;&#32463;&#25913;&#21892;&#20102;&#20849;&#20139;&#27773;&#36710;&#26381;&#21153;&#30340;&#36816;&#33829;&#21644;&#36710;&#38431;&#25511;&#21046;;&#28982;&#32780;&#65292;&#22312;&#25991;&#29486;&#20013;&#38271;&#26399;&#39044;&#27979;&#21644;&#31354;&#38388;&#20998;&#26512;&#26159;&#32570;&#20047;&#30340;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20855;&#26377;&#31354;&#38388;&#24863;&#30693;&#23398;&#20064;&#31639;&#27861;&#30340;&#24179;&#22343;&#26376;&#24230;&#38656;&#27714;&#26469;&#20998;&#26512;&#22522;&#20110;&#31449;&#28857;&#30340;&#20849;&#20139;&#27773;&#36710;&#26381;&#21153;&#65292;&#36825;&#31181;&#31639;&#27861;&#26082;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#21448;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20840;&#29699;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#19982;&#31354;&#38388;&#24863;&#30693;&#26041;&#27861;&#26469;&#39044;&#27979;&#24179;&#22343;&#27599;&#20010;&#31449;&#28857;&#30340;&#26376;&#24230;&#38656;&#27714;&#12290;&#35813;&#30740;&#31350;&#21033;&#29992;&#20102;&#20016;&#23500;&#30340;&#31038;&#20250;-&#20154;&#21475;&#23398;&#12289;&#22522;&#20110;&#20301;&#32622;&#30340;(&#20363;&#22914;POI)&#21644;&#20849;&#20139;&#27773;&#36710;&#29305;&#23450;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#20123;&#29305;&#24449;&#26469;&#33258;&#19968;&#20010;&#22823;&#22411;&#30340;&#19987;&#26377;&#20849;&#20139;&#27773;&#36710;&#25968;&#25454;&#38598;&#21644;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20840;&#29699;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26368;&#22909;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#31354;&#38388;&#24863;&#30693;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, car-sharing services have emerged as viable alternatives to private individual mobility, promising more sustainable and resource-efficient, but still comfortable transportation. Research on short-term prediction and optimization methods has improved operations and fleet control of car-sharing services; however, long-term projections and spatial analysis are sparse in the literature. We propose to analyze the average monthly demand in a station-based car-sharing service with spatially-aware learning algorithms that offer high predictive performance as well as interpretability. In particular, we compare the spatially-implicit Random Forest model with spatially-aware methods for predicting average monthly per-station demand. The study utilizes a rich set of socio-demographic, location-based (e.g., POIs), and car-sharing-specific features as input, extracted from a large proprietary car-sharing dataset and publicly available datasets. We show that the global Random Forest 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110; Deep LDA &#30340;&#22810;&#22218;&#21365;&#24034;&#32508;&#21512;&#30151;&#39044;&#27979;&#26041;&#26696;&#65292;&#37319;&#29992;&#24102;&#21464;&#21270;&#30340;&#28145;&#24230;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#31639;&#27861;&#65292;&#32469;&#36807;&#31616;&#21333;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#22270;&#24418;&#22788;&#29702;&#22120;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.14401</link><description>&lt;p&gt;
&#24102;&#21464;&#21270;&#30340;&#28145;&#24230;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#22312;&#22810;&#22218;&#21365;&#24034;&#32508;&#21512;&#30151;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Linear Discriminant Analysis with Variation for Polycystic Ovary Syndrome Classification. (arXiv:2303.14401v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110; Deep LDA &#30340;&#22810;&#22218;&#21365;&#24034;&#32508;&#21512;&#30151;&#39044;&#27979;&#26041;&#26696;&#65292;&#37319;&#29992;&#24102;&#21464;&#21270;&#30340;&#28145;&#24230;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#31639;&#27861;&#65292;&#32469;&#36807;&#31616;&#21333;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#22270;&#24418;&#22788;&#29702;&#22120;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#22218;&#21365;&#24034;&#32508;&#21512;&#30151;&#35786;&#26029;&#26159;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#39044;&#27979;&#23398;&#20064;&#31243;&#24207;&#26469;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#22312;&#22810;&#22218;&#21365;&#24034;&#32508;&#21512;&#30151;&#30340;&#35786;&#26029;&#26041;&#38754;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#26159;&#36825;&#20123;&#31639;&#27861;&#22312;&#21033;&#29992;&#22270;&#24418;&#22788;&#29702;&#22120;&#30340;&#22788;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#20808;&#36827;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#31616;&#21333;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#30340;&#32447;&#24615;&#38477;&#32500;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340; Deep LDA &#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110; Deep LDA &#30340;&#22810;&#22218;&#21365;&#24034;&#32508;&#21512;&#30151;&#39044;&#27979;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The polycystic ovary syndrome diagnosis is a problem that can be leveraged using prognostication based learning procedures. Many implementations of PCOS can be seen with Machine Learning but the algorithms have certain limitations in utilizing the processing power graphical processing units. The simple machine learning algorithms can be improved with advanced frameworks using Deep Learning. The Linear Discriminant Analysis is a linear dimensionality reduction algorithm for classification that can be boosted in terms of performance using deep learning with Deep LDA, a transformed version of the traditional LDA. In this result oriented paper we present the Deep LDA implementation with a variation for prognostication of PCOS.
&lt;/p&gt;</description></item><item><title>IFSeg&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#29305;&#23450;&#22270;&#20687;&#21644;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#22270;&#20687;&#20998;&#21106;&#23545;&#26356;&#26032;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#65292;&#23545;&#26410;&#30693;&#31867;&#21035;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#24378;&#12290;</title><link>http://arxiv.org/abs/2303.14396</link><description>&lt;p&gt;
IFSeg&#65306;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
IFSeg: Image-free Semantic Segmentation via Vision-Language Model. (arXiv:2303.14396v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14396
&lt;/p&gt;
&lt;p&gt;
IFSeg&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#29305;&#23450;&#22270;&#20687;&#21644;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#22270;&#20687;&#20998;&#21106;&#23545;&#26356;&#26032;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#65292;&#23545;&#26410;&#30693;&#31867;&#21035;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35270;&#35273;&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#22240;&#20854;&#22312;&#19981;&#21516;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#28789;&#27963;&#24615;&#65288;&#20363;&#22914;&#36328;&#27169;&#24577;&#36716;&#31227;&#65289;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;VL&#39537;&#21160;&#30340;&#20998;&#21106;&#20219;&#21153;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#24182;&#19988;&#29616;&#26377;&#26041;&#27861;&#20173;&#38656;&#35201;&#33719;&#21462;&#39069;&#22806;&#30340;&#35757;&#32451;&#22270;&#20687;&#29978;&#33267;&#20998;&#21106;&#27880;&#37322;&#26469;&#36866;&#24212;&#19979;&#28216;&#20998;&#21106;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#21482;&#26377;&#19968;&#32452;&#30446;&#26631;&#35821;&#20041;&#31867;&#21035;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#20294;&#19981;&#20351;&#29992;&#20219;&#20309;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#22270;&#20687;&#21644;&#27880;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;IFSeg&#65292;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;VL&#30340;&#20154;&#24037;&#22270;&#20687;&#20998;&#21106;&#23545;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;&#30340;VL&#27169;&#22411;&#20197;&#36866;&#24212;&#20998;&#21106;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#38543;&#26426;&#35821;&#20041;&#31867;&#21035;&#30340;2D&#22320;&#22270;&#20197;&#21450;&#21478;&#19968;&#20010;&#22320;&#22270;&#30340;&#30456;&#24212;&#21333;&#35789;&#26631;&#35760;&#26469;&#26500;&#36896;&#36825;&#20123;&#20154;&#36896;&#35757;&#32451;&#25968;&#25454;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#30340;VL&#27169;&#22411;&#21487;&#20197;&#23558;&#35821;&#20041;&#30701;&#35821;&#19982;&#20854;&#35270;&#35273;&#34920;&#31034;&#30456;&#20851;&#32852;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#29983;&#25104;&#24102;&#26377;&#22320;&#38754;&#30495;&#23454;&#35821;&#20041;&#20998;&#21106;&#25513;&#27169;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21644;&#19981;&#21516;&#32423;&#21035;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language (VL) pre-training has recently gained much attention for its transferability and flexibility in novel concepts (e.g., cross-modality transfer) across various visual tasks. However, VL-driven segmentation has been under-explored, and the existing approaches still have the burden of acquiring additional training images or even segmentation annotations to adapt a VL model to downstream segmentation tasks. In this paper, we introduce a novel image-free segmentation task where the goal is to perform semantic segmentation given only a set of the target semantic categories, but without any task-specific images and annotations. To tackle this challenging task, our proposed method, coined IFSeg, generates VL-driven artificial image-segmentation pairs and updates a pre-trained VL model to a segmentation task. We construct this artificial training data by creating a 2D map of random semantic categories and another map of their corresponding word tokens. Given that a pre-trained VL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#27744;&#21270;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;MP3DCNN&#65289;&#26469;&#25552;&#39640;fMRI&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#23558;&#20998;&#31867;&#20934;&#30830;&#29575;&#20174;1.684%&#25552;&#39640;&#21040;14.918%,&#36866;&#29992;&#20110;&#20998;&#31867;&#38754;&#37096;&#19982;&#23545;&#35937;&#20197;&#21450;&#38754;&#37096;&#21644;&#23545;&#35937;&#30340;&#23376;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2303.14391</link><description>&lt;p&gt;
&#22810;&#27744;&#21270;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;fMRI&#20998;&#31867;&#35270;&#35273;&#33041;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Multi-pooling 3D Convolutional Neural Network for fMRI Classification of Visual Brain States. (arXiv:2303.14391v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#27744;&#21270;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;MP3DCNN&#65289;&#26469;&#25552;&#39640;fMRI&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#23558;&#20998;&#31867;&#20934;&#30830;&#29575;&#20174;1.684%&#25552;&#39640;&#21040;14.918%,&#36866;&#29992;&#20110;&#20998;&#31867;&#38754;&#37096;&#19982;&#23545;&#35937;&#20197;&#21450;&#38754;&#37096;&#21644;&#23545;&#35937;&#30340;&#23376;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#25968;&#25454;&#23545;&#35270;&#35273;&#23545;&#35937;&#20998;&#31867;&#36827;&#34892;&#31070;&#32463;&#35299;&#30721;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#29702;&#35299;&#28508;&#22312;&#30340;&#33041;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27744;&#21270;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;MP3DCNN&#65289;&#26469;&#25552;&#39640;fMRI&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;MP3DCNN&#20027;&#35201;&#30001;&#19977;&#23618;3DCNN&#32452;&#25104;&#65292;&#20854;&#20013;&#31532;&#19968;&#23618;&#21644;&#31532;&#20108;&#23618;&#30340;3D&#21367;&#31215;&#21508;&#20855;&#26377;&#19968;&#20010;&#27744;&#21270;&#36830;&#25509;&#20998;&#25903;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20197;&#24448;&#30740;&#31350;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#20998;&#31867;&#20934;&#30830;&#29575;&#20174;1.684%&#25552;&#39640;&#21040;14.918% &#65292;&#29992;&#20110;&#20998;&#31867;&#38754;&#37096;&#19982;&#23545;&#35937;&#20197;&#21450;&#38754;&#37096;&#21644;&#23545;&#35937;&#30340;&#23376;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural decoding of visual object classification via functional magnetic resonance imaging (fMRI) data is challenging and is vital to understand underlying brain mechanisms. This paper proposed a multi-pooling 3D convolutional neural network (MP3DCNN) to improve fMRI classification accuracy. MP3DCNN is mainly composed of a three-layer 3DCNN, where the first and second layers of 3D convolutions each have a branch of pooling connection. The results showed that this model can improve the classification accuracy for categorical (face vs. object), face sub-categorical (male face vs. female face), and object sub-categorical (natural object vs. artificial object) classifications from 1.684% to 14.918% over the previous study in decoding brain mechanisms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#20027;&#21160;&#24494;&#35843;&#8221;&#20219;&#21153;&#65292;&#26088;&#22312;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#21033;&#29992;&#27880;&#37322;&#39044;&#31639;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ActiveFT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21442;&#25968;&#27169;&#22411;&#26469;&#36873;&#25321;&#19968;&#32452;&#25968;&#25454;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#20943;&#23567;&#25152;&#36873;&#23376;&#38598;&#30340;&#20998;&#24067;&#19982;&#25972;&#20010;&#25968;&#25454;&#27744;&#20043;&#38388;&#30340;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2303.14382</link><description>&lt;p&gt;
&#20027;&#21160;&#24494;&#35843;&#65306;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#21033;&#29992;&#27880;&#37322;&#39044;&#31639;
&lt;/p&gt;
&lt;p&gt;
Active Finetuning: Exploiting Annotation Budget in the Pretraining-Finetuning Paradigm. (arXiv:2303.14382v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#20027;&#21160;&#24494;&#35843;&#8221;&#20219;&#21153;&#65292;&#26088;&#22312;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#21033;&#29992;&#27880;&#37322;&#39044;&#31639;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ActiveFT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21442;&#25968;&#27169;&#22411;&#26469;&#36873;&#25321;&#19968;&#32452;&#25968;&#25454;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#20943;&#23567;&#25152;&#36873;&#23376;&#38598;&#30340;&#20998;&#24067;&#19982;&#25972;&#20010;&#25968;&#25454;&#27744;&#20043;&#38388;&#30340;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#39640;&#26114;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#22810;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#65292;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#24456;&#23569;&#20851;&#27880;&#21033;&#29992;&#27880;&#37322;&#39044;&#31639;&#26469;&#24494;&#35843;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#26412;&#25991;&#27491;&#24335;&#23450;&#20041;&#20102;&#36825;&#20010;&#26032;&#30340;&#20027;&#21160;&#24494;&#35843;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#36873;&#25321;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ActiveFT&#30340;&#26032;&#26041;&#27861;&#26469;&#36827;&#34892;&#20027;&#21160;&#24494;&#35843;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#20248;&#21270;&#21442;&#25968;&#27169;&#22411;&#65292;&#36873;&#25321;&#19982;&#25972;&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#27744;&#31867;&#20284;&#20998;&#24067;&#24182;&#20445;&#25345;&#36275;&#22815;&#22810;&#26679;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25152;&#36873;&#23376;&#38598;&#30340;&#20998;&#24067;&#21644;&#25972;&#20010;&#25968;&#25454;&#27744;&#20043;&#38388;&#30340;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20063;&#34987;&#20943;&#23567;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ActiveFT&#30456;&#23545;&#20110;&#22522;&#32447;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the large-scale data and the high annotation cost, pretraining-finetuning becomes a popular paradigm in multiple computer vision tasks. Previous research has covered both the unsupervised pretraining and supervised finetuning in this paradigm, while little attention is paid to exploiting the annotation budget for finetuning. To fill in this gap, we formally define this new active finetuning task focusing on the selection of samples for annotation in the pretraining-finetuning paradigm. We propose a novel method called ActiveFT for active finetuning task to select a subset of data distributing similarly with the entire unlabeled pool and maintaining enough diversity by optimizing a parametric model in the continuous space. We prove that the Earth Mover's distance between the distributions of the selected subset and the entire data pool is also reduced in this process. Extensive experiments show the leading performance and high efficiency of ActiveFT superior to baselines on both i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21482;&#26377;&#19968;&#20221;&#26631;&#27880;&#26679;&#26412;&#36827;&#34892;&#30333;&#36136;&#26463; (WM) &#20998;&#21106;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#27880;&#20876;&#24335;&#23792;&#20540;&#22686;&#24378; (RPA) &#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#31934;&#28860; (URe) &#27169;&#22359;&#26500;&#24314;&#65292;&#21487;&#20197;&#33719;&#24471;&#39640;&#31934;&#24230;&#21644;&#39640;&#39281;&#21644;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.14371</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#20876;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#21482;&#20351;&#29992;&#19968;&#20221;&#26631;&#27880;&#26679;&#26412;&#30340;&#30333;&#36136;&#26463;&#20998;&#21106;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Registration- and Uncertainty-based Framework for White Matter Tract Segmentation With Only One Annotated Subject. (arXiv:2303.14371v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21482;&#26377;&#19968;&#20221;&#26631;&#27880;&#26679;&#26412;&#36827;&#34892;&#30333;&#36136;&#26463; (WM) &#20998;&#21106;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#27880;&#20876;&#24335;&#23792;&#20540;&#22686;&#24378; (RPA) &#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#31934;&#28860; (URe) &#27169;&#22359;&#26500;&#24314;&#65292;&#21487;&#20197;&#33719;&#24471;&#39640;&#31934;&#24230;&#21644;&#39640;&#39281;&#21644;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687; (dMRI) &#30340;&#30333;&#36136;&#26463; (WM) &#20998;&#21106;&#22312;&#20154;&#31867;&#20581;&#24247;&#21644;&#22823;&#33041;&#30142;&#30149;&#20998;&#26512;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;WM tracts &#30340;&#27880;&#37322;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#65292;&#24182;&#19988;&#38656;&#35201;&#32463;&#39564;&#20016;&#23500;&#30340;&#31070;&#32463;&#35299;&#21078;&#23398;&#23478;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22312;&#26497;&#23569;&#27880;&#37322;&#24773;&#20917;&#19979;&#30340;&#30333;&#36136;&#26463;&#20998;&#21106;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20165;&#20351;&#29992;&#19968;&#20221;&#26631;&#27880;&#26679;&#26412; (subject-level one-shot) &#36827;&#34892;&#30333;&#36136;&#26463;&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#27880;&#20876;&#24335;&#23792;&#20540;&#22686;&#24378; (RPA) &#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#31934;&#28860; (URe) &#27169;&#22359;&#26500;&#24314;&#32780;&#25104;&#12290;&#20854;&#20013;&#65292;RPA&#27169;&#22359;&#32508;&#21512;&#20266;&#20027;&#20307;&#21450;&#20854;&#30456;&#24212;&#26631;&#31614;&#20197;&#25552;&#39640;&#30333;&#36136;&#26463;&#20998;&#21106;&#24615;&#33021;&#65292;URe&#27169;&#22359;&#21017;&#32531;&#35299;&#20302;&#32622;&#20449;&#24230;&#20687;&#32032;&#23545;&#20266;&#20027;&#20307;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22359;&#38750;&#24120;&#26377;&#25928;&#12290;&#32508;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#39640;&#39281;&#21644;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
White matter (WM) tract segmentation based on diffusion magnetic resonance imaging (dMRI) plays an important role in the analysis of human health and brain diseases. However, the annotation of WM tracts is time-consuming and needs experienced neuroanatomists. In this study, to explore tract segmentation in the challenging setting of minimal annotations, we propose a novel framework utilizing only one annotated subject (subject-level one-shot) for tract segmentation. Our method is constructed by proposed registration-based peak augmentation (RPA) and uncertainty-based refining (URe) modules. RPA module synthesizes pseudo subjects and their corresponding labels to improve the tract segmentation performance. The proposed URe module alleviates the negative influence of the low-confidence voxels on pseudo subjects. Experimental results show that our method outperforms other state-of-the-art methods by a large margin, and our proposed modules are effective. Overall, our method achieves accur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FlexNeRF&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21333;&#30446;&#35270;&#39057;&#20013;&#23454;&#29616;&#36816;&#21160;&#20154;&#20307;&#30340;&#30495;&#23454;&#33258;&#30001;&#35270;&#35282;&#28210;&#26579;&#12290;&#36890;&#36807;&#23545;&#26102;&#38388;&#21644;&#23039;&#24577;&#37197;&#32622;&#30340;&#20248;&#21270;&#20197;&#21450;&#39069;&#22806;&#30340;&#25439;&#22833;&#65292;&#21487;&#22312;&#35266;&#23519;&#35270;&#35282;&#21464;&#24471;&#26356;&#31232;&#30095;&#26102;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#36755;&#20986;&#65292;&#36825;&#22312;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#21450;&#33258;&#34892;&#25429;&#33719;&#30340;&#26102;&#23578;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14368</link><description>&lt;p&gt;
FlexNeRF&#65306;&#20174;&#31232;&#30095;&#35270;&#35282;&#20013;&#23454;&#29616;&#36816;&#21160;&#20154;&#20307;&#30340;&#30495;&#23454;&#33258;&#30001;&#35270;&#35282;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views. (arXiv:2303.14368v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FlexNeRF&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21333;&#30446;&#35270;&#39057;&#20013;&#23454;&#29616;&#36816;&#21160;&#20154;&#20307;&#30340;&#30495;&#23454;&#33258;&#30001;&#35270;&#35282;&#28210;&#26579;&#12290;&#36890;&#36807;&#23545;&#26102;&#38388;&#21644;&#23039;&#24577;&#37197;&#32622;&#30340;&#20248;&#21270;&#20197;&#21450;&#39069;&#22806;&#30340;&#25439;&#22833;&#65292;&#21487;&#22312;&#35266;&#23519;&#35270;&#35282;&#21464;&#24471;&#26356;&#31232;&#30095;&#26102;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#36755;&#20986;&#65292;&#36825;&#22312;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#21450;&#33258;&#34892;&#25429;&#33719;&#30340;&#26102;&#23578;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FlexNeRF&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21333;&#30446;&#35270;&#39057;&#20013;&#23454;&#29616;&#36816;&#21160;&#20154;&#20307;&#30340;&#30495;&#23454;&#33258;&#30001;&#35270;&#35282;&#28210;&#26579;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#31232;&#30095;&#35270;&#35282;&#65292;&#23588;&#20854;&#26159;&#24403;&#20027;&#39064;&#34920;&#29616;&#20986;&#24555;&#36895;/&#22797;&#26434;&#36816;&#21160;&#26102;&#65292;&#38656;&#35201;&#20811;&#26381;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20248;&#21270;&#35268;&#33539;&#26102;&#38388;&#21644;&#23039;&#24577;&#37197;&#32622;&#65292;&#20351;&#23039;&#24577;&#30456;&#20851;&#30340;&#36816;&#21160;&#22330;&#21644;&#23039;&#24577;&#26080;&#20851;&#30340;&#26102;&#38388;&#21464;&#24418;&#20114;&#34917;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#26102;&#38388;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#32422;&#26463;&#20197;&#21450;&#23545;&#20013;&#38388;&#34920;&#31034;&#30340;&#39069;&#22806;&#25439;&#22833;&#65288;&#22914;&#20998;&#21106;&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#36755;&#20986;&#65292;&#29978;&#33267;&#22312;&#35266;&#23519;&#21040;&#30340;&#35270;&#35282;&#21464;&#24471;&#26356;&#31232;&#30095;&#26102;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#23454;&#35777;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#21450;&#33258;&#34892;&#25429;&#33719;&#30340;&#26102;&#23578;&#25968;&#25454;&#38598;&#19978;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#39033;&#30446;&#39029;&#38754;&#32593;&#22336;&#20026;&#65306;https://flex-nerf.github.io/
&lt;/p&gt;
&lt;p&gt;
We present FlexNeRF, a method for photorealistic freeviewpoint rendering of humans in motion from monocular videos. Our approach works well with sparse views, which is a challenging scenario when the subject is exhibiting fast/complex motions. We propose a novel approach which jointly optimizes a canonical time and pose configuration, with a pose-dependent motion field and pose-independent temporal deformations complementing each other. Thanks to our novel temporal and cyclic consistency constraints along with additional losses on intermediate representation such as segmentation, our approach provides high quality outputs as the observed views become sparser. We empirically demonstrate that our method significantly outperforms the state-of-the-art on public benchmark datasets as well as a self-captured fashion dataset. The project page is available at: https://flex-nerf.github.io/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#28151;&#21512;&#27169;&#31946;-&#28165;&#26224;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#27169;&#31946;C&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#22312;&#32858;&#31867;&#22823;&#23567;&#24046;&#24322;&#24040;&#22823;&#26102;&#30340;&#19981;&#24179;&#34913;&#24433;&#21709;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#32858;&#31867;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14366</link><description>&lt;p&gt;
&#28151;&#21512;&#27169;&#31946;-&#28165;&#26224;&#32858;&#31867;&#31639;&#27861;&#65306;&#29702;&#35770;&#19982;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Hybrid Fuzzy-Crisp Clustering Algorithm: Theory and Experiments. (arXiv:2303.14366v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#28151;&#21512;&#27169;&#31946;-&#28165;&#26224;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#27169;&#31946;C&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#22312;&#32858;&#31867;&#22823;&#23567;&#24046;&#24322;&#24040;&#22823;&#26102;&#30340;&#19981;&#24179;&#34913;&#24433;&#21709;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#32858;&#31867;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#27169;&#31946;C&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#20013;&#65292;&#30001;&#20110;&#38582;&#23646;&#20989;&#25968;&#22987;&#32456;&#20026;&#27491;&#65292;&#24403;&#32858;&#31867;&#22823;&#23567;&#24046;&#24322;&#24040;&#22823;&#26102;&#65292;&#20250;&#23548;&#33268;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#21363;&#65292;&#19968;&#20010;&#26126;&#26174;&#26356;&#22823;&#30340;&#32858;&#31867;&#23558;&#25152;&#26377;&#20854;&#20182;&#32858;&#31867;&#22352;&#26631;&#28857;&#21560;&#24341;&#21040;&#20854;&#20013;&#24515;&#65292;&#26080;&#35770;&#23427;&#20204;&#26377;&#22810;&#36828;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38582;&#23646;&#24230;&#20989;&#25968;&#32447;&#24615;&#21644;&#20108;&#27425;&#39033;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#28151;&#21512;&#27169;&#31946;-&#28165;&#26224;&#32858;&#31867;&#31639;&#27861;&#12290;&#22312;&#35813;&#31639;&#27861;&#20013;&#65292;&#22914;&#26524;&#25968;&#25454;&#28857;&#36317;&#31163;&#32858;&#31867;&#20013;&#24515;&#8220;&#36275;&#22815;&#8221;&#36828;&#65292;&#21017;&#23558;&#20854;&#38582;&#23646;&#24230;&#31934;&#30830;&#22320;&#35774;&#32622;&#20026;&#38646;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#27169;&#31946;-&#28165;&#26224;&#32858;&#31867;&#31639;&#27861;&#21450;&#20854;&#20960;&#20309;&#35299;&#37322;&#12290;&#35813;&#31639;&#27861;&#22312;&#20108;&#21313;&#20010;&#27169;&#25311;&#30340;&#25968;&#25454;&#38598;&#21644;&#20116;&#20010;&#26469;&#33258;UCI&#25968;&#25454;&#20179;&#24211;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;&#27169;&#31946;&#21644;&#28165;&#26224;&#32858;&#31867;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#32858;&#31867;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the membership function being strictly positive, the conventional fuzzy c-means clustering method sometimes causes imbalanced influence when clusters of vastly different sizes exist. That is, an outstandingly large cluster drags to its center all the other clusters, however far they are separated. To solve this problem, we propose a hybrid fuzzy-crisp clustering algorithm based on a target function combining linear and quadratic terms of the membership function. In this algorithm, the membership of a data point to a cluster is automatically set to exactly zero if the data point is ``sufficiently'' far from the cluster center. In this paper, we present a new algorithm for hybrid fuzzy-crisp clustering along with its geometric interpretation. The algorithm is tested on twenty simulated data generated and five real-world datasets from the UCI repository and compared with conventional fuzzy and crisp clustering methods. The proposed algorithm is demonstrated to outperform the conventi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#30340;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#26469;&#32531;&#35299;&#31169;&#20154;&#26631;&#27880;&#22270;&#20687;&#30340;&#30701;&#32570;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#12289;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#65292;&#36798;&#21040;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.14357</link><description>&lt;p&gt;
&#22788;&#29702;&#24322;&#26500;3D MR&#33181;&#20851;&#33410;&#22270;&#20687;&#65306;&#19968;&#31181;&#20855;&#26377;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#30340;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dealing With Heterogeneous 3D MR Knee Images: A Federated Few-Shot Learning Method With Dual Knowledge Distillation. (arXiv:2303.14357v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#30340;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#26469;&#32531;&#35299;&#31169;&#20154;&#26631;&#27880;&#22270;&#20687;&#30340;&#30701;&#32570;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#12289;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#65292;&#36798;&#21040;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#21307;&#23398;&#26426;&#26500;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#19981;&#27719;&#24635;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23458;&#25143;(&#22914;&#21307;&#38498;)&#20043;&#38388;&#30340;&#21327;&#20316;&#22521;&#35757;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20026;&#22823;&#22411;3D&#22270;&#20687;&#25968;&#25454;&#38598;&#21019;&#24314;&#27880;&#37322;&#30340;&#25104;&#26412;&#39640;&#26114;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#30103;&#26426;&#26500;&#26469;&#35828;&#65292;&#20182;&#20204;&#27809;&#26377;&#36275;&#22815;&#30340;&#30417;&#30563;&#25968;&#25454;&#26469;&#36827;&#34892;&#26412;&#22320;&#22521;&#35757;&#12290;&#22240;&#27492;&#65292;&#26377;&#38480;&#30417;&#30563;&#19979;&#30340;&#21512;&#20316;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#26426;&#26500;&#26377;&#36164;&#28304;&#26469;&#32534;&#21046;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#26631;&#31614;&#25968;&#25454;&#23384;&#20648;&#24211;&#12290;&#22240;&#27492;&#65292;&#20010;&#20307;&#23458;&#25143;&#21487;&#20197;&#21033;&#29992;&#20174;&#20844;&#20849;&#25968;&#25454;&#23384;&#20648;&#24211;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#26469;&#32531;&#35299;&#31169;&#26377;&#26631;&#27880;&#22270;&#20687;&#30340;&#30701;&#32570;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#30340;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#22312;&#23458;&#25143;&#20043;&#38388;&#36827;&#34892;&#26377;&#38480;&#26631;&#27880;&#30340;&#32852;&#21512;&#22521;&#35757;&#65292;&#32780;&#19981;&#20250;&#21361;&#23475;&#38544;&#31169;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#30417;&#30563;&#23398;&#20064;&#20174;&#27599;&#20010;&#23458;&#25143;&#30340;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#32780;&#26080;&#30417;&#30563;&#23398;&#20064;&#20174;&#20844;&#20849;&#25968;&#25454;&#23384;&#20648;&#24211;&#20013;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#20197;&#29983;&#25104;&#26356;&#22810;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;&#23545;&#33181;&#20851;&#33410;MR&#22270;&#20687;&#30340;&#24322;&#26500;&#20020;&#24202;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;0.87&#30340;AUC&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning has gained popularity among medical institutions since it enables collaborative training between clients (e.g., hospitals) without aggregating data. However, due to the high cost associated with creating annotations, especially for large 3D image datasets, clinical institutions do not have enough supervised data for training locally. Thus, the performance of the collaborative model is subpar under limited supervision. On the other hand, large institutions have the resources to compile data repositories with high-resolution images and labels. Therefore, individual clients can utilize the knowledge acquired in the public data repositories to mitigate the shortage of private annotated images. In this paper, we propose a federated few-shot learning method with dual knowledge distillation. This method allows joint training with limited annotations across clients without jeopardizing privacy. The supervised learning of the proposed method extracts features from limited lab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;(mmLBRA)&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;O-RAN&#20013;&#30340;&#36127;&#36733;&#22343;&#34913;&#21644;&#36164;&#28304;&#20998;&#37197;&#65292;&#20197;&#35299;&#20915;&#32593;&#32476;&#25317;&#22622;&#21644;&#29992;&#25143;&#25925;&#38556;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.14355</link><description>&lt;p&gt;
O-RAN&#20013;&#30340;&#26234;&#33021;&#36127;&#36733;&#22343;&#34913;&#21644;&#36164;&#28304;&#20998;&#37197;&#65306;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Intelligent Load Balancing and Resource Allocation in O-RAN: A Multi-Agent Multi-Armed Bandit Approach. (arXiv:2303.14355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;(mmLBRA)&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;O-RAN&#20013;&#30340;&#36127;&#36733;&#22343;&#34913;&#21644;&#36164;&#28304;&#20998;&#37197;&#65292;&#20197;&#35299;&#20915;&#32593;&#32476;&#25317;&#22622;&#21644;&#29992;&#25143;&#25925;&#38556;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;(O-RAN)&#26550;&#26500;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26550;&#26500;&#30340;&#24320;&#25918;&#25509;&#21475;&#23454;&#29616;&#20102;&#32593;&#32476;&#21151;&#33021;&#34394;&#25311;&#21270;&#65292;&#20351;O-RAN&#25104;&#20026;&#20027;&#35201;&#30340;&#29992;&#25143;&#36890;&#20449;&#35774;&#22791;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#39057;&#29575;&#36164;&#28304;&#21644;&#20449;&#24687;&#29190;&#28856;&#20351;&#24471;&#22312;&#27809;&#26377;&#26377;&#25928;&#30340;&#27969;&#37327;&#25511;&#21046;&#25110;&#36164;&#28304;&#20998;&#37197;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20339;&#30340;&#32593;&#32476;&#20307;&#39564;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#36827;&#34892;&#22522;&#20110;&#31227;&#21160;&#24615;&#30340;&#36127;&#36733;&#22343;&#34913;&#26469;&#22343;&#21248;&#22320;&#20998;&#24067;&#32593;&#32476;&#36127;&#36733;&#65292;&#36991;&#20813;&#30001;&#21333;&#20010;&#24320;&#25918;&#24335;&#20998;&#24067;&#24335;&#21333;&#20803;(O-DU)&#31649;&#36758;&#30340;&#24320;&#25918;&#24335;&#26080;&#32447;&#21333;&#20803;(O-RU)&#19978;&#36807;&#24230;&#32858;&#38598;&#30340;&#36127;&#36733;&#23548;&#33268;&#32593;&#32476;&#25317;&#22622;&#21644;&#29992;&#25143;&#25925;&#38556;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#26041;&#27861;&#26469;&#23454;&#29616;&#36127;&#36733;&#22343;&#34913;&#21644;&#36164;&#28304;&#20998;&#37197;(mmLBRA)&#26041;&#26696;&#65292;&#26088;&#22312;&#23454;&#29616;&#36127;&#36733;&#22343;&#34913;&#21644;&#25552;&#39640;O-RAN&#32593;&#32476;&#30340;&#26377;&#25928;&#24635;&#21644;&#36895;&#29575;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The open radio access network (O-RAN) architecture offers a cost-effective and scalable solution for internet service providers to optimize their networks using machine learning algorithms. The architecture's open interfaces enable network function virtualization, with the O-RAN serving as the primary communication device for users. However, the limited frequency resources and information explosion make it difficult to achieve an optimal network experience without effective traffic control or resource allocation. To address this, we consider mobility-aware load balancing to evenly distribute loads across the network, preventing network congestion and user outages caused by excessive load concentration on open radio unit (O-RU) governed by a single open distributed unit (O-DU). We have proposed a multi-agent multi-armed bandit for load balancing and resource allocation (mmLBRA) scheme, designed to both achieve load balancing and improve the effective sum-rate performance of the O-RAN ne
&lt;/p&gt;</description></item><item><title>DiracDiffusion&#26159;&#19968;&#31181;&#26032;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22270;&#20687;&#21435;&#22122;&#21644;&#22686;&#37327;&#37325;&#24314;&#65292;&#24182;&#20445;&#35777;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14353</link><description>&lt;p&gt;
DiracDiffusion: &#30830;&#20445;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21435;&#22122;&#21644;&#22686;&#37327;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency. (arXiv:2303.14353v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14353
&lt;/p&gt;
&lt;p&gt;
DiracDiffusion&#26159;&#19968;&#31181;&#26032;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22270;&#20687;&#21435;&#22122;&#21644;&#22686;&#37327;&#37325;&#24314;&#65292;&#24182;&#20445;&#35777;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#65288;&#21253;&#25324;&#22270;&#20687;&#24674;&#22797;&#65289;&#24050;&#32463;&#24314;&#31435;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#22522;&#20110;&#25193;&#25955;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#20174;&#20005;&#37325;&#25439;&#22351;&#30340;&#27979;&#37327;&#25968;&#25454;&#20013;&#29983;&#25104;&#20986;&#20855;&#26377;&#20986;&#33394;&#35270;&#35273;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#25152;&#35859;&#30340;&#24863;&#30693;-&#22833;&#30495;&#26435;&#34913;&#20013;&#65292;&#24863;&#30693;&#25928;&#26524;&#20248;&#31168;&#30340;&#37325;&#24314;&#32467;&#26524;&#36890;&#24120;&#26159;&#20197;&#36864;&#21270;&#30340;&#22833;&#30495;&#24230;&#37327;&#65288;&#22914;PSNR&#65289;&#20026;&#20195;&#20215;&#30340;&#12290;&#22833;&#30495;&#24230;&#37327;&#34913;&#37327;&#23545;&#35266;&#23519;&#30340;&#24544;&#23454;&#24230;&#65292;&#36825;&#22312;&#36870;&#38382;&#39064;&#20013;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#35201;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#21363;&#25105;&#20204;&#20551;&#35774;&#35266;&#23519;&#20540;&#26469;&#33258;&#19968;&#20010;&#38543;&#26426;&#21155;&#21270;&#36807;&#31243;&#65292;&#36880;&#28176;&#38477;&#20302;&#21644;&#22122;&#22768;&#21270;&#21407;&#22987;&#24178;&#20928;&#22270;&#20687;&#65292;&#28982;&#21518;&#23398;&#20064;&#36870;&#36716;&#21155;&#21270;&#36807;&#31243;&#20197;&#24674;&#22797;&#24178;&#20928;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#25972;&#20010;&#36870;&#36716;&#36807;&#31243;&#20013;&#20445;&#25345;&#19982;&#21407;&#22987;&#27979;&#37327;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20801;&#35768;&#22312;&#24863;&#30693;&#36136;&#37327;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#20043;&#38388;&#21462;&#24471;&#24040;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;DiracDiffusion&#65292;&#22240;&#20026;&#23427;&#22522;&#20110;&#30001;Dirac&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#30340;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#21435;&#22122;&#21644;&#22686;&#37327;&#37325;&#24314;&#22312;&#20869;&#30340;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have established new state of the art in a multitude of computer vision tasks, including image restoration. Diffusion-based inverse problem solvers generate reconstructions of exceptional visual quality from heavily corrupted measurements. However, in what is widely known as the perception-distortion trade-off, the price of perceptually appealing reconstructions is often paid in declined distortion metrics, such as PSNR. Distortion metrics measure faithfulness to the observation, a crucial requirement in inverse problems. In this work, we propose a novel framework for inverse problem solving, namely we assume that the observation comes from a stochastic degradation process that gradually degrades and noises the original clean image. We learn to reverse the degradation process in order to recover the clean image. Our technique maintains consistency with the original measurement throughout the reverse process, and allows for great flexibility in trading off perceptual qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#24335;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#36164;&#28304;&#20998;&#37197;&#65288;mmRAL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#36866;&#24403;&#20998;&#37197;LEO&#21355;&#26143;&#26143;&#24231;&#30340;&#21487;&#29992;&#26080;&#32447;&#30005;&#36164;&#28304;&#65292;&#20197;&#35299;&#20915;&#36164;&#28304;&#31649;&#29702;&#21450;&#26080;&#20449;&#36947;&#20449;&#24687;&#25910;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.14351</link><description>&lt;p&gt;
&#23618;&#27425;&#21270;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#22312;&#22810;LEO&#21355;&#26143;&#26143;&#24231;&#32593;&#32476;&#20013;&#29992;&#20110;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multi-Agent Multi-Armed Bandit for Resource Allocation in Multi-LEO Satellite Constellation Networks. (arXiv:2303.14351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#24335;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#36164;&#28304;&#20998;&#37197;&#65288;mmRAL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#36866;&#24403;&#20998;&#37197;LEO&#21355;&#26143;&#26143;&#24231;&#30340;&#21487;&#29992;&#26080;&#32447;&#30005;&#36164;&#28304;&#65292;&#20197;&#35299;&#20915;&#36164;&#28304;&#31649;&#29702;&#21450;&#26080;&#20449;&#36947;&#20449;&#24687;&#25910;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36712;&#21355;&#26143;&#26143;&#24231;&#21487;&#20197;&#22312;&#19979;&#19968;&#20195;&#38750;&#22320;&#38754;&#32593;&#32476;&#65288;NTN&#65289;&#20013;&#25552;&#20379;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#39640;&#36895;&#26381;&#21153;&#21306;&#22495;&#12290;&#30001;&#20110;&#25805;&#20316;&#21151;&#29575;&#12289;&#27874;&#26463;&#21644;&#36890;&#36947;&#30340;&#26377;&#38480;&#26495;&#36733;&#36164;&#28304;&#65292;&#22240;&#27492;&#22312;&#22797;&#26434;&#30340;&#24178;&#25200;&#24773;&#20917;&#19979;&#65292;&#24377;&#24615;&#21644;&#39640;&#25928;&#30340;&#36164;&#28304;&#31649;&#29702;&#21464;&#24471;&#38750;&#24120;&#24517;&#35201;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;&#22320;&#38754;&#22522;&#31449;&#19981;&#21516;&#65292;LEO&#22312;&#30456;&#24403;&#39640;&#30340;&#39640;&#24230;&#21644;&#39640;&#36816;&#21160;&#29366;&#24577;&#19979;&#37096;&#32626;&#65292;&#20174;&#32780;&#22312;&#20256;&#36755;&#36807;&#31243;&#20013;&#20135;&#29983;&#22823;&#37327;&#24310;&#36831;&#21644;&#24178;&#25200;&#12290;&#22240;&#27492;&#65292;&#22312;&#19981;&#30693;&#36947;&#20449;&#36947;&#20449;&#24687;&#19988;&#19981;&#25910;&#38598;&#38271;&#26102;&#24310;&#22320;&#38754;&#32593;&#20851;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;&#21521;&#20256;&#36755;&#30340;&#26694;&#26550;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24403;&#20998;&#37197;&#21487;&#29992;&#26080;&#32447;&#30005;&#36164;&#28304;&#30340;&#20998;&#23618;&#24335;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#36164;&#28304;&#20998;&#37197;&#65288;mmRAL&#65289;&#29992;&#20110;LEO&#21355;&#26143;&#26143;&#24231;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low Earth orbit (LEO) satellite constellation is capable of providing global coverage area with high-rate services in the next sixth-generation (6G) non-terrestrial network (NTN). Due to limited onboard resources of operating power, beams, and channels, resilient and efficient resource management has become compellingly imperative under complex interference cases. However, different from conventional terrestrial base stations, LEO is deployed at considerable height and under high mobility, inducing substantially long delay and interference during transmission. As a result, acquiring the accurate channel state information between LEOs and ground users is challenging. Therefore, we construct a framework with a two-way transmission under unknown channel information and no data collected at long-delay ground gateway. In this paper, we propose hierarchical multi-agent multi-armed bandit resource allocation for LEO constellation (mmRAL) by appropriately assigning available radio resources. L
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22312;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#21464;&#37327;&#12289;&#20020;&#24202;&#25351;&#25968;&#21644;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24739;&#32773;&#30340;&#33041;MR&#22270;&#20687;&#20043;&#38388;&#30340;&#21453;&#20107;&#23454;&#33041;MR&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.14349</link><description>&lt;p&gt;
3D&#33041;MR&#30340;&#22240;&#26524;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Causal Image Synthesis of Brain MR in 3D. (arXiv:2303.14349v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22312;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#21464;&#37327;&#12289;&#20020;&#24202;&#25351;&#25968;&#21644;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24739;&#32773;&#30340;&#33041;MR&#22270;&#20687;&#20043;&#38388;&#30340;&#21453;&#20107;&#23454;&#33041;MR&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#38656;&#22522;&#20110;&#30495;&#23454;&#21307;&#23398;&#24433;&#20687;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#29702;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#22240;&#26524;&#22270;&#20687;&#21512;&#25104;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#20154;&#21475;&#32479;&#35745;&#21464;&#37327;&#12289;&#20020;&#24202;&#25351;&#25968;&#21644;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24739;&#32773;&#30340;&#33041;MR&#24433;&#20687;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#26469;&#25551;&#36848;&#22240;&#26524;&#20851;&#31995;&#65292;&#21033;&#29992;&#39118;&#26684;&#29983;&#25104;&#22120;&#26469;&#21512;&#25104;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#24314;&#27169;&#22797;&#26434;&#24615;&#24182;&#20351;&#23398;&#20064;&#21487;&#34892;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20302;&#32500;&#28508;&#22312;&#29305;&#24449;&#34920;&#31034;&#39640;&#32500;3D&#22270;&#20687;&#65292;&#32467;&#21512;&#22806;&#29983;&#22122;&#22768;&#65292;&#20197;&#24314;&#31435;&#22270;&#20687;&#21644;&#38750;&#22270;&#20687;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22522;&#20110;1586&#20010;&#21463;&#35797;&#32773;&#21644;3683&#20010;3D&#22270;&#20687;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#21512;&#25104;&#20102;&#22312;&#26576;&#20123;&#23646;&#24615;&#24178;&#39044;&#19979;&#30340;&#21453;&#20107;&#23454;&#33041;MR&#22270;&#20687;&#65292;&#22914;&#24180;&#40836;&#12289;&#33041;&#23481;&#31215;&#21644;&#35748;&#30693;&#27979;&#35797;&#24471;&#20998;&#31561;&#12290;&#23450;&#37327;&#25351;&#26631;&#21644;&#23545;&#21453;&#20107;&#23454;&#22270;&#20687;&#30340;&#23450;&#24615;&#35780;&#20272;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical decision making requires counterfactual reasoning based on a factual medical image and thus necessitates causal image synthesis. To this end, we present a novel method for modeling the causality between demographic variables, clinical indices and brain MR images for Alzheimer's Diseases. Specifically, we leverage a structural causal model to depict the causality and a styled generator to synthesize the image. Furthermore, as a crucial step to reduce modeling complexity and make learning tractable, we propose the use of low dimensional latent feature representation of a high-dimensional 3D image, together with exogenous noise, to build causal relationship between the image and non image variables. We experiment the proposed method based on 1586 subjects and 3683 3D images and synthesize counterfactual brain MR images intervened on certain attributes, such as age, brain volume and cognitive test score. Quantitative metrics and qualitative evaluation of counterfactual images demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21733;&#24503;&#23572;&#19981;&#23436;&#22791;&#23450;&#29702;&#21040;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#23436;&#22791;&#24615;&#30340;&#36923;&#36753;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#20219;&#20309;&#20449;&#20208;&#31995;&#32479;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#19988;&#19981;&#23436;&#22791;&#23450;&#29702;&#24847;&#21619;&#30528;&#23384;&#22312;&#30495;&#23454;&#20294;&#26080;&#27861;&#35777;&#26126;&#30340;&#38472;&#36848;&#65292;&#21487;&#20197;&#29992;&#26469;&#23450;&#20041;&#20986;&#19982;&#29616;&#26377;&#20449;&#20208;&#21644;&#20256;&#32479;&#19968;&#33268;&#30340;&#26032;&#23447;&#25945;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2303.14338</link><description>&lt;p&gt;
&#20174;&#21733;&#24503;&#23572;&#19981;&#23436;&#22791;&#23450;&#29702;&#21040;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#23436;&#22791;&#24615;&#65288;&#25193;&#23637;&#25688;&#35201;&#65289;
&lt;/p&gt;
&lt;p&gt;
From G\"odel's Incompleteness Theorem to the completeness of bot religions (Extended abstract). (arXiv:2303.14338v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21733;&#24503;&#23572;&#19981;&#23436;&#22791;&#23450;&#29702;&#21040;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#23436;&#22791;&#24615;&#30340;&#36923;&#36753;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#20219;&#20309;&#20449;&#20208;&#31995;&#32479;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#19988;&#19981;&#23436;&#22791;&#23450;&#29702;&#24847;&#21619;&#30528;&#23384;&#22312;&#30495;&#23454;&#20294;&#26080;&#27861;&#35777;&#26126;&#30340;&#38472;&#36848;&#65292;&#21487;&#20197;&#29992;&#26469;&#23450;&#20041;&#20986;&#19982;&#29616;&#26377;&#20449;&#20208;&#21644;&#20256;&#32479;&#19968;&#33268;&#30340;&#26032;&#23447;&#25945;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hilbert &#21644; Ackermann &#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19981;&#23436;&#22791;&#29702;&#35770;&#19968;&#33268;&#22320;&#25193;&#23637;&#21040;&#23436;&#22791;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#21733;&#24503;&#23572;&#22522;&#26412;&#19978;&#35777;&#26126;&#20102;&#20219;&#20309;&#33021;&#22815;&#23545;&#20854;&#33258;&#36523;&#38472;&#36848;&#21450;&#20854;&#35777;&#26126;&#36827;&#34892;&#32534;&#30721;&#30340;&#29702;&#35770;&#37117;&#21253;&#21547;&#20102;&#30495;&#23454;&#20294;&#19981;&#33021;&#34987;&#35777;&#26126;&#30340;&#38472;&#36848;&#12290;&#21733;&#24503;&#23572;&#30340;&#26500;&#36896;&#24182;&#27809;&#26377;&#22238;&#31572;&#24076;&#23572;&#20271;&#29305;&#30340;&#38382;&#39064;&#65292;&#24076;&#23572;&#20271;&#29305;&#35748;&#20026;&#29702;&#35770;&#21487;&#20197;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#20844;&#29702;&#26469;&#35777;&#26126;&#36234;&#26469;&#36234;&#22810;&#30340;&#30495;&#23454;&#38472;&#36848;&#65292;&#23601;&#20687;&#31185;&#23398;&#19968;&#26679;&#65292;&#23436;&#22791;&#24615;&#26159;&#28040;&#22833;&#28857;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24213;&#23618;&#30340;&#36923;&#36753;&#36807;&#31243;&#65292;&#24182;&#25551;&#36848;&#20102;&#23548;&#33268;&#21487;&#27979;&#35797;&#20294;&#19981;&#21487;&#34892;&#30340;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#36712;&#36857;&#65292;&#36825;&#20123;&#23447;&#25945;&#25193;&#23637;&#20102;&#20256;&#32479;&#23447;&#25945;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20202;&#24335;&#21644;&#20449;&#20208;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#20219;&#20309;&#20449;&#20208;&#31995;&#32479;&#37117;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#36923;&#36753;&#29702;&#35770;&#30340;&#24819;&#27861;&#65292;&#24182;&#19988;&#19981;&#23436;&#22791;&#23450;&#29702;&#24847;&#21619;&#30528;&#23384;&#22312;&#30495;&#23454;&#20294;&#26080;&#27861;&#35777;&#26126;&#30340;&#38472;&#36848;&#65292;&#21487;&#20197;&#24182;&#20837;&#36825;&#20010;&#29702;&#35770;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#26679;&#30340;&#20363;&#23376;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#20204;&#26469;&#23450;&#20041;&#19982;&#29616;&#26377;&#20449;&#20208;&#21644;&#20256;&#32479;&#19968;&#33268;&#30340;&#26032;&#23447;&#25945;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hilbert and Ackermann asked for a method to consistently extend incomplete theories to complete theories. G\"odel essentially proved that any theory capable of encoding its own statements and their proofs contains statements that are true but not provable. Hilbert did not accept that G\"odel's construction answered his question, and in his late writings and lectures, G\"odel agreed that it did not, since theories can be completed incrementally, by adding axioms to prove ever more true statements, as science normally does, with completeness as the vanishing point. This pragmatic view of validity is familiar not only to scientists who conjecture test hypotheses but also to real estate agents and other dealers, who conjure claims, albeit invalid, as necessary to close a deal, confident that they will be able to conjure other claims, albeit invalid, sufficient to make the first claims valid. We study the underlying logical process and describe the trajectories leading to testable but unfal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#23494;&#38598;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#38598;&#25104;&#65292;&#24182;&#27880;&#24847;&#21040;&#21333;&#20010;&#27169;&#22411;&#30340;&#26435;&#37325;&#35268;&#33539;&#21270;&#21644;&#38598;&#25104;&#26435;&#37325;&#30340;&#35843;&#25972;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25915;&#20987;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#20998;&#21106;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.14304</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#25104;&#30340;&#40657;&#30418;&#25915;&#20987;&#23494;&#38598;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Ensemble-based Blackbox Attacks on Dense Prediction. (arXiv:2303.14304v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#23494;&#38598;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#38598;&#25104;&#65292;&#24182;&#27880;&#24847;&#21040;&#21333;&#20010;&#27169;&#22411;&#30340;&#26435;&#37325;&#35268;&#33539;&#21270;&#21644;&#38598;&#25104;&#26435;&#37325;&#30340;&#35843;&#25972;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25915;&#20987;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#20998;&#21106;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23494;&#38598;&#39044;&#27979;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;(&#20363;&#22914;&#30446;&#26631;&#26816;&#27979;&#21644;&#20998;&#21106;)&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#21333;&#20010;&#26367;&#20195;&#27169;&#22411;&#20135;&#29983;&#30340;&#25915;&#20987;&#19981;&#36866;&#29992;&#20110;&#20219;&#24847;(&#40657;&#30418;)&#21463;&#23475;&#32773;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26377;&#38024;&#23545;&#24615;&#30340;&#25915;&#20987;&#36890;&#24120;&#27604;&#26080;&#38024;&#23545;&#24615;&#30340;&#25915;&#20987;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#38598;&#25104;&#21487;&#20197;&#20026;&#22810;&#20010;&#21463;&#23475;&#32773;&#27169;&#22411;&#21019;&#24314;&#26377;&#25928;&#30340;&#25915;&#20987;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#20110;&#21333;&#20010;&#27169;&#22411;&#30340;&#26435;&#37325;&#30340;&#35268;&#33539;&#21270;&#23545;&#20110;&#25915;&#20987;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26681;&#25454;&#21463;&#23475;&#32773;&#27169;&#22411;&#35843;&#25972;&#38598;&#25104;&#26435;&#37325;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25915;&#20987;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35768;&#22810;&#30446;&#26631;&#26816;&#27979;&#21644;&#20998;&#21106;&#23454;&#39564;&#65292;&#20197;&#31361;&#26174;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#38598;&#25104;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#21644;&#20998;&#21106;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#35843;&#25972;&#26041;&#27861;&#27979;&#24471;&#30340;&#24615;&#33021;&#20248;&#20110;&#35843;&#25972;&#21069;&#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach for adversarial attacks on dense prediction models (such as object detectors and segmentation). It is well known that the attacks generated by a single surrogate model do not transfer to arbitrary (blackbox) victim models. Furthermore, targeted attacks are often more challenging than the untargeted attacks. In this paper, we show that a carefully designed ensemble can create effective attacks for a number of victim models. In particular, we show that normalization of the weights for individual models plays a critical role in the success of the attacks. We then demonstrate that by adjusting the weights of the ensemble according to the victim model can further improve the performance of the attacks. We performed a number of experiments for object detectors and segmentation to highlight the significance of the our proposed methods. Our proposed ensemble-based method outperforms existing blackbox attack methods for object detection and segmentation. Finally we show t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#33021;&#26368;&#20339;&#30340;Concrete Autoencoder&#26041;&#27861;&#65292;&#25104;&#21151;&#35782;&#21035;&#20986;49,075&#20363;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#24739;&#32773;&#25968;&#25454;&#24211;&#20013;&#30340;&#26368;&#20339;100&#20010;&#29305;&#24449;&#65292;&#24182;&#35777;&#23454;Concrete Autoencoder&#26041;&#27861;&#20013;&#30340;&#26435;&#37325;&#35843;&#25972;&#33021;&#22815;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14303</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#35782;&#21035;ICD-10&#32534;&#30721;&#65292;&#20197;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#65306;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#24739;&#32773;&#38431;&#21015;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Feature Selection to Identify Important ICD-10 Codes for Machine Learning: A Case Study on a Coronary Artery Disease Patient Cohort. (arXiv:2303.14303v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#33021;&#26368;&#20339;&#30340;Concrete Autoencoder&#26041;&#27861;&#65292;&#25104;&#21151;&#35782;&#21035;&#20986;49,075&#20363;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#24739;&#32773;&#25968;&#25454;&#24211;&#20013;&#30340;&#26368;&#20339;100&#20010;&#29305;&#24449;&#65292;&#24182;&#35777;&#23454;Concrete Autoencoder&#26041;&#27861;&#20013;&#30340;&#26435;&#37325;&#35843;&#25972;&#33021;&#22815;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#20351;&#29992;&#30340;&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#65288;ICD&#65289;&#20195;&#30721;&#22240;&#20195;&#30721;&#25968;&#37327;&#36807;&#22810;&#32780;&#22312;&#36873;&#25321;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30456;&#20851;&#20195;&#30721;&#20316;&#20026;&#29305;&#24449;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;49,075&#20363;&#21152;&#25343;&#22823;&#38463;&#23572;&#20271;&#22612;&#30465;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#24739;&#32773;&#30340;ICD&#32534;&#30721;&#25968;&#25454;&#24211;&#30340;&#20960;&#31181;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#25289;&#26222;&#25289;&#26031;&#20998;&#25968;&#12289;&#22810;&#38598;&#32676;&#25968;&#25454;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#12289;&#33258;&#32534;&#30721;&#22120;&#21551;&#21457;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#12289;&#20027;&#35201;&#29305;&#24449;&#20998;&#26512;&#20197;&#21450;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;ICD&#26641;&#26435;&#37325;&#35843;&#25972;&#30340;Concrete Autoencoders&#26469;&#36873;&#25321;&#36229;&#36807;9,000&#20010;&#20195;&#30721;&#20013;&#30340;100&#20010;&#26368;&#20339;&#29305;&#24449;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36873;&#25321;&#30340;&#29305;&#24449;&#33021;&#21147;&#65292;&#22522;&#20110;&#20854;&#37325;&#24314;&#21021;&#22987;&#29305;&#24449;&#31354;&#38388;&#21644;&#39044;&#27979;&#20986;&#38498;&#21518;90&#22825;&#30340;&#27515;&#20129;&#29575;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20004;&#39033;&#20219;&#21153;&#20013;&#65292;Concrete Autoencoder&#26041;&#27861;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;Concrete Autoencoder&#26041;&#27861;&#20013;&#30340;&#26435;&#37325;&#35843;&#25972;&#32463;&#35777;&#23454;&#25552;&#39640;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of International Classification of Diseases (ICD) codes in healthcare presents a challenge in selecting relevant codes as features for machine learning models due to this system's large number of codes. In this study, we compared several unsupervised feature selection methods for an ICD code database of 49,075 coronary artery disease patients in Alberta, Canada. Specifically, we employed Laplacian Score, Unsupervised Feature Selection for Multi-Cluster Data, Autoencoder Inspired Unsupervised Feature Selection, Principal Feature Analysis, and Concrete Autoencoders with and without ICD tree weight adjustment to select the 100 best features from over 9,000 codes. We assessed the selected features based on their ability to reconstruct the initial feature space and predict 90-day mortality following discharge. Our findings revealed that the Concrete Autoencoder methods outperformed all other methods in both tasks. Furthermore, the weight adjustment in the Concrete Autoencoder method
&lt;/p&gt;</description></item><item><title>repliclust &#26159;&#19968;&#20010; Python &#21253;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#32858;&#31867;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#21407;&#22411;&#65292;&#25552;&#20379;&#20102;&#25918;&#32622;&#38598;&#32676;&#20013;&#24515;&#12289;&#37319;&#26679;&#38598;&#32676;&#24418;&#29366;&#12289;&#36873;&#25321;&#27599;&#20010;&#38598;&#32676;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#20197;&#21450;&#20026;&#38598;&#32676;&#20998;&#37197;&#27010;&#29575;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14301</link><description>&lt;p&gt;
repliclust&#65306;&#32858;&#31867;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
repliclust: Synthetic Data for Cluster Analysis. (arXiv:2303.14301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14301
&lt;/p&gt;
&lt;p&gt;
repliclust &#26159;&#19968;&#20010; Python &#21253;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#32858;&#31867;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#21407;&#22411;&#65292;&#25552;&#20379;&#20102;&#25918;&#32622;&#38598;&#32676;&#20013;&#24515;&#12289;&#37319;&#26679;&#38598;&#32676;&#24418;&#29366;&#12289;&#36873;&#25321;&#27599;&#20010;&#38598;&#32676;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#20197;&#21450;&#20026;&#38598;&#32676;&#20998;&#37197;&#27010;&#29575;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; repliclust&#65288;&#26469;&#33258;&#20110; repli-cate &#21644; clust-er&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#32858;&#31867;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340; Python &#21253;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#21407;&#22411;&#65292;&#21363;&#39640;&#32423;&#20960;&#20309;&#25551;&#36848;&#65292;&#29992;&#25143;&#21487;&#20197;&#20174;&#20013;&#21019;&#24314;&#35768;&#22810;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20855;&#26377;&#25152;&#38656;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#25105;&#20204;&#36719;&#20214;&#30340;&#26550;&#26500;&#26159;&#27169;&#22359;&#21270;&#21644;&#38754;&#21521;&#23545;&#35937;&#30340;&#65292;&#23558;&#25968;&#25454;&#29983;&#25104;&#20998;&#35299;&#25104;&#25918;&#32622;&#38598;&#32676;&#20013;&#24515;&#30340;&#31639;&#27861;&#12289;&#37319;&#26679;&#38598;&#32676;&#24418;&#29366;&#30340;&#31639;&#27861;&#12289;&#36873;&#25321;&#27599;&#20010;&#38598;&#32676;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#31639;&#27861;&#20197;&#21450;&#20026;&#38598;&#32676;&#20998;&#37197;&#27010;&#29575;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;repliclust.org &#39033;&#30446;&#32593;&#39029;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#29992;&#25143;&#25351;&#21335;&#21644;&#20840;&#38754;&#30340;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present repliclust (from repli-cate and clust-er), a Python package for generating synthetic data sets with clusters. Our approach is based on data set archetypes, high-level geometric descriptions from which the user can create many different data sets, each possessing the desired geometric characteristics. The architecture of our software is modular and object-oriented, decomposing data generation into algorithms for placing cluster centers, sampling cluster shapes, selecting the number of data points for each cluster, and assigning probability distributions to clusters. The project webpage, repliclust.org, provides a concise user guide and thorough documentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20840;&#23616;&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#39044;&#23450;&#30340;&#26597;&#35810;&#21019;&#24314;&#35268;&#21017;&#23454;&#29616;&#20102;&#23545;H&#246;lder&#36830;&#32493;&#22810;&#20803;&#20989;&#25968;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#21487;&#22312;&#32473;&#23450;&#26102;&#38388;&#27573;&#20869;&#33719;&#24471; minimax &#26368;&#20248;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.14293</link><description>&lt;p&gt;
&#38024;&#23545;H&#246;lder&#36830;&#32493;&#22810;&#20803;&#20989;&#25968;&#30340;&#39640;&#25928;Lipschitzian&#20840;&#23616;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Lipschitzian Global Optimization of H\"older Continuous Multivariate Functions. (arXiv:2303.14293v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20840;&#23616;&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#39044;&#23450;&#30340;&#26597;&#35810;&#21019;&#24314;&#35268;&#21017;&#23454;&#29616;&#20102;&#23545;H&#246;lder&#36830;&#32493;&#22810;&#20803;&#20989;&#25968;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#21487;&#22312;&#32473;&#23450;&#26102;&#38388;&#27573;&#20869;&#33719;&#24471; minimax &#26368;&#20248;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20840;&#23616;&#20248;&#21270;&#25216;&#26415;&#65292;&#19987;&#38376;&#38024;&#23545;H&#246;lder&#36830;&#32493;&#30340;&#22810;&#20803;&#20989;&#25968;&#12290;&#19982;&#26500;&#36896;&#19979;&#30028;&#20195;&#29702;&#20989;&#25968;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20010;&#31639;&#27861;&#37319;&#29992;&#20102;&#39044;&#23450;&#30340;&#26597;&#35810;&#21019;&#24314;&#35268;&#21017;&#65292;&#20351;&#20854;&#22312;&#35745;&#31639;&#19978;&#26356;&#20855;&#20248;&#21183;&#12290;&#31639;&#27861;&#30340;&#24615;&#33021;&#20351;&#29992;&#24179;&#22343;&#25110;&#32047;&#31215;&#36951;&#25022;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20063;&#24847;&#21619;&#30528;&#31616;&#21333;&#36951;&#25022;&#30340;&#30028;&#38480;&#65292;&#21453;&#26144;&#20102;&#35813;&#26041;&#27861;&#30340;&#25972;&#20307;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36866;&#24403;&#30340;&#21442;&#25968;&#65292;&#31639;&#27861;&#22312;&#32473;&#23450;&#26102;&#38388;&#27573;$T$&#20869;&#38024;&#23545;H&#246;lder&#36830;&#32493;&#30340;H&#246;lder&#25351;&#25968;&#20026;$\alpha$&#30340;&#30446;&#26631;&#20989;&#25968;&#22312;$n$&#32500;&#31354;&#38388;&#20013;&#33719;&#24471;&#20102;$O(T^{-\frac{\alpha}{n}})$&#30340;&#24179;&#22343;&#36951;&#25022;&#30028;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19968;&#30028;&#38480;&#26159;&#26497;&#23567;&#21270;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an effective global optimization technique designed for multivariate functions that are H\"older continuous. Unlike traditional methods that construct lower bounding proxy functions, this algorithm employs a predetermined query creation rule that makes it computationally superior. The algorithm's performance is assessed using the average or cumulative regret, which also implies a bound for the simple regret and reflects the overall effectiveness of the approach. The results show that with appropriate parameters the algorithm attains an average regret bound of $O(T^{-\frac{\alpha}{n}})$ for optimizing a H\"older continuous target function with H\"older exponent $\alpha$ in an $n$-dimensional space within a given time horizon $T$. We demonstrate that this bound is minimax optimal.
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#36807;&#31243;&#26159;&#19968;&#31181;&#36866;&#21512;&#25311;&#21512;&#23569;&#37327;&#25968;&#25454;&#19988;&#20805;&#20998;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20174;&#20998;&#23376;&#21040;&#40657;&#27934;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#39044;&#27979;&#21644;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2303.14291</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22312;&#26497;&#31471;&#38271;&#24230;&#23610;&#24230;&#19978;&#30340;&#24212;&#29992;&#65306;&#20174;&#20998;&#23376;&#21040;&#40657;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications of Gaussian Processes at Extreme Lengthscales: From Molecules to Black Holes. (arXiv:2303.14291v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14291
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#26159;&#19968;&#31181;&#36866;&#21512;&#25311;&#21512;&#23569;&#37327;&#25968;&#25454;&#19988;&#20805;&#20998;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20174;&#20998;&#23376;&#21040;&#40657;&#27934;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#39044;&#27979;&#21644;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#35266;&#27979;&#21644;&#23454;&#39564;&#31185;&#23398;&#39046;&#22495;&#65292;&#25968;&#25454;&#38750;&#24120;&#31232;&#32570;&#12290;&#22312;&#39640;&#33021;&#22825;&#20307;&#29289;&#29702;&#23398;&#20013;&#65292;&#21463;&#21040;&#22825;&#20307;&#36974;&#25377;&#21644;&#26377;&#38480;&#30340;&#26395;&#36828;&#38236;&#26102;&#38388;&#30340;&#24433;&#21709;&#65292;&#25968;&#25454;&#35266;&#27979;&#21463;&#21040;&#24178;&#25200;&#12290;&#32780;&#22312;&#21512;&#25104;&#21270;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#30340;&#23454;&#39564;&#23460;&#23454;&#39564;&#20013;&#24471;&#20986;&#30340;&#25968;&#25454;&#65292;&#32791;&#26102;&#21644;&#25104;&#26412;&#37117;&#38750;&#24120;&#39640;&#26114;&#12290;&#28982;&#32780;&#65292;&#22312;&#31185;&#23398;&#39046;&#22495;&#20013;&#36890;&#24120;&#21487;&#20197;&#33719;&#24471;&#26377;&#20851;&#25968;&#25454;&#20135;&#29983;&#26426;&#21046;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;&#23454;&#39564;&#35013;&#32622;&#30340;&#27979;&#37327;&#35823;&#24046;&#31561;&#12290;&#36825;&#20004;&#20010;&#29305;&#24449;&#65292;&#21363;&#25968;&#25454;&#37327;&#23567;&#21644;&#23545;&#22522;&#26412;&#29289;&#29702;&#21407;&#29702;&#30340;&#20102;&#35299;&#65292;&#20351;&#24471;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#25104;&#20026;&#36866;&#21512;&#25311;&#21512;&#27492;&#31867;&#25968;&#25454;&#38598;&#30340;&#29702;&#24819;&#20505;&#36873;&#12290;GPs &#33021;&#22815;&#32771;&#34385;&#21040;&#19981;&#30830;&#23450;&#24615;&#65292;&#20363;&#22914;&#22312;&#20998;&#23376;&#21644;&#26448;&#26009;&#30340;&#34394;&#25311;&#31579;&#36873;&#20013;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#23545;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#36827;&#34892;&#25512;&#26029;&#65292;&#20363;&#22914;&#20174;&#40657;&#27934;&#21560;&#31215;&#30424;&#30340;&#28508;&#22312;&#21457;&#23556;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;GPs&#30446;&#21069;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24037;&#20316;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#39044;&#35745;&#23558;&#25104;&#20026;&#24341;&#23548;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many areas of the observational and experimental sciences data is scarce. Data observation in high-energy astrophysics is disrupted by celestial occlusions and limited telescope time while data derived from laboratory experiments in synthetic chemistry and materials science is time and cost-intensive to collect. On the other hand, knowledge about the data-generation mechanism is often available in the sciences, such as the measurement error of a piece of laboratory apparatus. Both characteristics, small data and knowledge of the underlying physics, make Gaussian processes (GPs) ideal candidates for fitting such datasets. GPs can make predictions with consideration of uncertainty, for example in the virtual screening of molecules and materials, and can also make inferences about incomplete data such as the latent emission signature from a black hole accretion disc. Furthermore, GPs are currently the workhorse model for Bayesian optimisation, a methodology foreseen to be a guide for l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33609;&#22270;&#30340;&#36923;&#36753;&#22238;&#24402;coreset&#26500;&#24314;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#38477;&#32500;&#30340;&#26032;&#30028;&#38480;&#65292;&#24182;&#35299;&#20915;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#21040;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21069;&#21521;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2303.14284</link><description>&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#29305;&#24449;&#31354;&#38388;&#33609;&#22270;
&lt;/p&gt;
&lt;p&gt;
Feature Space Sketching for Logistic Regression. (arXiv:2303.14284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33609;&#22270;&#30340;&#36923;&#36753;&#22238;&#24402;coreset&#26500;&#24314;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#38477;&#32500;&#30340;&#26032;&#30028;&#38480;&#65292;&#24182;&#35299;&#20915;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#21040;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21069;&#21521;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#36923;&#36753;&#22238;&#24402;&#30340;coreset&#26500;&#24314;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#38477;&#32500;&#30340;&#26032;&#30028;&#38480;&#12290;&#36825;&#19977;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#35270;&#20026;&#36923;&#36753;&#22238;&#24402;&#36755;&#20837;&#30340;&#33609;&#22270;&#12290;&#22312;coreset&#26500;&#24314;&#26041;&#38754;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;coreset&#26500;&#36896;&#26041;&#27861;&#30340;&#22797;&#26434;&#24230;&#26032;&#30028;&#38480;&#12290;&#22312;&#29305;&#24449;&#36873;&#25321;&#21644;&#38477;&#32500;&#26041;&#38754;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#36923;&#36753;&#22238;&#24402;&#30340;&#21069;&#21521;&#35823;&#24046;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#21487;&#20197;&#25910;&#32039;&#65292;&#30452;&#21040;&#30830;&#23450;&#30340;&#22240;&#32032;&#20026;&#27490;&#65292;&#24182;&#19988;&#21069;&#21521;&#35823;&#24046;&#30028;&#38480;&#21487;&#20197;&#25193;&#23637;&#21040;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present novel bounds for coreset construction, feature selection, and dimensionality reduction for logistic regression. All three approaches can be thought of as sketching the logistic regression inputs. On the coreset construction front, we resolve open problems from prior work and present novel bounds for the complexity of coreset construction methods. On the feature selection and dimensionality reduction front, we initiate the study of forward error bounds for logistic regression. Our bounds are tight up to constant factors and our forward error bounds can be extended to Generalized Linear Models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015; Knockoffs (SEEK)&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#23454;&#29616;&#21464;&#37327;&#36873;&#25321;&#65292;&#35813;&#31639;&#27861;&#20272;&#35745;&#20102;&#26368;&#23567;&#20805;&#20998;&#29366;&#24577;&#65292;&#30830;&#20445;&#23398;&#20064;&#36827;&#31243;&#33391;&#22909;&#32780;&#19981;&#20250;&#20943;&#32531;&#12290;</title><link>http://arxiv.org/abs/2303.14281</link><description>&lt;p&gt;
&#22522;&#20110;&#24207;&#21015; Knockoffs &#30340;&#24378;&#21270;&#23398;&#20064;&#21464;&#37327;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Sequential Knockoffs for Variable Selection in Reinforcement Learning. (arXiv:2303.14281v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015; Knockoffs (SEEK)&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#23454;&#29616;&#21464;&#37327;&#36873;&#25321;&#65292;&#35813;&#31639;&#27861;&#20272;&#35745;&#20102;&#26368;&#23567;&#20805;&#20998;&#29366;&#24577;&#65292;&#30830;&#20445;&#23398;&#20064;&#36827;&#31243;&#33391;&#22909;&#32780;&#19981;&#20250;&#20943;&#32531;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#24456;&#38590;&#33719;&#24471;&#19968;&#20010;&#26082;&#31616;&#27905;&#21448;&#28385;&#36275;&#39532;&#23572;&#21487;&#22827;&#23646;&#24615;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#20808;&#39564;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#24120;&#35268;&#20570;&#27861;&#26159;&#26500;&#36896;&#19968;&#20010;&#27604;&#24517;&#35201;&#30340;&#35201;&#22823;&#30340;&#29366;&#24577;&#65292;&#20363;&#22914;&#23558;&#36830;&#32493;&#26102;&#38388;&#28857;&#19978;&#30340;&#27979;&#37327;&#20018;&#32852;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#22686;&#21152;&#29366;&#24577;&#30340;&#32500;&#25968;&#21487;&#33021;&#20250;&#20943;&#32531;&#23398;&#20064;&#36827;&#31243;&#24182;&#20351;&#23398;&#20064;&#31574;&#30053;&#27169;&#31946;&#19981;&#28165;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#20013;&#30340;&#26368;&#23567;&#20805;&#20998;&#29366;&#24577;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#21407;&#22987;&#29366;&#24577;&#19979;&#26368;&#23567;&#30340;&#23376;&#21521;&#37327;&#65292;&#20351;&#35813;&#36807;&#31243;&#20173;&#28982;&#26159;MDP&#65292;&#24182;&#19988;&#19982;&#21407;&#22987;&#36807;&#31243;&#20849;&#20139;&#30456;&#21516;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015; Knockoffs (SEEK)&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#39640;&#32500;&#22797;&#26434;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#30340;&#26368;&#23567;&#20805;&#20998;&#29366;&#24577;&#12290;&#22312;&#22823;&#26679;&#26412;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25511;&#21046;&#20102;&#20551;&#21457;&#29616;&#29575;&#65292;&#24182;&#19988;&#36873;&#25321;&#25152;&#26377;&#20805;&#20998;&#30340;&#21464;&#37327;&#30340;&#27010;&#29575;&#36235;&#36817;&#20110;1&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world applications of reinforcement learning, it is often challenging to obtain a state representation that is parsimonious and satisfies the Markov property without prior knowledge. Consequently, it is common practice to construct a state which is larger than necessary, e.g., by concatenating measurements over contiguous time points. However, needlessly increasing the dimension of the state can slow learning and obfuscate the learned policy. We introduce the notion of a minimal sufficient state in a Markov decision process (MDP) as the smallest subvector of the original state under which the process remains an MDP and shares the same optimal policy as the original process. We propose a novel sequential knockoffs (SEEK) algorithm that estimates the minimal sufficient state in a system with high-dimensional complex nonlinear dynamics. In large samples, the proposed method controls the false discovery rate, and selects all sufficient variables with probability approaching one. As
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#26816;&#27979;&#26032;&#22855;&#24615;&#24182;&#24555;&#36895;&#36866;&#24212;&#20854;&#39046;&#22495;&#27169;&#22411;&#21644;&#34892;&#21160;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.14272</link><description>&lt;p&gt;
&#36890;&#36807;&#36866;&#24212;&#35268;&#21010;&#27169;&#22411;&#23398;&#20064;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning to Operate in Open Worlds by Adapting Planning Models. (arXiv:2303.14272v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#26816;&#27979;&#26032;&#22855;&#24615;&#24182;&#24555;&#36895;&#36866;&#24212;&#20854;&#39046;&#22495;&#27169;&#22411;&#21644;&#34892;&#21160;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#20195;&#29702;&#22312;&#39046;&#22495;&#27169;&#22411;&#19981;&#33021;&#20934;&#30830;&#20195;&#34920;&#19990;&#30028;&#30340;&#26032;&#24773;&#20917;&#19979;&#26080;&#27861;&#24456;&#22909;&#22320;&#34892;&#21160;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#36825;&#31181;&#20195;&#29702;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#33021;&#22815;&#26816;&#27979;&#21040;&#26032;&#22855;&#24615;&#24182;&#26377;&#25928;&#22320;&#36866;&#24212;&#20854;&#39046;&#22495;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#34892;&#21160;&#36873;&#25321;&#12290;&#23427;&#21033;&#29992;&#34892;&#21160;&#25191;&#34892;&#30340;&#35266;&#23519;&#21644;&#26681;&#25454;&#29615;&#22659;&#27169;&#22411;&#30340;&#39044;&#26399;&#27979;&#37327;&#23427;&#20204;&#30340;&#20559;&#24046;&#26469;&#25512;&#26029;&#26032;&#22855;&#24615;&#30340;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#23545;&#27169;&#22411;&#21464;&#21270;&#30340;&#21551;&#21457;&#24335;&#25628;&#32034;&#26469;&#20462;&#35746;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;CartPole&#19978;&#25253;&#21578;&#20102;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#19988;&#21487;&#35299;&#37322;&#22320;&#22788;&#29702;&#19968;&#31867;&#26032;&#22855;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning agents are ill-equipped to act in novel situations in which their domain model no longer accurately represents the world. We introduce an approach for such agents operating in open worlds that detects the presence of novelties and effectively adapts their domain models and consequent action selection. It uses observations of action execution and measures their divergence from what is expected, according to the environment model, to infer existence of a novelty. Then, it revises the model through a heuristics-guided search over model changes. We report empirical evaluations on the CartPole problem, a standard Reinforcement Learning (RL) benchmark. The results show that our approach can deal with a class of novelties very quickly and in an interpretable fashion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#22312;&#20219;&#20309;&#27969;&#24418;&#19978;&#65292;&#23545;&#20110;&#19968;&#20010;&#22312;&#27969;&#24418;&#19978;&#20219;&#24847;&#32676;&#20316;&#29992;&#19979;&#19981;&#21464;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#26680;&#23725;&#22238;&#24402;&#30340;&#26368;&#23567;&#21270;&#26368;&#20248;&#29575;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#26377;&#25928;&#26679;&#26412;&#25968;&#37327;&#25110;&#38477;&#20302;&#20102;&#27969;&#24418;&#30340;&#32500;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.14269</link><description>&lt;p&gt;
&#20869;&#23481;&#19981;&#21464;&#24615;&#23545;&#27969;&#24418;&#26680;&#22238;&#24402;&#30340;&#31934;&#30830;&#26679;&#26412;&#22797;&#26434;&#24230;&#22686;&#30410;
&lt;/p&gt;
&lt;p&gt;
The Exact Sample Complexity Gain from Invariances for Kernel Regression on Manifolds. (arXiv:2303.14269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#22312;&#20219;&#20309;&#27969;&#24418;&#19978;&#65292;&#23545;&#20110;&#19968;&#20010;&#22312;&#27969;&#24418;&#19978;&#20219;&#24847;&#32676;&#20316;&#29992;&#19979;&#19981;&#21464;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#26680;&#23725;&#22238;&#24402;&#30340;&#26368;&#23567;&#21270;&#26368;&#20248;&#29575;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#26377;&#25928;&#26679;&#26412;&#25968;&#37327;&#25110;&#38477;&#20302;&#20102;&#27969;&#24418;&#30340;&#32500;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#23558;&#20869;&#23481;&#19981;&#21464;&#24615;&#32534;&#30721;&#36827;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#26412;&#25991;&#23545;&#20869;&#23481;&#19981;&#21464;&#24615;&#22914;&#20309;&#25913;&#21892;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#29702;&#35770;&#32467;&#26524;&#36827;&#34892;&#20102;&#32454;&#21270;&#21644;&#25512;&#24191;&#65292;&#29305;&#21035;&#22320;&#65292;&#22312;&#20219;&#20309;&#27969;&#24418;&#19978;&#65292;&#23545;&#20110;&#19968;&#20010;&#22312;&#27969;&#24418;&#19978;&#20219;&#24847;&#32676;&#20316;&#29992;&#19979;&#19981;&#21464;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26680;&#23725;&#22238;&#24402;&#30340;&#26368;&#23567;&#21270;&#26368;&#20248;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#65288;&#20960;&#20046;&#65289;&#20219;&#20309;&#32676;&#20316;&#29992;&#65292;&#29978;&#33267;&#26159;&#27491;&#32500;&#24230;&#30340;&#32676;&#12290;&#23545;&#20110;&#26377;&#38480;&#32676;&#65292;&#22686;&#30410;&#36890;&#36807;&#23558;&#8220;&#26377;&#25928;&#8221;&#26679;&#26412;&#25968;&#37327;&#25193;&#22823;&#21040;&#32676;&#30340;&#22823;&#23567;&#26469;&#23454;&#29616;&#12290;&#23545;&#20110;&#27491;&#32500;&#24230;&#30340;&#32676;&#65292;&#22686;&#30410;&#34920;&#29616;&#20026;&#38477;&#20302;&#27969;&#24418;&#30340;&#32500;&#25968;&#65292;&#21516;&#26102;&#36824;&#19982;&#21830;&#31354;&#38388;&#20307;&#31215;&#25104;&#27604;&#20363;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#20174;&#24494;&#20998;&#20960;&#20309;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#19982;&#20351;&#29992;&#19981;&#21464;&#22810;&#39033;&#24335;&#30340;&#26356;&#24120;&#35265;&#31574;&#30053;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#22312;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#23398;&#20064;&#20013;&#30340;&#26032;&#20960;&#20309;&#35270;&#35282;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practice, encoding invariances into models helps sample complexity. In this work, we tighten and generalize theoretical results on how invariances improve sample complexity. In particular, we provide minimax optimal rates for kernel ridge regression on any manifold, with a target function that is invariant to an arbitrary group action on the manifold. Our results hold for (almost) any group action, even groups of positive dimension. For a finite group, the gain increases the "effective" number of samples by the group size. For groups of positive dimension, the gain is observed by a reduction in the manifold's dimension, in addition to a factor proportional to the volume of the quotient space. Our proof takes the viewpoint of differential geometry, in contrast to the more common strategy of using invariant polynomials. Hence, this new geometric viewpoint on learning with invariances may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#33258;&#30417;&#30563;&#26694;&#26550;&#26469;&#36319;&#36394;&#21387;&#21147;&#21453;&#24212;&#30340;&#29983;&#29702;&#21069;&#20307;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.14267</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#30417;&#30563;&#26694;&#26550;&#26469;&#36890;&#36807;&#22810;&#27169;&#24577;&#34987;&#21160;&#24863;&#30693;&#25913;&#36827;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#21387;&#21147;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Self-supervised Framework for Improved Data-Driven Monitoring of Stress via Multi-modal Passive Sensing. (arXiv:2303.14267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#33258;&#30417;&#30563;&#26694;&#26550;&#26469;&#36319;&#36394;&#21387;&#21147;&#21453;&#24212;&#30340;&#29983;&#29702;&#21069;&#20307;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#31243;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#30340;&#26368;&#26032;&#36827;&#23637;&#22823;&#22823;&#24800;&#21450;&#20102;&#24739;&#32773;&#65292;&#24182;&#22312;&#25552;&#39640;&#20854;&#29983;&#27963;&#36136;&#37327;&#26041;&#38754;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29983;&#29702;&#20581;&#24247;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#23637;&#31034;&#20986;&#36234;&#26469;&#36234;&#22810;&#30340;&#25104;&#21151;&#21644;&#25104;&#29087;&#24615;&#65292;&#20294;&#31934;&#31070;&#20581;&#24247;&#38598;&#20013;&#30340;&#24212;&#29992;&#30456;&#23545;&#32780;&#35328;&#21462;&#24471;&#30340;&#25104;&#21151;&#36824;&#27604;&#36739;&#26377;&#38480;&#65292;&#23613;&#31649;&#21387;&#21147;&#21644;&#28966;&#34385;&#38556;&#30861;&#26159;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#26368;&#24120;&#35265;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#20026;&#20102;&#36890;&#36807;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#20998;&#26512;&#26694;&#26550;&#26469;&#27979;&#37327;&#31934;&#31070;&#20581;&#24247;&#30340;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#36319;&#36394;&#21387;&#21147;&#21453;&#24212;&#30340;&#29983;&#29702;&#21069;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#19981;&#21516;&#39046;&#22495;&#21644;&#20998;&#36776;&#29575;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#23558;&#30701;&#26399;&#24773;&#33410;&#26144;&#23556;&#21040;&#32473;&#23450;&#20219;&#21153;&#30340;&#35821;&#20041;&#39640;&#25928;&#23884;&#20837;&#20013;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#36328;&#27169;&#24577;&#23545;&#27604;&#26041;&#27861;&#26469;&#22686;&#24378;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in remote health monitoring systems have significantly benefited patients and played a crucial role in improving their quality of life. However, while physiological health-focused solutions have demonstrated increasing success and maturity, mental health-focused applications have seen comparatively limited success in spite of the fact that stress and anxiety disorders are among the most common issues people deal with in their daily lives. In the hopes of furthering progress in this domain through the development of a more robust analytic framework for the measurement of indicators of mental health, we propose a multi-modal semi-supervised framework for tracking physiological precursors of the stress response. Our methodology enables utilizing multi-modal data of differing domains and resolutions from wearable devices and leveraging them to map short-term episodes to semantically efficient embeddings for a given task. Additionally, we leverage an inter-modality contrasti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#23433;&#20840;&#38598;&#31639;&#27861;&#30417;&#27979;&#21644;&#20462;&#25913;&#26631;&#20934;&#25511;&#21046;&#65292;&#24182;&#22312;&#32858;&#31867;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#65292;&#21516;&#26102;&#20351;&#29992;&#19977;&#39033;&#25216;&#26415;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.14265</link><description>&lt;p&gt;
&#38754;&#21521;&#32858;&#31867;&#21160;&#24577;&#29615;&#22659;&#30340;&#23433;&#20840;&#12289;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe and Sample-efficient Reinforcement Learning for Clustered Dynamic Environments. (arXiv:2303.14265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#23433;&#20840;&#38598;&#31639;&#27861;&#30417;&#27979;&#21644;&#20462;&#25913;&#26631;&#20934;&#25511;&#21046;&#65292;&#24182;&#22312;&#32858;&#31867;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#65292;&#21516;&#26102;&#20351;&#29992;&#19977;&#39033;&#25216;&#26415;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#21644;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#26377;&#25928;&#23398;&#20064;&#12290;&#20026;&#20102;&#22312;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#20445;&#35777;&#23433;&#20840;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#23433;&#20840;&#38598;&#31639;&#27861;&#65288;SSA&#65289;&#26469;&#30417;&#27979;&#21644;&#20462;&#25913;&#26631;&#20934;&#25511;&#21046;&#65292;&#24182;&#22312;&#32858;&#31867;&#21160;&#24577;&#29615;&#22659;&#19979;&#35780;&#20272;SSA+RL&#26694;&#26550;&#65292;&#36825;&#22312;&#29616;&#26377;&#30340;RL&#31639;&#27861;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;SSA+RL&#26694;&#26550;&#36890;&#24120;&#22312;&#22870;&#21169;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#19981;&#22815;&#39640;&#25928;&#65292;&#36825;&#22312;&#20197;&#21069;&#30340;&#23433;&#20840;RL&#24037;&#20316;&#20013;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#12290;&#20026;&#20102;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#25216;&#26415;&#65306;&#65288;1&#65289;&#36890;&#36807;&#36866;&#24212;SSA&#26469;&#36991;&#20813;&#36807;&#24230;&#20445;&#23432;&#30340;&#34892;&#20026;&#65307;&#65288;2&#65289;&#20351;&#29992;&#20855;&#26377;&#23433;&#20840;&#32422;&#26463;&#30340;&#38543;&#26426;&#32593;&#32476;&#31934;&#28860;&#26469;&#40723;&#21169;&#23433;&#20840;&#25506;&#32034;&#65307;&#65288;3&#65289;&#36890;&#36807;&#23558;SSA&#35270;&#20026;&#19987;&#23478;&#28436;&#31034;&#24182;&#30452;&#25509;&#20174;&#20013;&#23398;&#20064;&#26469;&#25552;&#39640;&#31574;&#30053;&#30340;&#25910;&#25947;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65306;
&lt;/p&gt;
&lt;p&gt;
This study proposes a safe and sample-efficient reinforcement learning (RL) framework to address two major challenges in developing applicable RL algorithms: satisfying safety constraints and efficiently learning with limited samples. To guarantee safety in real-world complex environments, we use the safe set algorithm (SSA) to monitor and modify the nominal controls, and evaluate SSA+RL in a clustered dynamic environment which is challenging to be solved by existing RL algorithms. However, the SSA+RL framework is usually not sample-efficient especially in reward-sparse environments, which has not been addressed in previous safe RL works. To improve the learning efficiency, we propose three techniques: (1) avoiding behaving overly conservative by adapting the SSA; (2) encouraging safe exploration using random network distillation with safety constraints; (3) improving policy convergence by treating SSA as expert demonstrations and directly learn from that. The experimental results show
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#39057;&#35889;&#21644;&#26102;&#38388;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#22686;&#24378;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#36830;&#36143;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.14254</link><description>&lt;p&gt;
&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22810;&#26679;&#21270;&#21644;&#36830;&#36143;&#21270;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Diverse and Coherent Augmentation for Time-Series Forecasting. (arXiv:2303.14254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#39057;&#35889;&#21644;&#26102;&#38388;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#22686;&#24378;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#36830;&#36143;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22686;&#24378;&#25216;&#26415;&#32531;&#35299;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#20998;&#31867;&#38382;&#39064;&#35774;&#35745;&#65292;&#21363;&#20351;&#22686;&#24378;&#25913;&#21464;&#20102;&#26102;&#38388;&#21160;&#24577;&#65292;&#31867;&#21035;&#26631;&#31614;&#20063;&#21487;&#20197;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#38024;&#23545;&#39044;&#27979;&#35774;&#35745;&#30340;&#22686;&#24378;&#38656;&#35201;&#22810;&#26679;&#24615;&#21644;&#19982;&#21407;&#22987;&#26102;&#38388;&#21160;&#24577;&#30340;&#36830;&#36143;&#24615;&#12290;&#30001;&#20110;&#23454;&#38469;&#29289;&#29702;&#36807;&#31243;&#20135;&#29983;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20855;&#26377;&#26102;&#22495;&#21644;&#39057;&#22495;&#30340;&#29305;&#24449;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#32452;&#21512;&#39057;&#35889;&#21644;&#26102;&#38388;&#22686;&#24378;&#65288;STAug&#65289;&#26469;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#36830;&#36143;&#30340;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#39057;&#22495;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#26469;&#20998;&#35299;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#26435;&#37325;&#37325;&#26032;&#32452;&#35013;&#23376;&#20998;&#37327;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#19982;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#30340;&#20851;&#31995;&#36830;&#36143;&#19968;&#33268;&#65292;&#22240;&#20026;&#23427;&#20204;&#37117;&#21253;&#21547;&#30456;&#21516;&#30340;&#22522;&#30784;&#20998;&#37327;&#12290;&#22312;&#26102;&#38388;&#22495;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#28151;&#21512;&#31574;&#30053;&#26469;&#32452;&#21512;&#25968;&#25454;&#65292;&#36825;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#26679;&#26412;&#20043;&#38388;&#30340;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series data augmentation mitigates the issue of insufficient training data for deep learning models. Yet, existing augmentation methods are mainly designed for classification, where class labels can be preserved even if augmentation alters the temporal dynamics. We note that augmentation designed for forecasting requires diversity as well as coherence with the original temporal dynamics. As time-series data generated by real-life physical processes exhibit characteristics in both the time and frequency domains, we propose to combine Spectral and Time Augmentation (STAug) for generating more diverse and coherent samples. Specifically, in the frequency domain, we use the Empirical Mode Decomposition to decompose a time series and reassemble the subcomponents with random weights. This way, we generate diverse samples while being coherent with the original temporal relationships as they contain the same set of base components. In the time domain, we adapt a mix-up strategy that genera
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#22240;&#23376;&#21270;&#26041;&#27861;&#35757;&#32451;&#30340;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#21487;&#20197;&#25910;&#25947;&#65292;&#24182;&#19988;&#38544;&#24335;&#24179;&#34913;&#21644;&#27491;&#21017;&#21270;&#21487;&#20197;&#20419;&#36827;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.14244</link><description>&lt;p&gt;
&#38544;&#24335;&#24179;&#34913;&#21644;&#27491;&#21017;&#21270;&#65306;&#36807;&#21442;&#25968;&#21270;&#38750;&#23545;&#31216;&#30697;&#38453;&#24863;&#30693;&#20013;&#30340;&#27867;&#21270;&#21644;&#25910;&#25947;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Implicit Balancing and Regularization: Generalization and Convergence Guarantees for Overparameterized Asymmetric Matrix Sensing. (arXiv:2303.14244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#22240;&#23376;&#21270;&#26041;&#27861;&#35757;&#32451;&#30340;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#21487;&#20197;&#25910;&#25947;&#65292;&#24182;&#19988;&#38544;&#24335;&#24179;&#34913;&#21644;&#27491;&#21017;&#21270;&#21487;&#20197;&#20419;&#36827;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#23646;&#24615;&#26377;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#35768;&#22810;&#26041;&#38754;&#65292;&#21253;&#25324;&#23567;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#35282;&#33394;&#20197;&#21450;&#27169;&#22411;&#30340;&#21508;&#31181;&#21442;&#25968;&#22312;&#26799;&#24230;&#26356;&#26032;&#20013;&#22914;&#20309;&#32806;&#21512;&#20197;&#20419;&#36827;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#20173;&#28982;&#26159;&#24456;&#31070;&#31192;&#30340;&#12290;&#26368;&#36817;&#19968;&#31995;&#21015;&#30340;&#35770;&#25991;&#24050;&#32463;&#24320;&#22987;&#30740;&#31350;&#38750;&#20984;&#23545;&#31216;&#21322;&#27491;&#23450;&#65288;PSD&#65289;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#30340;&#24418;&#24335;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#38656;&#35201;&#20174;&#20960;&#20010;&#32447;&#24615;&#27979;&#37327;&#20013;&#37325;&#24314;&#19968;&#20010;&#20302;&#31209;PSD&#30697;&#38453;&#12290;&#36825;&#31181;&#24213;&#23618;&#30340;&#23545;&#31216;&#24615;/PSD&#24615;&#23545;&#20110;&#29616;&#26377;&#30340;&#36825;&#20010;&#38382;&#39064;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#20445;&#35777;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#36807;&#21442;&#25968;&#21270;&#30340;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#20854;&#20013;&#24076;&#26395;&#20174;&#23569;&#37327;&#30340;&#32447;&#24615;&#27979;&#37327;&#20013;&#37325;&#24314;&#19968;&#20010;&#38750;&#23545;&#31216;&#30697;&#24418;&#20302;&#31209;&#30697;&#38453;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#22240;&#23376;&#21270;&#26469;&#35757;&#32451;&#30340;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21487;&#20197;&#25910;&#25947;&#65292;&#32780;&#38544;&#24335;&#24179;&#34913;&#21644;&#27491;&#21017;&#21270;&#21487;&#20197;&#20419;&#36827;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been significant progress in understanding the convergence and generalization properties of gradient-based methods for training overparameterized learning models. However, many aspects including the role of small random initialization and how the various parameters of the model are coupled during gradient-based updates to facilitate good generalization remain largely mysterious. A series of recent papers have begun to study this role for non-convex formulations of symmetric Positive Semi-Definite (PSD) matrix sensing problems which involve reconstructing a low-rank PSD matrix from a few linear measurements. The underlying symmetry/PSDness is crucial to existing convergence and generalization guarantees for this problem. In this paper, we study a general overparameterized low-rank matrix sensing problem where one wishes to reconstruct an asymmetric rectangular low-rank matrix from a few linear measurements. We prove that an overparameterized model trained via factori
&lt;/p&gt;</description></item><item><title>IDGI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569; integrated gradients &#35299;&#37322;&#26174;&#33879;&#24615;&#22270;&#20013;&#30340;&#22122;&#22768;&#65292;&#24182;&#22312;&#20247;&#22810;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.14242</link><description>&lt;p&gt;
IDGI&#65306;&#19968;&#20010;&#28040;&#38500; integrated gradients &#35299;&#37322;&#22122;&#22768;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients. (arXiv:2303.14242v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14242
&lt;/p&gt;
&lt;p&gt;
IDGI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569; integrated gradients &#35299;&#37322;&#26174;&#33879;&#24615;&#22270;&#20013;&#30340;&#22122;&#22768;&#65292;&#24182;&#22312;&#20247;&#22810;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Integrated Gradients&#65288;IG&#65289;&#21450;&#20854;&#21464;&#20307;&#26159;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#25216;&#26415;&#12290;&#34429;&#28982;&#22522;&#20110;IG&#30340;&#26041;&#27861;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#23558;&#22122;&#22768;&#38598;&#25104;&#21040;&#20854;&#35299;&#37322;&#26174;&#33879;&#24615;&#22270;&#20013;&#65292;&#20174;&#32780;&#38477;&#20302;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#22122;&#22768;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#25214;&#20986;&#20102;&#22122;&#22768;&#30340;&#26469;&#28304;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#35299;&#37322;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#35201;&#26041;&#21521;&#26799;&#24230;&#38598;&#25104;&#65288;IDGI&#65289;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#38598;&#25104;&#21040;&#20219;&#20309;&#20351;&#29992;Reimann&#31215;&#20998;&#35745;&#31639;&#38598;&#25104;&#26799;&#24230;&#30340;&#22522;&#20110;IG&#30340;&#26041;&#27861;&#20013;&#12290;&#19977;&#31181;&#22522;&#20110;IG&#30340;&#26041;&#27861;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;IDGI&#22312;&#20247;&#22810;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrated Gradients (IG) as well as its variants are well-known techniques for interpreting the decisions of deep neural networks. While IG-based approaches attain state-of-the-art performance, they often integrate noise into their explanation saliency maps, which reduce their interpretability. To minimize the noise, we examine the source of the noise analytically and propose a new approach to reduce the explanation noise based on our analytical findings. We propose the Important Direction Gradient Integration (IDGI) framework, which can be easily incorporated into any IG-based method that uses the Reimann Integration for integrated gradient computation. Extensive experiments with three IG-based methods show that IDGI improves them drastically on numerous interpretability metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#26041;&#27861;InnerCore&#29992;&#20110;&#20998;&#26512;&#21306;&#22359;&#38142;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#32593;&#32476;&#23545;&#20851;&#38190;&#21442;&#19982;&#32773;&#30340;&#35782;&#21035;&#24182;&#25552;&#20379;&#24773;&#24863;&#25351;&#26631;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#36235;&#21183;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.14241</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#24515;&#30340;&#21306;&#22359;&#38142;&#32593;&#32476;&#36235;&#21183;&#30417;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Core-based Trend Detection in Blockchain Networks. (arXiv:2303.14241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#26041;&#27861;InnerCore&#29992;&#20110;&#20998;&#26512;&#21306;&#22359;&#38142;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#32593;&#32476;&#23545;&#20851;&#38190;&#21442;&#19982;&#32773;&#30340;&#35782;&#21035;&#24182;&#25552;&#20379;&#24773;&#24863;&#25351;&#26631;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#36235;&#21183;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#22312;&#36152;&#26131;&#37329;&#34701;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#27599;&#22825;&#20132;&#26131;&#30340;&#36164;&#20135;&#20215;&#20540;&#36798;&#25968;&#21313;&#20159;&#32654;&#20803;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#65292;&#23545;&#36825;&#20123;&#32593;&#32476;&#36827;&#34892;&#20998;&#26512;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#26041;&#27861;- InnerCore&#65292;&#29992;&#20110;&#35782;&#21035;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#21442;&#19982;&#32773;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#28145;&#24230;&#30340;&#20869;&#26680;&#20998;&#35299;&#21644;&#20013;&#24515;&#27169;&#24335;&#21457;&#29616;&#25552;&#20379;&#32593;&#32476;&#24773;&#24863;&#25351;&#26631;&#12290; InnerCore&#26159;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#12289;&#38750;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20998;&#26512;&#22823;&#35268;&#27169;&#26102;&#38388;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;LunaTerra&#30340;&#26368;&#36817;&#23849;&#28291;&#21644;&#20197;&#22826;&#22346;&#30340;PoS&#20999;&#25442;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#19968;&#23478;&#39046;&#20808;&#30340;&#21306;&#22359;&#38142;&#20998;&#26512;&#20844;&#21496;&#25910;&#38598;&#30340;&#22806;&#37096;&#22522;&#26412;&#20107;&#23454;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;InnerCore&#21487;&#20197;&#19982;&#21512;&#26684;&#30340;&#20998;&#26512;&#20934;&#30830;&#21305;&#37197;&#65292;&#19981;&#38656;&#35201;&#20154;&#20026;&#24178;&#39044;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#20280;&#32553;&#30340;&#21306;&#22359;&#38142;&#20998;&#26512;&#21644;&#36235;&#21183;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchains are now significantly easing trade finance, with billions of dollars worth of assets being transacted daily. However, analyzing these networks remains challenging due to the large size and complexity of the data. We introduce a scalable approach called "InnerCore" for identifying key actors in blockchain-based networks and providing a sentiment indicator for the networks using data depth-based core decomposition and centered-motif discovery. InnerCore is a computationally efficient, unsupervised approach suitable for analyzing large temporal graphs. We demonstrate its effectiveness through case studies on the recent collapse of LunaTerra and the Proof-of-Stake (PoS) switch of Ethereum, using external ground truth collected by a leading blockchain analysis company. Our experiments show that InnerCore can match the qualified analysis accurately without human involvement, automating blockchain analysis and its trend detection in a scalable manner.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20123;&#20195;&#29702;&#26080;&#27861;&#29702;&#35299;&#20182;&#20204;&#22312;&#22242;&#38431;&#34920;&#29616;&#20013;&#30340;&#30495;&#23454;&#24433;&#21709;&#65292;&#23548;&#33268;&#23398;&#20064;&#27425;&#20248;&#31574;&#30053;&#65292;&#34920;&#29616;&#25042;&#24816;&#12290;&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#26816;&#27979;&#24809;&#32602;&#25042;&#24816;&#20195;&#29702;&#24182;&#25913;&#21892;&#20854;&#34892;&#20026;&#65292;&#22242;&#38431;&#25972;&#20307;&#24615;&#33021;&#21644;&#27599;&#20010;&#20195;&#29702;&#30340;&#20010;&#20307;&#33021;&#21147;&#37117;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.14227</link><description>&lt;p&gt;
&#39640;&#25928;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Causality Detection for Efficient Multi-Agent Reinforcement Learning. (arXiv:2303.14227v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20123;&#20195;&#29702;&#26080;&#27861;&#29702;&#35299;&#20182;&#20204;&#22312;&#22242;&#38431;&#34920;&#29616;&#20013;&#30340;&#30495;&#23454;&#24433;&#21709;&#65292;&#23548;&#33268;&#23398;&#20064;&#27425;&#20248;&#31574;&#30053;&#65292;&#34920;&#29616;&#25042;&#24816;&#12290;&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#26816;&#27979;&#24809;&#32602;&#25042;&#24816;&#20195;&#29702;&#24182;&#25913;&#21892;&#20854;&#34892;&#20026;&#65292;&#22242;&#38431;&#25972;&#20307;&#24615;&#33021;&#21644;&#27599;&#20010;&#20195;&#29702;&#30340;&#20010;&#20307;&#33021;&#21147;&#37117;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20316;&#20026;&#22242;&#38431;&#23398;&#20064;&#20219;&#21153;&#26102;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#30340;&#19968;&#20123;&#20195;&#29702;&#21487;&#33021;&#26080;&#27861;&#29702;&#35299;&#20182;&#20204;&#22312;&#22242;&#38431;&#34920;&#29616;&#20013;&#30340;&#30495;&#23454;&#24433;&#21709;&#12290;&#36825;&#20123;&#20195;&#29702;&#26368;&#32456;&#20250;&#23398;&#20064;&#27425;&#20248;&#31574;&#30053;&#65292;&#34920;&#29616;&#20986;&#19981;&#33391;&#30340;&#25042;&#24816;&#34892;&#20026;&#12290;&#26412;&#25991;&#36890;&#36807;&#27491;&#24335;&#34920;&#36848;&#26102;&#38388;&#22240;&#26524;&#20851;&#31995;&#22312;MARL&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#26469;&#24809;&#32602;&#36825;&#20123;&#25042;&#24816;&#20195;&#29702;&#24182;&#25913;&#21892;&#20854;&#34892;&#20026;&#12290;&#36890;&#36807;&#29702;&#35299;&#20182;&#20204;&#30340;&#26412;&#22320;&#35266;&#27979;&#22914;&#20309;&#22240;&#26524;&#30456;&#20851;&#20110;&#22242;&#38431;&#22870;&#21169;&#65292;&#22242;&#38431;&#20013;&#30340;&#27599;&#20010;&#20195;&#29702;&#37117;&#21487;&#20197;&#26681;&#25454;&#20182;&#20204;&#26159;&#21542;&#26377;&#21161;&#20110;&#23548;&#33268;&#22870;&#21169;&#26469;&#35843;&#25972;&#20854;&#20010;&#20154;&#20449;&#29992;&#36129;&#29486;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;&#22312;MARL&#20013;&#20351;&#29992;&#22240;&#26524;&#20272;&#35745;&#19981;&#20165;&#21487;&#20197;&#25913;&#21892;&#22242;&#38431;&#25972;&#20307;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#25552;&#21319;&#27599;&#20010;&#20195;&#29702;&#30340;&#20010;&#20307;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19968;&#32452;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#25913;&#36827;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
When learning a task as a team, some agents in Multi-Agent Reinforcement Learning (MARL) may fail to understand their true impact in the performance of the team. Such agents end up learning sub-optimal policies, demonstrating undesired lazy behaviours. To investigate this problem, we start by formalising the use of temporal causality applied to MARL problems. We then show how causality can be used to penalise such lazy agents and improve their behaviours. By understanding how their local observations are causally related to the team reward, each agent in the team can adjust their individual credit based on whether they helped to cause the reward or not. We show empirically that using causality estimations in MARL improves not only the holistic performance of the team, but also the individual capabilities of each agent. We observe that the improvements are consistent in a set of different environments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#24178;&#39044;&#19979;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#26045;&#21152;&#28508;&#22312;&#32467;&#26500;&#36328;&#36234;&#21333;&#20301;&#21644;&#32452;&#21512;&#65292;&#22312;&#38477;&#20302;&#23454;&#39564;&#25968;&#37327;&#21644;&#22788;&#29702;&#28151;&#26434;&#38382;&#39064;&#26041;&#38754;&#26377;&#30528;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.14226</link><description>&lt;p&gt;
&#32452;&#21512;&#24178;&#39044;&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;:&#21512;&#25104;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions. (arXiv:2303.14226v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14226
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#24178;&#39044;&#19979;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#26045;&#21152;&#28508;&#22312;&#32467;&#26500;&#36328;&#36234;&#21333;&#20301;&#21644;&#32452;&#21512;&#65292;&#22312;&#38477;&#20302;&#23454;&#39564;&#25968;&#37327;&#21644;&#22788;&#29702;&#28151;&#26434;&#38382;&#39064;&#26041;&#38754;&#26377;&#30528;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21253;&#21547;N&#20010;&#24322;&#36136;&#21333;&#20301;&#21644;p&#20010;&#24178;&#39044;&#30340;&#35774;&#32622;&#12290; &#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#20219;&#24847;&#32452;&#21512;&#30340;&#21333;&#20301;&#29305;&#23450;&#28508;&#22312;&#32467;&#26524;&#65292;&#21363;N&#215;2 ^ p&#20010;&#22240;&#26524;&#21442;&#25968;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#33258;&#28982;&#20986;&#29616;&#20102;&#36873;&#25321;&#24178;&#39044;&#32452;&#21512;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#22240;&#23376;&#35774;&#35745;&#35797;&#39564;&#65292;&#25512;&#33616;&#24341;&#25806;(&#20363;&#22914;&#65292;&#20026;&#29992;&#25143;&#26174;&#31034;&#26368;&#22823;&#31243;&#24230;&#30340;&#21442;&#19982;&#24230;&#30340;&#19968;&#32452;&#30005;&#24433;)&#65292;&#21307;&#23398;&#20013;&#30340;&#32452;&#21512;&#30103;&#27861;&#65292;&#36873;&#25321;ML&#27169;&#22411;&#30340;&#37325;&#35201;&#29305;&#24449;&#31561;&#31561;&#12290;&#24403;N&#21644;p&#22686;&#38271;&#26102;&#65292;&#36827;&#34892;N&#215;2 ^ p&#20010;&#23454;&#39564;&#26469;&#20272;&#35745;&#21508;&#31181;&#21442;&#25968;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#32780;&#19988;&#65292;&#35266;&#27979;&#25968;&#25454;&#24456;&#21487;&#33021;&#23384;&#22312;&#28151;&#26434;&#65292;&#21363;&#21333;&#20301;&#26159;&#21542;&#22312;&#32452;&#21512;&#19979;&#20986;&#29616;&#19982;&#20854;&#22312;&#35813;&#32452;&#21512;&#19979;&#30340;&#28508;&#22312;&#32467;&#26524;&#30456;&#20851;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#22312;&#21333;&#20301;&#21644;&#32452;&#21512;&#20043;&#38388;&#37117;&#26045;&#21152;&#20102;&#28508;&#22312;&#32467;&#26500;&#12290;&#25105;&#20204;&#20551;&#35774;&#21333;&#20301;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#30456;&#20284;&#24615;(&#21363;&#31867;&#20284;&#21333;&#20301;&#30340;&#28508;&#22312;&#32467;&#26524;&#26159;&#30456;&#20284;&#30340;)&#65292;&#24182;&#19988;&#32452;&#21512;&#20043;&#38388;&#20063;&#23384;&#22312;&#28508;&#22312;&#30340;&#30456;&#20284;&#24615;(&#21363;&#31867;&#20284;&#32452;&#21512;&#30340;&#25928;&#26524;&#26159;&#30456;&#20284;&#30340;)&#12290;&#25105;&#20204;&#20351;&#29992;&#23618;&#27425;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#27169;&#22411;&#26469;&#24418;&#24335;&#21270;&#36825;&#19968;&#28857;&#65292;&#35813;&#27169;&#22411;&#32852;&#21512;&#32858;&#31867;&#21333;&#20803;&#21644;&#32452;&#21512;&#65292;&#24182;&#19988;&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#20197;&#27169;&#25311;&#36830;&#32493;&#25110;&#31163;&#25955;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#28436;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#26174;&#30528;&#20943;&#23569;&#23398;&#20064;&#22240;&#26524;&#21442;&#25968;&#25152;&#38656;&#30340;&#23454;&#39564;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a setting with $N$ heterogeneous units and $p$ interventions. Our goal is to learn unit-specific potential outcomes for any combination of these $p$ interventions, i.e., $N \times 2^p$ causal parameters. Choosing combinations of interventions is a problem that naturally arises in many applications such as factorial design experiments, recommendation engines (e.g., showing a set of movies that maximizes engagement for users), combination therapies in medicine, selecting important features for ML models, etc. Running $N \times 2^p$ experiments to estimate the various parameters is infeasible as $N$ and $p$ grow. Further, with observational data there is likely confounding, i.e., whether or not a unit is seen under a combination is correlated with its potential outcome under that combination. To address these challenges, we propose a novel model that imposes latent structure across both units and combinations. We assume latent similarity across units (i.e., the potential outco
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21152;&#36895;&#26032;&#22411;&#30899;&#25429;&#38598;&#26448;&#26009;&#21457;&#29616;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#32467;&#21512;&#23454;&#39564;&#23460;&#27979;&#35797;&#21644;&#20998;&#23376;&#25351;&#32441;&#27169;&#22411;&#26041;&#27861;&#65292;&#25104;&#21151;&#35782;&#21035;&#20986;&#28508;&#22312;&#30340;&#30899;&#25429;&#38598;&#28342;&#21058;&#12290;</title><link>http://arxiv.org/abs/2303.14223</link><description>&lt;p&gt;
&#26426;&#22120;&#36741;&#21161;&#21457;&#29616;&#26032;&#22411;&#30899;&#25429;&#38598;&#28342;&#21058;
&lt;/p&gt;
&lt;p&gt;
Machine Guided Discovery of Novel Carbon Capture Solvents. (arXiv:2303.14223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14223
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21152;&#36895;&#26032;&#22411;&#30899;&#25429;&#38598;&#26448;&#26009;&#21457;&#29616;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#32467;&#21512;&#23454;&#39564;&#23460;&#27979;&#35797;&#21644;&#20998;&#23376;&#25351;&#32441;&#27169;&#22411;&#26041;&#27861;&#65292;&#25104;&#21151;&#35782;&#21035;&#20986;&#28508;&#22312;&#30340;&#30899;&#25429;&#38598;&#28342;&#21058;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30899;&#25429;&#38598;&#25216;&#26415;&#22312;&#20943;&#23569;CO2&#25490;&#25918;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#38271;&#65292;&#25552;&#39640;&#25429;&#38598;&#26448;&#26009;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#38754;&#20020;&#30528;&#26448;&#26009;&#24320;&#21457;&#30340;&#25361;&#25112;&#65292;&#35813;&#36807;&#31243;&#21487;&#33021;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;&#26426;&#22120;&#23398;&#20064;&#36890;&#36807;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#30340;&#39640;&#25928;&#30456;&#20851;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#38477;&#20302;&#26448;&#26009;&#24320;&#21457;&#26102;&#38388;&#21644;&#36164;&#28304;&#36127;&#25285;&#30340;&#26377;&#21069;&#36884;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#21069;&#36884;&#20505;&#36873;&#26448;&#26009;&#30340;&#31579;&#36873;&#21644;&#20851;&#27880;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#29702;&#24565;&#65292;&#25105;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#23545;&#31471;&#30340;&#8220;&#21457;&#29616;&#29983;&#21629;&#21608;&#26399;&#8221;&#65292;&#20197;&#36873;&#25321;&#19982;&#21830;&#19994;&#21270;&#37240;&#27668;&#27927;&#28068;&#30899;&#25429;&#38598;&#20860;&#23481;&#30340;&#26032;&#22411;&#27700;&#33018;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#23545;CO2&#21560;&#25910;&#30340;&#31616;&#21333;&#12289;&#24555;&#36895;&#23454;&#39564;&#23460;&#27979;&#35797;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#23376;&#25351;&#32441;&#27169;&#22411;&#26041;&#27861;&#12290;&#39044;&#27979;&#36807;&#31243;&#22312;&#23454;&#39564;&#21644;&#26448;&#26009;&#21442;&#25968;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#22343;&#20026;60%&#65292;&#22312;&#22806;&#37096;&#27979;&#35797;&#38598;&#19978;&#21333;&#20010;&#21442;&#25968;&#30340;&#20934;&#30830;&#24615;&#36798;&#21040;&#20102;80%&#12290;&#21457;&#29616;&#29983;&#21629;&#21608;&#26399;&#31995;&#32479;&#25104;&#21151;&#22320;&#30830;&#23450;&#20102;&#20197;&#21069;&#26410;&#30693;&#30340;&#33018;&#31867;&#20505;&#36873;&#20154;&#20316;&#20026;&#28508;&#22312;&#30340;&#30899;&#25429;&#38598;&#28342;&#21058;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing importance of carbon capture technologies for deployment in remediating CO2 emissions, and thus the necessity to improve capture materials to allow scalability and efficiency, faces the challenge of materials development, which can require substantial costs and time. Machine learning offers a promising method for reducing the time and resource burdens of materials development through efficient correlation of structure-property relationships to allow down-selection and focusing on promising candidates. Towards demonstrating this, we have developed an end-to-end "discovery cycle" to select new aqueous amines compatible with the commercially viable acid gas scrubbing carbon capture. We combine a simple, rapid laboratory assay for CO2 absorption with a machine learning based molecular fingerprinting model approach. The prediction process shows 60% accuracy against experiment for both material parameters and 80% for a single parameter on an external test set. The discovery cy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#25152;&#38656;&#30340;&#24773;&#24863;&#21644;&#35821;&#20041;&#29305;&#24449;&#34920;&#31034;&#30340;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#24773;&#24863;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#24066;&#22330;&#36235;&#21183;&#26041;&#38754;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2303.14221</link><description>&lt;p&gt;
&#20449;&#24687;&#34920;&#31034;&#20043;&#20105;&#65306;&#27604;&#36739;&#24773;&#24863;&#19982;&#35821;&#20041;&#29305;&#24449;&#39044;&#27979;&#24066;&#22330;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
The Battle of Information Representations: Comparing Sentiment and Semantic Features for Forecasting Market Trends. (arXiv:2303.14221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#25152;&#38656;&#30340;&#24773;&#24863;&#21644;&#35821;&#20041;&#29305;&#24449;&#34920;&#31034;&#30340;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#24773;&#24863;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#24066;&#22330;&#36235;&#21183;&#26041;&#38754;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#32929;&#24066;&#26159;&#25581;&#31034;&#38544;&#34255;&#24066;&#22330;&#35268;&#24459;&#30340;&#20027;&#35201;&#26041;&#21521;&#12290;&#19982;&#20256;&#32479;&#20998;&#26512;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#30693;&#35782;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;&#37329;&#34701;&#24066;&#22330;&#21160;&#24577;&#24182;&#33719;&#24471;&#34892;&#20026;&#27934;&#23519;&#12290;&#32929;&#31080;&#20215;&#26684;&#19982;&#19990;&#30028;&#20107;&#20214;&#21644;&#31038;&#20250;&#35266;&#24565;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#22312;&#26500;&#24314;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#20851;&#38190;&#38454;&#27573;&#26159;&#21152;&#20837;&#36825;&#20123;&#20174;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21453;&#26144;&#20986;&#26469;&#30340;&#22806;&#37096;&#20449;&#24687;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#38544;&#24335;&#25110;&#26174;&#24335;&#30340;&#30693;&#35782;&#34920;&#31034;&#65306;&#65288;1&#65289;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#24773;&#24863;&#25110;&#65288;2&#65289;&#21407;&#22987;&#25991;&#26412;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#22312;&#24433;&#21709;&#37329;&#34701;&#27169;&#22411;&#39044;&#27979;&#33021;&#21147;&#26041;&#38754;&#65292;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#30452;&#25509;&#27604;&#36739;&#30740;&#31350;&#22826;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#24182;&#24324;&#28165;&#26970;&#35821;&#20041;&#29305;&#24449;&#26041;&#27861;&#33021;&#21542;&#36229;&#36234;&#22522;&#20110;&#24773;&#24863;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#21518;&#32773;&#22312;&#39044;&#27979;&#24066;&#22330;&#36235;&#21183;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#27010;&#36848;&#20102;&#20004;&#31181;&#34920;&#31034;&#26041;&#24335;&#30340;&#20248;&#28857;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of the stock market with the attraction of machine learning approaches is a major direction for revealing hidden market regularities. This knowledge contributes to a profound understanding of financial market dynamics and getting behavioural insights, which could hardly be discovered with traditional analytical methods. Stock prices are inherently interrelated with world events and social perception. Thus, in constructing the model for stock price prediction, the critical stage is to incorporate such information on the outside world, reflected through news and social media posts. To accommodate this, researchers leverage the implicit or explicit knowledge representations: (1) sentiments extracted from the texts or (2) raw text embeddings. However, there is too little research attention to the direct comparison of these approaches in terms of the influence on the predictive power of financial models. In this paper, we aim to close this gap and figure out whether the semantic f
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;&#21270;&#27969;&#30340;&#39640;&#32500;&#32437;&#21521;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36827;&#34892;&#22909;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.14220</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#20934;&#21270;&#27969;&#30340;&#21464;&#20998;&#25512;&#29702;&#22788;&#29702;&#32437;&#21521;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Variational Inference for Longitudinal Data Using Normalizing Flows. (arXiv:2303.14220v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14220
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;&#21270;&#27969;&#30340;&#39640;&#32500;&#32437;&#21521;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36827;&#34892;&#22909;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#32437;&#21521;&#25968;&#25454;&#65292;&#24182;&#20381;&#36182;&#20110;&#21464;&#20998;&#25512;&#29702;&#12290;&#20351;&#29992;&#26631;&#20934;&#21270;&#27969;&#26469;&#23545;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#23436;&#20840;&#21512;&#25104;&#30340;&#32437;&#21521;&#24207;&#21015;&#65292;&#20063;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#22312;&#24207;&#21015;&#20013;&#19982;&#22810;&#20010;&#25968;&#25454;&#26377;&#20851;&#30340;&#36712;&#36857;&#65292;&#24182;&#19988;&#22312;&#32570;&#22833;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;6&#20010;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#35813;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;&#20986;&#23427;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20284;&#28982;&#20272;&#35745;&#65292;&#20197;&#21450;&#26356;&#21487;&#38752;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#12290;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/clementchadebec/variational_inference_for_longitudinal_data}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new latent variable generative model able to handle high dimensional longitudinal data and relying on variational inference. The time dependency between the observations of an input sequence is modelled using normalizing flows over the associated latent variables. The proposed method can be used to generate either fully synthetic longitudinal sequences or trajectories that are conditioned on several data in a sequence and demonstrates good robustness properties to missing data. We test the model on 6 datasets of different complexity and show that it can achieve better likelihood estimates than some competitors as well as more reliable missing data imputation. A code is made available at \url{https://github.com/clementchadebec/variational_inference_for_longitudinal_data}.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;DRL&#22522;&#30784;&#30340;AVs&#20013;&#25506;&#32034;&#26368;&#20339;&#24179;&#28369;&#20998;&#24067;&#20197;&#28040;&#38500;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;&#36755;&#20837;&#20013;&#26469;&#20013;&#21644;&#25915;&#20987;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#26816;&#27979;&#21644;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2303.14197</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#36890;&#31995;&#32479;&#20013;&#21518;&#38376;&#28040;&#38500;&#30340;&#26368;&#20339;&#24179;&#28369;&#20998;&#24067;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems. (arXiv:2303.14197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;DRL&#22522;&#30784;&#30340;AVs&#20013;&#25506;&#32034;&#26368;&#20339;&#24179;&#28369;&#20998;&#24067;&#20197;&#28040;&#38500;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;&#36755;&#20837;&#20013;&#26469;&#20013;&#21644;&#25915;&#20987;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#26816;&#27979;&#21644;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#25552;&#39640;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#30340;&#25928;&#29575;&#65292;&#20294;&#20063;&#20351;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20132;&#36890;&#25317;&#22581;&#25110;&#30896;&#25758;&#12290;&#21518;&#38376;&#21151;&#33021;&#36890;&#24120;&#26159;&#36890;&#36807;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#20445;&#25345;&#23545;&#30495;&#23454;&#36755;&#20837;&#30340;&#39640;&#20934;&#30830;&#24615;&#24182;&#35825;&#23548;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#29305;&#23450;&#36755;&#20837;&#30340;&#26399;&#26395;&#65288;&#24694;&#24847;&#65289;&#36755;&#20986;&#26469;&#23454;&#29616;&#30340;&#12290;&#24403;&#21069;&#25269;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29305;&#24449;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#19978;&#65292;&#36825;&#19981;&#33021;&#24456;&#22909;&#22320;&#36716;&#31227;&#21040;DRL&#22522;&#30784;&#30340;AV&#25511;&#21046;&#22120;&#30340;&#22238;&#24402;&#20219;&#21153;&#65292;&#22240;&#20026;&#36755;&#20837;&#26159;&#36830;&#32493;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#21363;AV&#21450;&#20854;&#21608;&#22260;&#36710;&#36742;&#30340;&#36895;&#24230;&#21644;&#36317;&#31163;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23558;&#31934;&#24515;&#35774;&#35745;&#30340;&#22122;&#22768;&#28155;&#21152;&#21040;&#36755;&#20837;&#20013;&#26469;&#20013;&#21644;&#21518;&#38376;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23398;&#20064;&#19968;&#20010;&#26368;&#20339;&#24179;&#28369;&#65288;&#22122;&#22768;&#65289;&#20998;&#24067;&#26469;&#20445;&#25345;&#30495;&#23454;&#36755;&#20837;&#30340;&#27491;&#24120;&#21151;&#33021;&#21516;&#26102;&#20013;&#21644;&#21518;&#38376;&#25915;&#20987;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;DRL&#22522;&#30784;&#30340;AVs&#20013;&#26356;&#22909;&#30340;&#21518;&#38376;&#25915;&#20987;&#26816;&#27979;&#21644;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) enhances the efficiency of Autonomous Vehicles (AV), but also makes them susceptible to backdoor attacks that can result in traffic congestion or collisions. Backdoor functionality is typically incorporated by contaminating training datasets with covert malicious data to maintain high precision on genuine inputs while inducing the desired (malicious) outputs for specific inputs chosen by adversaries. Current defenses against backdoors mainly focus on image classification using image-based features, which cannot be readily transferred to the regression task of DRL-based AV controllers since the inputs are continuous sensor data, i.e., the combinations of velocity and distance of AV and its surrounding vehicles. Our proposed method adds well-designed noise to the input to neutralize backdoors. The approach involves learning an optimal smoothing (noise) distribution to preserve the normal functionality of genuine inputs while neutralizing backdoors. By do
&lt;/p&gt;</description></item><item><title>DeepEpiSolver&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20316;&#20026;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#20272;&#35745;SIR&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#30456;&#23545;&#20110;Physics Informed Neural Networks (PINNs)&#26041;&#27861;&#65292;&#20854;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#65292;&#19988;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#30340;SIDR&#36712;&#36857;&#19978;&#65292;&#24182;&#22312; COVID-19&#12289;HIV&#12289;&#22467;&#21338;&#25289;&#21644;&#30142;&#30149;&#20256;&#25773;&#26041;&#38754;&#21462;&#24471;&#39564;&#35777;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.14194</link><description>&lt;p&gt;
DeepEpiSolver&#65306;&#25581;&#31034;Covid&#65292;HIV&#65292;Ebola&#21644;&#30142;&#30149;&#20256;&#25773;&#30340;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
DeepEpiSolver: Unravelling Inverse problems in Covid, HIV, Ebola and Disease Transmission. (arXiv:2303.14194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14194
&lt;/p&gt;
&lt;p&gt;
DeepEpiSolver&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20316;&#20026;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#20272;&#35745;SIR&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#30456;&#23545;&#20110;Physics Informed Neural Networks (PINNs)&#26041;&#27861;&#65292;&#20854;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#65292;&#19988;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#30340;SIDR&#36712;&#36857;&#19978;&#65292;&#24182;&#22312; COVID-19&#12289;HIV&#12289;&#22467;&#21338;&#25289;&#21644;&#30142;&#30149;&#20256;&#25773;&#26041;&#38754;&#21462;&#24471;&#39564;&#35777;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20256;&#26579;&#30149;&#30340;&#20256;&#25773;&#37117;&#26159;&#29992;SIR&#38548;&#23460;&#27169;&#22411;&#30340;&#21464;&#20307;&#36827;&#34892;&#24314;&#27169;&#30340;&#65292;&#35813;&#27169;&#22411;&#26159;&#19968;&#32452;&#32806;&#21512;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290; SIR&#27169;&#22411;&#30340;&#31995;&#25968;&#30830;&#23450;&#20102;&#30142;&#30149;&#20256;&#25773;&#36712;&#36857;&#65292;&#22522;&#20110;&#27492;&#21487;&#20197;&#37319;&#21462;&#31215;&#26497;&#25514;&#26045;&#12290;&#22240;&#27492;&#65292;&#31995;&#25968;&#20272;&#35745;&#24517;&#39035;&#26082;&#24555;&#21448;&#20934;&#30830;&#12290;Shaier&#31561;&#20154;&#22312;&#35770;&#25991;&#8220;&#30142;&#30149;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#8221;&#20013;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20272;&#35745;&#20102;SIR&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#26377;&#20004;&#20010;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;PINN&#30340;&#35757;&#32451;&#26102;&#38388;&#24456;&#38271;&#65292;&#26576;&#20123;&#30142;&#30149;&#38656;&#35201;&#25509;&#36817;90&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20854;&#27425;&#65292;PINN&#23545;&#20110;&#26032;&#30340;SIDR&#36712;&#36857;&#19981;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#23398;&#20064;&#20854;&#23545;&#24212;&#30340;SIR&#21442;&#25968;&#38656;&#35201;&#20174;&#22836;&#37325;&#26032;&#35757;&#32451;PINN&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#28040;&#38500;&#36825;&#20004;&#20010;&#32570;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;LSODA&#31639;&#27861;&#35299;&#20915;&#21442;&#25968;&#21644;&#20256;&#25773;&#36712;&#36857;&#20043;&#38388;&#30340;&#27491;&#21521;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20316;&#20026;&#21453;&#28436;&#27714;&#35299;&#22120;&#65292;&#26681;&#25454;&#20256;&#25773;&#36712;&#36857;&#20272;&#35745;SIR&#27169;&#22411;&#30340;&#27491;&#30830;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;DeepEpiSolver&#65292;&#27604;PINN&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#30340;SIDR&#36712;&#36857;&#19978;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;&#30142;&#30149;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;Covid&#65292;HIV&#65292;Ebola&#21644;&#30142;&#30149;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of many infectious diseases is modeled using variants of the SIR compartmental model, which is a coupled differential equation. The coefficients of the SIR model determine the spread trajectories of disease, on whose basis proactive measures can be taken. Hence, the coefficient estimates must be both fast and accurate. Shaier et al. in the paper "Disease Informed Neural Networks" used Physics Informed Neural Networks (PINNs) to estimate the parameters of the SIR model. There are two drawbacks to this approach. First, the training time for PINNs is high, with certain diseases taking close to 90 hrs to train. Second, PINNs don't generalize for a new SIDR trajectory, and learning its corresponding SIR parameters requires retraining the PINN from scratch. In this work, we aim to eliminate both of these drawbacks. We generate a dataset between the parameters of ODE and the spread trajectories by solving the forward problem for a large distribution of parameters using the LSODA al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;(Instance-adaptive Adversarial Training, IAAT)&#36890;&#36807;&#24179;&#28369;&#23454;&#20363;&#32423;&#21035;&#30340;&#23545;&#25239;&#24615;&#25439;&#22833;&#65292;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#8220;&#38590;&#8221;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#36991;&#20813;&#29306;&#29298;&#29305;&#23450;&#30340;&#26679;&#26412;&#32780;&#20559;&#29233;&#20854;&#20182;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19979;&#30340;&#26368;&#26032;&#12289;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#19979;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14077</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#20363;&#32423;&#25439;&#22833;&#24179;&#28369;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing. (arXiv:2303.14077v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;(Instance-adaptive Adversarial Training, IAAT)&#36890;&#36807;&#24179;&#28369;&#23454;&#20363;&#32423;&#21035;&#30340;&#23545;&#25239;&#24615;&#25439;&#22833;&#65292;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#8220;&#38590;&#8221;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#36991;&#20813;&#29306;&#29298;&#29305;&#23450;&#30340;&#26679;&#26412;&#32780;&#20559;&#29233;&#20854;&#20182;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19979;&#30340;&#26368;&#26032;&#12289;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#19979;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#36755;&#20837;&#36827;&#34892;&#23545;&#25239;&#25200;&#21160;&#65306;&#21363;&#20154;&#31867;&#38590;&#20197;&#23519;&#35273;&#30340;&#20154;&#36896;&#22122;&#22768;&#65292;&#21487;&#20197;&#36731;&#26131;&#22320;&#36855;&#24785;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#32780;&#20570;&#20986;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#30446;&#21069;&#23545;&#25239;&#35757;&#32451;&#24050;&#25104;&#20026;&#26368;&#25104;&#21151;&#30340;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#26412;&#25991;&#33268;&#21147;&#20110;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;&#20197;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#20174;&#23454;&#20363;&#32423;&#21035;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#23545;&#25239;&#35757;&#32451;&#26399;&#38388;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#28436;&#21464;&#12290;&#21457;&#29616;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#36890;&#36807;&#29306;&#29298;&#30456;&#24403;&#27604;&#20363;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25239;&#24615;&#25439;&#22833;&#30340;&#25972;&#20307;&#38477;&#20302;&#65292;&#36825;&#23548;&#33268;&#20102;&#19981;&#21516;&#25968;&#25454;&#30340;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#20998;&#24067;&#19981;&#22343;&#34913;&#12290;&#36825;&#31181;&#8220;&#19981;&#22343;&#34913;&#33030;&#24369;&#24615;&#8221;&#22312;&#20960;&#31181;&#27969;&#34892;&#30340;&#40065;&#26834;&#24615;&#35757;&#32451;&#26041;&#27861;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#19982;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#36807;&#25311;&#21512;&#30456;&#20851;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65306;Instance-adaptive Adversarial Training (IAAT)&#12290;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#28369;&#23454;&#20363;&#32423;&#21035;&#30340;&#23545;&#25239;&#24615;&#25439;&#22833;&#65292;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#8220;&#38590;&#8221;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#36991;&#20813;&#29306;&#29298;&#29305;&#23450;&#30340;&#26679;&#26412;&#32780;&#20559;&#29233;&#20854;&#20182;&#26679;&#26412;&#12290;&#26412;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19979;&#37117;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#19979;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks can be easily fooled into making incorrect predictions through corruption of the input by adversarial perturbations: human-imperceptible artificial noise. So far adversarial training has been the most successful defense against such adversarial attacks. This work focuses on improving adversarial training to boost adversarial robustness. We first analyze, from an instance-wise perspective, how adversarial vulnerability evolves during adversarial training. We find that during training an overall reduction of adversarial loss is achieved by sacrificing a considerable proportion of training samples to be more vulnerable to adversarial attack, which results in an uneven distribution of adversarial vulnerability among data. Such "uneven vulnerability", is prevalent across several popular robust training methods and, more importantly, relates to overfitting in adversarial training. Motivated by this observation, we propose a new adversarial training method: Instance-adapt
&lt;/p&gt;</description></item><item><title>Topograph&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#31890;&#23376;&#34928;&#21464;&#33258;&#28982;&#35268;&#24459;&#30340;&#25299;&#25169;&#32467;&#26500;&#37325;&#24314;&#26041;&#27861;&#65292;&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#36824;&#39044;&#27979;&#20102;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#65292;&#27604;&#26631;&#20934;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#65292;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2303.13937</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#31890;&#23376;&#29289;&#29702;&#36807;&#31243;&#30340;&#25299;&#25169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Topological Reconstruction of Particle Physics Processes using Graph Neural Networks. (arXiv:2303.13937v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13937
&lt;/p&gt;
&lt;p&gt;
Topograph&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#31890;&#23376;&#34928;&#21464;&#33258;&#28982;&#35268;&#24459;&#30340;&#25299;&#25169;&#32467;&#26500;&#37325;&#24314;&#26041;&#27861;&#65292;&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#36824;&#39044;&#27979;&#20102;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#65292;&#27604;&#26631;&#20934;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#65292;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Topograph&#65292;&#23427;&#21033;&#29992;&#31890;&#23376;&#29289;&#29702;&#34928;&#21464;&#30340;&#26412;&#36136;&#21644;&#20449;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#65292;&#37325;&#24314;&#20102;&#21253;&#25324;&#20013;&#20171;&#31890;&#23376;&#22312;&#20869;&#30340;&#24213;&#23618;&#29289;&#29702;&#36807;&#31243;&#12290;Topograph&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#30340;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#23558;&#23427;&#20204;&#19982;&#23427;&#20204;&#21407;&#26469;&#30340;&#27597;&#31890;&#23376;&#20851;&#32852;&#36215;&#26469;&#65292;&#32780;&#19988;&#30452;&#25509;&#39044;&#27979;&#20102;&#30828;&#25955;&#23556;&#36807;&#31243;&#20013;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#12290;&#19982;&#26631;&#20934;&#30340;&#32452;&#21512;&#26041;&#27861;&#25110;&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#30340;&#22797;&#26434;&#24230;&#19982;&#37325;&#26500;&#23545;&#35937;&#30340;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#24212;&#29992;Topograph&#20110;&#20840;&#24378;&#23376;&#34928;&#21464;&#27169;&#24335;&#19979;&#30340;&#39030;&#22840;&#20811;&#23545;&#20135;&#29983;&#38382;&#39064;&#65292;&#30456;&#23545;&#26631;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new approach, the Topograph, which reconstructs underlying physics processes, including the intermediary particles, by leveraging underlying priors from the nature of particle physics decays and the flexibility of message passing graph neural networks. The Topograph not only solves the combinatoric assignment of observed final state objects, associating them to their original mother particles, but directly predicts the properties of intermediate particles in hard scatter processes and their subsequent decays. In comparison to standard combinatoric approaches or modern approaches using graph neural networks, which scale exponentially or quadratically, the complexity of Topographs scales linearly with the number of reconstructed objects.  We apply Topographs to top quark pair production in the all hadronic decay channel, where we outperform the standard approach and match the performance of the state-of-the-art machine learning technique.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;</title><link>http://arxiv.org/abs/2303.13763</link><description>&lt;p&gt;
&#26080;&#38656;&#36793;&#32536;&#20294;&#20855;&#26377;&#32467;&#26500;&#24863;&#30693;&#24615;&#65306;&#20174;GNN&#21040;MLP&#30340;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs. (arXiv:2303.13763v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#31934;&#24230;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#20219;&#21153;&#20013;&#21387;&#32553;&#25104;&#20302;&#24310;&#36831;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#24050;&#25104;&#20026;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#20250;&#23558;&#22270;&#30340;&#36793;&#32536;&#22788;&#29702;&#25104;&#39069;&#22806;&#30340;&#36755;&#20837;&#32473;MLP&#65292;&#20294;&#36825;&#26679;&#30340;&#22270;&#32467;&#26500;&#23545;&#20110;&#21508;&#31181;&#22330;&#26223;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GNN&#25945;&#24072;&#20013;&#30340;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21407;&#22411;&#22312;&#26080;&#36793;&#32536;&#35774;&#32622;&#20013;&#20174;GNN&#21040;MLP&#36827;&#34892;&#20102;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#27969;&#34892;&#30340;&#22270;&#24418;&#22522;&#20934;&#23454;&#39564;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PGKD&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic. However, MLPs rely exclusively on the node features and fail to capture the graph structural information. Previous methods address this issue by processing graph edges into extra inputs for MLPs, but such graph structures may be unavailable for various scenarios. To this end, we propose a Prototype-Guided Knowledge Distillation~(PGKD) method, which does not require graph edges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the graph structural information in GNN teachers, and distill such information from GNNs to MLPs via prototypes in an edge-free setting. Experimental results on popular graph benchmarks demonstrate the effectiveness and robustness of the proposed PGKD.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20135;&#21697;&#29983;&#20135;&#21644;&#20351;&#29992;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26816;&#27979;&#20135;&#21697;&#30340;&#30952;&#25439;&#29366;&#24577;&#24182;&#29992;&#20110;&#25913;&#36827;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#30340;&#38598;&#25104;&#21644;&#32467;&#26524;&#21462;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.13540</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;: &#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20419;&#36827;&#21487;&#25345;&#32493;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Sustainability: Facilitating Sustainable Smart Product-Service Systems with Computer Vision. (arXiv:2303.13540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20135;&#21697;&#29983;&#20135;&#21644;&#20351;&#29992;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26816;&#27979;&#20135;&#21697;&#30340;&#30952;&#25439;&#29366;&#24577;&#24182;&#29992;&#20110;&#25913;&#36827;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#30340;&#38598;&#25104;&#21644;&#32467;&#26524;&#21462;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#23454;&#29616;&#28165;&#27905;&#29983;&#20135;&#21644;&#21487;&#25345;&#32493;&#24615;&#30446;&#30340;&#30340;&#30740;&#31350;&#36824;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20135;&#21697;&#29983;&#20135;&#21644;&#20351;&#29992;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#23588;&#20854;&#26159;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26469;&#35782;&#21035;&#20135;&#21697;&#30340;&#30952;&#25439;&#29366;&#24577;&#65292;&#24182;&#23558;&#36825;&#20123;&#32467;&#26524;&#29992;&#20110;&#25913;&#36827;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#30340;&#38598;&#25104;&#21644;&#32467;&#26524;&#21462;&#21521;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25104;&#26524;&#39044;&#35745;&#23558;&#20419;&#36827;&#20135;&#21697;&#20351;&#29992;&#30340;&#25913;&#36827;&#21644;&#30740;&#21457;&#21019;&#26032;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#20135;&#21697;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;:&#21152;&#24037;&#24037;&#20855;&#21644;&#26059;&#36716;X&#23556;&#32447;&#38451;&#26497;&#12290;
&lt;/p&gt;
&lt;p&gt;
The usage and impact of deep learning for cleaner production and sustainability purposes remain little explored. This work shows how deep learning can be harnessed to increase sustainability in production and product usage. Specifically, we utilize deep learning-based computer vision to determine the wear states of products. The resulting insights serve as a basis for novel product-service systems with improved integration and result orientation. Moreover, these insights are expected to facilitate product usage improvements and R&amp;D innovations. We demonstrate our approach on two products: machining tools and rotating X-ray anodes. From a technical standpoint, we show that it is possible to recognize the wear state of these products using deep-learning-based computer vision. In particular, we detect wear through microscopic images of the two products. We utilize a U-Net for semantic segmentation to detect wear based on pixel granularity. The resulting mean dice coefficients of 0.631 and
&lt;/p&gt;</description></item><item><title>Xplainer&#26159;&#19968;&#20010;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#35786;&#26029;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#23384;&#22312;&#30340;&#25551;&#36848;&#24615;&#35266;&#23519;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#33258;&#21160;&#35786;&#26029;&#38598;&#25104;&#21040;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13391</link><description>&lt;p&gt;
Xplainer&#65306;&#20174;X&#23556;&#32447;&#35266;&#23519;&#21040;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis. (arXiv:2303.13391v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13391
&lt;/p&gt;
&lt;p&gt;
Xplainer&#26159;&#19968;&#20010;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#35786;&#26029;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#23384;&#22312;&#30340;&#25551;&#36848;&#24615;&#35266;&#23519;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#33258;&#21160;&#35786;&#26029;&#38598;&#25104;&#21040;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#33258;&#21160;&#35786;&#26029;&#39044;&#27979;&#65292;&#26159;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#22312;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#21307;&#23398;&#39046;&#22495;&#30340;&#27880;&#37322;&#25968;&#25454;&#24448;&#24448;&#24456;&#23569;&#12290;&#38646;&#26679;&#26412;&#26041;&#27861;&#36890;&#36807;&#20801;&#35768;&#22312;&#19981;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#20020;&#24202;&#32467;&#26524;&#30340;&#26032;&#35774;&#32622;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23558;&#33258;&#21160;&#35786;&#26029;&#38598;&#25104;&#21040;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#26041;&#27861;&#24212;&#35813;&#26159;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#65292;&#22686;&#21152;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#20449;&#20219;&#24182;&#20419;&#36827;&#27491;&#30830;&#24615;&#39564;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Xplainer&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#35786;&#26029;&#30340;&#26032;&#26694;&#26550;&#12290;Xplainer&#23558;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#31867;&#21363;&#25551;&#36848;&#26041;&#27861;&#36866;&#24212;&#20110;&#22810;&#26631;&#31614;&#21307;&#23398;&#35786;&#26029;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#31034;&#27169;&#22411;&#23545;&#23384;&#22312;&#30340;&#25551;&#36848;&#24615;&#35266;&#23519;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#39044;&#27979;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated diagnosis prediction from medical images is a valuable resource to support clinical decision-making. However, such systems usually need to be trained on large amounts of annotated data, which often is scarce in the medical domain. Zero-shot methods address this challenge by allowing a flexible adaption to new settings with different clinical findings without relying on labeled data. Further, to integrate automated diagnosis in the clinical workflow, methods should be transparent and explainable, increasing medical professionals' trust and facilitating correctness verification. In this work, we introduce Xplainer, a novel framework for explainable zero-shot diagnosis in the clinical setting. Xplainer adapts the classification-by-description approach of contrastive vision-language models to the multi-label medical diagnosis task. Specifically, instead of directly predicting a diagnosis, we prompt the model to classify the existence of descriptive observations, which a radiologi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.</title><link>http://arxiv.org/abs/2303.13035</link><description>&lt;p&gt;
SPeC&#65306;&#36719;&#25552;&#31034;&#26657;&#20934;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13035
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#23384;&#20648;&#30528;&#21253;&#25324;&#30149;&#21382;&#12289;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#26816;&#27979;&#32467;&#26524;&#22312;&#20869;&#30340;&#22823;&#37327;&#24739;&#32773;&#20449;&#24687;&#12290;&#36825;&#20123;&#35760;&#24405;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20570;&#20986;&#26126;&#26234;&#30340;&#24739;&#32773;&#25252;&#29702;&#20915;&#31574;&#38750;&#24120;&#20851;&#38190;&#12290;&#25688;&#35201;&#20020;&#24202;&#31508;&#35760;&#21487;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#26356;&#22909;&#22320;&#21457;&#29616;&#28508;&#22312;&#20581;&#24247;&#39118;&#38505;&#65292;&#20197;&#21450;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#36825;&#19968;&#36807;&#31243;&#36890;&#36807;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#35775;&#38382;&#26368;&#30456;&#20851;&#21644;&#26368;&#26032;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#38169;&#35823;&#24182;&#25552;&#39640;&#24739;&#32773;&#30340;&#25252;&#29702;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#25552;&#31034;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#20063;&#20250;&#23548;&#33268;&#36755;&#20986;&#26041;&#24046;&#22686;&#21152;&#65292;&#21363;&#20351;&#25552;&#31034;&#24847;&#20041;&#30456;&#20284;&#65292;&#36755;&#20986;&#20063;&#20250;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#36719;&#25552;&#31034;&#26657;&#20934;&#65288;SPeC&#65289;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#37319;&#29992;&#36719;&#25552;&#31034;&#23884;&#20837;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SPeC&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;LLM&#30340;&#24615;&#33021;&#21464;&#24322;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the efficacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prom
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#36229;&#36807;80&#20010;&#22312;&#38750;&#25104;&#20687; EMR &#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22823;&#22810;&#33539;&#22260;&#26377;&#38480;&#12289;&#35757;&#32451;&#38598;&#26377;&#38480;&#65292;&#19988;&#35780;&#20272;&#25351;&#26631;&#26410;&#23545;&#20854;&#23545;&#21307;&#30103;&#31995;&#32479;&#36129;&#29486;&#25552;&#20379;&#26377;&#24847;&#20041;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#25509;&#36817;&#20110;&#21307;&#30103;&#20445;&#20581;&#37325;&#35201;&#25351;&#26631;&#30340;&#21307;&#30103;&#22522;&#30784;&#27169;&#22411;&#25928;&#30410;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2303.12961</link><description>&lt;p&gt;
&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;&#30340;&#19981;&#31283;&#23450;&#22522;&#30784;&#65306;&#38024;&#23545; EMR &#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs. (arXiv:2303.12961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#36229;&#36807;80&#20010;&#22312;&#38750;&#25104;&#20687; EMR &#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22823;&#22810;&#33539;&#22260;&#26377;&#38480;&#12289;&#35757;&#32451;&#38598;&#26377;&#38480;&#65292;&#19988;&#35780;&#20272;&#25351;&#26631;&#26410;&#23545;&#20854;&#23545;&#21307;&#30103;&#31995;&#32479;&#36129;&#29486;&#25552;&#20379;&#26377;&#24847;&#20041;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#25509;&#36817;&#20110;&#21307;&#30103;&#20445;&#20581;&#37325;&#35201;&#25351;&#26631;&#30340;&#21307;&#30103;&#22522;&#30784;&#27169;&#22411;&#25928;&#30410;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284; ChatGPT &#21644; AlphaFold &#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20110;&#26500;&#24314;&#31867;&#20284;&#27169;&#22411;&#20197;&#25913;&#21892; EMR&#65288;&#30005;&#23376;&#30149;&#21382;&#65289;&#20197;&#25552;&#39640;&#24739;&#32773;&#25252;&#29702;&#21644;&#21307;&#38498;&#36816;&#33829;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#28818;&#20316;&#25513;&#30422;&#20102;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#33021;&#21147;&#30340;&#20851;&#38190;&#32570;&#22833;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#36229;&#36807;80&#20010;&#22312;&#38750;&#25104;&#20687; EMR &#25968;&#25454;&#65288;&#21363;&#20020;&#24202;&#25991;&#26412;&#21644;/&#25110;&#32467;&#26500;&#21270;&#25968;&#25454;&#65289;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#26469;&#35828;&#26126;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#28508;&#22312;&#29992;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26159;&#22312;&#23567;&#22411;&#12289;&#33539;&#22260;&#26377;&#38480;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;MIMIC-III&#65289;&#25110;&#24191;&#27867;&#30340;&#20844;&#20849;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#65288;&#20363;&#22914;PubMed&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#25552;&#20379;&#23545;&#20854;&#23545;&#21307;&#30103;&#31995;&#32479;&#26377;&#29992;&#22788;&#30340;&#26377;&#24847;&#20041;&#35265;&#35299;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#25509;&#36817;&#20110;&#21307;&#30103;&#20445;&#20581;&#37325;&#35201;&#25351;&#26631;&#30340;&#21307;&#30103;&#22522;&#30784;&#27169;&#22411;&#25928;&#30410;&#30340;&#25913;&#36827;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The successes of foundation models such as ChatGPT and AlphaFold have spurred significant interest in building similar models for electronic medical records (EMRs) to improve patient care and hospital operations. However, recent hype has obscured critical gaps in our understanding of these models' capabilities. We review over 80 foundation models trained on non-imaging EMR data (i.e. clinical text and/or structured data) and create a taxonomy delineating their architectures, training data, and potential use cases. We find that most models are trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or broad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that do not provide meaningful insights on their usefulness to health systems. In light of these findings, we propose an improved evaluation framework for measuring the benefits of clinical foundation models that is more closely grounded to metrics that matter in healthcare.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#19982;&#26631;&#20934;&#31639;&#27861;&#31283;&#23450;&#24615;&#30340;&#32852;&#31995;&#65292;&#20026;&#19968;&#31867;&#32479;&#35745;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#30340;&#26679;&#26412;&#26377;&#25928;&#31639;&#27861;&#32422;&#21270;&#65292;&#21516;&#26102;&#34920;&#26126;&#36825;&#31181;&#31561;&#20215;&#20851;&#31995;&#24517;&#39035;&#22312;&#35745;&#31639;&#19978;&#23849;&#28291;&#12290;</title><link>http://arxiv.org/abs/2303.12921</link><description>&lt;p&gt;
&#31283;&#23450;&#24615;&#31283;&#23450;&#65306;&#21487;&#22797;&#21046;&#24615;&#12289;&#38544;&#31169;&#21644;&#33258;&#36866;&#24212;&#25512;&#24191;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Stability is Stable: Connections between Replicability, Privacy, and Adaptive Generalization. (arXiv:2303.12921v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#19982;&#26631;&#20934;&#31639;&#27861;&#31283;&#23450;&#24615;&#30340;&#32852;&#31995;&#65292;&#20026;&#19968;&#31867;&#32479;&#35745;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#30340;&#26679;&#26412;&#26377;&#25928;&#31639;&#27861;&#32422;&#21270;&#65292;&#21516;&#26102;&#34920;&#26126;&#36825;&#31181;&#31561;&#20215;&#20851;&#31995;&#24517;&#39035;&#22312;&#35745;&#31639;&#19978;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Impagliazzo et al. [STOC '22]&#20013;&#24341;&#20837;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#25551;&#36848;&#22312;&#36755;&#20837;&#37325;&#26032;&#37319;&#26679;&#26102;&#31283;&#23450;&#30340;&#38543;&#26426;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#20854;&#38543;&#26426;&#24615;&#34987;&#22266;&#23450;&#19988;&#22312;&#20174;&#30456;&#21516;&#20998;&#24067;&#20013;&#32472;&#21046;&#30340;&#26032;&#30340;i.i.d.&#26679;&#26412;&#19978;&#36816;&#34892;&#26102;&#65292;&#21487;&#22797;&#21046;&#31639;&#27861;&#20250;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#32473;&#20986;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#20351;&#29992;&#21487;&#22797;&#21046;&#31639;&#27861;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#21487;&#20197;&#36890;&#36807;&#30830;&#20445;&#20998;&#26512;&#32467;&#26524;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20998;&#26512;&#26102;&#20855;&#26377;&#30456;&#21516;&#30340;&#32467;&#26524;&#26469;&#31616;&#21270;&#24050;&#21457;&#24067;&#32467;&#26524;&#30340;&#39564;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21487;&#22797;&#21046;&#24615;&#19982;&#31639;&#27861;&#31283;&#23450;&#24615;&#26631;&#20934;&#27010;&#24565;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#21644;&#20998;&#31163;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20026;&#19968;&#31867;&#24191;&#27867;&#30340;&#32479;&#35745;&#38382;&#39064;&#32473;&#20986;&#20102;&#23436;&#32654;&#25512;&#24191;&#12289;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#21644;&#21487;&#22797;&#21046;&#24615;&#20043;&#38388;&#30340;&#26679;&#26412;&#26377;&#25928;&#31639;&#27861;&#32422;&#21270;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#31561;&#20215;&#20851;&#31995;&#24517;&#39035;&#22312;&#35745;&#31639;&#19978;&#23849;&#28291;&#65306;&#23384;&#22312;&#20855;&#26377;&#21487;&#22797;&#21046;&#31639;&#27861;&#20294;&#19981;&#20855;&#26377;&#20219;&#20309;&#21487;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The notion of replicable algorithms was introduced in Impagliazzo et al. [STOC '22] to describe randomized algorithms that are stable under the resampling of their inputs. More precisely, a replicable algorithm gives the same output with high probability when its randomness is fixed and it is run on a new i.i.d. sample drawn from the same distribution. Using replicable algorithms for data analysis can facilitate the verification of published results by ensuring that the results of an analysis will be the same with high probability, even when that analysis is performed on a new data set.  In this work, we establish new connections and separations between replicability and standard notions of algorithmic stability. In particular, we give sample-efficient algorithmic reductions between perfect generalization, approximate differential privacy, and replicability for a broad class of statistical problems. Conversely, we show any such equivalence must break down computationally: there exist s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31435;&#26041;&#20307;&#30340;3D&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26469;&#37325;&#24314;CBCT&#24182;&#35299;&#20915;&#20102;&#23384;&#20648;&#25972;&#20010;&#27491;&#24358;&#22270;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#25972;&#20010;CBCT volume&#20998;&#25104;&#22810;&#20010;&#23567;&#31435;&#26041;&#20307;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#24182;&#22312;&#35270;&#35273;&#21644;&#23450;&#37327;&#35780;&#20215;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12861</link><description>&lt;p&gt;
&#38754;&#21521;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#38181;&#26463;CT&#37325;&#24314;&#30340;&#22522;&#20110;&#31435;&#26041;&#20307;&#30340;3D&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cube-Based 3D Denoising Diffusion Probabilistic Model for Cone Beam Computed Tomography Reconstruction with Incomplete Data. (arXiv:2303.12861v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31435;&#26041;&#20307;&#30340;3D&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26469;&#37325;&#24314;CBCT&#24182;&#35299;&#20915;&#20102;&#23384;&#20648;&#25972;&#20010;&#27491;&#24358;&#22270;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#25972;&#20010;CBCT volume&#20998;&#25104;&#22810;&#20010;&#23567;&#31435;&#26041;&#20307;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#24182;&#22312;&#35270;&#35273;&#21644;&#23450;&#37327;&#35780;&#20215;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25668;&#24433;&#65288;CT&#65289;&#37325;&#24314;&#20013;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#35270;&#22270;CT&#37325;&#24314;&#20013;&#12290;&#28982;&#32780;&#65292;&#23558;DL&#24212;&#29992;&#20110;&#31232;&#30095;&#35270;&#22270;&#38181;&#26463;CT&#65288;CBCT&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31435;&#26041;&#20307;&#30340;3D&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26469;&#37325;&#24314;CBCT&#65292;&#24182;&#35299;&#20915;&#20102;&#23384;&#20648;&#25972;&#20010;&#27491;&#24358;&#22270;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25972;&#20010;CBCT volume&#20998;&#25104;&#22810;&#20010;&#23567;&#31435;&#26041;&#20307;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35270;&#35273;&#21644;&#23450;&#37327;&#35780;&#20215;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been extensively researched in the field of computed tomography (CT) reconstruction with incomplete data, particularly in sparse-view CT reconstruction. However, applying DL to sparse-view cone beam CT (CBCT) remains challenging. Many models learn the mapping from sparse-view CT images to ground truth but struggle to achieve satisfactory performance in terms of global artifact removal. Incorporating sinogram data and utilizing dual-domain information can enhance anti-artifact performance, but this requires storing the entire sinogram in memory. This presents a memory issue for high-resolution CBCT sinograms, limiting further research and application. In this paper, we propose a cube-based 3D denoising diffusion probabilistic model (DDPM) for CBCT reconstruction using down-sampled data. A DDPM network, trained on cubes extracted from paired fully sampled sinograms and down-sampled sinograms, is employed to inpaint down-sampled sinograms. Our method divides the ent
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#21644;&#37325;&#26500;&#65292;&#19981;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#38388;&#26356;&#26032;&#27169;&#22411;&#26435;&#37325;&#65292;&#20063;&#19981;&#38656;&#35201;&#20351;&#29992;&#26356;&#22810;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#22686;&#24378;&#35757;&#32451;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.12848</link><description>&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#30340;&#27979;&#35797;&#26102;&#38388;&#38450;&#24481;&#65306;&#22522;&#20110;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#21644;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Test-time Defense against Adversarial Attacks: Detection and Reconstruction of Adversarial Examples via Masked Autoencoder. (arXiv:2303.12848v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12848
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#21644;&#37325;&#26500;&#65292;&#19981;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#38388;&#26356;&#26032;&#27169;&#22411;&#26435;&#37325;&#65292;&#20063;&#19981;&#38656;&#35201;&#20351;&#29992;&#26356;&#22810;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#22686;&#24378;&#35757;&#32451;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#35757;&#32451;&#26102;&#38388;&#21644;&#27979;&#35797;&#26102;&#38388;&#38450;&#24481;&#12290;&#35757;&#32451;&#26102;&#38388;&#38450;&#24481;&#38656;&#35201;&#22823;&#37327;&#30340;&#39069;&#22806;&#35757;&#32451;&#26102;&#38388;&#65292;&#36890;&#24120;&#26080;&#27861;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#25915;&#20987;&#12290;&#32780;&#27979;&#35797;&#26102;&#38388;&#38450;&#24481;&#38656;&#35201;&#35775;&#38382;&#65288;&#37096;&#20998;&#65289;&#27169;&#22411;&#26435;&#37325;&#20197;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#36825;&#23545;&#20110;&#20923;&#32467;&#26435;&#37325;&#30340;&#27169;&#22411;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;DRAM&#65292;&#23427;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#26816;&#27979;&#24182;&#37325;&#26500;&#22810;&#31181;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;MAE&#25439;&#22833;&#26500;&#24314;KS&#27979;&#35797;&#26469;&#26816;&#27979;&#23545;&#25239;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;MAE&#25439;&#22833;&#21487;&#20197;&#29992;&#20110;&#20462;&#22797;&#26410;&#35265;&#25915;&#20987;&#31867;&#22411;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;DRAM&#26082;&#19981;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#38388;&#26356;&#26032;&#27169;&#22411;&#26435;&#37325;&#65292;&#20063;&#19981;&#38656;&#35201;&#20351;&#29992;&#26356;&#22810;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#22686;&#24378;&#35757;&#32451;&#38598;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;ImageN&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;DRAM&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#24456;&#39640;&#30340;&#40065;&#26834;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing defense methods against adversarial attacks can be categorized into training time and test time defenses. Training time defense, i.e., adversarial training, requires a significant amount of extra time for training and is often not able to be generalized to unseen attacks. On the other hand, test time defense by test time weight adaptation requires access to perform gradient descent on (part of) the model weights, which could be infeasible for models with frozen weights. To address these challenges, we propose DRAM, a novel defense method to Detect and Reconstruct multiple types of Adversarial attacks via Masked autoencoder (MAE). We demonstrate how to use MAE losses to build a KS-test to detect adversarial attacks. Moreover, the MAE losses can be used to repair adversarial samples from unseen attack types. In this sense, DRAM neither requires model weight updates in test time nor augments the training set with more adversarial samples. Evaluating DRAM on the large-scale ImageN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;DPPMask&#65292;&#29992;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPPs&#65289;&#26367;&#25442;&#20102;&#38543;&#26426;&#36807;&#31243;&#20197;&#20943;&#23569;&#36974;&#30422;&#21518;&#22270;&#20687;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#20195;&#34920;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12736</link><description>&lt;p&gt;
DPPMask&#65306;&#22522;&#20110;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DPPMask: Masked Image Modeling with Determinantal Point Processes. (arXiv:2303.12736v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;DPPMask&#65292;&#29992;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPPs&#65289;&#26367;&#25442;&#20102;&#38543;&#26426;&#36807;&#31243;&#20197;&#20943;&#23569;&#36974;&#30422;&#21518;&#22270;&#20687;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#20195;&#34920;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#26088;&#22312;&#37325;&#24314;&#38543;&#26426;&#36974;&#30422;&#30340;&#22270;&#20687;&#65292;&#24182;&#24050;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#20195;&#34920;&#24615;&#33021;&#12290;&#23613;&#31649;&#20855;&#26377;&#23454;&#35777;&#25928;&#26524;&#65292;&#20294;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#24573;&#30053;&#20102;&#19968;&#20010;&#37325;&#35201;&#20107;&#23454;&#65292;&#21363;&#24378;&#21046;&#27169;&#22411;&#37325;&#24314;&#36229;&#20986;&#24674;&#22797;&#33539;&#22260;&#30340;&#29289;&#20307;&#65288;&#22914;&#37027;&#20123;&#34987;&#36974;&#30422;&#30340;&#29289;&#20307;&#65289;&#26159;&#19981;&#21512;&#29702;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22343;&#21248;&#38543;&#26426;&#36974;&#30422;&#19981;&#21487;&#36991;&#20813;&#22320;&#20002;&#22833;&#19968;&#20123;&#20851;&#38190;&#29289;&#20307;&#24182;&#26356;&#25913;&#21407;&#22987;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#31181;&#19981;&#23545;&#40784;&#38382;&#39064;&#24182;&#26368;&#32456;&#20260;&#23475;&#20102;&#20195;&#34920;&#24615;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPPs&#65289;&#26367;&#25442;&#20026;&#38543;&#26426;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36974;&#30422;&#31574;&#30053;&#65292;&#21363;DPPMask&#65292;&#20197;&#20943;&#23569;&#36974;&#30422;&#21518;&#22270;&#20687;&#30340;&#35821;&#20041;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#23454;&#29616;&#26102;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#25105;&#20204;&#29305;&#21035;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;MIM&#26694;&#26550;&#65292;MASK&#21644;GMS&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#20195;&#34920;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Image Modeling (MIM) has achieved impressive representative performance with the aim of reconstructing randomly masked images. Despite the empirical success, most previous works have neglected the important fact that it is unreasonable to force the model to reconstruct something beyond recovery, such as those masked objects. In this work, we show that uniformly random masking widely used in previous works unavoidably loses some key objects and changes original semantic information, resulting in a misalignment problem and hurting the representative learning eventually. To address this issue, we augment MIM with a new masking strategy namely the DPPMask by substituting the random process with Determinantal Point Process (DPPs) to reduce the semantic change of the image after masking. Our method is simple yet effective and requires no extra learnable parameters when implemented within various frameworks. In particular, we evaluate our method on two representative MIM frameworks, MA
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#21253;&#25324;&#22235;&#31181;&#31867;&#22411;&#30340;&#27665;&#20027;&#21270;&#65306;AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#35201;&#24819;&#23454;&#29616;&#26377;&#25928;&#30340;&#25919;&#31574;&#21644;&#26435;&#34913;&#35752;&#35770;&#65292;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#20915;&#31574;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2303.12642</link><description>&lt;p&gt;
&#26234;&#33021;&#21270;&#30340;&#27665;&#20027;&#21270;&#65306;&#22810;&#31181;&#21547;&#20041;&#12289;&#30446;&#26631;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Democratising AI: Multiple Meanings, Goals, and Methods. (arXiv:2303.12642v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12642
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#21253;&#25324;&#22235;&#31181;&#31867;&#22411;&#30340;&#27665;&#20027;&#21270;&#65306;AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#35201;&#24819;&#23454;&#29616;&#26377;&#25928;&#30340;&#25919;&#31574;&#21644;&#26435;&#34913;&#35752;&#35770;&#65292;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#20915;&#31574;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#21628;&#21505;&#23454;&#29616;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#20294;&#36825;&#20010;&#35789;&#35821;&#29992;&#26469;&#25351;&#20195;&#22810;&#31181;&#30446;&#26631;&#65292;&#26377;&#26102;&#20250;&#30456;&#20114;&#20914;&#31361;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#36890;&#24120;&#35752;&#35770;&#30340;&#22235;&#31181;AI&#27665;&#20027;&#21270;&#31867;&#22411;&#65306;(1) AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;(2) AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;(3) AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;(4) AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#23454;&#29616;&#27599;&#31181;&#27665;&#20027;&#21270;&#24418;&#24335;&#30340;&#22810;&#20010;&#30446;&#26631;&#21644;&#26041;&#27861;&#12290;&#20174;&#26412;&#25991;&#20013;&#20027;&#35201;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;AI&#30340;&#27665;&#20027;&#21270;&#26159;&#19968;&#20010;&#22810;&#20803;&#32780;&#26377;&#26102;&#20250;&#30456;&#20114;&#20914;&#31361;&#30340;&#27010;&#24565;&#65292;&#19981;&#24212;&#28151;&#28102;AI&#21487;&#35775;&#38382;&#24615;&#30340;&#25913;&#21892;&#12290;&#22914;&#26524;&#25105;&#20204;&#24819;&#35201;&#36229;&#36234;&#23545;&#26234;&#33021;&#21270;&#27665;&#20027;&#21270;&#30340;&#27169;&#31946;&#25215;&#35834;&#65292;&#36827;&#20837;&#20855;&#20307;&#25919;&#31574;&#21644;&#26435;&#34913;&#30340;&#29983;&#20135;&#24615;&#35752;&#35770;&#65292;&#25105;&#20204;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#36328;&#36234;&#20851;&#20110;&#20351;&#29992;&#12289;&#24320;&#21457;&#21644;&#21033;&#28070;&#30340;&#20915;&#31574;&#20013;&#23548;&#33322;&#26435;&#34913;&#21644;&#39118;&#38505;&#30340;&#20027;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous parties are calling for the democratisation of AI, but the phrase is used to refer to a variety of goals, the pursuit of which sometimes conflict. This paper identifies four kinds of AI democratisation that are commonly discussed: (1) the democratisation of AI use, (2) the democratisation of AI development, (3) the democratisation of AI profits, and (4) the democratisation of AI governance. Numerous goals and methods of achieving each form of democratisation are discussed. The main takeaway from this paper is that AI democratisation is a multifarious and sometimes conflicting concept that should not be conflated with improving AI accessibility. If we want to move beyond ambiguous commitments to democratising AI, to productive discussions of concrete policies and trade-offs, then we need to recognise the principal role of the democratisation of AI governance in navigating tradeoffs and risks across decisions around use, development, and profits.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#23616;&#37096;&#25193;&#23637;&#21040;&#20840;&#22495;&#21644;&#22810;&#23610;&#24230;&#33539;&#22260;&#20869;&#65292;&#21462;&#24471;&#20102;&#27604;ViT&#21644;AFNO&#26356;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.12398</link><description>&lt;p&gt;
&#36890;&#36807;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23454;&#29616;Transformers&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Multiscale Attention via Wavelet Neural Operators for Vision Transformers. (arXiv:2303.12398v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#23616;&#37096;&#25193;&#23637;&#21040;&#20840;&#22495;&#21644;&#22810;&#23610;&#24230;&#33539;&#22260;&#20869;&#65292;&#21462;&#24471;&#20102;&#27604;ViT&#21644;AFNO&#26356;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#12290;&#20854;&#26680;&#24515;&#26159;&#33258;&#27880;&#24847;&#26426;&#21046;&#65288;SA&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#24402;&#32435;&#20559;&#35265;&#65292;&#36890;&#36807;&#21152;&#26435;&#22522;&#30784;&#23558;&#36755;&#20837;&#20013;&#30340;&#27599;&#20010;token&#19982;&#27599;&#20010;&#20854;&#20182;token&#30456;&#20851;&#32852;&#12290;&#26631;&#20934;&#30340;SA&#26426;&#21046;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#38590;&#20197;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#20986;&#29616;&#30340;&#38271;&#24207;&#21015;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;Multiscale Wavelet Attention&#65288;MWA&#65289;&#65292;&#20351;&#29992;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#23616;&#37096;&#25193;&#23637;&#21040;&#20840;&#22495;&#21644;&#22810;&#23610;&#24230;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#29992;CIFAR&#21644;ImageNet&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;MWA&#27604;ViT&#21644;AFNO&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved widespread success in computer vision. At their heart, there is a Self-Attention (SA) mechanism, an inductive bias that associates each token in the input with every other token through a weighted basis. The standard SA mechanism has quadratic complexity with the sequence length, which impedes its utility to long sequences appearing in high resolution vision. Recently, inspired by operator learning for PDEs, Adaptive Fourier Neural Operators (AFNO) were introduced for high resolution attention based on global convolution that is efficiently implemented via FFT. However, the AFNO global filtering cannot well represent small and moderate scale structures that commonly appear in natural images. To leverage the coarse-to-fine scale structures we introduce a Multiscale Wavelet Attention (MWA) by leveraging wavelet neural operators which incurs linear complexity in the sequence size. We replace the attention in ViT with MWA and our experiments with CIFAR and ImageN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20855;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38543;&#26426;&#38750;&#20809;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22635;&#34917;&#20102;&#22312;&#20989;&#25968;&#38750;&#20809;&#28369;&#22330;&#26223;&#19979;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2303.12277</link><description>&lt;p&gt;
&#24102;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38543;&#26426;&#38750;&#20809;&#28369;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Nonsmooth Convex Optimization with Heavy-Tailed Noises. (arXiv:2303.12277v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20855;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38543;&#26426;&#38750;&#20809;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22635;&#34917;&#20102;&#22312;&#20989;&#25968;&#38750;&#20809;&#28369;&#22330;&#26223;&#19979;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#23558;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#32771;&#34385;&#22312;&#37325;&#23614;&#22122;&#22768;&#33539;&#24335;&#19979;&#65292;&#21363;&#20551;&#35774;&#38543;&#26426;&#26799;&#24230;&#21644;&#30495;&#23454;&#26799;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#20855;&#26377;&#26377;&#38480;&#30340; $p$ &#38454;&#30697;&#65288;&#20363;&#22914;&#34987;&#26576;&#20010; $\sigma \geq0$ &#19978;&#30028;&#38480;&#21046;&#20026; $\sigma^{p}$&#65289;&#65292;&#20854;&#20013; $p\in (1,2]$&#65292;&#36825;&#19981;&#20165;&#27867;&#21270;&#20102;&#20256;&#32479;&#30340;&#26377;&#38480;&#26041;&#24046;&#20551;&#35774;&#65288;$p=2$&#65289;&#65292;&#32780;&#19988;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#37117;&#34987;&#35266;&#23519;&#21040;&#12290;&#22312;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#38024;&#23545;&#20984;&#25110;&#38750;&#20984;&#38382;&#39064;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22810;&#26032;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21482;&#32771;&#34385;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#30456;&#21453;&#65292;&#22312;&#20989;&#25968;&#38750;&#20809;&#28369;&#26102;&#65292;&#20154;&#20204;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#24182;&#23436;&#20840;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#24102;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38543;&#26426;&#38750;&#20809;&#28369;&#20984;&#20248;&#21270;&#25552;&#20379;&#20840;&#38754;&#20998;&#26512;&#26469;&#22635;&#34917;&#36825;&#19968;&#20851;&#38190;&#31354;&#30333;&#12290;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;&#35009;&#21098;&#30340;&#31639;&#27861;&#65292;&#28982;&#32780;&#65292;&#36825;&#20010;&#31639;&#27861;&#21482;&#34987;&#35777;&#26126;&#33021;&#20197;&#26399;&#26395;&#26041;&#24335;&#25910;&#25947;&#65292;&#20294;&#22312;&#38468;&#21152;
&lt;/p&gt;
&lt;p&gt;
Recently, several studies consider the stochastic optimization problem but in a heavy-tailed noise regime, i.e., the difference between the stochastic gradient and the true gradient is assumed to have a finite $p$-th moment (say being upper bounded by $\sigma^{p}$ for some $\sigma\geq0$) where $p\in(1,2]$, which not only generalizes the traditional finite variance assumption ($p=2$) but also has been observed in practice for several different tasks. Under this challenging assumption, lots of new progress has been made for either convex or nonconvex problems, however, most of which only consider smooth objectives. In contrast, people have not fully explored and well understood this problem when functions are nonsmooth. This paper aims to fill this crucial gap by providing a comprehensive analysis of stochastic nonsmooth convex optimization with heavy-tailed noises. We revisit a simple clipping-based algorithm, whereas, which is only proved to converge in expectation but under the additi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#21452;&#26354;&#21644;&#29699;&#24418;&#27169;&#22411;&#31354;&#38388;&#30340;&#31435;&#20307;&#25237;&#24433;&#20197;&#21450;Riemannian&#38544;&#31354;&#38388;&#30340;&#20056;&#31215;&#24212;&#29992;&#20110;&#28508;&#22312;&#22270;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#19982;&#38750;&#25237;&#24433;&#31354;&#38388;&#30456;&#24403;&#30340;&#24615;&#33021;&#24182;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.11754</link><description>&lt;p&gt;
&#28508;&#22312;&#22270;&#25512;&#26029;&#20013;&#30340;&#27169;&#22411;&#31354;&#38388;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
Projections of Model Spaces for Latent Graph Inference. (arXiv:2303.11754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#21452;&#26354;&#21644;&#29699;&#24418;&#27169;&#22411;&#31354;&#38388;&#30340;&#31435;&#20307;&#25237;&#24433;&#20197;&#21450;Riemannian&#38544;&#31354;&#38388;&#30340;&#20056;&#31215;&#24212;&#29992;&#20110;&#28508;&#22312;&#22270;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#19982;&#38750;&#25237;&#24433;&#31354;&#38388;&#30456;&#24403;&#30340;&#24615;&#33021;&#24182;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#22270;&#30340;&#36830;&#25509;&#32467;&#26500;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#12290;&#28508;&#22312;&#22270;&#25512;&#26029;&#20851;&#27880;&#20110;&#23398;&#20064;&#19968;&#20010;&#21512;&#36866;&#30340;&#22270;&#32467;&#26500;&#26469;&#25193;&#25955;&#20449;&#24687;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#26412;&#25991;&#21033;&#29992;&#21452;&#26354;&#21644;&#29699;&#24418;&#27169;&#22411;&#31354;&#38388;&#30340;&#31435;&#20307;&#25237;&#24433;&#65292;&#20197;&#21450;Riemannian&#38544;&#31354;&#38388;&#30340;&#20056;&#31215;&#65292;&#29992;&#20110;&#28508;&#22312;&#22270;&#25512;&#26029;&#12290;&#22312;&#36991;&#20813;&#26354;&#29575;&#36235;&#20110;&#38646;&#26102;&#31354;&#38388;&#21457;&#25955;&#30340;&#29702;&#35770;&#20445;&#35777;&#19979;&#65292;&#31435;&#20307;&#25237;&#24433;&#27169;&#22411;&#31354;&#38388;&#33021;&#23454;&#29616;&#19982;&#38750;&#25237;&#24433;&#23545;&#24212;&#27169;&#22411;&#31354;&#38388;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks leverage the connectivity structure of graphs as an inductive bias. Latent graph inference focuses on learning an adequate graph structure to diffuse information on and improve the downstream performance of the model. In this work we employ stereographic projections of the hyperbolic and spherical model spaces, as well as products of Riemannian manifolds, for the purpose of latent graph inference. Stereographically projected model spaces achieve comparable performance to their non-projected counterparts, while providing theoretical guarantees that avoid divergence of the spaces when the curvature tends to zero. We perform experiments on both homophilic and heterophilic graphs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30456;&#37051;&#30340;&#22810;&#20445;&#30495;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#20849;&#20139;&#20302;&#20445;&#30495;&#24230;&#21644;&#39640;&#20445;&#30495;&#24230;&#35299;&#20915;&#26041;&#26696;&#30340;&#29305;&#24449;&#31354;&#38388;&#26469;&#20943;&#23569;&#25110;&#28040;&#38500;&#23545;&#39640;&#31934;&#24230;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#36825;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26102;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.11577</link><description>&lt;p&gt;
&#29305;&#24449;&#30456;&#37051;&#22810;&#20445;&#30495;&#29289;&#29702;&#23398;&#20064;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Feature-adjacent multi-fidelity physics-informed machine learning for partial differential equations. (arXiv:2303.11577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11577
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30456;&#37051;&#30340;&#22810;&#20445;&#30495;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#20849;&#20139;&#20302;&#20445;&#30495;&#24230;&#21644;&#39640;&#20445;&#30495;&#24230;&#35299;&#20915;&#26041;&#26696;&#30340;&#29305;&#24449;&#31354;&#38388;&#26469;&#20943;&#23569;&#25110;&#28040;&#38500;&#23545;&#39640;&#31934;&#24230;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#36825;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26102;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#22791;&#36873;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#38382;&#39064;&#65292;&#36825;&#31181;&#32593;&#32476;&#30340;&#35757;&#32451;&#20173;&#28982;&#38656;&#35201;&#39640;&#31934;&#24230;&#30340;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#30340;&#29983;&#25104;&#25104;&#26412;&#21487;&#33021;&#24456;&#39640;&#12290;&#20026;&#20102;&#20943;&#23569;&#29978;&#33267;&#28040;&#38500;&#23545;&#39640;&#20445;&#30495;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#30340;&#22810;&#20445;&#30495;&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#31354;&#38388;&#30001;&#20302;&#20445;&#30495;&#24230;&#21644;&#39640;&#20445;&#30495;&#24230;&#35299;&#20915;&#26041;&#26696;&#20849;&#20139;&#12290;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#20302;&#31934;&#24230;&#21644;&#39640;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#30340;&#25237;&#24433;&#30456;&#37051;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#23427;&#20204;&#30340;&#30456;&#23545;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#29305;&#24449;&#31354;&#38388;&#30001;&#32534;&#30721;&#22120;&#34920;&#31034;&#65292;&#20854;&#26144;&#23556;&#21040;&#21407;&#22987;&#35299;&#31354;&#38388;&#36890;&#36807;&#35299;&#30721;&#22120;&#23454;&#29616;&#12290;&#25152;&#25552;&#20986;&#30340;&#22810;&#20445;&#30495;&#26041;&#27861;&#22312;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#30340;&#23450;&#24577;&#21644;&#38750;&#23450;&#24577;&#38382;&#39064;&#30340;&#27491;&#38382;&#39064;&#21644;&#36870;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks have emerged as an alternative method for solving partial differential equations. However, for complex problems, the training of such networks can still require high-fidelity data which can be expensive to generate. To reduce or even eliminate the dependency on high-fidelity data, we propose a novel multi-fidelity architecture which is based on a feature space shared by the low- and high-fidelity solutions. In the feature space, the projections of the low-fidelity and high-fidelity solutions are adjacent by constraining their relative distance. The feature space is represented with an encoder and its mapping to the original solution space is effected through a decoder. The proposed multi-fidelity approach is validated on forward and inverse problems for steady and unsteady problems described by partial differential equations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.11525</link><description>&lt;p&gt;
SIFT: &#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#20197;&#26368;&#22823;&#38480;&#24230;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#25928;&#29575;&#65288;&#19982;&#35757;&#32451;FLOPS&#30456;&#20851;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#65289;&#12290; &#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;FLOP&#65292;&#20294;&#20351;&#29992;&#31232;&#30095;&#26435;&#37325;&#36827;&#34892;&#35757;&#32451;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#25439;&#22833;&#25110;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#21608;&#26399;&#65292;&#20351;&#24471;&#32467;&#26524;&#30340;&#35757;&#32451;&#25928;&#29575;&#19981;&#22815;&#28165;&#26224;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#31232;&#30095;&#24615;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#21516;&#30340;FLOPS&#65292;&#24182;&#36890;&#36807;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#23637;&#31034;&#35757;&#32451;&#25928;&#29575;&#25552;&#39640;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SIFT&#65292;&#19968;&#32452;&#29992;&#20316;&#23494;&#38598;&#23618;&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#20195;&#21697;&#26469;&#25552;&#39640;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;FLOP&#25928;&#29575;&#30340;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#12290; &#27599;&#20010;&#36716;&#25442;&#37117;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#65288;&#31232;&#30095;&#32423;&#21035;&#65289;&#21442;&#25968;&#21270;&#65292;&#24182;&#25552;&#20379;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31232;&#30095;&#25513;&#33180;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of weight sparsity to improve the training efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs). These works aim to reduce training FLOPs but training with sparse weights often leads to accuracy loss or requires longer train schedules, making the resulting training efficiency less clear. In contrast, we focus on using sparsity to increase accuracy while using the same FLOPS as the dense model and show training efficiency gains through higher accuracy. In this work, we introduce SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in replacements for dense layers to improve their representational capacity and FLOP efficiency. Each transformation is parameterized by a single parameter (sparsity level) and provides a larger search space to find optimal sparse masks. Without changing any training hyperparameters, replacing dense layers with SIFT leads to significant improvements across computer vision (CV) and
&lt;/p&gt;</description></item><item><title>STDLens &#26159;&#19968;&#31181;&#21487;&#20197;&#38450;&#27490;FL&#21463;&#21040;&#27169;&#22411;&#25375;&#25345;&#30340;&#25915;&#20987;&#30340;&#23433;&#20840;&#26041;&#27861;&#12290;&#23427;&#22522;&#20110;&#19977;&#23618;&#30340;&#21462;&#35777;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#25490;&#38500;&#29305;&#27530;&#30340;&#26799;&#24230;&#65292;&#24182;&#24674;&#22797;FL&#30340;&#24615;&#33021;&#12290;STDLens&#22312;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#24182;&#19988;&#20855;&#26377;&#38450;&#27490;&#27169;&#22411;&#25375;&#25345;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11511</link><description>&lt;p&gt;
STDLens&#65306;&#22522;&#20110;&#27169;&#22411;&#25375;&#25345;&#30340;&#29289;&#20307;&#26816;&#27979;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#38450;&#25252;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
STDLens: Model Hijacking-resilient Federated Learning for Object Detection. (arXiv:2303.11511v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11511
&lt;/p&gt;
&lt;p&gt;
STDLens &#26159;&#19968;&#31181;&#21487;&#20197;&#38450;&#27490;FL&#21463;&#21040;&#27169;&#22411;&#25375;&#25345;&#30340;&#25915;&#20987;&#30340;&#23433;&#20840;&#26041;&#27861;&#12290;&#23427;&#22522;&#20110;&#19977;&#23618;&#30340;&#21462;&#35777;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#25490;&#38500;&#29305;&#27530;&#30340;&#26799;&#24230;&#65292;&#24182;&#24674;&#22797;FL&#30340;&#24615;&#33021;&#12290;STDLens&#22312;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#24182;&#19988;&#20855;&#26377;&#38450;&#27490;&#27169;&#22411;&#25375;&#25345;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#22312;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#23427;&#20855;&#26377;&#35832;&#22810;&#20248;&#28857;&#65292;FL&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#25375;&#25345;&#30340;&#25915;&#20987;&#12290;&#25915;&#20987;&#32773;&#21487;&#20197;&#20165;&#20165;&#21033;&#29992;&#19968;&#23567;&#37096;&#20998;&#21487;&#20197;&#34987;&#25915;&#20987;&#30340;&#23458;&#25143;&#31471;&#25511;&#21046;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#30340;&#27491;&#30830;&#24615;&#65292;&#36890;&#36807;&#26893;&#20837;&#29305;&#27530;&#26799;&#24230;&#23454;&#29616;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STDLens&#30340;&#23433;&#20840;&#26041;&#27861;&#20197;&#20445;&#25252;FL&#20813;&#21463;&#27492;&#31867;&#25915;&#20987;&#12290;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;&#30340;&#32531;&#35299;&#26426;&#21046;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#31354;&#38388;&#32858;&#31867;&#20998;&#26512;&#26799;&#24230;&#26102;&#30001;&#20110;&#22266;&#26377;&#35823;&#24046;&#32780;&#20135;&#29983;&#30340;&#22833;&#36133;&#24773;&#20917;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#30340;&#21462;&#35777;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#25490;&#38500;&#36825;&#31181;&#29305;&#27530;&#30340;&#26799;&#24230;&#65292;&#24182;&#22312;FL&#36807;&#31243;&#20013;&#24674;&#22797;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#33258;&#36866;&#24212;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;STDLens&#23545;&#39640;&#32423;&#23545;&#25163;&#20855;&#26377;&#30340;&#31283;&#20581;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;STDLens&#22312;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#38450;&#27490;&#27169;&#22411;&#25375;&#25345;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has been gaining popularity as a collaborative learning framework to train deep learning-based object detection models over a distributed population of clients. Despite its advantages, FL is vulnerable to model hijacking. The attacker can control how the object detection system should misbehave by implanting Trojaned gradients using only a small number of compromised clients in the collaborative learning process. This paper introduces STDLens, a principled approach to safeguarding FL against such attacks. We first investigate existing mitigation mechanisms and analyze their failures caused by the inherent errors in spatial clustering analysis on gradients. Based on the insights, we introduce a three-tier forensic framework to identify and expel Trojaned gradients and reclaim the performance over the course of FL. We consider three types of adaptive attacks and demonstrate the robustness of STDLens against advanced adversaries. Extensive experiments show that STD
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#25429;&#33719;&#20102;&#22810;&#20307;&#21704;&#23494;&#39039;&#37327;&#20013;&#30340;&#25299;&#25169;&#24207;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26159;&#30740;&#31350;&#29289;&#24577;&#30456;&#30340;&#19968;&#20010;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2303.11207</link><description>&lt;p&gt;
&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#25299;&#25169;&#24207;
&lt;/p&gt;
&lt;p&gt;
Investigating Topological Order using Recurrent Neural Networks. (arXiv:2303.11207v2 [cond-mat.str-el] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#25429;&#33719;&#20102;&#22810;&#20307;&#21704;&#23494;&#39039;&#37327;&#20013;&#30340;&#25299;&#25169;&#24207;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26159;&#30740;&#31350;&#29289;&#24577;&#30456;&#30340;&#19968;&#20010;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#34987;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#23427;&#22312;&#25551;&#36848;&#24378;&#20851;&#32852;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#26041;&#38754;&#20063;&#24456;&#26377;&#21069;&#36884;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20108;&#32500;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20102;&#20004;&#20010;&#20856;&#22411;&#23637;&#29616;&#25299;&#25169;&#24207;&#30340;&#22810;&#20307;&#21704;&#23494;&#39039;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#20272;&#31639;&#20854;&#25299;&#25169;&#32416;&#32544;&#29109;&#65292;&#26377;&#25928;&#22320;&#25429;&#33719;&#25197;&#26354;&#32534;&#30721;&#21644;&#34562;&#24034;&#26684;&#19978; Bose-Hubbard &#33258;&#26059;&#28082;&#20307;&#30340;&#25299;&#25169;&#24207;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26356;&#20542;&#21521;&#20110;&#21033;&#29992;&#24456;&#23569;&#32416;&#32544;&#29366;&#24577;&#30340;&#30456;&#24178;&#21472;&#21152;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26159;&#30740;&#31350;&#19981;&#23545;&#31216;&#30772;&#32570;&#20043;&#22806;&#29289;&#24577;&#30456;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural networks (RNNs), originally developed for natural language processing, hold great promise for accurately describing strongly correlated quantum many-body systems. Here, we employ 2D RNNs to investigate two prototypical quantum many-body Hamiltonians exhibiting topological order. Specifically, we demonstrate that RNN wave functions can effectively capture the topological order of the toric code and a Bose-Hubbard spin liquid on the kagome lattice by estimating their topological entanglement entropies. We also find that RNNs favor coherent superpositions of minimally-entangled states over minimally-entangled states themselves. Overall, our findings demonstrate that RNN wave functions constitute a powerful tool to study phases of matter beyond Landau's symmetry-breaking paradigm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#23454;&#29616;&#20102;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#21644;&#19982;&#23454;&#38469;&#24212;&#29992;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.10880</link><description>&lt;p&gt;
&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;: &#36890;&#36807;&#35302;&#35273;&#23454;&#29616;&#25163;&#37096;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rotating without Seeing: Towards In-hand Dexterity through Touch. (arXiv:2303.10880v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#23454;&#29616;&#20102;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#21644;&#19982;&#23454;&#38469;&#24212;&#29992;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35302;&#24863;&#20449;&#24687;&#22312;&#20154;&#31867;&#28789;&#24039;&#24615;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#26377;&#29992;&#30340;&#25509;&#35302;&#20449;&#24687;&#65292;&#30452;&#25509;&#20174;&#35270;&#35273;&#20013;&#26080;&#27861;&#25512;&#26029;&#12290;&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#33021;&#22815;&#20351;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#20855;&#22791;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#20351;&#29992;&#35206;&#30422;&#25972;&#20010;&#26426;&#22120;&#20154;&#25163;&#30340;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#65288;&#35302;&#25720;&#25110;&#26410;&#35302;&#25720;&#65289;&#20195;&#26367;&#20165;&#20165;&#22312;&#23567;&#21306;&#22495;&#20869;&#36827;&#34892;&#31934;&#20934;&#30340;&#35302;&#35273;&#20256;&#24863;&#65292;&#20351;&#31995;&#32479;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#35206;&#30422;&#33539;&#22260;&#24191;&#31561;&#20248;&#28857;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#22810;&#26679;&#30340;&#29289;&#20307;&#27169;&#25311;&#20013;&#35757;&#32451;&#20986;&#20102;&#19968;&#31181;&#35302;&#24863;&#26059;&#36716;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#25163;&#19978;&#30452;&#25509;&#23454;&#26045;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#26032;&#22411;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactile information plays a critical role in human dexterity. It reveals useful contact information that may not be inferred directly from vision. In fact, humans can even perform in-hand dexterous manipulation without using vision. Can we enable the same ability for the multi-finger robot hand? In this paper, we present Touch Dexterity, a new system that can perform in-hand object rotation using only touching without seeing the object. Instead of relying on precise tactile sensing in a small region, we introduce a new system design using dense binary force sensors (touch or no touch) overlaying one side of the whole robot hand (palm, finger links, fingertips). Such a design is low-cost, giving a larger coverage of the object, and minimizing the Sim2Real gap at the same time. We train an in-hand rotation policy using Reinforcement Learning on diverse objects in simulation. Relying on touch-only sensing, we can directly deploy the policy in a real robot hand and rotate novel objects tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#27969;&#21644;&#25193;&#25955;&#26041;&#27861;&#30340;&#32479;&#19968;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#65292;&#29992;&#20110;&#23558;&#20004;&#20010;&#20219;&#24847;&#30340;&#23494;&#24230;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#31934;&#30830;&#22320;&#36830;&#25509;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22522;&#20110;&#27010;&#29575;&#24494;&#20998;&#26041;&#31243;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29983;&#25104;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;</title><link>http://arxiv.org/abs/2303.08797</link><description>&lt;p&gt;
&#38543;&#26426;&#25554;&#20540;&#65306;&#27969;&#21644;&#25193;&#25955;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Stochastic Interpolants: A Unifying Framework for Flows and Diffusions. (arXiv:2303.08797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#27969;&#21644;&#25193;&#25955;&#26041;&#27861;&#30340;&#32479;&#19968;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#65292;&#29992;&#20110;&#23558;&#20004;&#20010;&#20219;&#24847;&#30340;&#23494;&#24230;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#31934;&#30830;&#22320;&#36830;&#25509;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22522;&#20110;&#27010;&#29575;&#24494;&#20998;&#26041;&#31243;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29983;&#25104;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31867;&#24314;&#31435;&#22312;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#19978;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#26694;&#26550;&#26159;&#22522;&#20110;Albergo&#65286;Vanden-Eijnden&#65288;2023&#65289;&#25552;&#20986;&#30340;&#65292;&#22312;&#27969;&#21644;&#25193;&#25955;&#26041;&#27861;&#19978;&#23454;&#29616;&#32479;&#19968;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22914;&#20309;&#26500;&#24314;&#19968;&#31867;&#24191;&#27867;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#65292;&#20854;&#26102;&#38388;&#20381;&#36182;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#31934;&#30830;&#22320;&#36830;&#25509;&#20004;&#20010;&#20219;&#24847;&#30340;&#23494;&#24230;&#12290;&#36825;&#20123;&#8220;&#38543;&#26426;&#25554;&#20540;&#22120;&#8221;&#26159;&#36890;&#36807;&#23558;&#26469;&#33258;&#20004;&#20010;&#23494;&#24230;&#30340;&#25968;&#25454;&#19982;&#20854;&#20182;&#28508;&#22312;&#21464;&#37327;&#30456;&#32467;&#21512;&#26500;&#24314;&#30340;&#65292;&#24182;&#19988;&#26500;&#36896;&#30340;&#20855;&#20307;&#32454;&#33410;&#21487;&#20197;&#28789;&#27963;&#22320;&#22609;&#36896;&#23548;&#33268;&#30340;&#26102;&#38388;&#20381;&#36182;&#23494;&#24230;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#26426;&#25554;&#20540;&#22120;&#30340;&#26102;&#38388;&#20381;&#36182;&#23494;&#24230;&#28385;&#36275;&#19968;&#38454;&#36755;&#36816;&#26041;&#31243;&#20197;&#21450;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#35843;&#25193;&#25955;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;Fokker-Planck&#26041;&#31243;&#26063;; &#22312;&#32771;&#34385;&#21333;&#20010;&#26679;&#26412;&#30340;&#26102;&#38388;&#28436;&#21270;&#26102;&#65292;&#36825;&#20010;&#35266;&#28857;&#31435;&#21363;&#23548;&#33268;&#20102;&#22522;&#20110;&#27010;&#29575;&#24494;&#20998;&#26041;&#31243;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a class of generative models based on the stochastic interpolant framework proposed in Albergo &amp; Vanden-Eijnden (2023) that unifies flow-based and diffusion-based methods. We first show how to construct a broad class of continuous-time stochastic processes whose time-dependent probability density function bridges two arbitrary densities exactly in finite time. These `stochastic interpolants' are built by combining data from the two densities with an additional latent variable, and the specific details of the construction can be leveraged to shape the resulting time-dependent density in a flexible way. We then show that the time-dependent density of the stochastic interpolant satisfies a first-order transport equation as well as a family of forward and backward Fokker-Planck equations with tunable diffusion; upon consideration of the time evolution of an individual sample, this viewpoint immediately leads to both deterministic and stochastic generative models based on proba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#21051;&#27169;&#22411;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#22797;&#20540;&#31070;&#32463;&#22330;&#25191;&#34892;&#20809;&#23398;&#26680;&#22238;&#24402;&#24182;&#23558;&#20809;&#21051;&#31995;&#32479;&#25286;&#35299;&#20026;&#38750;&#21442;&#25968;&#25513;&#27169;&#25805;&#20316;&#21644;&#21253;&#21547;&#34892;&#21015;&#24335;&#28304;&#12289;&#30643;&#23380;&#21644;&#20809;&#21051;&#20449;&#24687;&#30340;&#23398;&#20064;&#20809;&#23398;&#26680;&#65292;&#20351;&#29992;&#23567;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08435</link><description>&lt;p&gt;
&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#22330;&#30340;&#29289;&#29702;&#20449;&#24687;&#20809;&#23398;&#26680;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Optical Kernel Regression Using Complex-valued Neural Fields. (arXiv:2303.08435v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#21051;&#27169;&#22411;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#22797;&#20540;&#31070;&#32463;&#22330;&#25191;&#34892;&#20809;&#23398;&#26680;&#22238;&#24402;&#24182;&#23558;&#20809;&#21051;&#31995;&#32479;&#25286;&#35299;&#20026;&#38750;&#21442;&#25968;&#25513;&#27169;&#25805;&#20316;&#21644;&#21253;&#21547;&#34892;&#21015;&#24335;&#28304;&#12289;&#30643;&#23380;&#21644;&#20809;&#21051;&#20449;&#24687;&#30340;&#23398;&#20064;&#20809;&#23398;&#26680;&#65292;&#20351;&#29992;&#23567;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#21051;&#26159;&#38598;&#25104;&#30005;&#36335;&#21046;&#36896;&#30340;&#22522;&#30784;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#21051;&#27169;&#22411;&#30340;&#21457;&#23637;&#32531;&#35299;&#20102;&#21046;&#36896;&#36807;&#31243;&#24320;&#38144;&#21644;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#20197;&#21069;&#30340;&#26041;&#27861;&#37117;&#23558;&#20809;&#21051;&#31995;&#32479;&#35270;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#40657;&#30418;&#26144;&#23556;&#65292;&#21033;&#29992;&#32593;&#32476;&#21442;&#25968;&#36890;&#36807;&#27515;&#35760;&#30828;&#32972;&#26144;&#23556;&#22823;&#37327;&#30340;&#25513;&#27169;&#21040;&#31354;&#20013;&#25110;&#25513;&#27169;&#21040;&#30005;&#38459;&#22270;&#20687;&#23545;&#65292;&#23548;&#33268;&#25512;&#24191;&#33021;&#21147;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#23558;&#20005;&#26684;&#30340;&#20809;&#21051;&#27169;&#22411;&#25286;&#35299;&#20026;&#38750;&#21442;&#25968;&#25513;&#27169;&#25805;&#20316;&#21644;&#21253;&#21547;&#34892;&#21015;&#24335;&#28304;&#12289;&#30643;&#23380;&#21644;&#20809;&#21051;&#20449;&#24687;&#30340;&#23398;&#20064;&#20809;&#23398;&#26680;&#12290;&#36890;&#36807;&#20248;&#21270;&#22797;&#20540;&#31070;&#32463;&#22330;&#20197;&#25191;&#34892;&#20809;&#23398;&#26680;&#22238;&#24402;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#20809;&#21051;&#31995;&#32479;&#65292;&#21516;&#26102;&#20351;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#36827;&#34892;&#23567;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lithography is fundamental to integrated circuit fabrication, necessitating large computation overhead. The advancement of machine learning (ML)-based lithography models alleviates the trade-offs between manufacturing process expense and capability. However, all previous methods regard the lithography system as an image-to-image black box mapping, utilizing network parameters to learn by rote mappings from massive mask-to-aerial or mask-to-resist image pairs, resulting in poor generalization capability. In this paper, we propose a new ML-based paradigm disassembling the rigorous lithographic model into non-parametric mask operations and learned optical kernels containing determinant source, pupil, and lithography information. By optimizing complex-valued neural fields to perform optical kernel regression from coordinates, our method can accurately restore lithography system using a small-scale training dataset with fewer parameters, demonstrating superior generalization capability as w
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20351;&#29992;&#20108;&#32500;&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;Moir&#233;&#24037;&#31243;&#20013;Moir&#233;&#21407;&#23376;&#20013;&#30005;&#23376;&#30340;&#30456;&#20114;&#20316;&#29992;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#24378;Coulomb&#30456;&#20114;&#20316;&#29992;&#19982;&#21508;&#21521;&#24322;&#24615;Moir&#233;&#21183;&#33021;&#30456;&#32467;&#21512;&#21487;&#20197;&#20135;&#29983;&#8220;Wigner&#20998;&#23376;&#8221;&#30005;&#33655;&#23494;&#24230;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.08162</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#20154;&#36896;&#26448;&#26009;&#65306;Moir&#233;&#21407;&#23376;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence for artificial materials: moir\'e atom. (arXiv:2303.08162v1 [cond-mat.str-el])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08162
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20351;&#29992;&#20108;&#32500;&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;Moir&#233;&#24037;&#31243;&#20013;Moir&#233;&#21407;&#23376;&#20013;&#30005;&#23376;&#30340;&#30456;&#20114;&#20316;&#29992;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#24378;Coulomb&#30456;&#20114;&#20316;&#29992;&#19982;&#21508;&#21521;&#24322;&#24615;Moir&#233;&#21183;&#33021;&#30456;&#32467;&#21512;&#21487;&#20197;&#20135;&#29983;&#8220;Wigner&#20998;&#23376;&#8221;&#30005;&#33655;&#23494;&#24230;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#23376;&#34180;&#23618;Van der Waals&#24322;&#36136;&#32467;&#26500;&#20013;&#30340;Moir&#233;&#24037;&#31243;&#21019;&#24314;&#20102;&#20855;&#26377;&#35774;&#35745;&#24072;&#23646;&#24615;&#30340;&#20154;&#36896;&#37327;&#23376;&#26448;&#26009;&#12290;&#25105;&#20204;&#20351;&#29992;2D&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;&#30456;&#20114;&#20316;&#29992;&#30005;&#23376;&#21463;&#38480;&#20110;Moir&#233;&#36229;&#26230;&#26684;&#21183;&#33021;&#26368;&#23567;&#20540;&#65288;Moir&#233;&#21407;&#23376;&#65289;&#30340;&#22810;&#20307;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#24378;Coulomb&#30456;&#20114;&#20316;&#29992;&#19982;&#21508;&#21521;&#24322;&#24615;Moir&#233;&#21183;&#33021;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20135;&#29983;&#8220;Wigner&#20998;&#23376;&#8221;&#30005;&#33655;&#23494;&#24230;&#20998;&#24067;&#65292;&#21487;&#36890;&#36807;&#25195;&#25551;&#38567;&#36947;&#26174;&#24494;&#38236;&#35266;&#23519;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moir\'e engineering in atomically thin van der Waals heterostructures creates artificial quantum materials with designer properties. We solve the many-body problem of interacting electrons confined to a moir\'e superlattice potential minimum (the moir\'e atom) using a 2D fermionic neural network. We show that strong Coulomb interactions in combination with the anisotropic moir\'e potential lead to striking ``Wigner molecule" charge density distributions observable with scanning tunneling microscopy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#29992;&#20110;&#23384;&#20648;&#36328;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#30340;&#23548;&#33322;&#26041;&#24335;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;AVLMaps&#23454;&#29616;&#20102;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#30340;&#38646;&#27425;&#23398;&#20064;&#24335;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.07522</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Audio Visual Language Maps for Robot Navigation. (arXiv:2303.07522v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#29992;&#20110;&#23384;&#20648;&#36328;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#30340;&#23548;&#33322;&#26041;&#24335;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;AVLMaps&#23454;&#29616;&#20102;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#30340;&#38646;&#27425;&#23398;&#20064;&#24335;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#19990;&#30028;&#30340;&#20114;&#21160;&#26159;&#19968;&#31181;&#22810;&#24863;&#23448;&#30340;&#20307;&#39564;&#65292;&#20294;&#26159;&#35768;&#22810;&#26426;&#22120;&#20154;&#20173;&#28982;&#20027;&#35201;&#20381;&#36182;&#35270;&#35273;&#24863;&#30693;&#26469;&#32472;&#21046;&#21644;&#23548;&#33322;&#20182;&#20204;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;3D&#31354;&#38388;&#22320;&#22270;&#34920;&#31034;&#65292;&#29992;&#20110;&#23384;&#20648;&#26469;&#33258;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#35821;&#35328;&#32447;&#32034;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#12290;&#22312;&#23548;&#33322;&#30340;&#24773;&#22659;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AVLMaps&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#31995;&#32479;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;(&#20363;&#22914;&#65292;&#25991;&#26412;&#25551;&#36848;&#12289;&#22270;&#20687;&#25110;&#22320;&#26631;&#30340;&#38899;&#39057;&#29255;&#27573;)&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#12290;&#29305;&#21035;&#26159;&#65292;&#28155;&#21152;&#38899;&#39057;&#20449;&#24687;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26356;&#21487;&#38752;&#22320;&#28040;&#38500;&#30446;&#26631;&#20301;&#32622;&#30340;&#27495;&#20041;&#24615;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AVLMaps&#33021;&#22815;&#23454;&#29616;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#36827;&#34892;&#38646;&#27425;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#22312;&#27169;&#31946;&#22330;&#26223;&#20013;&#25552;&#20379;50%&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While interacting in the world is a multi-sensory experience, many robots continue to predominantly rely on visual perception to map and navigate in their environments. In this work, we propose Audio-Visual-Language Maps (AVLMaps), a unified 3D spatial map representation for storing cross-modal information from audio, visual, and language cues. AVLMaps integrate the open-vocabulary capabilities of multimodal foundation models pre-trained on Internet-scale data by fusing their features into a centralized 3D voxel grid. In the context of navigation, we show that AVLMaps enable robot systems to index goals in the map based on multimodal queries, e.g., textual descriptions, images, or audio snippets of landmarks. In particular, the addition of audio information enables robots to more reliably disambiguate goal locations. Extensive experiments in simulation show that AVLMaps enable zero-shot multimodal goal navigation from multimodal prompts and provide 50% better recall in ambiguous scenar
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#24179;&#34913;&#25191;&#34892;&#24046;&#20215;&#21644;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#65292;&#35828;&#26126;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#22823;&#23567;&#21644;&#29366;&#24577;&#31354;&#38388;&#30340;&#21464;&#21270;&#65292;&#20250;&#23545;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#36896;&#25104;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.07393</link><description>&lt;p&gt;
&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#19982;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Many learning agents interacting with an agent-based market model. (arXiv:2303.07393v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#24179;&#34913;&#25191;&#34892;&#24046;&#20215;&#21644;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#65292;&#35828;&#26126;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#22823;&#23567;&#21644;&#29366;&#24577;&#31354;&#38388;&#30340;&#21464;&#21270;&#65292;&#20250;&#23545;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#36896;&#25104;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#22312;&#20107;&#20214;&#26102;&#38388;&#19979;&#30340;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#21160;&#24577;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#20010;&#24066;&#22330;&#29983;&#24577;&#31995;&#32479;&#65292;&#30001;&#19977;&#20010;&#33829;&#20859;&#32423;&#21035;&#20195;&#34920;&#65306;&#26368;&#20248;&#25191;&#34892;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#26368;&#23567;&#26234;&#33021;&#30340;&#27969;&#21160;&#24615;&#38656;&#35201;&#32773;&#21644;&#24555;&#36895;&#30340;&#30005;&#23376;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#12290;&#26368;&#20248;&#25191;&#34892;&#20195;&#29702;&#31867;&#21035;&#21253;&#25324;&#20080;&#20837;&#21644;&#21334;&#20986;&#20195;&#29702;&#65292;&#21487;&#20197;&#20351;&#29992;&#38480;&#20215;&#21333;&#21644;&#24066;&#20215;&#21333;&#30340;&#32452;&#21512;&#65292;&#25110;&#32773;&#20165;&#20351;&#29992;&#24066;&#20215;&#21333;&#36827;&#34892;&#20132;&#26131;&#12290;&#22870;&#21169;&#20989;&#25968;&#26126;&#30830;&#24179;&#34913;&#20102;&#20132;&#26131;&#25191;&#34892;&#24046;&#20215;&#19982;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22810;&#20010;&#31454;&#20105;&#23398;&#20064;&#26234;&#33021;&#20307;&#22914;&#20309;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#30340;&#22823;&#23567;&#21644;&#29992;&#20110;&#23398;&#20064;&#30340;&#29366;&#24577;&#31354;&#38388;&#30340;&#20989;&#25968;&#24433;&#21709;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#12290;&#25105;&#20204;&#20351;&#29992;&#30456;&#31354;&#38388;&#22270;&#26469;&#30740;&#31350;ABM&#30340;&#21160;&#24577;&#65292;&#24403;&#29305;&#23450;&#35268;&#33539;&#34987;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
We consider the dynamics and the interactions of multiple reinforcement learning optimal execution trading agents interacting with a reactive Agent-Based Model (ABM) of a financial market in event time. The model represents a market ecology with 3-trophic levels represented by: optimal execution learning agents, minimally intelligent liquidity takers, and fast electronic liquidity providers. The optimal execution agent classes include buying and selling agents that can either use a combination of limit orders and market orders, or only trade using market orders. The reward function explicitly balances trade execution slippage against the penalty of not executing the order timeously. This work demonstrates how multiple competing learning agents impact a minimally intelligent market simulation as functions of the number of agents, the size of agents' initial orders, and the state spaces used for learning. We use phase space plots to examine the dynamics of the ABM, when various specifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#24182;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2303.06246</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Zone-based Federated Learning for Mobile Sensing Data. (arXiv:2303.06246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#24182;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a zone-based federated learning method for training deep learning models with mobile sensing data. The method divides the physical space into geographical zones and maps them to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model that adapts well to the data and behaviors of users in that zone, while protecting user data privacy.
&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;mHealth&#21644;&#20581;&#24247;&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#20174;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#25110;&#21487;&#31359;&#25140;&#35774;&#22791;&#25910;&#38598;&#30340;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20013;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#31227;&#21160;&#24863;&#30693;DL&#31995;&#32479;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#36866;&#24212;&#29992;&#25143;&#30340;&#31227;&#21160;&#34892;&#20026;&#65292;&#38543;&#30528;&#29992;&#25143;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;ZoneFL&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#35201;&#27714;&#12290;ZoneFL&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#31216;&#20026;&#21306;&#22495;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#12290;&#21463;&#30410;&#20110;FL&#35774;&#35745;&#65292;ZoneFL&#22521;&#35757;&#26399;&#38388;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#21512;&#35757;&#32451;&#31639;&#27861;&#26469;&#20248;&#21270;&#21306;&#22495;&#27169;&#22411;&#20197;&#36866;&#24212;&#29992;&#25143;&#30340;&#31227;&#21160;&#34892;&#20026;&#65306;&#21306;&#22495;&#21512;&#24182;&#21644;&#20998;&#35010;&#65288;ZMS&#65289;&#21644;Zo
&lt;/p&gt;
&lt;p&gt;
Mobile apps, such as mHealth and wellness applications, can benefit from deep learning (DL) models trained with mobile sensing data collected by smart phones or wearable devices. However, currently there is no mobile sensing DL system that simultaneously achieves good model accuracy while adapting to user mobility behavior, scales well as the number of users increases, and protects user data privacy. We propose Zone-based Federated Learning (ZoneFL) to address these requirements. ZoneFL divides the physical space into geographical zones mapped to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model, called a zone model, which adapts well to data and behaviors of users in that zone. Benefiting from the FL design, the user data privacy is protected during the ZoneFL training. We propose two novel zone-based federated training algorithms to optimize zone models to user mobility behavior: Zone Merge and Split (ZMS) and Zo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34917;&#20805;&#31232;&#30095;&#21270;&#30340;&#27169;&#22411;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#32852;&#37030;&#23398;&#20064;&#20013;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.06237</link><description>&lt;p&gt;
&#20302;&#24320;&#38144;&#27169;&#22411;&#21098;&#26525;&#65306;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#34917;&#20805;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Complement Sparsification: Low-Overhead Model Pruning for Federated Learning. (arXiv:2303.06237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34917;&#20805;&#31232;&#30095;&#21270;&#30340;&#27169;&#22411;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#32852;&#37030;&#23398;&#20064;&#20013;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a model pruning mechanism called Complement Sparsification (CS), which satisfies the requirements of low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy in federated learning through complementary and collaborative pruning done at the server and the clients.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#33539;&#20363;&#65292;&#28041;&#21450;&#22823;&#37327;&#36890;&#20449;&#21644;&#35745;&#31639;&#24037;&#20316;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#31227;&#21160;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#27169;&#22411;&#21098;&#26525;/&#31232;&#30095;&#21270;&#24320;&#21457;&#20102;&#21487;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#31232;&#30095;&#27169;&#22411;&#65292;&#20294;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#35299;&#20915;&#26041;&#26696;&#19981;&#33021;&#21516;&#26102;&#28385;&#36275;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#65292;&#22312;FL&#20551;&#35774;&#19979;&#65292;&#26381;&#21153;&#22120;&#26080;&#27861;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#20197;&#24494;&#35843;&#20462;&#21098;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#34917;&#20805;&#31232;&#30095;&#21270;&#65288;CS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;CS&#21019;&#24314;&#19968;&#20010;&#20840;&#23616;&#31232;&#30095;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#25429;&#33719;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#19968;&#33324;&#25968;&#25454;&#20998;&#24067;&#30340;&#26435;&#37325;&#65292;&#32780;&#23458;&#25143;&#31471;&#21017;&#21019;&#24314;&#26412;&#22320;&#31232;&#30095;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a privacy-preserving distributed deep learning paradigm that involves substantial communication and computation effort, which is a problem for resource-constrained mobile and IoT devices. Model pruning/sparsification develops sparse models that could solve this problem, but existing sparsification solutions cannot satisfy at the same time the requirements for low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy, under the FL assumption that the server does not have access to raw data to fine-tune the pruned models. We propose Complement Sparsification (CS), a pruning mechanism that satisfies all these requirements through a complementary and collaborative pruning done at the server and the clients. At each round, CS creates a global sparse model that contains the weights that capture the general data distribution of all clients, while the clients create local sparse model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37329;&#34701;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.02841</link><description>&lt;p&gt;
&#37329;&#34701;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-Agnostic Meta-Learning for Natural Language Understanding Tasks in Finance. (arXiv:2303.02841v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37329;&#34701;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#22240;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#21644;&#29305;&#27530;&#35821;&#35328;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23398;&#20064;&#31283;&#20581;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#24494;&#35843;&#32463;&#24120;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#21487;&#33021;&#20250;&#20559;&#34962;&#22823;&#37327;&#25968;&#25454;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#37329;&#34701;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;1.&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#22810;&#31181;&#31867;&#22411;&#20219;&#21153;&#30340;MAML&#26041;&#27861;&#30340;&#24615;&#33021;&#65306;GLUE&#25968;&#25454;&#38598;&#65292;SNLI&#65292;Sci-Tail&#21644;Financial PhraseBank&#65307;2.&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#30495;&#23454;&#22330;&#26223;&#30340;&#22522;&#20110;&#25512;&#29305;&#25991;&#26412;&#30340;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#38382;&#39064;&#20013;&#20351;&#29992;MAML&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language understanding(NLU) is challenging for finance due to the lack of annotated data and the specialized language in that domain. As a result, researchers have proposed to use pre-trained language model and multi-task learning to learn robust representations. However, aggressive fine-tuning often causes over-fitting and multi-task learning may favor tasks with significantly larger amounts data, etc. To address these problems, in this paper, we investigate model-agnostic meta-learning algorithm(MAML) in low-resource financial NLU tasks. Our contribution includes: 1. we explore the performance of MAML method with multiple types of tasks: GLUE datasets, SNLI, Sci-Tail and Financial PhraseBank; 2. we study the performance of MAML method with multiple single-type tasks: a real scenario stock price prediction problem with twitter text data. Our models achieve the state-of-the-art performance according to the experimental results, which demonstrate that our method can adapt fast a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#21521;&#37327;&#35270;&#20026;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#65292;&#24182;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#30340;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.14383</link><description>&lt;p&gt;
&#24847;&#20041;&#30340;&#32447;&#24615;&#31354;&#38388;&#65306;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#21521;&#37327;&#35270;&#20026;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#65292;&#24182;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#30340;&#25968;&#25454;&#23884;&#20837;&#30340;&#32452;&#21512;&#32467;&#26500;&#12290;&#20256;&#32479;&#19978;&#65292;&#32452;&#21512;&#24615;&#19982;&#39044;&#20808;&#23384;&#22312;&#30340;&#35789;&#27719;&#34920;&#20013;&#30340;&#21333;&#35789;&#23884;&#20837;&#30340;&#20195;&#25968;&#36816;&#31639;&#26377;&#20851;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35797;&#22270;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#36825;&#20123;&#21521;&#37327;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#22312;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104;&#27010;&#24565;&#30340;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#20960;&#20309;&#23398;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#29702;&#35299;&#32452;&#21512;&#32467;&#26500;&#30340;&#26694;&#26550;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;VLM&#23884;&#20837;&#22312;&#27010;&#29575;&#19978;&#30340;&#36825;&#20123;&#32452;&#21512;&#32467;&#26500;&#30340;&#21547;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#20135;&#29983;&#30340;&#30452;&#35273;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#20998;&#31867;&#12289;&#21435;&#20559;&#21644;&#26816;&#32034;&#31561;&#19981;&#21516;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;&#31354;&#38388;&#20013;&#31616;&#21333;&#30340;&#32447;&#24615;&#20195;&#25968;&#36816;&#31639;&#21487;&#20197;&#23454;&#29616;&#19982;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#24847;&#20041;&#30340;&#32447;&#24615;&#31354;&#38388;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate compositional structures in data embeddings from pre-trained vision-language models (VLMs). Traditionally, compositionality has been associated with algebraic operations on embeddings of words from a pre-existing vocabulary. In contrast, we seek to approximate representations from an encoder as combinations of a smaller set of vectors in the embedding space. These vectors can be seen as "ideal words" for generating concepts directly within the embedding space of the model. We first present a framework for understanding compositional structures from a geometric perspective. We then explain what these compositional structures entail probabilistically in the case of VLM embeddings, providing intuitions for why they arise in practice. Finally, we empirically explore these structures in CLIP's embeddings and we evaluate their usefulness for solving different vision-language tasks such as classification, debiasing, and retrieval. Our results show that simple linear algebraic o
&lt;/p&gt;</description></item><item><title>VoxFormer&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#20174;2D&#22270;&#20687;&#36755;&#20986;&#23436;&#25972;&#30340;&#19977;&#32500;&#20307;&#31215;&#35821;&#20041;&#12290;&#23427;&#37319;&#29992;&#20004;&#38454;&#27573;&#35774;&#35745;&#65292;&#20174;&#21487;&#35265;&#30340;&#20307;&#32032;&#26597;&#35810;&#24320;&#22987;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#26469;&#20256;&#25773;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#19977;&#32500;&#22330;&#26223;&#34917;&#20840;&#12290;</title><link>http://arxiv.org/abs/2302.12251</link><description>&lt;p&gt;
VoxFormer&#65306; &#22522;&#20110;&#31232;&#30095;&#20307;&#32032;&#21464;&#25442;&#30340;&#22522;&#20110;&#30456;&#26426;&#30340;&#19977;&#32500;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion. (arXiv:2302.12251v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12251
&lt;/p&gt;
&lt;p&gt;
VoxFormer&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#20174;2D&#22270;&#20687;&#36755;&#20986;&#23436;&#25972;&#30340;&#19977;&#32500;&#20307;&#31215;&#35821;&#20041;&#12290;&#23427;&#37319;&#29992;&#20004;&#38454;&#27573;&#35774;&#35745;&#65292;&#20174;&#21487;&#35265;&#30340;&#20307;&#32032;&#26597;&#35810;&#24320;&#22987;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#26469;&#20256;&#25773;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#19977;&#32500;&#22330;&#26223;&#34917;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#24456;&#23481;&#26131;&#24819;&#35937;&#34987;&#36974;&#25377;&#29289;&#20307;&#21644;&#22330;&#26223;&#30340;&#23436;&#25972;&#19977;&#32500;&#20960;&#20309;&#24418;&#29366;&#65292;&#22312;&#35782;&#21035;&#21644;&#29702;&#35299;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#20351;AI&#31995;&#32479;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VoxFormer&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#20174;2D&#22270;&#20687;&#36755;&#20986;&#23436;&#25972;&#30340;&#19977;&#32500;&#20307;&#31215;&#35821;&#20041;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20004;&#38454;&#27573;&#35774;&#35745;&#65292;&#20174;&#28145;&#24230;&#20272;&#35745;&#30340;&#31232;&#30095;&#21487;&#35265;&#21644;&#21344;&#29992;&#20307;&#32032;&#26597;&#35810;&#24320;&#22987;&#65292;&#38543;&#21518;&#36827;&#34892;&#29983;&#25104;&#31264;&#23494;3D&#20307;&#32032;&#30340;&#31264;&#23494;&#21270;&#38454;&#27573;&#12290;&#36825;&#20010;&#35774;&#35745;&#30340;&#19968;&#20010;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;2D&#22270;&#20687;&#19978;&#30340;&#35270;&#35273;&#29305;&#24449;&#20165;&#23545;&#24212;&#20110;&#21487;&#35265;&#22330;&#26223;&#32467;&#26500;&#32780;&#19981;&#26159;&#36974;&#25377;&#25110;&#31354;&#38388;&#12290;&#22240;&#27492;&#65292;&#20174;&#21487;&#35265;&#32467;&#26500;&#30340;&#29305;&#24449;&#21270;&#21644;&#39044;&#27979;&#24320;&#22987;&#26356;&#21152;&#21487;&#38752;&#12290;&#19968;&#26086;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#32452;&#31232;&#30095;&#26597;&#35810;&#65292;&#25105;&#20204;&#23601;&#24212;&#29992;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#35774;&#35745;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#23558;&#20449;&#24687;&#20256;&#25773;&#21040;&#25152;&#26377;&#20307;&#32032;&#12290;&#22312;SemanticKITTI&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can easily imagine the complete 3D geometry of occluded objects and scenes. This appealing ability is vital for recognition and understanding. To enable such capability in AI systems, we propose VoxFormer, a Transformer-based semantic scene completion framework that can output complete 3D volumetric semantics from only 2D images. Our framework adopts a two-stage design where we start from a sparse set of visible and occupied voxel queries from depth estimation, followed by a densification stage that generates dense 3D voxels from the sparse ones. A key idea of this design is that the visual features on 2D images correspond only to the visible scene structures rather than the occluded or empty spaces. Therefore, starting with the featurization and prediction of the visible structures is more reliable. Once we obtain the set of sparse queries, we apply a masked autoencoder design to propagate the information to all the voxels by self-attention. Experiments on SemanticKITTI show th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36924;&#36817;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;Fisher-Rao&#36317;&#31163;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#26354;&#32447;&#36830;&#25509;&#27491;&#24577;&#20998;&#24067;&#24182;&#36924;&#36817;&#30456;&#37051;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Rao&#36317;&#31163;&#65292;&#35780;&#20272;&#20102;&#36924;&#36817;&#25216;&#26415;&#30340;&#36136;&#37327;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20123;&#20449;&#24687;&#20960;&#20309;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2302.08175</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;Fisher-Rao&#36317;&#31163;&#30340;&#25968;&#20540;&#36924;&#36817;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A numerical approximation method for the Fisher-Rao distance between multivariate normal distributions. (arXiv:2302.08175v5 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36924;&#36817;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;Fisher-Rao&#36317;&#31163;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#26354;&#32447;&#36830;&#25509;&#27491;&#24577;&#20998;&#24067;&#24182;&#36924;&#36817;&#30456;&#37051;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Rao&#36317;&#31163;&#65292;&#35780;&#20272;&#20102;&#36924;&#36817;&#25216;&#26415;&#30340;&#36136;&#37327;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20123;&#20449;&#24687;&#20960;&#20309;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#36924;&#36817;&#22522;&#20110;&#31163;&#25955;&#21270;&#26354;&#32447;&#36830;&#25509;&#24444;&#27492;&#30340;&#27491;&#24577;&#20998;&#24067;&#21644;&#36924;&#36817;&#36825;&#20123;&#26354;&#32447;&#19978;&#30456;&#37051;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Rao&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Jeffreys&#31163;&#25955;&#24230;&#30340;&#24179;&#26041;&#26681;&#65292;&#21363;&#23545;&#31216;&#21270;&#30340;Kullback-Leibler&#31163;&#25955;&#24230;&#12290;&#25105;&#20204;&#23454;&#39564;&#32771;&#34385;&#20102;&#26222;&#36890;&#12289;&#33258;&#28982;&#21644;&#26399;&#26395;&#21442;&#25968;&#21270;&#30340;&#27491;&#24577;&#20998;&#24067;&#30340;&#32447;&#24615;&#25554;&#20540;&#26354;&#32447;&#65292;&#24182;&#23558;&#36825;&#20123;&#26354;&#32447;&#19982;&#28304;&#33258;Calvo &#21644;Oller&#23558;Fisher-Rao $d$-variate&#27491;&#24120;&#27969;&#24418;&#31561;&#36317;&#23884;&#20837;$(d+1)\times (d+1)$&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#38181;&#20307;&#30340;&#19968;&#26465;&#26354;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#23558;&#25968;&#20540;&#36924;&#36817;&#19982;&#19978;&#38480;&#21644;&#19979;&#38480;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#25105;&#20204;&#36924;&#36817;&#25216;&#26415;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Calvo&#21644;Oller&#30340;&#19968;&#20123;&#20449;&#24687;&#20960;&#20309;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple method to approximate Rao's distance between multivariate normal distributions based on discretizing curves joining normal distributions and approximating Rao's distances between successive nearby normal distributions on the curves by the square root of Jeffreys divergence, the symmetrized Kullback-Leibler divergence. We consider experimentally the linear interpolation curves in the ordinary, natural and expectation parameterizations of the normal distributions, and compare these curves with a curve derived from the Calvo and Oller's isometric embedding of the Fisher-Rao $d$-variate normal manifold into the cone of $(d+1)\times (d+1)$ symmetric positive-definite matrices [Journal of multivariate analysis 35.2 (1990): 223-242]. We report on our experiments and assess the quality of our approximation technique by comparing the numerical approximations with both lower and upper bounds. Finally, we present several information-geometric properties of the Calvo and Oller'
&lt;/p&gt;</description></item><item><title>&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26159;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#20998;&#23376;&#32467;&#26500;&#19978;&#26377;&#23616;&#38480;&#24615;&#12290;&#26368;&#36817;&#65292;&#26377;&#24456;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#31216;&#23427;&#20204;&#27604;&#20256;&#32479;&#26041;&#27861;&#20248;&#36234;&#35768;&#22810;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#26080;&#21442;&#32858;&#31867;&#31639;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30456;&#23218;&#32654;&#25110;&#26356;&#20248;&#65292;&#24076;&#26395;&#36825;&#20010;&#21457;&#29616;&#33021;&#20419;&#36827;MCG&#20013;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20462;&#27491;&#12290;</title><link>http://arxiv.org/abs/2302.07061</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#20013;&#30495;&#30340;&#34920;&#29616;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Deep Learning Methods Really Perform Better in Molecular Conformation Generation?. (arXiv:2302.07061v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07061
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26159;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#20998;&#23376;&#32467;&#26500;&#19978;&#26377;&#23616;&#38480;&#24615;&#12290;&#26368;&#36817;&#65292;&#26377;&#24456;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#31216;&#23427;&#20204;&#27604;&#20256;&#32479;&#26041;&#27861;&#20248;&#36234;&#35768;&#22810;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#26080;&#21442;&#32858;&#31867;&#31639;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30456;&#23218;&#32654;&#25110;&#26356;&#20248;&#65292;&#24076;&#26395;&#36825;&#20010;&#21457;&#29616;&#33021;&#20419;&#36827;MCG&#20013;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26159;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#21644;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#20256;&#32479;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#26469;&#35299;&#20915;MCG&#38382;&#39064;&#65292;&#20363;&#22914;&#31995;&#32479;&#25628;&#32034;&#12289;&#27169;&#22411;&#26500;&#24314;&#12289;&#38543;&#26426;&#25628;&#32034;&#12289;&#36317;&#31163;&#20960;&#20309;&#12289;&#20998;&#23376;&#21160;&#21147;&#23398;&#12289;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#31561;&#31561;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#20998;&#23376;&#32467;&#26500;&#19978;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#26368;&#36817;&#65292;&#26377;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MCG&#26041;&#27861;&#22768;&#31216;&#23427;&#20204;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#24265;&#20215;&#30340;&#31639;&#27861;&#65288;&#26080;&#38656;&#21442;&#25968;&#65289;&#65292;&#22522;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#21457;&#29616;&#23427;&#19982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MCG&#26041;&#27861;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;GEOM-QM9&#21644;GEOM-Drugs&#22522;&#20934;&#27979;&#35797;&#20013;&#21487;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#31639;&#27861;&#21482;&#26159;RDKIT&#29983;&#25104;&#26500;&#35937;&#30340;&#32858;&#31867;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#24110;&#21161;&#31038;&#21306;&#20462;&#27491;MCG&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#20195;&#30721;&#21487;&#22312;https://gis&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular conformation generation (MCG) is a fundamental and important problem in drug discovery. Many traditional methods have been developed to solve the MCG problem, such as systematic searching, model-building, random searching, distance geometry, molecular dynamics, Monte Carlo methods, etc. However, they have some limitations depending on the molecular structures. Recently, there are plenty of deep learning based MCG methods, which claim they largely outperform the traditional methods. However, to our surprise, we design a simple and cheap algorithm (parameter-free) based on the traditional methods and find it is comparable to or even outperforms deep learning based MCG methods in the widely used GEOM-QM9 and GEOM-Drugs benchmarks. In particular, our design algorithm is simply the clustering of the RDKIT-generated conformations. We hope our findings can help the community to revise the deep learning methods for MCG. The code of the proposed algorithm could be found at https://gis
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21363;&#22238;&#39038;&#38142;&#65292;&#21487;&#20197;&#36731;&#26494;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.02676</link><description>&lt;p&gt;
&#22238;&#39038;&#38142;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#21453;&#39304;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21363;&#22238;&#39038;&#38142;&#65292;&#21487;&#20197;&#36731;&#26494;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#36825;&#26679;&#25165;&#33021;&#23545;&#20154;&#31867;&#26377;&#25152;&#24110;&#21161;&#24182;&#31526;&#21512;&#20154;&#31867;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#26469;&#29702;&#35299;&#21644;&#36981;&#24490;&#25351;&#20196;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#26159;&#22522;&#20110;&#34987;&#20154;&#31867;&#27880;&#37322;&#32773;&#21916;&#27426;&#30340;&#25163;&#21160;&#25361;&#36873;&#30340;&#27169;&#22411;&#29983;&#25104;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#25968;&#25454;&#21033;&#29992;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#19988;&#26222;&#36941;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#22870;&#21169;&#20989;&#25968;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#36825;&#23481;&#26131;&#20986;&#29616;&#22870;&#21169;&#20989;&#25968;&#19981;&#23436;&#32654;&#21644;&#26497;&#38590;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#8220;&#22238;&#39038;&#38142;&#8221;&#65292;&#23427;&#26131;&#20110;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#21463;&#21040;&#20102;&#20154;&#31867;&#22914;&#20309;&#20174;&#20197;&#35821;&#35328;&#24418;&#24335;&#21576;&#29616;&#30340;&#24191;&#27867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#31867;&#22411;&#30340;&#21453;&#39304;&#36716;&#25442;&#25104;&#21477;&#23376;&#65292;&#28982;&#21518;&#29992;&#23427;&#20204;&#26469;&#24494;&#35843;&#27169;&#22411;&#65292;&#20174;&#32780;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human preferences is important for language models to be helpful and useful for humans, and to align with human and social values. Prior work have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them ineffective in terms of data utilization and challenging to apply in general, or they depend on reward functions and reinforcement learning, which are prone to imperfect reward function and extremely challenging to optimize. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sentences, which are then used to fine-tune the model, allowing us to take advantage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#26041;&#27861; ESC&#65292;&#23427;&#20174;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#36716;&#31227;&#24120;&#35782;&#30693;&#35782;&#65292;&#21487;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2301.13166</link><description>&lt;p&gt;
ESC&#65306;&#20855;&#22791;&#36719;&#20214;&#24120;&#35782;&#32422;&#26463;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation. (arXiv:2301.13166v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#26041;&#27861; ESC&#65292;&#23427;&#20174;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#36716;&#31227;&#24120;&#35782;&#30693;&#35782;&#65292;&#21487;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#23450;&#20301;&#21644;&#23548;&#33322;&#21040;&#29305;&#23450;&#29289;&#20307;&#30340;&#33021;&#21147;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25805;&#20316;&#24182;&#19982;&#29289;&#20307;&#20132;&#20114;&#20197;&#23436;&#25104;&#20219;&#21153;&#30340;&#23454;&#20307;&#20195;&#29702;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#29289;&#20307;&#23548;&#33322;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#22312;&#20855;&#26377;&#26631;&#35760;&#29289;&#20307;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#36825;&#31181;&#35757;&#32451;&#25928;&#26524;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#26032;&#39062;&#29289;&#20307;&#19978;&#27867;&#21270;&#25928;&#26524;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#26041;&#27861;&#8212;&#8212;&#20855;&#22791;&#36719;&#20214;&#24120;&#35782;&#32422;&#26463;&#30340;&#25506;&#32034;&#65288;ESC&#65289;&#65292;&#23427;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#30340;&#24120;&#35782;&#30693;&#35782;&#36716;&#31227;&#21040;&#22312;&#35270;&#35273;&#29615;&#22659;&#19978;&#36827;&#34892;&#24320;&#25918;&#19990;&#30028;&#29289;&#20307;&#23548;&#33322;&#26102;&#19981;&#38656;&#35201;&#36827;&#34892;&#23548;&#33322;&#25110;&#20854;&#20182;&#35270;&#35273;&#29615;&#22659;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;ESC&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#19990;&#30028;&#22522;&#20110;&#25552;&#31034;&#30340;&#25509;&#22320;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#24120;&#35782;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25151;&#38388;&#21644;&#29289;&#20307;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;ESC&#36890;&#36807;&#23558;&#24120;&#35782;&#30693;&#35782;&#24314;&#27169;&#20026;&#36719;&#36923;&#36753;&#35859;&#35789;&#26469;&#20351;&#20854;&#36716;&#21270;&#20026;&#23548;&#33322;&#21160;&#20316;&#65292;&#20174;&#32780;&#36827;&#34892;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#22312;MP3D&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;......
&lt;/p&gt;
&lt;p&gt;
The ability to accurately locate and navigate to a specific object is a crucial capability for embodied agents that operate in the real world and interact with objects to complete tasks. Such object navigation tasks usually require large-scale training in visual environments with labeled objects, which generalizes poorly to novel objects in unknown environments. In this work, we present a novel zero-shot object navigation method, Exploration with Soft Commonsense constraints (ESC), that transfers commonsense knowledge in pre-trained models to open-world object navigation without any navigation experience nor any other training on the visual environments. First, ESC leverages a pre-trained vision and language model for open-world prompt-based grounding and a pre-trained commonsense language model for room and object reasoning. Then ESC converts commonsense knowledge into navigation actions by modeling it as soft logic predicates for efficient exploration. Extensive experiments on MP3D, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21738;&#20123;&#26679;&#26412;&#26368;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20174;&#32780;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#26368;&#19981;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26816;&#27979;&#22120;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12896</link><description>&lt;p&gt;
&#37492;&#23450;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26679;&#26412;&#21644;&#24378;&#38887;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Identifying Adversarially Attackable and Robust Samples. (arXiv:2301.12896v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21738;&#20123;&#26679;&#26412;&#26368;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20174;&#32780;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#26368;&#19981;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26816;&#27979;&#22120;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#23558;&#24494;&#23567;&#30340;&#65292;&#38590;&#20197;&#24863;&#30693;&#30340;&#25200;&#21160;&#25554;&#20837;&#36755;&#20837;&#26679;&#26412;&#65292;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#21457;&#29983;&#22823;&#37327;&#19981;&#26399;&#26395;&#30340;&#21464;&#21270;&#12290;&#34429;&#28982;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#29983;&#25104;&#21644;&#38450;&#24481;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20174;&#36755;&#20837;&#25968;&#25454;&#35282;&#24230;&#29702;&#35299;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26679;&#26412;&#25915;&#20987;&#24615;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#30830;&#23450;&#26368;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26679;&#26412;&#65288;&#25915;&#20987;&#24615;&#26679;&#26412;&#65289;&#65292;&#20174;&#32780;&#21453;&#36807;&#26469;&#30830;&#23450;&#26368;&#19981;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#30340;&#26679;&#26412;&#65288;&#24378;&#38887;&#26679;&#26412;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#38024;&#23545;&#26410;&#30693;&#30446;&#26631;&#27169;&#22411;&#30340;&#26410;&#35265;&#25968;&#25454;&#38598;&#20013;&#65292;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#24378;&#38887;&#24615;&#26679;&#26412;&#12290;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#28145;&#24230;&#25915;&#20987;&#24615;&#26816;&#27979;&#22120;&#22312;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#21487;&#31227;&#26893;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#22522;&#20110;&#31616;&#21333;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#25514;&#26045;&#30456;&#27604;&#65292;&#28145;&#24230;&#25915;&#20987;&#24615;&#26816;&#27979;&#22120;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks insert small, imperceptible perturbations to input samples that cause large, undesired changes to the output of deep learning models. Despite extensive research on generating adversarial attacks and building defense systems, there has been limited research on understanding adversarial attacks from an input-data perspective. This work introduces the notion of sample attackability, where we aim to identify samples that are most susceptible to adversarial attacks (attackable samples) and conversely also identify the least susceptible samples (robust samples). We propose a deep-learning-based method to detect the adversarially attackable and robust samples in an unseen dataset for an unseen target model. Experiments on standard image classification datasets enables us to assess the portability of the deep attackability detector across a range of architectures. We find that the deep attackability detector performs better than simple model uncertainty-based measures for i
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#25239;&#33258;&#25105;&#30417;&#30563;&#30340;&#27010;&#24565;&#24615;&#26032;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#31454;&#20105;&#24615;Barlow Twins&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#20004;&#20010;GNN&#21487;&#20197;&#19968;&#36215;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2301.11517</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#21327;&#20316;&#23454;&#29616;&#20219;&#21153;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Task-Agnostic Graph Neural Network Evaluation via Adversarial Collaboration. (arXiv:2301.11517v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11517
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#25239;&#33258;&#25105;&#30417;&#30563;&#30340;&#27010;&#24565;&#24615;&#26032;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#31454;&#20105;&#24615;Barlow Twins&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#20004;&#20010;GNN&#21487;&#20197;&#19968;&#36215;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30740;&#31350;&#36827;&#23637;&#30340;&#35780;&#20272;&#38656;&#35201;&#21487;&#38752;&#30340;&#26041;&#27861;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;GNN&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20110;&#27604;&#36739;GNN&#22312;&#26576;&#20123;&#33410;&#28857;/&#22270;&#20998;&#31867;/&#22238;&#24402;&#20219;&#21153;&#21644;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#31181;&#21407;&#21017;&#24615;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#30452;&#25509;&#27604;&#36739;&#20004;&#20010;GNN&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#37117;&#20250;&#23558;&#25968;&#25454;&#25163;&#24037;&#25193;&#22686;&#65292;&#20294;&#30001;&#20110;&#22270;&#30340;&#29305;&#27530;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#30340;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphAC&#65288;Graph Adversarial Collaboration&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#27010;&#24565;&#24615;&#12289;&#21407;&#21017;&#24615;&#12289;&#20219;&#21153;&#26080;&#20851;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#23545;&#27604;&#33258;&#25105;&#30417;&#30563;&#26469;&#35780;&#20272;GNN&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#31454;&#20105;&#24615; Barlow Twins &#30446;&#26631;&#20989;&#25968;&#65292;&#20801;&#35768;&#20004;&#20010; GNN &#20849;&#21516;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been increasingly demanding to develop reliable methods to evaluate the progress of Graph Neural Network (GNN) research for molecular representation learning. Existing GNN benchmarking methods for molecular representation learning focus on comparing the GNNs' performances on some node/graph classification/regression tasks on certain datasets. However, there lacks a principled, task-agnostic method to directly compare two GNNs. Additionally, most of the existing self-supervised learning works incorporate handcrafted augmentations to the data, which has several severe difficulties to be applied on graphs due to their unique characteristics. To address the aforementioned issues, we propose GraphAC (Graph Adversarial Collaboration) -a conceptually novel, principled, task-agnostic, and stable framework for evaluating GNNs through contrastive self-supervision. We introduce a novel objective function: the Competitive Barlow Twins, that allow two GNNs to jointly update themselves from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item><item><title>Boosting&#31639;&#27861;&#26080;&#27861;&#36827;&#34892;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#65292;&#38656;&#35201;&#25351;&#25968;&#32423;&#21035;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21542;&#21017;&#24182;&#34892;&#21270;&#30340;&#25928;&#26524;&#24182;&#19981;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2301.09627</link><description>&lt;p&gt;
Boosting&#31639;&#27861;&#30340;&#24182;&#34892;&#21270;&#19981;&#21487;&#33021;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Impossibility of Parallelizing Boosting. (arXiv:2301.09627v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09627
&lt;/p&gt;
&lt;p&gt;
Boosting&#31639;&#27861;&#26080;&#27861;&#36827;&#34892;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#65292;&#38656;&#35201;&#25351;&#25968;&#32423;&#21035;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21542;&#21017;&#24182;&#34892;&#21270;&#30340;&#25928;&#26524;&#24182;&#19981;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Boosting&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#23558;&#19968;&#31995;&#21015;&#24369;&#23398;&#20064;&#22120;&#38598;&#25104;&#25104;&#19968;&#20010;&#24378;&#23398;&#20064;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#26159;&#23436;&#20840;&#39034;&#24207;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Boosting&#31639;&#27861;&#30340;&#24182;&#34892;&#21270;&#21487;&#33021;&#24615;&#65292;&#21457;&#29616;&#20102;&#19968;&#20010;&#24378;&#28872;&#30340;&#36127;&#38754;&#32467;&#26524;&#65292;&#21363;&#26174;&#33879;&#30340;&#24182;&#34892;&#21270;&#38656;&#35201;&#25351;&#25968;&#32423;&#21035;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#23436;&#25104;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of boosting is to convert a sequence of weak learners into a strong learner. At their heart, these methods are fully sequential. In this paper, we investigate the possibility of parallelizing boosting. Our main contribution is a strong negative result, implying that significant parallelization of boosting requires an exponential blow-up in the total computing resources needed for training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PIRLNav&#65292;&#36890;&#36807;&#20154;&#31867;&#28436;&#31034;&#30340;BC&#39044;&#35757;&#32451;&#21644;RL&#24494;&#35843;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#25104;&#21151;&#29575;&#36798;&#21040;ObjectNav&#30340;65.0&#65285;&#65292;&#27604;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#39640;5.0&#65285;&#12290;</title><link>http://arxiv.org/abs/2301.07302</link><description>&lt;p&gt;
PIRLNav: &#23545;&#20110;ObjectNav&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PIRLNav: Pretraining with Imitation and RL Finetuning for ObjectNav. (arXiv:2301.07302v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PIRLNav&#65292;&#36890;&#36807;&#20154;&#31867;&#28436;&#31034;&#30340;BC&#39044;&#35757;&#32451;&#21644;RL&#24494;&#35843;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#25104;&#21151;&#29575;&#36798;&#21040;ObjectNav&#30340;65.0&#65285;&#65292;&#27604;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#39640;5.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ObjectGoal Navigation&#8212;&#8212;&#22312;&#26032;&#29615;&#22659;&#20013;&#35201;&#27714;&#34394;&#25311;&#26426;&#22120;&#20154;&#23548;&#33322;&#21040;&#19968;&#20010;&#29289;&#20307;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20154;&#31867;&#28436;&#31034;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#23384;&#22312;&#23616;&#38480;&#24615;&#8212;&#8212;1&#65289;&#30001;&#20110;&#35757;&#32451;&#21482;&#27169;&#20223;&#21160;&#20316;&#32780;&#19981;&#26159;&#21518;&#26524;&#65292;&#22240;&#27492;BC&#31574;&#30053;&#23545;&#26032;&#29366;&#24577;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;2&#65289;&#25910;&#38598;&#28436;&#31034;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23481;&#26131;&#25193;&#23637;&#65292;&#20294;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#22870;&#21169;&#26469;&#23454;&#29616;&#29702;&#24819;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;PIRLNav&#65292;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#20154;&#31867;&#28436;&#31034;&#36827;&#34892;BC&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;RL&#24494;&#35843;&#12290;&#36825;&#23548;&#33268;&#35813;&#31574;&#30053;&#22312;ObjectNav&#19978;&#23454;&#29616;&#20102;65.0&#65285;&#30340;&#25104;&#21151;&#29575;&#65288;&#32477;&#23545;&#20540;&#27604;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#39640;5.0&#65285;&#65289;&#12290;&#20351;&#29992;&#36825;&#31181;BC $ \rightarrow $ RL&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#35777;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#20154;&#31867;&#28436;&#31034;&#26159;&#21542;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study ObjectGoal Navigation -- where a virtual robot situated in a new environment is asked to navigate to an object. Prior work has shown that imitation learning (IL) using behavior cloning (BC) on a dataset of human demonstrations achieves promising results. However, this has limitations -- 1) BC policies generalize poorly to new states, since the training mimics actions not their consequences, and 2) collecting demonstrations is expensive. On the other hand, reinforcement learning (RL) is trivially scalable, but requires careful reward engineering to achieve desirable behavior. We present PIRLNav, a two-stage learning scheme for BC pretraining on human demonstrations followed by RL-finetuning. This leads to a policy that achieves a success rate of $65.0\%$ on ObjectNav ($+5.0\%$ absolute over previous state-of-the-art). Using this BC$\rightarrow$RL training recipe, we present a rigorous empirical analysis of design choices. First, we investigate whether human demonstrations can b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AstFocus&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#20887;&#20313;&#26469;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#26799;&#24230;&#20272;&#35745;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#26597;&#35810;&#27425;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#23545;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#35782;&#21035;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#21518;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.00896</link><description>&lt;p&gt;
&#35270;&#39057;&#20013;&#30340;&#23545;&#25239;&#31354;&#38388;-&#26102;&#38388;&#32858;&#28966;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#24378;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus on Videos. (arXiv:2301.00896v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00896
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AstFocus&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#20887;&#20313;&#26469;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#26799;&#24230;&#20272;&#35745;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#26597;&#35810;&#27425;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#23545;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#35782;&#21035;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#21518;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#35782;&#21035;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#35780;&#20272;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;&#19982;&#22270;&#29255;&#30456;&#27604;&#65292;&#35270;&#39057;&#20855;&#26377;&#26356;&#39640;&#30340;&#32500;&#24230;&#65292;&#29983;&#25104;&#23545;&#25239;&#35270;&#39057;&#26102;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#20887;&#20313;&#26469;&#23454;&#29616;&#22312;&#32553;&#23567;&#30340;&#25628;&#32034;&#31354;&#38388;&#19978;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#26799;&#24230;&#20272;&#35745;&#30340;&#26041;&#26696;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#23545;&#25239;&#31354;&#38388;-&#26102;&#38388;&#32858;&#28966;&#65288;AstFocus&#65289;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#20174;&#35270;&#39057;&#30340;&#24103;&#38388;&#21644;&#24103;&#20869;&#21516;&#26102;&#32858;&#28966;&#20851;&#38190;&#24103;&#21644;&#20851;&#38190;&#21306;&#22495;&#36827;&#34892;&#25915;&#20987;&#65292;&#22522;&#20110;&#21512;&#20316;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26694;&#26550;&#26469;&#27169;&#25311;&#23545;&#30446;&#26631;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#23558;&#20854;&#24212;&#29992;&#20110;&#35780;&#20272;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#35782;&#21035;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness assessment for video recognition models has raised concerns owing to their wide applications on safety-critical tasks. Compared with images, videos have much high dimension, which brings huge computational costs when generating adversarial videos. This is especially serious for the query-based black-box attacks where gradient estimation for the threat models is usually utilized, and high dimensions will lead to a large number of queries. To mitigate this issue, we propose to simultaneously eliminate the temporal and spatial redundancy within the video to achieve an effective and efficient gradient estimation on the reduced searching space, and thus query number could decrease. To implement this idea, we design the novel Adversarial spatial-temporal Focus (AstFocus) attack on videos, which performs attacks on the simultaneously focused key frames and key regions from the inter-frames and intra-frames in the video. AstFocus attack is based on the cooperative Multi-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20116;&#31181;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#25216;&#26415;&#22312;&#31934;&#20934;&#39044;&#27979;&#28183;&#36879;&#38408;&#20540;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#21512;&#25104;&#20272;&#35745;&#22120;&#65292;&#22312;&#39044;&#27979;&#20301;&#32622;&#21644;&#29190;&#28856;&#24615;&#28183;&#36879;&#26102;&#20063;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.14694</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#22810;&#31181;&#32593;&#32476;&#28183;&#36879;&#38408;&#20540;&#30340;&#31934;&#20934;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning as an Accurate Predictor for Percolation Threshold of Diverse Networks. (arXiv:2212.14694v2 [physics.soc-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20116;&#31181;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#25216;&#26415;&#22312;&#31934;&#20934;&#39044;&#27979;&#28183;&#36879;&#38408;&#20540;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#21512;&#25104;&#20272;&#35745;&#22120;&#65292;&#22312;&#39044;&#27979;&#20301;&#32622;&#21644;&#29190;&#28856;&#24615;&#28183;&#36879;&#26102;&#20063;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28183;&#36879;&#38408;&#20540;&#26159;&#30830;&#23450;&#22823;&#22411;&#32593;&#32476;&#20869;&#22312;&#21018;&#24230;&#30340;&#37325;&#35201;&#24230;&#37327;&#12290;&#39044;&#27979;&#22823;&#22411;&#32593;&#32476;&#30340;&#28183;&#36879;&#38408;&#20540;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#24320;&#21457;&#19981;&#20381;&#36182;&#25968;&#20540;&#27169;&#25311;&#30340;&#32593;&#32476;&#28183;&#36879;&#38408;&#20540;&#39044;&#27979;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20116;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22238;&#24402;&#25216;&#26415;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#28183;&#36879;&#38408;&#20540;&#12290;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#24635;&#20849;777&#20010;&#30495;&#23454;&#21644;&#21512;&#25104;&#32593;&#32476;&#12290;&#23427;&#21253;&#25324;&#32593;&#32476;&#30340;5&#31181;&#32479;&#35745;&#21644;&#32467;&#26500;&#24615;&#36136;&#20316;&#20026;&#29305;&#24449;&#65292;&#20197;&#21450;&#25968;&#20540;&#35745;&#31639;&#24471;&#20986;&#30340;&#28183;&#36879;&#38408;&#20540;&#20316;&#20026;&#36755;&#20986;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20248;&#20110;&#19977;&#31181;&#29616;&#26377;&#30340;&#21512;&#25104;&#20272;&#35745;&#22120;&#30340;&#38190;&#21512;&#28183;&#36879;&#38408;&#20540;&#65292;&#36824;&#23558;&#27492;&#23454;&#39564;&#25193;&#23637;&#21040;&#39044;&#27979;&#20301;&#32622;&#21644;&#29190;&#28856;&#24615;&#28183;&#36879;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;P&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The percolation threshold is an important measure to determine the inherent rigidity of large networks. Predictors of the percolation threshold for large networks are computationally intense to run, hence it is a necessity to develop predictors of the percolation threshold of networks, that do not rely on numerical simulations. We demonstrate the efficacy of five machine learning-based regression techniques for the accurate prediction of the percolation threshold. The dataset generated to train the machine learning models contains a total of 777 real and synthetic networks. It consists of 5 statistical and structural properties of networks as features and the numerically computed percolation threshold as the output attribute. We establish that the machine learning models outperform three existing empirical estimators of bond percolation threshold, and extend this experiment to predict site and explosive percolation. Further, we compared the performance of our models in predicting the p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;(PCNNs) &#22312;&#27169;&#25311;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;PCNNs&#26082;&#30830;&#20445;&#20102;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21448;&#33021;&#22312;&#22797;&#26434;&#30340;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#21462;&#24471;&#39640;&#31934;&#24230;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#22312;&#21487;&#29992;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2212.12380</link><description>&lt;p&gt;
&#38754;&#21521;&#21487;&#25193;&#23637;&#29289;&#29702;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#22312;&#25968;&#25454;&#39537;&#21160;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models. (arXiv:2212.12380v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;(PCNNs) &#22312;&#27169;&#25311;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;PCNNs&#26082;&#30830;&#20445;&#20102;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21448;&#33021;&#22312;&#22797;&#26434;&#30340;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#21462;&#24471;&#39640;&#31934;&#24230;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#22312;&#21487;&#29992;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#34987;&#25910;&#38598;&#65292;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#22312;&#29289;&#29702;&#19978;&#26159;&#21487;&#38752;&#30340;&#65292;&#20294;&#36890;&#24120;&#24456;&#38590;&#35782;&#21035;&#21644;&#25193;&#23637;&#65292;&#24182;&#19988;&#21463;&#20854;&#26377;&#38480;&#30340;&#34920;&#29616;&#21147;&#24433;&#21709;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#20934;&#30830;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24120;&#24120;&#20381;&#36182;&#31070;&#32463;&#32593;&#32476; (NNs) &#30340;&#32463;&#20856;&#40657;&#30418;&#26041;&#27861;&#36890;&#24120;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#32479;&#35745;&#27169;&#24335;&#65292;&#21363;&#20351;&#22312;&#25193;&#23637;&#26041;&#38754;&#20063;&#33021;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#28508;&#22312;&#30340;&#29289;&#29702;&#23450;&#24459;&#23436;&#20840;&#26080;&#35270;&#65292;&#22914;&#26524;&#22522;&#20110;&#23427;&#20204;&#20570;&#20915;&#31574;&#29992;&#20110;&#23454;&#38469;&#29289;&#29702;&#31995;&#32479;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#26368;&#36817;&#24320;&#21457;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476; (PCNNs) &#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30830;&#20445;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992; NNs &#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558; PCNN &#25193;&#23637;&#21040;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#19982;&#32463;&#20856;&#28784;&#30418;&#21644;&#40657;&#30418;&#26041;&#27861;&#30340;&#24443;&#24213;&#27604;&#36739;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#22810;&#21306;&#22495;&#24314;&#31569;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#21306;&#22495;&#30340;&#28909;&#34892;&#20026;&#30001;&#33021;&#37327;&#24179;&#34913;&#26041;&#31243;&#24335;&#32479;&#27835;&#65292;&#20854;&#21442;&#25968;&#24517;&#39035;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#36827;&#34892;&#35782;&#21035;&#12290;&#25152;&#24471;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#28041;&#21450;&#35768;&#22810;&#30456;&#20114;&#20316;&#29992;&#30340;&#32452;&#20214;&#26500;&#25104;&#30340;&#22797;&#26434;&#21644;&#21160;&#24577;&#31995;&#32479;&#65292;PCNNs &#20063;&#21487;&#20197;&#22312;&#30830;&#20445;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126; PCNN &#22312;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#65292;&#22312;&#26377;&#38480;&#30340;&#21487;&#29992;&#35757;&#32451;&#25968;&#25454;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more data being collected, data-driven modeling methods have been gaining in popularity in recent years. While physically sound, classical gray-box models are often cumbersome to identify and scale, and their accuracy might be hindered by their limited expressiveness. On the other hand, classical black-box methods, typically relying on Neural Networks (NNs) nowadays, often achieve impressive performance, even at scale, by deriving statistical patterns from data. However, they remain completely oblivious to the underlying physical laws, which may lead to potentially catastrophic failures if decisions for real-world physical systems are based on them. Physically Consistent Neural Networks (PCNNs) were recently developed to address these aforementioned issues, ensuring physical consistency while still leveraging NNs to attain state-of-the-art accuracy.  In this work, we scale PCNNs to model building temperature dynamics and propose a thorough comparison with classical gray-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#36873;&#25321;&#24615;&#32553;&#25918;&#65292;&#36890;&#36807;&#23558;&#27491;&#30830;/&#38169;&#35823;&#39044;&#27979;&#20998;&#24320;&#36827;&#34892;&#32553;&#25918;&#65292;&#24182;&#26356;&#21152;&#20851;&#27880;&#38169;&#35823;&#39044;&#27979;&#30340;&#36923;&#36753;&#24179;&#28369;&#65292;&#27492;&#26041;&#27861;&#22312;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.12053</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#65306;&#20998;&#26512;&#19982;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Calibrating Semantic Segmentation Models: Analyses and An Algorithm. (arXiv:2212.12053v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#36873;&#25321;&#24615;&#32553;&#25918;&#65292;&#36890;&#36807;&#23558;&#27491;&#30830;/&#38169;&#35823;&#39044;&#27979;&#20998;&#24320;&#36827;&#34892;&#32553;&#25918;&#65292;&#24182;&#26356;&#21152;&#20851;&#27880;&#38169;&#35823;&#39044;&#27979;&#30340;&#36923;&#36753;&#24179;&#28369;&#65292;&#27492;&#26041;&#27861;&#22312;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#22270;&#20687;&#20998;&#31867;&#32622;&#20449;&#24230;&#30340;&#27169;&#22411;&#35823;&#26657;&#20934;&#65292;&#20294;&#33267;&#20170;&#20026;&#27490;&#65292;&#23545;&#35821;&#20041;&#20998;&#21106;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#30740;&#31350;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#25105;&#20204;&#23545;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#23481;&#37327;&#12289;&#35009;&#21098;&#22823;&#23567;&#12289;&#22810;&#23610;&#24230;&#27979;&#35797;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#23545;&#26657;&#20934;&#26377;&#24433;&#21709;&#12290;&#20854;&#20013;&#65292;&#39044;&#27979;&#27491;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#38169;&#35823;&#39044;&#27979;&#65292;&#23545;&#30001;&#20110;&#36807;&#24230;&#32622;&#20449;&#32780;&#23548;&#33268;&#30340;&#35823;&#26657;&#20934;&#26356;&#20026;&#37325;&#35201;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#32479;&#19968;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#32553;&#25918;&#65292;&#36890;&#36807;&#23558;&#27491;&#30830;/&#38169;&#35823;&#39044;&#27979;&#20998;&#24320;&#36827;&#34892;&#32553;&#25918;&#65292;&#24182;&#26356;&#21152;&#20851;&#27880;&#38169;&#35823;&#39044;&#27979;&#30340;&#36923;&#36753;&#24179;&#28369;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#29616;&#26377;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#36873;&#25321;&#24615;&#32553;&#25918;&#22312;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of semantic segmentation calibration. Lots of solutions have been proposed to approach model miscalibration of confidence in image classification. However, to date, confidence calibration research on semantic segmentation is still limited. We provide a systematic study on the calibration of semantic segmentation models and propose a simple yet effective approach. First, we find that model capacity, crop size, multi-scale testing, and prediction correctness have impact on calibration. Among them, prediction correctness, especially misprediction, is more important to miscalibration due to over-confidence. Next, we propose a simple, unifying, and effective approach, namely selective scaling, by separating correct/incorrect prediction for scaling and more focusing on misprediction logit smoothing. Then, we study popular existing calibration methods and compare them with selective scaling on semantic segmentation calibration. We conduct extensive experiments with a vari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#27861;&#25193;&#23637;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#39044;&#27979;&#26032;&#27010;&#24565;&#30340;&#8220;&#29238;&#27597;&#8221;&#65292;&#28982;&#21518;&#30001;&#20154;&#31867;&#19987;&#23478;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20445;&#35777;&#39044;&#27979;&#30340;&#29238;&#27597;&#36317;&#31163;&#27010;&#24565;&#30340;&#30495;&#23454;&#29238;&#27597;&#8220;&#36817;&#8221;&#65292;&#33021;&#22815;&#25552;&#39640;&#20154;&#31639;&#21327;&#20316;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#26032;&#38395;&#21644;&#23089;&#20048;&#39046;&#22495;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2212.05189</link><description>&lt;p&gt;
&#20154;&#24037;&#21442;&#19982;&#30340;&#30693;&#35782;&#22270;&#35889;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
Expanding Knowledge Graphs with Humans in the Loop. (arXiv:2212.05189v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#27861;&#25193;&#23637;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#39044;&#27979;&#26032;&#27010;&#24565;&#30340;&#8220;&#29238;&#27597;&#8221;&#65292;&#28982;&#21518;&#30001;&#20154;&#31867;&#19987;&#23478;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20445;&#35777;&#39044;&#27979;&#30340;&#29238;&#27597;&#36317;&#31163;&#27010;&#24565;&#30340;&#30495;&#23454;&#29238;&#27597;&#8220;&#36817;&#8221;&#65292;&#33021;&#22815;&#25552;&#39640;&#20154;&#31639;&#21327;&#20316;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#26032;&#38395;&#21644;&#23089;&#20048;&#39046;&#22495;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#24515;&#31574;&#21010;&#30340;&#30693;&#35782;&#22270;&#35889;&#32534;&#30721;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#25552;&#39640;&#25512;&#33616;&#12289;&#20998;&#21106;&#12289;&#24191;&#21578;&#23450;&#21521;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#39046;&#22495;&#20013;&#20986;&#29616;&#26032;&#27010;&#24565;&#65292;&#30693;&#35782;&#22270;&#35889;&#24517;&#39035;&#25193;&#23637;&#20197;&#20445;&#25345;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#22312;&#35268;&#27169;&#19978;&#25163;&#21160;&#25193;&#23637;&#30693;&#35782;&#22270;&#35889;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#25299;&#23637;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39044;&#27979;&#20102;&#38656;&#35201;&#28155;&#21152;&#21040;&#27492;&#22270;&#35889;&#20013;&#30340;&#26032;&#27010;&#24565;&#30340;&#8220;&#29238;&#27597;&#8221;&#65292;&#20197;&#20379;&#20154;&#31867;&#19987;&#23478;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20934;&#30830;&#21448;&#21487;&#35777;&#26126;&#26159;&#8220;&#20154;&#24615;&#21270;&#8221;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#39044;&#27979;&#19981;&#27491;&#30830;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#33021;&#39044;&#27979;&#36317;&#31163;&#27010;&#24565;&#30340;&#30495;&#23454;&#29238;&#27597;&#8220;&#36817;&#8221;&#30340;&#29238;&#27597;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#21463;&#25511;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#28385;&#36275;&#27492;&#23646;&#24615;&#21487;&#20197;&#22686;&#21152;&#20154;&#31639;&#21327;&#20316;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26032;&#38395;&#21644;&#23089;&#20048;&#39046;&#22495;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curated knowledge graphs encode domain expertise and improve the performance of recommendation, segmentation, ad targeting, and other machine learning systems in several domains. As new concepts emerge in a domain, knowledge graphs must be expanded to preserve machine learning performance. Manually expanding knowledge graphs, however, is infeasible at scale. In this work, we propose a method for knowledge graph expansion with humans-in-the-loop. Concretely, given a knowledge graph, our method predicts the "parents" of new concepts to be added to this graph for further verification by human experts. We show that our method is both accurate and provably "human-friendly". Specifically, we prove that our method predicts parents that are "near" concepts' true parents in the knowledge graph, even when the predictions are incorrect. We then show, with a controlled experiment, that satisfying this property increases both the speed and the accuracy of the human-algorithm collaboration. We furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#38754;&#37096;&#35270;&#39057;&#32534;&#36753;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#25552;&#21462;&#20986;&#20998;&#35299;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#31616;&#21333;&#30340;&#29305;&#24449;&#35843;&#25972;&#26469;&#30830;&#20445;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#28385;&#36275;&#37325;&#24314;&#21644;&#32534;&#36753;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#21463;&#37326;&#22806;&#38754;&#37096;&#35270;&#39057;&#30340;&#35282;&#33853;&#24773;&#20917;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.02802</link><description>&lt;p&gt;
&#25193;&#25955;&#35270;&#39057;&#33258;&#32534;&#30721;&#22120;&#65306;&#36890;&#36807;&#20998;&#35299;&#35270;&#39057;&#29305;&#24449;&#23454;&#29616;&#19968;&#33268;&#30340;&#20154;&#33080;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding. (arXiv:2212.02802v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#38754;&#37096;&#35270;&#39057;&#32534;&#36753;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#25552;&#21462;&#20986;&#20998;&#35299;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#31616;&#21333;&#30340;&#29305;&#24449;&#35843;&#25972;&#26469;&#30830;&#20445;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#28385;&#36275;&#37325;&#24314;&#21644;&#32534;&#36753;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#21463;&#37326;&#22806;&#38754;&#37096;&#35270;&#39057;&#30340;&#35282;&#33853;&#24773;&#20917;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#26368;&#36817;&#38754;&#37096;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#30340;&#24778;&#20154;&#34920;&#29616;&#30340;&#21551;&#21457;&#65292;&#33258;&#28982;&#20250;&#26377;&#20960;&#39033;&#30740;&#31350;&#26469;&#25193;&#23637;&#36825;&#20123;&#26041;&#27861;&#20197;&#24212;&#29992;&#20110;&#38754;&#37096;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#12290; &#36825;&#37324;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#32534;&#36753;&#24103;&#20043;&#38388;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#36825;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#38754;&#37096;&#35270;&#39057;&#32534;&#36753;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25104;&#21151;&#22320;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#25552;&#21462;&#20986;&#20998;&#35299;&#30340;&#29305;&#24449;-&#39318;&#27425;&#20316;&#20026;&#38754;&#37096;&#35270;&#39057;&#32534;&#36753;&#27169;&#22411;-&#26631;&#35782;&#21644;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the impressive performance of recent face image editing methods, several studies have been naturally proposed to extend these methods to the face video editing task. One of the main challenges here is temporal consistency among edited frames, which is still unresolved. To this end, we propose a novel face video editing framework based on diffusion autoencoders that can successfully extract the decomposed features - for the first time as a face video editing model - of identity and motion from a given video. This modeling allows us to edit the video by simply manipulating the temporally invariant feature to the desired direction for the consistency. Another unique strength of our model is that, since our model is based on diffusion models, it can satisfy both reconstruction and edit capabilities at the same time, and is robust to corner cases in wild face videos (e.g. occluded faces) unlike the existing GAN-based methods.
&lt;/p&gt;</description></item><item><title>ObjectMatch&#26159;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#23545;&#35937;&#35782;&#21035;&#26469;&#23454;&#29616;&#38388;&#25509;&#23545;&#24212;&#20851;&#31995;&#30340;&#40065;&#26834;&#24615;&#27880;&#20876;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#24456;&#23569;&#25110;&#27809;&#26377;&#24103;&#20043;&#38388;&#37325;&#21472;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312; RGB-D &#24207;&#21015;&#30340;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.01985</link><description>&lt;p&gt;
ObjectMatch: &#20351;&#29992;&#35268;&#33539;&#23545;&#35937;&#23545;&#24212;&#20851;&#31995;&#30340;&#40065;&#26834;&#27880;&#20876;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ObjectMatch: Robust Registration using Canonical Object Correspondences. (arXiv:2212.01985v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01985
&lt;/p&gt;
&lt;p&gt;
ObjectMatch&#26159;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#23545;&#35937;&#35782;&#21035;&#26469;&#23454;&#29616;&#38388;&#25509;&#23545;&#24212;&#20851;&#31995;&#30340;&#40065;&#26834;&#24615;&#27880;&#20876;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#24456;&#23569;&#25110;&#27809;&#26377;&#24103;&#20043;&#38388;&#37325;&#21472;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312; RGB-D &#24207;&#21015;&#30340;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ObjectMatch&#65292;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#21644;&#29289;&#20307;&#20013;&#24515;&#30340;RGB-D SLAM&#30456;&#26426;&#23039;&#24577;&#20272;&#35745;&#22120;&#12290;&#29616;&#20195;&#30456;&#26426;&#23039;&#24577;&#20272;&#35745;&#22120;&#20381;&#36182;&#20110;&#24103;&#20043;&#38388;&#30340;&#30452;&#25509;&#23545;&#24212;&#20851;&#31995;&#65292;&#28982;&#32780;&#23427;&#20204;&#26080;&#27861;&#23545;&#40784;&#20855;&#26377;&#24456;&#23569;&#25110;&#27809;&#26377;&#37325;&#21472;&#30340;&#30456;&#26426;&#24103;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#36890;&#36807;&#35821;&#20041;&#23545;&#35937;&#35782;&#21035;&#33719;&#24471;&#30340;&#38388;&#25509;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#27599;&#20010;&#20687;&#32032;&#32423;&#21035;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#20851;&#38190;&#28857;&#21305;&#37197;&#30456;&#32467;&#21512;&#65292;&#24182;&#20351;&#29992;&#32852;&#21512;&#39640;&#26031;&#29275;&#39039;&#20248;&#21270;&#26469;&#35299;&#20915;&#33021;&#37327;&#20844;&#24335;&#12290;&#22312;&#25104;&#23545;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#30340;&#29305;&#24449;&#21305;&#37197;&#30340;&#27880;&#20876;&#21484;&#22238;&#29575;&#65292;&#21253;&#25324;&#22312;&#24103;&#20043;&#38388;&#37325;&#21472;&#29575;&#20302;&#20110;10%&#30340;&#23545;&#20013;&#65292;&#20174;24%&#21040;45%&#12290;&#22312;&#27880;&#20876;RGB-D&#24207;&#21015;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ObjectMatch, a semantic and object-centric camera pose estimator for RGB-D SLAM pipelines. Modern camera pose estimators rely on direct correspondences of overlapping regions between frames; however, they cannot align camera frames with little or no overlap. In this work, we propose to leverage indirect correspondences obtained via semantic object identification. For instance, when an object is seen from the front in one frame and from the back in another frame, we can provide additional pose constraints through canonical object correspondences. We first propose a neural network to predict such correspondences on a per-pixel level, which we then combine in our energy formulation with state-of-the-art keypoint matching solved with a joint Gauss-Newton optimization. In a pairwise setting, our method improves registration recall of state-of-the-art feature matching, including from 24% to 45% in pairs with 10% or less inter-frame overlap. In registering RGB-D sequences, our meth
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#26032;&#30340;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#32593;&#32476;&#35268;&#33539;&#21270;&#21644;&#36229;&#21442;&#25968;&#25628;&#32034;&#26469;&#25552;&#39640;&#35299;&#37322;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.17174</link><description>&lt;p&gt;
&#36890;&#36807;&#32593;&#32476;&#35268;&#33539;&#21270;&#21644;&#36229;&#21442;&#25968;&#25628;&#32034;&#20248;&#21270;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Optimizing Explanations by Network Canonization and Hyperparameter Search. (arXiv:2211.17174v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17174
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#26032;&#30340;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#32593;&#32476;&#35268;&#33539;&#21270;&#21644;&#36229;&#21442;&#25968;&#25628;&#32034;&#26469;&#25552;&#39640;&#35299;&#37322;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27491;&#22312;&#36880;&#28176;&#25104;&#20026;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#35268;&#21017;&#21644;&#20462;&#25913;&#21518;&#30340;&#21453;&#21521;&#20256;&#25773;XAI&#26041;&#27861;&#24448;&#24448;&#22312;&#24212;&#29992;&#20110;&#29616;&#20195;&#27169;&#22411;&#26550;&#26500;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#21253;&#25324;&#21019;&#26032;&#30340;&#23618;&#26500;&#24314;&#22359;&#65292;&#36825;&#26159;&#30001;&#20004;&#20010;&#21407;&#22240;&#36896;&#25104;&#30340;&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;XAI&#26041;&#27861;&#30340;&#39640;&#28789;&#27963;&#24615;&#23548;&#33268;&#20102;&#35768;&#22810;&#28508;&#22312;&#30340;&#21442;&#25968;&#21270;&#12290;&#20854;&#27425;&#65292;&#35768;&#22810;XAI&#26041;&#27861;&#30772;&#22351;&#20102;&#23454;&#29616;&#19981;&#21464;&#24615;&#20844;&#29702;&#65292;&#22240;&#20026;&#20182;&#20204;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#27169;&#22411;&#32452;&#20214;&#65292;&#20363;&#22914;BatchNorm&#23618;&#12290;&#21518;&#32773;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#35268;&#33539;&#21270;&#26469;&#35299;&#20915;&#65292;&#27169;&#22411;&#35268;&#33539;&#21270;&#26159;&#37325;&#26032;&#32452;&#32455;&#27169;&#22411;&#20197;&#24573;&#30053;&#26377;&#38382;&#39064;&#30340;&#32452;&#20214;&#32780;&#19981;&#25913;&#21464;&#22522;&#26412;&#20989;&#25968;&#30340;&#36807;&#31243;&#12290;&#34429;&#28982;&#23545;&#20110;&#31616;&#21333;&#30340;&#26550;&#26500;&#65288;&#20363;&#22914;VGG&#12289;ResNet&#65289;&#65292;&#27169;&#22411;&#35268;&#33539;&#21270;&#24456;&#31616;&#21333;&#65292;&#20294;&#23545;&#20110;&#26356;&#22797;&#26434;&#21644;&#39640;&#24230;&#20114;&#32852;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;DenseNet&#65289;&#65292;&#23427;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;&#24456;&#23569;&#30340;&#21487;&#37327;&#21270;&#35777;&#25454;&#34920;&#26126;&#27169;&#22411;&#35268;&#33539;&#21270;&#23545;XAI&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) is slowly becoming a key component for many AI applications. Rule-based and modified backpropagation XAI approaches however often face challenges when being applied to modern model architectures including innovative layer building blocks, which is caused by two reasons. Firstly, the high flexibility of rule-based XAI methods leads to numerous potential parameterizations. Secondly, many XAI methods break the implementation-invariance axiom because they struggle with certain model components, e.g., BatchNorm layers. The latter can be addressed with model canonization, which is the process of re-structuring the model to disregard problematic components without changing the underlying function. While model canonization is straightforward for simple architectures (e.g., VGG, ResNet), it can be challenging for more complex and highly interconnected models (e.g., DenseNet). Moreover, there is only little quantifiable evidence that model canonization is beneficial for XAI.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#21487;&#38752;&#24615;&#35780;&#20272;&#25215;&#35834;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21033;&#29992;&#22522;&#20110; GNN &#30340;&#32467;&#26500;&#39044;&#27979;&#21457;&#30005;&#26426;&#30340;&#25215;&#35834;&#21644;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#23545;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#32467;&#26524;&#30340;&#21487;&#34892;&#24615;&#20462;&#22797;&#65292;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.15755</link><description>&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23398;&#20064;&#21487;&#38752;&#24615;&#35780;&#20272;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
Confidence-Aware Graph Neural Networks for Learning Reliability Assessment Commitments. (arXiv:2211.15755v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#21487;&#38752;&#24615;&#35780;&#20272;&#25215;&#35834;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21033;&#29992;&#22522;&#20110; GNN &#30340;&#32467;&#26500;&#39044;&#27979;&#21457;&#30005;&#26426;&#30340;&#25215;&#35834;&#21644;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#23545;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#32467;&#26524;&#30340;&#21487;&#34892;&#24615;&#20462;&#22797;&#65292;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#27604;&#20363;&#22686;&#21152;&#21644;&#39044;&#27979;&#35823;&#24046;&#30340;&#25552;&#39640;&#65292;&#21487;&#38752;&#24615;&#35780;&#20272;&#25215;&#35834;(Reliability Assessment Commitment, RAC)&#20248;&#21270;&#22312;&#30005;&#32593;&#36816;&#34892;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;RAC&#20844;&#24335;&#25193;&#23637;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#23427;&#25552;&#20986;&#20102;RACLearn&#65292;&#21033;&#29992;&#22522;&#20110;Graph Neural Network (GNN)&#30340;&#26550;&#26500;&#26469;&#39044;&#27979;&#21457;&#30005;&#26426;&#30340;&#25215;&#35834;&#21644;&#32447;&#36335;&#32422;&#26463;&#65292;&#20026;&#27599;&#20010;&#25215;&#35834;&#39044;&#27979;&#20851;&#32852;&#19968;&#20010;&#32622;&#20449;&#24230;&#20540;&#65292;&#24182;&#36873;&#25321;&#19968;&#32452;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#65292;&#23545;&#20854;&#36827;&#34892;&#21487;&#34892;&#24615;&#20462;&#22797;&#65292;&#24182;&#21033;&#29992;&#21487;&#34892;&#30340;&#39044;&#27979;&#19982;&#32422;&#26463;&#29366;&#24577;&#24341;&#23548;&#26368;&#20808;&#36827;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RACLearn&#22312;Midcontinent Independent System Operator (MISO)&#20351;&#29992;&#30340;&#20934;&#30830;RAC&#20844;&#24335;&#19978;&#65292;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26174;&#30528;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability Assessment Commitment (RAC) Optimization is increasingly important in grid operations due to larger shares of renewable generations in the generation mix and increased prediction errors. Independent System Operators (ISOs) also aim at using finer time granularities, longer time horizons, and possibly stochastic formulations for additional economic and reliability benefits. The goal of this paper is to address the computational challenges arising in extending the scope of RAC formulations. It presents RACLearn that (1) uses a Graph Neural Network (GNN) based architecture to predict generator commitments and active line constraints, (2) associates a confidence value to each commitment prediction, (3) selects a subset of the high-confidence predictions, which are (4) repaired for feasibility, and (5) seeds a state-of-the-art optimization algorithm with feasible predictions and active constraints. Experimental results on exact RAC formulations used by the Midcontinent Independe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;Shifted Diffusion&#27169;&#22411;&#26356;&#22909;&#22320;&#29983;&#25104;&#26469;&#33258;&#36755;&#20837;&#25991;&#26412;&#30340;&#22270;&#20687;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#21644;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#25903;&#25345;&#21322;&#30417;&#30563;&#21644;&#26080;&#35821;&#35328;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2211.15388</link><description>&lt;p&gt;
Shifted Diffusion&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Shifted Diffusion for Text-to-image Generation. (arXiv:2211.15388v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;Shifted Diffusion&#27169;&#22411;&#26356;&#22909;&#22320;&#29983;&#25104;&#26469;&#33258;&#36755;&#20837;&#25991;&#26412;&#30340;&#22270;&#20687;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#21644;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#25903;&#25345;&#21322;&#30417;&#30563;&#21644;&#26080;&#35821;&#35328;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Corgi&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;Corgi&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;Shifted Diffusion&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#26469;&#33258;&#36755;&#20837;&#25991;&#26412;&#30340;&#22270;&#20687;&#23884;&#20837;&#12290;&#19982;DALL-E 2&#20013;&#20351;&#29992;&#30340;&#22522;&#32447;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#21021;&#22987;&#21270;&#20998;&#24067;&#21644;&#25193;&#25955;&#30340;&#26032;&#36807;&#28193;&#27493;&#39588;&#65292;&#22312;&#20854;&#25193;&#25955;&#36807;&#31243;&#20013;&#26080;&#32541;&#22320;&#32534;&#30721;&#20102;&#39044;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#19982;&#24378;&#21170;&#30340;DALL-E 2&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20174;&#25991;&#26412;&#20013;&#29983;&#25104;&#22270;&#20687;&#23884;&#20837;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#24182;&#22312;&#23450;&#37327;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#24378;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#21322;&#30417;&#30563;&#21644;&#26080;&#35821;&#35328;&#35757;&#32451;&#65292;&#21482;&#38656;&#35201;&#37096;&#20998;&#25110;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#19982;&#36755;&#20837;&#25991;&#26412;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Corgi, a novel method for text-to-image generation. Corgi is based on our proposed shifted diffusion model, which achieves better image embedding generation from input text. Unlike the baseline diffusion model used in DALL-E 2, our method seamlessly encodes prior knowledge of the pre-trained CLIP model in its diffusion process by designing a new initialization distribution and a new transition step of the diffusion. Compared to the strong DALL-E 2 baseline, our method performs better in generating image embedding from the text in terms of both efficiency and effectiveness, resulting in better text-to-image generation. Extensive large-scale experiments are conducted and evaluated in terms of both quantitative measures and human evaluation, indicating a stronger generation ability of our method compared to existing ones. Furthermore, our model enables semi-supervised and language-free training for text-to-image generation, where only part or none of the images in the training 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#38382;&#39064;&#65292;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; MUPPET &#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#22810;&#27169;&#24577;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#22810;&#27169;&#24577;&#32858;&#31867;&#31639;&#27861;&#26469;&#32452;&#21512;&#26102;&#24207;&#36830;&#32493;&#30340;&#29255;&#27573;&#65292;&#35299;&#20915;&#20102;&#38382;&#39064;&#12290;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#19981;&#21516;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14905</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Few-Shot Temporal Action Detection. (arXiv:2211.14905v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14905
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#38382;&#39064;&#65292;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; MUPPET &#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#22810;&#27169;&#24577;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#22810;&#27169;&#24577;&#32858;&#31867;&#31639;&#27861;&#26469;&#32452;&#21512;&#26102;&#24207;&#36830;&#32493;&#30340;&#29255;&#27573;&#65292;&#35299;&#20915;&#20102;&#38382;&#39064;&#12290;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#19981;&#21516;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412; (FS) &#21644;&#38646;&#26679;&#26412; (ZS) &#23398;&#20064;&#26159;&#32553;&#25918;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979; (TAD) &#21040;&#26032;&#31867;&#30340;&#20004;&#31181;&#19981;&#21516;&#26041;&#27861;&#12290;&#21069;&#32773;&#23558;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#36866;&#24212;&#20110;&#26032;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#30001;&#27599;&#31867;&#20165;&#26377;&#19968;&#20010;&#35270;&#39057;&#34920;&#31034;&#65292;&#32780;&#21518;&#32773;&#36890;&#36807;&#21033;&#29992;&#26032;&#31867;&#30340;&#35821;&#20041;&#25551;&#36848;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#31034;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412; (MMFS) TAD &#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#34987;&#35270;&#20026;&#36890;&#36807;&#20849;&#21516;&#21033;&#29992;&#23569;&#25968;&#25903;&#25345;&#35270;&#39057;&#21644;&#26032;&#31867;&#21517;&#23383;&#26469;&#32467;&#21512; FS-TAD &#21644; ZS-TAD &#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; MUlti-modality PromPt mETa-learning (MUPPET) &#26041;&#27861;&#12290;&#36825;&#26159;&#36890;&#36807;&#26377;&#25928;&#22320;&#36830;&#25509;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#37325;&#29992;&#24050;&#32463;&#23398;&#20064;&#30340;&#33021;&#21147;&#26469;&#23454;&#29616;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20803;&#23398;&#20064;&#36866;&#37197;&#22120;&#35013;&#22791;&#30340;&#35270;&#35273;&#35821;&#20041;&#20998;&#35789;&#22120;&#65292;&#23558;&#25903;&#25345;&#35270;&#39057;&#26144;&#23556;&#21040;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#26631;&#35760;&#31354;&#38388;&#20013;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#30340;&#31867;&#20869;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#32858;&#31867;&#31639;&#27861;&#26469;&#23545;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#25552;&#35758;&#29255;&#27573;&#36827;&#34892;&#20998;&#32452;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#19981;&#21516;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot (FS) and zero-shot (ZS) learning are two different approaches for scaling temporal action detection (TAD) to new classes. The former adapts a pretrained vision model to a new task represented by as few as a single video per class, whilst the latter requires no training examples by exploiting a semantic description of the new class. In this work, we introduce a new multi-modality few-shot (MMFS) TAD problem, which can be considered as a marriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and new class names jointly. To tackle this problem, we further introduce a novel MUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled by efficiently bridging pretrained vision and language models whilst maximally reusing already learned capacity. Concretely, we construct multi-modal prompts by mapping support videos into the textual token space of a vision-language model using a meta-learned adapter-equipped visual semantics tokenizer. To tackle large intra-clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21098;&#26525;&#25216;&#26415;&#35774;&#35745;&#21387;&#32553;&#29256;&#30340;&#39640;&#24230;&#21387;&#32553;&#36731;&#37327;&#32423;&#29289;&#20307;&#36319;&#36394;&#22120;&#12290;&#36890;&#36807;&#23545;CNN&#21644;Transformers&#36319;&#36394;&#22120;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#25581;&#31034;&#20986;&#22312;&#35774;&#35745;&#36731;&#37327;&#32423;&#36319;&#36394;&#22120;&#26102;&#30340;&#26368;&#20339;&#26550;&#26500;&#36873;&#25321;&#12290;&#26368;&#21518;&#65292;&#25552;&#20379;&#20102;&#26497;&#31471;&#21098;&#26525;&#29575;&#30340;&#36319;&#36394;&#32467;&#26524;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#20102;&#35299;&#32593;&#32476;&#21098;&#26525;&#22312;&#29289;&#20307;&#36319;&#36394;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2211.13769</link><description>&lt;p&gt;
&#36890;&#36807;&#32593;&#32476;&#21098;&#26525;&#35774;&#35745;&#36731;&#37327;&#32423;&#29289;&#20307;&#36319;&#36394;&#22120;&#65306;&#20351;&#29992;CNN&#36824;&#26159;Transformers?
&lt;/p&gt;
&lt;p&gt;
On Designing Light-Weight Object Trackers through Network Pruning: Use CNNs or Transformers?. (arXiv:2211.13769v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21098;&#26525;&#25216;&#26415;&#35774;&#35745;&#21387;&#32553;&#29256;&#30340;&#39640;&#24230;&#21387;&#32553;&#36731;&#37327;&#32423;&#29289;&#20307;&#36319;&#36394;&#22120;&#12290;&#36890;&#36807;&#23545;CNN&#21644;Transformers&#36319;&#36394;&#22120;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#25581;&#31034;&#20986;&#22312;&#35774;&#35745;&#36731;&#37327;&#32423;&#36319;&#36394;&#22120;&#26102;&#30340;&#26368;&#20339;&#26550;&#26500;&#36873;&#25321;&#12290;&#26368;&#21518;&#65292;&#25552;&#20379;&#20102;&#26497;&#31471;&#21098;&#26525;&#29575;&#30340;&#36319;&#36394;&#32467;&#26524;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#20102;&#35299;&#32593;&#32476;&#21098;&#26525;&#22312;&#29289;&#20307;&#36319;&#36394;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#29289;&#20307;&#36319;&#36394;&#22120;&#38656;&#35201;&#36731;&#37327;&#32423;&#35774;&#35745;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#26500;&#24314;&#20110;CNN&#25110;transformers&#19978;&#30340;&#35745;&#31639;&#23494;&#38598;&#30340;&#20027;&#24178;&#32593;&#32476;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#24040;&#22823;&#23610;&#23544;&#19981;&#20801;&#35768;&#22312;&#20302;&#21151;&#32791;&#26465;&#20214;&#19979;&#37096;&#32626;&#23427;&#20204;&#65292;&#22240;&#27492;&#35774;&#35745;&#21387;&#32553;&#29256;&#30340;&#22823;&#22411;&#36319;&#36394;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#31070;&#32463;&#32467;&#26500;&#21098;&#26525;&#35774;&#35745;&#39640;&#24230;&#21387;&#32553;&#30340;&#36731;&#37327;&#32423;&#29289;&#20307;&#36319;&#36394;&#22120;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#27604;&#36739;&#30740;&#31350;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#35774;&#35745;&#36731;&#37327;&#32423;&#36319;&#36394;&#22120;&#30340;&#26550;&#26500;&#36873;&#25321;&#12290;&#36824;&#25552;&#20379;&#20102;&#20351;&#29992;CNNs&#12289;transformers&#20197;&#21450;&#20004;&#32773;&#32452;&#21512;&#30340;&#26368;&#26032;&#36319;&#36394;&#22120;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#20197;&#30740;&#31350;&#23427;&#20204;&#22312;&#21508;&#31181;&#21387;&#32553;&#27604;&#19979;&#30340;&#31283;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#23637;&#31034;&#20102;&#26497;&#31471;&#21098;&#26525;&#22330;&#26223;&#19979;&#30340;&#32467;&#26524;&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#21098;&#26525;&#27604;&#29575;&#20302;&#33267;1%&#65292;&#20197;&#30740;&#31350;&#32593;&#32476;&#21098;&#26525;&#22312;&#29289;&#20307;&#36319;&#36394;&#20013;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object trackers deployed on low-power devices need to be light-weight, however, most of the current state-of-the-art (SOTA) methods rely on using compute-heavy backbones built using CNNs or transformers. Large sizes of such models do not allow their deployment in low-power conditions and designing compressed variants of large tracking models is of great importance. This paper demonstrates how highly compressed light-weight object trackers can be designed using neural architectural pruning of large CNN and transformer based trackers. Further, a comparative study on architectural choices best suited to design light-weight trackers is provided. A comparison between SOTA trackers using CNNs, transformers as well as the combination of the two is presented to study their stability at various compression ratios. Finally results for extreme pruning scenarios going as low as 1% in some cases are shown to study the limits of network pruning in object tracking. This work provides deeper insights 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; BiasBed&#65292;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#20005;&#26684;&#35780;&#20272;&#38477;&#20302;&#32441;&#29702;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#37197;&#22791;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#21327;&#35758;&#20197;&#25581;&#31034;&#20854;&#26174;&#33879;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.13190</link><description>&lt;p&gt;
BiasBed -- &#20005;&#26684;&#30340;&#32441;&#29702;&#20559;&#24046;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
BiasBed -- Rigorous Texture Bias Evaluation. (arXiv:2211.13190v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; BiasBed&#65292;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#20005;&#26684;&#35780;&#20272;&#38477;&#20302;&#32441;&#29702;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#37197;&#22791;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#21327;&#35758;&#20197;&#25581;&#31034;&#20854;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#32441;&#29702;&#20559;&#24046;&#30340;&#38382;&#39064;&#24050;&#26377;&#20805;&#20998;&#25991;&#29486;&#35777;&#26126;&#65292;&#36825;&#23548;&#33268;&#20986;&#29616;&#20102;&#22823;&#37327;&#31639;&#27861;&#65292;&#24378;&#35843;&#24418;&#29366;&#32447;&#32034;&#65292;&#20197;&#25903;&#25345;&#21040;&#26032;&#39046;&#22495;&#30340;&#27010;&#25324;&#12290;&#28982;&#32780;&#65292;&#26222;&#36890;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#19968;&#33324;&#30340;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#37117;&#32570;&#22833;&#65292;&#19988;&#27809;&#26377;&#20849;&#35782;&#30340;&#12289;&#20005;&#26684;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38477;&#20302;&#20102;&#32441;&#29702;&#20559;&#24046;&#30340;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#26102;&#30340;&#22256;&#38590;&#21644;&#38480;&#21046;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36866;&#24403;&#30340;&#35780;&#20272;&#21644;&#26041;&#27861;&#20043;&#38388;&#26377;&#24847;&#20041;&#30340;&#27604;&#36739;&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#24773;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; BiasBed&#65292;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#32441;&#29702;&#21644;&#39118;&#26684;&#20559;&#24046;&#35757;&#32451;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#21253;&#25324;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#31995;&#21015;&#29616;&#26377;&#31639;&#27861;&#12290;&#23427;&#37197;&#22791;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#21253;&#25324;&#20005;&#26684;&#30340;&#20551;&#35774;&#26816;&#39564;&#65292;&#20197;&#34913;&#37327;&#32467;&#26524;&#30340;&#26174;&#33879;&#24615;&#65292;&#23613;&#31649;&#19968;&#20123;&#39118;&#26684;&#20559;&#24046;&#26041;&#27861;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#38656;&#35201;&#20180;&#32454;&#12289;&#22522;&#20110;&#32479;&#35745;&#30340;&#23545;&#32441;&#29702;&#20559;&#24046;&#38477;&#20302;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#20005;&#26684;&#30340;&#35780;&#20272;&#21327;&#35758;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The well-documented presence of texture bias in modern convolutional neural networks has led to a plethora of algorithms that promote an emphasis on shape cues, often to support generalization to new domains. Yet, common datasets, benchmarks and general model selection strategies are missing, and there is no agreed, rigorous evaluation protocol. In this paper, we investigate difficulties and limitations when training networks with reduced texture bias. In particular, we also show that proper evaluation and meaningful comparisons between methods are not trivial. We introduce BiasBed, a testbed for textureand style-biased training, including multiple datasets and a range of existing algorithms. It comes with an extensive evaluation protocol that includes rigorous hypothesis testing to gauge the significance of the results, despite the considerable training instability of some style bias methods. Our extensive experiments, shed new light on the need for careful, statistically founded ev
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#22686;&#25439;&#22833;&#39033;&#26368;&#23567;&#21270;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#25968;&#25454;&#38598;&#31934;&#28860;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#32593;&#32476;&#26550;&#26500;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.11004</link><description>&lt;p&gt;
&#36890;&#36807;&#20943;&#23569;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#26469;&#25552;&#39640;&#25968;&#25454;&#38598;&#31934;&#28860;&#25928;&#26524;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation. (arXiv:2211.11004v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11004
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#22686;&#25439;&#22833;&#39033;&#26368;&#23567;&#21270;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#25968;&#25454;&#38598;&#31934;&#28860;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#32593;&#32476;&#26550;&#26500;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#22914;&#27492;&#22823;&#37327;&#30340;&#25968;&#25454;&#22312;&#35745;&#31639;&#12289;&#23384;&#20648;&#12289;&#35757;&#32451;&#21644;&#25628;&#23547;&#33391;&#22909;&#30340;&#31070;&#32463;&#32467;&#26500;&#31561;&#26041;&#38754;&#25104;&#26412;&#30456;&#24403;&#39640;&#26114;&#65292;&#22240;&#27492;&#25968;&#25454;&#38598;&#31934;&#28860;&#36817;&#26399;&#25104;&#20026;&#28966;&#28857;&#12290;&#36825;&#31181;&#33539;&#24335;&#28041;&#21450;&#33719;&#21462;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#24182;&#23558;&#20854;&#25552;&#28860;&#20026;&#24494;&#23567;&#32039;&#20945;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#22788;&#29702;&#21518;&#32773;&#30340;&#34920;&#29616;&#31867;&#20284;&#20110;&#21069;&#32773;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#38752;&#23398;&#20064;&#36890;&#36807;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#20043;&#38388;&#21305;&#37197;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#30340;&#26799;&#24230;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26799;&#24230;&#21305;&#37197;&#26041;&#27861;&#21463;&#21040;&#25152;&#35859;&#30340;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#27492;&#35823;&#24046;&#26159;&#30001;&#20110;&#31934;&#28860;&#21644;&#21518;&#32493;&#35780;&#20272;&#20043;&#38388;&#19981;&#19968;&#33268;&#23548;&#33268;&#30340;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#40723;&#21169;&#23558;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#20174;&#35757;&#32451;&#38454;&#27573;&#36716;&#31227;&#21040;&#31934;&#28860;&#38454;&#27573;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25439;&#22833;&#39033;&#65292;&#22312;&#31934;&#28860;&#36807;&#31243;&#20013;&#26368;&#23567;&#21270;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#32593;&#32476;&#26550;&#26500;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24182;&#26174;&#33879;&#20943;&#23569;&#32047;&#31215;&#36712;&#36857;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based deep learning has achieved astounding successes due in part to the availability of large-scale real-world data. However, processing such massive amounts of data comes at a considerable cost in terms of computations, storage, training and the search for good neural architectures. Dataset distillation has thus recently come to the fore. This paradigm involves distilling information from large real-world datasets into tiny and compact synthetic datasets such that processing the latter ideally yields similar performances as the former. State-of-the-art methods primarily rely on learning the synthetic dataset by matching the gradients obtained during training between the real and synthetic data. However, these gradient-matching methods suffer from the so-called accumulated trajectory error caused by the discrepancy between the distillation and subsequent evaluation. To mitigate the adverse impact of this accumulated trajectory error, we propose a novel approach that encourages t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#30340;Magic3D&#26041;&#27861;&#65292;&#37319;&#29992;&#20004;&#27493;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#27604;&#20043;&#21069;&#30340;DreamFusion&#26041;&#27861;&#24555;2&#20493;&#21019;&#24314;&#39640;&#36136;&#37327;3D&#32593;&#26684;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;NeRF&#20248;&#21270;&#36895;&#24230;&#26497;&#24930;&#21644;&#23545;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#31354;&#38388;&#30417;&#30563;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2211.10440</link><description>&lt;p&gt;
Magic3D&#65306;&#39640;&#20998;&#36776;&#29575;&#25991;&#26412;&#21040;3D&#20869;&#23481;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Magic3D: High-Resolution Text-to-3D Content Creation. (arXiv:2211.10440v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#30340;Magic3D&#26041;&#27861;&#65292;&#37319;&#29992;&#20004;&#27493;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#27604;&#20043;&#21069;&#30340;DreamFusion&#26041;&#27861;&#24555;2&#20493;&#21019;&#24314;&#39640;&#36136;&#37327;3D&#32593;&#26684;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;NeRF&#20248;&#21270;&#36895;&#24230;&#26497;&#24930;&#21644;&#23545;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#31354;&#38388;&#30417;&#30563;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;DreamFusion&#23637;&#31034;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;text-to-image diffusion model&#65289;&#20248;&#21270;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;Neural Radiance Fields&#65292;NeRF&#65289;&#30340;&#23454;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#24778;&#20154;&#30340;&#25991;&#26412;&#21040;3D&#21512;&#25104;&#32467;&#26524;&#12290;&#28982;&#32780;&#35813;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#22266;&#26377;&#38480;&#21046;&#65306;&#65288;a&#65289;NeRF&#30340;&#20248;&#21270;&#36895;&#24230;&#26497;&#24930;&#65292;&#65288;b&#65289;&#23545;NeRF&#30340;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#31354;&#38388;&#30417;&#30563;&#65292;&#23548;&#33268;&#38271;&#26102;&#38388;&#22788;&#29702;&#30340;&#20302;&#36136;&#37327;3D&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#20004;&#27493;&#20248;&#21270;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20302;&#20998;&#36776;&#29575;&#25193;&#25955;&#20808;&#39564;&#33719;&#24471;&#19968;&#20010;&#31895;&#31961;&#27169;&#22411;&#65292;&#24182;&#21152;&#36895;&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#30340;3D&#21704;&#24076;&#32593;&#26684;&#32467;&#26500;&#12290;&#20351;&#29992;&#31895;&#30053;&#34920;&#31034;&#20316;&#20026;&#21021;&#22987;&#21270;&#65292;&#25105;&#20204;&#37319;&#29992;&#39640;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30456;&#20114;&#20316;&#29992;&#30340;&#39640;&#25928;&#21487;&#24494;&#28210;&#26579;&#22120;&#36827;&#19968;&#27493;&#20248;&#21270;&#32441;&#29702;3D&#32593;&#26684;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;Magic3D&#65292;&#21487;&#20197;&#22312;40&#20998;&#38047;&#20869;&#21019;&#24314;&#39640;&#36136;&#37327;3D&#32593;&#26684;&#27169;&#22411;&#65292;&#36825;&#27604;DreamFusion&#65288;&#25454;&#25253;&#36947;&#24179;&#22343;&#38656;&#35201;1.5&#23567;&#26102;&#65289;&#24555;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
DreamFusion has recently demonstrated the utility of a pre-trained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF), achieving remarkable text-to-3D synthesis results. However, the method has two inherent limitations: (a) extremely slow optimization of NeRF and (b) low-resolution image space supervision on NeRF, leading to low-quality 3D models with a long processing time. In this paper, we address these limitations by utilizing a two-stage optimization framework. First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure. Using the coarse representation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer interacting with a high-resolution latent diffusion model. Our method, dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is 2x faster than DreamFusion (reportedly taking 1.5 hours on average), while also achieving hi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#24314;&#27169;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#24847;&#22270;&#20316;&#20026;&#28508;&#22312;&#21464;&#37327;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#24182;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#20307;&#39564;&#65292;&#35777;&#26126;&#20854;&#22312;&#31163;&#32447;&#20998;&#26512;&#21644;&#23454;&#26102;&#23454;&#39564;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.09832</link><description>&lt;p&gt;
&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#22120;&#30340;&#28508;&#22312;&#29992;&#25143;&#24847;&#22270;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Latent User Intent Modeling for Sequential Recommenders. (arXiv:2211.09832v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09832
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#24314;&#27169;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#24847;&#22270;&#20316;&#20026;&#28508;&#22312;&#21464;&#37327;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#24182;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#20307;&#39564;&#65292;&#35777;&#26126;&#20854;&#22312;&#31163;&#32447;&#20998;&#26512;&#21644;&#23454;&#26102;&#23454;&#39564;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#27169;&#22411;&#26159;&#29616;&#20195;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#22312;&#24179;&#21488;&#19978;&#30340;&#20114;&#21160;&#21382;&#21490;&#65292;&#39044;&#27979;&#29992;&#25143;&#21487;&#33021;&#20250;&#19982;&#21738;&#20123;&#39033;&#30446;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24207;&#21015;&#25512;&#33616;&#22120;&#32570;&#20047;&#23545;&#29992;&#25143;&#24847;&#22270;&#30340;&#39640;&#32423;&#29702;&#35299;&#65292;&#32780;&#29992;&#25143;&#24847;&#22270;&#36890;&#24120;&#39537;&#21160;&#30528;&#22312;&#32447;&#29992;&#25143;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#24847;&#22270;&#24314;&#27169;&#23545;&#20110;&#29702;&#35299;&#29992;&#25143;&#24182;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#24847;&#22270;&#20316;&#20026;&#28508;&#22312;&#21464;&#37327;&#65292;&#24182;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26681;&#25454;&#29992;&#25143;&#34892;&#20026;&#20449;&#21495;&#36827;&#34892;&#25512;&#26029;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#25512;&#26029;&#20986;&#30340;&#29992;&#25143;&#24847;&#22270;&#35843;&#25972;&#25512;&#33616;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#31163;&#32447;&#20998;&#26512;&#21644;&#22823;&#35268;&#27169;&#24037;&#19994;&#25512;&#33616;&#24179;&#21488;&#19978;&#30340;&#23454;&#26102;&#23454;&#39564;&#35777;&#26126;&#20102;&#28508;&#22312;&#29992;&#25143;&#24847;&#22270;&#24314;&#27169;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommender models are essential components of modern industrial recommender systems. These models learn to predict the next items a user is likely to interact with based on his/her interaction history on the platform. Most sequential recommenders however lack a higher-level understanding of user intents, which often drive user behaviors online. Intent modeling is thus critical for understanding users and optimizing long-term user experience. We propose a probabilistic modeling approach and formulate user intent as latent variables, which are inferred based on user behavior signals using variational autoencoders (VAE). The recommendation policy is then adjusted accordingly given the inferred user intent. We demonstrate the effectiveness of the latent user intent modeling via offline analyses as well as live experiments on a large-scale industrial recommendation platform.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21452;&#21521;&#32852;&#24819;&#35760;&#24518;&#30340;&#24179;&#34913;&#24615;&#36136;&#65292;&#34920;&#24449;&#20102;&#20854;&#38543;&#26426;&#25193;&#23637;&#22312;&#28909;&#21147;&#23398;&#26497;&#38480;&#19979;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#26377;&#38480;&#28201;&#24230;&#21644;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#30456;&#22270;&#65292;&#20998;&#26512;&#20102;&#20020;&#30028;&#36127;&#36733;&#12290;</title><link>http://arxiv.org/abs/2211.09694</link><description>&lt;p&gt;
&#21452;&#21521;&#32852;&#24819;&#35760;&#24518;&#30340;&#28909;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Thermodynamics of bidirectional associative memories. (arXiv:2211.09694v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09694
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21452;&#21521;&#32852;&#24819;&#35760;&#24518;&#30340;&#24179;&#34913;&#24615;&#36136;&#65292;&#34920;&#24449;&#20102;&#20854;&#38543;&#26426;&#25193;&#23637;&#22312;&#28909;&#21147;&#23398;&#26497;&#38480;&#19979;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#26377;&#38480;&#28201;&#24230;&#21644;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#30456;&#22270;&#65292;&#20998;&#26512;&#20102;&#20020;&#30028;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21452;&#21521;&#32852;&#24819;&#35760;&#24518;&#65288;BAM&#65289;&#30340;&#24179;&#34913;&#24615;&#36136;&#12290;&#26368;&#31616;&#21333;&#30340;&#32467;&#26500;&#26159;&#30001;&#20004;&#23618;&#31070;&#32463;&#20803;&#23450;&#20041;&#30340;&#65292;&#20165;&#22312;&#19981;&#21516;&#23618;&#20043;&#38388;&#30340;&#21333;&#20803;&#20043;&#38388;&#23384;&#22312;&#31361;&#35302;&#36830;&#25509;&#65306;&#21363;&#20351;&#22312;&#27599;&#23618;&#20869;&#27809;&#26377;&#20869;&#37096;&#36830;&#25509;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#31070;&#32463;&#27963;&#21160;&#22312;&#23618;&#19982;&#23618;&#20043;&#38388;&#20256;&#36882;&#65292;&#20449;&#24687;&#30340;&#23384;&#20648;&#21644;&#26816;&#32034;&#20173;&#28982;&#26159;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#32479;&#35745;&#29289;&#29702;&#30340;&#20005;&#26684;&#25216;&#26415;&#65292;&#34920;&#24449;&#20102;&#27492;&#27169;&#22411;&#30340;&#38543;&#26426;&#25193;&#23637;&#22312;&#28909;&#21147;&#23398;&#26497;&#38480;&#19979;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22797;&#21046;&#23545;&#31216;&#23618;&#38754;&#19978;&#30456;&#22270;&#30340;&#35814;&#32454;&#25551;&#32472;&#65292;&#21253;&#25324;&#26377;&#38480;&#28201;&#24230;&#21644;&#26080;&#22122;&#22768;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20020;&#30028;&#36127;&#36733;&#65292;&#30452;&#21040;&#19968;&#27493;&#22797;&#21046;&#23545;&#31216;&#24615;&#30772;&#32570;&#12290;&#25105;&#20204;&#23545;&#36716;&#25442;&#26354;&#32447;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#25968;&#20540;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we investigate the equilibrium properties of bidirectional associative memories (BAMs). Introduced by Kosko in 1988 as a generalization of the Hopfield model to a bipartite structure, the simplest architecture is defined by two layers of neurons, with synaptic connections only between units of different layers: even without internal connections within each layer, information storage and retrieval are still possible through the reverberation of neural activities passing from one layer to another. We characterize the computational capabilities of a stochastic extension of this model in the thermodynamic limit, by applying rigorous techniques from statistical physics. A detailed picture of the phase diagram at the replica symmetric level is provided, both at finite temperature and in the noiseless regimes. Also for the latter, the critical load is further investigated up to one step of replica symmetry breaking. An analytical and numerical inspection of the transition curves
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23646;&#24615;&#36873;&#25321;&#26426;&#21046;&#30340;&#22825;&#28982;&#21487;&#35299;&#37322;&#27169;&#22411;&#26469;&#22788;&#29702;&#23567;&#26679;&#26412;&#23398;&#20064;&#65292;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;episode&#20013;&#28041;&#21450;&#30340;&#23646;&#24615;&#25968;&#37327;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#26816;&#27979;&#24182;&#34917;&#20607;&#20154;&#24037;&#26234;&#33021;&#23646;&#24615;&#27744;&#19981;&#36275;&#30340;episode&#12290;</title><link>http://arxiv.org/abs/2211.09107</link><description>&lt;p&gt;
&#22312;&#32447;&#23646;&#24615;&#36873;&#25321;&#30340;&#21487;&#35299;&#37322;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interpretable Few-shot Learning with Online Attribute Selection. (arXiv:2211.09107v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23646;&#24615;&#36873;&#25321;&#26426;&#21046;&#30340;&#22825;&#28982;&#21487;&#35299;&#37322;&#27169;&#22411;&#26469;&#22788;&#29702;&#23567;&#26679;&#26412;&#23398;&#20064;&#65292;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;episode&#20013;&#28041;&#21450;&#30340;&#23646;&#24615;&#25968;&#37327;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#26816;&#27979;&#24182;&#34917;&#20607;&#20154;&#24037;&#26234;&#33021;&#23646;&#24615;&#27744;&#19981;&#36275;&#30340;episode&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#23398;&#20064;(few-shot learning, FSL)&#26159;&#19968;&#31181;&#25361;&#25112;&#24615;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#24456;&#23569;&#30340;&#26679;&#26412;&#21487;&#29992;&#12290;&#22312;FSL&#20013;&#20915;&#31574;&#30340;&#35299;&#37322;&#27604;&#20256;&#32479;&#20998;&#31867;&#26356;&#21152;&#37325;&#35201;&#65292;&#22240;&#20026;&#38169;&#35823;&#30340;&#20960;&#29575;&#26356;&#22823;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;FSL&#26041;&#27861;&#37117;&#26159;&#40657;&#21283;&#23376;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26131;&#20110;&#29702;&#35299;&#30340;&#23646;&#24615;&#30340;&#22825;&#28982;&#21487;&#35299;&#37322;&#27169;&#22411;&#26469;&#22788;&#29702;FSL&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23646;&#24615;&#36873;&#25321;&#26426;&#21046;&#65292;&#20197;&#26377;&#25928;&#36807;&#28388;&#27599;&#20010;episode&#20013;&#19981;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;&#35813;&#23646;&#24615;&#36873;&#25321;&#26426;&#21046;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;episode&#20013;&#28041;&#21450;&#30340;&#23646;&#24615;&#25968;&#37327;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#33258;&#21160;&#26816;&#27979;&#20154;&#24037;&#26234;&#33021;&#23646;&#24615;&#27744;&#19981;&#36275;&#30340;episode&#65292;&#24182;&#36890;&#36807;&#28041;&#21450;&#23398;&#20064;&#30340;&#26410;&#30693;&#23646;&#24615;&#26469;&#34917;&#20607;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#19982;&#40657;&#21283;&#23376;&#23567;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning (FSL) is a challenging learning problem in which only a few samples are available for each class. Decision interpretation is more important in few-shot classification since there is a greater chance of error than in traditional classification. However, most of the previous FSL methods are black-box models. In this paper, we propose an inherently interpretable model for FSL based on human-friendly attributes. Moreover, we propose an online attribute selection mechanism that can effectively filter out irrelevant attributes in each episode. The attribute selection mechanism improves the accuracy and helps with interpretability by reducing the number of participated attributes in each episode. We propose a mechanism that automatically detects the episodes where the pool of human-friendly attributes are not adequate, and compensates by engaging learned unknown attributes. We demonstrate that the proposed method achieves results on par with black-box few-shot-learning model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#33021;&#37327;&#26448;&#26009;&#35774;&#35745;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#21450;&#24212;&#29992;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30830;&#23450;&#26368;&#20339;&#26448;&#26009;&#35774;&#35745;&#20197;&#21450;&#25351;&#21521;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#21644;&#24615;&#33021;&#25351;&#26631;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2211.08179</link><description>&lt;p&gt;
&#33021;&#37327;&#26448;&#26009;&#35774;&#35745;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65306;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence approaches for materials-by-design of energetic materials: state-of-the-art, challenges, and future directions. (arXiv:2211.08179v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#33021;&#37327;&#26448;&#26009;&#35774;&#35745;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#21450;&#24212;&#29992;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30830;&#23450;&#26368;&#20339;&#26448;&#26009;&#35774;&#35745;&#20197;&#21450;&#25351;&#21521;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#21644;&#24615;&#33021;&#25351;&#26631;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#35299;&#20915;&#21508;&#31181;&#22797;&#26434;&#26448;&#26009;&#35774;&#35745;&#38382;&#39064;&#30340;&#24037;&#20855;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#39038;&#26368;&#36817;&#22312;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#19979;&#30340;&#33021;&#37327;&#26448;&#26009;&#35774;&#35745;&#26041;&#38754;&#30340;&#36827;&#23637;&#21450;&#20854;&#24212;&#29992;&#12290;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21487;&#20197;&#35782;&#21035;&#26448;&#26009;&#35774;&#35745;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#36235;&#21183;&#21644;&#27169;&#24335;&#65292;&#30830;&#23450;&#26368;&#20339;&#26448;&#26009;&#35774;&#35745;&#20197;&#21450;&#25351;&#21521;&#20855;&#26377;&#20248;&#36234;/&#30446;&#26631;&#24615;&#33021;&#21644;&#24615;&#33021;&#25351;&#26631;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#20851;&#27880;&#26448;&#26009;&#35774;&#35745;&#30340;&#19977;&#20010;&#20027;&#35201;&#38454;&#27573;&#65292;&#21363;&#24494;&#32467;&#26500;&#24418;&#24577;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#32467;&#26500;-&#24615;&#33021;-&#24615;&#33021; (S-P-P) &#32852;&#31995;&#30340;&#20272;&#35745;&#20197;&#21450;&#20248;&#21270;/&#35774;&#35745;&#25506;&#32034;&#12290;&#25105;&#20204;&#23558;&#25552;&#20379;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12289;&#23454;&#29992;&#24615;&#21644;&#21151;&#25928;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is rapidly emerging as an enabling tool for solving various complex materials design problems. This paper aims to review recent advances in AI-driven materials-by-design and their applications to energetic materials (EM). Trained with data from numerical simulations and/or physical experiments, AI models can assimilate trends and patterns within the design parameter space, identify optimal material designs (micro-morphologies, combinations of materials in composites, etc.), and point to designs with superior/targeted property and performance metrics. We review approaches focusing on such capabilities with respect to the three main stages of materials-by-design, namely representation learning of microstructure morphology (i.e., shape descriptors), structure-property-performance (S-P-P) linkage estimation, and optimization/design exploration. We provide a perspective view of these methods in terms of their potential, practicality, and efficacy towards the rea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#25233;&#37057;&#30151;&#29366;&#26816;&#27979;&#20998;&#31867;&#22120;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25552;&#21462;&#20020;&#24202;&#30456;&#20851;&#29305;&#24449;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#29992;&#25143;&#30340;&#20020;&#24202;&#25233;&#37057;&#30151;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#26102;&#38388;&#31890;&#24230;&#30340;&#20934;&#30830;&#24230;&#24230;&#37327;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.07717</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#28145;&#24230;&#26102;&#38388;&#24314;&#27169;&#22312;&#20020;&#24202;&#25233;&#37057;&#30151;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Temporal Modelling of Clinical Depression through Social Media Text. (arXiv:2211.07717v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#25233;&#37057;&#30151;&#29366;&#26816;&#27979;&#20998;&#31867;&#22120;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25552;&#21462;&#20020;&#24202;&#30456;&#20851;&#29305;&#24449;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#29992;&#25143;&#30340;&#20020;&#24202;&#25233;&#37057;&#30151;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#26102;&#38388;&#31890;&#24230;&#30340;&#20934;&#30830;&#24230;&#24230;&#37327;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#26102;&#38388;&#36724;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#29992;&#25143;&#30340;&#20020;&#24202;&#25233;&#37057;&#30151;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#25233;&#37057;&#30151;&#29366;&#26816;&#27979;&#65288;DSD&#65289;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#22522;&#20110;&#26368;&#22823;&#25968;&#37327;&#30340;&#24050;&#32463;&#36807;&#20020;&#24202;&#21307;&#24072;&#27880;&#37322;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#38543;&#21518;&#20351;&#29992;&#25105;&#20204;&#30340;DSD&#27169;&#22411;&#26469;&#25552;&#21462;&#20020;&#24202;&#30456;&#20851;&#29305;&#24449;&#65292;&#20363;&#22914;&#25233;&#37057;&#30151;&#35780;&#20998;&#21450;&#20854;&#38543;&#21518;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#20197;&#21450;&#29992;&#25143;&#21457;&#24067;&#27963;&#21160;&#27169;&#24335;&#65292;&#20363;&#22914;&#37327;&#21270;&#20182;&#20204;&#30340;&#8220;&#26080;&#27963;&#21160;&#8221;&#25110;&#8220;&#27785;&#40664;&#8221;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#25552;&#21462;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#31181;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#20004;&#20010;&#29616;&#26377;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#29992;&#25143;&#32423;&#21035;&#25233;&#37057;&#30151;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#21333;&#20010;&#29305;&#24449;&#12289;&#22522;&#32447;&#29305;&#24449;&#21644;&#29305;&#24449;&#21066;&#20943;&#27979;&#35797;&#30340;&#20934;&#30830;&#24230;&#24230;&#37327;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#31890;&#24230;&#30340;&#20960;&#20010;&#32423;&#21035;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe the development of a model to detect user-level clinical depression based on a user's temporal social media posts. Our model uses a Depression Symptoms Detection (DSD) classifier, which is trained on the largest existing samples of clinician annotated tweets for clinical depression symptoms. We subsequently use our DSD model to extract clinically relevant features, e.g., depression scores and their consequent temporal patterns, as well as user posting activity patterns, e.g., quantifying their ``no activity'' or ``silence.'' Furthermore, to evaluate the efficacy of these extracted features, we create three kinds of datasets including a test dataset, from two existing well-known benchmark datasets for user-level depression detection. We then provide accuracy measures based on single features, baseline features and feature ablation tests, at several different levels of temporal granularity. The relevant data distributions and clinical depression detection related settings can
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#26377;&#36164;&#28304;&#32447;&#24615;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#30340;&#21464;&#31181;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31616;&#21333;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21516;&#26102;&#33021;&#22815;&#23454;&#29616;&#36739;&#20302;&#30340;&#21518;&#24724;&#12290;&#27492;&#22806;&#65292;&#24403;&#26576;&#20123;&#32422;&#26463;&#34987;&#36829;&#21453;&#26102;&#65292;&#31639;&#27861;&#22312;&#32479;&#35745;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2211.07484</link><description>&lt;p&gt;
&#24102;&#35013;&#36733;&#21644;&#35206;&#30422;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#65306;&#22522;&#20110;&#22238;&#24402;&#30340;&#27169;&#22359;&#21270;Lagrangian&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression. (arXiv:2211.07484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07484
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#26377;&#36164;&#28304;&#32447;&#24615;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#30340;&#21464;&#31181;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31616;&#21333;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21516;&#26102;&#33021;&#22815;&#23454;&#29616;&#36739;&#20302;&#30340;&#21518;&#24724;&#12290;&#27492;&#22806;&#65292;&#24403;&#26576;&#20123;&#32422;&#26463;&#34987;&#36829;&#21453;&#26102;&#65292;&#31639;&#27861;&#22312;&#32479;&#35745;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#30340;&#21464;&#31181;&#65292;&#20854;&#20013;&#31639;&#27861;&#22312;&#24635;&#28040;&#36153;&#30340;&#32447;&#24615;&#32422;&#26463;&#19979;&#20351;&#29992;&#22810;&#20010;&#36164;&#28304;&#12290;&#36825;&#20010;&#38382;&#39064;&#25512;&#24191;&#20102;&#24102;&#32972;&#21253;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;(CBwK)&#65292;&#20801;&#35768;&#35013;&#36733;&#21644;&#35206;&#30422;&#32422;&#26463;&#65292;&#20197;&#21450;&#27491;&#36127;&#36164;&#28304;&#28040;&#32791;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31616;&#21333;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#33021;&#22815;&#23454;&#29616;&#36864;&#21270;&#30340;&#21518;&#24724;&#12290;&#24403;&#26576;&#20123;&#32422;&#26463;&#34987;&#36829;&#21453;&#26102;&#65292;&#23545;&#20110;CBwK&#65292;&#23427;&#22312;&#32479;&#35745;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;LagrangianBwK(Immorlica&#31561;&#20154;&#65292;FOCS 2019)&#65292;&#36825;&#26159;&#19968;&#31181;&#38754;&#21521;CBwK&#30340;Lagrangian&#25216;&#26415;&#65292;&#20197;&#21450;SquareCB(Foster&#21644;Rakhlin&#65292;ICML 2020)&#65292;&#36825;&#26159;&#19968;&#31181;&#38754;&#21521;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#30340;&#22238;&#24402;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21033;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#26412;&#36136;&#19978;&#30340;&#27169;&#22359;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a variant of contextual bandits in which the algorithm consumes multiple resources subject to linear constraints on total consumption. This problem generalizes contextual bandits with knapsacks (CBwK), allowing for packing and covering constraints, as well as positive and negative resource consumption. We present a new algorithm that is simple, computationally efficient, and admits vanishing regret. It is statistically optimal for CBwK when an algorithm must stop once some constraint is violated. Our algorithm builds on LagrangeBwK (Immorlica et al., FOCS 2019) , a Lagrangian-based technique for CBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-based technique for contextual bandits. Our analysis leverages the inherent modularity of both techniques.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#27425;&#22810;&#39033;&#24335;&#24352;&#37327;&#20998;&#35299;&#38382;&#39064;&#30340;&#24179;&#22343;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#22312;$r \ll n^{3/2}$&#26102;&#23384;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#20294;&#24403;$r \lesssim n^2$&#26102;&#65292;&#35813;&#38382;&#39064;&#21482;&#33021;&#22312;&#21407;&#21017;&#19978;&#24674;&#22797;&#31209;-1&#20998;&#37327;&#65292;&#26159;&#19968;&#20010;&#35745;&#31639;&#22256;&#38590;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.05274</link><description>&lt;p&gt;
&#20302;&#27425;&#22810;&#39033;&#24335;&#24352;&#37327;&#20998;&#35299;&#30340;&#24179;&#22343;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Average-Case Complexity of Tensor Decomposition for Low-Degree Polynomials. (arXiv:2211.05274v2 [cs.CC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05274
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#27425;&#22810;&#39033;&#24335;&#24352;&#37327;&#20998;&#35299;&#38382;&#39064;&#30340;&#24179;&#22343;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#22312;$r \ll n^{3/2}$&#26102;&#23384;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#20294;&#24403;$r \lesssim n^2$&#26102;&#65292;&#35813;&#38382;&#39064;&#21482;&#33021;&#22312;&#21407;&#21017;&#19978;&#24674;&#22797;&#31209;-1&#20998;&#37327;&#65292;&#26159;&#19968;&#20010;&#35745;&#31639;&#22256;&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#25105;&#20204;&#32473;&#23450;&#19968;&#20010;&#30001;$r$&#20010;&#38543;&#26426;&#31209;-1&#39033;&#32452;&#25104;&#30340;$n$&#32500;&#19977;&#38454;&#23545;&#31216;&#24352;&#37327;$T \in (\mathbb{R}^n)^{\otimes 3}$&#65292;&#21017;&#24403;$r \lesssim n^2$&#26102;&#65292;&#21487;&#20197;&#22312;&#21407;&#21017;&#19978;&#24674;&#22797;&#31209;-1&#20998;&#37327;&#65292;&#20294;&#26159;&#20165;&#22312;$r \ll n^{3/2}$&#30340;&#24773;&#20917;&#19979;&#25165;&#30693;&#36947;&#23384;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#35768;&#22810;&#39640;&#32500;&#25512;&#26029;&#20219;&#21153;&#20013;&#23384;&#22312;&#31867;&#20284;&#30340;&#8220;&#32479;&#35745;&#35745;&#31639;&#24046;&#36317;&#8221;&#65292;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#38024;&#23545;&#32479;&#35745;&#26597;&#35810;&#65288;SQ&#65289;&#65292;&#24179;&#26041;&#21644;&#65288;SoS&#65289;&#21644;&#20302;&#27425;&#22810;&#39033;&#24335;&#65288;LDP&#65289;&#31561;&#21463;&#38480;&#65288;&#20294;&#24378;&#22823;&#65289;&#30340;&#35745;&#31639;&#27169;&#22411;&#35777;&#26126;&#19979;&#38480;&#65292;&#20197;&#35299;&#37322;&#36825;&#20123;&#38382;&#39064;&#30340;&#35745;&#31639;&#38590;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#24352;&#37327;&#20998;&#35299;&#20013;&#19981;&#23384;&#22312;&#31867;&#20284;&#8220;&#26893;&#20837;&#23545;&#31354;&#8221;&#30340;&#27979;&#35797;&#38382;&#39064;&#26469;&#35299;&#37322;&#20854;&#38590;&#24230;&#65292;&#36825;&#20063;&#26159;&#30446;&#21069;&#19981;&#23384;&#22312;&#20219;&#20309;&#27492;&#31867;&#24037;&#20316;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#38543;&#26426;&#19977;&#38454;&#24352;&#37327;&#20998;&#35299;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#20998;&#37327;&#30340;&#33539;&#25968;&#30053;&#22823;&#20110;&#20854;&#20313;&#20998;&#37327;&#65288;&#20197;&#30772;&#22351;&#23545;&#31216;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Suppose we are given an $n$-dimensional order-3 symmetric tensor $T \in (\mathbb{R}^n)^{\otimes 3}$ that is the sum of $r$ random rank-1 terms. The problem of recovering the rank-1 components is possible in principle when $r \lesssim n^2$ but polynomial-time algorithms are only known in the regime $r \ll n^{3/2}$. Similar "statistical-computational gaps" occur in many high-dimensional inference tasks, and in recent years there has been a flurry of work on explaining the apparent computational hardness in these problems by proving lower bounds against restricted (yet powerful) models of computation such as statistical queries (SQ), sum-of-squares (SoS), and low-degree polynomials (LDP). However, no such prior work exists for tensor decomposition, largely because its hardness does not appear to be explained by a "planted versus null" testing problem.  We consider a model for random order-3 tensor decomposition where one component is slightly larger in norm than the rest (to break symmetr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;$k$-DS&#32500;&#24230;&#65292;&#23436;&#20840;&#34920;&#24449;&#20102;$k$-&#21015;&#34920;&#23398;&#20064;&#24615;&#65292;&#24182;&#25351;&#20986;&#24403;&#19988;&#20165;&#24403;&#35813;&#20551;&#35774;&#31867;&#30340;$k$-DS&#32500;&#24230;&#26377;&#38480;&#65292;&#35813;&#20551;&#35774;&#31867;&#25165;$k$-&#21015;&#34920;&#21487;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2211.04956</link><description>&lt;p&gt;
&#21015;&#34920;&#21487;&#23398;&#20064;&#24615;&#30340;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
A Characterization of List Learnability. (arXiv:2211.04956v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;$k$-DS&#32500;&#24230;&#65292;&#23436;&#20840;&#34920;&#24449;&#20102;$k$-&#21015;&#34920;&#23398;&#20064;&#24615;&#65292;&#24182;&#25351;&#20986;&#24403;&#19988;&#20165;&#24403;&#35813;&#20551;&#35774;&#31867;&#30340;$k$-DS&#32500;&#24230;&#26377;&#38480;&#65292;&#35813;&#20551;&#35774;&#31867;&#25165;$k$-&#21015;&#34920;&#21487;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#32463;&#20856;&#32467;&#26524;&#34920;&#26126;&#65292;&#20108;&#20803;&#20551;&#35774;&#31867;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#19982;VC&#32500;&#24230;&#30340;&#26377;&#38480;&#24615;&#31561;&#25928;&#12290;&#23558;&#20854;&#25193;&#23637;&#21040;&#22810;&#31867;&#21035;&#35774;&#32622;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#36817;&#26399;&#36890;&#36807;&#26089;&#26399;&#30001;&#20025;&#23612;&#23572;&#21644;&#27801;&#21015;&#22827;-&#26045;&#29926;&#33576;&#24341;&#20837;&#30340;DS&#32500;&#24230;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#32771;&#34385;&#21015;&#34920;PAC&#23398;&#20064;&#65292;&#20854;&#30446;&#26631;&#26159;&#36755;&#20986;k&#20010;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#24050;&#32463;&#24320;&#21457;&#20102;&#21015;&#34920;&#23398;&#20064;&#31639;&#27861;&#65292;&#20107;&#23454;&#19978;&#65292;&#22312;&#26368;&#36817;&#30340;&#22810;&#31867;&#23398;&#20064;&#30340;&#34920;&#24449;&#20013;&#65292;&#21015;&#34920;&#23398;&#20064;&#25198;&#28436;&#20102;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#20197;&#19979;&#38382;&#39064;&#65306;&#20309;&#26102;&#21487;&#20197;&#29992;&#21015;&#34920;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#20551;&#35774;&#31867;&#65311;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;$k$-DS&#32500;&#24230;&#30340;DS&#32500;&#24230;&#27867;&#21270;&#23436;&#20840;&#34920;&#24449;$k$-&#21015;&#34920;&#23398;&#20064;&#24615;&#12290;&#36890;&#36807;&#23545;&#22810;&#31867;&#23398;&#20064;&#30340;&#26368;&#36817;&#34920;&#24449;&#36827;&#34892;&#27867;&#21270;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#20551;&#35774;&#31867;$k$-&#21015;&#34920;&#21487;&#23398;&#20064;&#65292;&#24403;&#19988;&#20165;&#24403;...
&lt;/p&gt;
&lt;p&gt;
A classical result in learning theory shows the equivalence of PAC learnability of binary hypothesis classes and the finiteness of VC dimension. Extending this to the multiclass setting was an open problem, which was settled in a recent breakthrough result characterizing multiclass PAC learnability via the DS dimension introduced earlier by Daniely and Shalev-Shwartz. In this work we consider list PAC learning where the goal is to output a list of $k$ predictions. List learning algorithms have been developed in several settings before and indeed, list learning played an important role in the recent characterization of multiclass learnability. In this work we ask: when is it possible to $k$-list learn a hypothesis class? We completely characterize $k$-list learnability in terms of a generalization of DS dimension that we call the $k$-DS dimension. Generalizing the recent characterization of multiclass learnability, we show that a hypothesis class is $k$-list learnable if and only if the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#33719;&#24471;&#20005;&#26684;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#21644;&#38477;&#20302;&#35299;&#37322;&#35823;&#24046;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.17426</link><description>&lt;p&gt;
&#20613;&#31435;&#21494;&#20998;&#26512;&#23454;&#29616;&#19968;&#33268;&#19988;&#30495;&#23454;&#30340;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Consistent and Truthful Interpretation with Fourier Analysis. (arXiv:2210.17426v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#33719;&#24471;&#20005;&#26684;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#21644;&#38477;&#20302;&#35299;&#37322;&#35823;&#24046;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#36328;&#23398;&#31185;&#39046;&#22495;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#37322;&#38656;&#35201;&#19982;&#24403;&#21069;&#26696;&#20363;&#30456;&#20851;&#30340;&#20551;&#35774;&#24773;&#26223;&#19968;&#33268;&#65292;&#21363;&#22914;&#26524;&#19968;&#20010;&#22240;&#32032;&#25913;&#21464;&#65292;&#27169;&#22411;&#20250;&#22914;&#20309;&#21453;&#24212;&#65311;&#23613;&#31649;&#24402;&#22240;&#26041;&#27861;&#30001;&#20248;&#38597;&#30340;&#20844;&#29702;&#31995;&#32479;&#25903;&#25345;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#36755;&#20837;&#65292;&#24182;&#19988;&#36890;&#24120;&#19981;&#19968;&#33268;&#12290;&#20026;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#24212;&#29992;&#24067;&#23572;&#20989;&#25968;&#30340;&#20613;&#31435;&#21494;&#20998;&#26512;&#26469;&#33719;&#24471;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#21508;&#31181;&#21322;&#24452;&#30340;&#37051;&#22495;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#23454;&#29616;2&#20493;&#33267;50&#20493;&#26356;&#20302;&#30340;&#35299;&#37322;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many interdisciplinary fields, ML interpretations need to be consistent with what-if scenarios related to the current case, i.e., if one factor changes, how does the model react? Although the attribution methods are supported by the elegant axiomatic systems, they mainly focus on individual inputs, and are generally inconsistent. To support what-if scenarios, we introduce a new notion called truthful interpretation, and apply Fourier analysis of Boolean functions to get rigorous guarantees. Experimental results show that for neighborhoods with various radii, our method achieves 2x - 50x lower interpretation error compared with the other methods.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#21457;&#29616;&#30340;&#26032;&#22411;&#21407;&#23376;&#21183;&#27169;&#22411;&#21151;&#33021;&#24418;&#24335;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20110;&#38108;&#31561;&#21270;&#23398;&#23646;&#24615;&#31867;&#20284;&#30340;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2210.15124</link><description>&lt;p&gt;
&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#21457;&#29616;&#30340;&#21407;&#23376;&#21183;&#27169;&#22411;&#30340;&#21151;&#33021;&#24418;&#24335;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalizability of Functional Forms for Interatomic Potential Models Discovered by Symbolic Regression. (arXiv:2210.15124v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15124
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#21457;&#29616;&#30340;&#26032;&#22411;&#21407;&#23376;&#21183;&#27169;&#22411;&#21151;&#33021;&#24418;&#24335;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20110;&#38108;&#31561;&#21270;&#23398;&#23646;&#24615;&#31867;&#20284;&#30340;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#21407;&#23376;&#21183;&#27169;&#22411;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#21183;&#27169;&#22411;&#36890;&#24120;&#27604;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#20294;&#27604;&#22914;&#23884;&#20837;&#21407;&#23376;&#26041;&#27861;&#31561;&#29289;&#29702;&#23548;&#20986;&#27169;&#22411;&#24930;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#24320;&#21457;&#20986;&#20102;&#21151;&#33021;&#24418;&#24335;&#31867;&#20284;&#20110;&#23884;&#20837;&#21407;&#23376;&#26041;&#27861;&#30340;&#24555;&#36895;&#12289;&#20934;&#30830;&#21644;&#21487;&#36801;&#31227;&#30340;&#38108;&#21407;&#23376;&#21183;&#27169;&#22411;&#12290;&#20026;&#20102;&#30830;&#23450;&#36825;&#20123;&#24418;&#24335;&#30340;&#25104;&#21151;&#31243;&#24230;&#26159;&#21542;&#29305;&#23450;&#20110;&#38108;&#65292;&#25105;&#20204;&#22312;&#27492;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20854;&#20182;&#38754;&#24515;&#31435;&#26041;&#36807;&#28193;&#37329;&#23646;&#20013;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#20960;&#31181;&#26448;&#26009;&#24615;&#36136;&#19978;&#30340;&#26679;&#22806;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20123;&#24418;&#24335;&#22312;&#21270;&#23398;&#19978;&#31867;&#20284;&#20110;&#38108;&#30340;&#20803;&#32032;&#19978;&#24037;&#20316;&#24471;&#29305;&#21035;&#22909;&#12290;&#19982;&#32463;&#36807;&#20248;&#21270;&#30340;Sutton-Chen&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#21151;&#33021;&#24418;&#24335;&#30340;&#24615;&#33021;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years there has been great progress in the use of machine learning algorithms to develop interatomic potential models. Machine-learned potential models are typically orders of magnitude faster than density functional theory but also orders of magnitude slower than physics-derived models such as the embedded atom method. In our previous work, we used symbolic regression to develop fast, accurate and transferrable interatomic potential models for copper with novel functional forms that resemble those of the embedded atom method. To determine the extent to which the success of these forms was specific to copper, here we explore the generalizability of these models to other face-centered cubic transition metals and analyze their out-of-sample performance on several material properties. We found that these forms work particularly well on elements that are chemically similar to copper. When compared to optimized Sutton-Chen models, which have similar complexity, the functional form
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#24449;&#33021;&#21147;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20989;&#25968;&#19982;&#20559;&#23548;&#25968;&#32452;&#21512;&#20135;&#29983;&#30340;&#20989;&#25968;Barron&#33539;&#25968;&#19981;&#36229;&#36807;$B_Lb^p$&#65292;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;Barron&#33539;&#25968; $O\left(\left(dB_L\right)^{\max\{p \log(1/ \epsilon), p^{\log(1/\epsilon)}}\right)}$ &#30340;&#20989;&#25968;$\epsilon$-&#36924;&#36817;PDE&#35299;&#12290;</title><link>http://arxiv.org/abs/2210.12101</link><description>&lt;p&gt;
&#36229;&#36234;&#32447;&#24615;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#65306;&#19968;&#31181;&#34920;&#24449;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Neural Network Approximations of PDEs Beyond Linearity: A Representational Perspective. (arXiv:2210.12101v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#24449;&#33021;&#21147;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20989;&#25968;&#19982;&#20559;&#23548;&#25968;&#32452;&#21512;&#20135;&#29983;&#30340;&#20989;&#25968;Barron&#33539;&#25968;&#19981;&#36229;&#36807;$B_Lb^p$&#65292;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;Barron&#33539;&#25968; $O\left(\left(dB_L\right)^{\max\{p \log(1/ \epsilon), p^{\log(1/\epsilon)}}\right)}$ &#30340;&#20989;&#25968;$\epsilon$-&#36924;&#36817;PDE&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#19968;&#31995;&#21015;&#30340;&#30740;&#31350;&#20511;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#20174;&#32780;&#24320;&#21551;&#20102;&#29702;&#35770;&#25506;&#31350;&#30340;&#22823;&#38376;&#65292;&#20351;&#20154;&#20204;&#24320;&#22987;&#25506;&#35752;&#36825;&#20123;&#27169;&#22411;&#26159;&#22914;&#20309;&#36991;&#20813;&#32500;&#24230;&#28798;&#38590;&#30340;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#29702;&#35770;&#20998;&#26512;&#37117;&#23616;&#38480;&#20110;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#34920;&#24449;&#33021;&#21147;&#12290;&#25105;&#20204;&#38598;&#20013;&#30740;&#31350;&#20102;&#19968;&#31867;&#31216;&#20026;\emph{&#38750;&#32447;&#24615;&#26925;&#22278;&#21464;&#20998;&#20559;&#24494;&#20998;&#26041;&#31243;}&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#20854;&#35299;&#26368;&#23567;&#21270;&#19968;&#20010;\emph{&#27431;&#25289;-&#25289;&#26684;&#26391;&#26085;}&#33021;&#37327;&#27867;&#20989;$\mathcal{E}(u) = \int_\Omega L(x, u(x), \nabla u(x)) - f(x) u(x)dx$&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#29992; Barron &#33539;&#25968;&#20026; $b$ &#30340;&#20989;&#25968;&#19982; $L$ &#30340;&#20559;&#23548;&#25968;&#32452;&#21512;&#21487;&#20197;&#20135;&#29983; Barron &#33539;&#25968;&#19981;&#36229;&#36807; $B_Lb^p$ &#30340;&#20989;&#25968;&#65292;&#21017;&#21487;&#20197;&#36890;&#36807;&#25317;&#26377; Barron &#33539;&#25968;&#20026; $O\left(\left(dB_L\right)^{\max\{p \log(1/ \epsilon), p^{\log(1/\epsilon)}}\right)}$ &#30340;&#20989;&#25968; $\epsilon$-&#36924;&#36817; $L^2$ &#24847;&#20041;&#19979;&#30340; PDE &#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
A burgeoning line of research leverages deep neural networks to approximate the solutions to high dimensional PDEs, opening lines of theoretical inquiry focused on explaining how it is that these models appear to evade the curse of dimensionality. However, most prior theoretical analyses have been limited to linear PDEs. In this work, we take a step towards studying the representational power of neural networks for approximating solutions to nonlinear PDEs. We focus on a class of PDEs known as \emph{nonlinear elliptic variational PDEs}, whose solutions minimize an \emph{Euler-Lagrange} energy functional $\mathcal{E}(u) = \int_\Omega L(x, u(x), \nabla u(x)) - f(x) u(x)dx$. We show that if composing a function with Barron norm $b$ with partial derivatives of $L$ produces a function of Barron norm at most $B_L b^p$, the solution to the PDE can be $\epsilon$-approximated in the $L^2$ sense by a function with Barron norm $O\left(\left(dB_L\right)^{\max\{p \log(1/ \epsilon), p^{\log(1/\epsil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#20869;&#27491;&#20132;&#35757;&#32451;&#30340;&#26041;&#27861;(LOT)&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26080;&#38480;&#21046;&#30697;&#38453;&#26469;&#21442;&#25968;&#21270;&#27491;&#20132;&#30697;&#38453;&#26469;&#26377;&#25928;&#35757;&#32451;1-Lipschitz&#21367;&#31215;&#23618;&#65292;&#24182;&#35777;&#26126;&#20102;&#21322;&#30417;&#30563;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#21033;&#26222;&#24076;&#33576;&#32422;&#26463;&#27169;&#22411;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#12290;&#22312;&#30830;&#23450;&#24615;l2&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#26041;&#38754;&#65292;LOT&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#65292;&#24182;&#33021;&#22815;&#25193;&#23637;&#21040;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2210.11620</link><description>&lt;p&gt;
LOT: &#22522;&#20110;&#23618;&#20869;&#27491;&#20132;&#35757;&#32451;&#26469;&#25552;&#39640;$\ell_2$ &#20445;&#25252;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LOT: Layer-wise Orthogonal Training on Improving $\ell_2$ Certified Robustness. (arXiv:2210.11620v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#20869;&#27491;&#20132;&#35757;&#32451;&#30340;&#26041;&#27861;(LOT)&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26080;&#38480;&#21046;&#30697;&#38453;&#26469;&#21442;&#25968;&#21270;&#27491;&#20132;&#30697;&#38453;&#26469;&#26377;&#25928;&#35757;&#32451;1-Lipschitz&#21367;&#31215;&#23618;&#65292;&#24182;&#35777;&#26126;&#20102;&#21322;&#30417;&#30563;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#21033;&#26222;&#24076;&#33576;&#32422;&#26463;&#27169;&#22411;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#12290;&#22312;&#30830;&#23450;&#24615;l2&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#26041;&#38754;&#65292;LOT&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#65292;&#24182;&#33021;&#22815;&#25193;&#23637;&#21040;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21033;&#26222;&#24076;&#33576;&#32422;&#26463;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#33021;&#22815;&#22686;&#24378;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#21644;&#20854;&#20182;&#27169;&#22411;&#29305;&#24615;&#65292;&#20363;&#22914;&#31283;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#20869;&#27491;&#20132;&#35757;&#32451;&#26041;&#27861;&#65288;LOT&#65289;&#26469;&#26377;&#25928;&#35757;&#32451;1-Lipschitz&#21367;&#31215;&#23618;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26080;&#38480;&#21046;&#30697;&#38453;&#26469;&#21442;&#25968;&#21270;&#19968;&#20010;&#27491;&#20132;&#30697;&#38453;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36755;&#20837;&#22495;&#36716;&#25442;&#20026;&#20613;&#37324;&#21494;&#39057;&#22495;&#26469;&#39640;&#25928;&#35745;&#31639;&#21367;&#31215;&#26680;&#30340;&#24179;&#26041;&#26681;&#30340;&#36870;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#21322;&#30417;&#30563;&#35757;&#32451;&#26377;&#21161;&#20110;&#25552;&#39640;&#32463;&#39564;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#24046;&#36317;&#65292;&#24182;&#35777;&#26126;&#21322;&#30417;&#30563;&#23398;&#20064;&#20063;&#20250;&#25552;&#39640;&#21033;&#26222;&#24076;&#33576;&#32422;&#26463;&#27169;&#22411;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23545;LOT&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;LOT&#22312;&#30830;&#23450;&#24615;l2&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#65292;&#24182;&#33021;&#22815;&#25193;&#23637;&#21040;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#30417;&#30563;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#32431;&#30417;&#30563;&#35757;&#32451;&#30456;&#27604;&#65292;&#21322;&#30417;&#30563;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20445;&#25252;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that training deep neural networks (DNNs) with Lipschitz constraints are able to enhance adversarial robustness and other model properties such as stability. In this paper, we propose a layer-wise orthogonal training method (LOT) to effectively train 1-Lipschitz convolution layers via parametrizing an orthogonal matrix with an unconstrained matrix. We then efficiently compute the inverse square root of a convolution kernel by transforming the input domain to the Fourier frequency domain. On the other hand, as existing works show that semi-supervised training helps improve empirical robustness, we aim to bridge the gap and prove that semi-supervised learning also improves the certified robustness of Lipschitz-bounded models. We conduct comprehensive evaluations for LOT under different settings. We show that LOT significantly outperforms baselines regarding deterministic l2 certified robustness, and scales to deeper neural networks. Under the supervised scenario, we i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31163;&#25955;&#32593;&#26684;&#19978;&#30340;WB&#19982;&#24213;&#23618;&#27969;&#24418;&#30340;&#20960;&#20309;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#22312;&#31163;&#25955;&#24418;&#29366;&#19978;&#35745;&#31639;&#30340;WB&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10535</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;Wasserstein&#37325;&#24515;&#30340;&#31283;&#23450;&#24615;&#21450;&#20854;&#22312;&#38543;&#26426;&#20960;&#20309;&#22270;&#24418;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stability of Entropic Wasserstein Barycenters and application to random geometric graphs. (arXiv:2210.10535v2 [cs.CG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31163;&#25955;&#32593;&#26684;&#19978;&#30340;WB&#19982;&#24213;&#23618;&#27969;&#24418;&#30340;&#20960;&#20309;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#22312;&#31163;&#25955;&#24418;&#29366;&#19978;&#35745;&#31639;&#30340;WB&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#23545;&#22270;&#24418;&#25968;&#25454;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#38271;&#65292;&#35745;&#31639;&#21508;&#31181;&#20960;&#20309;&#24037;&#20855;&#24050;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26576;&#20123;&#39046;&#22495;&#65292;&#22914;&#32593;&#26684;&#22788;&#29702;&#65292;&#36825;&#20123;&#24037;&#20855;&#36890;&#24120;&#20381;&#36182;&#20110;&#31163;&#25955;&#27969;&#24418;&#20013;&#30340;&#27979;&#22320;&#32447;&#21644;&#26368;&#30701;&#36335;&#24452;&#30340;&#35745;&#31639;&#12290;&#27700;&#21360;&#37325;&#24515;(WB)&#30340;&#35745;&#31639;&#26159;&#36825;&#31181;&#24037;&#20855;&#30340;&#19968;&#20010;&#26368;&#26032;&#20363;&#23376;&#12290;&#36825;&#26159;&#20174;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#20013;&#23548;&#20986;&#30340;&#19968;&#31181;&#38750;&#24120;&#36890;&#29992;&#30340;&#37325;&#24515;&#27010;&#24565;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#29109;&#27491;&#21017;&#21270;&#21464;&#20307;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31163;&#25955;&#32593;&#26684;&#19978;&#30340;WB&#19982;&#24213;&#23618;&#27969;&#24418;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#31283;&#23450;&#24615;&#32467;&#26524;&#65292;&#20851;&#20110;&#36755;&#20837;&#25104;&#26412;&#30697;&#38453;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27492;&#32467;&#26524;&#24212;&#29992;&#20110;&#29992;&#20110;&#27969;&#24418;&#30340;&#38543;&#26426;&#20960;&#20309;&#22270;&#24418;&#19978;&#65292;&#23427;&#20204;&#30340;&#26368;&#30701;&#36335;&#24452;&#25910;&#25947;&#20110;&#27979;&#22320;&#32447;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#22312;&#31163;&#25955;&#24418;&#29366;&#19978;&#35745;&#31639;&#30340;WB&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As interest in graph data has grown in recent years, the computation of various geometric tools has become essential. In some area such as mesh processing, they often rely on the computation of geodesics and shortest paths in discretized manifolds. A recent example of such a tool is the computation of Wasserstein barycenters (WB), a very general notion of barycenters derived from the theory of Optimal Transport, and their entropic-regularized variant. In this paper, we examine how WBs on discretized meshes relate to the geometry of the underlying manifold. We first provide a generic stability result with respect to the input cost matrices. We then apply this result to random geometric graphs on manifolds, whose shortest paths converge to geodesics, hence proving the consistency of WBs computed on discretized shapes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#39057;&#20998;&#26512;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411; TFAD&#12290;&#22312;&#35774;&#35745;&#30340;&#26102;&#39057;&#26550;&#26500;&#20013;&#65292;&#21516;&#26102;&#21152;&#20837;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#21644;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#65292;&#20197;&#25552;&#21319;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#24615;&#33021;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2210.09693</link><description>&lt;p&gt;
TFAD: &#19968;&#31181;&#22522;&#20110;&#26102;&#39057;&#20998;&#26512;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
TFAD: A Decomposition Time Series Anomaly Detection Architecture with Time-Frequency Analysis. (arXiv:2210.09693v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#39057;&#20998;&#26512;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411; TFAD&#12290;&#22312;&#35774;&#35745;&#30340;&#26102;&#39057;&#26550;&#26500;&#20013;&#65292;&#21516;&#26102;&#21152;&#20837;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#21644;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#65292;&#20197;&#25552;&#21319;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#24615;&#33021;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#26377;&#38480;&#30340;&#26631;&#31614;&#25968;&#25454;&#12290;&#34429;&#28982;&#19968;&#20123;&#31639;&#27861;&#65292;&#21253;&#25324;&#20256;&#32479;&#21644;&#28145;&#24230;&#27169;&#22411;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20294;&#22823;&#22810;&#25968;&#31639;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#26102;&#38388;&#22495;&#24314;&#27169;&#65292;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#39057;&#22495;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#39057;&#20998;&#26512;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411; TFAD&#65292;&#20197;&#21033;&#29992;&#26102;&#38388;&#21644;&#39057;&#22495;&#36827;&#34892;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#35774;&#35745;&#30340;&#26102;&#39057;&#26550;&#26500;&#20013;&#21152;&#20837;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#21644;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/DAMO-DI-ML/CIKM22-TFAD &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection is a challenging problem due to the complex temporal dependencies and the limited label data. Although some algorithms including both traditional and deep models have been proposed, most of them mainly focus on time-domain modeling, and do not fully utilize the information in the frequency domain of the time series data. In this paper, we propose a Time-Frequency analysis based time series Anomaly Detection model, or TFAD for short, to exploit both time and frequency domains for performance improvement. Besides, we incorporate time series decomposition and data augmentation mechanisms in the designed time-frequency architecture to further boost the abilities of performance and interpretability. Empirical studies on widely used benchmark datasets show that our approach obtains state-of-the-art performance in univariate and multivariate time series anomaly detection tasks. Code is provided at https://github.com/DAMO-DI-ML/CIKM22-TFAD.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;Masked Autoencoders (MAE)&#23398;&#20064;&#26377;&#24847;&#20041;&#29305;&#24449;&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;MAE&#19982;&#23545;&#27604;&#23398;&#20064;&#20043;&#38388;&#30340;&#32039;&#23494;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22343;&#21248;&#24615;&#30340;MAE (U-MAE) &#25439;&#22833;&#20989;&#25968;&#12290;&#36825;&#20123;&#29702;&#35770;&#21644;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#24102;&#26469;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.08344</link><description>&lt;p&gt;
&#25506;&#31350;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65306;Masked Autoencoders &#30340;&#29702;&#35770;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders. (arXiv:2210.08344v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;Masked Autoencoders (MAE)&#23398;&#20064;&#26377;&#24847;&#20041;&#29305;&#24449;&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;MAE&#19982;&#23545;&#27604;&#23398;&#20064;&#20043;&#38388;&#30340;&#32039;&#23494;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22343;&#21248;&#24615;&#30340;MAE (U-MAE) &#25439;&#22833;&#20989;&#25968;&#12290;&#36825;&#20123;&#29702;&#35770;&#21644;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#24102;&#26469;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37325;&#26500;&#20219;&#21153;&#30340;Masked Autoencoders (MAE)&#24050;&#25104;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064; (SSL) &#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#20173;&#23384;&#22312;&#26377;&#38480;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MAE&#22914;&#20309;&#36890;&#36807;&#25513;&#30721;&#26041;&#24335;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;MAE&#19982;&#23545;&#27604;&#23398;&#20064;&#20043;&#38388;&#30340;&#32039;&#23494;&#32852;&#31995;&#65292;&#34920;&#26126;MAE&#38544;&#24335;&#22320;&#23545;&#40784;&#20102;&#30001;&#25513;&#30721;&#24341;&#23548;&#30340;&#27491;&#23545;&#26144;&#26679;&#26412;&#12290;&#22522;&#20110;&#36825;&#19968;&#32852;&#31995;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;MAE&#26041;&#27861;&#30340;&#31532;&#19968;&#20010;&#19979;&#28216;&#20445;&#35777;&#65292;&#24182;&#20998;&#26512;&#20102;&#25513;&#30721;&#27604;&#20363;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#38544;&#24335;&#23545;&#40784;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;MAE&#30340;&#32500;&#24230;&#22349;&#22604;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22343;&#21248;&#24615;&#30340;MAE (U-MAE) &#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#21253;&#25324;CIFAR-10&#12289;ImageNet-100&#21644;ImageNet-1K&#22312;&#20869;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#24102;&#26469;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoders (MAE) based on a reconstruction task have risen to be a promising paradigm for self-supervised learning (SSL) and achieve state-of-the-art performance across different benchmark datasets. However, despite its impressive empirical success, there is still limited theoretical understanding of it. In this paper, we propose a theoretical understanding of how masking matters for MAE to learn meaningful features. We establish a close connection between MAE and contrastive learning, which shows that MAE implicit aligns the mask-induced positive pairs. Built upon this connection, we develop the first downstream guarantees for MAE methods, and analyze the effect of mask ratio. Besides, as a result of the implicit alignment, we also point out the dimensional collapse issue of MAE, and propose a Uniformity-enhanced MAE (U-MAE) loss that can effectively address this issue and bring significant improvements on real-world datasets, including CIFAR-10, ImageNet-100, and ImageNet-1K
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23481;&#24525;&#24322;&#24120;&#20540;&#24178;&#25200;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;(DRO)&#20844;&#24335;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#21487;&#20197;&#20351;&#20854;&#23545;&#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#38543;&#26426;&#35757;&#32451;&#26041;&#27861;&#65292;&#27979;&#35797;&#38169;&#35823;&#29575;&#20943;&#23569;&#39640;&#36798;83.5&#65285;&#65292;&#25439;&#22833;&#20943;&#23569;&#39640;&#36798;91.3&#65285;&#12290;</title><link>http://arxiv.org/abs/2210.08198</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#40065;&#26834;&#22810;&#20998;&#31867;&#21644;&#22312;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Multiclass Classification and Applications in Deep Image Classifiers. (arXiv:2210.08198v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08198
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23481;&#24525;&#24322;&#24120;&#20540;&#24178;&#25200;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;(DRO)&#20844;&#24335;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#21487;&#20197;&#20351;&#20854;&#23545;&#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#38543;&#26426;&#35757;&#32451;&#26041;&#27861;&#65292;&#27979;&#35797;&#38169;&#35823;&#29575;&#20943;&#23569;&#39640;&#36798;83.5&#65285;&#65292;&#25439;&#22833;&#20943;&#23569;&#39640;&#36798;91.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;(DRO)&#30340;&#22810;&#20998;&#31867;&#36923;&#36753;&#22238;&#24402;(MLR)&#20844;&#24335;&#65292;&#21487;&#20197;&#23481;&#24525;&#21463;&#21040;&#24322;&#24120;&#20540;&#24178;&#25200;&#30340;&#25968;&#25454;&#12290;DRO&#26694;&#26550;&#20351;&#29992;&#19968;&#20010;&#27010;&#29575;&#27169;&#31946;&#38598;&#21512;&#65292;&#35813;&#38598;&#21512;&#34987;&#23450;&#20041;&#20026;&#25509;&#36817;&#20110;Wasserstein&#24230;&#37327;&#24847;&#20041;&#19979;&#30340;&#35757;&#32451;&#38598;&#32463;&#39564;&#20998;&#24067;&#30340;&#20998;&#24067;&#29699;&#12290; &#25105;&#20204;&#23558;DRO&#20844;&#24335;&#25918;&#26494;&#20026;&#19968;&#20010;&#35268;&#21017;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#35268;&#21017;&#21270;&#22120;&#26159;&#31995;&#25968;&#30697;&#38453;&#30340;&#33539;&#25968;&#12290; &#25105;&#20204;&#20026;&#25105;&#20204;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#24314;&#31435;&#20102;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#65292;&#20026;&#25511;&#21046;&#39044;&#27979;&#35823;&#24046;&#30340;&#35268;&#21017;&#21270;&#22120;&#30340;&#20316;&#29992;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#28145;&#24230;&#35270;&#35273;&#21464;&#25442;&#22120;(ViT)&#20026;&#22522;&#30784;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#65292;&#20351;&#20854;&#23545;&#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#35757;&#32451;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#27979;&#35797;&#38169;&#35823;&#29575;&#20943;&#23569;&#39640;&#36798;83.5&#65285;&#21644;&#25439;&#22833;&#20943;&#23569;&#39640;&#36798;91.3&#65285;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a Distributionally Robust Optimization (DRO) formulation for Multiclass Logistic Regression (MLR), which could tolerate data contaminated by outliers. The DRO framework uses a probabilistic ambiguity set defined as a ball of distributions that are close to the empirical distribution of the training set in the sense of the Wasserstein metric. We relax the DRO formulation into a regularized learning problem whose regularizer is a norm of the coefficient matrix. We establish out-of-sample performance guarantees for the solutions to our model, offering insights on the role of the regularizer in controlling the prediction error. We apply the proposed method in rendering deep Vision Transformer (ViT)-based image classifiers robust to random and adversarial attacks. Specifically, using the MNIST and CIFAR-10 datasets, we demonstrate reductions in test error rate by up to 83.5% and loss by up to 91.3% compared with baseline methods, by adopting a novel random training method.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22810;&#31181;&#22270;&#31639;&#27861;&#26500;&#24314;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#25506;&#32034;&#20102;&#22270;&#32467;&#26500;&#29983;&#25104;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#24452;&#26597;&#25214;&#31639;&#27861;&#29992;&#20110;&#19979;&#28216;&#30340;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2210.07453</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31639;&#27861;&#39044;&#35757;&#32451;&#22270;&#34917;&#20840;Transformer
&lt;/p&gt;
&lt;p&gt;
Using Graph Algorithms to Pretrain Graph Completion Transformers. (arXiv:2210.07453v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22810;&#31181;&#22270;&#31639;&#27861;&#26500;&#24314;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#25506;&#32034;&#20102;&#22270;&#32467;&#26500;&#29983;&#25104;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#24452;&#26597;&#25214;&#31639;&#27861;&#29992;&#20110;&#19979;&#28216;&#30340;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#19979;&#28216;&#30340;&#22270;&#12289;&#38142;&#25509;&#21644;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#23436;&#20840;&#35843;&#26597;&#65292;&#29305;&#21035;&#26159;&#22312;&#19979;&#28216;&#30340;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#20219;&#21153;&#20013;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#35843;&#26597;&#20102;&#26500;&#24314;&#20110;&#20960;&#31181;&#22270;&#31639;&#27861;&#21644;&#26080;&#22806;&#37096;&#25968;&#25454;&#30340;&#20116;&#20010;&#19981;&#21516;&#39044;&#35757;&#32451;&#20449;&#21495;&#21450;&#20854;&#32452;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#65292;&#25506;&#32034;&#20102;&#36890;&#24120;&#19981;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#22270;&#23884;&#20837;&#26041;&#27861;&#30340;&#22270;&#32467;&#26500;&#29983;&#25104;&#39044;&#35757;&#32451;&#20219;&#21153;&#65288;&#21363;&#36335;&#24452;&#21644;k-hop&#37051;&#22495;&#29983;&#25104;&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#24452;&#26597;&#25214;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30001;&#20449;&#24687;&#22686;&#30410;&#24341;&#23548;&#65292;&#24182;&#21457;&#29616;&#23427;&#26159;&#22312;&#19977;&#20010;&#19979;&#28216;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#36335;&#24452;&#26597;&#25214;&#31639;&#27861;&#20316;&#20026;&#39044;&#35757;&#32451;&#20449;&#21495;&#25552;&#20379;2
&lt;/p&gt;
&lt;p&gt;
Recent work on Graph Neural Networks has demonstrated that self-supervised pretraining can further enhance performance on downstream graph, link, and node classification tasks. However, the efficacy of pretraining tasks has not been fully investigated for downstream large knowledge graph completion tasks. Using a contextualized knowledge graph embedding approach, we investigate five different pretraining signals, constructed using several graph algorithms and no external data, as well as their combination. We leverage the versatility of our Transformer-based model to explore graph structure generation pretraining tasks (i.e. path and k-hop neighborhood generation), typically inapplicable to most graph embedding methods. We further propose a new path-finding algorithm guided by information gain and find that it is the best-performing pretraining task across three downstream knowledge graph completion datasets. While using our new path-finding algorithm as a pretraining signal provides 2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#26032;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#65292;&#36890;&#36807;&#36880;&#20010;&#26679;&#26412;&#35780;&#20272;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#65292;&#38480;&#21046;&#32852;&#21512;&#20984;&#20989;&#25968;&#19978;&#30028;&#65292;&#19982;&#20808;&#21069;&#30340;&#30028;&#38480;&#30456;&#27604;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26356;&#32039;&#23494;&#22320;&#21051;&#30011;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24635;&#20307;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2210.06422</link><description>&lt;p&gt;
&#20351;&#29992;&#36880;&#20010;&#26679;&#26412;&#35780;&#20272;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#25552;&#20986;&#19968;&#31867;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
A New Family of Generalization Bounds Using Samplewise Evaluated CMI. (arXiv:2210.06422v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#26032;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#65292;&#36890;&#36807;&#36880;&#20010;&#26679;&#26412;&#35780;&#20272;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#65292;&#38480;&#21046;&#32852;&#21512;&#20984;&#20989;&#25968;&#19978;&#30028;&#65292;&#19982;&#20808;&#21069;&#30340;&#30028;&#38480;&#30456;&#27604;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26356;&#32039;&#23494;&#22320;&#21051;&#30011;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24635;&#20307;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#26032;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#65292;&#20854;&#20013;&#36890;&#36807;&#19968;&#20010;&#32852;&#21512;&#20984;&#20989;&#25968;&#27604;&#36739;&#35757;&#32451;&#25439;&#22833;&#21644;&#24635;&#20307;&#25439;&#22833;&#12290;&#36825;&#20010;&#20989;&#25968;&#30340;&#19978;&#30028;&#36890;&#36807;&#20998;&#35299;&#12289;&#36880;&#20010;&#26679;&#26412;&#35780;&#20272;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;CMI&#65289;&#21152;&#20197;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#19982;&#25152;&#36873;&#20551;&#35774;&#20135;&#29983;&#30340;&#25439;&#22833;&#26377;&#20851;&#32780;&#19981;&#26159;&#20551;&#35774;&#26412;&#36523;&#26377;&#20851;&#30340;&#20449;&#24687;&#24230;&#37327;&#65292;&#36825;&#22312;&#22823;&#22810;&#25968;&#36817;&#20284;&#27491;&#30830;&#24615;&#65288;PAC&#65289;- &#36125;&#21494;&#26031;&#32467;&#26524;&#20013;&#24456;&#24120;&#35265;&#12290;&#36890;&#36807;&#24674;&#22797;&#21644;&#25193;&#23637;&#20043;&#21069;&#24050;&#30693;&#30340;&#20449;&#24687;&#35770;&#30028;&#38480;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26694;&#26550;&#30340;&#26222;&#36866;&#24615;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#35780;&#20272;&#30340;CMI&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;Seeger&#30340;PAC-Bayesian&#30028;&#38480;&#30340;&#36880;&#20010;&#26679;&#26412;&#30340;&#24179;&#22343;&#29256;&#26412;&#65292;&#20854;&#20013;&#20984;&#20989;&#25968;&#26159;&#20108;&#20803;KL&#25955;&#24230;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26032;&#30340;&#30028;&#38480;&#27604;&#20808;&#21069;&#30340;&#30028;&#38480;&#26356;&#32039;&#23494;&#22320;&#21051;&#30011;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24635;&#20307;&#25439;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#20123;&#36825;&#20123;&#24179;&#22343;&#30028;&#38480;&#30340;&#39640;&#27010;&#29575;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new family of information-theoretic generalization bounds, in which the training loss and the population loss are compared through a jointly convex function. This function is upper-bounded in terms of the disintegrated, samplewise, evaluated conditional mutual information (CMI), an information measure that depends on the losses incurred by the selected hypothesis, rather than on the hypothesis itself, as is common in probably approximately correct (PAC)-Bayesian results. We demonstrate the generality of this framework by recovering and extending previously known information-theoretic bounds. Furthermore, using the evaluated CMI, we derive a samplewise, average version of Seeger's PAC-Bayesian bound, where the convex function is the binary KL divergence. In some scenarios, this novel bound results in a tighter characterization of the population loss of deep neural networks than previous bounds. Finally, we derive high-probability versions of some of these average bounds. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31561;&#21464;&#26426;&#22120;&#23398;&#20064;&#65292;&#20854;&#20013;&#20989;&#25968;&#23558;&#19982;&#26576;&#20010;&#32676;&#20316;&#29992;&#30456;&#20851;&#65292;&#20351;&#29992;&#19981;&#21487;&#32422;&#34920;&#31034;&#25110;&#19981;&#21464;&#37327;&#29702;&#35770;&#26469;&#21442;&#25968;&#21270;&#36825;&#20123;&#20989;&#25968;&#30340;&#31354;&#38388;&#12290; Malgrange&#30340;&#19968;&#33324;&#36807;&#31243;&#29992;&#26469;&#34920;&#36798;&#32676;$G$&#20316;&#29992;&#19979;&#25152;&#26377;&#22810;&#39033;&#24335;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2209.14991</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#19982;&#19981;&#21464;&#37327;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Machine learning and invariant theory. (arXiv:2209.14991v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31561;&#21464;&#26426;&#22120;&#23398;&#20064;&#65292;&#20854;&#20013;&#20989;&#25968;&#23558;&#19982;&#26576;&#20010;&#32676;&#20316;&#29992;&#30456;&#20851;&#65292;&#20351;&#29992;&#19981;&#21487;&#32422;&#34920;&#31034;&#25110;&#19981;&#21464;&#37327;&#29702;&#35770;&#26469;&#21442;&#25968;&#21270;&#36825;&#20123;&#20989;&#25968;&#30340;&#31354;&#38388;&#12290; Malgrange&#30340;&#19968;&#33324;&#36807;&#31243;&#29992;&#26469;&#34920;&#36798;&#32676;$G$&#20316;&#29992;&#19979;&#25152;&#26377;&#22810;&#39033;&#24335;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#29702;&#23450;&#24459;&#30340;&#21551;&#21457;&#19979;&#65292;&#31561;&#21464;&#26426;&#22120;&#23398;&#20064;&#23558;&#23398;&#20064;&#38480;&#21046;&#22312;&#20551;&#35774;&#31354;&#38388;&#20013;&#65292;&#20854;&#20013;&#25152;&#26377;&#20989;&#25968;&#37117;&#20851;&#20110;&#26576;&#20010;&#32676;&#20316;&#29992;&#31561;&#21464;&#12290;&#36890;&#24120;&#20351;&#29992;&#19981;&#21487;&#32422;&#34920;&#31034;&#25110;&#19981;&#21464;&#37327;&#29702;&#35770;&#26469;&#21442;&#25968;&#21270;&#36825;&#20123;&#20989;&#25968;&#30340;&#31354;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#36825;&#19968;&#20027;&#39064;&#65292;&#24182;&#35299;&#37322;&#20102;&#19968;&#20123;&#29992;&#20110;&#26126;&#30830;&#21442;&#25968;&#21270;&#31561;&#21464;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#34987;&#20351;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;Malgrange&#30340;&#19968;&#33324;&#36807;&#31243;&#65292;&#32473;&#23450;&#36739;&#22823;&#31354;&#38388;&#19978;&#19981;&#21464;&#22810;&#39033;&#24335;&#30340;&#34920;&#24449;&#65292;&#34920;&#36798;&#32676;$G$&#20316;&#29992;&#19979;&#25152;&#26377;&#22810;&#39033;&#24335;&#26144;&#23556;&#65292;&#35813;&#26041;&#27861;&#36824;&#22312;$G$&#26159;&#32039;Lie&#32676;&#30340;&#24773;&#20917;&#19979;&#21442;&#25968;&#21270;&#20102;&#20809;&#39034;&#31561;&#21464;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by constraints from physical law, equivariant machine learning restricts the learning to a hypothesis class where all the functions are equivariant with respect to some group action. Irreducible representations or invariant theory are typically used to parameterize the space of such functions. In this article, we introduce the topic and explain a couple of methods to explicitly parameterize equivariant functions that are being used in machine learning applications. In particular, we explicate a general procedure, attributed to Malgrange, to express all polynomial maps between linear spaces that are equivariant under the action of a group $G$, given a characterization of the invariant polynomials on a bigger space. The method also parametrizes smooth equivariant maps in the case that $G$ is a compact Lie group.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20027;&#35201;&#23637;&#31034;&#20102;AdaGrad&#22312;&#24179;&#28369;&#20984;&#20989;&#25968;&#21644;&#26356;&#19968;&#33324;&#30340;quasar&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25216;&#26415;&#65292;&#26126;&#30830;&#38480;&#23450;&#20102;vanilla AdaGrad&#22312;&#26080;&#32422;&#26463;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;AdaGrad&#21464;&#31181;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2209.14827</link><description>&lt;p&gt;
&#35770;AdaGrad&#22312;$\R^{d}$&#19978;&#30340;&#25910;&#25947;&#24615;&#65306;&#36229;&#36234;&#20984;&#24615;&#12289;&#38750;&#28176;&#36817;&#36895;&#29575;&#21644;&#21152;&#36895;&#65288;arXiv&#65306;2209.14827v2 [cs.LG] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of AdaGrad on $\R^{d}$: Beyond Convexity, Non-Asymptotic Rate and Acceleration. (arXiv:2209.14827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20027;&#35201;&#23637;&#31034;&#20102;AdaGrad&#22312;&#24179;&#28369;&#20984;&#20989;&#25968;&#21644;&#26356;&#19968;&#33324;&#30340;quasar&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25216;&#26415;&#65292;&#26126;&#30830;&#38480;&#23450;&#20102;vanilla AdaGrad&#22312;&#26080;&#32422;&#26463;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;AdaGrad&#21464;&#31181;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20851;&#20110;&#24179;&#28369;&#20984;&#20248;&#21270;&#30340;AdaGrad&#21644;&#20854;&#20182;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#20998;&#26512;&#36890;&#24120;&#26159;&#38024;&#23545;&#20855;&#26377;&#26377;&#30028;&#23450;&#20041;&#22495;&#30452;&#24452;&#30340;&#20989;&#25968;&#12290;&#22312;&#26080;&#32422;&#26463;&#38382;&#39064;&#20013;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#20445;&#35777;&#20102;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#65292;&#20294;&#27809;&#26377;&#26126;&#30830;&#30340;&#24658;&#23450;&#22240;&#23376;&#65292;&#36825;&#36866;&#29992;&#20110;&#25972;&#20010;&#20989;&#25968;&#31867;&#12290;&#27492;&#22806;&#65292;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#65292;&#21482;&#20998;&#26512;&#20102;&#19968;&#20010;&#20462;&#25913;&#29256;&#26412;&#30340;AdaGrad&#65292;&#19982;&#36890;&#24120;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#29256;&#26412;&#19981;&#21516;&#65292;&#22312;&#36825;&#20010;&#22238;&#24402;&#20013;&#19981;&#20351;&#29992;&#26368;&#26032;&#30340;&#26799;&#24230;&#26469;&#26356;&#26032;&#27493;&#24133;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#24182;&#22312;&#24179;&#28369;&#20984;&#20989;&#25968;&#30340;&#26631;&#20934;&#24773;&#20917;&#19979;&#20197;&#21450;&#26356;&#19968;&#33324;&#30340;quasar&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#28145;&#20837;&#29702;&#35299;AdaGrad&#21450;&#20854;&#21464;&#31181;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26032;&#25216;&#26415;&#65292;&#26126;&#30830;&#22320;&#38480;&#23450;&#20102;vanilla AdaGrad&#22312;&#26080;&#32422;&#26463;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#26080;&#35770;&#26159;&#30830;&#23450;&#24615;&#30340;&#36824;&#26159;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;AdaGrad&#21464;&#31181;&#65292;&#25105;&#20204;&#21487;&#20197;&#23637;&#31034;l&#30340;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Existing analysis of AdaGrad and other adaptive methods for smooth convex optimization is typically for functions with bounded domain diameter. In unconstrained problems, previous works guarantee an asymptotic convergence rate without an explicit constant factor that holds true for the entire function class. Furthermore, in the stochastic setting, only a modified version of AdaGrad, different from the one commonly used in practice, in which the latest gradient is not used to update the stepsize, has been analyzed. Our paper aims at bridging these gaps and developing a deeper understanding of AdaGrad and its variants in the standard setting of smooth convex functions as well as the more general setting of quasar convex functions. First, we demonstrate new techniques to explicitly bound the convergence rate of the vanilla AdaGrad for unconstrained problems in both deterministic and stochastic settings. Second, we propose a variant of AdaGrad for which we can show the convergence of the l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#32422;&#26463;&#30340;&#31561;&#22823;&#23567;&#30828;EM&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#35299;&#30721;&#22120;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#26679;&#30340;&#23545;&#35805;&#29983;&#25104;&#65292;&#21487;&#22312;&#23567;&#22411;&#27169;&#22411;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#26679;&#21270;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2209.14627</link><description>&lt;p&gt;
&#12298;&#19968;&#31181;&#38024;&#23545;&#22810;&#26679;&#23545;&#35805;&#29983;&#25104;&#30340;&#31561;&#22823;&#23567;&#30828;EM&#31639;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation. (arXiv:2209.14627v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#32422;&#26463;&#30340;&#31561;&#22823;&#23567;&#30828;EM&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#35299;&#30721;&#22120;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#26679;&#30340;&#23545;&#35805;&#29983;&#25104;&#65292;&#21487;&#22312;&#23567;&#22411;&#27169;&#22411;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#26679;&#21270;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20197;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#19982;&#20154;&#31867;&#20114;&#21160;&#12290;&#23613;&#31649;&#20687;ChatGPT&#36825;&#26679;&#30340;&#36229;&#22823;&#22411;&#23545;&#35805;&#31995;&#32479;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20351;&#29992;&#20013;&#23567;&#22411;&#23545;&#35805;&#31995;&#32479;&#20173;&#28982;&#26159;&#24120;&#35265;&#30340;&#20570;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#21152;&#36731;&#20415;&#26131;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#23545;&#35805;&#21709;&#24212;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31561;&#22823;&#23567;&#30828;EM&#65288;EqHard-EM&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#35299;&#30721;&#22120;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#26679;&#30340;&#23545;&#35805;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20197;&#30828;&#26041;&#24335;&#23558;&#26679;&#26412;&#20998;&#37197;&#32473;&#35299;&#30721;&#22120;&#65292;&#24182;&#39069;&#22806;&#26045;&#21152;&#24179;&#34913;&#32422;&#26463;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#25152;&#26377;&#35299;&#30721;&#22120;&#37117;&#32463;&#36807;&#20805;&#20998;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;EqHard-EM&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#26679;&#21270;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain dialogue systems aim to interact with humans through natural language texts in an open-ended fashion. Despite the recent success of super large dialogue systems such as ChatGPT, using medium-to-small-sized dialogue systems remains the common practice as they are more lightweight and accessible; however, generating diverse dialogue responses is challenging, especially with smaller models. In this work, we propose an Equal-size Hard Expectation--Maximization (EqHard-EM) algorithm to train a multi-decoder model for diverse dialogue generation. Our algorithm assigns a sample to a decoder in a hard manner and additionally imposes an equal-assignment constraint to ensure that all decoders are well-trained. We provide detailed theoretical analysis to justify our approach. Further, experiments on two large-scale open-domain dialogue datasets verify that our EqHard-EM algorithm generates high-quality diverse responses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;ViT&#39592;&#24178;&#32593;&#32476;U-ViT&#65292;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#22522;&#20110;CNN&#30340;U-Net&#27169;&#22411;&#65292;U-ViT&#20855;&#26377;&#21487;&#27604;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#21019;&#36896;&#20102;&#26032;&#30340;FID&#20998;&#25968;&#35760;&#24405;&#12290;</title><link>http://arxiv.org/abs/2209.12152</link><description>&lt;p&gt;
&#20840;&#37096;&#20540;&#24471;&#19968;&#35797;&#65306;&#36866;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;ViT&#39592;&#24178;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
All are Worth Words: A ViT Backbone for Diffusion Models. (arXiv:2209.12152v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;ViT&#39592;&#24178;&#32593;&#32476;U-ViT&#65292;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#22522;&#20110;CNN&#30340;U-Net&#27169;&#22411;&#65292;U-ViT&#20855;&#26377;&#21487;&#27604;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#21019;&#36896;&#20102;&#26032;&#30340;FID&#20998;&#25968;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;U-Net&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#20173;&#28982;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#36890;&#29992;&#30340;ViT&#39592;&#24178;&#26550;&#26500;&#65288;&#31216;&#20026;U-ViT&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#12290;U-ViT&#30340;&#29305;&#28857;&#26159;&#23558;&#25152;&#26377;&#36755;&#20837;&#65292;&#21253;&#25324;&#26102;&#38388;&#12289;&#26465;&#20214;&#21644;&#22122;&#22768;&#22270;&#20687;&#22359;&#37117;&#35270;&#20026;&#20196;&#29260;&#65292;&#24182;&#22312;&#27973;&#23618;&#21644;&#28145;&#23618;&#20043;&#38388;&#20351;&#29992;&#38271;&#36339;&#36291;&#36830;&#25509;&#12290;&#25105;&#20204;&#22312;&#26080;&#26465;&#20214;&#21644;&#31867;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#20197;&#21450;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;U-ViT&#65292;&#22312;&#30456;&#20284;&#22823;&#23567;&#30340;&#22522;&#20110;&#8220;U-Net&#8221;&#30340;CNN&#27169;&#22411;&#20013;&#65292;U-ViT&#20855;&#26377;&#21487;&#27604;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;U-ViT&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#22312;ImageNet 256x256&#31867;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;2.29&#30340;&#26368;&#20339;FID&#20998;&#25968;&#65292;&#22312;MS-COCO&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;5.48&#30340;&#26368;&#20339;FID&#20998;&#25968;&#65292;&#30456;&#27604;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#22312;&#35757;&#32451;&#26399;&#38388;&#35775;&#38382;&#22823;&#22411;&#22806;&#37096;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25193;&#25955;&#22411;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20687;U-ViT&#36825;&#26679;&#30340;ViT&#39592;&#24178;&#26550;&#26500;&#21487;&#20197;&#23454;&#29616;&#19982;&#20256;&#32479;&#22522;&#20110;CNN&#30340;U-Net&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#21019;&#36896;&#26032;&#30340;FID&#20998;&#25968;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViT) have shown promise in various vision tasks while the U-Net based on a convolutional neural network (CNN) remains dominant in diffusion models. We design a simple and general ViT-based architecture (named U-ViT) for image generation with diffusion models. U-ViT is characterized by treating all inputs including the time, condition and noisy image patches as tokens and employing long skip connections between shallow and deep layers. We evaluate U-ViT in unconditional and class-conditional image generation, as well as text-to-image generation tasks, where U-ViT is comparable if not superior to a CNN-based U-Net of a similar size. In particular, latent diffusion models with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional image generation on ImageNet 256x256, and 5.48 in text-to-image generation on MS-COCO, among methods without accessing large external datasets during the training of generative models. Our results suggest that, for diffusion-b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;DRAM&#30340;&#20869;&#23384;&#20013;&#22788;&#29702;&#65288;PIM&#65289;&#26041;&#27861;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;PIM&#26550;&#26500;&#65292;&#24471;&#20986;PIM&#26497;&#22823;&#22320;&#26377;&#21033;&#20110;&#20869;&#23384;&#21463;&#38480;&#30340;NN&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2209.08938</link><description>&lt;p&gt;
&#22522;&#20110;DRAM&#30340;&#22788;&#29702;&#22120;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#65306;&#20174;&#36793;&#32536;&#21040;&#20113;&#31471;
&lt;/p&gt;
&lt;p&gt;
Accelerating Neural Network Inference with Processing-in-DRAM: From the Edge to the Cloud. (arXiv:2209.08938v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;DRAM&#30340;&#20869;&#23384;&#20013;&#22788;&#29702;&#65288;PIM&#65289;&#26041;&#27861;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;PIM&#26550;&#26500;&#65292;&#24471;&#20986;PIM&#26497;&#22823;&#22320;&#26377;&#21033;&#20110;&#20869;&#23384;&#21463;&#38480;&#30340;NN&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#37325;&#35201;&#24615;&#21644;&#22797;&#26434;&#24615;&#19978;&#19981;&#26029;&#22686;&#38271;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65288;&#21450;&#33021;&#25928;&#65289;&#21487;&#33021;&#20250;&#21463;&#21040;&#35745;&#31639;&#25110;&#20869;&#23384;&#36164;&#28304;&#30340;&#38480;&#21046;&#12290;&#22312;&#20869;&#23384;&#38453;&#21015;&#38468;&#36817;&#25110;&#20869;&#37096;&#25918;&#32622;&#35745;&#31639;&#30340;&#20869;&#23384;&#20013;&#22788;&#29702;&#65288;PIM&#65289;&#33539;&#20363;&#26159;&#21152;&#36895;&#20869;&#23384;&#21463;&#38480;&#30340;NN&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;PIM&#26550;&#26500;&#22240;&#24418;&#24335;&#32780;&#24322;&#65292;&#19981;&#21516;&#30340;PIM&#26041;&#27861;&#23548;&#33268;&#19981;&#21516;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20998;&#26512;&#12289;&#35752;&#35770;&#21644;&#23545;&#27604;&#22522;&#20110;DRAM&#30340;PIM&#26550;&#26500;&#23545;NN&#24615;&#33021;&#21644;&#33021;&#25928;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;PIM&#26550;&#26500;&#65306;&#65288;1&#65289;UPMEM&#65292;&#23558;&#22788;&#29702;&#22120;&#21644;DRAM&#38453;&#21015;&#38598;&#25104;&#21040;&#21333;&#20010;&#20108;&#32500;&#33455;&#29255;&#20013;&#65307;&#65288;2&#65289;Mensa&#65292;&#19968;&#31181;&#38024;&#23545;&#36793;&#32536;&#35774;&#22791;&#37327;&#36523;&#23450;&#21046;&#30340;&#19977;&#32500;&#22534;&#26632;&#24335;PIM&#26550;&#26500;&#65307;&#21644;&#65288;3&#65289;SIMDRAM&#65292;&#23427;&#20351;&#29992;DRAM&#30340;&#27169;&#25311;&#21407;&#29702;&#25191;&#34892;&#20301;&#20018;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;PIM&#26497;&#22823;&#22320;&#26377;&#21033;&#20110;&#20869;&#23384;&#21463;&#38480;&#30340;NN&#65306;&#65288;1&#65289;&#24403;GPU&#38656;&#35201;&#20869;&#23384;&#26102;&#65292;UPMEM&#25552;&#20379;&#20102;&#39640;&#31471;GPU&#30340;23&#20493;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) are growing in importance and complexity. A neural network's performance (and energy efficiency) can be bound either by computation or memory resources. The processing-in-memory (PIM) paradigm, where computation is placed near or within memory arrays, is a viable solution to accelerate memory-bound NNs. However, PIM architectures vary in form, where different PIM approaches lead to different trade-offs. Our goal is to analyze, discuss, and contrast DRAM-based PIM architectures for NN performance and energy efficiency. To do so, we analyze three state-of-the-art PIM architectures: (1) UPMEM, which integrates processors and DRAM arrays into a single 2D chip; (2) Mensa, a 3D-stack-based PIM architecture tailored for edge devices; and (3) SIMDRAM, which uses the analog principles of DRAM to execute bit-serial operations. Our analysis reveals that PIM greatly benefits memory-bound NNs: (1) UPMEM provides 23x the performance of a high-end GPU when the GPU requires memor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23398;&#20064;&#26080;&#27169;&#22411;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24377;&#24615;&#25191;&#34892;&#22120;&#30340;&#21160;&#21147;&#23398;&#24615;&#36136;&#20248;&#21270;&#21160;&#24577;&#36816;&#21160;&#25511;&#21046;&#65292;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#21487;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#22235;&#36275;&#36816;&#21160;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2209.07171</link><description>&lt;p&gt;
&#23398;&#20064;&#21033;&#29992;&#24377;&#24615;&#25191;&#34892;&#22120;&#36827;&#34892;&#22235;&#36275;&#21160;&#29289;&#30340;&#36816;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning to Exploit Elastic Actuators for Quadruped Locomotion. (arXiv:2209.07171v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23398;&#20064;&#26080;&#27169;&#22411;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24377;&#24615;&#25191;&#34892;&#22120;&#30340;&#21160;&#21147;&#23398;&#24615;&#36136;&#20248;&#21270;&#21160;&#24577;&#36816;&#21160;&#25511;&#21046;&#65292;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#21487;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#22235;&#36275;&#36816;&#21160;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22235;&#36275;&#36816;&#21160;&#20013;&#65292;&#22522;&#20110;&#24377;&#31783;&#30340;&#25191;&#34892;&#22120;&#21487;&#20197;&#25552;&#39640;&#33021;&#25928;&#21644;&#24615;&#33021;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#38590;&#24230;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#26159;&#36890;&#36807;&#24314;&#27169;&#19982;&#20223;&#30495;&#26469;&#23547;&#25214;&#26368;&#20248;&#30340;&#25511;&#21046;&#22120;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#30452;&#25509;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23398;&#20064;&#26080;&#27169;&#22411;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20808;&#30001;&#20013;&#22830;&#27169;&#24335;&#21457;&#29983;&#22120;&#65288;CPGs&#65289;&#21512;&#25104;&#27493;&#24577;&#65292;&#36890;&#36807;&#20248;&#21270;&#36825;&#20123;&#21442;&#25968;&#24471;&#21040;&#19968;&#31181;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#36816;&#21160;&#30340;&#24320;&#29615;&#25511;&#21046;&#22120;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20351;&#25511;&#21046;&#22120;&#26356;&#21152;&#31283;&#20581;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#26412;&#25991;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#38381;&#29615;&#25511;&#21046;&#65292;&#23398;&#20064;&#22312;CPGs&#20043;&#19978;&#30340;&#30699;&#27491;&#21160;&#20316;&#12290;&#25105;&#20204;&#22312;DLR&#24377;&#24615;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#23398;&#20064;&#21040;&#30340;&#24930;&#36305;&#21644;&#36339;&#36291;&#27493;&#24577;&#34920;&#26126;&#65292;&#22312;&#20248;&#21270;&#21160;&#24577;&#36816;&#21160;&#26102;&#21033;&#29992;&#24377;&#24615;&#25191;&#34892;&#22120;&#21160;&#21147;&#23398;&#26159;&#33258;&#28982;&#21457;&#29983;&#30340;&#65292;&#23613;&#31649;&#26080;&#27169;&#22411;&#25511;&#21046;&#22120;&#65292;&#20294;&#20173;&#21487;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#36816;&#21160;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spring-based actuators in legged locomotion provide energy-efficiency and improved performance, but increase the difficulty of controller design. While previous work has focused on extensive modeling and simulation to find optimal controllers for such systems, we propose to learn model-free controllers directly on the real robot. In our approach, gaits are first synthesized by central pattern generators (CPGs), whose parameters are optimized to quickly obtain an open-loop controller that achieves efficient locomotion. Then, to make this controller more robust and further improve the performance, we use reinforcement learning to close the loop, to learn corrective actions on top of the CPGs. We evaluate the proposed approach on the DLR elastic quadruped bert. Our results in learning trotting and pronking gaits show that exploitation of the spring actuator dynamics emerges naturally from optimizing for dynamic motions, yielding high-performing locomotion despite being model-free. The who
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#19978;&#19979;&#25991;&#30340;&#39118;&#38505;&#24863;&#30693;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#39118;&#38505;&#24230;&#37327;&#30340;&#32622;&#20449;&#24207;&#21015;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;UCB&#31639;&#27861;&#20197;&#23398;&#20064;&#26368;&#20339;&#30340;&#39118;&#38505;&#24863;&#30693;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2209.07154</link><description>&lt;p&gt;
&#24102;&#20984;&#25439;&#22833;&#30340;&#39118;&#38505;&#24863;&#30693;&#32447;&#24615;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Risk-aware linear bandits with convex loss. (arXiv:2209.07154v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#19978;&#19979;&#25991;&#30340;&#39118;&#38505;&#24863;&#30693;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#39118;&#38505;&#24230;&#37327;&#30340;&#32622;&#20449;&#24207;&#21015;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;UCB&#31639;&#27861;&#20197;&#23398;&#20064;&#26368;&#20339;&#30340;&#39118;&#38505;&#24863;&#30693;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#31561;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#20195;&#29702;&#36890;&#36807;&#20248;&#21270;&#26576;&#31181;&#21453;&#39304;&#36827;&#34892;&#39034;&#24207;&#23398;&#20064;&#12290;&#34429;&#28982;&#24179;&#22343;&#22238;&#25253;&#26631;&#20934;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#21453;&#26144;&#23545;&#19981;&#33391;&#32467;&#26524;&#30340;&#21388;&#24694;&#30340;&#20854;&#20182;&#24230;&#37327;&#65292;&#20363;&#22914;&#26041;&#24046;&#12289;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65288;CVaR&#65289;&#65292;&#21487;&#33021;&#23545;&#20851;&#38190;&#24212;&#29992;&#65288;&#21307;&#30103;&#20445;&#20581;&#12289;&#20892;&#19994;&#65289;&#26377;&#29992;&#12290;&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#36172;&#21338;&#21453;&#39304;&#19979;&#25552;&#20986;&#20102;&#29992;&#20110;&#27492;&#31867;&#39118;&#38505;&#24863;&#30693;&#24230;&#37327;&#30340;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#19978;&#19979;&#25991;&#30340;&#36172;&#24466;&#65292;&#22312;&#36825;&#20123;&#36172;&#21338;&#26426;&#21453;&#39304;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#20984;&#25439;&#22833;&#30340;&#26368;&#23567;&#21270;&#26469;&#25552;&#21462;&#36825;&#31181;&#39118;&#38505;&#24230;&#37327;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#32447;&#24615;&#20989;&#25968;&#12290;&#31526;&#21512;&#27492;&#26694;&#26550;&#30340;&#20856;&#22411;&#31034;&#20363;&#26159;expectile&#24230;&#37327;&#65292;&#23427;&#26159;&#36890;&#36807;&#19981;&#23545;&#31216;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#30340;&#35299;&#24471;&#21040;&#30340;&#12290;&#20351;&#29992;&#21345;&#26364;&#36229;&#34701;&#21512;&#26041;&#27861;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#29992;&#20110;&#20272;&#35745;&#27492;&#31867;&#39118;&#38505;&#24230;&#37327;&#30340;&#32622;&#20449;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;UCB&#31639;&#27861;&#65292;&#20197;&#23398;&#20064;&#26368;&#20339;&#30340;&#39118;&#38505;&#24863;&#30693;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
In decision-making problems such as the multi-armed bandit, an agent learns sequentially by optimizing a certain feedback. While the mean reward criterion has been extensively studied, other measures that reflect an aversion to adverse outcomes, such as mean-variance or conditional value-at-risk (CVaR), can be of interest for critical applications (healthcare, agriculture). Algorithms have been proposed for such risk-aware measures under bandit feedback without contextual information. In this work, we study contextual bandits where such risk measures can be elicited as linear functions of the contexts through the minimization of a convex loss. A typical example that fits within this framework is the expectile measure, which is obtained as the solution of an asymmetric least-square problem. Using the method of mixtures for supermartingales, we derive confidence sequences for the estimation of such risk measures. We then propose an optimistic UCB algorithm to learn optimal risk-aware act
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26799;&#24230;&#19979;&#38477;&#20998;&#24067;&#24335;&#23398;&#20064;&#22312;&#20998;&#31163;&#25968;&#25454;&#19978;&#30340;&#24191;&#20041;&#24615;&#36136;&#65292;&#24182;&#25512;&#23548;&#20986;&#26032;&#39062;&#30340;&#26377;&#38480;&#26102;&#38388;&#24191;&#20041;&#24615;&#33021;&#30028;&#38480;&#65292;&#36825;&#23545;&#20110;&#24403;&#21069;&#20165;&#38480;&#20110;&#38598;&#20013;&#24335;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#30740;&#31350;&#26377;&#25152;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2209.07116</link><description>&lt;p&gt;
&#35770;&#20998;&#31163;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Generalization of Decentralized Learning with Separable Data. (arXiv:2209.07116v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26799;&#24230;&#19979;&#38477;&#20998;&#24067;&#24335;&#23398;&#20064;&#22312;&#20998;&#31163;&#25968;&#25454;&#19978;&#30340;&#24191;&#20041;&#24615;&#36136;&#65292;&#24182;&#25512;&#23548;&#20986;&#26032;&#39062;&#30340;&#26377;&#38480;&#26102;&#38388;&#24191;&#20041;&#24615;&#33021;&#30028;&#38480;&#65292;&#36825;&#23545;&#20110;&#24403;&#21069;&#20165;&#38480;&#20110;&#38598;&#20013;&#24335;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#30740;&#31350;&#26377;&#25152;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25968;&#25454;&#33258;&#28982;&#22320;&#20998;&#24067;&#22312;&#22522;&#30784;&#22270;&#19978;&#20256;&#36882;&#30340;&#20195;&#29702;&#20043;&#38388;&#26102;&#65292;&#20998;&#24067;&#24335;&#23398;&#20064;&#25552;&#20379;&#20102;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290; &#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#34987;&#35757;&#32451;&#20026;&#38646;&#35757;&#32451;&#25439;&#22833;&#12290; &#26412;&#25991;&#30740;&#31350;&#20102;&#26799;&#24230;&#19979;&#38477;&#20998;&#24067;&#24335;&#23398;&#20064;&#22312;&#20998;&#31163;&#25968;&#25454;&#19978;&#30340;&#31639;&#27861;&#21644;&#24191;&#20041;&#24615;&#36136;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#21644;&#19968;&#31995;&#21015;&#22312;&#26080;&#38480;&#36828;&#22788;&#28176;&#36817;&#20026;&#38646;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;&#21253;&#25324;&#25351;&#25968;&#21644;&#23545;&#25968;&#25439;&#22833;&#65289;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#39062;&#30340;&#26377;&#38480;&#26102;&#38388;&#24191;&#20041;&#24615;&#33021;&#30028;&#38480;&#12290; &#36825;&#34917;&#20805;&#20102;&#26368;&#36817;&#19968;&#31995;&#21015;&#30740;&#31350;&#26799;&#24230;&#19979;&#38477;&#22312;&#20998;&#31163;&#25968;&#25454;&#19978;&#30340;&#24191;&#20041;&#24615;&#33021;&#21644;&#38544;&#24335;&#20559;&#24046;&#65292;&#20294;&#30446;&#21069;&#20165;&#38480;&#20110;&#38598;&#20013;&#24335;&#23398;&#20064;&#22330;&#26223;&#30340;&#24037;&#20316;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27867;&#21270;&#30028;&#38480;&#19982;&#20854;&#38598;&#20013;&#24335;&#23545;&#24212;&#29289;&#30340;&#22823;&#23567;&#36817;&#20046;&#30456;&#31561;&#12290; &#29420;&#31435;&#20110;&#27492;&#65292;&#26412;&#25991;&#36824;&#24314;&#31435;&#20102;&#20851;&#20110;DGD&#30340;&#26032;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized learning offers privacy and communication efficiency when data are naturally distributed among agents communicating over an underlying graph. Motivated by overparameterized learning settings, in which models are trained to zero training loss, we study algorithmic and generalization properties of decentralized learning with gradient descent on separable data. Specifically, for decentralized gradient descent (DGD) and a variety of loss functions that asymptote to zero at infinity (including exponential and logistic losses), we derive novel finite-time generalization bounds. This complements a long line of recent work that studies the generalization performance and the implicit bias of gradient descent over separable data, but has thus far been limited to centralized learning scenarios. Notably, our generalization bounds approximately match in order their centralized counterparts. Critical behind this, and of independent interest, is establishing novel bounds on the training
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22312;&#32447;&#26368;&#23567;&#21644;&#38598;&#35206;&#30422;&#31639;&#27861;&#65292;&#20854;&#31639;&#27861;&#33021;&#22815;&#22312;&#19981;&#30693;&#36947;&#26410;&#26469;&#38598;&#21512;&#30340;&#21069;&#25552;&#19979;&#37325;&#26032;&#25490;&#24207;&#20803;&#32032;&#65292;&#20197;&#33267;&#20110;&#21015;&#34920;&#21069;&#30340;&#33267;&#23569;&#19968;&#20010;&#20803;&#32032;&#33021;&#22815;&#21305;&#37197;&#19978;&#27969;&#20013;&#30340;&#26576;&#20010;&#20803;&#32032;&#65292;&#21516;&#26102;&#31639;&#27861;&#30340;&#34920;&#29616;&#20063;&#20248;&#20110;&#20197;&#24448;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.04870</link><description>&lt;p&gt;
&#19968;&#31181;&#25913;&#36827;&#30340;&#22312;&#32447;&#26368;&#23567;&#21644;&#38598;&#35206;&#30422;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Improved Algorithm For Online Min-Sum Set Cover. (arXiv:2209.04870v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22312;&#32447;&#26368;&#23567;&#21644;&#38598;&#35206;&#30422;&#31639;&#27861;&#65292;&#20854;&#31639;&#27861;&#33021;&#22815;&#22312;&#19981;&#30693;&#36947;&#26410;&#26469;&#38598;&#21512;&#30340;&#21069;&#25552;&#19979;&#37325;&#26032;&#25490;&#24207;&#20803;&#32032;&#65292;&#20197;&#33267;&#20110;&#21015;&#34920;&#21069;&#30340;&#33267;&#23569;&#19968;&#20010;&#20803;&#32032;&#33021;&#22815;&#21305;&#37197;&#19978;&#27969;&#20013;&#30340;&#26576;&#20010;&#20803;&#32032;&#65292;&#21516;&#26102;&#31639;&#27861;&#30340;&#34920;&#29616;&#20063;&#20248;&#20110;&#20197;&#24448;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#26412;&#30340;&#22312;&#32447;&#20559;&#22909;&#32858;&#21512;&#27169;&#22411;&#65292;&#20854;&#20013;&#31639;&#27861;&#32500;&#25252;&#20102;&#19968;&#20010;&#21253;&#21547; n &#20010;&#20803;&#32032;&#30340;&#26377;&#24207;&#21015;&#34920;&#12290;&#36755;&#20837;&#26159;&#19968;&#20010;&#39318;&#36873;&#38598;&#21512; R1&#65292;R2&#65292;...&#65292;Rt&#65292;... &#30340;&#27969;&#12290;&#22312;&#30475;&#21040;Rt&#26102;&#65292;&#31639;&#27861;&#19981;&#30693;&#36947;&#26410;&#26469;&#30340;&#20219;&#20309;&#38598;&#21512;&#65292;&#24517;&#39035;&#37325;&#26032;&#25490;&#24207;&#20803;&#32032;&#65288;&#26356;&#25913;&#21015;&#34920;&#25490;&#24207;&#65289;&#65292;&#20197;&#20415;&#22312;&#21015;&#34920;&#21069;&#37096;&#33267;&#23569;&#25214;&#21040;&#19968;&#20010;Rt&#30340;&#20803;&#32032;&#12290;&#20135;&#29983;&#30340;&#25104;&#26412;&#26159;&#21015;&#34920;&#26356;&#26032;&#25104;&#26412;&#65288;&#30456;&#37051;&#21015;&#34920;&#20803;&#32032;&#20132;&#25442;&#27425;&#25968;&#65289;&#21644;&#23384;&#21462;&#25104;&#26412;&#65288;Rt&#30340;&#31532;&#19968;&#20010;&#20803;&#32032;&#22312;&#21015;&#34920;&#19978;&#30340;&#20301;&#32622;&#65289;&#30340;&#24635;&#21644;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#24212;&#29992;&#31243;&#24207;&#20013;&#33258;&#28982;&#21457;&#29983;&#65292;&#22312;&#20351;&#29992;&#21830;&#24215;&#23458;&#25143;&#30340;&#32858;&#21512;&#20559;&#22909;&#35746;&#36141;&#39033;&#30446;&#26102;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#34987;&#31216;&#20026;&#26368;&#23567;&#21644;&#38598;&#35206;&#30422;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#65288;Fotakis&#31561;&#20154;&#65292;ICALP 2020&#65292;NIPS 2020&#65289;&#22823;&#22810;&#30740;&#31350;&#20102;&#22312;&#32447;&#31639;&#27861;ALG&#19982;&#38745;&#24577;&#26368;&#20248;&#35299;&#65288;&#21807;&#19968;&#26368;&#20248;&#21015;&#34920;&#25490;&#24207;&#65289;&#20043;&#38388;&#30340;&#24615;&#33021;&#19981;&#21516;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35770;&#35777;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#31639;&#27861;RR&#21450;&#20854;&#25193;&#23637;RR+&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;RR+&#23454;&#29616;&#20102;&#19968;&#20010;$\tilde{O}(n^{2/3})$-competitive ratio&#30456;&#23545;&#20110;&#26368;&#20248;&#31163;&#32447;&#35299;&#65292;&#36825;&#26159;&#22312;&#32447;&#35774;&#32622;&#20013;&#24050;&#30693;&#30340;&#26368;&#20339;&#32465;&#23450;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a fundamental model of online preference aggregation, where an algorithm maintains an ordered list of $n$ elements. An input is a stream of preferred sets $R_1, R_2, \dots, R_t, \dots$. Upon seeing $R_t$ and without knowledge of any future sets, an algorithm has to rerank elements (change the list ordering), so that at least one element of $R_t$ is found near the list front. The incurred cost is a sum of the list update costs (the number of swaps of neighboring list elements) and access costs (position of the first element of $R_t$ on the list). This scenario occurs naturally in applications such as ordering items in an online shop using aggregated preferences of shop customers. The theoretical underpinning of this problem is known as Min-Sum Set Cover.  Unlike previous work (Fotakis et al., ICALP 2020, NIPS 2020) that mostly studied the performance of an online algorithm ALG against the static optimal solution (a single optimal list ordering), in this paper, we study an argua
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#20998;&#23618;VAE&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26032;&#35774;&#35745;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65292;&#35753;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#33021;&#22815;&#23481;&#26131;&#22320;&#36827;&#34892;&#37327;&#21270;&#21644;&#29109;&#32534;&#30721;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#31895;&#31961;&#21040;&#31934;&#32454;&#30340;&#26041;&#24335;&#21387;&#32553;&#22270;&#20687;&#65292;&#25903;&#25345;&#24182;&#34892;&#32534;&#35299;&#30721;&#65292;&#22312;&#33258;&#28982;&#22270;&#20687;&#26377;&#25439;&#21387;&#32553;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2208.13056</link><description>&lt;p&gt;
&#37327;&#21270;&#20998;&#23618;VAE&#30340;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Lossy Image Compression with Quantized Hierarchical VAEs. (arXiv:2208.13056v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13056
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#20998;&#23618;VAE&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26032;&#35774;&#35745;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65292;&#35753;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#33021;&#22815;&#23481;&#26131;&#22320;&#36827;&#34892;&#37327;&#21270;&#21644;&#29109;&#32534;&#30721;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#31895;&#31961;&#21040;&#31934;&#32454;&#30340;&#26041;&#24335;&#21387;&#32553;&#22270;&#20687;&#65292;&#25903;&#25345;&#24182;&#34892;&#32534;&#35299;&#30721;&#65292;&#22312;&#33258;&#28982;&#22270;&#20687;&#26377;&#25439;&#21387;&#32553;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#19982;&#36895;&#29575;&#22833;&#30495;&#29702;&#35770;&#20043;&#38388;&#30340;&#24378;&#22823;&#29702;&#35770;&#32852;&#31995;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#20174;&#29983;&#25104;&#24314;&#27169;&#30340;&#35282;&#24230;&#32771;&#34385;&#20102;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#37327;&#21270;&#24863;&#30693;&#21518;&#39564;&#21644;&#20808;&#39564;&#37325;&#26032;&#35774;&#35745;ResNet VAE&#30340;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#36731;&#26494;&#37327;&#21270;&#21644;&#29109;&#32534;&#30721;&#12290;&#25105;&#20204;&#37319;&#29992;&#25913;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21576;&#29616;&#20102;&#19968;&#20010;&#24378;&#22823;&#32780;&#39640;&#25928;&#30340;&#27169;&#22411;&#65292;&#20854;&#22312;&#33258;&#28982;&#22270;&#20687;&#26377;&#25439;&#21387;&#32553;&#19978;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#31895;&#31961;&#21040;&#31934;&#32454;&#30340;&#26041;&#24335;&#21387;&#32553;&#22270;&#20687;&#65292;&#25903;&#25345;&#24182;&#34892;&#32534;&#35299;&#30721;&#65292;&#20174;&#32780;&#22312;GPU&#19978;&#24555;&#36895;&#25191;&#34892;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/duanzhiihao/lossy-vae&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown a strong theoretical connection between variational autoencoders (VAEs) and the rate-distortion theory. Motivated by this, we consider the problem of lossy image compression from the perspective of generative modeling. Starting with ResNet VAEs, which are originally designed for data (image) distribution modeling, we redesign their latent variable model using a quantization-aware posterior and prior, enabling easy quantization and entropy coding at test time. Along with improved neural network architecture, we present a powerful and efficient model that outperforms previous methods on natural image lossy compression. Our model compresses images in a coarse-to-fine fashion and supports parallel encoding and decoding, leading to fast execution on GPUs. Code is available at https://github.com/duanzhiihao/lossy-vae.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#19981;&#30456;&#23481;&#24615;&#27979;&#37327;&#25351;&#26631;&#8220;&#20849;&#21516;&#26412;&#24449;&#31354;&#38388;&#24178;&#25200;&#8221;&#65292;&#24182;&#36827;&#19968;&#27493;&#36890;&#36807;&#37327;&#23376;&#24320;&#20851;&#23454;&#29616;&#37327;&#21270;&#65292;&#26041;&#20415;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#22788;&#29702;&#12290;&#21516;&#26102;&#65292;&#31639;&#27861;&#21487;&#20197;&#23545;&#27979;&#37327;&#32467;&#26524;&#36827;&#34892;&#32858;&#31867;&#65292;&#30830;&#23450;&#20849;&#20139;&#30456;&#20284;&#27979;&#37327;&#29615;&#22659;&#30340;&#35266;&#23519;&#32773;&#32452;&#12290;</title><link>http://arxiv.org/abs/2208.06210</link><description>&lt;p&gt;
&#29992;&#37327;&#23376;&#24320;&#20851;&#27979;&#37327;&#19981;&#30456;&#23481;&#24615;&#24182;&#23545;&#37327;&#23376;&#35266;&#27979;&#32467;&#26524;&#36827;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Measuring incompatibility and clustering quantum observables with a quantum switch. (arXiv:2208.06210v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#19981;&#30456;&#23481;&#24615;&#27979;&#37327;&#25351;&#26631;&#8220;&#20849;&#21516;&#26412;&#24449;&#31354;&#38388;&#24178;&#25200;&#8221;&#65292;&#24182;&#36827;&#19968;&#27493;&#36890;&#36807;&#37327;&#23376;&#24320;&#20851;&#23454;&#29616;&#37327;&#21270;&#65292;&#26041;&#20415;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#22788;&#29702;&#12290;&#21516;&#26102;&#65292;&#31639;&#27861;&#21487;&#20197;&#23545;&#27979;&#37327;&#32467;&#26524;&#36827;&#34892;&#32858;&#31867;&#65292;&#30830;&#23450;&#20849;&#20139;&#30456;&#20284;&#27979;&#37327;&#29615;&#22659;&#30340;&#35266;&#23519;&#32773;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30456;&#23481;&#35266;&#27979;&#32467;&#26524;&#26159;&#37327;&#23376;&#21147;&#23398;&#30340;&#22522;&#30707;&#65292;&#24182;&#19988;&#26159;&#37327;&#23376;&#25216;&#26415;&#20013;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30456;&#23481;&#24615;&#27979;&#37327;&#25351;&#26631;&#65292;&#21363;&#8220;&#20849;&#21516;&#26412;&#24449;&#31354;&#38388;&#24178;&#25200;&#8221;&#65292;&#21487;&#20197;&#37327;&#21270;&#19968;&#20010;&#31934;&#30830;&#35266;&#27979;&#37327;&#23545;&#21478;&#19968;&#20010;&#26412;&#24449;&#31354;&#38388;&#30340;&#24178;&#25200;&#37327;&#12290;&#35813;&#25351;&#26631;&#25552;&#20379;&#20102; von Neumann &#27979;&#37327;&#31354;&#38388;&#30340;&#24230;&#37327;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#31216;&#20026;&#8220;&#37327;&#23376;&#24320;&#20851;&#8221;&#30340;&#35774;&#32622;&#65292;&#35753;&#27979;&#37327;&#36807;&#31243;&#20197;&#19981;&#30830;&#23450;&#30340;&#39034;&#24207;&#36827;&#34892;&#65292;&#20174;&#32780;&#39640;&#25928;&#22320;&#36827;&#34892;&#20272;&#35745;&#12290;&#30001;&#20110;&#36825;&#20123;&#29305;&#24615;&#65292;MED&#21487;&#20197;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26080;&#30417;&#30563;&#31639;&#27861;&#65292;&#23545;&#26410;&#30693;&#30340;von Neumann &#27979;&#37327;&#32467;&#26524;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#30830;&#23450;&#20849;&#20139;&#36817;&#20284;&#30456;&#21516;&#27979;&#37327;&#29615;&#22659;&#30340;&#35266;&#23519;&#32773;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existence of incompatible observables is a cornerstone of quantum mechanics and a valuable resource in quantum technologies. Here we introduce a measure of incompatibility, called the mutual eigenspace disturbance (MED), which quantifies the amount of disturbance induced by the measurement of a sharp observable on the eigenspaces of another. The MED provides a metric on the space of von Neumann measurements, and can be efficiently estimated by letting the measurement processes act in an indefinite order, using a setup known as the quantum switch, which also allows one to quantify the noncommutativity of arbitrary quantum processes. Thanks to these features, the MED can be used in quantum machine learning tasks. We demonstrate this application by providing an unsupervised algorithm that clusters unknown von Neumann measurements. Our algorithm is robust to noise can be used to identify groups of observers that share approximately the same measurement context.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#21270;&#23398;&#21453;&#24212;&#34920;&#31034;&#20026;&#36229;&#22270;&#65292;&#26500;&#24314;&#20102;&#22522;&#20110;AI&#30340;&#26377;&#26426;&#21270;&#23398;&#36229;&#22270;&#32593;&#32476;&#24182;&#36827;&#34892;&#20102;&#32479;&#35745;&#30740;&#31350;&#65292;&#20026;&#21453;&#24212;&#20998;&#31867;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2208.01647</link><description>&lt;p&gt;
&#22522;&#20110;AI&#30340;&#26377;&#26426;&#21270;&#23398;&#36229;&#22270;&#32593;&#32476;&#65306;&#32593;&#32476;&#32479;&#35745;&#21644;&#21453;&#24212;&#20998;&#31867;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AI-driven Hypergraph Network of Organic Chemistry: Network Statistics and Applications in Reaction Classification. (arXiv:2208.01647v2 [q-bio.MN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#21270;&#23398;&#21453;&#24212;&#34920;&#31034;&#20026;&#36229;&#22270;&#65292;&#26500;&#24314;&#20102;&#22522;&#20110;AI&#30340;&#26377;&#26426;&#21270;&#23398;&#36229;&#22270;&#32593;&#32476;&#24182;&#36827;&#34892;&#20102;&#32479;&#35745;&#30740;&#31350;&#65292;&#20026;&#21453;&#24212;&#20998;&#31867;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39640;&#36890;&#37327;&#31579;&#36873;&#30340;&#36827;&#23637;&#12289;&#26356;&#22797;&#26434;&#21270;&#23398;&#35774;&#35745;&#31354;&#38388;&#30340;&#21487;&#35775;&#38382;&#24615;&#20197;&#21450;&#31934;&#30830;&#30340;&#20998;&#23376;&#24314;&#27169;&#26694;&#26550;&#30340;&#21457;&#23637;&#65292;&#20419;&#36827;&#20102;&#26032;&#21453;&#24212;&#21644;&#20998;&#23376;&#30340;&#24555;&#36895;&#21457;&#29616;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#36827;&#34892;&#20851;&#27880;&#26368;&#36817;&#36235;&#21183;&#24182;&#23558;&#20854;&#22806;&#25512;&#21040;&#21487;&#33021;&#30340;&#26410;&#26469;&#36712;&#36857;&#30340;&#32508;&#21512;&#24615;&#30740;&#31350;&#26469;&#29702;&#35299;&#19981;&#26029;&#22686;&#38271;&#30340;&#21270;&#23398;&#25991;&#29486;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#25253;&#21578;&#20102;&#20960;&#20010;&#22522;&#20110;&#32593;&#32476;&#29702;&#35770;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#30740;&#31350;&#20351;&#29992;&#21270;&#23398;&#21453;&#24212;&#30340;&#26377;&#21521;&#22270;&#34920;&#31034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#36229;&#22270;&#26469;&#34920;&#31034;&#21270;&#23398;&#21453;&#24212;&#65292;&#20854;&#20013;&#36229;&#36793;&#34920;&#31034;&#21270;&#23398;&#21453;&#24212;&#65292;&#33410;&#28857;&#34920;&#31034;&#21442;&#19982;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;&#21453;&#24212;&#25968;&#25454;&#38598;&#26500;&#36896;&#20102;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#24182;&#25253;&#21578;&#20854;&#32479;&#35745;&#20449;&#24687;&#65292;&#20363;&#22914;&#24230;&#20998;&#24067;&#12289;&#24179;&#22343;&#36335;&#24452;&#38271;&#24230;&#12289;&#21516;&#37197;&#24615;&#25110;&#24230;&#30456;&#20851;&#24615;&#12289;PageRank &#20013;&#24515;&#24615;&#21644;&#22522;&#20110;&#22270;&#30340;&#32858;&#31867;&#65288;&#25110;&#31038;&#21306;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid discovery of new reactions and molecules in recent years has been facilitated by the advancements in high throughput screening, accessibility to a much more complex chemical design space, and the development of accurate molecular modeling frameworks. A holistic study of the growing chemistry literature is, therefore, required that focuses on understanding the recent trends and extrapolating them into possible future trajectories. To this end, several network theory-based studies have been reported that use a directed graph representation of chemical reactions. Here, we perform a study based on representing chemical reactions as hypergraphs where the hyperedges represent chemical reactions and nodes represent the participating molecules. We use a standard reactions dataset to construct a hypernetwork and report its statistics such as degree distributions, average path length, assortativity or degree correlations, PageRank centrality, and graph-based clusters (or communities). We a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#24320;&#22987;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.15387</link><description>&lt;p&gt;
&#20174;&#21738;&#37324;&#24320;&#22987;&#65311;&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#21644;&#21021;&#22987;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning. (arXiv:2206.15387v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#24320;&#22987;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#24120;&#35265;&#25361;&#25112;&#26159;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#25968;&#25454;&#24322;&#26500;&#24615;&#26159;&#25351;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#21487;&#33021;&#36981;&#24490;&#38750;&#24120;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#31995;&#32479;&#24322;&#26500;&#24615;&#26159;&#25351;&#23458;&#25143;&#31471;&#35774;&#22791;&#20855;&#26377;&#19981;&#21516;&#30340;&#31995;&#32479;&#33021;&#21147;&#12290;&#35768;&#22810;&#32852;&#37030;&#20248;&#21270;&#26041;&#27861;&#37117;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#36890;&#24120;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#24320;&#22987;&#32852;&#37030;&#35757;&#32451;&#65292;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#26381;&#21153;&#22120;&#21487;&#20197;&#35775;&#38382;&#29992;&#20110;&#35757;&#32451;&#20219;&#21153;&#30340;&#20195;&#29702;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#36825;&#20123;&#25968;&#25454;&#22312;&#24320;&#22987;&#32852;&#37030;&#35757;&#32451;&#20043;&#21069;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20351;&#29992;&#22235;&#20010;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20174;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#24320;&#22987;&#30340;&#24433;&#21709;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#20174;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#24320;&#22987;&#21487;&#20197;&#20943;&#23569;&#36798;&#21040;&#30446;&#26631;&#35823;&#24046;&#29575;&#25152;&#38656;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#20351;&#27169;&#22411;&#35757;&#32451;&#26356;&#20934;&#30830;&#65288;&#39640;&#36798;40&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
An oft-cited challenge of federated learning is the presence of heterogeneity. \emph{Data heterogeneity} refers to the fact that data from different clients may follow very different distributions. \emph{System heterogeneity} refers to client devices having different system capabilities. A considerable number of federated optimization methods address this challenge. In the literature, empirical evaluations usually start federated training from random initialization. However, in many practical applications of federated learning, the server has access to proxy data for the training task that can be used to pre-train a model before starting federated training. Using four standard federated learning benchmark datasets, we empirically study the impact of starting from a pre-trained model in federated learning. Unsurprisingly, starting from a pre-trained model reduces the training time required to reach a target error rate and enables the training of more accurate models (up to 40\%) than is
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#22411;&#20998;&#25955;&#24335;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#32593;&#32476;&#20013;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#21021;&#27493;&#35777;&#26126;&#20102;&#20854;&#22312;&#38750;&#20984;&#24378;&#20984;&#38382;&#39064;&#26041;&#38754;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2206.15025</link><description>&lt;p&gt;
&#20851;&#20110;&#32593;&#32476;&#20013;&#20998;&#24067;&#24335;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Distributed Stochastic Bilevel Optimization Algorithms over a Network. (arXiv:2206.15025v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#22411;&#20998;&#25955;&#24335;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#32593;&#32476;&#20013;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#21021;&#27493;&#35777;&#26126;&#20102;&#20854;&#22312;&#38750;&#20984;&#24378;&#20984;&#38382;&#39064;&#26041;&#38754;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#24050;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#24182;&#19988;&#36817;&#24180;&#26469;&#24050;&#24320;&#21457;&#20986;&#35768;&#22810;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#37117;&#23558;&#37325;&#28857;&#25918;&#22312;&#21333;&#26426;&#35774;&#32622;&#19978;&#65292;&#22240;&#27492;&#26080;&#27861;&#22788;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#25152;&#26377;&#21442;&#19982;&#32773;&#32452;&#25104;&#32593;&#32476;&#24182;&#22312;&#35813;&#32593;&#32476;&#20013;&#36827;&#34892;&#28857;&#23545;&#28857;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#22522;&#20110;&#28176;&#21464;&#36319;&#36394;&#36890;&#20449;&#26426;&#21046;&#21644;&#20004;&#31181;&#19981;&#21516;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#26032;&#22411;&#20998;&#25955;&#24335;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36816;&#29992;&#26032;&#22411;&#29702;&#35770;&#20998;&#26512;&#31574;&#30053;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#38750;&#20984;&#24378;&#20984;&#38382;&#39064;&#26041;&#38754;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#30340;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#31639;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization has been applied to a wide variety of machine learning models, and numerous stochastic bilevel optimization algorithms have been developed in recent years. However, most existing algorithms restrict their focus on the single-machine setting so that they are incapable of handling the distributed data. To address this issue, under the setting where all participants compose a network and perform peer-to-peer communication in this network, we developed two novel decentralized stochastic bilevel optimization algorithms based on the gradient tracking communication mechanism and two different gradient estimators. Additionally, we established their convergence rates for nonconvex-strongly-convex problems with novel theoretical analysis strategies. To our knowledge, this is the first work achieving these theoretical results. Finally, we applied our algorithms to practical machine learning models, and the experimental results confirmed the efficacy of our algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.13508</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65306;&#19968;&#20221;&#32508;&#36848;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation techniques in time series domain: A survey and taxonomy. (arXiv:2206.13508v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21033;&#29992;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#20013;&#20986;&#33394;&#24615;&#33021;&#30340;&#26041;&#24335;&#24182;&#19981;&#38656;&#35201;&#22826;&#38271;&#26102;&#38388;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24182;&#19981;&#20016;&#23500;&#65292;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#21644;&#38656;&#35201;&#20445;&#35777;&#30340;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#25552;&#39640;&#25968;&#25454;&#37327;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#25110;&#32622;&#25442;&#36824;&#26159;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#29616;&#29366;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#21487;&#29992;&#31639;&#27861;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#30456;&#20851;&#30740;&#31350;&#30340;&#20998;&#31867;&#27861;&#12290;&#19981;&#21516;&#21464;&#20307;&#30340;&#25928;&#29575;&#23558;&#20316;&#20026;&#35813;&#36807;&#31243;&#30340;&#20013;&#24515;&#37096;&#20998;&#36827;&#34892;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#23558;&#35780;&#20272;&#19981;&#21516;&#30340;&#24615;&#33021;&#25351;&#26631;&#20197;&#21450;&#27599;&#20010;&#27169;&#22411;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the latest advances in Deep Learning-based} generative models, it has not taken long to take advantage of their remarkable performance in the area of time series. Deep neural networks used to work with time series heavily depend on the size and consistency of the datasets used in training. These features are not usually abundant in the real world, where they are usually limited and often have constraints that must be guaranteed. Therefore, an effective way to increase the amount of data is by using Data Augmentation techniques, either by adding noise or permutations and by generating new synthetic data. This work systematically reviews the current state-of-the-art in the area to provide an overview of all available algorithms and proposes a taxonomy of the most relevant research. The efficiency of the different variants will be evaluated as a central part of the process, as well as the different metrics to evaluate the performance and the main problems concerning each model will b
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FEATHERS&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#35774;&#32622;&#20013;&#30340;&#31070;&#32463;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#65292;&#20855;&#26377;&#39640;&#25928;&#29575;&#21644;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.12342</link><description>&lt;p&gt;
FEATHERS&#65306;&#32852;&#37030;&#20307;&#31995;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
FEATHERS: Federated Architecture and Hyperparameter Search. (arXiv:2206.12342v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12342
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FEATHERS&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#35774;&#32622;&#20013;&#30340;&#31070;&#32463;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#65292;&#20855;&#26377;&#39640;&#25928;&#29575;&#21644;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32467;&#26500;&#23545;&#20110;&#24403;&#20170;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#24615;&#33021;&#26377;&#30528;&#28145;&#36828;&#24433;&#21709;&#65292;&#28982;&#32780;&#20854;&#35774;&#35745;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#31867;&#30340;&#20808;&#21069;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20197;&#21450;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HO&#65289;&#26377;&#21161;&#20110;&#20943;&#23569;&#36825;&#31181;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20998;&#24067;&#24335;&#23384;&#20648;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#26368;&#20808;&#36827;&#30340;NAS&#21644;HO&#24448;&#24448;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#36890;&#24120;&#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#65292;&#22914;GDPR&#21644;CCPA&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FEATHERS&#65288;&#32852;&#37030;&#20307;&#31995;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#25628;&#32034;&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#35774;&#32622;&#20013;&#32852;&#21512;&#20248;&#21270;&#31070;&#32463;&#32467;&#26500;&#21644;&#20248;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#65292;&#32780;&#19988;&#36824;&#36890;&#36807;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#36981;&#23432;&#25968;&#25454;&#38544;&#31169;&#35268;&#23450;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;FEATHERS&#21487;&#20197;&#26377;&#25928;&#22320;&#20248;&#21270;&#20307;&#31995;&#32467;&#26500;&#21644;&#20248;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#23637;&#31034;&#25910;&#25947;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural architectures have profound impact on achieved performance in many of today's AI tasks, yet, their design still heavily relies on human prior knowledge and experience. Neural architecture search (NAS) together with hyperparameter optimization (HO) helps to reduce this dependence. However, state of the art NAS and HO rapidly become infeasible with increasing amount of data being stored in a distributed fashion, typically violating data privacy regulations such as GDPR and CCPA. As a remedy, we introduce FEATHERS $\textbf{FE}$derated $\textbf{A}$rchi$\textbf{T}$ecture and $\textbf{H}$yp$\textbf{ER}$parameter $\textbf{S}$earch, a method that not only optimizes both neural architectures and optimization-related hyperparameters jointly in distributed data settings, but further adheres to data privacy through the use of differential privacy (DP). We show that FEATHERS efficiently optimizes architectural and optimization-related hyperparameters alike, while demonstrating converg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FreeKD&#30340;&#33258;&#30001;&#26041;&#21521;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#20004;&#20010;&#36739;&#27973;&#30340;GNN&#65292;&#20351;&#20854;&#22312;&#23427;&#20204;&#20043;&#38388;&#20132;&#25442;&#30693;&#35782;&#65292;&#26080;&#38656;&#25552;&#20379;&#28145;&#24230;&#20248;&#21270;&#30340;&#25945;&#24072;GNN&#65292;&#24182;&#19988;&#37319;&#29992;&#21160;&#24577;&#21644;&#33258;&#30001;&#26041;&#21521;&#30340;&#30693;&#35782;&#36716;&#31227;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2206.06561</link><description>&lt;p&gt;
FreeKD&#65306;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#30001;&#26041;&#21521;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
FreeKD: Free-direction Knowledge Distillation for Graph Neural Networks. (arXiv:2206.06561v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FreeKD&#30340;&#33258;&#30001;&#26041;&#21521;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#20004;&#20010;&#36739;&#27973;&#30340;GNN&#65292;&#20351;&#20854;&#22312;&#23427;&#20204;&#20043;&#38388;&#20132;&#25442;&#30693;&#35782;&#65292;&#26080;&#38656;&#25552;&#20379;&#28145;&#24230;&#20248;&#21270;&#30340;&#25945;&#24072;GNN&#65292;&#24182;&#19988;&#37319;&#29992;&#21160;&#24577;&#21644;&#33258;&#30001;&#26041;&#21521;&#30340;&#30693;&#35782;&#36716;&#31227;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#24050;&#32463;&#35777;&#26126;&#20102;&#19968;&#23450;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#20854;&#30446;&#26631;&#26159;&#23558;&#26469;&#33258;&#28145;&#24230;&#25945;&#24072;GNN&#30340;&#30693;&#35782;&#27987;&#32553;&#21040;&#36739;&#27973;&#30340;&#23398;&#29983;GNN&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24050;&#30693;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#26500;&#24314;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#28145;&#23618;&#25945;&#24072;GNN&#23454;&#38469;&#19978;&#26159;&#22256;&#38590;&#30340;&#65292;&#36825;&#23548;&#33268;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26080;&#25928;&#30693;&#35782;&#20256;&#36882;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33258;&#30001;&#26041;&#21521;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;FreeKD&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#38024;&#23545;GNN&#65292;&#20854;&#19981;&#20877;&#38656;&#35201;&#25552;&#20379;&#28145;&#24230;&#20248;&#21270;&#30340;&#25945;&#24072;GNN&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20197;&#20998;&#23618;&#26041;&#24335;&#21327;&#20316;&#22320;&#26500;&#24314;&#20004;&#20010;&#36739;&#27973;&#30340;GNN&#65292;&#20197;&#22312;&#23427;&#20204;&#20043;&#38388;&#20132;&#25442;&#30693;&#35782;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19968;&#20010;&#20856;&#22411;&#30340;GNN&#27169;&#22411;&#36890;&#24120;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22312;&#19981;&#21516;&#33410;&#28857;&#26377;&#36739;&#22909;&#21644;&#36739;&#24046;&#30340;&#34920;&#29616;&#65292;&#22240;&#27492;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#21644;&#33258;&#30001;&#26041;&#21521;&#30340;&#30693;&#35782;&#36716;&#31227;&#31574;&#30053;&#65292;&#21253;&#25324;&#12295;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) has demonstrated its effectiveness to boost the performance of graph neural networks (GNNs), where its goal is to distill knowledge from a deeper teacher GNN into a shallower student GNN. However, it is actually difficult to train a satisfactory teacher GNN due to the well-known over-parametrized and over-smoothing issues, leading to invalid knowledge transfer in practical applications. In this paper, we propose the first Free-direction Knowledge Distillation framework via Reinforcement learning for GNNs, called FreeKD, which is no longer required to provide a deeper well-optimized teacher GNN. The core idea of our work is to collaboratively build two shallower GNNs in an effort to exchange knowledge between them via reinforcement learning in a hierarchical way. As we observe that one typical GNN model often has better and worse performances at different nodes during training, we devise a dynamic and free-direction knowledge transfer strategy that consists o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Consistent Attack&#30340;&#26041;&#27861;&#29992;&#20110;&#29983;&#25104;&#35270;&#35273;&#23548;&#33322;&#20013;&#30340;&#26222;&#36866;&#24615;&#23545;&#25239;&#25200;&#21160;&#65292;&#22312;&#32771;&#34385;&#31995;&#32479;&#21160;&#24577;&#30340;&#21516;&#26102;&#20248;&#21270;&#23545;&#25239;&#25200;&#21160;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;UAP&#26041;&#27861;&#19988;&#34920;&#29616;&#26368;&#20808;&#36827;&#12290;</title><link>http://arxiv.org/abs/2206.05751</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#20915;&#31574;&#30340;&#35270;&#35273;&#23548;&#33322;&#26222;&#36866;&#24615;&#23545;&#25239;&#25200;&#21160;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Consistent Attack: Universal Adversarial Perturbation on Embodied Vision Navigation. (arXiv:2206.05751v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Consistent Attack&#30340;&#26041;&#27861;&#29992;&#20110;&#29983;&#25104;&#35270;&#35273;&#23548;&#33322;&#20013;&#30340;&#26222;&#36866;&#24615;&#23545;&#25239;&#25200;&#21160;&#65292;&#22312;&#32771;&#34385;&#31995;&#32479;&#21160;&#24577;&#30340;&#21516;&#26102;&#20248;&#21270;&#23545;&#25239;&#25200;&#21160;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;UAP&#26041;&#27861;&#19988;&#34920;&#29616;&#26368;&#20808;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#23548;&#33322;&#23884;&#20837;&#24335;&#26234;&#33021;&#20307;&#26222;&#36941;&#23384;&#22312;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#23041;&#32961;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Consistent Attack"&#30340;&#26222;&#36866;&#24615;&#23545;&#25239;&#25200;&#21160;&#29983;&#25104;&#26041;&#27861;&#65292;&#20854;&#22312;&#25200;&#21160;&#25915;&#20987;&#26102;&#19981;&#20165;&#32771;&#34385;&#20102;&#31995;&#32479;&#21160;&#24577;&#65292;&#36824;&#20248;&#21270;&#20102;&#23545;&#25239;&#25200;&#21160;&#65292;&#26368;&#23567;&#21270;&#26234;&#33021;&#20307;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#19978;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;UAP&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied agents in vision navigation coupled with deep neural networks have attracted increasing attention. However, deep neural networks have been shown vulnerable to malicious adversarial noises, which may potentially cause catastrophic failures in Embodied Vision Navigation. Among different adversarial noises, universal adversarial perturbations (UAP), i.e., a constant image-agnostic perturbation applied on every input frame of the agent, play a critical role in Embodied Vision Navigation since they are computation-efficient and application-practical during the attack. However, existing UAP methods ignore the system dynamics of Embodied Vision Navigation and might be sub-optimal. In order to extend UAP to the sequential decision setting, we formulate the disturbed environment under the universal noise $\delta$, as a $\delta$-disturbed Markov Decision Process ($\delta$-MDP). Based on the formulation, we analyze the properties of $\delta$-MDP and propose two novel Consistent Attack me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30830;&#23450;&#20102;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#27169;&#22411;&#30340;&#26368;&#20339;&#28608;&#27963;&#20989;&#25968;&#12290;&#36825;&#20123;&#20989;&#25968;&#21487;&#33021;&#26159;&#32447;&#24615;&#30340;&#12289;&#39281;&#21644;&#32447;&#24615;&#20989;&#25968;&#25110;&#22522;&#20110;Hermite&#22810;&#39033;&#24335;&#30340;&#20989;&#25968;&#12290;&#20351;&#29992;&#26368;&#20339;&#28608;&#27963;&#20989;&#25968;&#20250;&#24433;&#21709;RFR&#27169;&#22411;&#30340;&#37325;&#35201;&#29305;&#24615;&#65292;&#22914;&#21452;&#23792;&#26354;&#32447;&#21644;&#26368;&#20339;&#27491;&#21017;&#21270;&#21442;&#25968;&#19982;&#22122;&#22768;&#27700;&#24179;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.01332</link><description>&lt;p&gt;
&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#27169;&#22411;&#30340;&#26368;&#20339;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Optimal Activation Functions for the Random Features Regression Model. (arXiv:2206.01332v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30830;&#23450;&#20102;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#27169;&#22411;&#30340;&#26368;&#20339;&#28608;&#27963;&#20989;&#25968;&#12290;&#36825;&#20123;&#20989;&#25968;&#21487;&#33021;&#26159;&#32447;&#24615;&#30340;&#12289;&#39281;&#21644;&#32447;&#24615;&#20989;&#25968;&#25110;&#22522;&#20110;Hermite&#22810;&#39033;&#24335;&#30340;&#20989;&#25968;&#12290;&#20351;&#29992;&#26368;&#20339;&#28608;&#27963;&#20989;&#25968;&#20250;&#24433;&#21709;RFR&#27169;&#22411;&#30340;&#37325;&#35201;&#29305;&#24615;&#65292;&#22914;&#21452;&#23792;&#26354;&#32447;&#21644;&#26368;&#20339;&#27491;&#21017;&#21270;&#21442;&#25968;&#19982;&#22122;&#22768;&#27700;&#24179;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#24050;&#30740;&#31350;&#20102;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#27169;&#22411;(RFR)&#30340;&#28176;&#36817;&#22343;&#26041;&#27979;&#35797;&#35823;&#24046;&#21644;&#28789;&#25935;&#24230;&#12290;&#25105;&#20204;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#20989;&#25968;&#31616;&#27905;&#27010;&#24565;&#65292;&#30830;&#23450;&#20102;&#22312;&#38381;&#21512;&#24418;&#24335;&#19979;&#26497;&#23567;&#21270;RFR&#27979;&#35797;&#35823;&#24046;&#21644;&#28789;&#25935;&#24230;&#32452;&#21512;&#30340;&#28608;&#27963;&#20989;&#25968;(AF)&#26063;&#32676;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#65292;&#26368;&#20339;AF&#21487;&#20197;&#26159;&#32447;&#24615;&#30340;&#12289;&#39281;&#21644;&#32447;&#24615;&#20989;&#25968;&#25110;&#22522;&#20110;Hermite&#22810;&#39033;&#24335;&#34920;&#31034;&#30340;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26368;&#20339;AF&#22914;&#20309;&#24433;&#21709;RFR&#27169;&#22411;&#30340;&#37325;&#35201;&#29305;&#24615;&#65292;&#27604;&#22914;&#21452;&#23792;&#26354;&#32447;&#21644;&#20854;&#26368;&#20339;&#27491;&#21017;&#21270;&#21442;&#25968;&#19982;&#35266;&#23519;&#22122;&#22768;&#27700;&#24179;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The asymptotic mean squared test error and sensitivity of the Random Features Regression model (RFR) have been recently studied. We build on this work and identify in closed-form the family of Activation Functions (AFs) that minimize a combination of the test error and sensitivity of the RFR under different notions of functional parsimony. We find scenarios under which the optimal AFs are linear, saturated linear functions, or expressible in terms of Hermite polynomials. Finally, we show how using optimal AFs impacts well-established properties of the RFR model, such as its double descent curve, and the dependency of its optimal regularization parameter on the observation noise level.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#20132;&#25442;&#20256;&#36882;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;3D&#29289;&#20307;&#26816;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20855;&#26377;&#21387;&#32553;&#34920;&#31034;&#25439;&#22833;&#21644;&#22836;&#37096;&#27880;&#24847;&#21147;&#25439;&#22833;&#31561;&#20248;&#21270;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2205.15531</link><description>&lt;p&gt;
itKD: &#22522;&#20110;&#20132;&#25442;&#20256;&#36882;&#30340;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;3D&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
itKD: Interchange Transfer-based Knowledge Distillation for 3D Object Detection. (arXiv:2205.15531v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15531
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#20132;&#25442;&#20256;&#36882;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;3D&#29289;&#20307;&#26816;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20855;&#26377;&#21387;&#32553;&#34920;&#31034;&#25439;&#22833;&#21644;&#22836;&#37096;&#27880;&#24847;&#21147;&#25439;&#22833;&#31561;&#20248;&#21270;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#30340;3D&#29289;&#20307;&#26816;&#27979;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#35745;&#31639;&#25928;&#29575;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#39118;&#26684;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#20132;&#25442;&#20256;&#36882;&#30340;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#36890;&#36947;&#21387;&#32553;&#21644;&#35299;&#21387;&#32553;&#12290;&#20026;&#20102;&#23398;&#20064;&#25945;&#24072;&#32593;&#32476;&#30340;&#22320;&#22270;&#35270;&#22270;&#29305;&#24449;&#65292;&#20174;&#25945;&#24072;&#21644;&#23398;&#29983;&#32593;&#32476;&#24471;&#21040;&#30340;&#29305;&#24449;&#34987;&#29420;&#31435;&#22320;&#36890;&#36807;&#20849;&#20139;&#30340;&#33258;&#32534;&#30721;&#22120;&#20256;&#36882;&#65307;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21387;&#32553;&#34920;&#31034;&#25439;&#22833;&#65292;&#23558;&#23398;&#29983;&#21644;&#25945;&#24072;&#32593;&#32476;&#30340;&#36890;&#36947;&#21387;&#32553;&#30693;&#35782;&#20316;&#20026;&#19968;&#31181;&#27491;&#21017;&#21270;&#36827;&#34892;&#32465;&#23450;&#12290;&#35299;&#21387;&#30340;&#29305;&#24449;&#21453;&#21521;&#20256;&#36882;&#20197;&#20943;&#23567;&#20132;&#25442;&#37325;&#26500;&#20013;&#30340;&#24046;&#36317;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22836;&#37096;&#27880;&#24847;&#21147;&#25439;&#22833;&#65292;&#20197;&#21305;&#37197;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#25152;&#25552;&#21462;&#30340;3D&#29289;&#20307;&#26816;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point-cloud based 3D object detectors recently have achieved remarkable progress. However, most studies are limited to the development of network architectures for improving only their accuracy without consideration of the computational efficiency. In this paper, we first propose an autoencoder-style framework comprising channel-wise compression and decompression via interchange transfer-based knowledge distillation. To learn the map-view feature of a teacher network, the features from teacher and student networks are independently passed through the shared autoencoder; here, we use a compressed representation loss that binds the channel-wised compression knowledge from both student and teacher networks as a kind of regularization. The decompressed features are transferred in opposite directions to reduce the gap in the interchange reconstructions. Lastly, we present an head attention loss to match the 3D object detection information drawn by the multi-head self-attention mechanism. Th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#24102;&#26377;&#38544;&#34255;&#29366;&#24577;&#30340;&#21160;&#24577;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#20998;&#26512;&#38382;&#39064;&#65292;&#24182;&#22312;&#20083;&#33146;&#30284;&#26696;&#20363;&#30740;&#31350;&#20013;&#25104;&#21151;&#24212;&#29992;&#12290;&#36890;&#36807;&#20248;&#21270;&#26041;&#27861;&#24471;&#21040;&#20102;&#21453;&#20107;&#23454;&#37327;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#65292;&#24182;&#19988;&#35813;&#35770;&#25991;&#26159;&#31532;&#19968;&#20010;&#22312;&#21160;&#24577;&#28508;&#22312;&#29366;&#24577;&#27169;&#22411;&#20013;&#36827;&#34892;&#36825;&#31181;&#35745;&#31639;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2205.13832</link><description>&lt;p&gt;
&#21160;&#24577;&#28508;&#22312;&#29366;&#24577;&#27169;&#22411;&#20013;&#30340;&#21453;&#20107;&#23454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Analysis in Dynamic Latent State Models. (arXiv:2205.13832v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13832
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#24102;&#26377;&#38544;&#34255;&#29366;&#24577;&#30340;&#21160;&#24577;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#20998;&#26512;&#38382;&#39064;&#65292;&#24182;&#22312;&#20083;&#33146;&#30284;&#26696;&#20363;&#30740;&#31350;&#20013;&#25104;&#21151;&#24212;&#29992;&#12290;&#36890;&#36807;&#20248;&#21270;&#26041;&#27861;&#24471;&#21040;&#20102;&#21453;&#20107;&#23454;&#37327;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#65292;&#24182;&#19988;&#35813;&#35770;&#25991;&#26159;&#31532;&#19968;&#20010;&#22312;&#21160;&#24577;&#28508;&#22312;&#29366;&#24577;&#27169;&#22411;&#20013;&#36827;&#34892;&#36825;&#31181;&#35745;&#31639;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#26694;&#26550;&#26469;&#25191;&#34892;&#20855;&#26377;&#38544;&#34255;&#29366;&#24577;&#30340;&#21160;&#24577;&#27169;&#22411;&#20013;&#30340;&#21453;&#20107;&#23454;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20197;&#8220;&#29468;&#27979;&#12289;&#34892;&#21160;&#21644;&#39044;&#27979;&#8221;&#26041;&#27861;&#20026;&#22522;&#30784;&#65292;&#20197;&#22238;&#31572;&#21453;&#20107;&#23454;&#26597;&#35810;&#65292;&#24182;&#35299;&#20915;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;(1)&#29366;&#24577;&#26159;&#38544;&#34255;&#30340;&#65292;(2)&#27169;&#22411;&#26159;&#21160;&#24577;&#30340;&#12290;&#32771;&#34385;&#21040;&#23545;&#28508;&#22312;&#22240;&#26524;&#26426;&#21046;&#30340;&#32570;&#20047;&#20102;&#35299;&#20197;&#21450;&#21487;&#33021;&#23384;&#22312;&#26080;&#38480;&#22810;&#20010;&#36825;&#26679;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#22312;&#35813;&#31354;&#38388;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#35745;&#31639;&#25152;&#20851;&#24515;&#30340;&#21453;&#20107;&#23454;&#37327;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27719;&#38598;&#20102;&#22240;&#26524;&#20851;&#31995;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#24605;&#24819;&#65292;&#24182;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#26696;&#20363;&#30740;&#31350;&#20013;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#21160;&#24577;&#28508;&#22312;&#29366;&#24577;&#27169;&#22411;&#20013;&#35745;&#31639;&#21453;&#20107;&#23454;&#26597;&#35810;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide an optimization-based framework to perform counterfactual analysis in a dynamic model with hidden states. Our framework is grounded in the ``abduction, action, and prediction'' approach to answer counterfactual queries and handles two key challenges where (1) the states are hidden and (2) the model is dynamic. Recognizing the lack of knowledge on the underlying causal mechanism and the possibility of infinitely many such mechanisms, we optimize over this space and compute upper and lower bounds on the counterfactual quantity of interest. Our work brings together ideas from causality, state-space models, simulation, and optimization, and we apply it on a breast cancer case study. To the best of our knowledge, we are the first to compute lower and upper bounds on a counterfactual query in a dynamic latent-state model.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#27850;&#26494;&#36817;&#20284;&#20284;&#28982;(PAL)&#26041;&#27861;&#65292;&#19982;ODE&#26041;&#27861;&#19981;&#21516;&#65292;PAL&#26159;&#20174;&#26377;&#38480;&#20154;&#21475;&#25968;&#37327;&#30340;&#38543;&#26426;&#20998;&#21306;&#27169;&#22411;&#30340;&#36817;&#20284;&#28388;&#27874;&#26041;&#31243;&#23548;&#20986;&#30340;&#65292;&#24182;&#19988;&#22823;&#20154;&#21475;&#25968;&#37327;&#38480;&#21046;&#25512;&#21160;&#26368;&#22823;PAL&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13602</link><description>&lt;p&gt;
&#21033;&#29992;&#27850;&#26494;&#36817;&#20284;&#20284;&#28982;&#36827;&#34892;&#27969;&#34892;&#30149;&#20998;&#21306;&#27169;&#22411;&#20013;&#30340;&#19968;&#33268;&#19988;&#24555;&#36895;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Consistent and fast inference in compartmental models of epidemics using Poisson Approximate Likelihoods. (arXiv:2205.13602v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13602
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#27850;&#26494;&#36817;&#20284;&#20284;&#28982;(PAL)&#26041;&#27861;&#65292;&#19982;ODE&#26041;&#27861;&#19981;&#21516;&#65292;PAL&#26159;&#20174;&#26377;&#38480;&#20154;&#21475;&#25968;&#37327;&#30340;&#38543;&#26426;&#20998;&#21306;&#27169;&#22411;&#30340;&#36817;&#20284;&#28388;&#27874;&#26041;&#31243;&#23548;&#20986;&#30340;&#65292;&#24182;&#19988;&#22823;&#20154;&#21475;&#25968;&#37327;&#38480;&#21046;&#25512;&#21160;&#26368;&#22823;PAL&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#27969;&#34892;&#30149;&#23398;&#25512;&#29702;&#21521;&#22797;&#26434;&#21644;&#24322;&#36136;&#24615;&#27169;&#22411;&#30340;&#25193;&#23637;&#38590;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27850;&#26494;&#36817;&#20284;&#20284;&#28982;(PAL)&#26041;&#27861;&#12290;&#19982;&#37319;&#29992;&#22823;&#20154;&#21475;&#25968;&#37327;&#38480;&#21046;&#26469;&#28608;&#21169;&#30830;&#23450;&#24615;&#27169;&#22411;&#30340;ODE&#26041;&#27861;&#19981;&#21516;&#65292;PAL&#26159;&#20174;&#26377;&#38480;&#20154;&#21475;&#25968;&#37327;&#30340;&#38543;&#26426;&#20998;&#21306;&#27169;&#22411;&#30340;&#36817;&#20284;&#28388;&#27874;&#26041;&#31243;&#23548;&#20986;&#30340;&#65292;&#24182;&#19988;&#22823;&#20154;&#21475;&#25968;&#37327;&#38480;&#21046;&#25512;&#21160;&#26368;&#22823;PAL&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#20284;&#20046;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#24191;&#27867;&#31867;&#21035;&#30340;&#37096;&#20998;&#35266;&#23519;&#38543;&#26426;&#20998;&#21306;&#27169;&#22411;&#21644;&#35299;&#20915;&#22823;&#20154;&#21475;&#25968;&#37327;&#23616;&#38480;&#24615;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#21442;&#25968;&#20272;&#35745;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;PALs&#23454;&#29616;&#31616;&#21333;&#65292;&#20165;&#28041;&#21450;&#22522;&#26412;&#31639;&#26415;&#25805;&#20316;&#21644;&#27809;&#26377;&#35843;&#25972;&#21442;&#25968;&#65292;&#19988;&#35780;&#20272;&#36895;&#24230;&#24555;&#65292;&#19981;&#38656;&#35201;&#20174;&#27169;&#22411;&#36827;&#34892;&#27169;&#25311;&#65292;&#35745;&#31639;&#25104;&#26412;&#19982;&#20154;&#21475;&#35268;&#27169;&#26080;&#20851;&#12290;&#36890;&#36807;&#31034;&#20363;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;PALs
&lt;/p&gt;
&lt;p&gt;
Addressing the challenge of scaling-up epidemiological inference to complex and heterogeneous models, we introduce Poisson Approximate Likelihood (PAL) methods. In contrast to the popular ODE approach to compartmental modelling, in which a large population limit is used to motivate a deterministic model, PALs are derived from approximate filtering equations for finite-population, stochastic compartmental models, and the large population limit drives consistency of maximum PAL estimators. Our theoretical results appear to be the first likelihood-based parameter estimation consistency results which apply to a broad class of partially observed stochastic compartmental models and address the large population limit. PALs are simple to implement, involving only elementary arithmetic operations and no tuning parameters, and fast to evaluate, requiring no simulation from the model and having computational cost independent of population size. Through examples we demonstrate how PALs can be used
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#38382;&#39064;&#35299;&#20915;&#20013;&#65292;&#35838;&#31243;&#39034;&#24207;&#21644;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#23545;&#20154;&#31867;&#29702;&#35299;&#21147;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2205.10250</link><description>&lt;p&gt;
&#39034;&#24207;&#20154;&#31867;&#25945;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explanatory machine learning for sequential human teaching. (arXiv:2205.10250v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#38382;&#39064;&#35299;&#20915;&#20013;&#65292;&#35838;&#31243;&#39034;&#24207;&#21644;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#23545;&#20154;&#31867;&#29702;&#35299;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#20351;&#29992;&#36923;&#36753;&#32534;&#31243;&#25216;&#26415;&#22522;&#20110;&#32553;&#30053;&#25512;&#29702;&#21644;&#24402;&#32435;&#25512;&#29702;&#65292;&#20174;&#23569;&#37327;&#30340;&#25968;&#25454;&#20013;&#24471;&#20986;&#36923;&#36753;&#29702;&#35770;&#12290;&#23398;&#24471;&#30340;&#29702;&#35770;&#20197;&#35268;&#21017;&#24418;&#24335;&#34920;&#31034;&#65292;&#26159;&#25152;&#33719;&#30693;&#35782;&#30340;&#22768;&#26126;&#24615;&#25551;&#36848;&#12290;&#22312;&#26089;&#26399;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#39318;&#27425;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#36923;&#36753;&#35268;&#21017;&#30340;&#31616;&#21333;&#20998;&#31867;&#20219;&#21153;&#23545;&#20154;&#31867;&#29702;&#35299;&#33021;&#21147;&#26377;&#21487;&#34913;&#37327;&#30340;&#25552;&#39640;&#30340;&#35777;&#25454;&#12290;&#22312;&#21518;&#32493;&#30340;&#30740;&#31350;&#20013;&#65292;&#21457;&#29616;&#21521;&#20154;&#31867;&#23637;&#31034;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322; &#22312;&#28216;&#25103;&#23398;&#20064;&#29615;&#22659;&#20013;&#20135;&#29983;&#20102;&#26377;&#30410;&#21644;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#27010;&#24565;&#21576;&#29616;&#30340;&#39034;&#24207;&#23545;&#20154;&#31867;&#29702;&#35299;&#21147;&#30340;&#24433;&#21709;&#26469;&#32487;&#32493;&#25506;&#35752;&#21487;&#29702;&#35299;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35838;&#31243;&#39034;&#24207;&#21644;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#23545;&#20110;&#39034;&#24207;&#38382;&#39064;&#35299;&#20915;&#30340;&#35299;&#37322;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The topic of comprehensibility of machine-learned theories has recently drawn increasing attention. Inductive Logic Programming (ILP) uses logic programming to derive logic theories from small data based on abduction and induction techniques. Learned theories are represented in the form of rules as declarative descriptions of obtained knowledge. In earlier work, the authors provided the first evidence of a measurable increase in human comprehension based on machine-learned logic rules for simple classification tasks. In a later study, it was found that the presentation of machine-learned explanations to humans can produce both beneficial and harmful effects in the context of game learning. We continue our investigation of comprehensibility by examining the effects of the ordering of concept presentations on human comprehension. In this work, we examine the explanatory effects of curriculum order and the presence of machine-learned explanations for sequential problem-solving. We show th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#35268;&#21017;&#25628;&#32034;&#25216;&#26415;&#65292;&#26089;&#26399;&#35782;&#21035;&#26377;&#21487;&#33021;&#25104;&#20026;&#38271;&#26399;&#25110;&#24930;&#24615;&#26080;&#23478;&#21487;&#24402;&#32773;&#25910;&#23481;&#25152;&#30340;&#39640;&#39118;&#38505;&#20154;&#32676;&#12290;&#22312;&#23454;&#26102;&#20132;&#20184;&#25903;&#25345;&#24615;&#20303;&#25151;&#35745;&#21010;&#30340;&#26694;&#26550;&#20869;&#65292;&#24212;&#29992;&#26412;&#25991;&#26041;&#27861;&#20351;&#24471;&#26089;&#26399;&#35782;&#21035;&#22788;&#20110;&#24930;&#24615;&#26080;&#23478;&#21487;&#24402;&#32773;&#39118;&#38505;&#30340;&#23458;&#25143;&#30340;&#20013;&#20301;&#26102;&#38388;&#20174;297&#22825;&#38477;&#33267;162&#22825;&#12290;</title><link>http://arxiv.org/abs/2205.09883</link><description>&lt;p&gt;
&#19968;&#20010;&#35268;&#21017;&#25628;&#32034;&#26694;&#26550;&#29992;&#20110;&#26089;&#26399;&#35782;&#21035;&#24930;&#24615;&#32039;&#24613;&#26080;&#23478;&#21487;&#24402;&#32773;
&lt;/p&gt;
&lt;p&gt;
A Rule Search Framework for the Early Identification of Chronic Emergency Homeless Shelter Clients. (arXiv:2205.09883v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#35268;&#21017;&#25628;&#32034;&#25216;&#26415;&#65292;&#26089;&#26399;&#35782;&#21035;&#26377;&#21487;&#33021;&#25104;&#20026;&#38271;&#26399;&#25110;&#24930;&#24615;&#26080;&#23478;&#21487;&#24402;&#32773;&#25910;&#23481;&#25152;&#30340;&#39640;&#39118;&#38505;&#20154;&#32676;&#12290;&#22312;&#23454;&#26102;&#20132;&#20184;&#25903;&#25345;&#24615;&#20303;&#25151;&#35745;&#21010;&#30340;&#26694;&#26550;&#20869;&#65292;&#24212;&#29992;&#26412;&#25991;&#26041;&#27861;&#20351;&#24471;&#26089;&#26399;&#35782;&#21035;&#22788;&#20110;&#24930;&#24615;&#26080;&#23478;&#21487;&#24402;&#32773;&#39118;&#38505;&#30340;&#23458;&#25143;&#30340;&#20013;&#20301;&#26102;&#38388;&#20174;297&#22825;&#38477;&#33267;162&#22825;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#35268;&#21017;&#25628;&#32034;&#25216;&#26415;&#65292;&#26089;&#26399;&#35782;&#21035;&#37027;&#20123;&#26377;&#21487;&#33021;&#25104;&#20026;&#38271;&#26399;&#25110;&#24930;&#24615;&#20303;&#22312;&#26080;&#23478;&#21487;&#24402;&#32773;&#25910;&#23481;&#25152;&#30340;&#39640;&#39118;&#38505;&#20154;&#32676;&#12290;&#20351;&#29992;&#19968;&#23478;&#21271;&#32654;&#20027;&#35201;&#25910;&#23481;&#25152;&#26381;&#21153;&#19982;&#36229;&#36807;40,000&#20154;&#30340;12&#24180;&#20132;&#20114;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#20248;&#21270;&#20462;&#21098;&#26080;&#24207;&#25628;&#32034;(OPUS)&#31639;&#27861;&#65292;&#24320;&#21457;&#30452;&#35266;&#32780;&#26377;&#25928;&#30340;&#35268;&#21017;&#12290;&#22312;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#20132;&#20184;&#25903;&#25345;&#24615;&#20303;&#25151;&#35745;&#21010;&#30340;&#26694;&#26550;&#20869;&#65292;&#23545;&#36825;&#20123;&#35268;&#21017;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24212;&#29992;&#26412;&#25991;&#26041;&#27861;&#20351;&#24471;&#26089;&#26399;&#35782;&#21035;&#22788;&#20110;&#24930;&#24615;&#26080;&#23478;&#21487;&#24402;&#32773;&#39118;&#38505;&#30340;&#23458;&#25143;&#30340;&#20013;&#20301;&#26102;&#38388;&#20174;297&#22825;&#38477;&#33267;162&#22825;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper uses rule search techniques for the early identification of emergency homeless shelter clients who are at risk of becoming long term or chronic shelter users. Using a data set from a major North American shelter containing 12 years of service interactions with over 40,000 individuals, the optimized pruning for unordered search (OPUS) algorithm is used to develop rules that are both intuitive and effective. The rules are evaluated within a framework compatible with the real-time delivery of a housing program meant to transition high risk clients to supportive housing. Results demonstrate that the median time to identification of clients at risk of chronic shelter use drops from 297 days to 162 days when the methods in this paper are applied.
&lt;/p&gt;</description></item><item><title>FedGiA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#28151;&#21512;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#33410;&#30465;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2205.01438</link><description>&lt;p&gt;
FedGiA: &#19968;&#31181;&#39640;&#25928;&#30340;&#28151;&#21512;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedGiA: An Efficient Hybrid Algorithm for Federated Learning. (arXiv:2205.01438v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01438
&lt;/p&gt;
&lt;p&gt;
FedGiA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#28151;&#21512;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#33410;&#30465;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36817;&#26469;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20173;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#31639;&#27861;&#22914;&#20309;&#33410;&#30465;&#36890;&#20449;&#36164;&#28304;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#21450;&#23427;&#20204;&#26159;&#21542;&#20250;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#65288;FedGiA&#65289;&#65292;&#23558;&#26799;&#24230;&#19979;&#38477;&#21644;&#19981;&#31934;&#30830;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#21644;&#25968;&#20540;&#19978;&#27604;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#26356;&#20855;&#36890;&#20449;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#23427;&#36824;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has shown its advances recently but is still facing many challenges, such as how algorithms save communication resources and reduce computational costs, and whether they converge. To address these critical issues, we propose a hybrid federated learning algorithm (FedGiA) that combines the gradient descent and the inexact alternating direction method of multipliers. The proposed algorithm is more communication- and computation-efficient than several state-of-the-art algorithms theoretically and numerically. Moreover, it also converges globally under mild conditions.
&lt;/p&gt;</description></item><item><title>CLIP-Dissect&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#21363;&#23558;&#20869;&#37096;&#31070;&#32463;&#20803;&#26631;&#35760;&#20026;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#30340;&#24320;&#25918;&#27010;&#24565;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.10965</link><description>&lt;p&gt;
CLIP-Dissect&#65306;&#28145;&#24230;&#35270;&#35273;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#34920;&#31034;&#30340;&#33258;&#21160;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks. (arXiv:2204.10965v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10965
&lt;/p&gt;
&lt;p&gt;
CLIP-Dissect&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#21363;&#23558;&#20869;&#37096;&#31070;&#32463;&#20803;&#26631;&#35760;&#20026;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#30340;&#24320;&#25918;&#27010;&#24565;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;CLIP-Dissect&#65292;&#21487;&#20197;&#33258;&#21160;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#21333;&#20010;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;CLIP-Dissect&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#22810;&#27169;&#24577;&#35270;&#35273;/&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#23558;&#20869;&#37096;&#31070;&#32463;&#20803;&#26631;&#35760;&#20026;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#20154;&#31867;&#31034;&#20363;&#30340;&#24320;&#25918;&#27010;&#24565;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CLIP-Dissect&#25552;&#20379;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#30340;&#25551;&#36848;&#65292;&#20854;&#20013;&#21253;&#25324;&#20855;&#22791;&#8220;&#22320;&#38754;&#30495;&#30456;&#8221;&#65288;ground-truth&#65289;&#30340;&#26368;&#21518;&#19968;&#23618;&#31070;&#32463;&#20803;&#20197;&#21450;&#20855;&#22791;&#23450;&#24615;&#22909;&#30340;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#38750;&#24120;&#28789;&#27963;&#65306;&#23427;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#21487;&#20197;&#36731;&#26494;&#22788;&#29702;&#26032;&#27010;&#24565;&#65292;&#21487;&#20197;&#25193;&#23637;&#20197;&#21033;&#29992;&#26410;&#26469;&#26356;&#22909;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;CLIP-Dissect&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21487;&#20197;&#22312;&#30701;&#30701;4&#20998;&#38047;&#20869;&#26631;&#35760;ResNet-50&#30340;&#20116;&#23618;&#25152;&#26377;&#31070;&#32463;&#20803;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#24555;10&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/Trustworthy-ML-Lab/CLIP-dissect &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose CLIP-Dissect, a new technique to automatically describe the function of individual hidden neurons inside vision networks. CLIP-Dissect leverages recent advances in multimodal vision/language models to label internal neurons with open-ended concepts without the need for any labeled data or human examples. We show that CLIP-Dissect provides more accurate descriptions than existing methods for last layer neurons where the ground-truth is available as well as qualitatively good descriptions for hidden layer neurons. In addition, our method is very flexible: it is model agnostic, can easily handle new concepts and can be extended to take advantage of better multimodal models in the future. Finally CLIP-Dissect is computationally efficient and can label all neurons from five layers of ResNet-50 in just 4 minutes, which is more than 10 times faster than existing methods. Our code is available at https://github.com/Trustworthy-ML-Lab/CLIP-dissect.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CgAT&#26041;&#27861;&#65292;&#22522;&#20110;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;&#28145;&#24230;Hashing&#32593;&#32476;&#26816;&#32034;&#30340;&#40065;&#26834;&#24615;&#65292;&#21462;&#24471;&#20102;&#39046;&#20808;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.10779</link><description>&lt;p&gt;
CgAT&#65306;&#22522;&#20110;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#25552;&#21319;Hashing&#26816;&#32034;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
CgAT: Center-Guided Adversarial Training for Deep Hashing-Based Retrieval. (arXiv:2204.10779v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CgAT&#26041;&#27861;&#65292;&#22522;&#20110;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;&#28145;&#24230;Hashing&#32593;&#32476;&#26816;&#32034;&#30340;&#40065;&#26834;&#24615;&#65292;&#21462;&#24471;&#20102;&#39046;&#20808;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;Hashing&#22312;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;min-max&#30340;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65288;CgAT&#65289;&#65292;&#36890;&#36807;&#26368;&#22351;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#25552;&#39640;&#28145;&#24230;Hashing&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#28145;&#24230;Hashing&#26816;&#32034;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#23545;&#25239;&#24615;&#38450;&#24481;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep hashing has been extensively utilized in massive image retrieval because of its efficiency and effectiveness. However, deep hashing models are vulnerable to adversarial examples, making it essential to develop adversarial defense methods for image retrieval. Existing solutions achieved limited defense performance because of using weak adversarial samples for training and lacking discriminative optimization objectives to learn robust features. In this paper, we present a min-max based Center-guided Adversarial Training, namely CgAT, to improve the robustness of deep hashing networks through worst adversarial examples. Specifically, we first formulate the center code as a semantically-discriminative representative of the input image content, which preserves the semantic similarity with positive samples and dissimilarity with negative examples. We prove that a mathematical formula can calculate the center code immediately. After obtaining the center codes in each optimization iterati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22270;&#32858;&#31867;&#31639;&#27861;CGC&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#32467;&#21512;&#36319;&#36394;&#27169;&#22359;&#20197;&#24212;&#23545;&#21160;&#24577;&#22270;&#25299;&#25169;&#21464;&#21270;&#65292;&#22312;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;&#26041;&#38754;&#34920;&#29616;&#20986;&#39046;&#20808;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2204.08504</link><description>&lt;p&gt;
CGC: &#23545;&#27604;&#22270;&#32858;&#31867;&#29992;&#20110;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
CGC: Contrastive Graph Clustering for Community Detection and Tracking. (arXiv:2204.08504v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22270;&#32858;&#31867;&#31639;&#27861;CGC&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#32467;&#21512;&#36319;&#36394;&#27169;&#22359;&#20197;&#24212;&#23545;&#21160;&#24577;&#22270;&#25299;&#25169;&#21464;&#21270;&#65292;&#22312;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;&#26041;&#38754;&#34920;&#29616;&#20986;&#39046;&#20808;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22270;&#32858;&#31867;&#30340;&#35282;&#24230;&#20837;&#25163;&#65292;&#25506;&#35752;&#22312;&#32593;&#32476;&#25968;&#25454;&#20013;&#21457;&#29616;&#23454;&#20307;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#20197;&#21450;&#23545;&#23427;&#20204;&#36827;&#34892;&#31038;&#21306;&#36319;&#36394;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;CGC&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#37319;&#29992;&#20102;&#36319;&#36394;&#27169;&#22359;&#20197;&#24212;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#22270;&#24418;&#25299;&#25169;&#12290;&#22312;&#21508;&#20010;&#30495;&#23454;&#22330;&#26223;&#21644;&#21512;&#25104;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#23545;CGC&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#21160;&#24577;&#22270;&#19978;&#34920;&#29616;&#20986;&#20102;&#39046;&#20808;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given entities and their interactions in the web data, which may have occurred at different time, how can we find communities of entities and track their evolution? In this paper, we approach this important task from graph clustering perspective. Recently, state-of-the-art clustering performance in various domains has been achieved by deep clustering methods. Especially, deep graph clustering (DGC) methods have successfully extended deep clustering to graph-structured data by learning node representations and cluster assignments in a joint optimization framework. Despite some differences in modeling choices (e.g., encoder architectures), existing DGC methods are mainly based on autoencoders and use the same clustering objective with relatively minor adaptations. Also, while many real-world graphs are dynamic, previous DGC methods considered only static graphs. In this work, we develop CGC, a novel end-to-end framework for graph clustering, which fundamentally differs from existing meth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24863;&#30693;&#24490;&#29615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(PARC)&#65292;&#21487;&#20174;&#25968;&#20540;&#27169;&#25311;&#20013;&#23398;&#20064;&#20171;&#35266;&#23610;&#24230;&#19979;&#30340;&#28909;&#21147;&#23398;&#21644;&#21147;&#23398;&#21709;&#24212;&#65292;&#39044;&#27979;&#21463;&#20914;&#20987;&#30340;&#33021;&#37327;&#26448;&#26009;&#30340;&#28909;&#21147;&#23398;&#21453;&#24212;&#65292;&#20855;&#26377;&#20934;&#30830;&#24615;&#39640;&#65292;&#35745;&#31639;&#26102;&#38388;&#30701;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2204.07234</link><description>&lt;p&gt;
PARC: &#29289;&#29702;&#24863;&#30693;&#24490;&#29615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21516;&#21270;&#20013;&#23610;&#24230;&#21453;&#24212;&#24615;&#26448;&#26009;&#30340;&#20171;&#35266;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
PARC: Physics-Aware Recurrent Convolutional Neural Networks to Assimilate Meso-scale Reactive Mechanics of Energetic Materials. (arXiv:2204.07234v3 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07234
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24863;&#30693;&#24490;&#29615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(PARC)&#65292;&#21487;&#20174;&#25968;&#20540;&#27169;&#25311;&#20013;&#23398;&#20064;&#20171;&#35266;&#23610;&#24230;&#19979;&#30340;&#28909;&#21147;&#23398;&#21644;&#21147;&#23398;&#21709;&#24212;&#65292;&#39044;&#27979;&#21463;&#20914;&#20987;&#30340;&#33021;&#37327;&#26448;&#26009;&#30340;&#28909;&#21147;&#23398;&#21453;&#24212;&#65292;&#20855;&#26377;&#20934;&#30830;&#24615;&#39640;&#65292;&#35745;&#31639;&#26102;&#38388;&#30701;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20914;&#20987;&#21551;&#21160;&#30340;&#33021;&#37327;&#26448;&#26009;&#65288;EM&#65289;&#30340;&#28909;&#21147;&#23398;&#21453;&#24212;&#21463;&#20854;&#24494;&#35266;&#32467;&#26500;&#30340;&#39640;&#24230;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#22312;&#8220;&#26448;&#26009;&#35774;&#35745;&#8221;&#26694;&#26550;&#20013;&#35774;&#35745;EM&#24494;&#35266;&#32467;&#26500;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35774;&#35745;&#23454;&#36341;&#21463;&#38480;&#65292;&#38656;&#35201;&#22823;&#37327;&#27169;&#25311;&#26469;&#26500;&#24314;&#22797;&#26434;&#30340;EM&#32467;&#26500; - &#24615;&#33021; - &#24615;&#33021;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29289;&#29702;&#24863;&#30693;&#24490;&#29615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PARC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#39640;&#20998;&#36776;&#29575;&#30340;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#65288;DNS&#65289;&#20013;&#23398;&#20064;EM&#30340;&#20171;&#35266;&#28909;&#21147;&#23398;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;PARC&#21487;&#20197;&#39044;&#27979;&#21463;&#20914;&#20987;&#30340;EM&#30340;&#28909;&#21147;&#23398;&#21453;&#24212;&#65292;&#20854;&#20934;&#30830;&#24615;&#19982;DNS&#30456;&#24403;&#65292;&#20294;&#35745;&#31639;&#26102;&#38388;&#26126;&#26174;&#26356;&#30701;&#12290;PARC&#30340;&#29289;&#29702;&#24863;&#30693;&#24615;&#22686;&#24378;&#20102;&#20854;&#24314;&#27169;&#33021;&#21147;&#21644;&#36890;&#29992;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#26410;&#35265;&#39044;&#27979;&#26041;&#26696;&#26102;&#21463;&#21040;&#25361;&#25112;&#26102;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;P
&lt;/p&gt;
&lt;p&gt;
The thermo-mechanical response of shock-initiated energetic materials (EM) is highly influenced by their microstructures, presenting an opportunity to engineer EM microstructure in a "materials-by-design" framework. However, the current design practice is limited, as a large ensemble of simulations is required to construct the complex EM structure-property-performance linkages. We present the Physics-Aware Recurrent Convolutional (PARC) Neural Network, a deep-learning algorithm capable of learning the mesoscale thermo-mechanics of EM from a modest number of high-resolution direct numerical simulations (DNS). Validation results demonstrated that PARC could predict the themo-mechanical response of shocked EM with a comparable accuracy to DNS but with notably less computation time. The physics awareness of PARC enhances its modeling capabilities and generalizability, especially when challenged in unseen prediction scenarios. We also demonstrate that visualizing the artificial neurons at P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#20102;&#22522;&#20110;&#29289;&#29702;/&#27169;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;LDCT&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#23558;&#29289;&#29702;/&#27169;&#22411;&#19982;&#28145;&#24230;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#30340;&#26032;&#20852;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2203.15725</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;/&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#20302;&#21058;&#37327;&#35745;&#31639;&#26426;&#26029;&#23618;&#25668;&#24433;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Physics-/Model-Based and Data-Driven Methods for Low-Dose Computed Tomography: A survey. (arXiv:2203.15725v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#20102;&#22522;&#20110;&#29289;&#29702;/&#27169;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;LDCT&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#23558;&#29289;&#29702;/&#27169;&#22411;&#19982;&#28145;&#24230;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#30340;&#26032;&#20852;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2016&#24180;&#20197;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20302;&#21058;&#37327;&#35745;&#31639;&#26426;&#26029;&#23618;&#25668;&#24433;&#65288;LDCT&#65289;&#25104;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#21463;&#21040;&#22823;&#25968;&#25454;&#30340;&#25512;&#21160;&#65292;LDCT&#21435;&#22122;&#21644;&#32431;&#31471;&#21040;&#31471;&#37325;&#24314;&#32593;&#32476;&#24120;&#24120;&#36973;&#21463;&#40657;&#21283;&#23376;&#30340;&#24615;&#36136;&#21644;&#31283;&#23450;&#24615;&#31561;&#37325;&#22823;&#38382;&#39064;&#65292;&#36825;&#26159;&#23558;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#20302;&#21058;&#37327;CT&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290; &#19968;&#31181;&#26032;&#20852;&#30340;&#36235;&#21183;&#26159;&#23558;&#25104;&#20687;&#29289;&#29702;&#21644;&#27169;&#22411;&#19982;&#28145;&#24230;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#29289;&#29702;/&#27169;&#22411;&#39537;&#21160;&#21644;&#25968;&#25454;&#39537;&#21160;&#20803;&#32032;&#30340;&#28151;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#22522;&#20110;&#29289;&#29702;/&#27169;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;LDCT&#20013;&#30340;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#38382;&#39064;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since 2016, deep learning (DL) has advanced tomographic imaging with remarkable successes, especially in low-dose computed tomography (LDCT) imaging. Despite being driven by big data, the LDCT denoising and pure end-to-end reconstruction networks often suffer from the black box nature and major issues such as instabilities, which is a major barrier to apply deep learning methods in low-dose CT applications. An emerging trend is to integrate imaging physics and model into deep networks, enabling a hybridization of physics/model-based and data-driven elements. %This type of hybrid methods has become increasingly influential. In this paper, we systematically review the physics/model-based data-driven methods for LDCT, summarize the loss functions and training strategies, evaluate the performance of different methods, and discuss relevant issues and future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;GAN&#30340;&#26368;&#26032;&#26550;&#26500;&#12289;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#12289;&#39564;&#35777;&#25351;&#26631;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;GAN&#30340;&#29616;&#29366;&#12290;</title><link>http://arxiv.org/abs/2203.11242</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;GAN&#32508;&#36848;&#65306;&#26368;&#26032;&#30740;&#31350;&#12289;&#20998;&#26512;&#21644;&#20998;&#31867;&#65288;arXiv&#65306;2203.11242v2 [cs.LG] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
A survey on GANs for computer vision: Recent research, analysis and taxonomy. (arXiv:2203.11242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;GAN&#30340;&#26368;&#26032;&#26550;&#26500;&#12289;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#12289;&#39564;&#35777;&#25351;&#26631;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;GAN&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24050;&#32463;&#36827;&#34892;&#20102;&#20960;&#27425;&#38761;&#21629;&#65292;&#20854;&#20013;&#26368;&#21463;&#20851;&#27880;&#30340;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#24040;&#22823;&#24433;&#21709;&#12290;GAN&#19981;&#20165;&#22312;&#23450;&#20041;&#20854;&#27169;&#22411;&#26102;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#26550;&#26500;&#65292;&#32780;&#19988;&#29983;&#25104;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#65292;&#23545;&#31038;&#20250;&#20135;&#29983;&#20102;&#30452;&#25509;&#24433;&#21709;&#12290;&#30001;&#20110;GAN&#24102;&#26469;&#30340;&#37325;&#22823;&#25913;&#36827;&#21644;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#31038;&#21306;&#19981;&#26029;&#25552;&#20986;&#26032;&#30340;&#30740;&#31350;&#65292;&#20351;&#24471;&#36319;&#19978;&#26102;&#20195;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;GAN&#30340;&#27010;&#36848;&#65292;&#23637;&#31034;&#26368;&#26032;&#30340;&#26550;&#26500;&#12289;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21270;&#12289;&#39564;&#35777;&#25351;&#26631;&#21644;&#26368;&#24191;&#27867;&#35748;&#21487;&#30340;&#21464;&#20307;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#23558;&#35780;&#20272;&#19981;&#21516;&#21464;&#20307;&#30340;&#27169;&#22411;&#26550;&#26500;&#25928;&#29575;&#65292;&#23637;&#31034;&#26368;&#20339;&#30340;&#24212;&#29992;&#39046;&#22495;&#65307;&#20316;&#20026;&#35813;&#36807;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23558;&#20998;&#26512;&#35780;&#20272;GAN&#24615;&#33021;&#30340;&#19981;&#21516;&#24230;&#37327;&#26631;&#20934;&#21644;&#32463;&#24120;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#23558;&#25552;&#20986;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;GAN&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, there have been several revolutions in the field of deep learning, mainly headlined by the large impact of Generative Adversarial Networks (GANs). GANs not only provide an unique architecture when defining their models, but also generate incredible results which have had a direct impact on society. Due to the significant improvements and new areas of research that GANs have brought, the community is constantly coming up with new researches that make it almost impossible to keep up with the times. Our survey aims to provide a general overview of GANs, showing the latest architectures, optimizations of the loss functions, validation metrics and application areas of the most widely recognized variants. The efficiency of the different variants of the model architecture will be evaluated, as well as showing the best application area; as a vital part of the process, the different metrics for evaluating the performance of GANs and the frequently used loss functions will
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;&#37325;&#26500;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#22806;&#26679;&#26412;&#26816;&#27979;&#65292;&#22312;&#26368;&#22823;&#21387;&#32553;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#21644;&#20445;&#35777;&#37325;&#26500;&#33021;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#37325;&#26500;&#12289;&#25968;&#25454;&#30830;&#23450;&#24615;&#20998;&#35299;&#21644;&#26631;&#20934;&#21270;L2&#36317;&#31163;&#31561;&#31574;&#30053;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#35760;&#22806;&#26679;&#26412;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2203.02194</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#37325;&#26500;&#33258;&#32534;&#30721;&#22120;&#30340;&#22806;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection. (arXiv:2203.02194v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;&#37325;&#26500;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#22806;&#26679;&#26412;&#26816;&#27979;&#65292;&#22312;&#26368;&#22823;&#21387;&#32553;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#21644;&#20445;&#35777;&#37325;&#26500;&#33021;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#37325;&#26500;&#12289;&#25968;&#25454;&#30830;&#23450;&#24615;&#20998;&#35299;&#21644;&#26631;&#20934;&#21270;L2&#36317;&#31163;&#31561;&#31574;&#30053;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#35760;&#22806;&#26679;&#26412;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20998;&#31867;&#22120;&#38656;&#35201;&#26816;&#27979;&#36828;&#31163;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#22806;&#26679;&#26412;&#12290;&#37325;&#26500;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#21033;&#29992;&#36755;&#20837;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#26032;&#39062;&#24615;&#19982;&#27491;&#24120;&#24615;&#30340;&#24230;&#37327;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#30340;&#26412;&#36136;&#34920;&#36848;&#20026;&#20855;&#26377;&#23545;&#26465;&#20214;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#30340;&#20195;&#29702;&#26597;&#35810;&#30340;&#22235;&#20803;&#32452;&#22495;&#36716;&#25442;&#65292;&#20854;&#26377;&#20869;&#22312;&#20559;&#35265;&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#25913;&#36827;&#26041;&#21521;&#34987;&#24418;&#24335;&#21270;&#20026;&#26368;&#22823;&#21387;&#32553;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#65292;&#21516;&#26102;&#30830;&#20445;&#20854;&#37325;&#26500;&#33021;&#21147;&#65292;&#20197;&#20805;&#24403;&#25152;&#25551;&#36848;&#30340;&#22495;&#36716;&#25442;&#22120;&#12290;&#20174;&#20013;&#65292;&#24341;&#20837;&#20102;&#31574;&#30053;&#65292;&#21253;&#25324;&#35821;&#20041;&#37325;&#26500;&#12289;&#25968;&#25454;&#30830;&#23450;&#24615;&#20998;&#35299;&#21644;&#26631;&#20934;&#21270;L2&#36317;&#31163;&#65292;&#20197;&#23454;&#36136;&#24615;&#25913;&#21892;&#21407;&#22987;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20849;&#21516;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#24314;&#31435;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#65292;&#22312;Wide-ResNet&#19978;&#65292;CIFAR-100&#19982;TinyImagenet-crop&#30340;FPR@95%TPR&#20026;0.2%&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#26631;&#35760;&#22806;&#26679;&#26412;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In some scenarios, classifier requires detecting out-of-distribution samples far from its training data. With desirable characteristics, reconstruction autoencoder-based methods deal with this problem by using input reconstruction error as a metric of novelty vs. normality. We formulate the essence of such approach as a quadruplet domain translation with an intrinsic bias to only query for a proxy of conditional data uncertainty. Accordingly, an improvement direction is formalized as maximumly compressing the autoencoder's latent space while ensuring its reconstructive power for acting as a described domain translator. From it, strategies are introduced including semantic reconstruction, data certainty decomposition and normalized L2 distance to substantially improve original methods, which together establish state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method works without any addit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#30740;&#31350;&#20102;&#30001;&#20855;&#26377;ReLU&#28608;&#27963;&#21333;&#20803;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2203.00246</link><description>&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#30340;&#20449;&#24687;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Information-Theoretic Framework for Supervised Learning. (arXiv:2203.00246v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#30740;&#31350;&#20102;&#30001;&#20855;&#26377;ReLU&#28608;&#27963;&#21333;&#20803;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#24180;&#65292;&#28145;&#24230;&#23398;&#20064;&#23637;&#31034;&#20986;&#26356;&#21152;&#26032;&#39062;&#21644;&#20248;&#31168;&#30340;&#32463;&#39564;&#32467;&#26524;&#65292;&#20854;&#20013;&#37319;&#29992;&#26356;&#28145;&#21644;&#26356;&#24191;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#36229;&#36807;&#20004;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#22256;&#38590;&#30340;&#65292;&#38500;&#38750;&#35785;&#35832;&#20110;&#35745;&#25968;&#21442;&#25968;&#25110;&#36973;&#36935;&#28145;&#24230;&#25351;&#25968;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#12290;&#22240;&#27492;&#65292;&#22312;&#19981;&#21516;&#30340;&#35282;&#24230;&#19979;&#20998;&#26512;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#26159;&#26377;&#25104;&#26524;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#20855;&#26377;&#33258;&#24049;&#30340;&#36951;&#25022;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#27010;&#24565;&#65292;&#29992;&#20110;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#38656;&#27714;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19968;&#20123;&#32463;&#20856;&#26696;&#20363;&#65292;&#20363;&#22914;&#26631;&#37327;&#20272;&#35745;&#21644;&#32447;&#24615;&#22238;&#24402;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#30452;&#35273;&#21644;&#20171;&#32461;&#19968;&#33324;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#35813;&#26694;&#26550;&#30740;&#31350;&#20102;&#30001;&#20855;&#26377;ReLU&#28608;&#27963;&#21333;&#20803;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23545;&#20110;&#26435;&#37325;&#30340;&#29305;&#23450;&#20808;&#39564;&#20998;&#24067;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Each year, deep learning demonstrates new and improved empirical results with deeper and wider neural networks. Meanwhile, with existing theoretical frameworks, it is difficult to analyze networks deeper than two layers without resorting to counting parameters or encountering sample complexity bounds that are exponential in depth. Perhaps it may be fruitful to try to analyze modern machine learning under a different lens. In this paper, we propose a novel information-theoretic framework with its own notions of regret and sample complexity for analyzing the data requirements of machine learning. With our framework, we first work through some classical examples such as scalar estimation and linear regression to build intuition and introduce general techniques. Then, we use the framework to study the sample complexity of learning from data generated by deep neural networks with ReLU activation units. For a particular prior distribution on weights, we establish sample complexity bounds tha
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38543;&#26426;&#22122;&#22768;&#30340;&#22312;&#32447;&#24191;&#20041;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#65292;&#33719;&#24471;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#38480;&#12290;&#21516;&#26102;&#65292;&#38024;&#23545;&#20855;&#26377;&#24322;&#26041;&#24046;&#22122;&#22768;&#30340;&#24191;&#20041;&#32447;&#24615;Bandits&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FTRL&#30340;&#31639;&#27861;&#23454;&#29616;&#31532;&#19968;&#20010;&#26041;&#24046;&#24863;&#30693;&#30340;&#36951;&#25022;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2202.13603</link><description>&lt;p&gt;
&#20855;&#26377;&#38543;&#26426;&#22122;&#22768;&#30340;&#22312;&#32447;&#24191;&#20041;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#35299;&#21450;&#20854;&#22312;&#24322;&#26041;&#24046;Bandits&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Optimal Online Generalized Linear Regression with Stochastic Noise and Its Application to Heteroscedastic Bandits. (arXiv:2202.13603v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13603
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38543;&#26426;&#22122;&#22768;&#30340;&#22312;&#32447;&#24191;&#20041;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#65292;&#33719;&#24471;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#38480;&#12290;&#21516;&#26102;&#65292;&#38024;&#23545;&#20855;&#26377;&#24322;&#26041;&#24046;&#22122;&#22768;&#30340;&#24191;&#20041;&#32447;&#24615;Bandits&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FTRL&#30340;&#31639;&#27861;&#23454;&#29616;&#31532;&#19968;&#20010;&#26041;&#24046;&#24863;&#30693;&#30340;&#36951;&#25022;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#32972;&#26223;&#19979;&#22312;&#32447;&#24191;&#20041;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#26631;&#31614;&#26159;&#30001;&#21487;&#33021;&#20855;&#26377;&#26080;&#30028;&#21152;&#24615;&#22122;&#22768;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#23545;&#32463;&#20856;&#30340;&#36319;&#38543;&#27491;&#21017;&#21270;&#39046;&#34966;&#65288;FTRL&#65289;&#31639;&#27861;&#36827;&#34892;&#20102;&#23574;&#38160;&#30340;&#20998;&#26512;&#65292;&#20197;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;$\sigma$-&#23376;&#39640;&#26031;&#26631;&#31614;&#22122;&#22768;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20010;&#36951;&#25022;&#30340;&#19978;&#38480;$O(\sigma^2 d \log T) + o(\log T)$&#65292;&#20854;&#20013;$d$&#26159;&#36755;&#20837;&#21521;&#37327;&#30340;&#32500;&#25968;, $T$ &#26159;&#24635;&#22238;&#21512;&#25968;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#38543;&#26426;&#22312;&#32447;&#32447;&#24615;&#22238;&#24402;&#30340;$\Omega(\sigma^2d\log(T/d))$&#19979;&#38480;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#19978;&#38480;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#20102;&#26356;&#31934;&#32454;&#30340;&#20271;&#24681;&#26031;&#22374;&#22122;&#22768;&#26465;&#20214;&#12290;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#24322;&#26041;&#24046;&#22122;&#22768;&#30340;&#24191;&#20041;&#32447;&#24615;Bandits&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FTRL&#30340;&#31639;&#27861;&#23454;&#29616;&#31532;&#19968;&#20010;&#26041;&#24046;&#24863;&#30693;&#30340;&#36951;&#25022;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of online generalized linear regression in the stochastic setting, where the label is generated from a generalized linear model with possibly unbounded additive noise. We provide a sharp analysis of the classical follow-the-regularized-leader (FTRL) algorithm to cope with the label noise. More specifically, for $\sigma$-sub-Gaussian label noise, our analysis provides a regret upper bound of $O(\sigma^2 d \log T) + o(\log T)$, where $d$ is the dimension of the input vector, $T$ is the total number of rounds. We also prove a $\Omega(\sigma^2d\log(T/d))$ lower bound for stochastic online linear regression, which indicates that our upper bound is nearly optimal. In addition, we extend our analysis to a more refined Bernstein noise condition. As an application, we study generalized linear bandits with heteroscedastic noise and propose an algorithm based on FTRL to achieve the first variance-aware regret bound.
&lt;/p&gt;</description></item><item><title>PGMax&#26159;&#19968;&#20010;&#29992;&#20110;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#30340;&#22240;&#23376;&#22270;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;JAX&#20013;&#33258;&#21160;&#36816;&#34892;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#65292;&#19982;&#29616;&#26377;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;PGMax&#33719;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#32467;&#26524;&#65292;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#39640;&#36798;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2202.04110</link><description>&lt;p&gt;
PGMax: &#29992;&#20110;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#21644;JAX&#20013;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#30340;&#22240;&#23376;&#22270;
&lt;/p&gt;
&lt;p&gt;
PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX. (arXiv:2202.04110v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04110
&lt;/p&gt;
&lt;p&gt;
PGMax&#26159;&#19968;&#20010;&#29992;&#20110;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#30340;&#22240;&#23376;&#22270;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;JAX&#20013;&#33258;&#21160;&#36816;&#34892;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#65292;&#19982;&#29616;&#26377;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;PGMax&#33719;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#32467;&#26524;&#65292;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#39640;&#36798;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
PGMax is a factor graph tool for discrete probabilistic graphical models that automatically runs efficient and scalable loopy belief propagation in JAX. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups.
&lt;/p&gt;
&lt;p&gt;
PGMax&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#29992;&#20110;&#36731;&#26494;&#25351;&#23450;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#20316;&#20026;&#22240;&#23376;&#22270;&#65292;&#24182;&#22312;JAX&#20013;&#33258;&#21160;&#36816;&#34892;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#65288;LBP&#65289;&#12290;PGMax&#25903;&#25345;&#20855;&#26377;&#21487;&#22788;&#29702;&#22240;&#23376;&#30340;&#19968;&#33324;&#22240;&#23376;&#22270;&#65292;&#24182;&#21033;&#29992;&#29616;&#20195;&#21152;&#36895;&#22120;&#65288;&#22914;GPU&#65289;&#36827;&#34892;&#25512;&#29702;&#12290;&#19982;&#29616;&#26377;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;PGMax&#33719;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#32467;&#26524;&#65292;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#39640;&#36798;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;PGMax&#36824;&#19982;&#24555;&#36895;&#22686;&#38271;&#30340;JAX&#29983;&#24577;&#31995;&#32479;&#26080;&#32541;&#20132;&#20114;&#65292;&#24320;&#21551;&#20102;&#26032;&#30340;&#30740;&#31350;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#12289;&#31034;&#20363;&#21644;&#25991;&#26723;&#21487;&#22312;https://github.com/deepmind/PGMax&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
PGMax is an open-source Python package for (a) easily specifying discrete Probabilistic Graphical Models (PGMs) as factor graphs; and (b) automatically running efficient and scalable loopy belief propagation (LBP) in JAX. PGMax supports general factor graphs with tractable factors, and leverages modern accelerators like GPUs for inference. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups. PGMax additionally interacts seamlessly with the rapidly growing JAX ecosystem, opening up new research possibilities. Our source code, examples and documentation are available at https://github.com/deepmind/PGMax.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepDECS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20316;&#20026;&#20915;&#31574;&#36807;&#31243;&#30340;&#24863;&#30693;&#27493;&#39588;&#30340;&#33258;&#20027;&#31995;&#32479;&#21512;&#25104;&#26500;&#24314;&#27491;&#30830;&#30340;&#31163;&#25955;&#20107;&#20214;&#25511;&#21046;&#22120;&#12290;&#27169;&#22411;&#33021;&#22815;&#20445;&#35777;&#28385;&#36275;&#33258;&#20027;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#35201;&#27714;&#65292;&#24182;&#19982;&#19968;&#32452;&#20248;&#21270;&#30446;&#26631;&#30340;Pareto&#26368;&#20248;&#30456;&#23545;&#24212;&#12290;</title><link>http://arxiv.org/abs/2202.03360</link><description>&lt;p&gt;
&#24102;&#26377;&#28145;&#24230;&#23398;&#20064;&#24863;&#30693;&#32452;&#20214;&#30340;&#33258;&#20027;&#31995;&#32479;&#30340;&#31163;&#25955;&#20107;&#20214;&#25511;&#21046;&#22120;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Discrete-Event Controller Synthesis for Autonomous Systems with Deep-Learning Perception Components. (arXiv:2202.03360v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepDECS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20316;&#20026;&#20915;&#31574;&#36807;&#31243;&#30340;&#24863;&#30693;&#27493;&#39588;&#30340;&#33258;&#20027;&#31995;&#32479;&#21512;&#25104;&#26500;&#24314;&#27491;&#30830;&#30340;&#31163;&#25955;&#20107;&#20214;&#25511;&#21046;&#22120;&#12290;&#27169;&#22411;&#33021;&#22815;&#20445;&#35777;&#28385;&#36275;&#33258;&#20027;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#35201;&#27714;&#65292;&#24182;&#19982;&#19968;&#32452;&#20248;&#21270;&#30446;&#26631;&#30340;Pareto&#26368;&#20248;&#30456;&#23545;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepDECS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20998;&#31867;&#22120;&#20316;&#20026;&#20915;&#31574;&#36807;&#31243;&#30340;&#24863;&#30693;&#27493;&#39588;&#30340;&#33258;&#20027;&#31995;&#32479;&#21512;&#25104;&#26500;&#24314;&#27491;&#30830;&#30340;&#31163;&#25955;&#20107;&#20214;&#25511;&#21046;&#22120;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20026;&#36825;&#20123;&#31995;&#32479;&#25552;&#20379;&#23433;&#20840;&#20445;&#35777;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#25511;&#21046;&#22120;&#32508;&#21512;&#26041;&#27861;&#36890;&#36807;&#23558;DNN&#39564;&#35777;&#19982;&#24050;&#39564;&#35777;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#21512;&#25104;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#21512;&#25104;&#30340;&#27169;&#22411;&#23545;&#24212;&#20110;&#20445;&#35777;&#28385;&#36275;&#33258;&#20027;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#35201;&#27714;&#65292;&#24182;&#19982;&#19968;&#32452;&#20248;&#21270;&#30446;&#26631;&#30340;Pareto&#26368;&#20248;&#30456;&#23545;&#24212;&#30340;&#31163;&#25955;&#20107;&#20214;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#20351;&#29992;&#35813;&#26041;&#27861;&#21512;&#25104;&#29992;&#20110;&#31227;&#21160;&#26426;&#22120;&#20154;&#30896;&#25758;&#32531;&#35299;&#21644;&#32500;&#25252;&#20849;&#20139;&#25511;&#21046;&#33258;&#21160;&#39550;&#39542;&#20013;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#30340;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DeepDECS, a new method for the synthesis of correct-by-construction discrete-event controllers for autonomous systems that use deep neural network (DNN) classifiers for the perception step of their decision-making processes. Despite major advances in deep learning in recent years, providing safety guarantees for these systems remains very challenging. Our controller synthesis method addresses this challenge by integrating DNN verification with the synthesis of verified Markov models. The synthesised models correspond to discrete-event controllers guaranteed to satisfy the safety, dependability and performance requirements of the autonomous system, and to be Pareto optimal with respect to a set of optimisation objectives. We use the method in simulation to synthesise controllers for mobile-robot collision mitigation and for maintaining driver attentiveness in shared-control autonomous driving.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#19968;&#20123;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#25239;&#30456;&#20851;&#22122;&#22768;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#21644;&#24120;&#35268;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#36825;&#26159;&#22240;&#20026; Anti-PGD &#33021;&#22815;&#31227;&#21160;&#21040;&#26356;&#23485;&#30340;&#26368;&#23567;&#20540;&#28857;&#65292;&#32780; GD &#21644; PGD &#20250;&#20572;&#28382;&#22312;&#27425;&#20248;&#21306;&#22495;&#29978;&#33267;&#21457;&#25955;&#12290;</title><link>http://arxiv.org/abs/2202.02831</link><description>&lt;p&gt;
&#25239;&#30456;&#20851;&#22122;&#22768;&#27880;&#20837;&#29992;&#20110;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Anticorrelated Noise Injection for Improved Generalization. (arXiv:2202.02831v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#19968;&#20123;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#25239;&#30456;&#20851;&#22122;&#22768;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#21644;&#24120;&#35268;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#36825;&#26159;&#22240;&#20026; Anti-PGD &#33021;&#22815;&#31227;&#21160;&#21040;&#26356;&#23485;&#30340;&#26368;&#23567;&#20540;&#28857;&#65292;&#32780; GD &#21644; PGD &#20250;&#20572;&#28382;&#22312;&#27425;&#20248;&#21306;&#22495;&#29978;&#33267;&#21457;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20154;&#24037;&#22122;&#22768;&#27880;&#20837;&#26799;&#24230;&#19979;&#38477;&#24120;&#24120;&#34987;&#29992;&#20110;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#24120;&#65292;&#36825;&#31181;&#25200;&#21160;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20351;&#29992;&#30340;&#26159;&#19981;&#30456;&#20851;&#30340;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#26159;&#21542;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#30456;&#20851;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#30446;&#26631;&#20989;&#25968;&#65292;&#21457;&#29616;&#24102;&#26377;&#25239;&#30456;&#20851;&#25200;&#21160;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;"Anti-PGD"&#65289;&#27604;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#21644;&#24120;&#35268;&#30340;&#65288;&#19981;&#30456;&#20851;&#30340;&#65289;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#26377;&#30528;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20123;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102; Anti-PGD &#33021;&#22815;&#31227;&#21160;&#21040;&#26356;&#23485;&#30340;&#26368;&#23567;&#20540;&#28857;&#65292;&#32780; GD &#21644; PGD &#20250;&#20572;&#28382;&#22312;&#27425;&#20248;&#21306;&#22495;&#29978;&#33267;&#21457;&#25955;&#12290;&#36825;&#19968;&#26032;&#39062;&#30340;&#25239;&#30456;&#20851;&#22122;&#22768;&#19982;&#27867;&#21270;&#24615;&#33021;&#30340;&#32852;&#31995;&#20026;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Injecting artificial noise into gradient descent (GD) is commonly employed to improve the performance of machine learning models. Usually, uncorrelated noise is used in such perturbed gradient descent (PGD) methods. It is, however, not known if this is optimal or whether other types of noise could provide better generalization performance. In this paper, we zoom in on the problem of correlating the perturbations of consecutive PGD steps. We consider a variety of objective functions for which we find that GD with anticorrelated perturbations ("Anti-PGD") generalizes significantly better than GD and standard (uncorrelated) PGD. To support these experimental findings, we also derive a theoretical analysis that demonstrates that Anti-PGD moves to wider minima, while GD and PGD remain stuck in suboptimal regions or even diverge. This new connection between anticorrelated noise and generalization opens the field to novel ways to exploit noise for training machine learning models.
&lt;/p&gt;</description></item><item><title>PROMPT&#26159;&#19968;&#20010;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#21644;&#20027;&#21160;QoS&#39044;&#27979;&#30340;&#36164;&#28304;&#20998;&#37197;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#36164;&#28304;&#20248;&#21270;&#65292;&#22312;&#22788;&#29702;&#30701;&#26242;&#27874;&#21160;&#26041;&#38754;&#26356;&#21152;&#31283;&#23450;&#65292;&#24182;&#19988;&#22312;&#35843;&#24230;&#26032;&#30340;&#26368;&#20339;&#21162;&#21147;&#24037;&#20316;&#36127;&#36733;&#26102;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.07916</link><description>&lt;p&gt;
PROMPT&#65306;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#23398;&#20064;&#21160;&#24577;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
PROMPT: Learning Dynamic Resource Allocation Policies for Network Applications. (arXiv:2201.07916v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07916
&lt;/p&gt;
&lt;p&gt;
PROMPT&#26159;&#19968;&#20010;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#21644;&#20027;&#21160;QoS&#39044;&#27979;&#30340;&#36164;&#28304;&#20998;&#37197;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#36164;&#28304;&#20248;&#21270;&#65292;&#22312;&#22788;&#29702;&#30701;&#26242;&#27874;&#21160;&#26041;&#38754;&#26356;&#21152;&#31283;&#23450;&#65292;&#24182;&#19988;&#22312;&#35843;&#24230;&#26032;&#30340;&#26368;&#20339;&#21162;&#21147;&#24037;&#20316;&#36127;&#36733;&#26102;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#26381;&#21153;&#25552;&#20379;&#21830;&#27491;&#22312;&#25506;&#32034;&#36890;&#36807;&#23558;&#39640;&#20248;&#20808;&#32423;&#30340;&#24310;&#36831;&#20851;&#38190;&#24037;&#20316;&#36127;&#36733;&#19982;&#26368;&#20248;&#21162;&#21147;&#24037;&#20316;&#36127;&#36733;&#36827;&#34892;&#21327;&#21516;&#35843;&#24230;&#26469;&#25552;&#39640;&#26381;&#21153;&#22120;&#21033;&#29992;&#29575;&#21644;&#38477;&#20302;&#21151;&#32791;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#23454;&#36341;&#38656;&#35201;&#22312;&#24037;&#20316;&#36127;&#36733;&#20043;&#38388;&#36827;&#34892;&#20005;&#26684;&#30340;&#36164;&#28304;&#20998;&#37197;&#65292;&#20197;&#20943;&#23569;&#20105;&#29992;&#24182;&#20445;&#25345;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing number of service providers are exploring methods to improve server utilization and reduce power consumption by co-scheduling high-priority latency-critical workloads with best-effort workloads. This practice requires strict resource allocation between workloads to reduce contention and maintain Quality-of-Service (QoS) guarantees. Prior work demonstrated promising opportunities to dynamically allocate resources based on workload demand, but may fail to meet QoS objectives in more stringent operating environments due to the presence of resource allocation cliffs, transient fluctuations in workload performance, and rapidly changing resource demand. We therefore propose PROMPT, a novel resource allocation framework using proactive QoS prediction to guide a reinforcement learning controller. PROMPT enables more precise resource optimization, more consistent handling of transient behaviors, and more robust generalization when co-scheduling new best-effort workloads not encountere
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26410;&#21463;&#30417;&#30563;&#32534;&#30721;&#22120;&#38754;&#20020;&#30340;&#30423;&#31363;&#25915;&#20987;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861; Cont-Steal&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#32534;&#30721;&#22120;&#30340;&#20016;&#23500;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2201.07513</link><description>&lt;p&gt;
&#26080;&#27861;&#30423;&#31363;&#65311;&#23581;&#35797;&#23545;&#25239;&#31363;&#21548;&#65281;&#38024;&#23545;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#23545;&#27604;&#31363;&#21548;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders. (arXiv:2201.07513v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26410;&#21463;&#30417;&#30563;&#32534;&#30721;&#22120;&#38754;&#20020;&#30340;&#30423;&#31363;&#25915;&#20987;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861; Cont-Steal&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#32534;&#30721;&#22120;&#30340;&#20016;&#23500;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#12290;&#23427;&#20204;&#23558;&#22270;&#20687;&#32534;&#30721;&#20026;&#20016;&#23500;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#23545;&#19979;&#28216;&#20219;&#21153;&#26159;&#19981;&#21487;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#20854;&#38761;&#21629;&#24615;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#38656;&#35201;&#19987;&#29992;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20351;&#22270;&#20687;&#32534;&#30721;&#22120;&#38754;&#20020;&#30528;&#28508;&#22312;&#30340;&#27169;&#22411;&#30423;&#31363;&#25915;&#20987;&#39118;&#38505;&#65292;&#36825;&#26159;&#19968;&#31181;&#24265;&#20215;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#27169;&#20223;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#24615;&#33021;&#32780;&#35268;&#36991;&#33499;&#21051;&#30340;&#35201;&#27714;&#12290;&#20294;&#26159;&#20256;&#32479;&#25915;&#20987;&#20165;&#38024;&#23545;&#20110;&#26377;&#26631;&#31614;&#21644;/&#25110;&#21518;&#39564;&#30340;&#21463;&#30417;&#30563;&#20998;&#31867;&#22120;&#65292;&#22240;&#27492;&#26410;&#21463;&#30417;&#30563;&#30340;&#32534;&#30721;&#22120;&#30340;&#28431;&#27934;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning techniques have been developing rapidly to make full use of unlabeled images. They encode images into rich features that are oblivious to downstream tasks. Behind their revolutionary representation power, the requirements for dedicated model designs and a massive amount of computation resources expose image encoders to the risks of potential model stealing attacks - a cheap way to mimic the well-trained encoder performance while circumventing the demanding requirements. Yet conventional attacks only target supervised classifiers given their predicted labels and/or posteriors, which leaves the vulnerability of unsupervised encoders unexplored.  In this paper, we first instantiate the conventional stealing attacks against encoders and demonstrate their severer vulnerability compared with downstream classifiers. To better leverage the rich representation of encoders, we further propose Cont-Steal, a contrastive-learning-based attack, and validate it
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FLSys&#65292;&#19968;&#20010;&#31227;&#21160;-&#20113;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#65292;&#21487;&#20197;&#25104;&#20026;FL&#27169;&#22411;&#21644;&#24212;&#29992;&#31243;&#24207;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;FLSys&#26088;&#22312;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#31227;&#21160;&#24863;&#27979;&#25968;&#25454;&#12290;&#23427;&#24179;&#34913;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#28040;&#32791;&#65292;&#23481;&#24525;&#36890;&#20449;&#25925;&#38556;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;FLSys&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#21644;&#19968;&#20010;&#36890;&#29992;&#30340;API&#65292;&#20379;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#20154;&#21592;&#35775;&#38382;FL&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2111.09445</link><description>&lt;p&gt;
FLSys&#65306;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#31227;&#21160;&#24212;&#29992;&#30340;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FLSys: Toward an Open Ecosystem for Federated Learning Mobile Apps. (arXiv:2111.09445v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLSys&#65292;&#19968;&#20010;&#31227;&#21160;-&#20113;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#65292;&#21487;&#20197;&#25104;&#20026;FL&#27169;&#22411;&#21644;&#24212;&#29992;&#31243;&#24207;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;FLSys&#26088;&#22312;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#31227;&#21160;&#24863;&#27979;&#25968;&#25454;&#12290;&#23427;&#24179;&#34913;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#28040;&#32791;&#65292;&#23481;&#24525;&#36890;&#20449;&#25925;&#38556;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;FLSys&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#21644;&#19968;&#20010;&#36890;&#29992;&#30340;API&#65292;&#20379;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#20154;&#21592;&#35775;&#38382;FL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces FLSys, a mobile-cloud federated learning (FL) system that can be a key component for an open ecosystem of FL models and apps. FLSys is designed to work on smart phones with mobile sensing data. It balances model performance with resource consumption, tolerates communication failures, and achieves scalability. FLSys provides advanced privacy preserving mechanisms and a common API for third-party app developers to access FL models.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLSys&#30340;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#31227;&#21160;-&#20113;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#65292;&#21487;&#20197;&#25104;&#20026;FL&#27169;&#22411;&#21644;&#24212;&#29992;&#31243;&#24207;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;FLSys&#26088;&#22312;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#31227;&#21160;&#24863;&#27979;&#25968;&#25454;&#12290;&#23427;&#24179;&#34913;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#28040;&#32791;&#65292;&#23481;&#24525;&#36890;&#20449;&#25925;&#38556;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;FLSys&#20013;&#65292;&#19981;&#21516;&#30340;DL&#27169;&#22411;&#21644;&#19981;&#21516;&#30340;FL&#32858;&#21512;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#34987;&#19981;&#21516;&#30340;&#24212;&#29992;&#31243;&#24207;&#35757;&#32451;&#21644;&#35775;&#38382;&#12290;&#27492;&#22806;&#65292;FLSys&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#21644;&#19968;&#20010;&#36890;&#29992;&#30340;API&#65292;&#20379;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#20154;&#21592;&#35775;&#38382;FL&#27169;&#22411;&#12290;FLSys&#37319;&#29992;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#23454;&#29616;&#22312;Android&#21644;AWS&#20113;&#20013;&#12290;&#25105;&#20204;&#19982;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#27169;&#22411;&#20849;&#21516;&#35774;&#35745;&#20102;FLSys&#12290;&#22312;4&#20010;&#26376;&#30340;&#26102;&#38388;&#37324;&#65292;&#20174;100&#22810;&#21517;&#22823;&#23398;&#29983;&#20013;&#25910;&#38598;&#20102;HAR&#24863;&#27979;&#25968;&#25454;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;HAR-Wild&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#31227;&#21160;&#35774;&#22791;&#37327;&#36523;&#23450;&#21046;&#30340;CNN&#27169;&#22411;&#65292;&#20855;&#26377;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#20197;&#20943;&#36731;p
&lt;/p&gt;
&lt;p&gt;
This article presents the design, implementation, and evaluation of FLSys, a mobile-cloud federated learning (FL) system, which can be a key component for an open ecosystem of FL models and apps. FLSys is designed to work on smart phones with mobile sensing data. It balances model performance with resource consumption, tolerates communication failures, and achieves scalability. In FLSys, different DL models with different FL aggregation methods can be trained and accessed concurrently by different apps. Furthermore, FLSys provides advanced privacy preserving mechanisms and a common API for third-party app developers to access FL models. FLSys adopts a modular design and is implemented in Android and AWS cloud. We co-designed FLSys with a human activity recognition (HAR) model. HAR sensing data was collected in the wild from 100+ college students during a 4-month period. We implemented HAR-Wild, a CNN model tailored to mobile devices, with a data augmentation mechanism to mitigate the p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#36923;&#36753;&#22238;&#24402;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20351;&#22270;&#20687;&#20998;&#31867;&#22120;&#23545;&#38543;&#26426;&#21644;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#20351;&#29992;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#26102;&#65292;&#30456;&#27604;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#26032;&#30340;&#38543;&#26426;&#35757;&#32451;&#26041;&#27861;&#65292;&#27979;&#35797;&#38169;&#35823;&#29575;&#30340;&#38477;&#20302;&#39640;&#36798;83.5%&#65292;&#25439;&#22833;&#38477;&#20302;&#39640;&#36798;91.3%&#12290;</title><link>http://arxiv.org/abs/2109.12772</link><description>&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#30340;&#22810;&#31867;&#20998;&#31867;&#21450;&#20854;&#22312;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Multiclass Classification and Applications in Deep Image Classifiers. (arXiv:2109.12772v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#36923;&#36753;&#22238;&#24402;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20351;&#22270;&#20687;&#20998;&#31867;&#22120;&#23545;&#38543;&#26426;&#21644;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#20351;&#29992;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#26102;&#65292;&#30456;&#27604;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#26032;&#30340;&#38543;&#26426;&#35757;&#32451;&#26041;&#27861;&#65292;&#27979;&#35797;&#38169;&#35823;&#29575;&#30340;&#38477;&#20302;&#39640;&#36798;83.5%&#65292;&#25439;&#22833;&#38477;&#20302;&#39640;&#36798;91.3%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#36923;&#36753;&#22238;&#24402;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#23481;&#24525;&#25968;&#25454;&#21463;&#21040;&#31163;&#32676;&#20540;&#30340;&#24178;&#25200;&#12290;&#35813;DRO&#26694;&#26550;&#20351;&#29992;&#20855;&#26377;&#25509;&#36817;Wasserstein&#36317;&#31163;&#24847;&#20041;&#19979;&#30340;&#32463;&#39564;&#20998;&#24067;&#30340;&#20998;&#24067;&#29699;&#30340;&#27010;&#29575;&#27169;&#31946;&#38598;&#26469;&#23450;&#20041;&#12290;&#25105;&#20204;&#23558;DRO&#24418;&#24335;&#21270;&#31616;&#20026;&#19968;&#20010;&#27491;&#21017;&#21270;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#27491;&#21017;&#21270;&#39033;&#26159;&#31995;&#25968;&#30697;&#38453;&#30340;&#33539;&#25968;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#24314;&#31435;&#20102;&#26679;&#22806;&#24615;&#33021;&#20445;&#35777;&#65292;&#20026;&#25105;&#20204;&#25511;&#21046;&#39044;&#27979;&#35823;&#24046;&#30340;&#27491;&#21017;&#21270;&#22120;&#30340;&#20316;&#29992;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20351;&#22522;&#20110;&#28145;&#24230;Vision Transformer&#65288;ViT&#65289;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#23545;&#38543;&#26426;&#21644;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#35757;&#32451;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#27979;&#35797;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;&#39640;&#36798;83.5%&#65292;&#25439;&#22833;&#38477;&#20302;&#20102;&#39640;&#36798;91.3%&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a Distributionally Robust Optimization (DRO) formulation for Multiclass Logistic Regression (MLR), which could tolerate data contaminated by outliers. The DRO framework uses a probabilistic ambiguity set defined as a ball of distributions that are close to the empirical distribution of the training set in the sense of the Wasserstein metric. We relax the DRO formulation into a regularized learning problem whose regularizer is a norm of the coefficient matrix. We establish out-of-sample performance guarantees for the solutions to our model, offering insights on the role of the regularizer in controlling the prediction error. We apply the proposed method in rendering deep Vision Transformer (ViT)-based image classifiers robust to random and adversarial attacks. Specifically, using the MNIST and CIFAR-10 datasets, we demonstrate reductions in test error rate by up to 83.5% and loss by up to 91.3% compared with baseline methods, by adopting a novel random training method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#30693;&#36947;&#27169;&#22411;&#30340;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2109.08549</link><description>&lt;p&gt;
&#22312;&#26410;&#30693;&#25935;&#24863;&#23646;&#24615;&#24773;&#20917;&#19979;&#34913;&#37327;&#20844;&#24179;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#30340;&#26041;&#27861; (arXiv:2109.08549v5 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Measuring Fairness Under Unawareness of Sensitive Attributes: A Quantification-Based Approach. (arXiv:2109.08549v5 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.08549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#30693;&#36947;&#27169;&#22411;&#30340;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#21644;&#27169;&#22411;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#24433;&#21709;&#20154;&#20204;&#30340;&#29983;&#27963;&#30340;&#20915;&#31574;&#20013;&#65292;&#22240;&#27492;&#24433;&#21709;&#19981;&#21516;&#20154;&#32676;&#30340;&#36523;&#20221;&#35748;&#21516;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#22914;&#31181;&#26063;&#25110;&#24615;&#21035;&#31561;&#65292;&#24182;&#19981;&#20250;&#21463;&#21040;&#19981;&#20844;&#27491;&#30340;&#24453;&#36935;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#23545;&#20110;&#37027;&#20123;&#24320;&#21457;&#36825;&#20123;&#27169;&#22411;&#30340;&#20154;&#26469;&#35828;&#65292;&#24517;&#39035;&#20180;&#32454;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#24433;&#21709;&#24182;&#20559;&#29233;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21644;&#23384;&#20648;&#36825;&#20123;&#23646;&#24615;&#36890;&#24120;&#19982;&#25968;&#25454;&#26368;&#23567;&#21270;&#21644;&#38544;&#31169;&#35268;&#23450;&#23384;&#22312;&#20914;&#31361;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#26159;&#22312;&#24320;&#21457;&#20844;&#21496;&#20869;&#37096;&#65292;&#20063;&#24456;&#38590;&#34913;&#37327;&#24050;&#35757;&#32451;&#27169;&#22411;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#37327;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#22312;&#26410;&#30693;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#34913;&#37327;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms and models are increasingly deployed to inform decisions about people, inevitably affecting their lives. As a consequence, those in charge of developing these models must carefully evaluate their impact on different groups of people and favour group fairness, that is, ensure that groups determined by sensitive demographic attributes, such as race or sex, are not treated unjustly. To achieve this goal, the availability (awareness) of these demographic attributes to those evaluating the impact of these models is fundamental. Unfortunately, collecting and storing these attributes is often in conflict with industry practices and legislation on data minimisation and privacy. For this reason, it can be hard to measure the group fairness of trained models, even from within the companies developing them. In this work, we tackle the problem of measuring group fairness under unawareness of sensitive attributes, by using techniques from quantification, a supervised learning task concer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30697;&#38453;&#20998;&#35299;&#26469;&#36827;&#34892;&#30697;&#38453;&#35745;&#31639;&#30340;&#26041;&#24046;&#32553;&#20943;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#38382;&#39064;&#19978;&#21487;&#20197;&#23454;&#29616;&#20219;&#24847;&#26356;&#22909;&#30340;&#38543;&#26426;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#30697;&#38453;&#20056;&#31215;&#36857;&#30340;&#20998;&#35299;&#20272;&#35745;&#22120;&#19982;&#23545;&#21322;&#27491;&#23450;&#30697;&#38453;&#34892;&#21015;&#24335;&#30340;&#23545;&#25968;&#20272;&#35745;&#22120;&#22343;&#26377;&#26174;&#33879;&#30340;&#25928;&#29575;&#25552;&#39640;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2106.14565</link><description>&lt;p&gt;
&#30697;&#38453;&#35745;&#31639;&#30340;&#26041;&#24046;&#32553;&#20943;&#21450;&#20854;&#22312;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Variance Reduction for Matrix Computations with Applications to Gaussian Processes. (arXiv:2106.14565v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.14565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30697;&#38453;&#20998;&#35299;&#26469;&#36827;&#34892;&#30697;&#38453;&#35745;&#31639;&#30340;&#26041;&#24046;&#32553;&#20943;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#38382;&#39064;&#19978;&#21487;&#20197;&#23454;&#29616;&#20219;&#24847;&#26356;&#22909;&#30340;&#38543;&#26426;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#30697;&#38453;&#20056;&#31215;&#36857;&#30340;&#20998;&#35299;&#20272;&#35745;&#22120;&#19982;&#23545;&#21322;&#27491;&#23450;&#30697;&#38453;&#34892;&#21015;&#24335;&#30340;&#23545;&#25968;&#20272;&#35745;&#22120;&#22343;&#26377;&#26174;&#33879;&#30340;&#25928;&#29575;&#25552;&#39640;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#36895;&#24230;&#21644;&#20869;&#23384;&#23481;&#37327;&#30340;&#19981;&#26029;&#25552;&#39640;&#65292;&#26041;&#27861;&#23398;&#19978;&#30340;&#36827;&#27493;&#20063;&#20026;&#38543;&#26426;&#27169;&#25311;&#30340;&#24615;&#33021;&#24102;&#26469;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#36890;&#36807;&#30697;&#38453;&#20998;&#35299;&#26469;&#36827;&#34892;&#30697;&#38453;&#35745;&#31639;&#30340;&#26041;&#24046;&#32553;&#20943;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#26377;&#26041;&#24046;&#32553;&#20943;&#26041;&#27861;&#22312;&#20272;&#35745;&#22823;&#22411;&#30697;&#38453;&#20803;&#32032;&#26102;&#26410;&#33021;&#21033;&#29992;&#30697;&#38453;&#20998;&#35299;&#25152;&#24102;&#26469;&#30340;&#26041;&#24046;&#32553;&#20943;&#65292;&#32780;&#22914;&#20309;&#36890;&#36807;&#35745;&#31639;&#30697;&#38453;&#30340;&#24179;&#26041;&#26681;&#20998;&#35299;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#23454;&#29616;&#20219;&#24847;&#26356;&#22909;&#30340;&#38543;&#26426;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30697;&#38453;&#20056;&#31215;&#36857;&#30340;&#20998;&#35299;&#20272;&#35745;&#22120;&#65292;&#24182;&#22312;&#25968;&#20540;&#19978;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#39640;&#26031;&#36807;&#31243;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#20272;&#35745;&#30340;&#38382;&#39064;&#19978;&#65292;&#35813;&#20272;&#35745;&#22120;&#30340;&#25928;&#29575;&#21487;&#20197;&#25552;&#39640;1000&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#21322;&#27491;&#23450;&#30697;&#38453;&#34892;&#21015;&#24335;&#30340;&#23545;&#25968;&#20272;&#35745;&#22120;&#65292;&#20854;&#26041;&#24046;&#27604;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#20272;&#35745;&#22120;&#24555;&#36895;&#34928;&#20943;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#20013;&#35828;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In addition to recent developments in computing speed and memory, methodological advances have contributed to significant gains in the performance of stochastic simulation. In this paper, we focus on variance reduction for matrix computations via matrix factorization. We provide insights into existing variance reduction methods for estimating the entries of large matrices. Popular methods do not exploit the reduction in variance that is possible when the matrix is factorized. We show how computing the square root factorization of the matrix can achieve in some important cases arbitrarily better stochastic performance. In addition, we propose a factorized estimator for the trace of a product of matrices and numerically demonstrate that the estimator can be up to 1,000 times more efficient on certain problems of estimating the log-likelihood of a Gaussian process. Additionally, we provide a new estimator of the log-determinant of a positive semi-definite matrix where the log-determinant 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#30830;&#23450;&#24615;&#22270;&#20687;&#20998;&#31867;&#22120;&#22312;&#32473;&#23450;&#36755;&#20837;&#25968;&#25454;&#19978;&#32467;&#26524;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23450;&#20041;&#20102;&#24800;&#28789;&#39039;&#21518;&#39564;&#20316;&#20026;&#20998;&#24067;&#20197;&#34920;&#26126;&#21487;&#33021;&#30001;&#29983;&#25104;&#32473;&#23450;&#22270;&#20687;&#30340;&#30456;&#21516;&#22330;&#26223;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2106.13870</link><description>&lt;p&gt;
&#22330;&#26223;&#19981;&#30830;&#23450;&#24615;&#19982;&#30830;&#23450;&#24615;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24800;&#28789;&#39039;&#21518;&#39564;
&lt;/p&gt;
&lt;p&gt;
Scene Uncertainty and the Wellington Posterior of Deterministic Image Classifiers. (arXiv:2106.13870v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.13870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#30830;&#23450;&#24615;&#22270;&#20687;&#20998;&#31867;&#22120;&#22312;&#32473;&#23450;&#36755;&#20837;&#25968;&#25454;&#19978;&#32467;&#26524;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23450;&#20041;&#20102;&#24800;&#28789;&#39039;&#21518;&#39564;&#20316;&#20026;&#20998;&#24067;&#20197;&#34920;&#26126;&#21487;&#33021;&#30001;&#29983;&#25104;&#32473;&#23450;&#22270;&#20687;&#30340;&#30456;&#21516;&#22330;&#26223;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#22312;&#32473;&#23450;&#36755;&#20837;&#25968;&#25454;&#19978;&#22270;&#20687;&#20998;&#31867;&#22120;&#32467;&#26524;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#20174;&#36755;&#20837;&#22270;&#20687;&#21040;&#36755;&#20986;&#31867;&#21035;&#30340;&#30830;&#23450;&#24615;&#26144;&#23556;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#32473;&#23450;&#25968;&#25454;&#19978;&#30340;&#32467;&#26524;&#19981;&#28041;&#21450;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#27492;&#24517;&#39035;&#22312;&#23450;&#20041;&#12289;&#27979;&#37327;&#21644;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23558;&#8220;&#32622;&#20449;&#24230;&#8221;&#24402;&#22240;&#20110;&#32467;&#26524;&#26102;&#25351;&#23450;&#25152;&#24341;&#29992;&#30340;&#21464;&#21270;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24800;&#28789;&#39039;&#21518;&#39564;&#65292;&#23427;&#26159;&#22312;&#21487;&#33021;&#30001;&#29983;&#25104;&#32473;&#23450;&#22270;&#20687;&#30340;&#30456;&#21516;&#22330;&#26223;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#21709;&#24212;&#20013;&#33719;&#24471;&#30340;&#32467;&#26524;&#20998;&#24067;&#12290;&#30001;&#20110;&#21487;&#20197;&#29983;&#25104;&#20219;&#20309;&#32473;&#23450;&#22270;&#20687;&#30340;&#26080;&#38480;&#22810;&#20010;&#22330;&#26223;&#65292;&#22240;&#27492;&#24800;&#28789;&#39039;&#21518;&#39564;&#28041;&#21450;&#26469;&#33258;&#38500;&#25152;&#25551;&#32472;&#30340;&#22330;&#26223;&#20043;&#22806;&#30340;&#22330;&#26223;&#30340;&#24402;&#32435;&#20256;&#36882;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#12289;&#20002;&#24323;&#12289;&#38598;&#25104;&#12289;&#21333;&#35270;&#22270;&#37325;&#24314;&#21644;&#27169;&#22411;&#32447;&#24615;&#21270;&#26469;&#35745;&#31639;&#24800;&#28789;&#39039;&#21518;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to estimate the uncertainty of the outcome of an image classifier on a given input datum. Deep neural networks commonly used for image classification are deterministic maps from an input image to an output class. As such, their outcome on a given datum involves no uncertainty, so we must specify what variability we are referring to when defining, measuring and interpreting uncertainty, and attributing "confidence" to the outcome. To this end, we introduce the Wellington Posterior, which is the distribution of outcomes that would have been obtained in response to data that could have been generated by the same scene that produced the given image. Since there are infinitely many scenes that could have generated any given image, the Wellington Posterior involves inductive transfer from scenes other than the one portrayed. We explore the use of data augmentation, dropout, ensembling, single-view reconstruction, and model linearization to compute a Wellington Posterior. 
&lt;/p&gt;</description></item><item><title>FGLP&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#21644;&#39044;&#27979;&#27169;&#22411;&#32452;&#25104;&#30340;&#31995;&#32479;&#65292;&#23558;&#26234;&#33021;&#25163;&#26426;&#19978;&#25910;&#38598;&#30340;GPS&#36712;&#36857;&#25277;&#35937;&#20026;2D&#31354;&#38388;&#20013;&#30340;&#30456;&#23545;&#28857;&#65292;&#21512;&#24182;&#20102;BiLSTM&#21644;CNN&#20197;&#25429;&#33719;&#26102;&#38388;&#21644;&#31354;&#38388;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;8&#31859;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#19988;&#25968;&#25454;&#21457;&#36865;&#24320;&#38144;&#38477;&#20302;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2106.08946</link><description>&lt;p&gt;
FGLP&#65306;&#31227;&#21160;&#29992;&#25143;&#32852;&#37030;&#32454;&#31890;&#24230;&#20301;&#32622;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FGLP: A Federated Fine-Grained Location Prediction System for Mobile Users. (arXiv:2106.08946v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.08946
&lt;/p&gt;
&lt;p&gt;
FGLP&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#21644;&#39044;&#27979;&#27169;&#22411;&#32452;&#25104;&#30340;&#31995;&#32479;&#65292;&#23558;&#26234;&#33021;&#25163;&#26426;&#19978;&#25910;&#38598;&#30340;GPS&#36712;&#36857;&#25277;&#35937;&#20026;2D&#31354;&#38388;&#20013;&#30340;&#30456;&#23545;&#28857;&#65292;&#21512;&#24182;&#20102;BiLSTM&#21644;CNN&#20197;&#25429;&#33719;&#26102;&#38388;&#21644;&#31354;&#38388;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;8&#31859;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#19988;&#25968;&#25454;&#21457;&#36865;&#24320;&#38144;&#38477;&#20302;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;&#32454;&#31890;&#24230;&#20301;&#32622;&#39044;&#27979;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#24212;&#29992;&#31243;&#24207;/&#31995;&#32479;&#24615;&#33021;&#12290; &#24212;&#29992;&#22330;&#26223;&#21253;&#25324;&#26681;&#25454;&#39044;&#27979;&#29992;&#25143;&#20301;&#32622;&#30340;5G&#32593;&#32476;&#36136;&#37327;&#33258;&#36866;&#24212;&#35270;&#39057;&#36136;&#37327;&#65292;&#20197;&#21450;&#22522;&#20110;&#39044;&#27979;&#29992;&#25143;&#20301;&#32622;&#21152;&#36895;&#20869;&#23481;&#28210;&#26579;&#30340;&#22686;&#24378;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#12290; &#36825;&#20123;&#29992;&#20363;&#35201;&#27714;&#39044;&#27979;&#35823;&#24046;&#19982;GPS&#35823;&#24046;&#30456;&#21516;&#65292;&#24182;&#19988;&#30446;&#21069;&#22312;&#20301;&#32622;&#39044;&#27979;&#26041;&#38754;&#27809;&#26377;&#20219;&#20309;&#29616;&#26377;&#24037;&#20316;&#21487;&#20197;&#23454;&#29616;&#36825;&#31181;&#31934;&#24230;&#27700;&#24179;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25163;&#26426;&#19978;&#25910;&#38598;&#30340;GPS&#36712;&#36857;&#30340;&#31227;&#21160;&#29992;&#25143;&#30340;&#32454;&#31890;&#24230;&#20301;&#32622;&#39044;&#27979;&#31995;&#32479;&#65288;FGLP&#65289;&#12290; FGLP&#26377;&#20004;&#20010;&#32452;&#20214;&#65306;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#21644;&#39044;&#27979;&#27169;&#22411;&#12290; &#35813;&#26694;&#26550;&#22312;&#29992;&#25143;&#30340;&#25163;&#26426;&#19978;&#20197;&#21450;&#22312;&#21327;&#35843;&#31995;&#32479;&#20013;&#30340;&#25152;&#26377;&#29992;&#25143;&#30340;&#26381;&#21153;&#22120;&#19978;&#36816;&#34892;&#23398;&#20064;&#12290; FGLP&#23558;&#29992;&#25143;&#20301;&#32622;&#25968;&#25454;&#34920;&#31034;&#20026;&#25277;&#35937;2D&#31354;&#38388;&#20013;&#30340;&#30456;&#23545;&#28857;&#65292;&#36825;&#20351;&#24471;&#21487;&#20197;&#36328;&#19981;&#21516;&#30340;&#29289;&#29702;&#31354;&#38388;&#36827;&#34892;&#23398;&#20064;&#12290; &#35813;&#27169;&#22411;&#21512;&#24182;&#20102;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;BiLSTM&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20197;&#25429;&#33719;GPS&#36712;&#36857;&#20013;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#27169;&#24335;&#12290; &#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;FGLP&#23454;&#29616;&#20102;8&#31859;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#36825;&#19982;GPS&#35823;&#24046;&#30456;&#24403;&#65292;&#21516;&#26102;&#23558;&#25968;&#25454;&#21457;&#36865;&#24320;&#38144;&#38477;&#20302;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#30456;&#23545;&#20110;&#38598;&#20013;&#24335;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained location prediction on smart phones can be used to improve app/system performance. Application scenarios include video quality adaptation as a function of the 5G network quality at predicted user locations, and augmented reality apps that speed up content rendering based on predicted user locations. Such use cases require prediction error in the same range as the GPS error, and no existing works on location prediction can achieve this level of accuracy. We present a system for fine-grained location prediction (FGLP) of mobile users, based on GPS traces collected on the phones. FGLP has two components: a federated learning framework and a prediction model. The framework runs on the phones of the users and also on a server that coordinates learning from all users in the system. FGLP represents the user location data as relative points in an abstract 2D space, which enables learning across different physical spaces. The model merges Bidirectional Long Short-Term Memory (BiLST
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#20851;&#30340;&#22522;&#20110;&#26680;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#32479;&#35745;&#37327;&#22312;&#27979;&#37327;&#27969;&#24418;&#25968;&#25454;&#26102;&#30340;&#24212;&#29992;&#12290;&#25991;&#31456;&#23637;&#31034;&#20102;&#26816;&#39564;&#27700;&#24179;&#21644;&#21151;&#29575;&#19982;&#26680;&#24102;&#23485;&#12289;&#26679;&#26412;&#25968;&#37327;&#21644;&#27969;&#24418;&#20869;&#22312;&#32500;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#24314;&#31435;&#20102;&#27979;&#35797;&#21151;&#29575;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2105.03425</link><description>&lt;p&gt;
&#27979;&#37327;&#27969;&#24418;&#25968;&#25454;&#30340;&#26680;&#21452;&#26679;&#26412;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Kernel Two-Sample Tests for Manifold Data. (arXiv:2105.03425v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.03425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#20851;&#30340;&#22522;&#20110;&#26680;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#32479;&#35745;&#37327;&#22312;&#27979;&#37327;&#27969;&#24418;&#25968;&#25454;&#26102;&#30340;&#24212;&#29992;&#12290;&#25991;&#31456;&#23637;&#31034;&#20102;&#26816;&#39564;&#27700;&#24179;&#21644;&#21151;&#29575;&#19982;&#26680;&#24102;&#23485;&#12289;&#26679;&#26412;&#25968;&#37327;&#21644;&#27969;&#24418;&#20869;&#22312;&#32500;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#24314;&#31435;&#20102;&#27979;&#35797;&#21151;&#29575;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#27969;&#24418;&#25968;&#25454;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#20851;&#30340;&#22522;&#20110;&#26680;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#32479;&#35745;&#37327;&#65292;&#20551;&#35774;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#25509;&#36817;&#20110;&#20302;&#32500;&#27969;&#24418;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#27979;&#35797;&#27700;&#24179;&#21644;&#21151;&#29575;&#19982;&#26680;&#24102;&#23485;&#12289;&#26679;&#26412;&#25968;&#37327;&#21644;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#25968;&#25454;&#23494;&#24230;&#25903;&#25345;&#22312;&#19968;&#20010;&#23884;&#20837;&#21040;$m$&#32500;&#31354;&#38388;&#20013;&#30340;$d$&#32500;&#23376;&#27969;&#24418;$\mathcal{M}$&#19978;&#26102;&#65292;&#20174;&#26381;&#20174;&#20110;&#19968;&#23545;&#20998;&#24067;$p$&#21644;$q$&#25277;&#21462;&#30340;&#25968;&#25454;&#36827;&#34892;&#26680;&#21452;&#26679;&#26412;&#26816;&#39564;&#65292;&#36825;&#23545;&#20998;&#24067;$ p $&#21644;$q$&#26159;&#20855;&#26377;H\"older&#38454;$\beta$&#65288;&#26368;&#39640;2&#65289;&#65292;&#26679;&#26412;&#25968;&#37327;$n$&#36275;&#22815;&#22823;&#65292;&#20351;&#24471;$\Delta_2\gtrsim n^{- {2\beta/(d+4\beta)}}$&#65292;&#20854;&#20013;$\Delta_2$&#26159;&#27969;&#24418;&#19978;$p$&#21644;$q$&#20043;&#38388;&#30340;&#24179;&#26041;$L^2$-&#24046;&#24322;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36275;&#22815;&#22823;&#19988;&#26377;&#38480;$n$&#30340;&#27979;&#35797;&#21151;&#29575;&#19979;&#30028;&#65292;&#20854;&#20013;&#26680;&#24102;&#23485;&#21442;&#25968;$\gamma$&#30340;&#27604;&#20363;&#23610;&#24230;&#20026;$n^ {-1/(d+4\beta)}$&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a study of a kernel-based two-sample test statistic related to the Maximum Mean Discrepancy (MMD) in the manifold data setting, assuming that high-dimensional observations are close to a low-dimensional manifold. We characterize the test level and power in relation to the kernel bandwidth, the number of samples, and the intrinsic dimensionality of the manifold. Specifically, we show that when data densities are supported on a $d$-dimensional sub-manifold $\mathcal{M}$ embedded in an $m$-dimensional space, the kernel two-sample test for data sampled from a pair of distributions $p$ and $q$ that are H\"older with order $\beta$ (up to 2) is powerful when the number of samples $n$ is large such that $\Delta_2 \gtrsim n^{- { 2 \beta/( d + 4 \beta ) }}$, where $\Delta_2$ is the squared $L^2$-divergence between $p$ and $q$ on manifold. We establish a lower bound on the test power for finite $n$ that is sufficiently large, where the kernel bandwidth parameter $\gamma$ scales as $n^{
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#21516;&#26102;&#22788;&#29702;&#36830;&#32493;&#21644;&#20998;&#31867;&#21464;&#37327;&#65292;&#29983;&#25104;&#30340;&#26679;&#26412;&#20855;&#26377;&#23454;&#29992;&#24615;&#25351;&#23548;&#24847;&#20041;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2105.00703</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#20998;&#31867;&#27169;&#22411;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causality-based Counterfactual Explanation for Classification Models. (arXiv:2105.00703v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.00703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#21516;&#26102;&#22788;&#29702;&#36830;&#32493;&#21644;&#20998;&#31867;&#21464;&#37327;&#65292;&#29983;&#25104;&#30340;&#26679;&#26412;&#20855;&#26377;&#23454;&#29992;&#24615;&#25351;&#23548;&#24847;&#20041;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#23427;&#20135;&#29983;&#25200;&#21160;&#26679;&#26412;&#20197;&#25913;&#21464;&#27169;&#22411;&#30340;&#21407;&#22987;&#20915;&#31574;&#12290;&#29983;&#25104;&#30340;&#26679;&#26412;&#21487;&#20197;&#20316;&#20026;&#24314;&#35758;&#65292;&#24110;&#21161;&#26368;&#32456;&#29992;&#25143;&#23454;&#29616;&#20182;&#20204;&#25152;&#38656;&#30340;&#32467;&#26524;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#26159;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#21482;&#33021;&#20248;&#21270;&#36830;&#32493;&#21464;&#37327;&#30340;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#12290;&#25454;&#27492;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#20998;&#31867;&#21464;&#37327;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#20960;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;1&#65289;&#29983;&#25104;&#21453;&#20107;&#23454;&#26102;&#36890;&#24120;&#24573;&#30053;&#29305;&#24449;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#21487;&#33021;&#23548;&#33268;&#23545;&#20915;&#31574;&#32773;&#30340;&#25351;&#23548;&#19981;&#20999;&#23454;&#38469;&#12290;2&#65289;&#21453;&#20107;&#23454;&#35299;&#37322;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#35843;&#25972;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#20248;&#26435;&#37325;&#65292;&#36825;&#24517;&#39035;&#20026;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#37325;&#22797;&#36827;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#29305;&#24449;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#22788;&#29702;&#36830;&#32493;&#21644;&#20998;&#31867;&#21464;&#37327;&#65292;&#26080;&#38656;&#20219;&#20309;&#29992;&#25143;&#25351;&#23450;&#30340;&#36229;&#21442;&#25968;&#12290;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#26679;&#26412;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanation is one branch of interpretable machine learning that produces a perturbation sample to change the model's original decision. The generated samples can act as a recommendation for end-users to achieve their desired outputs. Most of the current counterfactual explanation approaches are the gradient-based method, which can only optimize the differentiable loss functions with continuous variables. Accordingly, the gradient-free methods are proposed to handle the categorical variables, which however have several major limitations: 1) causal relationships among features are typically ignored when generating the counterfactuals, possibly resulting in impractical guidelines for decision-makers; 2) the counterfactual explanation algorithm requires a great deal of effort into parameter tuning for dertermining the optimal weight for each loss functions which must be conducted repeatedly for different datasets and settings. In this work, to address the above limitations,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36127;&#30456;&#20851;&#23398;&#20064;&#30340;&#28151;&#21512;&#38598;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#21152;&#26435;&#23376;&#27169;&#22411;&#26469;&#35299;&#20915;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2104.02317</link><description>&lt;p&gt;
&#19968;&#31181;&#24102;&#26377;&#36127;&#30456;&#20851;&#23398;&#20064;&#30340;&#28151;&#21512;&#38598;&#25104;&#31639;&#27861;&#36827;&#34892;&#22238;&#24402;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A hybrid ensemble method with negative correlation learning for regression. (arXiv:2104.02317v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.02317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36127;&#30456;&#20851;&#23398;&#20064;&#30340;&#28151;&#21512;&#38598;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#21152;&#26435;&#23376;&#27169;&#22411;&#26469;&#35299;&#20915;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#38598;&#25104;&#26159;&#38598;&#25104;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20998;&#25903;&#65292;&#22312;&#22238;&#24402;&#39046;&#22495;&#24471;&#21040;&#20102;&#36805;&#36895;&#21457;&#23637;&#65292;&#24182;&#35777;&#23454;&#20102;&#22810;&#26679;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#38598;&#25104;&#26041;&#27861;&#20027;&#35201;&#26159;&#22312;&#23376;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#32771;&#34385;&#22810;&#26679;&#24615;&#65292;&#25913;&#21892;&#25928;&#26524;&#26377;&#38480;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36825;&#39033;&#30740;&#31350;&#20174;&#24322;&#26500;&#27169;&#22411;&#27744;&#20013;&#33258;&#21160;&#36873;&#25321;&#21644;&#21152;&#26435;&#23376;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#20869;&#28857;&#36807;&#28388;&#32447;&#24615;&#25628;&#32034;&#31639;&#27861;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#30446;&#26631;&#20989;&#25968;&#21019;&#26032;&#22320;&#23558;&#36127;&#30456;&#20851;&#23398;&#20064;&#20316;&#20026;&#19968;&#20010;&#24809;&#32602;&#39033;&#65292;&#24182;&#36873;&#25321;&#22810;&#26679;&#21270;&#23376;&#27169;&#22411;&#23376;&#38598;&#12290;&#36873;&#25321;&#27599;&#20010;&#27169;&#22411;&#31867;&#30340;&#26368;&#20339;&#23376;&#27169;&#22411;&#26469;&#26500;&#24314;NCL&#38598;&#25104;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#31616;&#21333;&#24179;&#22343;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#21152;&#26435;&#26041;&#27861;&#12290;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;NCL&#38598;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hybrid ensemble, an essential branch of ensembles, has flourished in the regression field, with studies confirming diversity's importance. However, previous ensembles consider diversity in the sub-model training stage, with limited improvement compared to single models. In contrast, this study automatically selects and weights sub-models from a heterogeneous model pool. It solves an optimization problem using an interior-point filtering linear-search algorithm. The objective function innovatively incorporates negative correlation learning as a penalty term, with which a diverse model subset can be selected. The best sub-models from each model class are selected to build the NCL ensemble, which performance is better than the simple average and other state-of-the-art weighting methods. It is also possible to improve the NCL ensemble with a regularization term in the objective function. In practice, it is difficult to conclude the optimal sub-model for a dataset prior due to the model unc
&lt;/p&gt;</description></item><item><title>Transformer&#34429;&#28982;&#22312;&#35768;&#22810;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#20294;&#20854;&#22797;&#26434;&#24230;&#19982;&#24207;&#21015;&#38271;&#24230;&#30456;&#20851;&#12290;&#36817;&#26399;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#31867;&#20284;&#20110;Longformer&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#30452;&#25509;&#35299;&#20915;Transformer&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20854;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2103.14636</link><description>&lt;p&gt;
&#12298;&#26356;&#24555;&#26356;&#36731;&#30340;Transformer&#30340;&#23454;&#29992;&#35843;&#26597;&#12299;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
A Practical Survey on Faster and Lighter Transformers. (arXiv:2103.14636v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.14636
&lt;/p&gt;
&lt;p&gt;
Transformer&#34429;&#28982;&#22312;&#35768;&#22810;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#20294;&#20854;&#22797;&#26434;&#24230;&#19982;&#24207;&#21015;&#38271;&#24230;&#30456;&#20851;&#12290;&#36817;&#26399;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#31867;&#20284;&#20110;Longformer&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#30452;&#25509;&#35299;&#20915;Transformer&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20854;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26159;&#22788;&#29702;&#24207;&#21015;&#30340;&#26377;&#25928;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#22266;&#26377;&#30340;&#39034;&#24207;&#24615;&#65292;&#23427;&#20204;&#26080;&#27861;&#23398;&#20064;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;Vaswani&#31561;&#20154;&#25552;&#20986;&#20102;Transformer&#65292;&#19968;&#31181;&#20165;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20851;&#32852;&#36755;&#20837;&#24207;&#21015;&#30340;&#20219;&#24847;&#20004;&#20010;&#20301;&#32622;&#65292;&#20174;&#32780;&#24314;&#27169;&#20219;&#24847;&#38271;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;Transformer&#25913;&#21892;&#20102;&#20247;&#22810;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#30340;&#20195;&#20215;&#26159;&#19982;&#24207;&#21015;&#38271;&#24230;&#30456;&#20851;&#30340;&#20108;&#27425;&#35745;&#31639;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#30340;&#24212;&#29992;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#19968;&#30452;&#33268;&#21147;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#21442;&#25968;&#20849;&#20139;&#12289;&#21098;&#26525;&#12289;&#28151;&#21512;&#31934;&#24230;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#35774;&#35745;&#20302;&#22797;&#26434;&#24230;&#30340;&#26367;&#20195;&#21697;&#65292;&#22914;Longformer&#31561;&#65292;&#30452;&#25509;&#35299;&#20915;&#20102;Transformer&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural networks are effective models to process sequences. However, they are unable to learn long-term dependencies because of their inherent sequential nature. As a solution, Vaswani et al. introduced the Transformer, a model solely based on the attention mechanism that is able to relate any two positions of the input sequence, hence modelling arbitrary long dependencies. The Transformer has improved the state-of-the-art across numerous sequence modelling tasks. However, its effectiveness comes at the expense of a quadratic computational and memory complexity with respect to the sequence length, hindering its adoption. Fortunately, the deep learning community has always been interested in improving the models' efficiency, leading to a plethora of solutions such as parameter sharing, pruning, mixed-precision, and knowledge distillation. Recently, researchers have directly addressed the Transformer's limitation by designing lower-complexity alternatives such as the Longformer,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20849;&#20139;&#25968;&#25454;&#34920;&#31034;&#21644;&#21508;&#33258;&#26412;&#22320;&#25311;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32447;&#24615;&#25910;&#25947;&#21040;&#30495;&#23454;&#20540;&#12290;</title><link>http://arxiv.org/abs/2102.07078</link><description>&lt;p&gt;
&#21033;&#29992;&#20849;&#20139;&#34920;&#31034;&#36827;&#34892;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Shared Representations for Personalized Federated Learning. (arXiv:2102.07078v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.07078
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20849;&#20139;&#25968;&#25454;&#34920;&#31034;&#21644;&#21508;&#33258;&#26412;&#22320;&#25311;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32447;&#24615;&#25910;&#25947;&#21040;&#30495;&#23454;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#23637;&#31034;&#20986;&#33021;&#22815;&#20174;&#22270;&#20687;&#21644;&#25991;&#26412;&#31561;&#25968;&#25454;&#20013;&#25552;&#21462;&#26222;&#36866;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#36825;&#23545;&#20110;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#37117;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#30340;&#25104;&#26524;&#12290;&#34429;&#28982;&#32852;&#37030;&#25968;&#25454;&#36890;&#24120;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#19981;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#20294;&#20013;&#24515;&#21270;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#34920;&#26126;&#25968;&#25454;&#36890;&#24120;&#20849;&#20139;&#20840;&#23616;&#29305;&#24449;&#34920;&#31034;&#65292;&#32780;&#23458;&#25143;&#31471;&#25110;&#20219;&#21153;&#20043;&#38388;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#38598;&#20013;&#22312;&#26631;&#31614;&#19978;&#12290;&#22522;&#20110;&#36825;&#19968;&#30452;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#36328;&#23458;&#25143;&#31471;&#30340;&#20849;&#20139;&#25968;&#25454;&#34920;&#31034;&#21644;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#21807;&#19968;&#26412;&#22320;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#33021;&#21147;&#65292;&#38024;&#23545;&#27599;&#20010;&#34920;&#31034;&#26356;&#26032;&#36827;&#34892;&#35768;&#22810;&#20302;&#32500;&#26412;&#22320;&#21442;&#25968;&#30340;&#26412;&#22320;&#26356;&#26032;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#32447;&#24615;&#25910;&#25947;&#21040;&#30495;&#23454;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have shown the ability to extract universal feature representations from data such as images and text that have been useful for a variety of learning tasks. However, the fruits of representation learning have yet to be fully-realized in federated settings. Although data in federated settings is often non-i.i.d. across clients, the success of centralized deep learning suggests that data often shares a global feature representation, while the statistical heterogeneity across clients or tasks is concentrated in the labels. Based on this intuition, we propose a novel federated learning framework and algorithm for learning a shared data representation across clients and unique local heads for each client. Our algorithm harnesses the distributed computational power across clients to perform many local-updates with respect to the low-dimensional local parameters for every update of the representation. We prove that this method obtains linear convergence to the ground-trut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#19981;&#38656;&#35201;&#35843;&#25972;&#25351;&#25968;&#21442;&#25968;&#30340;MNL-Contextual Bandit&#38382;&#39064;&#30340;&#31616;&#20415;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#31639;&#27861;&#20855;&#26377;&#19982;&#35813;&#38382;&#39064;&#30340;&#26368;&#20339;&#29702;&#35770;&#30028;&#38480;&#21305;&#37197;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2011.14033</link><description>&lt;p&gt;
MNL&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#30340;&#31616;&#20415;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Tractable Online Learning Algorithm for the Multinomial Logit Contextual Bandit. (arXiv:2011.14033v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.14033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#19981;&#38656;&#35201;&#35843;&#25972;&#25351;&#25968;&#21442;&#25968;&#30340;MNL-Contextual Bandit&#38382;&#39064;&#30340;&#31616;&#20415;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#31639;&#27861;&#20855;&#26377;&#19982;&#35813;&#38382;&#39064;&#30340;&#26368;&#20339;&#29702;&#35770;&#30028;&#38480;&#21305;&#37197;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;MNL-Bandit&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#21464;&#20307;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21160;&#24577;&#38598;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#32773;&#21521;&#28040;&#36153;&#32773;&#25552;&#20379;&#19968;&#32452;&#20135;&#21697;&#65288;&#36141;&#29289;&#28165;&#21333;&#65289;&#65292;&#24182;&#22312;&#27599;&#20010;&#22238;&#21512;&#35266;&#23519;&#21709;&#24212;&#12290;&#28040;&#36153;&#32773;&#36141;&#20080;&#20135;&#21697;&#20197;&#26368;&#22823;&#21270;&#20182;&#20204;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#20551;&#35774;&#19968;&#32452;&#23646;&#24615;&#25551;&#36848;&#20102;&#20135;&#21697;&#65292;&#20135;&#21697;&#30340;&#24179;&#22343;&#25928;&#29992;&#19982;&#36825;&#20123;&#23646;&#24615;&#30340;&#20540;&#21576;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#20351;&#29992;&#30340;Multinomial Logit&#65288;MNL&#65289;&#27169;&#22411;&#24314;&#27169;&#28040;&#36153;&#32773;&#36873;&#25321;&#34892;&#20026;&#65292;&#24182;&#32771;&#34385;&#22312;&#20248;&#21270;&#38144;&#21806;&#21608;&#26399;$T$&#20869;&#32047;&#31215;&#25910;&#30410;&#30340;&#21516;&#26102;&#21160;&#24577;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#30340;&#20915;&#31574;&#32773;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20010;&#38382;&#39064;&#36817;&#26469;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#20294;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#20182;&#20204;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#21462;&#20915;&#20110;&#19968;&#20010;&#21487;&#33021;&#38750;&#24120;&#22823;&#30340;&#38382;&#39064;&#30456;&#20851;&#21442;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#35843;&#25972;&#38543;&#30528;&#23646;&#24615;&#38598;&#35268;&#27169;&#25351;&#25968;&#22686;&#38271;&#30340;&#35843;&#25972;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MNL-Contextual Bandit&#38382;&#39064;&#30340;&#31616;&#20415;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#35843;&#25972;&#27492;&#31867;&#25351;&#25968;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#19982;&#35813;&#38382;&#39064;&#30340;&#26368;&#20339;&#29702;&#35770;&#30028;&#38480;&#21305;&#37197;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the contextual variant of the MNL-Bandit problem. More specifically, we consider a dynamic set optimization problem, where a decision-maker offers a subset (assortment) of products to a consumer and observes the response in every round. Consumers purchase products to maximize their utility. We assume that a set of attributes describe the products, and the mean utility of a product is linear in the values of these attributes. We model consumer choice behavior using the widely used Multinomial Logit (MNL) model and consider the decision maker problem of dynamically learning the model parameters while optimizing cumulative revenue over the selling horizon $T$. Though this problem has attracted considerable attention in recent times, many existing methods often involve solving an intractable non-convex optimization problem. Their theoretical performance guarantees depend on a problem-dependent parameter which could be prohibitively large. In particular, existing 
&lt;/p&gt;</description></item><item><title>DeepTopPush&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;Accuracy at the Top&#38382;&#39064;&#30340;&#31616;&#21333;&#32780;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22320;&#36873;&#25321;&#23569;&#37327;&#37325;&#35201;&#30340;&#26679;&#26412;&#65292;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;</title><link>http://arxiv.org/abs/2006.12293</link><description>&lt;p&gt;
DeepTopPush: &#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;Accuracy at the Top&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepTopPush: Simple and Scalable Method for Accuracy at the Top. (arXiv:2006.12293v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.12293
&lt;/p&gt;
&lt;p&gt;
DeepTopPush&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;Accuracy at the Top&#38382;&#39064;&#30340;&#31616;&#21333;&#32780;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22320;&#36873;&#25321;&#23569;&#37327;&#37325;&#35201;&#30340;&#26679;&#26412;&#65292;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Accuracy at the top&#26159;&#19968;&#31867;&#29305;&#27530;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#20165;&#22312;&#23569;&#25968;&#30456;&#20851;&#65288;&#39030;&#37096;&#65289;&#26679;&#26412;&#19978;&#35780;&#20272;&#12290;&#24212;&#29992;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#25110;&#38656;&#35201;&#25163;&#21160;&#65288;&#26114;&#36149;&#65289;&#21518;&#22788;&#29702;&#30340;&#24037;&#33402;&#12290;&#36825;&#23548;&#33268;&#26368;&#23567;&#21270;&#36229;&#36807;&#38408;&#20540;&#30340;&#26080;&#20851;&#26679;&#26412;&#25968;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20197;&#20219;&#24847;&#65288;&#28145;&#24230;&#65289;&#32593;&#32476;&#30340;&#24418;&#24335;&#26500;&#24314;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DeepTopPush&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#39030;&#37096;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#30001;&#20110;&#38408;&#20540;&#21462;&#20915;&#20110;&#25152;&#26377;&#26679;&#26412;&#65292;&#22240;&#27492;&#38382;&#39064;&#26159;&#19981;&#21487;&#20998;&#35299;&#30340;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20197;&#22788;&#29702;&#38750;&#21487;&#20998;&#35299;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24403;&#21069;&#36855;&#20320;&#25209;&#27425;&#20540;&#21644;&#19968;&#20010;&#24310;&#36831;&#20540;&#20272;&#35745;&#38408;&#20540;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DeepTopPush&#22312;&#35270;&#35273;&#35782;&#21035;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#30495;&#23454;&#24212;&#29992;&#20013;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;&#31532;&#19968;&#20010;&#24212;&#29992;&#31243;&#24207;&#36873;&#25321;&#23569;&#37327;&#20998;&#23376;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#33647;&#29289;&#27979;&#35797;&#12290;&#31532;&#20108;&#20010;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
Accuracy at the top is a special class of binary classification problems where the performance is evaluated only on a small number of relevant (top) samples. Applications include information retrieval systems or processes with manual (expensive) postprocessing. This leads to minimizing the number of irrelevant samples above a threshold. We consider classifiers in the form of an arbitrary (deep) network and propose a new method DeepTopPush for minimizing the loss function at the top. Since the threshold depends on all samples, the problem is non-decomposable. We modify the stochastic gradient descent to handle the non-decomposability in an end-to-end training manner and propose a way to estimate the threshold only from values on the current minibatch and one delayed value. We demonstrate the excellent performance of DeepTopPush on visual recognition datasets and two real-world applications. The first one selects a small number of molecules for further drug testing. The second one uses r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#35757;&#32451;&#32447;&#24615;SVM&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#26816;&#27979;&#38544;&#34255;&#29305;&#24449;&#21521;&#37327;&#21644;softmax&#29305;&#24449;&#21521;&#37327;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#65292;&#26469;&#35782;&#21035;&#22235;&#31181;&#31867;&#22411;&#30340;&#38169;&#35823;&#25968;&#25454;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#12289;&#25439;&#22351;&#30340;&#12289;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#21644;&#35823;&#20998;&#31867;&#30340;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2002.12520</link><description>&lt;p&gt;
&#21033;&#29992;&#32593;&#32476;&#29305;&#24615;&#26816;&#27979;&#38169;&#35823;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Utilizing Network Properties to Detect Erroneous Inputs. (arXiv:2002.12520v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.12520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#35757;&#32451;&#32447;&#24615;SVM&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#26816;&#27979;&#38544;&#34255;&#29305;&#24449;&#21521;&#37327;&#21644;softmax&#29305;&#24449;&#21521;&#37327;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#65292;&#26469;&#35782;&#21035;&#22235;&#31181;&#31867;&#22411;&#30340;&#38169;&#35823;&#25968;&#25454;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#12289;&#25439;&#22351;&#30340;&#12289;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#21644;&#35823;&#20998;&#31867;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#38169;&#35823;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#12289;&#25439;&#22351;&#30340;&#12289;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#21644;&#35823;&#20998;&#31867;&#30340;&#31034;&#20363;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#29305;&#24449;&#21521;&#37327;&#21644;softmax&#29305;&#24449;&#21521;&#37327;&#65292;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;SVM&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#36825;&#22235;&#31181;&#31867;&#22411;&#30340;&#38169;&#35823;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#38169;&#35823;&#25968;&#25454;&#36890;&#24120;&#34920;&#29616;&#20986;&#19982;&#27491;&#30830;&#31034;&#20363;&#30340;&#32447;&#24615;&#21487;&#20998;&#28608;&#27963;&#23646;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25298;&#32477;&#22351;&#30340;&#36755;&#20837;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#24320;&#38144;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#39046;&#22495;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#23545;&#25239;&#25915;&#20987;&#19978;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are vulnerable to a wide range of erroneous inputs such as adversarial, corrupted, out-of-distribution, and misclassified examples. In this work, we train a linear SVM classifier to detect these four types of erroneous data using hidden and softmax feature vectors of pre-trained neural networks. Our results indicate that these faulty data types generally exhibit linearly separable activation properties from correct examples, giving us the ability to reject bad inputs with no extra training or overhead. We experimentally validate our findings across a diverse range of datasets, domains, pre-trained models, and adversarial attacks.
&lt;/p&gt;</description></item></channel></rss>