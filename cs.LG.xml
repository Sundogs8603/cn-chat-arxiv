<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>Steered Diffusion&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36817;&#26399;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#29983;&#25104;&#25511;&#21046;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.00224</link><description>&lt;p&gt;
Steered Diffusion: &#19968;&#31181;&#24191;&#20041;&#30340;&#25554;&#20214;&#24335;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis. (arXiv:2310.00224v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00224
&lt;/p&gt;
&lt;p&gt;
Steered Diffusion&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36817;&#26399;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#29983;&#25104;&#25511;&#21046;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#35757;&#32451;&#38598;&#25165;&#33021;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#33021;&#22815;&#25191;&#34892;&#25554;&#20214;&#24335;&#21512;&#25104;&#30340;&#27169;&#22411;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#65292;&#21363;&#20351;&#29992;&#39044;&#23450;&#20041;&#25110;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#65288;&#20363;&#22914;&#20351;&#29992;&#35821;&#35328;&#65289;&#65292;&#32780;&#35813;&#27169;&#22411;&#24182;&#27809;&#26377;&#26126;&#30830;&#35757;&#32451;&#22312;&#29983;&#25104;&#20219;&#21153;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25351;&#23548;&#36890;&#24120;&#21482;&#23545;&#21512;&#25104;&#39640;&#32423;&#35821;&#20041;&#26377;&#29992;&#65292;&#32780;&#19981;&#26159;&#32534;&#36753;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#20013;&#30340;&#32454;&#31890;&#24230;&#32454;&#33410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#20511;&#21161;&#26368;&#36817;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#30340;&#24378;&#22823;&#32454;&#31890;&#24230;&#29983;&#25104;&#25511;&#21046;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Steered Diffusion&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#20026;&#26080;&#26465;&#20214;&#29983;&#25104;&#32780;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#36924;&#30495;&#30340;&#38646;&#26679;&#26412;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#35774;&#35745;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#36870;&#27169;&#22411;&#25439;&#22833;&#26469;&#22312;&#25512;&#29702;&#26102;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional generative models typically demand large annotated training sets to achieve high-quality synthesis. As a result, there has been significant interest in designing models that perform plug-and-play generation, i.e., to use a predefined or pretrained model, which is not explicitly trained on the generative task, to guide the generative process (e.g., using language). However, such guidance is typically useful only towards synthesizing high-level semantics rather than editing fine-grained details as in image-to-image translation tasks. To this end, and capitalizing on the powerful fine-grained generative control offered by the recent diffusion-based generative models, we introduce Steered Diffusion, a generalized framework for photorealistic zero-shot conditional image generation using a diffusion model trained for unconditional generation. The key idea is to steer the image generation of the diffusion model at inference time via designing a loss using a pre-trained inverse mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#38544;&#24615;&#24378;&#30423;&#35774;&#32622;&#21644;&#19981;&#21516;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#35780;&#20272;&#20102;&#38544;&#31169;&#21644;&#25512;&#33616;&#22120;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20026;&#23450;&#21046;&#38544;&#31169;&#25216;&#26415;&#30340;&#38656;&#27714;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20010;&#20307;&#29992;&#25143;&#30340;&#25968;&#25454;&#35760;&#24405;&#28155;&#21152;&#25289;&#26222;&#25289;&#26031;&#26426;&#21046;&#30340;&#22122;&#22768;&#26159;&#19981;&#21512;&#36866;&#30340;&#36873;&#25321;&#65292;&#23427;&#22312;&#20219;&#20309;&#22122;&#22768;&#27700;&#24179;&#19979;&#37117;&#20250;&#20135;&#29983;&#26368;&#22823;&#30340;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2310.00221</link><description>&lt;p&gt;
&#36229;&#36234;&#38543;&#26426;&#22122;&#22768;&#65306;&#36890;&#36807;&#38544;&#24615;&#24378;&#30423;&#30740;&#31350;&#27934;&#23519;&#21311;&#21517;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Beyond Random Noise: Insights on Anonymization Strategies from a Latent Bandit Study. (arXiv:2310.00221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#38544;&#24615;&#24378;&#30423;&#35774;&#32622;&#21644;&#19981;&#21516;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#35780;&#20272;&#20102;&#38544;&#31169;&#21644;&#25512;&#33616;&#22120;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20026;&#23450;&#21046;&#38544;&#31169;&#25216;&#26415;&#30340;&#38656;&#27714;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20010;&#20307;&#29992;&#25143;&#30340;&#25968;&#25454;&#35760;&#24405;&#28155;&#21152;&#25289;&#26222;&#25289;&#26031;&#26426;&#21046;&#30340;&#22122;&#22768;&#26159;&#19981;&#21512;&#36866;&#30340;&#36873;&#25321;&#65292;&#23427;&#22312;&#20219;&#20309;&#22122;&#22768;&#27700;&#24179;&#19979;&#37117;&#20250;&#20135;&#29983;&#26368;&#22823;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#20849;&#20139;&#30693;&#35782;&#36827;&#34892;&#25512;&#33616;&#20219;&#21153;&#30340;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#22686;&#21152;&#20102;&#36129;&#29486;&#65292;&#24182;&#24378;&#35843;&#20102;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#25915;&#20987;&#27169;&#24335;&#32780;&#38750;&#20381;&#36182;&#19968;&#20992;&#20999;&#35299;&#20915;&#26041;&#26696;&#30340;&#23450;&#21046;&#38544;&#31169;&#25216;&#26415;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#38544;&#24615;&#24378;&#30423;&#35774;&#32622;&#26469;&#35780;&#20272;&#38544;&#31169;&#21644;&#25512;&#33616;&#22120;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#32858;&#21512;&#31574;&#30053;&#65292;&#22914;&#24179;&#22343;&#12289;&#26368;&#36817;&#37051;&#21644;&#32858;&#31867;&#32467;&#21512;&#22122;&#22768;&#27880;&#20837;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#19968;&#20010;&#21033;&#29992;&#23545;&#25163;&#25910;&#38598;&#30340;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#36741;&#21161;&#20449;&#24687;&#36827;&#34892;&#38142;&#25509;&#25915;&#20987;&#30340;&#24773;&#26223;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24320;&#25918;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20010;&#20307;&#29992;&#25143;&#30340;&#25968;&#25454;&#35760;&#24405;&#28155;&#21152;&#25289;&#26222;&#25289;&#26031;&#26426;&#21046;&#30340;&#22122;&#22768;&#26159;&#19968;&#20010;&#31967;&#31957;&#30340;&#36873;&#25321;&#12290;&#23427;&#30456;&#23545;&#20110;&#21435;&#21311;&#21517;&#21270;&#27010;&#29575;&#21644;ADS&#24230;&#37327;&#26469;&#35828;&#65292;&#22312;&#20219;&#20309;&#22122;&#22768;&#27700;&#24179;&#19979;&#37117;&#25552;&#20379;&#20102;&#26368;&#22823;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the issue of privacy in a learning scenario where users share knowledge for a recommendation task. Our study contributes to the growing body of research on privacy-preserving machine learning and underscores the need for tailored privacy techniques that address specific attack patterns rather than relying on one-size-fits-all solutions. We use the latent bandit setting to evaluate the trade-off between privacy and recommender performance by employing various aggregation strategies, such as averaging, nearest neighbor, and clustering combined with noise injection. More specifically, we simulate a linkage attack scenario leveraging publicly available auxiliary information acquired by the adversary. Our results on three open real-world datasets reveal that adding noise using the Laplace mechanism to an individual user's data record is a poor choice. It provides the highest regret for any noise level, relative to de-anonymization probability and the ADS metric. Inst
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2310.00212</link><description>&lt;p&gt;
&#20004;&#20004;&#37051;&#36817;&#31574;&#30053;&#20248;&#21270;: &#21033;&#29992;&#30456;&#23545;&#21453;&#39304;&#36827;&#34892;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#39044;&#20808;&#35757;&#32451;&#26469;&#33719;&#21462;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25509;&#35302;&#21040;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;LLMs&#21487;&#33021;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#19981;&#19968;&#33268;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#24341;&#23548;LLMs&#26397;&#30528;&#26377;&#30410;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#30340;&#20027;&#23548;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20854;&#20013;Proximal Policy Optimization&#65288;PPO&#65289;&#26159;&#40664;&#35748;&#30340;RL&#20248;&#21270;&#22120;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;PPO&#22312;&#20248;&#21270;&#22522;&#20110;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#38656;&#35201;&#26657;&#20934;&#22870;&#21169;&#23610;&#24230;&#65292;PPO&#23545;&#20110;&#21253;&#21547;&#30456;&#21516;&#20559;&#22909;&#20449;&#24687;&#30340;&#31561;&#20215;&#22870;&#21169;&#20989;&#25968;&#19981;&#20855;&#22791;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#36712;&#36857;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;PPO&#23545;&#20110;&#22522;&#20110;&#20196;&#29260;&#30340;&#26356;&#26032;&#30340;&#38656;&#27714;&#24341;&#20837;&#20102;&#20989;&#25968;&#36924;&#36817;&#21644;&#31639;&#27861;&#35774;&#35745;&#26041;&#38754;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#30456;&#23545;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;Pairwise Proximal Policy Optimization&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pair
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HiCS-FL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#31471;&#30340;&#32593;&#32476;&#36755;&#20986;&#23618;&#26356;&#26032;&#26469;&#20272;&#35745;&#25968;&#25454;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#21033;&#29992;&#27492;&#20449;&#24687;&#36827;&#34892;&#23458;&#25143;&#31471;&#30340;&#32858;&#31867;&#36873;&#25321;&#65292;&#20197;&#21152;&#36895;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.00198</link><description>&lt;p&gt;
&#21033;&#29992;&#24322;&#26500;&#24341;&#23548;&#30340;&#23458;&#25143;&#31471;&#37319;&#26679;&#21152;&#36895;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Non-IID Federated Learning via Heterogeneity-Guided Client Sampling. (arXiv:2310.00198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HiCS-FL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#31471;&#30340;&#32593;&#32476;&#36755;&#20986;&#23618;&#26356;&#26032;&#26469;&#20272;&#35745;&#25968;&#25454;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#21033;&#29992;&#27492;&#20449;&#24687;&#36827;&#34892;&#23458;&#25143;&#31471;&#30340;&#32858;&#31867;&#36873;&#25321;&#65292;&#20197;&#21152;&#36895;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#31471;&#35774;&#22791;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#20351;&#24471;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#20013;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26159;&#65292;&#22312;&#30001;&#20110;&#36164;&#28304;&#38480;&#21046;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#23458;&#25143;&#31471;&#33021;&#21442;&#19982;&#20219;&#20309;&#19968;&#36718;FL&#30340;&#35774;&#32622;&#20013;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#26041;&#27861;&#33268;&#21147;&#20110;&#35757;&#32451;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;FL&#31995;&#32479;&#20013;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#23427;&#20204;&#30528;&#37325;&#20110;&#24320;&#21457;&#37319;&#26679;&#26356;&#20855;&#20449;&#24687;&#26356;&#26032;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#25216;&#26415;&#35201;&#20040;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#35201;&#20040;&#21482;&#22312;&#23458;&#25143;&#31471;&#20855;&#26377;&#31867;&#20284;&#24322;&#36136;&#24615;&#37197;&#32622;&#25991;&#20214;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HiCS-FL&#65288;&#36890;&#36807;&#20998;&#23618;&#32858;&#31867;&#37319;&#26679;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#27861;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#20351;&#29992;&#23458;&#25143;&#31471;&#32593;&#32476;&#36755;&#20986;&#23618;&#30340;&#26356;&#26032;&#26469;&#20272;&#35745;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#20381;&#36182;&#27492;&#20449;&#24687;&#26469;&#36827;&#34892;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical heterogeneity of data present at client devices in a federated learning (FL) system renders the training of a global model in such systems difficult. Particularly challenging are the settings where due to resource constraints only a small fraction of clients can participate in any given round of FL. Recent approaches to training a global model in FL systems with non-IID data have focused on developing client selection methods that aim to sample clients with more informative updates of the model. However, existing client selection techniques either introduce significant computation overhead or perform well only in the scenarios where clients have data with similar heterogeneity profiles. In this paper, we propose HiCS-FL (Federated Learning via Hierarchical Clustered Sampling), a novel client selection method in which the server estimates statistical heterogeneity of a client's data using the client's update of the network's output layer and relies on this information to clu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20004;&#20010;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#22270;&#21367;&#31215;&#21487;&#20197;&#34987;&#35270;&#20026;Mixup&#30340;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#65292;&#23427;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#37117;&#34987;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.00183</link><description>&lt;p&gt;
&#22270;&#21367;&#31215;&#21644;Mixup&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Equivalence of Graph Convolution and Mixup. (arXiv:2310.00183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00183
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20004;&#20010;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#22270;&#21367;&#31215;&#21487;&#20197;&#34987;&#35270;&#20026;Mixup&#30340;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#65292;&#23427;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#37117;&#34987;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#21367;&#31215;&#21644;Mixup&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22270;&#21367;&#31215;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#26159;&#36890;&#36807;&#32858;&#21512;&#37051;&#23621;&#26679;&#26412;&#30340;&#29305;&#24449;&#26469;&#23398;&#20064;&#29305;&#23450;&#33410;&#28857;&#25110;&#26679;&#26412;&#30340;&#20195;&#34920;&#24615;&#29305;&#24449;&#12290;&#32780;Mixup&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#26679;&#26412;&#30340;&#29305;&#24449;&#21644;&#29420;&#28909;&#26631;&#31614;&#36827;&#34892;&#24179;&#22343;&#26469;&#29983;&#25104;&#26032;&#30340;&#31034;&#20363;&#12290;&#36825;&#20004;&#31181;&#25216;&#26415;&#20043;&#38388;&#30340;&#19968;&#20010;&#20849;&#21516;&#20043;&#22788;&#26159;&#23427;&#20204;&#21033;&#29992;&#20102;&#26469;&#33258;&#22810;&#20010;&#26679;&#26412;&#30340;&#20449;&#24687;&#26469;&#24471;&#20986;&#29305;&#24449;&#34920;&#31034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;&#22312;&#20004;&#20010;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#22270;&#21367;&#31215;&#21487;&#20197;&#34987;&#35270;&#20026;Mixup&#30340;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#65292;&#23427;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#37117;&#34987;&#24212;&#29992;&#12290;&#36825;&#20004;&#20010;&#26465;&#20214;&#26159;&#65306;1&#65289;\textit{&#21516;&#36136;&#25913;&#26631;} - &#23558;&#30446;&#26631;&#33410;&#28857;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#25152;&#26377;&#37051;&#23621;&#65292;&#20197;&#21450;2&#65289;\textit{&#27979;&#35797;&#26102;Mixup} - &#22312;&#27979;&#35797;&#26102;&#23545;&#29305;&#24449;&#36827;&#34892;Mixup&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20004;&#20010;&#26465;&#20214;&#30340;&#25968;&#23398;&#34920;&#36798;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20010;&#31561;&#20215;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the relationship between graph convolution and Mixup techniques. Graph convolution in a graph neural network involves aggregating features from neighboring samples to learn representative features for a specific node or sample. On the other hand, Mixup is a data augmentation technique that generates new examples by averaging features and one-hot labels from multiple samples. One commonality between these techniques is their utilization of information from multiple samples to derive feature representation. This study aims to explore whether a connection exists between these two approaches. Our investigation reveals that, under two mild conditions, graph convolution can be viewed as a specialized form of Mixup that is applied during both the training and testing phases. The two conditions are: 1) \textit{Homophily Relabel} - assigning the target node's label to all its neighbors, and 2) \textit{Test-Time Mixup} - Mixup the feature during the test time. We establis
&lt;/p&gt;</description></item><item><title>MARL&#26159;&#19968;&#31181;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#20174;&#24314;&#31569;&#24211;&#20013;&#25552;&#21462;&#20960;&#20309;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#23610;&#24230;&#21306;&#22495;&#30340;&#22478;&#24066;&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#65292;&#20855;&#26377;&#33258;&#21160;&#29983;&#25104;&#12289;&#28789;&#27963;&#36866;&#24212;&#19981;&#21516;&#23610;&#23544;&#24314;&#31569;&#36718;&#24275;&#20197;&#21450;&#20445;&#25345;&#20960;&#20309;&#29305;&#24449;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.00180</link><description>&lt;p&gt;
MARL&#65306;&#22478;&#24066;&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#30340;&#22810;&#23610;&#24230;&#21407;&#22411;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MARL: Multi-scale Archetype Representation Learning for Urban Building Energy Modeling. (arXiv:2310.00180v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00180
&lt;/p&gt;
&lt;p&gt;
MARL&#26159;&#19968;&#31181;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#20174;&#24314;&#31569;&#24211;&#20013;&#25552;&#21462;&#20960;&#20309;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#23610;&#24230;&#21306;&#22495;&#30340;&#22478;&#24066;&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#65292;&#20855;&#26377;&#33258;&#21160;&#29983;&#25104;&#12289;&#28789;&#27963;&#36866;&#24212;&#19981;&#21516;&#23610;&#23544;&#24314;&#31569;&#36718;&#24275;&#20197;&#21450;&#20445;&#25345;&#20960;&#20309;&#29305;&#24449;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#21407;&#22411;&#26159;&#24314;&#31569;&#24211;&#30340;&#20195;&#34920;&#24615;&#27169;&#22411;&#65292;&#22312;&#22478;&#24066;&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#20013;&#38750;&#24120;&#20851;&#38190;&#12290;&#30446;&#21069;&#24191;&#27867;&#37319;&#29992;&#30340;&#24314;&#31569;&#21407;&#22411;&#26159;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#24320;&#21457;&#30340;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#24403;&#22320;&#24314;&#31569;&#29289;&#20960;&#20309;&#29305;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#23610;&#24230;&#21407;&#22411;&#34920;&#31034;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#20174;&#29305;&#23450;&#30340;&#24314;&#31569;&#24211;&#20013;&#25552;&#21462;&#20960;&#20309;&#29305;&#24449;&#12290;MARL&#24314;&#31435;&#22312;VQ-AE&#19978;&#65292;&#23558;&#24314;&#31569;&#29289;&#30340;&#36718;&#24275;&#32534;&#30721;&#65292;&#24182;&#23558;&#20960;&#20309;&#20449;&#24687;&#32431;&#21270;&#20026;&#30001;&#22810;&#20010;&#24314;&#31569;&#23398;&#19978;&#28216;&#20219;&#21153;&#32422;&#26463;&#30340;&#28508;&#22312;&#21521;&#37327;&#12290;&#36825;&#20123;&#23450;&#21046;&#30340;&#34920;&#31034;&#23545;&#36827;&#19968;&#27493;&#30340;&#32858;&#31867;&#21644;&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#20248;&#28857;&#26159;&#23427;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24314;&#31569;&#29289;&#36718;&#24275;&#23610;&#23544;&#65292;&#33021;&#22815;&#22312;&#22810;&#23610;&#24230;&#21306;&#22495;&#33258;&#21160;&#29983;&#25104;&#65292;&#24182;&#33021;&#20445;&#25345;&#37051;&#22495;&#21644;&#26412;&#22320;&#29983;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building archetypes, representative models of building stock, are crucial for precise energy simulations in Urban Building Energy Modeling. The current widely adopted building archetypes are developed on a nationwide scale, potentially neglecting the impact of local buildings' geometric specificities. We present Multi-scale Archetype Representation Learning (MARL), an approach that leverages representation learning to extract geometric features from a specific building stock. Built upon VQ-AE, MARL encodes building footprints and purifies geometric information into latent vectors constrained by multiple architectural downstream tasks. These tailored representations are proven valuable for further clustering and building energy modeling. The advantages of our algorithm are its adaptability with respect to the different building footprint sizes, the ability for automatic generation across multi-scale regions, and the preservation of geometric features across neighborhoods and local ecolo
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36817;&#20284;&#36870;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.00177</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36842;&#37324;&#20999;&#29305;&#21644;&#35834;&#26364;&#36793;&#30028;&#26465;&#20214;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#27850;&#26494;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions. (arXiv:2310.00177v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00177
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36817;&#20284;&#36870;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#39044;&#22788;&#29702;&#30340;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#20855;&#26377;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#12290;&#27850;&#26494;&#26041;&#31243;&#22312;&#31185;&#23398;&#35745;&#31639;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65306;&#23427;&#25511;&#21046;&#30528;&#24191;&#27867;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#22312;&#35768;&#22810;&#25968;&#20540;&#31639;&#27861;&#20013;&#20316;&#20026;&#23376;&#38382;&#39064;&#20986;&#29616;&#65292;&#24182;&#19988;&#20316;&#20026;&#26356;&#24191;&#27867;&#30340;&#26925;&#22278;PDE&#31867;&#30340;&#27169;&#22411;&#38382;&#39064;&#12290;&#26368;&#27969;&#34892;&#30340;&#27850;&#26494;&#31163;&#25955;&#21270;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#22823;&#22411;&#31232;&#30095;&#32447;&#24615;&#31995;&#32479;&#12290;&#22312;&#39640;&#20998;&#36776;&#29575;&#21644;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#65292;&#36845;&#20195;&#27714;&#35299;&#22120;&#32467;&#21512;&#24378;&#22823;&#30340;&#39044;&#22788;&#29702;&#22120;&#21487;&#20197;&#25552;&#20379;&#20248;&#21183;&#12290;&#25105;&#20204;&#27714;&#35299;&#22120;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#36817;&#20284;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#36870;&#31639;&#23376;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#24418;&#29366;&#30340;&#22495;&#21644;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#38382;&#39064;&#30340;&#32467;&#26500;&#28608;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#65292;&#35813;&#26550;&#26500;&#20063;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#39044;&#22788;&#29702;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26696;&#20363;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a neural-preconditioned iterative solver for Poisson equations with mixed boundary conditions. The Poisson equation is ubiquitous in scientific computing: it governs a wide array of physical phenomena, arises as a subproblem in many numerical algorithms, and serves as a model problem for the broader class of elliptic PDEs. The most popular Poisson discretizations yield large sparse linear systems. At high resolution, and for performance-critical applications, iterative solvers can be advantageous for these -- but only when paired with powerful preconditioners. The core of our solver is a neural network trained to approximate the inverse of a discrete structured-grid Laplace operator for a domain of arbitrary shape and with mixed boundary conditions. The structure of this problem motivates a novel network architecture that we demonstrate is highly effective as a preconditioner even for boundary conditions outside the training set. We show that on challenging test cases aris
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20307;&#31215;&#36328;&#24230;&#22120;&#30340;&#32039;&#33268;&#30028;&#38480;&#21644;&#24212;&#29992;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#20110;&#25152;&#26377; $\ell_p$ &#33539;&#25968;&#30340;&#20960;&#20046;&#26368;&#20248;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#32467;&#26524;&#22312;&#23547;&#25214;&#26368;&#23567;&#20307;&#31215;&#21253;&#22260;&#26925;&#29699;&#65288;MVEE&#65289;&#38382;&#39064;&#30340;&#26680;&#24515;&#38598;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.00175</link><description>&lt;p&gt;
&#20307;&#31215;&#36328;&#24230;&#22120;&#21644;&#24212;&#29992;&#30340;&#32039;&#33268;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Tight Bounds for Volumetric Spanners and Applications. (arXiv:2310.00175v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20307;&#31215;&#36328;&#24230;&#22120;&#30340;&#32039;&#33268;&#30028;&#38480;&#21644;&#24212;&#29992;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#20110;&#25152;&#26377; $\ell_p$ &#33539;&#25968;&#30340;&#20960;&#20046;&#26368;&#20248;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#32467;&#26524;&#22312;&#23547;&#25214;&#26368;&#23567;&#20307;&#31215;&#21253;&#22260;&#26925;&#29699;&#65288;MVEE&#65289;&#38382;&#39064;&#30340;&#26680;&#24515;&#38598;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#32452;&#24863;&#20852;&#36259;&#30340;&#28857;&#65292;&#20307;&#31215;&#36328;&#24230;&#22120;&#26159;&#20351;&#29992;&#20854;&#20013;&#30340;&#19968;&#20010;&#23376;&#38598;&#21487;&#20197;&#29992;&#8220;&#23567;&#30340;&#8221;&#31995;&#25968;&#65288;&#22312;&#36866;&#24403;&#30340;&#33539;&#25968;&#20013;&#24230;&#37327;&#65289;&#34920;&#31034;&#25152;&#26377;&#28857;&#30340;&#26041;&#27861;&#12290;&#24418;&#24335;&#19978;&#65292;&#32473;&#23450;&#19968;&#32452;&#21521;&#37327; $X = \{v_1, v_2, \dots, v_n\}$&#65292;&#30446;&#26631;&#26159;&#25214;&#21040; $T \subseteq [n]$&#65292;&#20351;&#24471;&#27599;&#20010; $v \in X$ &#21487;&#20197;&#34920;&#31034;&#20026; $\sum_{i\in T} \alpha_i v_i$&#65292;&#20854;&#20013; $\|\alpha\|$ &#36739;&#23567;&#12290;&#36825;&#20010;&#27010;&#24565;&#65292;&#20063;&#34987;&#31216;&#20026;&#33391;&#22909;&#26465;&#20214;&#30340;&#22522;&#65292;&#24050;&#32463;&#25214;&#21040;&#20102;&#20960;&#20010;&#24212;&#29992;&#65292;&#21253;&#25324;&#36125;&#21494;&#26031;&#32447;&#24615;&#20248;&#21270;&#12289;&#34892;&#21015;&#24335;&#26368;&#22823;&#21270;&#21644;&#30697;&#38453;&#20302;&#31209;&#36924;&#36817;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23545;&#20110;&#25152;&#26377; $\ell_p$ &#33539;&#25968;&#30340;&#20307;&#31215;&#36328;&#24230;&#22120;&#22823;&#23567;&#30340;&#20960;&#20046;&#26368;&#20248;&#30028;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#23616;&#37096;&#25628;&#32034;&#36807;&#31243;&#26500;&#24314;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#32467;&#26524;&#22312;&#20854;&#20182;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#23547;&#25214;&#26368;&#23567;&#20307;&#31215;&#21253;&#22260;&#26925;&#29699;&#65288;MVEE&#65289;&#38382;&#39064;&#30340;&#26680;&#24515;&#38598;&#38382;&#39064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a set of points of interest, a volumetric spanner is a subset of the points using which all the points can be expressed using "small" coefficients (measured in an appropriate norm). Formally, given a set of vectors $X = \{v_1, v_2, \dots, v_n\}$, the goal is to find $T \subseteq [n]$ such that every $v \in X$ can be expressed as $\sum_{i\in T} \alpha_i v_i$, with $\|\alpha\|$ being small. This notion, which has also been referred to as a well-conditioned basis, has found several applications, including bandit linear optimization, determinant maximization, and matrix low rank approximation. In this paper, we give almost optimal bounds on the size of volumetric spanners for all $\ell_p$ norms, and show that they can be constructed using a simple local search procedure. We then show the applications of our result to other tasks and in particular the problem of finding coresets for the Minimum Volume Enclosing Ellipsoid (MVEE) problem.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#25193;&#23637;&#36830;&#25509;&#25351;&#32441;&#65288;ECFP&#65289;&#12289;Avalon&#12289;ErG&#25351;&#32441;&#21644;200&#20010;&#20998;&#23376;&#23646;&#24615;&#30340;&#32452;&#21512;&#65292;&#20197;&#21450;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#31639;&#27861;&#65288;&#23588;&#20854;&#26159;CatBoost&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#25351;&#32441;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#39044;&#27979;&#20102;ADMET&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#26356;&#20016;&#23500;&#30340;&#20998;&#23376;&#34920;&#31034;&#23545;&#20110;&#20934;&#30830;&#30340;&#24615;&#36136;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00174</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23376;&#25351;&#32441;&#32452;&#21512;&#36827;&#34892;ADMET&#24615;&#36136;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ADMET property prediction through combinations of molecular fingerprints. (arXiv:2310.00174v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00174
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25193;&#23637;&#36830;&#25509;&#25351;&#32441;&#65288;ECFP&#65289;&#12289;Avalon&#12289;ErG&#25351;&#32441;&#21644;200&#20010;&#20998;&#23376;&#23646;&#24615;&#30340;&#32452;&#21512;&#65292;&#20197;&#21450;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#31639;&#27861;&#65288;&#23588;&#20854;&#26159;CatBoost&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#25351;&#32441;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#39044;&#27979;&#20102;ADMET&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#26356;&#20016;&#23500;&#30340;&#20998;&#23376;&#34920;&#31034;&#23545;&#20110;&#20934;&#30830;&#30340;&#24615;&#36136;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30740;&#31350;&#23567;&#20998;&#23376;&#25928;&#33021;&#39044;&#27979;&#26041;&#27861;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#25110;&#25903;&#25345;&#21521;&#37327;&#26426;&#19982;&#25193;&#23637;&#36830;&#25509;&#25351;&#32441;&#65288;ECFP&#65289;&#30340;&#32452;&#21512;&#19968;&#33268;&#20248;&#20110;&#26368;&#36817;&#24320;&#21457;&#30340;&#26041;&#27861;&#12290;&#23545;&#22238;&#24402;&#31639;&#27861;&#21644;&#20998;&#23376;&#25351;&#32441;&#30340;&#35814;&#32454;&#35843;&#26597;&#34920;&#26126;&#65292;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#29305;&#21035;&#26159;CatBoost&#65292;&#20197;&#21450;ECFP&#12289;Avalon&#21644;ErG&#25351;&#32441;&#30340;&#32452;&#21512;&#65292;&#20197;&#21450;200&#20010;&#20998;&#23376;&#23646;&#24615;&#65292;&#26368;&#20026;&#26377;&#25928;&#12290;&#24341;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#25351;&#32441;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#22312;22&#20010;&#27835;&#30103;&#25968;&#25454;&#20849;&#20139;&#20013;&#24515;&#30340;ADMET&#22522;&#20934;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#26356;&#20016;&#23500;&#30340;&#20998;&#23376;&#34920;&#31034;&#23545;&#20110;&#20934;&#30830;&#30340;&#24615;&#36136;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While investigating methods to predict small molecule potencies, we found random forests or support vector machines paired with extended-connectivity fingerprints (ECFP) consistently outperformed recently developed methods. A detailed investigation into regression algorithms and molecular fingerprints revealed gradient-boosted decision trees, particularly CatBoost, in conjunction with a combination of ECFP, Avalon, and ErG fingerprints, as well as 200 molecular properties, to be most effective. Incorporating a graph neural network fingerprint further enhanced performance. We successfully validated our model across 22 Therapeutics Data Commons ADMET benchmarks. Our findings underscore the significance of richer molecular representations for accurate property prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Motif&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20132;&#20114;&#26469;&#33719;&#24471;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20195;&#29702;&#31243;&#24207;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Motif&#30340;&#20869;&#22312;&#22870;&#21169;&#30456;&#27604;&#30452;&#25509;&#26368;&#22823;&#21270;&#24471;&#20998;&#30340;&#31639;&#27861;&#22312;&#25361;&#25112;&#24615;&#28216;&#25103;&#20013;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#28216;&#25103;&#24471;&#20998;&#65292;&#24182;&#22312;&#20043;&#21069;&#27809;&#26377;&#21462;&#24471;&#36827;&#23637;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.00166</link><description>&lt;p&gt;
Motif: &#26469;&#33258;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#30340;&#20869;&#22312;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Motif: Intrinsic Motivation from Artificial Intelligence Feedback. (arXiv:2310.00166v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Motif&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20132;&#20114;&#26469;&#33719;&#24471;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20195;&#29702;&#31243;&#24207;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Motif&#30340;&#20869;&#22312;&#22870;&#21169;&#30456;&#27604;&#30452;&#25509;&#26368;&#22823;&#21270;&#24471;&#20998;&#30340;&#31639;&#27861;&#22312;&#25361;&#25112;&#24615;&#28216;&#25103;&#20013;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#28216;&#25103;&#24471;&#20998;&#65292;&#24182;&#22312;&#20043;&#21069;&#27809;&#26377;&#21462;&#24471;&#36827;&#23637;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#25506;&#32034;&#20016;&#23500;&#30340;&#29615;&#22659;&#24182;&#35780;&#20272;&#33258;&#24049;&#30340;&#34892;&#21160;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Motif&#65292;&#19968;&#31181;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#20808;&#39564;&#30693;&#35782;&#19982;&#20195;&#29702;&#31243;&#24207;&#25509;&#21475;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;Motif&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#23558;LLMs&#29992;&#20110;&#20915;&#31574;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65306;&#23427;&#36890;&#36807;&#20174;LLM&#20013;&#20135;&#29983;&#23545;&#37197;&#23545;&#26631;&#39064;&#30340;&#20559;&#22909;&#26469;&#26500;&#24314;&#20869;&#22312;&#22870;&#21169;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#22870;&#21169;&#23545;&#20195;&#29702;&#31243;&#24207;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#12289;&#24320;&#25918;&#24615;&#21644;&#31243;&#24207;&#29983;&#25104;&#30340;NetHack&#28216;&#25103;&#19978;&#35780;&#20272;&#20102;Motif&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20165;&#36890;&#36807;&#23398;&#20064;&#26368;&#22823;&#21270;&#20854;&#20869;&#22312;&#22870;&#21169;&#65292;Motif&#30340;&#28216;&#25103;&#24471;&#20998;&#27604;&#30452;&#25509;&#35757;&#32451;&#20197;&#26368;&#22823;&#21270;&#24471;&#20998;&#30340;&#31639;&#27861;&#26356;&#39640;&#12290;&#24403;&#23558;Motif&#30340;&#20869;&#22312;&#22870;&#21169;&#19982;&#29615;&#22659;&#22870;&#21169;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#20197;&#21069;&#20174;&#26410;&#21462;&#24471;&#36827;&#23637;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring rich environments and evaluating one's actions without prior knowledge is immensely challenging. In this paper, we propose Motif, a general method to interface such prior knowledge from a Large Language Model (LLM) with an agent. Motif is based on the idea of grounding LLMs for decision-making without requiring them to interact with the environment: it elicits preferences from an LLM over pairs of captions to construct an intrinsic reward, which is then used to train agents with reinforcement learning. We evaluate Motif's performance and behavior on the challenging, open-ended and procedurally-generated NetHack game. Surprisingly, by only learning to maximize its intrinsic reward, Motif achieves a higher game score than an algorithm directly trained to maximize the score itself. When combining Motif's intrinsic reward with the environment reward, our method significantly outperforms existing approaches and makes progress on tasks where no advancements have ever been made with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SCoRe&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#22330;&#26223;&#19979;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23376;&#27169;&#32452;&#21512;&#20989;&#25968;&#65292;&#25105;&#20204;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#29305;&#24449;&#31751;&#30340;&#22810;&#26679;&#24615;&#21644;&#21512;&#20316;&#24615;&#12290;&#36825;&#23545;&#20110;&#20811;&#26381;&#31867;&#21035;&#19981;&#24179;&#34913;&#22312;&#33258;&#20027;&#23548;&#33322;&#21644;&#21307;&#23398;&#35786;&#26029;&#31561;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.00165</link><description>&lt;p&gt;
SCoRe&#65306;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#31867;&#21035;&#19981;&#24179;&#34913;&#22330;&#26223;&#30340;&#23376;&#27169;&#32858;&#21512;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SCoRe: Submodular Combinatorial Representation Learning for Real-World Class-Imbalanced Settings. (arXiv:2310.00165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SCoRe&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#22330;&#26223;&#19979;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23376;&#27169;&#32452;&#21512;&#20989;&#25968;&#65292;&#25105;&#20204;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#29305;&#24449;&#31751;&#30340;&#22810;&#26679;&#24615;&#21644;&#21512;&#20316;&#24615;&#12290;&#36825;&#23545;&#20110;&#20811;&#26381;&#31867;&#21035;&#19981;&#24179;&#34913;&#22312;&#33258;&#20027;&#23548;&#33322;&#21644;&#21307;&#23398;&#35786;&#26029;&#31561;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#28436;&#36827;&#36807;&#31243;&#20013;&#65292;&#30495;&#23454;&#19990;&#30028;&#31867;&#21035;&#19981;&#24179;&#34913;&#22330;&#26223;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#32597;&#35265;&#31867;&#21035;&#30340;&#35270;&#35273;&#21644;&#32467;&#26500;&#29305;&#24449;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#38480;&#21046;&#20102;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26377;&#21306;&#20998;&#24230;&#30340;&#29305;&#24449;&#31751;&#12290;&#36825;&#20307;&#29616;&#20026;&#25968;&#25454;&#38598;&#20013;&#32597;&#35265;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#22823;&#22411;&#31867;&#38388;&#20559;&#24046;&#20197;&#21450;&#20016;&#23500;&#31867;&#21035;&#20043;&#38388;&#30340;&#39640;&#20869;&#31867;&#21464;&#21270;&#12290;&#34429;&#28982;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#22312;&#33258;&#20027;&#23548;&#33322;&#21644;&#21307;&#23398;&#35786;&#26029;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#65292;&#20173;&#38656;&#35201;&#20316;&#20986;&#37325;&#22823;&#25913;&#36827;&#26469;&#20811;&#26381;&#31867;&#21035;&#19981;&#24179;&#34913;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#38598;&#21512;&#30340;&#32452;&#21512;&#20989;&#25968;&#65292;&#22914;&#23376;&#27169;&#20449;&#24687;&#24230;&#37327;&#65292;&#20855;&#26377;&#27169;&#25311;&#29305;&#24449;&#31751;&#22810;&#26679;&#24615;&#21644;&#21512;&#20316;&#24615;&#30340;&#29305;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SCoRe&#65288;&#23376;&#27169;&#32858;&#21512;&#34920;&#31034;&#23398;&#20064;&#65289;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23376;&#27169;C&#23478;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation Learning in real-world class-imbalanced settings has emerged as a challenging task in the evolution of deep learning. Lack of diversity in visual and structural features for rare classes restricts modern neural networks to learn discriminative feature clusters. This manifests in the form of large inter-class bias between rare object classes and elevated intra-class variance among abundant classes in the dataset. Although deep metric learning approaches have shown promise in this domain, significant improvements need to be made to overcome the challenges associated with class-imbalance in mission critical tasks like autonomous navigation and medical diagnostics. Set-based combinatorial functions like Submodular Information Measures exhibit properties that allow them to simultaneously model diversity and cooperation among feature clusters. In this paper, we introduce the SCoRe (Submodular Combinatorial Representation Learning) framework and propose a family of Submodular C
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#26816;&#27979;&#22120;&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#22312;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;40.4&#30340;&#25513;&#30721;AP$_r$&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00161</link><description>&lt;p&gt;
&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detection-Oriented Image-Text Pretraining for Open-Vocabulary Detection. (arXiv:2310.00161v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00161
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#26816;&#27979;&#22120;&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#22312;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;40.4&#30340;&#25513;&#30721;AP$_r$&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#26032;&#30340;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#29992;&#26816;&#27979;&#22120;&#26550;&#26500;&#26367;&#20195;&#24120;&#29992;&#30340;&#20998;&#31867;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#26816;&#27979;&#22120;&#22836;&#37096;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#65292;&#26356;&#22909;&#22320;&#28385;&#36275;&#26816;&#27979;&#30340;&#21306;&#22495;&#32423;&#35782;&#21035;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#26631;&#20934;&#30340;&#23545;&#27604;&#25439;&#22833;&#32780;&#19981;&#20351;&#29992;&#20266;&#26631;&#31614;&#65292;&#26159;&#23545;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31383;&#21475;&#27880;&#24847;&#21147;&#30340;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#26356;&#21152;&#40065;&#26834;&#12289;&#24179;&#31227;&#19981;&#21464;&#65292;&#24182;&#19988;&#19981;&#21463;&#31383;&#21475;&#27169;&#24335;&#30340;&#20559;&#24046;&#24433;&#21709;&#12290;&#22312;&#27969;&#34892;&#30340;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#24120;&#35265;&#30340;ViT-L&#20027;&#24178;&#32593;&#32476;&#21462;&#24471;&#20102;40.4&#30340;&#25513;&#30721;AP$_r$&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new open-vocabulary detection approach based on detection-oriented image-text pretraining to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we replace the commonly used classification architecture with the detector architecture, which better serves the region-level recognition needs of detection by enabling the detector heads to learn from noisy image-text pairs. Using only standard contrastive loss and no pseudo-labeling, our approach is a simple yet effective extension of the contrastive learning method to learn emergent object-semantic cues. In addition, we propose a shifted-window learning approach upon window attention to make the backbone representation more robust, translation-invariant, and less biased by the window pattern. On the popular LVIS open-vocabulary detection benchmark, our approach sets a new state of the art of 40.4 mask AP$_r$ using the common ViT-L backbone, significantly outperforming t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#65292;&#23558;&#38745;&#24577;&#25968;&#25454;&#38598;&#22686;&#24378;&#20026;&#21253;&#21547;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00158</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#20998;&#31867;&#20013;&#30340;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Feedback-guided Data Synthesis for Imbalanced Classification. (arXiv:2310.00158v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#65292;&#23558;&#38745;&#24577;&#25968;&#25454;&#38598;&#22686;&#24378;&#20026;&#21253;&#21547;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29616;&#29366;&#26159;&#20351;&#29992;&#26469;&#33258;&#38271;&#23614;&#20998;&#24067;&#30340;&#30495;&#23454;&#22270;&#20687;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#29992;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#36825;&#20123;&#38745;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#25253;&#21578;&#20102;&#36866;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#24615;&#33021;&#25552;&#21319;&#21463;&#21040;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#19981;&#36275;&#30340;&#38480;&#21046;&#65292;&#36825;&#23558;&#20419;&#36827;&#29983;&#25104;&#26679;&#26412;&#30340;&#26377;&#29992;&#24615;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#22686;&#24378;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#12290;&#20026;&#20102;&#20351;&#35813;&#26694;&#26550;&#26377;&#25928;&#65292;&#25105;&#20204;&#21457;&#29616;&#26679;&#26412;&#24517;&#39035;&#25509;&#36817;&#25163;&#22836;&#20219;&#21153;&#30340;&#30495;&#23454;&#25968;&#25454;&#25903;&#25345;&#65292;&#24182;&#19988;&#20855;&#26377;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38271;&#23614;&#25968;&#25454;&#38598;&#65288;ImageNe...&#19978;&#39564;&#35777;&#20102;&#19977;&#20010;&#21453;&#39304;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current status quo in machine learning is to use static datasets of real images for training, which often come from long-tailed distributions. With the recent advances in generative models, researchers have started augmenting these static datasets with synthetic data, reporting moderate performance improvements on classification tasks. We hypothesize that these performance gains are limited by the lack of feedback from the classifier to the generative model, which would promote the usefulness of the generated samples to improve the classifier's performance. In this work, we introduce a framework for augmenting static datasets with useful synthetic samples, which leverages one-shot feedback from the classifier to drive the sampling of the generative model. In order for the framework to be effective, we find that the samples must be close to the support of the real data of the task at hand, and be sufficiently diverse. We validate three feedback criteria on a long-tailed dataset (ImageNe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21407;&#22987;-&#23545;&#20598;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#35299;&#20915;&#21463;&#38480;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#20998;&#26512;&#20219;&#21153;&#23618;&#38754;&#21644;&#26679;&#26412;&#23618;&#38754;&#30340;&#32422;&#26463;&#65292;&#22312;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#20013;&#20998;&#37197;&#36164;&#28304;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00154</link><description>&lt;p&gt;
&#21407;&#22987;-&#23545;&#20598;&#25345;&#32493;&#23398;&#20064;&#65306;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#23454;&#29616;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers. (arXiv:2310.00154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21407;&#22987;-&#23545;&#20598;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#35299;&#20915;&#21463;&#38480;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#20998;&#26512;&#20219;&#21153;&#23618;&#38754;&#21644;&#26679;&#26412;&#23618;&#38754;&#30340;&#32422;&#26463;&#65292;&#22312;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#20013;&#20998;&#37197;&#36164;&#28304;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#22266;&#26377;&#22320;&#26159;&#19968;&#20010;&#21463;&#38480;&#23398;&#20064;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#22312;&#8220;&#26080;&#36951;&#24536;&#8221;&#35201;&#27714;&#19979;&#23398;&#20064;&#19968;&#20010;&#39044;&#27979;&#22120;&#12290;&#23613;&#31649;&#20043;&#21069;&#26377;&#20960;&#39033;&#30740;&#31350;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#26126;&#30830;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#26159;&#21487;&#34892;&#19988;&#26377;&#30410;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#38480;&#21046;&#24615;&#23398;&#20064;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21487;&#20197;&#23558;&#20808;&#21069;&#20219;&#21153;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#26679;&#26412;&#23384;&#20648;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#30340;&#20004;&#20010;&#29256;&#26412;&#65306;&#19968;&#20010;&#22312;&#20219;&#21153;&#23618;&#38754;&#19978;&#26377;&#32422;&#26463;&#30340;&#31895;&#31961;&#26041;&#27861;&#21644;&#19968;&#20010;&#22312;&#26679;&#26412;&#23618;&#38754;&#19978;&#26377;&#32422;&#26463;&#30340;&#31934;&#32454;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20598;&#21464;&#37327;&#25351;&#31034;&#20102;&#26368;&#20248;&#20540;&#23545;&#20110;&#32422;&#26463;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#22312;&#31895;&#31961;&#26041;&#27861;&#20013;&#23545;&#32531;&#20914;&#21306;&#36827;&#34892;&#20102;&#21010;&#20998;&#65292;&#23558;&#26356;&#22810;&#36164;&#28304;&#20998;&#37197;&#32473;&#26356;&#38590;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning is inherently a constrained learning problem. The goal is to learn a predictor under a \emph{no-forgetting} requirement. Although several prior studies formulate it as such, they do not solve the constrained problem explicitly. In this work, we show that it is both possible and beneficial to undertake the constrained optimization problem directly. To do this, we leverage recent results in constrained learning through Lagrangian duality. We focus on memory-based methods, where a small subset of samples from previous tasks can be stored in a replay buffer. In this setting, we analyze two versions of the continual learning problem: a coarse approach with constraints at the task level and a fine approach with constraints at the sample level. We show that dual variables indicate the sensitivity of the optimal value with respect to constraint perturbations. We then leverage this result to partition the buffer in the coarse approach, allocating more resources to harder task
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#29992;&#21333;&#19968;&#22270;&#27169;&#22411;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#22270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#19981;&#21516;&#23646;&#24615;&#21644;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#12289;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00149</link><description>&lt;p&gt;
&#19968;&#20992;&#20999;&#65306;&#21521;&#33021;&#22815;&#35757;&#32451;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#21333;&#19968;&#22270;&#27169;&#22411;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
One for All: Towards Training One Graph Model for All Classification Tasks. (arXiv:2310.00149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00149
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#29992;&#21333;&#19968;&#22270;&#27169;&#22411;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#22270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#19981;&#21516;&#23646;&#24615;&#21644;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#12289;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#30340;&#21333;&#19968;&#27169;&#22411;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#38271;&#26399;&#30446;&#26631;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#35821;&#35328;&#39046;&#22495;&#20869;&#25972;&#21512;&#21644;&#35299;&#20915;&#19981;&#21516;&#20219;&#21153;&#30340;&#24322;&#24120;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#23398;&#20064;&#39046;&#22495;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#30340;&#32479;&#19968;&#27169;&#22411;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22270;&#23398;&#20064;&#39046;&#22495;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#23646;&#24615;&#21644;&#36981;&#24490;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#24046;&#24322;&#20351;&#24471;&#24456;&#38590;&#23558;&#22270;&#34920;&#31034;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#12290;&#20854;&#27425;&#65292;&#22270;&#19978;&#30340;&#20219;&#21153;&#20998;&#21270;&#20026;&#33410;&#28857;&#12289;&#38142;&#25509;&#21644;&#22270;&#20219;&#21153;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#23884;&#20837;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36866;&#24403;&#22270;&#25552;&#31034;&#33539;&#24335;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"&#19968;&#20992;&#20999;"&#65288;OFA&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20351;&#29992;&#21333;&#19968;&#22270;&#27169;&#22411;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing a single model that addresses multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in integrating and solving different tasks within the language domain. However, a unified model for various tasks on graphs remains underexplored, primarily due to the challenges unique to the graph learning domain. First, graph data from different areas carry distinct attributes and follow different distributions. Such discrepancy makes it hard to represent graphs in a single representation space. Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies. Finally, an appropriate graph prompting paradigm for in-context learning is unclear. Striving to handle all the aforementioned challenges, we propose One for All (OFA), the first general framework that can use a single graph model to address the above challenges. Specifically, OFA proposes text-at
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#37319;&#26679;&#22686;&#24378;&#30340;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#20197;&#22826;&#22346;&#32593;&#32476;&#20013;&#30340;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#19982;&#26102;&#24577;&#38543;&#26426;&#28216;&#36208;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#26356;&#31934;&#32454;&#30340;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#22312;&#26816;&#27979;&#24322;&#24120;&#21644;&#20132;&#26131;&#31361;&#21457;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#20197;&#22826;&#22346;&#20132;&#26131;&#25968;&#25454;&#20013;&#26102;&#38388;&#32447;&#32034;&#30340;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#35813;&#26694;&#26550;&#36827;&#34892;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00144</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#37319;&#26679;&#22686;&#24378;&#30340;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65306;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20197;&#22826;&#22346;&#32593;&#32476;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Sampling-Enhanced Temporal-Spatial GCN: A Scalable Framework for Transaction Anomaly Detection in Ethereum Networks. (arXiv:2310.00144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#37319;&#26679;&#22686;&#24378;&#30340;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#20197;&#22826;&#22346;&#32593;&#32476;&#20013;&#30340;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#19982;&#26102;&#24577;&#38543;&#26426;&#28216;&#36208;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#26356;&#31934;&#32454;&#30340;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#22312;&#26816;&#27979;&#24322;&#24120;&#21644;&#20132;&#26131;&#31361;&#21457;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#20197;&#22826;&#22346;&#20132;&#26131;&#25968;&#25454;&#20013;&#26102;&#38388;&#32447;&#32034;&#30340;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#35813;&#26694;&#26550;&#36827;&#34892;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#22826;&#22346;&#32593;&#32476;&#30340;&#24555;&#36895;&#28436;&#36827;&#38656;&#35201;&#20808;&#36827;&#30340;&#25216;&#26415;&#26469;&#30830;&#20445;&#20854;&#23545;&#28508;&#22312;&#23041;&#32961;&#30340;&#40065;&#26834;&#24615;&#24182;&#20445;&#25345;&#36879;&#26126;&#24230;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#27492;&#31867;&#24179;&#21488;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#20808;&#23548;&#24615;&#25104;&#26524;&#65292;&#20294;&#25429;&#25417;&#31354;&#38388;&#21644;&#26102;&#38388;&#20107;&#21153;&#27169;&#24335;&#30340;&#22797;&#26434;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#19982;&#20351;&#29992;&#27010;&#29575;&#37319;&#26679;&#22686;&#24378;&#30340;&#26102;&#24577;&#38543;&#26426;&#28216;&#36208;&#65288;TRW&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#19982;&#20256;&#32479;&#30340;GCNs&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;TRW&#30340;&#20248;&#21183;&#26469;&#35782;&#21035;&#20197;&#22826;&#22346;&#20132;&#26131;&#20013;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#20837;&#24494;&#30340;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#26426;&#21046;&#12290;&#21021;&#27493;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;TRW-GCN&#26694;&#26550;&#22312;&#26816;&#27979;&#24322;&#24120;&#21644;&#20132;&#26131;&#31361;&#21457;&#30340;&#24615;&#33021;&#25351;&#26631;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#20256;&#32479;GCNs&#30340;&#34920;&#29616;&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#24378;&#35843;&#20102;&#20197;&#22826;&#22346;&#20132;&#26131;&#25968;&#25454;&#20013;&#26102;&#38388;&#32447;&#32034;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20351;&#29992;&#27010;&#29575;&#37319;&#26679;&#22686;&#24378;&#30340;&#26102;&#31354;GCNs&#36827;&#34892;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid evolution of the Ethereum network necessitates sophisticated techniques to ensure its robustness against potential threats and to maintain transparency. While Graph Neural Networks (GNNs) have pioneered anomaly detection in such platforms, capturing the intricacies of both spatial and temporal transactional patterns has remained a challenge. This study presents a fusion of Graph Convolutional Networks (GCNs) with Temporal Random Walks (TRW) enhanced by probabilistic sampling to bridge this gap. Our approach, unlike traditional GCNs, leverages the strengths of TRW to discern complex temporal sequences in Ethereum transactions, thereby providing a more nuanced transaction anomaly detection mechanism. Preliminary evaluations demonstrate that our TRW-GCN framework substantially advances the performance metrics over conventional GCNs in detecting anomalies and transaction bursts. This research not only underscores the potential of temporal cues in Ethereum transactional data but a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#36827;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#36890;&#29992;&#26041;&#27861;&#65288;GASS&#65289;&#65292;&#22312;&#26377;&#38480;&#20998;&#24067;&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#22768;&#38899;&#20107;&#20214;&#21644;&#35821;&#38899;&#20998;&#31163;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31163;&#36229;&#20986;&#20998;&#24067;&#30340;&#30005;&#24433;&#21644;&#38899;&#20048;&#20869;&#23481;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.00140</link><description>&lt;p&gt;
GASS&#65306;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#36827;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#27867;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GASS: Generalizing Audio Source Separation with Large-scale Data. (arXiv:2310.00140v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#36827;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#36890;&#29992;&#26041;&#27861;&#65288;GASS&#65289;&#65292;&#22312;&#26377;&#38480;&#20998;&#24067;&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#22768;&#38899;&#20107;&#20214;&#21644;&#35821;&#38899;&#20998;&#31163;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31163;&#36229;&#20986;&#20998;&#24067;&#30340;&#30005;&#24433;&#21644;&#38899;&#20048;&#20869;&#23481;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#28304;&#20998;&#31163;&#30340;&#30446;&#26631;&#26159;&#20998;&#31163;&#20219;&#24847;&#28151;&#21512;&#38899;&#39057;&#20013;&#30340;&#38899;&#39057;&#28304;&#65292;&#28040;&#38500;&#20165;&#25805;&#20316;&#20110;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;&#35821;&#38899;&#25110;&#38899;&#20048;&#65289;&#30340;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#28304;&#20998;&#31163;&#30340;&#28508;&#21147;&#21463;&#38480;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#20851;&#27880;&#20855;&#26377;&#20027;&#35201;&#22768;&#38899;&#20107;&#20214;&#30340;&#28151;&#21512;&#20197;&#21450;&#23567;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#38480;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20197;&#26377;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#30340;&#21333;&#19968;&#36890;&#29992;&#38899;&#39057;&#28304;&#20998;&#31163;&#65288;GASS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20998;&#31163;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#22768;&#38899;&#20107;&#20214;&#12290;&#25105;&#20204;&#23545;GASS&#27169;&#22411;&#36827;&#34892;&#20102;&#22810;&#26679;&#21270;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#24378;&#26377;&#21147;&#20998;&#24067;&#32467;&#26524;&#26174;&#31034;&#20102;GASS&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#22312;&#22768;&#38899;&#20107;&#20214;&#21644;&#35821;&#38899;&#20998;&#31163;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#21017;&#26174;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;GASS&#27169;&#22411;&#22312;&#20998;&#31163;&#36229;&#20986;&#20998;&#24067;&#30340;&#30005;&#24433;&#21644;&#38899;&#20048;&#20869;&#23481;&#26041;&#38754;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#23545;GASS&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#22987;&#32456;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Universal source separation targets at separating the audio sources of an arbitrary mix, removing the constraint to operate on a specific domain like speech or music. Yet, the potential of universal source separation is limited because most existing works focus on mixes with predominantly sound events, and small training datasets also limit its potential for supervised learning. Here, we study a single general audio source separation (GASS) model trained to separate speech, music, and sound events in a supervised fashion with a large-scale dataset. We assess GASS models on a diverse set of tasks. Our strong in-distribution results show the feasibility of GASS models, and the competitive out-of-distribution performance in sound event and speech separation shows its generalization abilities. Yet, it is challenging for GASS models to generalize for separating out-of-distribution cinematic and music content. We also fine-tune GASS models on each dataset and consistently outperform the ones
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#31351;&#23485;&#24230;&#26497;&#38480;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#19982;&#26680;&#26041;&#27861;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#34429;&#28982;&#22312;&#21512;&#25104;&#26550;&#26500;&#20013;&#23637;&#31034;&#20102;&#19968;&#20123;&#20248;&#21183;&#65292;&#22914;&#26356;&#24555;&#30340;&#20248;&#21270;&#21644;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20294;&#23454;&#38469;&#30456;&#20851;&#30340;&#26550;&#26500;&#38656;&#35201;&#27604;&#28145;&#24230;&#22823;&#24456;&#22810;&#20493;&#30340;&#23485;&#24230;&#25165;&#33021;&#23454;&#29616;&#36825;&#20123;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.00137</link><description>&lt;p&gt;
&#20851;&#20110;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#29702;&#35770;&#19982;&#23454;&#36341;&#30340;&#33073;&#33410;
&lt;/p&gt;
&lt;p&gt;
On the Disconnect Between Theory and Practice of Overparametrized Neural Networks. (arXiv:2310.00137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#31351;&#23485;&#24230;&#26497;&#38480;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#19982;&#26680;&#26041;&#27861;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#34429;&#28982;&#22312;&#21512;&#25104;&#26550;&#26500;&#20013;&#23637;&#31034;&#20102;&#19968;&#20123;&#20248;&#21183;&#65292;&#22914;&#26356;&#24555;&#30340;&#20248;&#21270;&#21644;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20294;&#23454;&#38469;&#30456;&#20851;&#30340;&#26550;&#26500;&#38656;&#35201;&#27604;&#28145;&#24230;&#22823;&#24456;&#22810;&#20493;&#30340;&#23485;&#24230;&#25165;&#33021;&#23454;&#29616;&#36825;&#20123;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#26080;&#31351;&#23485;&#24230;&#26497;&#38480;&#20316;&#20026;&#20998;&#26512;&#22823;&#35268;&#27169;&#12289;&#36807;&#21442;&#25968;&#21270;&#32593;&#32476;&#34892;&#20026;&#30340;&#29702;&#35770;&#26694;&#26550;&#24050;&#32463;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#12290;&#36890;&#36807;&#25509;&#36817;&#26080;&#38480;&#23485;&#24230;&#65292;NNs&#21487;&#20197;&#26377;&#25928;&#22320;&#25910;&#25947;&#21040;&#19968;&#20010;&#20855;&#26377;&#30001;&#31070;&#32463;&#20999;&#32447;&#26680;(NTK)&#29305;&#24449;&#21270;&#30340;&#32447;&#24615;&#27169;&#22411;&#12290;&#36825;&#24314;&#31435;&#20102;NNs&#21644;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#21518;&#32773;&#26159;&#34987;&#20805;&#20998;&#29702;&#35299;&#30340;&#12290;&#22522;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#24050;&#32463;&#20551;&#35774;&#24182;&#22312;&#21512;&#25104;&#26550;&#26500;&#20013;&#20174;&#29702;&#35770;&#19978;&#21644;&#31639;&#27861;&#19978;&#39564;&#35777;&#20102;&#19968;&#20123;&#20248;&#21183;&#12290;&#36825;&#20123;&#20248;&#21183;&#21253;&#25324;&#26356;&#24555;&#30340;&#20248;&#21270;&#12289;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#25913;&#36827;&#30340;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#37327;&#21270;&#21521;&#26680;&#24515;&#39046;&#22495;&#25910;&#25947;&#36895;&#24230;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#36825;&#20123;&#20248;&#21183;&#38656;&#35201;&#27604;&#28145;&#24230;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#26550;&#26500;&#12290;&#36825;&#20010;&#20551;&#35774;&#24341;&#21457;&#20102;&#23545;&#23454;&#38469;&#30456;&#20851;&#26550;&#26500;&#26159;&#21542;&#34920;&#29616;&#22914;&#39044;&#27979;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
The infinite-width limit of neural networks (NNs) has garnered significant attention as a theoretical framework for analyzing the behavior of large-scale, overparametrized networks. By approaching infinite width, NNs effectively converge to a linear model with features characterized by the neural tangent kernel (NTK). This establishes a connection between NNs and kernel methods, the latter of which are well understood. Based on this link, theoretical benefits and algorithmic improvements have been hypothesized and empirically demonstrated in synthetic architectures. These advantages include faster optimization, reliable uncertainty quantification and improved continual learning. However, current results quantifying the rate of convergence to the kernel regime suggest that exploiting these benefits requires architectures that are orders of magnitude wider than they are deep. This assumption raises concerns that practically relevant architectures do not exhibit behavior as predicted via 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#37325;&#32593;&#26684;&#24352;&#37327;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;MG-TFNO&#65289;&#30340;&#26032;&#22411;&#25968;&#25454;&#26377;&#25928;&#19988;&#39640;&#24230;&#24182;&#34892;&#21270;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#30340;&#20998;&#35299;&#26469;&#25193;&#23637;&#33267;&#22823;&#23610;&#24230;&#30340;&#20998;&#36776;&#29575;&#12290;&#20854;&#21019;&#26032;&#21253;&#25324;&#22810;&#37325;&#32593;&#26684;&#30340;&#22495;&#20998;&#35299;&#12289;&#22312;&#20613;&#37324;&#21494;&#22495;&#20013;&#30340;&#39640;&#38454;&#28508;&#22312;&#23376;&#31354;&#38388;&#34920;&#31034;&#21442;&#25968;&#20197;&#21450;&#23545;&#26550;&#26500;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.00120</link><description>&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#22810;&#37325;&#32593;&#26684;&#24352;&#37327;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Multi-Grid Tensorized Fourier Neural Operator for High-Resolution PDEs. (arXiv:2310.00120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#37325;&#32593;&#26684;&#24352;&#37327;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;MG-TFNO&#65289;&#30340;&#26032;&#22411;&#25968;&#25454;&#26377;&#25928;&#19988;&#39640;&#24230;&#24182;&#34892;&#21270;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#30340;&#20998;&#35299;&#26469;&#25193;&#23637;&#33267;&#22823;&#23610;&#24230;&#30340;&#20998;&#36776;&#29575;&#12290;&#20854;&#21019;&#26032;&#21253;&#25324;&#22810;&#37325;&#32593;&#26684;&#30340;&#22495;&#20998;&#35299;&#12289;&#22312;&#20613;&#37324;&#21494;&#22495;&#20013;&#30340;&#39640;&#38454;&#28508;&#22312;&#23376;&#31354;&#38388;&#34920;&#31034;&#21442;&#25968;&#20197;&#21450;&#23545;&#26550;&#26500;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20869;&#23384;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#38459;&#30861;&#20102;&#23398;&#20064;&#39640;&#20998;&#36776;&#29575;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#31639;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#26377;&#25928;&#19988;&#39640;&#24230;&#24182;&#34892;&#21270;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#38477;&#20302;&#20102;&#20869;&#23384;&#38656;&#27714;&#24182;&#25913;&#36827;&#20102;&#27867;&#21270;&#24615;&#33021;&#65292;&#31216;&#20026;&#22810;&#37325;&#32593;&#26684;&#24352;&#37327;&#21270;&#31070;&#32463;&#31639;&#23376;&#65288;MG-TFNO&#65289;&#12290;MG-TFNO&#36890;&#36807;&#21033;&#29992;&#23436;&#25972;&#19990;&#30028;&#29616;&#35937;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#36890;&#36807;&#36755;&#20837;&#22495;&#21644;&#31639;&#23376;&#21442;&#25968;&#31354;&#38388;&#30340;&#20998;&#35299;&#26469;&#25193;&#23637;&#21040;&#22823;&#23610;&#24230;&#30340;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;i&#65289;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#37325;&#32593;&#26684;&#30340;&#22495;&#20998;&#35299;&#23454;&#29616;&#20102;&#23545;&#36755;&#20837;&#26679;&#26412;&#30340;&#24182;&#34892;&#21270;&#65292;ii&#65289;&#25105;&#20204;&#36890;&#36807;&#20613;&#37324;&#21494;&#22495;&#20013;&#30340;&#39640;&#38454;&#28508;&#22312;&#23376;&#31354;&#38388;&#23558;&#27169;&#22411;&#21442;&#25968;&#34920;&#31034;&#65292;&#36890;&#36807;&#20840;&#23616;&#24352;&#37327;&#20998;&#35299;&#65292;&#22823;&#22823;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#24182;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;iii&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;b&#30340;&#26550;&#26500;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory complexity and data scarcity have so far prohibited learning solution operators of partial differential equations (PDEs) at high resolutions. We address these limitations by introducing a new data efficient and highly parallelizable operator learning approach with reduced memory requirement and better generalization, called multi-grid tensorized neural operator (MG-TFNO). MG-TFNO scales to large resolutions by leveraging local and global structures of full-scale, real-world phenomena, through a decomposition of both the input domain and the operator's parameter space. Our contributions are threefold: i) we enable parallelization over input samples with a novel multi-grid-based domain decomposition, ii) we represent the parameters of the model in a high-order latent subspace of the Fourier domain, through a global tensor factorization, resulting in an extreme reduction in the number of parameters and improved generalization, and iii) we propose architectural improvements to the b
&lt;/p&gt;</description></item><item><title>ABScribe&#26159;&#19968;&#31181;&#30028;&#38754;&#65292;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#24555;&#36895;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20197;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.00117</link><description>&lt;p&gt;
ABScribe: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models. (arXiv:2310.00117v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00117
&lt;/p&gt;
&lt;p&gt;
ABScribe&#26159;&#19968;&#31181;&#30028;&#38754;&#65292;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#24555;&#36895;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20197;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#20070;&#20889;&#25991;&#26412;&#26469;&#25506;&#32034;&#26367;&#20195;&#24819;&#27861;&#26159;&#20889;&#20316;&#36807;&#31243;&#30340;&#20851;&#38190;&#12290;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#31616;&#21270;&#20889;&#20316;&#21464;&#21270;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30028;&#38754;&#23384;&#22312;&#21516;&#26102;&#32771;&#34385;&#22810;&#31181;&#21464;&#21270;&#30340;&#25361;&#25112;&#65306;&#22312;&#19981;&#35206;&#30422;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#26032;&#30340;&#29256;&#26412;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#32780;&#25353;&#39034;&#24207;&#31896;&#36148;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;&#25991;&#26723;&#21464;&#24471;&#26434;&#20081;&#65292;&#22686;&#21152;&#24037;&#20316;&#37327;&#65292;&#24182;&#25171;&#26029;&#20316;&#32773;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ABScribe&#65292;&#19968;&#31181;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#19988;&#32467;&#26500;&#21270;&#22320;&#25506;&#32034;&#20889;&#20316;&#21464;&#21270;&#30340;&#30028;&#38754;&#12290;&#36890;&#36807;ABScribe&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;LLM&#25552;&#31034;&#24555;&#36895;&#20135;&#29983;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20250;&#33258;&#21160;&#36716;&#25442;&#25104;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#12290;&#21464;&#20307;&#22312;&#25991;&#26412;&#27573;&#33853;&#20013;&#34987;&#23384;&#20648;&#22312;&#30456;&#37051;&#20301;&#32622;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#19978;&#30340;&#40736;&#26631;&#24748;&#20572;&#20132;&#20114;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;12&#21517;&#25776;&#20889;&#20154;&#21592;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;ABScribe&#33021;&#26174;&#33879;&#20943;&#36731;&#20219;&#21153;&#36127;&#33655;&#65288;d = 1.20, p &lt; 0.001&#65289;&#65292;&#25552;&#39640;&#29992;&#25143;&#30340;&#35748;&#30693;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p &lt; 0.001), enhances user perceptions o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21644;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00116</link><description>&lt;p&gt;
&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization. (arXiv:2310.00116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21644;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20363;&#22914;&#35774;&#35745;&#20855;&#26377;&#26356;&#22909;&#40065;&#26834;&#24615;&#24615;&#36136;&#30340;&#26032;&#26550;&#26500;&#65288;&#20363;&#22914;&#65292;Lipschitz-capped&#32593;&#32476;&#65289;&#25110;&#20462;&#25913;&#35757;&#32451;&#36807;&#31243;&#26412;&#36523;&#65288;&#20363;&#22914;&#65292;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#65292;&#32422;&#26463;&#23398;&#20064;&#25110;&#27491;&#21017;&#21270;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#22686;&#21152;&#36755;&#20837;&#65288;&#29305;&#24449;&#65289;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21487;&#33021;&#24182;&#19981;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#23545;&#24320;&#21457;&#33021;&#22815;&#30452;&#25509;&#25805;&#32437;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#30340;&#35757;&#32451;&#36807;&#31243;&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#35813;&#31867;&#21035;&#30340;&#26368;&#26032;&#21457;&#23637;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#40065;&#26834;&#35757;&#32451;&#31639;&#27861;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#36755;&#20986;&#65288;logit&#65289;&#31354;&#38388;&#20013;&#22686;&#21152;&#36793;&#30028;&#65292;&#24182;&#27839;&#30528;&#33030;&#24369;&#26041;&#21521;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20004;&#20010;&#30446;&#26631;&#21487;&#20197;&#30452;&#25509;&#20419;&#36827;&#36755;&#20837;&#31354;&#38388;&#20013;&#26356;&#22823;&#30340;&#36793;&#30028;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
To improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., Lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). These approaches, however, might not be effective at increasing the margin in the input (feature) space. As a result, there has been an increasing interest in developing training procedures that can directly manipulate the decision boundary in the input space. In this paper, we build upon recent developments in this category by developing a robust training algorithm whose objective is to increase the margin in the output (logit) space while regularizing the Lipschitz constant of the model along vulnerable directions. We show that these two objectives can directly promote larger margins in the input space. To this end, we develop a scalable method for calcula
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;MoleculAR Conformer Ensemble Learning&#65288;MARCEL&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22312;&#26500;&#35937;&#38598;&#21512;&#19978;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.00115</link><description>&lt;p&gt;
&#23398;&#20064;&#20998;&#23376;&#26500;&#35937;&#38598;&#21512;&#65306;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks. (arXiv:2310.00115v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;MoleculAR Conformer Ensemble Learning&#65288;MARCEL&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22312;&#26500;&#35937;&#38598;&#21512;&#19978;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#37238;&#35774;&#35745;&#31561;&#20247;&#22810;&#29983;&#29289;&#21270;&#23398;&#24212;&#29992;&#20013;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#24448;&#24448;&#24573;&#30053;&#20102;&#20998;&#23376;&#30340;&#28789;&#27963;&#24615;&#65292;&#20998;&#23376;&#36890;&#36807;&#21270;&#23398;&#38190;&#26059;&#36716;&#21644;&#24494;&#23567;&#25391;&#21160;&#25200;&#21160;&#19981;&#26029;&#22312;&#26500;&#35937;&#20043;&#38388;&#30456;&#20114;&#36716;&#21270;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#32771;&#34385;&#20998;&#23376;&#30340;&#28789;&#27963;&#24615;&#65292;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#23558;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#23450;&#20041;&#20026;&#19968;&#20010;&#38598;&#21512;&#23398;&#20064;&#38382;&#39064;&#65292;&#19987;&#27880;&#20110;&#20174;&#19968;&#32452;&#26500;&#35937;&#32467;&#26500;&#20013;&#26126;&#30830;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#22312;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;MoleculAR Conformer Ensemble Learning&#65288;MARCEL&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22312;&#26500;&#35937;&#38598;&#21512;&#19978;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;MARCEL&#21253;&#25324;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#22810;&#26679;&#30340;&#20998;&#23376;&#21644;&#21453;&#24212;&#27700;&#24179;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular Representation Learning (MRL) has proven impactful in numerous biochemical applications such as drug discovery and enzyme design. While Graph Neural Networks (GNNs) are effective at learning molecular representations from a 2D molecular graph or a single 3D structure, existing works often overlook the flexible nature of molecules, which continuously interconvert across conformations via chemical bond rotations and minor vibrational perturbations. To better account for molecular flexibility, some recent works formulate MRL as an ensemble learning problem, focusing on explicitly learning from a set of conformer structures. However, most of these studies have limited datasets, tasks, and models. In this work, we introduce the first MoleculAR Conformer Ensemble Learning (MARCEL) benchmark to thoroughly evaluate the potential of learning on conformer ensembles and suggest promising research directions. MARCEL includes four datasets covering diverse molecule- and reaction-level pro
&lt;/p&gt;</description></item><item><title>HyperMask&#26159;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00113</link><description>&lt;p&gt;
HyperMask: &#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning. (arXiv:2310.00113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00113
&lt;/p&gt;
&lt;p&gt;
HyperMask&#26159;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#39034;&#24207;&#35757;&#32451;&#26102;&#65292;&#24448;&#24448;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#23384;&#22312;&#35768;&#22810;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#26368;&#26377;&#25928;&#30340;&#20043;&#19968;&#26159;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36229;&#32593;&#32476;&#26681;&#25454;&#20219;&#21153;&#30340;&#29305;&#24449;&#29983;&#25104;&#30446;&#26631;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#20027;&#35201;&#38480;&#21046;&#26159;&#36229;&#32593;&#32476;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#21487;&#20197;&#20135;&#29983;&#23436;&#20840;&#19981;&#21516;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#22240;&#27492;&#27599;&#20010;&#20219;&#21153;&#37117;&#26159;&#21333;&#29420;&#35299;&#20915;&#30340;&#12290;&#27169;&#22411;&#22312;&#23398;&#20064;&#21518;&#32493;&#20219;&#21153;&#26102;&#19981;&#20351;&#29992;&#20043;&#21069;&#20219;&#21153;&#25152;&#20851;&#32852;&#30340;&#32593;&#32476;&#20449;&#24687;&#65292;&#24182;&#23454;&#38469;&#19978;&#20135;&#29983;&#20102;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#24425;&#31080;&#31080;&#35777;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#35748;&#20026;&#23384;&#22312;&#31232;&#30095;&#30340;&#23376;&#32593;&#32476;&#65288;&#21363;&#20013;&#22870;&#31080;&#65289;&#65292;&#21487;&#20197;&#20445;&#25345;&#23436;&#25972;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperMask&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#12290;&#36229;&#32593;&#32476;&#20135;&#29983;&#21322;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#20197;&#33719;&#21462;&#30446;&#26631;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, there exist many continual learning strategies. One of the most effective is the hypernetwork-based approach. The hypernetwork generates the weights of a target model based on the task's identity. The model's main limitation is that hypernetwork can produce completely different nests for each task. Consequently, each task is solved separately. The model does not use information from the network dedicated to previous tasks and practically produces new architectures when it learns the subsequent tasks. To solve such a problem, we use the lottery ticket hypothesis, which postulates the existence of sparse subnetworks, named winning tickets, that preserve the performance of a full network.  In the paper, we propose a method called HyperMask, which trains a single network for all tasks. Hypernetwork produces semi-binary masks to obtain target subnetw
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33410;&#28857;&#36873;&#25321;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#27169;&#25311;&#25972;&#20010;&#26641;&#30340;&#29366;&#24577;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#26469;&#36873;&#25321;&#33410;&#28857;&#12290;&#23613;&#31649;&#21482;&#22312;&#21512;&#25104;TSP&#23454;&#20363;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22797;&#26434;&#38382;&#39064;&#38598;&#19978;&#24471;&#21040;&#20102;&#39640;&#36136;&#37327;&#30340;&#33410;&#28857;&#36873;&#25321;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.00112</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#20013;&#30340;&#33410;&#28857;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Node Selection in Branch-and-Bound. (arXiv:2310.00112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33410;&#28857;&#36873;&#25321;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#27169;&#25311;&#25972;&#20010;&#26641;&#30340;&#29366;&#24577;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#26469;&#36873;&#25321;&#33410;&#28857;&#12290;&#23613;&#31649;&#21482;&#22312;&#21512;&#25104;TSP&#23454;&#20363;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22797;&#26434;&#38382;&#39064;&#38598;&#19978;&#24471;&#21040;&#20102;&#39640;&#36136;&#37327;&#30340;&#33410;&#28857;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#20174;&#25628;&#32034;&#26641;&#20013;&#30830;&#23450;&#26368;&#20248;&#33410;&#28857;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#22120;&#35201;&#20040;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#38598;&#21512;&#65292;&#33258;&#21160;&#20999;&#25442;&#20026;&#22825;&#30495;&#30340;&#23376;&#33410;&#28857;&#36873;&#25321;&#22120;&#65292;&#35201;&#20040;&#20351;&#29992;&#20381;&#36182;&#20110;&#20010;&#21035;&#33410;&#28857;&#25968;&#25454;&#30340;&#23398;&#20064;&#33410;&#28857;&#36873;&#25321;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#27169;&#25311;&#25216;&#26415;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#32771;&#34385;&#25972;&#20010;&#26641;&#29366;&#24577;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#23396;&#31435;&#30340;&#33410;&#28857;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#26681;&#25454;&#27169;&#22411;&#20174;&#26681;&#33410;&#28857;&#21040;&#8220;&#24453;&#36873;&#25321;&#8221;&#21494;&#23376;&#33410;&#28857;&#30340;&#36335;&#24452;&#20135;&#29983;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#12290;&#23558;&#33410;&#28857;&#36873;&#25321;&#24314;&#27169;&#20026;&#27010;&#29575;&#20998;&#24067;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#22312;&#33410;&#28857;&#36136;&#37327;&#21644;&#33410;&#28857;&#35780;&#20272;&#25104;&#26412;&#12290;&#23613;&#31649;&#21482;&#26159;&#22312;&#19987;&#38376;&#35774;&#35745;&#30340;&#21512;&#25104;TSP&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#32452;&#22810;&#26679;&#19988;&#22797;&#26434;&#30340;&#38382;&#39064;&#38598;&#19978;&#24341;&#20986;&#20102;&#39640;&#36136;&#37327;&#30340;&#33410;&#28857;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
A big challenge in branch and bound lies in identifying the optimal node within the search tree from which to proceed. Current state-of-the-art selectors utilize either hand-crafted ensembles that automatically switch between naive sub-node selectors, or learned node selectors that rely on individual node data. We propose a novel bi-simulation technique that uses reinforcement learning (RL) while considering the entire tree state, rather than just isolated nodes. To achieve this, we train a graph neural network that produces a probability distribution based on the path from the model's root to its ``to-be-selected'' leaves. Modelling node-selection as a probability distribution allows us to train the model using state-of-the-art RL techniques that capture both intrinsic node-quality and node-evaluation costs. Our method induces a high quality node selection policy on a set of varied and complex problem sets, despite only being trained on specially designed, synthetic TSP instances. Exp
&lt;/p&gt;</description></item><item><title>GUESS is a new sampling strategy for global fit that combines predictive posterior uncertainty and higher-order Taylor expansion values to reduce the number of samples needed for accurate surrogate modeling. - GUESS &#26159;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#25311;&#21512;&#37319;&#26679;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;&#39044;&#27979;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#21644;&#39640;&#38454;&#27888;&#21202;&#23637;&#24320;&#20540;&#65292;&#21487;&#20197;&#20943;&#23569;&#20934;&#30830;&#30340;&#20195;&#29702;&#27169;&#22411;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.00110</link><description>&lt;p&gt;
Gradient and Uncertainty Enhanced Sequential Sampling for Global Fit. (arXiv:2310.00110v1 [stat.ML]) - &#20840;&#23616;&#25311;&#21512;&#20013;&#30340;&#26799;&#24230;&#21644;&#19981;&#30830;&#23450;&#24615;&#22686;&#24378;&#30340;&#39034;&#24207;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Gradient and Uncertainty Enhanced Sequential Sampling for Global Fit. (arXiv:2310.00110v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00110
&lt;/p&gt;
&lt;p&gt;
GUESS is a new sampling strategy for global fit that combines predictive posterior uncertainty and higher-order Taylor expansion values to reduce the number of samples needed for accurate surrogate modeling. - GUESS &#26159;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#25311;&#21512;&#37319;&#26679;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;&#39044;&#27979;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#21644;&#39640;&#38454;&#27888;&#21202;&#23637;&#24320;&#20540;&#65292;&#21487;&#20197;&#20943;&#23569;&#20934;&#30830;&#30340;&#20195;&#29702;&#27169;&#22411;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20195;&#29702;&#27169;&#22411;&#24050;&#25104;&#20026;&#29616;&#20195;&#24037;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#29992;&#20197;&#21462;&#20195;&#26114;&#36149;&#30340;&#35745;&#31639;&#26426;&#27169;&#25311;&#12290;&#21019;&#24314;&#20195;&#29702;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#23545;&#20110;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#21463;&#21040;&#25104;&#26412;&#21644;&#26102;&#38388;&#38480;&#21046;&#30340;&#38480;&#21046;&#12290;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#21019;&#24314;&#20934;&#30830;&#27169;&#22411;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#25311;&#21512;&#37319;&#26679;&#31574;&#30053;&#65292;&#31216;&#20026;Gradient and Uncertainty Enhanced Sequential Sampling (GUESS)&#12290;&#37319;&#29992;&#20004;&#20010;&#26415;&#35821;&#30340;&#25910;&#36141;&#21151;&#33021;&#65306;&#29992;&#20110;&#25506;&#32034;&#26410;&#35265;&#21306;&#22495;&#30340;&#20195;&#29702;&#27169;&#22411;&#30340;&#39044;&#27979;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#21450;&#29992;&#20110;&#24320;&#21457;&#30340;&#20108;&#38454;&#21450;&#26356;&#39640;&#38454;&#27888;&#21202;&#23637;&#24320;&#20540;&#30340;&#21152;&#26435;&#36924;&#36817;&#20540;&#12290;&#23613;&#31649;&#36804;&#20170;&#20026;&#27490;&#24050;&#25552;&#20986;&#20102;&#21508;&#31181;&#37319;&#26679;&#31574;&#30053;&#65292;&#20294;&#36873;&#25321;&#21512;&#36866;&#30340;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#31574;&#30053;&#19982;&#22522;&#20110;26&#20010;&#19981;&#21516;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;9&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate models based on machine learning methods have become an important part of modern engineering to replace costly computer simulations. The data used for creating a surrogate model are essential for the model accuracy and often restricted due to cost and time constraints. Adaptive sampling strategies have been shown to reduce the number of samples needed to create an accurate model. This paper proposes a new sampling strategy for global fit called Gradient and Uncertainty Enhanced Sequential Sampling (GUESS). The acquisition function uses two terms: the predictive posterior uncertainty of the surrogate model for exploration of unseen regions and a weighted approximation of the second and higher-order Taylor expansion values for exploitation. Although various sampling strategies have been proposed so far, the selection of a suitable method is not trivial. Therefore, we compared our proposed strategy to 9 adaptive sampling strategies for global surrogate modeling, based on 26 diff
&lt;/p&gt;</description></item><item><title>FedAIoT&#26159;&#19968;&#20010;&#29992;&#20110;AIoT&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#65292;&#21253;&#25324;&#20843;&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;FL&#26694;&#26550;&#12290;&#23427;&#22635;&#34917;&#20102;&#29616;&#26377;FL&#30740;&#31350;&#20013;&#32570;&#20047;&#30495;&#23454;&#29289;&#32852;&#32593;&#35774;&#22791;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#24046;&#36317;&#65292;&#24182;&#25581;&#31034;&#20102;FL&#22312;AIoT&#39046;&#22495;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.00109</link><description>&lt;p&gt;
FedAIoT: &#19968;&#31181;&#29992;&#20110;&#29289;&#32852;&#32593;&#20154;&#24037;&#26234;&#33021;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things. (arXiv:2310.00109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00109
&lt;/p&gt;
&lt;p&gt;
FedAIoT&#26159;&#19968;&#20010;&#29992;&#20110;AIoT&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#65292;&#21253;&#25324;&#20843;&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;FL&#26694;&#26550;&#12290;&#23427;&#22635;&#34917;&#20102;&#29616;&#26377;FL&#30740;&#31350;&#20013;&#32570;&#20047;&#30495;&#23454;&#29289;&#32852;&#32593;&#35774;&#22791;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#24046;&#36317;&#65292;&#24182;&#25581;&#31034;&#20102;FL&#22312;AIoT&#39046;&#22495;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20855;&#26377;&#37325;&#35201;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FL&#30740;&#31350;&#24182;&#19981;&#26159;&#22522;&#20110;&#20174;&#30495;&#23454;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#25429;&#25417;&#20102;&#29289;&#32852;&#32593;&#25968;&#25454;&#30340;&#29420;&#29305;&#27169;&#24335;&#21644;&#22266;&#26377;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedAIoT&#65292;&#19968;&#31181;&#29992;&#20110;AIoT&#30340;FL&#22522;&#20934;&#65292;&#20197;&#22635;&#34917;&#36825;&#20010;&#20851;&#38190;&#30340;&#24046;&#36317;&#12290;FedAIoT&#21253;&#25324;&#20174;&#21508;&#31181;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#30340;&#20843;&#20010;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#29289;&#32852;&#32593;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#24182;&#38024;&#23545;AIoT&#30340;&#20856;&#22411;&#24212;&#29992;&#12290;FedAIoT&#36824;&#21253;&#25324;&#19968;&#31181;&#29992;&#20110;AIoT&#30340;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;FL&#26694;&#26550;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#38598;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#25581;&#31034;&#20102;FL&#22312;AIoT&#39046;&#22495;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#24076;&#26395;FedAIoT&#33021;&#25104;&#20026;&#22312;FL for AIoT&#36825;&#19968;&#37325;&#35201;&#39046;&#22495;&#25512;&#21160;&#36827;&#23637;&#30340;&#29645;&#36149;&#36164;&#28304;&#12290;FedAIoT&#30340;&#20195;&#30721;&#20179;&#24211;&#20301;&#20110;https://github.com/AIoT-MLSys-Lab/FedAIoT&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, an FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. FedAIoT also includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope FedAIoT could serve as an invaluable resource to foster advancements in the important field of FL for AIoT. The repository of FedAIoT is maintained at https://github.com/AIoT-MLSys-Lab/FedAIoT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#28857;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#36827;&#34892;&#38408;&#20540;&#22788;&#29702;&#65292;&#24182;&#36890;&#36807;&#32858;&#21512;&#30456;&#20284;&#24230;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00108</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#65306;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models: A Pilot Study. (arXiv:2310.00108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#28857;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#36827;&#34892;&#38408;&#20540;&#22788;&#29702;&#65292;&#24182;&#36890;&#36807;&#32858;&#21512;&#30456;&#20284;&#24230;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIAs&#65289;&#26088;&#22312;&#25512;&#26029;&#25968;&#25454;&#28857;&#26159;&#21542;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#25915;&#20987;&#21487;&#29992;&#20110;&#35782;&#21035;&#28508;&#22312;&#30340;&#38544;&#31169;&#28431;&#27934;&#21644;&#26816;&#27979;&#20010;&#20154;&#25968;&#25454;&#30340;&#26410;&#32463;&#25480;&#26435;&#20351;&#29992;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#30740;&#31350;&#30340;&#26159;&#38024;&#23545;&#31616;&#21333;&#20998;&#31867;&#27169;&#22411;&#30340;MIAs&#65292;&#20294;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65288;&#22914;CLIP&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#30340;&#38750;&#24120;&#22909;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#35268;&#27169;&#32473;&#25191;&#34892;&#36825;&#20123;&#25915;&#20987;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#24320;&#21457;&#38024;&#23545;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23454;&#38469;MIAs&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#30446;&#26631;&#28857;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#36827;&#34892;&#38408;&#20540;&#22788;&#29702;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#23545;&#30446;&#26631;&#30340;&#21464;&#25442;&#36827;&#34892;&#20313;&#24358;&#30456;&#20284;&#24230;&#32858;&#21512;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#22522;&#20934;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#25915;&#20987;&#26041;&#27861;&#65292;&#23545;&#27169;&#22411;&#36827;&#34892;&#27867;&#21270;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference attacks (MIAs) aim to infer whether a data point has been used to train a machine learning model. These attacks can be employed to identify potential privacy vulnerabilities and detect unauthorized use of personal data. While MIAs have been traditionally studied for simple classification models, recent advancements in multi-modal pre-training, such as CLIP, have demonstrated remarkable zero-shot performance across a range of computer vision tasks. However, the sheer scale of data and models presents significant computational challenges for performing the attacks.  This paper takes a first step towards developing practical MIAs against large-scale multi-modal models. We introduce a simple baseline strategy by thresholding the cosine similarity between text and image features of a target point and propose further enhancing the baseline by aggregating cosine similarity across transformations of the target. We also present a new weakly supervised attack method that lev
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#28508;&#31354;&#38388;LieGAN&#65288;LaLiGAN&#65289;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#23545;&#31216;&#24615;&#65292;&#24182;&#20135;&#29983;&#32467;&#26500;&#33391;&#22909;&#30340;&#28508;&#31354;&#38388;&#65292;&#23545;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.00105</link><description>&lt;p&gt;
&#28508;&#31354;&#38388;&#30340;&#23545;&#31216;&#24615;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Latent Space Symmetry Discovery. (arXiv:2310.00105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00105
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#28508;&#31354;&#38388;LieGAN&#65288;LaLiGAN&#65289;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#23545;&#31216;&#24615;&#65292;&#24182;&#20135;&#29983;&#32467;&#26500;&#33391;&#22909;&#30340;&#28508;&#31354;&#38388;&#65292;&#23545;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#26126;&#30830;&#30693;&#36947;&#23545;&#31216;&#32676;&#12290;&#33258;&#21160;&#23545;&#31216;&#24615;&#21457;&#29616;&#26041;&#27861;&#26088;&#22312;&#25918;&#23485;&#36825;&#20010;&#32422;&#26463;&#65292;&#24182;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#31216;&#24615;&#21457;&#29616;&#26041;&#27861;&#22312;&#25628;&#32034;&#31354;&#38388;&#20013;&#20165;&#38480;&#20110;&#32447;&#24615;&#23545;&#31216;&#24615;&#65292;&#26080;&#27861;&#22788;&#29702;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#23545;&#31216;&#24615;&#22797;&#26434;&#24615;&#65292;&#23588;&#20854;&#26159;&#39640;&#32500;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#28508;&#31354;&#38388;LieGAN&#65288;LaLiGAN&#65289;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#23545;&#31216;&#24615;&#12290;&#23427;&#23398;&#20064;&#20102;&#19968;&#31181;&#20174;&#25968;&#25454;&#21040;&#28508;&#31354;&#38388;&#30340;&#26144;&#23556;&#65292;&#22312;&#20854;&#20013;&#23545;&#31216;&#24615;&#21464;&#24471;&#32447;&#24615;&#65292;&#24182;&#21516;&#26102;&#21457;&#29616;&#28508;&#31354;&#38388;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#34920;&#31034;&#20219;&#20309;&#38750;&#32447;&#24615;&#23545;&#31216;&#24615;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#39640;&#32500;&#35266;&#27979;&#20013;&#30340;&#20869;&#22312;&#23545;&#31216;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#32467;&#26500;&#33391;&#22909;&#30340;&#28508;&#31354;&#38388;&#65292;&#23545;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;LaLiGAN&#22312;&#25913;&#36827;&#26041;&#31243;&#21457;&#29616;&#26041;&#38754;&#30340;&#24212;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivariant neural networks require explicit knowledge of the symmetry group. Automatic symmetry discovery methods aim to relax this constraint and learn invariance and equivariance from data. However, existing symmetry discovery methods are limited to linear symmetries in their search space and cannot handle the complexity of symmetries in real-world, often high-dimensional data. We propose a novel generative model, Latent LieGAN (LaLiGAN), which can discover nonlinear symmetries from data. It learns a mapping from data to a latent space where the symmetries become linear and simultaneously discovers symmetries in the latent space. Theoretically, we show that our method can express any nonlinear symmetry under certain conditions. Experimentally, our method can capture the intrinsic symmetry in high-dimensional observations, which results in a well-structured latent space that is useful for other downstream tasks. We demonstrate the use cases for LaLiGAN in improving equation discovery
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#22823;&#22411;Transformer&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#24314;&#31435;&#20102;&#22522;&#32447;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00098</link><description>&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Differential Privacy for End-to-End Speech Recognition. (arXiv:2310.00098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#22823;&#22411;Transformer&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#24314;&#31435;&#20102;&#22522;&#32447;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#20165;&#38480;&#20110;&#21021;&#27493;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#32852;&#37030;&#23398;&#20064;&#19981;&#33021;&#26412;&#36136;&#19978;&#20445;&#35777;&#29992;&#25143;&#38544;&#31169;&#65292;&#24182;&#38656;&#35201;&#24046;&#20998;&#38544;&#31169;&#26469;&#25552;&#20379;&#31283;&#20581;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#19981;&#28165;&#26970;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20026;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#24046;&#20998;&#38544;&#31169;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#22522;&#20934;&#65292;&#24182;&#24314;&#31435;&#31532;&#19968;&#20010;&#22522;&#32447;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#26368;&#26032;&#30340;&#22823;&#22411;&#31471;&#21040;&#31471;Transformer&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#38754;&#65306;&#26550;&#26500;&#35774;&#35745;&#65292;&#31181;&#23376;&#27169;&#22411;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#39046;&#22495;&#36716;&#31227;&#65292;&#20197;&#21450;cohort&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#21512;&#29702;&#30340;&#20013;&#22830;&#32858;&#21512;&#25968;&#37327;&#65292;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#20986;&#21363;&#20351;&#22312;&#24322;&#26500;&#25968;&#25454;&#12289;&#26469;&#33258;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#31181;&#23376;&#27169;&#22411;&#25110;&#26080;&#39044;&#20808;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#25509;&#36817;&#26368;&#20248;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
While federated learning (FL) has recently emerged as a promising approach to train machine learning models, it is limited to only preliminary explorations in the domain of automatic speech recognition (ASR). Moreover, FL does not inherently guarantee user privacy and requires the use of differential privacy (DP) for robust privacy guarantees. However, we are not aware of prior work on applying DP to FL for ASR. In this paper, we aim to bridge this research gap by formulating an ASR benchmark for FL with DP and establishing the first baselines. First, we extend the existing research on FL for ASR by exploring different aspects of recent $\textit{large end-to-end transformer models}$: architecture design, seed models, data heterogeneity, domain shift, and impact of cohort size. With a $\textit{practical}$ number of central aggregations we are able to train $\textbf{FL models}$ that are \textbf{nearly optimal} even with heterogeneous data, a seed model from another domain, or no pre-trai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#23569;&#27425;&#35843;&#29992;&#30340;&#26041;&#27861;&#31363;&#21462;&#40657;&#30418;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#31361;&#30772;&#20102;&#35775;&#38382;&#38480;&#21046;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2310.00096</link><description>&lt;p&gt;
&#36817;&#20284;&#40657;&#30418;&#27169;&#22411;&#31363;&#21462;&#30340;&#23569;&#27425;&#35843;&#29992;&#26041;&#27861;&#65306;&#27963;&#36291;&#33258;&#36866;&#24212;&#30693;&#35782;&#33976;&#39311;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation. (arXiv:2310.00096v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#23569;&#27425;&#35843;&#29992;&#30340;&#26041;&#27861;&#31363;&#21462;&#40657;&#30418;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#31361;&#30772;&#20102;&#35775;&#38382;&#38480;&#21046;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20363;&#65292;&#21363;&#22312;&#27809;&#26377;&#35775;&#38382;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#12289;&#26550;&#26500;&#21644;&#27169;&#22411;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#22797;&#21046;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#65292;&#21363;&#21482;&#33021;&#36890;&#36807;&#25512;&#29702;API&#20351;&#29992;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21482;&#33021;&#35266;&#23519;&#21040;&#19968;&#20123;&#22270;&#20687;&#26679;&#26412;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#27169;&#22411;&#26102;&#30340;&#65288;&#36719;&#24615;&#25110;&#30828;&#24615;&#65289;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#38480;&#21046;&#27169;&#22411;&#35843;&#29992;&#27425;&#25968;&#30340;&#39069;&#22806;&#32422;&#26463;&#65292;&#20027;&#35201;&#20851;&#27880;&#20110;&#23569;&#27425;&#35843;&#29992;&#30340;&#27169;&#22411;&#31363;&#21462;&#12290;&#20026;&#20102;&#22312;&#24212;&#29992;&#38480;&#21046;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#27169;&#22411;&#25552;&#21462;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#26694;&#26550;&#12290;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#19988;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#21019;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#20195;&#29702;&#25968;&#25454;&#38598;&#65289;&#12290;&#32473;&#23450;&#20801;&#35768;&#30340;&#26368;&#22823;API&#35843;&#29992;&#27425;&#25968;&#65292;&#25105;&#20204;&#20256;&#36882;&#30456;&#24212;&#25968;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models showcased strong capabilities in image synthesis, being used in many computer vision tasks with great success. To this end, we propose to explore a new use case, namely to copy black-box classification models without having access to the original training data, the architecture, and the weights of the model, \ie~the model is only exposed through an inference API. More specifically, we can only observe the (soft or hard) labels for some image samples passed as input to the model. Furthermore, we consider an additional constraint limiting the number of model calls, mostly focusing our research on few-call model stealing. In order to solve the model extraction task given the applied restrictions, we propose the following framework. As training data, we create a synthetic data set (called proxy data set) by leveraging the ability of diffusion models to generate realistic and diverse images. Given a maximum number of allowed API calls, we pass the respective number of sampl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;(DataDAM)&#26041;&#27861;&#12290;&#36890;&#36807;&#21305;&#37197;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#21512;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.00093</link><description>&lt;p&gt;
DataDAM: &#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
DataDAM: Efficient Dataset Distillation with Attention Matching. (arXiv:2310.00093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;(DataDAM)&#26041;&#27861;&#12290;&#36890;&#36807;&#21305;&#37197;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#21512;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#22312;&#23613;&#37327;&#20943;&#23569;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#26356;&#22823;&#30495;&#23454;&#25968;&#25454;&#38598;&#20449;&#24687;&#30340;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#26368;&#32456;&#23454;&#29616;&#19982;&#25972;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#26041;&#27861;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#24182;&#19981;&#33021;&#20687;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#37027;&#26679;&#20998;&#24067;&#21644;&#21306;&#20998;&#65292;&#32780;&#19988;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#20294;&#31934;&#28860;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;(DataDAM)&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#21305;&#37197;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have long tried to minimize training costs in deep learning while maintaining strong generalization across diverse datasets. Emerging research on dataset distillation aims to reduce training costs by creating a small synthetic set that contains the information of a larger real dataset and ultimately achieves test accuracy equivalent to a model trained on the whole dataset. Unfortunately, the synthetic data generated by previous methods are not guaranteed to distribute and discriminate as well as the original training data, and they incur significant computational costs. Despite promising results, there still exists a significant performance gap between models trained on condensed synthetic sets and those trained on the whole dataset. In this paper, we address these challenges using efficient Dataset Distillation with Attention Matching (DataDAM), achieving state-of-the-art performance while reducing training costs. Specifically, we learn synthetic images by matching the spa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#21644;&#40657;&#30418;&#20248;&#21270;&#20043;&#38388;&#30340;&#20132;&#21449;&#24212;&#29992;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#35780;&#20272;&#20102;&#20302;&#25104;&#26412;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00077</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#22312;BBOB&#21644;OpenAI Gym&#19978;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Low-budget Black-box Optimization Algorithms Evaluated on BBOB and OpenAI Gym. (arXiv:2310.00077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#21644;&#40657;&#30418;&#20248;&#21270;&#20043;&#38388;&#30340;&#20132;&#21449;&#24212;&#29992;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#35780;&#20272;&#20102;&#20302;&#25104;&#26412;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#40657;&#30418;&#20248;&#21270;&#65288;BBO&#65289;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#29305;&#21035;&#20851;&#27880;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#12290;&#22522;&#20110;BO&#30340;&#31639;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#29992;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#31639;&#27861;&#37197;&#32622;&#31561;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#38382;&#39064;&#32500;&#24230;&#21644;&#35780;&#20272;&#39044;&#31639;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#25928;&#29575;&#20250;&#38477;&#20302;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#22312;&#20248;&#21270;&#31038;&#21306;&#20013;&#29420;&#31435;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#20102;&#35299;&#26159;&#21542;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;BBO&#20043;&#38388;&#36827;&#34892;&#20132;&#21449;&#21463;&#31934;&#65292;&#21363;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#22312;BBO&#20013;&#26159;&#21542;&#21516;&#26679;&#26377;&#25928;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#27604;&#36739;&#23454;&#39564;&#36890;&#24120;&#28041;&#21450;&#30456;&#23545;&#36739;&#23567;&#30340;&#22522;&#20934;&#21644;&#23454;&#39564;&#35774;&#32622;&#20013;&#30340;&#21487;&#35265;&#38382;&#39064;&#65292;&#22914;&#22522;&#32447;&#21021;&#22987;&#21270;&#19981;&#33391;&#12289;&#36807;&#24230;&#25311;&#21512;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing ubiquity of machine learning (ML) has led it to enter various areas of computer science, including black-box optimization (BBO). Recent research is particularly concerned with Bayesian optimization (BO). BO-based algorithms are popular in the ML community, as they are used for hyperparameter optimization and more generally for algorithm configuration. However, their efficiency decreases as the dimensionality of the problem and the budget of evaluations increase. Meanwhile, derivative-free optimization methods have evolved independently in the optimization community. Therefore, we urge to understand whether cross-fertilization is possible between the two communities, ML and BBO, i.e., whether algorithms that are heavily used in ML also work well in BBO and vice versa. Comparative experiments often involve rather small benchmarks and show visible problems in the experimental setup, such as poor initialization of baselines, overfitting due to problem-specific setting of hyperp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#22320;&#29983;&#25104;LHC&#21943;&#27880;&#20026;&#28857;&#20113;&#65292;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#21305;&#37197;&#25193;&#25955;&#27169;&#22411;&#30340;&#31561;&#21464;&#28857;&#20113;&#26550;&#26500;&#65292;&#21478;&#19968;&#31181;&#26159;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#30340;&#31561;&#25928;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00049</link><description>&lt;p&gt;
EPiC-ly&#24555;&#36895;&#29983;&#25104;&#24102;&#26377;&#27969;&#21305;&#37197;&#21644;&#25193;&#25955;&#30340;&#31890;&#23376;&#20113;
&lt;/p&gt;
&lt;p&gt;
EPiC-ly Fast Particle Cloud Generation with Flow-Matching and Diffusion. (arXiv:2310.00049v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#22320;&#29983;&#25104;LHC&#21943;&#27880;&#20026;&#28857;&#20113;&#65292;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#21305;&#37197;&#25193;&#25955;&#27169;&#22411;&#30340;&#31561;&#21464;&#28857;&#20113;&#26550;&#26500;&#65292;&#21478;&#19968;&#31181;&#26159;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#30340;&#31561;&#25928;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LHC&#20013;&#30340;&#21943;&#27880;&#36890;&#24120;&#30001;&#22823;&#37327;&#39640;&#24230;&#30456;&#20851;&#30340;&#31890;&#23376;&#32452;&#25104;&#65292;&#26159;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#30340;&#19968;&#20010;&#26377;&#36259;&#23454;&#39564;&#23460;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#29983;&#25104;LHC&#21943;&#27880;&#20026;&#28857;&#20113;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#28145;&#24230;&#38598;&#21512;&#26694;&#26550;&#30340;&#31561;&#21464;&#28857;&#20113;&#65288;EPiC&#65289;&#26550;&#26500;&#30340;\epcjedi&#65292;&#35813;&#26550;&#26500;&#23558;&#35780;&#20998;&#21305;&#37197;&#25193;&#25955;&#27169;&#22411;&#19982;&#20043;&#32467;&#21512;&#12290;&#35813;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#27604;&#20043;&#21069;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#25193;&#25955;&#27169;&#22411;&#26356;&#24555;&#30340;&#36873;&#25321;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#29983;&#25104;&#21943;&#27880;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#31890;&#23376;&#20113;&#29983;&#25104;&#30340;&#39318;&#20010;&#31561;&#25928;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#65288;CNF&#65289;\epcfm&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#21487;&#25193;&#23637;&#26131;&#35757;&#32451;&#30340;&#27969;&#21305;&#37197;&#30446;&#26631;&#65292;&#30452;&#25509;&#22238;&#24402;&#36830;&#25509;&#39640;&#26031;&#22122;&#22768;&#20808;&#39564;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#30690;&#37327;&#22330;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;\epcjedi&#21644;\epcfm&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jets at the LHC, typically consisting of a large number of highly correlated particles, are a fascinating laboratory for deep generative modeling. In this paper, we present two novel methods that generate LHC jets as point clouds efficiently and accurately. We introduce \epcjedi, which combines score-matching diffusion models with the Equivariant Point Cloud (EPiC) architecture based on the deep sets framework. This model offers a much faster alternative to previous transformer-based diffusion models without reducing the quality of the generated jets. In addition, we introduce \epcfm, the first permutation equivariant continuous normalizing flow (CNF) for particle cloud generation. This model is trained with {\it flow-matching}, a scalable and easy-to-train objective based on optimal transport that directly regresses the vector fields connecting the Gaussian noise prior to the data distribution. Our experiments demonstrate that \epcjedi and \epcfm both achieve state-of-the-art performa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#21464;&#25442;&#20013;&#30340;Clifford&#20960;&#20309;&#19981;&#21464;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;ADE Coxeter&#20803;&#32032;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#25366;&#25496;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#21464;&#37327;&#30340;&#36755;&#20986;&#23436;&#20840;&#21462;&#20915;&#20110;&#31616;&#21333;&#26681;&#30340;&#36873;&#25321;&#21644;&#23545;&#24212;&#21453;&#23556;&#30340;&#25490;&#21015;&#39034;&#24207;&#65292;&#23384;&#22312;&#24040;&#22823;&#30340;&#36864;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00041</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;ADE Coxeter&#20803;&#32032;&#30340;Clifford&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Clifford invariants of ADE Coxeter elements. (arXiv:2310.00041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#21464;&#25442;&#20013;&#30340;Clifford&#20960;&#20309;&#19981;&#21464;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;ADE Coxeter&#20803;&#32032;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#25366;&#25496;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#21464;&#37327;&#30340;&#36755;&#20986;&#23436;&#20840;&#21462;&#20915;&#20110;&#31616;&#21333;&#26681;&#30340;&#36873;&#25321;&#21644;&#23545;&#24212;&#21453;&#23556;&#30340;&#25490;&#21015;&#39034;&#24207;&#65292;&#23384;&#22312;&#24040;&#22823;&#30340;&#36864;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32447;&#24615;&#21464;&#25442;&#30340;Clifford&#20960;&#20309;&#19981;&#21464;&#37327;&#19978;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#19968;&#31867;&#22312;&#26681;&#31995;&#12289;&#21453;&#23556;&#32676;&#12289;&#26446;&#32676;&#21644;&#26446;&#20195;&#25968;&#30340;&#32972;&#26223;&#20013;&#24863;&#20852;&#36259;&#30340;&#20960;&#20309;&#21464;&#25442;&#30340;&#19981;&#21464;&#37327;&#30340;&#30740;&#31350;: Coxeter&#21464;&#25442;&#12290;&#25105;&#20204;&#23545;$A_8$&#12289;$D_8$&#21644;$E_8$&#30340;&#25152;&#26377;Coxeter&#21464;&#25442;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#35745;&#31639;&#65292;&#24182;&#20351;&#29992;&#39640;&#24615;&#33021;&#35745;&#31639;&#26426;&#35745;&#31639;&#20102;&#23427;&#20204;&#30340;&#19981;&#21464;&#37327;&#12290;&#36825;&#31181;&#35745;&#31639;&#20195;&#25968;&#30340;&#33539;&#24335;&#29983;&#25104;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#21033;&#29992;&#25968;&#25454;&#31185;&#23398;&#30340;&#25216;&#26415;&#65292;&#22914;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26469;&#25366;&#25496;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#12290;&#30001;&#20110;&#36755;&#20986; - &#19981;&#21464;&#37327; - &#23436;&#20840;&#30001;&#31616;&#21333;&#26681;&#30340;&#36873;&#25321;&#21644;Coxeter&#20803;&#32032;&#20013;&#23545;&#24212;&#21453;&#23556;&#30340;&#25490;&#21015;&#39034;&#24207;&#20915;&#23450;&#65292;&#25105;&#20204;&#26399;&#26395;&#22312;&#26144;&#23556;&#20013;&#23384;&#22312;&#24040;&#22823;&#30340;&#36864;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been recent interest in novel Clifford geometric invariants of linear transformations. This motivates the investigation of such invariants for a certain type of geometric transformation of interest in the context of root systems, reflection groups, Lie groups and Lie algebras: the Coxeter transformations. We perform exhaustive calculations of all Coxeter transformations for $A_8$, $D_8$ and $E_8$ for a choice of basis of simple roots and compute their invariants, using high-performance computing. This computational algebra paradigm generates a dataset that can then be mined using techniques from data science such as supervised and unsupervised machine learning. In this paper we focus on neural network classification and principal component analysis. Since the output -- the invariants -- is fully determined by the choice of simple roots and the permutation order of the corresponding reflections in the Coxeter element, we expect huge degeneracy in the mapping. This provides the
&lt;/p&gt;</description></item><item><title>Cleanba&#26159;&#19968;&#31181;&#21487;&#22797;&#29616;&#21644;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24179;&#21488;&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#39640;&#24230;&#21487;&#22797;&#29616;&#30340;&#26550;&#26500;&#21644;&#32463;&#36807;&#39640;&#24230;&#20248;&#21270;&#30340;&#20998;&#24067;&#24335;&#21464;&#31181;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22797;&#29616;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#26356;&#20855;&#22797;&#29616;&#24615;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;</title><link>http://arxiv.org/abs/2310.00036</link><description>&lt;p&gt;
Cleanba: &#19968;&#31181;&#21487;&#22797;&#29616;&#21644;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform. (arXiv:2310.00036v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00036
&lt;/p&gt;
&lt;p&gt;
Cleanba&#26159;&#19968;&#31181;&#21487;&#22797;&#29616;&#21644;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24179;&#21488;&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#39640;&#24230;&#21487;&#22797;&#29616;&#30340;&#26550;&#26500;&#21644;&#32463;&#36807;&#39640;&#24230;&#20248;&#21270;&#30340;&#20998;&#24067;&#24335;&#21464;&#31181;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22797;&#29616;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#26356;&#20855;&#22797;&#29616;&#24615;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26088;&#22312;&#21033;&#29992;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#20197;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#35757;&#32451;&#33258;&#20027;&#26234;&#33021;&#20307;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22797;&#29616;&#24615;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#21363;&#20351;&#25511;&#21046;&#20102;&#36229;&#21442;&#25968;&#65292;&#20856;&#22411;&#30340;actor-learner&#26694;&#26550;&#20063;&#21487;&#33021;&#23384;&#22312;&#22797;&#29616;&#24615;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Cleanba&#65292;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#30340;&#20998;&#24067;&#24335;DRL&#24179;&#21488;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#21487;&#22797;&#29616;&#30340;&#26550;&#26500;&#12290;Cleanba&#23454;&#29616;&#20102;&#32463;&#36807;&#39640;&#24230;&#20248;&#21270;&#30340;PPO&#21644;IMPALA&#30340;&#20998;&#24067;&#24335;&#21464;&#31181;&#12290;&#25105;&#20204;&#22312;Atari&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#36825;&#20123;&#21464;&#31181;&#22312;moolib&#21644;torchbeast&#30340;&#24378;IMPALA&#22522;&#32447;&#20197;&#21450;CleanRL&#30340;PPO&#22522;&#32447;&#20013;&#21487;&#20197;&#33719;&#24471;&#30456;&#31561;&#25110;&#26356;&#39640;&#30340;&#24471;&#20998;&#12290;&#28982;&#32780;&#65292;Cleanba&#30340;&#21464;&#20307;&#22312;&#19981;&#21516;&#30828;&#20214;&#35774;&#32622;&#19979;&#21576;&#29616;&#20986;1&#65289;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;2&#65289;&#26356;&#20855;&#22797;&#29616;&#24615;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;Cleanba&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/vwxyzjn/cleanba}&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Deep Reinforcement Learning (DRL) aims to leverage more computational resources to train autonomous agents with less training time. Despite recent progress in the field, reproducibility issues have not been sufficiently explored. This paper first shows that the typical actor-learner framework can have reproducibility issues even if hyperparameters are controlled. We then introduce Cleanba, a new open-source platform for distributed DRL that proposes a highly reproducible architecture. Cleanba implements highly optimized distributed variants of PPO and IMPALA. Our Atari experiments show that these variants can obtain equivalent or higher scores than strong IMPALA baselines in moolib and torchbeast and PPO baseline in CleanRL. However, Cleanba variants present 1) shorter training time and 2) more reproducible learning curves in different hardware settings. Cleanba's source code is available at \url{https://github.com/vwxyzjn/cleanba}
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;LoRA&#36866;&#37197;&#22120;&#38598;&#25104;&#65292;&#24182;&#20855;&#26377;&#19982;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#36817;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.00035</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;LoRA&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
LoRA ensembles for large language model fine-tuning. (arXiv:2310.00035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;LoRA&#36866;&#37197;&#22120;&#38598;&#25104;&#65292;&#24182;&#20855;&#26377;&#19982;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#36817;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#34920;&#29616;&#20026;&#36807;&#20110;&#33258;&#20449;&#12289;&#26657;&#20934;&#19981;&#20339;&#20197;&#21450;&#23545;&#27979;&#35797;&#25968;&#25454;&#25110;&#36229;&#20986;&#20998;&#24067;&#30340;&#26679;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#36825;&#20123;&#20302;&#31209;&#36866;&#37197;&#22120;&#34920;&#31034;&#30340;&#21442;&#25968;&#25968;&#37327;&#38750;&#24120;&#23567;&#65292;&#27604;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#23567;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;LoRA&#36866;&#37197;&#22120;&#38598;&#25104;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as overconfidence, poor calibration, and unreliable prediction results on test data or out-of-distribution samples. One approach commonly used in vision for alleviating this issue is a deep ensemble, which constructs an ensemble by training the same model multiple times using different random initializations. However, there is a huge challenge to ensembling LLMs: the most effective LLMs are very, very large. Keeping a single LLM in memory is already challenging enough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many settings. To address these issues, we propose an ensemble approach using Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique. Critically, these low-rank adapters represent a very small number of parameters, orders of magnitude less than the underlying pre-trained model. Thus, it is possible to construct large ensembles of LoRA adapters with almost the same computat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;PB-LLM&#26159;&#19968;&#31181;&#37096;&#20998;&#20108;&#20540;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31561;&#26041;&#27861;&#24674;&#22797;&#37327;&#21270;LLMM&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.00034</link><description>&lt;p&gt;
PB-LLM: &#37096;&#20998;&#20108;&#20540;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PB-LLM: Partially Binarized Large Language Models. (arXiv:2310.00034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;PB-LLM&#26159;&#19968;&#31181;&#37096;&#20998;&#20108;&#20540;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31561;&#26041;&#27861;&#24674;&#22797;&#37327;&#21270;LLMM&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32593;&#32476;&#20108;&#20540;&#21270;&#65292;&#19968;&#31181;&#21387;&#32553;&#27169;&#22411;&#26435;&#37325;&#20026;&#21333;&#20010;&#27604;&#29305;&#30340;&#37327;&#21270;&#30340;&#28608;&#36827;&#24418;&#24335;&#65292;&#19987;&#38376;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21387;&#32553;&#12290;&#30001;&#20110;&#20043;&#21069;&#30340;&#20108;&#20540;&#21270;&#26041;&#27861;&#20250;&#23548;&#33268;LLMs&#23849;&#28291;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#37096;&#20998;&#20108;&#20540;&#21270;LLM&#65288;PB-LLM&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#37327;&#21270;LLMs&#30340;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#25581;&#31034;&#20102;&#29616;&#26377;&#20108;&#20540;&#21270;&#31639;&#27861;&#30340;&#21407;&#29983;&#24212;&#29992;&#30340;&#26080;&#25928;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#26174;&#33879;&#26435;&#37325;&#22312;&#23454;&#29616;&#20302;&#20301;&#37327;&#21270;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;PB-LLM&#22312;&#20108;&#36827;&#21046;&#21270;&#36807;&#31243;&#20013;&#36807;&#28388;&#20102;&#19968;&#23567;&#37096;&#20998;&#26174;&#33879;&#26435;&#37325;&#65292;&#23558;&#23427;&#20204;&#20998;&#37197;&#21040;&#39640;&#20301;&#23384;&#20648;&#20013;&#65292;&#21363;&#37096;&#20998;&#20108;&#20540;&#21270;&#12290;PB-LLM&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#30340;&#35282;&#24230;&#20998;&#26512;&#21518;&#65292;&#25193;&#23637;&#20102;&#24674;&#22797;&#37327;&#21270;LLMM&#23481;&#37327;&#30340;&#33021;&#21147;&#12290;&#22312;PTQ&#19979;&#65292;&#32467;&#21512;&#20102;GPTQ&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#37325;&#26500;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#39118;&#38505;&#35748;&#30693;&#26469;&#29983;&#25104;&#23545;&#25163;&#39550;&#39542;&#34892;&#20026;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26377;&#25928;&#24615;&#21644;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.00029</link><description>&lt;p&gt;
&#34701;&#20837;&#20154;&#31867;&#39118;&#38505;&#35748;&#30693;&#30340;&#23545;&#25239;&#39550;&#39542;&#34892;&#20026;&#29983;&#25104;&#25216;&#26415;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation. (arXiv:2310.00029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#39118;&#38505;&#35748;&#30693;&#26469;&#29983;&#25104;&#23545;&#25163;&#39550;&#39542;&#34892;&#20026;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26377;&#25928;&#24615;&#21644;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20851;&#27880;&#20110;&#24320;&#21457;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#23545;&#25163;&#39550;&#39542;&#34892;&#20026;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#26292;&#38706;&#20986;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38754;&#23545;&#30340;&#26377;&#25928;&#21644;&#21512;&#29702;&#30340;&#39118;&#38505;&#20107;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#25163;&#34892;&#20026;&#65292;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#33021;&#22815;&#34920;&#31034;&#20154;&#31867;&#30340;&#39118;&#38505;&#35748;&#30693;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;&#29256;&#26412;&#30340;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#23545;&#25163;&#31574;&#30053;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#39640;&#20445;&#30495;&#30340;&#30828;&#20214;&#22312;&#29615;&#65288;HiL&#65289;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#22522;&#20110;&#24182;&#32447;&#24773;&#26223;&#30340;&#23545;&#27604;&#26696;&#20363;&#30740;&#31350;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#23545;&#25163;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#34987;&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicle (AV) evaluation has been the subject of increased interest in recent years both in industry and in academia. This paper focuses on the development of a novel framework for generating adversarial driving behavior of background vehicle interfering against the AV to expose effective and rational risky events. Specifically, the adversarial behavior is learned by a reinforcement learning (RL) approach incorporated with the cumulative prospect theory (CPT) which allows representation of human risk cognition. Then, the extended version of deep deterministic policy gradient (DDPG) technique is proposed for training the adversarial policy while ensuring training stability as the CPT action-value function is leveraged. A comparative case study regarding the cut-in scenario is conducted on a high fidelity Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial effectiveness to infer the weakness of the tested AV.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26080;&#26631;&#35760;&#30340;&#22495;&#22806;&#25968;&#25454;&#32435;&#20837;&#21322;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#65292;&#20174;&#32780;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#19982;&#33258;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#20102;&#39640;&#25928;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#35813;&#26694;&#26550;&#22312;&#39640;&#26031;&#28151;&#21512;&#20998;&#31867;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.00027</link><description>&lt;p&gt;
&#26080;&#26631;&#35760;&#30340;&#22495;&#22806;&#25968;&#25454;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlabeled Out-Of-Domain Data Improves Generalization. (arXiv:2310.00027v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00027
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26080;&#26631;&#35760;&#30340;&#22495;&#22806;&#25968;&#25454;&#32435;&#20837;&#21322;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#65292;&#20174;&#32780;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#19982;&#33258;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#20102;&#39640;&#25928;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#35813;&#26694;&#26550;&#22312;&#39640;&#26031;&#28151;&#21512;&#20998;&#31867;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26080;&#26631;&#35760;&#25968;&#25454;&#32435;&#20837;&#21322;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#26368;&#23567;&#21270;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#25110;&#38750;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#26223;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#20801;&#35768;&#26080;&#26631;&#35760;&#26679;&#26412;&#22312;&#24635;&#21464;&#24046;&#24847;&#20041;&#19978;&#30053;&#24494;&#20559;&#31163;&#22495;&#20869;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#19982;&#33258;&#30417;&#30563;&#35757;&#32451;&#30456;&#32467;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#20102;&#35757;&#32451;&#38454;&#27573;&#30340;&#39640;&#25928;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#22312;$\mathbb{R}^d$&#20013;&#30340;&#20004;&#20010;&#39640;&#26031;&#28151;&#21512;&#20998;&#31867;&#38382;&#39064;&#65292;&#38500;&#20102;&#26469;&#33258;&#30495;&#23454;&#20998;&#24067;&#30340;$m$&#20010;&#29420;&#31435;&#26631;&#35760;&#26679;&#26412;&#20043;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#19968;&#32452;$n$&#20010;&#65288;&#36890;&#24120;$n\gg m$&#65289;&#22495;&#22806;&#21644;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;&#24050;&#30693;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#65292;&#27867;&#21270;&#35823;&#24046;&#21487;&#20197;&#36890;&#36807;$\propto\left(d/m\right)$&#36827;&#34892;&#30028;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework for incorporating unlabeled data into semi-supervised classification problems, where scenarios involving the minimization of either i) adversarially robust or ii) non-robust loss functions have been considered. Notably, we allow the unlabeled samples to deviate slightly (in total variation sense) from the in-domain distribution. The core idea behind our framework is to combine Distributionally Robust Optimization (DRO) with self-supervised training. As a result, we also leverage efficient polynomial-time algorithms for the training stage. From a theoretical standpoint, we apply our framework on the classification problem of a mixture of two Gaussians in $\mathbb{R}^d$, where in addition to the $m$ independent and labeled samples from the true distribution, a set of $n$ (usually with $n\gg m$) out of domain and unlabeled samples are gievn as well. Using only the labeled data, it is known that the generalization error can be bounded by $\propto\left(d/m\right
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;De-SaTE&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#21435;&#22122;&#27169;&#22359;&#20197;&#21450;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;&#65292;&#20934;&#30830;&#39044;&#27979;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#65292;&#20026;&#39044;&#38450;&#24615;&#32500;&#25252;&#21644;&#39044;&#27979;&#24615;&#20998;&#26512;&#25552;&#20379;&#20851;&#38190;&#25351;&#26631;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.00023</link><description>&lt;p&gt;
De-SaTE&#65306;&#29992;&#20110;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#39044;&#27979;&#30340;&#21435;&#22122;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion Battery Health Prognostics. (arXiv:2310.00023v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;De-SaTE&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#21435;&#22122;&#27169;&#22359;&#20197;&#21450;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;&#65292;&#20934;&#30830;&#39044;&#27979;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#65292;&#20026;&#39044;&#38450;&#24615;&#32500;&#25252;&#21644;&#39044;&#27979;&#24615;&#20998;&#26512;&#25552;&#20379;&#20851;&#38190;&#25351;&#26631;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38146;&#31163;&#23376;&#30005;&#27744;&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20174;&#20026;&#20415;&#25658;&#24335;&#30005;&#23376;&#35774;&#22791;&#20379;&#30005;&#21040;&#25512;&#21160;&#30005;&#21160;&#27773;&#36710;&#21644;&#25903;&#25345;&#33021;&#28304;&#23384;&#20648;&#31995;&#32479;&#12290;&#26377;&#25928;&#31649;&#29702;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#20934;&#30830;&#39044;&#27979;&#20854;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#65292;&#36825;&#26159;&#39044;&#38450;&#24615;&#32500;&#25252;&#21644;&#39044;&#27979;&#24615;&#20998;&#26512;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#21435;&#22122;&#27169;&#22359;&#30340;&#33021;&#37327;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#32463;&#36807;&#35757;&#32451;&#26469;&#22788;&#29702;&#30005;&#27744;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#22122;&#22768;&#31867;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23567;&#27874;&#21435;&#22122;&#22120;&#26469;&#29983;&#25104;&#32534;&#30721;/&#20998;&#35299;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#36890;&#36807;&#19987;&#29992;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;&#36827;&#34892;&#22788;&#29702;&#12290;&#22312;NASA&#21644;CALCE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#21518;&#65292;&#25105;&#20204;&#33021;&#22815;&#34920;&#24449;&#22810;&#31181;&#22122;&#22768;&#27169;&#24335;&#19979;&#30340;&#24191;&#27867;&#20581;&#24247;&#25351;&#26631;&#20272;&#35745;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25253;&#21578;&#30340;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Lithium Ion (Li-ion) batteries have gained widespread popularity across various industries, from powering portable electronic devices to propelling electric vehicles and supporting energy storage systems. A central challenge in managing Li-ion batteries effectively is accurately predicting their Remaining Useful Life (RUL), which is a critical measure for proactive maintenance and predictive analytics. This study presents a novel approach that harnesses the power of multiple denoising modules, each trained to address specific types of noise commonly encountered in battery data. Specifically we use a denoising auto-encoder and a wavelet denoiser to generate encoded/decomposed representations, which are subsequently processed through dedicated self-attention transformer encoders. After extensive experimentation on the NASA and CALCE datasets, we are able to characterize a broad spectrum of health indicator estimations under a set of diverse noise patterns. We find that our reported error
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37327;&#23376;&#36864;&#28779;&#25216;&#26415;&#65292;&#30740;&#31350;&#21457;&#29616;COVID-19&#30123;&#24773;&#23545;&#22823;&#23398;&#29983;&#25233;&#37057;&#30151;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#20102;&#22522;&#20110;&#37327;&#23376;&#36864;&#28779;&#21644;&#20256;&#32479;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00018</link><description>&lt;p&gt;
&#36890;&#36807;&#37327;&#23376;&#36864;&#28779;&#26426;&#22120;&#20154;&#30740;&#31350;COVID-19&#30123;&#24773;&#23545;&#22823;&#23398;&#29983;&#25233;&#37057;&#30151;&#24433;&#21709;&#30340;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Investigation of factors regarding the effects of COVID-19 pandemic on college students' depression by quantum annealer. (arXiv:2310.00018v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37327;&#23376;&#36864;&#28779;&#25216;&#26415;&#65292;&#30740;&#31350;&#21457;&#29616;COVID-19&#30123;&#24773;&#23545;&#22823;&#23398;&#29983;&#25233;&#37057;&#30151;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#20102;&#22522;&#20110;&#37327;&#23376;&#36864;&#28779;&#21644;&#20256;&#32479;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25253;&#21578;&#20102;COVID-19&#30123;&#24773;&#23545;&#24515;&#29702;&#20581;&#24247;&#30340;&#24433;&#21709;&#20197;&#21450;&#30456;&#20851;&#22240;&#32032;&#12290;&#30001;&#20110;&#22823;&#23398;&#29983;&#26131;&#21463;&#21040;&#30123;&#24773;&#30340;&#24433;&#21709;&#65292;&#20182;&#20204;&#32463;&#24120;&#34987;&#36873;&#20026;&#30446;&#26631;&#20154;&#32676;&#12290;&#26412;&#30740;&#31350;&#25910;&#38598;&#20102;751&#21517;&#22823;&#23398;&#29983;&#30340;&#22810;&#21464;&#37327;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#21508;&#31181;&#24515;&#29702;&#20581;&#24247;&#22240;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#21830;&#19994;D-Wave&#37327;&#23376;&#35745;&#31639;&#26426;&#25191;&#34892;&#30340;&#22522;&#20110;&#37327;&#23376;&#36864;&#28779;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#20197;&#30830;&#23450;&#30123;&#24773;&#21069;&#21518;&#30456;&#20851;&#22240;&#32032;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#21464;&#21270;&#12290;&#36824;&#24212;&#29992;&#20102;&#22810;&#21464;&#37327;&#32447;&#24615;&#22238;&#24402;&#65288;MLR&#65289;&#21644;XGBoost&#27169;&#22411;&#26469;&#39564;&#35777;&#22522;&#20110;&#37327;&#23376;&#36864;&#28779;&#30340;&#31639;&#27861;&#12290;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#30830;&#35748;&#22522;&#20110;&#37327;&#23376;&#36864;&#28779;&#30340;&#31639;&#27861;&#20855;&#26377;&#19982;&#20808;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;MLR&#27169;&#22411;&#22312;&#22240;&#32032;&#20998;&#26512;&#30740;&#31350;&#20013;&#30456;&#24403;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diverse cases regarding the impact, with its related factors, of the COVID-19 pandemic on mental health have been reported in previous studies. College student groups have been frequently selected as the target population in previous studies because they are easily affected by pandemics. In this study, multivariable datasets were collected from 751 college students based on the complex relationships between various mental health factors. We utilized quantum annealing (QA)-based feature selection algorithms that were executed by commercial D-Wave quantum computers to determine the changes in the relative importance of the associated factors before and after the pandemic. Multivariable linear regression (MLR) and XGBoost models were also applied to validate the QA-based algorithms. Based on the experimental results, we confirm that QA-based algorithms have comparable capabilities in factor analysis research to the MLR models that have been widely used in previous studies. Furthermore, th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#20849;&#24773;&#30340;&#20998;&#31867;&#30740;&#31350;&#65292;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25351;&#20986;&#35757;&#32451;&#20154;&#24037;&#20849;&#24773;&#30340;&#26631;&#20934;&#27969;&#31243;&#21253;&#25324;&#24773;&#32490;&#35782;&#21035;&#12289;&#20998;&#26512;&#21644;&#21709;&#24212;&#21160;&#20316;&#12290;&#20854;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#34394;&#25311;&#20195;&#29702;&#21644;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#26377;&#36739;&#39640;&#24433;&#21709;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.00010</link><description>&lt;p&gt;
&#20154;&#24037;&#20849;&#24773;&#20998;&#31867;&#65306;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Artificial Empathy Classification: A Survey of Deep Learning Techniques, Datasets, and Evaluation Scales. (arXiv:2310.00010v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00010
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#20849;&#24773;&#30340;&#20998;&#31867;&#30740;&#31350;&#65292;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25351;&#20986;&#35757;&#32451;&#20154;&#24037;&#20849;&#24773;&#30340;&#26631;&#20934;&#27969;&#31243;&#21253;&#25324;&#24773;&#32490;&#35782;&#21035;&#12289;&#20998;&#26512;&#21644;&#21709;&#24212;&#21160;&#20316;&#12290;&#20854;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#34394;&#25311;&#20195;&#29702;&#21644;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#26377;&#36739;&#39640;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#21313;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#36741;&#21161;&#21457;&#23637;&#26426;&#22120;&#20154;&#23398;&#65288;ADR&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;&#20154;&#24037;&#20849;&#24773;&#65288;AE&#65289;&#20316;&#20026;&#21487;&#33021;&#30340;&#26410;&#26469;&#20154;&#26426;&#20132;&#20114;&#65288;HRI&#65289;&#33539;&#24335;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#20154;&#31867;&#20174;&#20986;&#29983;&#24320;&#22987;&#23601;&#23398;&#20250;&#20849;&#24773;&#65292;&#22240;&#27492;&#22312;&#26426;&#22120;&#20154;&#21644;&#26234;&#33021;&#26426;&#22120;&#20013;&#28748;&#36755;&#36825;&#31181;&#24863;&#35273;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#25968;&#25454;&#21644;&#26102;&#38388;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#27169;&#20223;&#20849;&#24773;&#12290;&#20154;&#24037;&#20849;&#24773;&#30340;&#26631;&#20934;&#24037;&#20316;&#27969;&#31243;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;1&#65289;&#20351;&#29992;&#20174;&#35270;&#39057;&#25110;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#65288;ER&#65289;&#65292;2&#65289;&#20998;&#26512;&#24863;&#30693;&#30340;&#24773;&#32490;&#25110;&#20849;&#24773;&#31243;&#24230;&#20197;&#36873;&#25321;&#26368;&#20339;&#34892;&#21160;&#26041;&#26696;&#65292;3&#65289;&#25191;&#34892;&#21709;&#24212;&#21160;&#20316;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#20351;&#29992;&#34394;&#25311;&#20195;&#29702;&#25110;&#26426;&#22120;&#20154;&#30340;AE&#24120;&#24120;&#28041;&#21450;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
From the last decade, researchers in the field of machine learning (ML) and assistive developmental robotics (ADR) have taken an interest in artificial empathy (AE) as a possible future paradigm for human-robot interaction (HRI). Humans learn empathy since birth, therefore, it is challenging to instill this sense in robots and intelligent machines. Nevertheless, by training over a vast amount of data and time, imitating empathy, to a certain extent, can be possible for robots. Training techniques for AE, along with findings from the field of empathetic AI research, are ever-evolving. The standard workflow for artificial empathy consists of three stages: 1) Emotion Recognition (ER) using the retrieved features from video or textual data, 2) analyzing the perceived emotion or degree of empathy to choose the best course of action, and 3) carrying out a response action. Recent studies that show AE being used with virtual agents or robots often include Deep Learning (DL) techniques. For ins
&lt;/p&gt;</description></item><item><title>L2CEval&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#30340;&#24037;&#20316;&#65292;&#20998;&#26512;&#20102;&#24433;&#21709;&#20854;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#24182;&#23545;&#32622;&#20449;&#24230;&#26657;&#20934;&#21644;&#20154;&#24037;&#35780;&#20272;&#36827;&#34892;&#20102;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.17446</link><description>&lt;p&gt;
L2CEval:&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models. (arXiv:2309.17446v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17446
&lt;/p&gt;
&lt;p&gt;
L2CEval&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#30340;&#24037;&#20316;&#65292;&#20998;&#26512;&#20102;&#24433;&#21709;&#20854;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#24182;&#23545;&#32622;&#20449;&#24230;&#26657;&#20934;&#21644;&#20154;&#24037;&#35780;&#20272;&#36827;&#34892;&#20102;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#22312;&#20195;&#30721;&#19978;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#22312;&#20960;&#27425;&#35757;&#32451;&#29978;&#33267;&#38646;&#27425;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#29983;&#25104;&#31243;&#24207;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#23613;&#31649;&#26377;&#30528;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#29305;&#23450;&#20219;&#21153;&#12289;&#27169;&#22411;&#26550;&#26500;&#25110;&#23398;&#20064;&#33539;&#24335;&#19978;&#65292;&#23548;&#33268;&#23545;&#25972;&#20307;&#24773;&#20917;&#30340;&#29702;&#35299;&#38646;&#25955;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2CEval&#65292;&#23545;LLM&#22312;&#35821;&#20041;&#35299;&#26512;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;Python&#32534;&#31243;&#30340;7&#20010;&#20219;&#21153;&#19978;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#21487;&#33021;&#24433;&#21709;&#20854;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#22914;&#27169;&#22411;&#22823;&#23567;&#12289;&#39044;&#35757;&#32451;&#25968;&#25454;&#12289;&#25351;&#20196;&#35843;&#25972;&#21644;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;&#38500;&#20102;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#25105;&#20204;&#36824;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#32622;&#20449;&#24230;&#26657;&#20934;&#30340;&#27979;&#37327;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs in a few-shot or even zero-shot manner. Despite promising results, there is a notable lack of a comprehensive evaluation of these models language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition to assessing model performance, we measure confidence calibration for the models and conduct human evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#29992;&#20110;&#31579;&#36873;&#22823;&#22411;&#26410;&#31574;&#21010;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.17425</link><description>&lt;p&gt;
&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Data Filtering Networks. (arXiv:2309.17425v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#29992;&#20110;&#31579;&#36873;&#22823;&#22411;&#26410;&#31574;&#21010;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35757;&#32451;&#38598;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30707;&#65292;&#24182;&#20026;&#35821;&#35328;&#24314;&#27169;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#37319;&#38598;&#20173;&#28982;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#33539;&#24335;&#65292;&#20294;&#25968;&#25454;&#31574;&#21010;&#24448;&#24448;&#20173;&#28982;&#26159;&#20020;&#26102;&#30340;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#39318;&#20808;&#20174;&#32593;&#32476;&#19978;&#25910;&#38598;&#22823;&#37327;&#25968;&#25454;&#65292;&#28982;&#21518;&#36890;&#36807;&#21508;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#23558;&#27492;&#20505;&#36873;&#27744;&#31579;&#36873;&#21040;&#23454;&#38469;&#30340;&#35757;&#32451;&#38598;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65288;DFN&#65289;&#29992;&#20110;&#31579;&#36873;&#22823;&#22411;&#26410;&#31574;&#21010;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#29992;&#20110;&#31579;&#36873;&#30340;&#32593;&#32476;&#30340;&#36136;&#37327;&#19982;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26159;&#19981;&#21516;&#30340;&#65306;&#20363;&#22914;&#65292;&#19968;&#20010;&#22312;ImageNet&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#27604;&#19968;&#20010;&#22312;ImageNet&#19978;&#20934;&#30830;&#29575;&#36739;&#20302;&#20294;&#22312;&#19968;&#23567;&#37096;&#20998;&#39640;&#36136;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#24046;&#30340;&#35757;&#32451;&#38598;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#25968;&#25454;&#38598;DFN-5B&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a data filtering network (DFN) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train 
&lt;/p&gt;</description></item><item><title>PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;</title><link>http://arxiv.org/abs/2309.17260</link><description>&lt;p&gt;
PlaceNav: &#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
PlaceNav: Topological Navigation through Place Recognition. (arXiv:2309.17260v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17260
&lt;/p&gt;
&lt;p&gt;
PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#25299;&#25169;&#23548;&#33322;&#20998;&#20026;&#26426;&#22120;&#20154;&#26080;&#20851;&#21644;&#26426;&#22120;&#20154;&#29305;&#23450;&#30340;&#32452;&#20214;&#21487;&#20197;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#26426;&#22120;&#20154;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23548;&#33322;&#26041;&#27861;&#20173;&#21463;&#21040;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#35745;&#31639;&#32553;&#25918;&#24615;&#24046;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PlaceNav&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#26469;&#36873;&#25321;&#25299;&#25169;&#23548;&#33322;&#27969;&#31243;&#20013;&#30340;&#23376;&#30446;&#26631;&#12290;&#36825;&#20351;&#24471;&#23376;&#30446;&#26631;&#36873;&#25321;&#26356;&#39640;&#25928;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22320;&#28857;&#35782;&#21035;&#20351;&#24471;&#36125;&#21494;&#26031;&#28388;&#27874;&#25104;&#20026;&#21487;&#33021;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#23376;&#30446;&#26631;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#19968;&#35774;&#35745;&#65292;&#24182;&#19988;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present~\methodname, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayes filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76% higher 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#32852;&#37030;&#23398;&#20064;&#21644;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#35299;&#20915;MRI&#21069;&#21015;&#33146;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#22312;&#21327;&#20316;&#23398;&#20064;&#30340;&#24773;&#26223;&#20013;&#36827;&#34892;&#25104;&#26412;&#25928;&#30410;&#22522;&#20934;&#27979;&#35797;&#65292;&#39318;&#27425;&#20351;&#29992;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#35299;&#20915;&#21327;&#20316;&#23398;&#20064;&#38382;&#39064;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.17097</link><description>&lt;p&gt;
Collaborative Learning&#26041;&#27861;&#22312;&#21069;&#21015;&#33146;&#20998;&#21106;&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Collaborative Learning Methods Cost-Effectiveness for Prostate Segmentation. (arXiv:2309.17097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#32852;&#37030;&#23398;&#20064;&#21644;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#35299;&#20915;MRI&#21069;&#21015;&#33146;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#22312;&#21327;&#20316;&#23398;&#20064;&#30340;&#24773;&#26223;&#20013;&#36827;&#34892;&#25104;&#26412;&#25928;&#30410;&#22522;&#20934;&#27979;&#35797;&#65292;&#39318;&#27425;&#20351;&#29992;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#35299;&#20915;&#21327;&#20316;&#23398;&#20064;&#38382;&#39064;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#25968;&#25454;&#36890;&#24120;&#34987;&#20998;&#21106;&#25104;&#22810;&#20010;&#21307;&#38498;&#30340;&#20013;&#23567;&#22411;&#38598;&#21512;&#65292;&#24182;&#19988;&#30001;&#20110;&#38544;&#31169;&#35268;&#23450;&#30340;&#38480;&#21046;&#65292;&#35775;&#38382;&#36825;&#20123;&#25968;&#25454;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#32473;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24102;&#26469;&#20102;&#22256;&#38590;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#20197;&#25968;&#25454;&#20026;&#22522;&#30784;&#12290;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#20351;&#29992;&#21327;&#20316;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#65292;&#36825;&#20801;&#35768;&#21307;&#38498;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35299;&#20915;&#19968;&#20010;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#21327;&#20316;&#22330;&#26223;&#19979;&#30340;MRI&#21069;&#21015;&#33146;&#20998;&#21106;&#38382;&#39064;&#65306;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#65288;CBM&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#20351;&#29992;&#21253;&#25324;&#26631;&#31614;&#34701;&#21512;&#25216;&#26415;&#22312;&#20869;&#30340;CBM&#35299;&#20915;&#21327;&#20316;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;CBM&#23558;&#26469;&#33258;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#32452;&#21512;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#29702;&#24819;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#39044;&#27979;&#26041;&#24046;&#30340;&#32852;&#37030;&#24378;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare data is often split into medium/small-sized collections across multiple hospitals and access to it is encumbered by privacy regulations. This brings difficulties to use them for the development of machine learning and deep learning models, which are known to be data-hungry. One way to overcome this limitation is to use collaborative learning (CL) methods, which allow hospitals to work collaboratively to solve a task, without the need to explicitly share local data.  In this paper, we address a prostate segmentation problem from MRI in a collaborative scenario by comparing two different approaches: federated learning (FL) and consensus-based methods (CBM).  To the best of our knowledge, this is the first work in which CBM, such as label fusion techniques, are used to solve a problem of collaborative learning. In this setting, CBM combine predictions from locally trained models to obtain a federated strong learner with ideally improved robustness and predictive variance proper
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#34920;&#36798;&#33021;&#21147;&#21644;$k$&#32500;Weisfeiler-Leman ($k$WL)&#27979;&#35797;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#21457;&#29616;&#20102;$k$WL&#27979;&#35797;&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#20986;&#29616;&#27425;&#25968;&#30340;&#27169;&#24335;&#22270;$P$&#30340;&#22270;&#24418;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#24335;&#22270;&#35745;&#25968;&#38382;&#39064;&#30340;&#26368;&#23567;&#32500;&#24230;$k$&#12290;</title><link>http://arxiv.org/abs/2309.17053</link><description>&lt;p&gt;
&#20851;&#20110;Weisfeiler-Leman&#27979;&#35797;&#22312;&#22270;&#24418;&#27169;&#24335;&#21442;&#25968;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters. (arXiv:2309.17053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#34920;&#36798;&#33021;&#21147;&#21644;$k$&#32500;Weisfeiler-Leman ($k$WL)&#27979;&#35797;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#21457;&#29616;&#20102;$k$WL&#27979;&#35797;&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#20986;&#29616;&#27425;&#25968;&#30340;&#27169;&#24335;&#22270;$P$&#30340;&#22270;&#24418;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#24335;&#22270;&#35745;&#25968;&#38382;&#39064;&#30340;&#26368;&#23567;&#32500;&#24230;$k$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;$k$&#32500;Weisfeiler-Leman&#65288;$k$WL&#65289;&#27979;&#35797;&#20043;&#38388;&#30340;&#30452;&#25509;&#23545;&#24212;&#20851;&#31995;&#65292;$k$WL&#27979;&#35797;&#26159;&#19968;&#31181;&#24191;&#20026;&#35748;&#21487;&#30340;&#29992;&#20110;&#39564;&#35777;&#22270;&#21516;&#26500;&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#36830;&#25509;&#37325;&#26032;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;$k$WL&#27979;&#35797;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#30340;&#29305;&#23450;&#22270;&#23646;&#24615;&#30340;&#20852;&#36259;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#30340;&#20013;&#24515;&#26159;&#30830;&#23450;&#26368;&#23567;&#32500;&#24230;$k$&#65292;&#20351;&#24471;$k$WL&#21487;&#20197;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#20986;&#29616;&#27425;&#25968;&#30340;&#27169;&#24335;&#22270;$P$&#30340;&#22270;&#24418;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26368;&#23567;$k$&#31216;&#20026;&#36825;&#20010;&#27169;&#24335;&#35745;&#25968;&#38382;&#39064;&#30340;WL&#32500;&#24230;&#12290;&#36825;&#20010;&#35843;&#26597;&#20256;&#32479;&#19978;&#25506;&#35752;&#19982;&#22270;&#26696;&#30456;&#20851;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#35745;&#25968;&#38382;&#39064;&#65306;&#23376;&#22270;&#35745;&#25968;&#21644;&#35825;&#23548;&#23376;&#22270;&#35745;&#25968;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#23613;&#31649;&#23427;&#20204;&#26368;&#21021;&#30475;&#36215;&#26469;&#26159;&#20855;&#26377;&#30475;&#20284;&#19981;&#21516;&#26041;&#27861;&#30340;&#29420;&#31435;&#25361;&#25112;&#65292;&#20294;&#36825;&#20004;&#20010;&#38382;&#39064;&#37117;&#26159;&#19968;&#20010;&#26356;&#20840;&#38754;&#38382;&#39064;&#30340;&#30456;&#20114;&#20851;&#32852;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seminal research in the field of graph neural networks (GNNs) has revealed a direct correspondence between the expressive capabilities of GNNs and the $k$-dimensional Weisfeiler-Leman ($k$WL) test, a widely-recognized method for verifying graph isomorphism. This connection has reignited interest in comprehending the specific graph properties effectively distinguishable by the $k$WL test. A central focus of research in this field revolves around determining the least dimensionality $k$, for which $k$WL can discern graphs with different number of occurrences of a pattern graph $P$. We refer to such a least $k$ as the WL-dimension of this pattern counting problem. This inquiry traditionally delves into two distinct counting problems related to patterns: subgraph counting and induced subgraph counting. Intriguingly, despite their initial appearance as separate challenges with seemingly divergent approaches, both of these problems are interconnected components of a more comprehensive proble
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.16595</link><description>&lt;p&gt;
LLM&#33021;&#21542;&#26377;&#25928;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#22270;&#23398;&#20064;&#65306;&#20309;&#26102;&#20309;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why. (arXiv:2309.16595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#29305;&#21035;&#26159;&#22270;&#25968;&#25454;&#65289;&#19978;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;LLM&#25991;&#29486;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#37325;&#35201;&#25968;&#25454;&#24418;&#24577;&#12290;&#25105;&#20204;&#26088;&#22312;&#20102;&#35299;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20309;&#26102;&#20309;&#22320;&#24341;&#20837;&#22270;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#8220;&#20309;&#26102;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#35774;&#32622;&#20013;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#20016;&#23500;&#25110;&#31232;&#32570;&#12290;&#23545;&#20110;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLM&#24615;&#33021;&#30340;&#20004;&#20010;&#28508;&#22312;&#22240;&#32032;&#65306;&#25968;&#25454;&#27844;&#38706;&#21644;&#21516;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65307;&#65288;ii&#65289;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#35777;&#25454;&#34920;&#26126;LLM&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#26377;&#26174;&#33879;&#30456;&#20851;&#65307;&#65288;iii&#65289;LLM&#22312;&#30446;&#26631;&#33410;&#28857;&#19978;&#30340;&#24615;&#33021;&#19982;&#27491;&#21521;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Large Language Models (LLMs) for structured data--particularly graphs--a crucial data modality that remains underexplored in the LLM literature. We aim to understand when and why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs on node classification tasks. To address the ``when'' question, we examine a variety of prompting methods for encoding structural information, in settings where textual node features are either rich or scarce. For the ``why'' questions, we probe into two potential contributing factors to the LLM performance: data leakage and homophily. Our exploration of these questions reveals that (i) LLMs can benefit from structural information, especially when textual node features are scarce; (ii) there is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage; and (iii) the performance of LLMs on a target node is strongly positively relat
&lt;/p&gt;</description></item><item><title>Stackelberg&#25209;&#37327;&#31574;&#30053;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#21338;&#24328;&#35770;&#30340;&#35266;&#28857;&#65292;&#23545;&#31574;&#30053;&#23398;&#20064;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#20102;&#20248;&#21270;&#26223;&#35266;&#20013;&#30340;&#20998;&#23618;&#20915;&#31574;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16188</link><description>&lt;p&gt;
Stackelberg&#25209;&#37327;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stackelberg Batch Policy Learning. (arXiv:2309.16188v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16188
&lt;/p&gt;
&lt;p&gt;
Stackelberg&#25209;&#37327;&#31574;&#30053;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#21338;&#24328;&#35770;&#30340;&#35266;&#28857;&#65292;&#23545;&#31574;&#30053;&#23398;&#20064;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#20102;&#20248;&#21270;&#26223;&#35266;&#20013;&#30340;&#20998;&#23618;&#20915;&#31574;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#23450;&#20041;&#20102;&#20174;&#22266;&#23450;&#30340;&#25968;&#25454;&#25209;&#27425;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#32570;&#20047;&#35814;&#23613;&#30340;&#25506;&#32034;&#12290;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#31639;&#27861;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#26469;&#26657;&#20934;&#20215;&#20540;&#20989;&#25968;&#27169;&#22411;&#65292;&#24182;&#22312;&#23398;&#20064;&#27169;&#22411;&#19979;&#25191;&#34892;&#26576;&#31181;&#24754;&#35266;&#35780;&#20272;&#65292;&#24050;&#32463;&#25104;&#20026;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20010;&#27969;&#27966;&#30340;&#29616;&#20195;&#30740;&#31350;&#36890;&#24120;&#24573;&#35270;&#20102;&#20248;&#21270;&#26223;&#35266;&#20013;&#38544;&#34255;&#30340;&#20998;&#23618;&#20915;&#31574;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#21338;&#24328;&#35770;&#30340;&#35266;&#28857;&#65292;&#23558;&#31574;&#30053;&#23398;&#20064;&#22270;&#34920;&#24314;&#27169;&#20026;&#20855;&#26377;&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#32467;&#26500;&#30340;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65306;StackelbergLearner&#65292;&#39046;&#23548;&#32773;&#26681;&#25454;&#20854;&#30446;&#26631;&#30340;&#20840;&#23548;&#25968;&#36827;&#34892;&#26356;&#26032;&#65292;&#32780;&#19981;&#26159;&#36890;&#24120;&#30340;&#20010;&#20307;&#26799;&#24230;&#65292;&#32780;&#36319;&#38543;&#32773;&#36827;&#34892;&#20010;&#20307;&#26356;&#26032;&#24182;&#30830;&#20445;&#36807;&#28193;&#19968;&#33268;&#30340;&#24754;&#35266;&#25512;&#29702;&#12290;&#25512;&#23548;&#20986;&#30340;&#23398;&#20064;&#21160;&#21147;
&lt;/p&gt;
&lt;p&gt;
Batch reinforcement learning (RL) defines the task of learning from a fixed batch of data lacking exhaustive exploration. Worst-case optimality algorithms, which calibrate a value-function model class from logged experience and perform some type of pessimistic evaluation under the learned model, have emerged as a promising paradigm for batch RL. However, contemporary works on this stream have commonly overlooked the hierarchical decision-making structure hidden in the optimization landscape. In this paper, we adopt a game-theoretical viewpoint and model the policy learning diagram as a two-player general-sum game with a leader-follower structure. We propose a novel stochastic gradient-based learning algorithm: StackelbergLearner, in which the leader player updates according to the total derivative of its objective instead of the usual individual gradient, and the follower player makes individual updates and ensures transition-consistent pessimistic reasoning. The derived learning dynam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ICML 2023&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#35201;&#27714;&#21442;&#19982;&#32773;&#22312;&#20004;&#20010;&#26376;&#20869;&#25552;&#20379;&#24320;&#28304;&#23454;&#29616;&#30340;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65292;&#21560;&#24341;&#20102;28&#20010;&#21512;&#26684;&#30340;&#25552;&#20132;&#12290;</title><link>http://arxiv.org/abs/2309.15188</link><description>&lt;p&gt;
ICML 2023&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;&#65306;&#35774;&#35745;&#19982;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
ICML 2023 Topological Deep Learning Challenge : Design and Results. (arXiv:2309.15188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ICML 2023&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#35201;&#27714;&#21442;&#19982;&#32773;&#22312;&#20004;&#20010;&#26376;&#20869;&#25552;&#20379;&#24320;&#28304;&#23454;&#29616;&#30340;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65292;&#21560;&#24341;&#20102;28&#20010;&#21512;&#26684;&#30340;&#25552;&#20132;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ICML 2023&#25299;&#25169;&#19982;&#20960;&#20309;&#26426;&#22120;&#23398;&#20064;&#30740;&#35752;&#20250;&#20013;&#20030;&#21150;&#30340;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#35745;&#31639;&#25361;&#25112;&#12290;&#35813;&#27604;&#36187;&#35201;&#27714;&#21442;&#19982;&#32773;&#36890;&#36807;&#36129;&#29486;&#20110;python&#21253;TopoNetX&#65288;&#25968;&#25454;&#22788;&#29702;&#65289;&#21644;TopoModelX&#65288;&#28145;&#24230;&#23398;&#20064;&#65289;&#30340;&#24320;&#28304;&#23454;&#29616;&#26469;&#25552;&#20379;&#25991;&#29486;&#20013;&#30340;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#25361;&#25112;&#22312;&#20004;&#20010;&#26376;&#30340;&#26102;&#38388;&#20869;&#21560;&#24341;&#20102;28&#20010;&#21512;&#26684;&#30340;&#25552;&#20132;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25361;&#25112;&#30340;&#35774;&#35745;&#24182;&#24635;&#32467;&#20102;&#20854;&#20027;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the computational challenge on topological deep learning that was hosted within the ICML 2023 Workshop on Topology and Geometry in Machine Learning. The competition asked participants to provide open-source implementations of topological neural networks from the literature by contributing to the python packages TopoNetX (data processing) and TopoModelX (deep learning). The challenge attracted twenty-eight qualifying submissions in its two-month duration. This paper describes the design of the challenge and summarizes its main findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#20351;&#29992;&#23567;&#25209;&#37327;SGD&#31639;&#27861;&#65292;&#22312;&#20855;&#26377;&#20108;&#27425;&#30495;&#23454;&#20989;&#25968;&#20998;&#38548;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#35757;&#32451;&#25968;&#37327;&#32423;&#20026;$d \:\text{polylog}(d)$&#30340;&#26679;&#26412;&#65292;&#23558;&#32593;&#32476;&#35757;&#32451;&#21040;&#20102;&#20154;&#21475;&#35823;&#24046;&#20026;$o(1)$&#30340;&#31243;&#24230;&#12290;&#36825;&#26159;&#39318;&#27425;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#19978;&#20197;&#21450;&#26631;&#20934;&#35757;&#32451;&#19979;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#21521;&#21516;&#24615;&#25968;&#25454;&#19978;&#39640;&#25928;&#23398;&#20064;XOR&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\tilde{O}(d)$&#12290;</title><link>http://arxiv.org/abs/2309.15111</link><description>&lt;p&gt;
SGD&#22312;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#23547;&#25214;&#24182;&#35843;&#25972;&#29305;&#24449;&#65306;&#20197;XOR&#38382;&#39064;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem. (arXiv:2309.15111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#20351;&#29992;&#23567;&#25209;&#37327;SGD&#31639;&#27861;&#65292;&#22312;&#20855;&#26377;&#20108;&#27425;&#30495;&#23454;&#20989;&#25968;&#20998;&#38548;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#35757;&#32451;&#25968;&#37327;&#32423;&#20026;$d \:\text{polylog}(d)$&#30340;&#26679;&#26412;&#65292;&#23558;&#32593;&#32476;&#35757;&#32451;&#21040;&#20102;&#20154;&#21475;&#35823;&#24046;&#20026;$o(1)$&#30340;&#31243;&#24230;&#12290;&#36825;&#26159;&#39318;&#27425;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#19978;&#20197;&#21450;&#26631;&#20934;&#35757;&#32451;&#19979;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#21521;&#21516;&#24615;&#25968;&#25454;&#19978;&#39640;&#25928;&#23398;&#20064;XOR&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\tilde{O}(d)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#20855;&#26377;&#20108;&#27425;&#30495;&#23454;&#20989;&#25968;&#20998;&#38548;&#25968;&#25454;&#30340;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#20174;$d$&#32500;&#24067;&#23572;&#36229;&#31435;&#26041;&#20307;&#20013;&#30001;&#20108;&#27425;&#8220;XOR&#8221;&#20989;&#25968;$y = -x_ix_j$&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#36890;&#36807;&#26631;&#20934;&#23567;&#25209;&#37327;SGD&#22312;&#36923;&#36753;&#25439;&#22833;&#19978;&#21516;&#26102;&#35757;&#32451;&#20004;&#23618;ReLU&#28608;&#27963;&#30340;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;$d \:\text{polylog}(d)$&#20010;&#26679;&#26412;&#23558;&#20854;&#35757;&#32451;&#21040;&#20154;&#21475;&#35823;&#24046;&#20026;$o(1)$&#30340;&#31243;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#32473;&#20986;&#20102;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#19978;&#20197;&#21450;&#26631;&#20934;&#35757;&#32451;&#19979;&#65292;&#23545;&#20110;&#22312;&#21508;&#21521;&#21516;&#24615;&#25968;&#25454;&#19978;&#39640;&#25928;&#23398;&#20064;XOR&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\tilde{O}(d)$&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#26159;&#23637;&#31034;&#32593;&#32476;&#28436;&#21270;&#26377;&#20004;&#20010;&#38454;&#27573;&#65306;&#19968;&#20010;&#8221;&#20449;&#21495;&#21457;&#29616;&#8220;&#38454;&#27573;&#65292;&#22312;&#27492;&#32593;&#32476;&#35268;&#27169;&#36739;&#23567;&#19988;&#35768;&#22810;&#31070;&#32463;&#20803;&#29420;&#31435;&#28436;&#21270;&#20197;&#23547;&#25214;&#29305;&#24449;&#65292;&#20197;&#21450;&#19968;&#20010;&#8221;&#20449;&#21495;&#23494;&#38598;&#8220;&#38454;&#27573;&#65292;&#20854;&#20013;&#35768;&#22810;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#20197;&#20248;&#21270;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we consider the optimization process of minibatch stochastic gradient descent (SGD) on a 2-layer neural network with data separated by a quadratic ground truth function. We prove that with data drawn from the $d$-dimensional Boolean hypercube labeled by the quadratic ``XOR'' function $y = -x_ix_j$, it is possible to train to a population error $o(1)$ with $d \:\text{polylog}(d)$ samples. Our result considers simultaneously training both layers of the two-layer-neural network with ReLU activations via standard minibatch SGD on the logistic loss. To our knowledge, this work is the first to give a sample complexity of $\tilde{O}(d)$ for efficiently learning the XOR function on isotropic data on a standard neural network with standard training. Our main technique is showing that the network evolves in two phases: a $\textit{signal-finding}$ phase where the network is small and many of the neurons evolve independently to find features, and a $\textit{signal-heavy}$ phase, wher
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15048</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class Incremental Learning via Likelihood Ratio Based Task Prediction. (arXiv:2309.15048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26159;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#26029;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#36890;&#36807;&#39034;&#24207;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#27599;&#20010;&#20219;&#21153;&#30001;&#19968;&#32452;&#21807;&#19968;&#30340;&#31867;&#32452;&#25104;&#12290;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102;&#19981;&#25552;&#20379;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#26631;&#35782;&#31526;&#65288;&#25110;&#20219;&#21153;ID&#65289;&#12290;&#20026;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#39044;&#27979;&#20219;&#21153;ID&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19968;&#31181;&#26032;&#20852;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#26681;&#25454;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20849;&#20139;&#32593;&#32476;&#20013;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#27599;&#20010;&#20219;&#21153;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#20197;&#22788;&#29702;&#36951;&#24536;&#12290;&#35813;&#26041;&#27861;&#20013;&#27599;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#38750;&#24120;&#35268;&#20998;&#31867;&#22120;&#32780;&#19981;&#26159;&#20256;&#32479;&#20998;&#31867;&#22120;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#12290;&#31163;&#32676;&#26816;&#27979;&#22120;&#21487;&#20197;&#23545;&#20219;&#21153;&#20869;&#65288;&#20998;&#24067;&#20869;&#65288;IND&#65289;&#65289;&#30340;&#31867;&#36827;&#34892;&#39044;&#27979;&#21644;&#35782;&#21035;&#31163;&#32676;&#25968;&#25454;&#12290;&#22312;&#25512;&#26029;&#26399;&#38388;&#65292;&#31163;&#32676;&#26816;&#27979;&#33021;&#21147;&#26159;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;ID&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#35748;&#20026;&#20351;&#29992;&#20256;&#32479;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;ID&#39044;&#27979;&#26159;&#27425;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample. Predicting the task-id for each test sample is a challenging problem. An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting. The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier. The OOD detector can perform both within-task (in-distribution (IND)) class prediction and OOD detection. The OOD detection capability is the key for task-id prediction during inference for each test sample. However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14681</link><description>&lt;p&gt;
&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26377;&#24517;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#33391;&#22909;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#20294;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#26631;&#20934;&#33539;&#24335;&#20013;&#23384;&#22312;&#20197;&#19979;&#24330;&#31471;&#65306;&#26131;&#21463;&#36873;&#23450;&#28436;&#31034;&#30340;&#24433;&#21709;&#65292;&#29983;&#25104;&#36825;&#20123;&#28436;&#31034;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;ICL&#65292;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19981;&#20381;&#36182;&#20154;&#24037;&#28436;&#31034;&#30340;&#33539;&#20363;&#12290;SEC&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#65292;&#19981;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#31034;&#20363;&#20316;&#20026;ICL&#20013;&#30340;&#28436;&#31034;&#65292;&#32780;&#26159;&#35201;&#27714;LLMs&#39318;&#20808;&#33258;&#34892;&#21019;&#24314;&#28436;&#31034;&#65292;&#28982;&#21518;&#29983;&#25104;&#26368;&#32456;&#36755;&#20986;&#12290;SEC&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21487;&#36866;&#24212;&#21407;&#22987;ICL&#21644;&#8220;&#24605;&#32500;&#38142;&#8221;&#65288;CoT&#65289;&#65292;&#24182;&#19988;&#26356;&#21152;&#20415;&#25463;&#65306;&#22240;&#20026;&#21487;&#20197;&#33410;&#30465;&#31034;&#20363;&#21644;&#29702;&#30001;&#30340;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#31639;&#26415;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understandin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#12289;&#27604;&#36739;&#21644;&#36870;&#21521;&#25628;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2309.14402</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;3.2&#37096;&#20998;&#65292;&#30693;&#35782;&#25805;&#25511;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 3.2, Knowledge Manipulation. (arXiv:2309.14402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#12289;&#27604;&#36739;&#21644;&#36870;&#21521;&#25628;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23384;&#20648;&#22823;&#37327;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#22312;&#20351;&#29992;&#36825;&#20123;&#30693;&#35782;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#20854;&#23384;&#20648;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#22235;&#31181;&#25805;&#25511;&#31867;&#22411;&#65306;&#26816;&#32034;&#65288;&#20363;&#22914;&#65292;&#8220;A&#30340;&#23646;&#24615;X&#26159;&#20160;&#20040;&#8221;&#65289;&#12289;&#20998;&#31867;&#65288;&#20363;&#22914;&#65292;&#8220;A&#30340;&#23646;&#24615;X&#26159;&#22855;&#25968;&#36824;&#26159;&#20598;&#25968;&#8221;&#65289;&#12289;&#27604;&#36739;&#65288;&#20363;&#22914;&#65292;&#8220;&#22312;&#23646;&#24615;X&#20013;A&#26159;&#21542;&#22823;&#20110;B&#8221;&#65289;&#21644;&#36870;&#21521;&#25628;&#32034;&#65288;&#20363;&#22914;&#65292;&#8220;&#21738;&#20010;&#20154;&#30340;&#23646;&#24615;X&#31561;&#20110;T&#8221;&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20687;GPT2/3/4&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#25110;&#27604;&#36739;&#20219;&#21153;&#20013;&#24456;&#38590;&#32988;&#20219;&#65292;&#38500;&#38750;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;Chain of Thoughts&#65288;CoTs&#65289;&#12290;&#26080;&#35770;&#25552;&#31034;&#26159;&#20160;&#20040;&#65292;&#23427;&#20204;&#22312;&#36870;&#21521;&#30693;&#35782;&#25628;&#32034;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#20026;&#25511;&#21046;&#23454;&#39564;&#32780;&#35774;&#35745;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35777;&#23454;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models can store vast amounts of factual knowledge, but their ability to use this knowledge for logical reasoning remains questionable. This paper explores a language model's ability to manipulate its stored knowledge during inference. We focus on four manipulation types: retrieval (e.g., "What is person A's attribute X"), classification (e.g., "Is A's attribute X even or odd?"), comparison (e.g., "Is A greater than B in attribute X?") and inverse search (e.g., "Which person's attribute X equals T?")  We observe that pre-trained language models like GPT2/3/4 excel in knowledge retrieval but struggle with simple classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. They also perform poorly in inverse knowledge search, irrespective of the prompts. Our primary contribution is a synthetic dataset for a controlled experiment that confirms these inherent weaknesses: a language model cannot efficiently manipulate knowledge
&lt;/p&gt;</description></item><item><title>LinGCN&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#20056;&#27861;&#28145;&#24230;&#21644;&#20248;&#21270;HE&#22522;&#20110;GCN&#25512;&#26029;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#32447;&#24615;&#21270;&#31639;&#27861;&#21644;&#21442;&#25968;&#21270;&#30340;&#31163;&#25955;&#25351;&#31034;&#20989;&#25968;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#33410;&#28857;&#32423;&#38750;&#32447;&#24615;&#20301;&#32622;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.14331</link><description>&lt;p&gt;
LinGCN: &#32467;&#26500;&#21270;&#30340;&#32447;&#24615;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#21516;&#24577;&#21152;&#23494;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference. (arXiv:2309.14331v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14331
&lt;/p&gt;
&lt;p&gt;
LinGCN&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#20056;&#27861;&#28145;&#24230;&#21644;&#20248;&#21270;HE&#22522;&#20110;GCN&#25512;&#26029;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#32447;&#24615;&#21270;&#31639;&#27861;&#21644;&#21442;&#25968;&#21270;&#30340;&#31163;&#25955;&#25351;&#31034;&#20989;&#25968;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#33410;&#28857;&#32423;&#38750;&#32447;&#24615;&#20301;&#32622;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#38271;&#24050;&#32463;&#22312;&#20010;&#20154;&#21307;&#30103;&#21644;&#37329;&#34701;&#31995;&#32479;&#31561;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#34920;&#29616;&#30340;&#38761;&#21629;&#24615;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20113;&#31471;&#37096;&#32626;GCN&#24341;&#21457;&#20102;&#23545;&#23458;&#25143;&#25968;&#25454;&#21487;&#33021;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#65292;&#37319;&#29992;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65288;PPML&#65289;&#21487;&#20197;&#30830;&#20445;&#25935;&#24863;&#23458;&#25143;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#24341;&#20837;&#20102;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LinGCN&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#20056;&#27861;&#28145;&#24230;&#24182;&#20248;&#21270;HE&#22522;&#20110;GCN&#25512;&#26029;&#24615;&#33021;&#30340;&#26694;&#26550;&#12290;LinGCN&#22260;&#32469;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#23637;&#24320;&#65306;&#65288;1&#65289;&#21487;&#24494;&#30340;&#32467;&#26500;&#21270;&#32447;&#24615;&#21270;&#31639;&#27861;&#65292;&#25645;&#37197;&#21442;&#25968;&#21270;&#30340;&#31163;&#25955;&#25351;&#31034;&#20989;&#25968;&#65292;&#36890;&#36807;&#19982;&#27169;&#22411;&#26435;&#37325;&#19968;&#36215;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#20197;&#28385;&#36275;&#20248;&#21270;&#30446;&#26631;&#12290;&#36825;&#31181;&#31574;&#30053;&#20419;&#36827;&#20102;&#32454;&#31890;&#24230;&#30340;&#33410;&#28857;&#32423;&#38750;&#32447;&#24615;&#20301;&#32622;&#36873;&#25321;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
The growth of Graph Convolution Network (GCN) model sizes has revolutionized numerous applications, surpassing human performance in areas such as personal healthcare and financial systems. The deployment of GCNs in the cloud raises privacy concerns due to potential adversarial attacks on client data. To address security concerns, Privacy-Preserving Machine Learning (PPML) using Homomorphic Encryption (HE) secures sensitive client data. However, it introduces substantial computational overhead in practical applications. To tackle those challenges, we present LinGCN, a framework designed to reduce multiplication depth and optimize the performance of HE based GCN inference. LinGCN is structured around three key elements: (1) A differentiable structural linearization algorithm, complemented by a parameterized discrete indicator function, co-trained with model weights to meet the optimization goal. This strategy promotes fine-grained node-level non-linear location selection, resulting in a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#32570;&#36135;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#22810;&#31181;&#20998;&#31867;&#25216;&#26415;&#21644;&#25104;&#26412;&#32771;&#34385;&#65292;&#36890;&#36807;&#25552;&#39640;&#26381;&#21153;&#27700;&#24179;&#26469;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#21644;&#25972;&#20307;&#32452;&#32455;&#32489;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.13837</link><description>&lt;p&gt;
&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#32570;&#36135;&#39044;&#27979;: &#20998;&#31867;&#25216;&#26415;&#21644;&#25104;&#26412;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Backorder Prediction in Inventory Management: Classification Techniques and Cost Considerations. (arXiv:2309.13837v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#32570;&#36135;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#22810;&#31181;&#20998;&#31867;&#25216;&#26415;&#21644;&#25104;&#26412;&#32771;&#34385;&#65292;&#36890;&#36807;&#25552;&#39640;&#26381;&#21153;&#27700;&#24179;&#26469;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#21644;&#25972;&#20307;&#32452;&#32455;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#32570;&#36135;&#24773;&#20917;&#12290;&#32570;&#36135;&#26159;&#25351;&#30001;&#20110;&#24211;&#23384;&#32791;&#23613;&#32780;&#26080;&#27861;&#31435;&#21363;&#28385;&#36275;&#30340;&#35746;&#21333;&#12290;&#26412;&#25991;&#20351;&#29992;&#22810;&#31181;&#20998;&#31867;&#25216;&#26415;&#65292;&#21253;&#25324;&#24179;&#34913;&#35013;&#34955;&#20998;&#31867;&#22120;&#12289;&#27169;&#31946;&#36923;&#36753;&#12289;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;-&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;ROC-AUC&#21644;PR-AUC&#31561;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#32771;&#34385;&#20102;&#21033;&#28070;&#20989;&#25968;&#21644;&#38169;&#20998;&#25104;&#26412;&#65292;&#32771;&#34385;&#20102;&#19982;&#24211;&#23384;&#31649;&#29702;&#21644;&#32570;&#36135;&#22788;&#29702;&#30456;&#20851;&#30340;&#36130;&#21153;&#24433;&#21709;&#21644;&#25104;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#27979;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;&#24211;&#23384;&#31995;&#32479;&#30340;&#26381;&#21153;&#27700;&#24179;&#65292;&#20174;&#32780;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#21644;&#25972;&#20307;&#32452;&#32455;&#32489;&#25928;&#12290;&#22312;&#21830;&#19994;&#24212;&#29992;&#20013;&#32771;&#34385;&#21487;&#35299;&#37322;&#24615;&#26159;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#26412;&#30740;&#31350;&#36824;&#24212;&#29992;&#20102;&#25490;&#21015;&#37325;&#35201;&#24615;&#26041;&#27861;&#26469;&#36873;&#25321;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces an advanced analytical approach for predicting backorders in inventory management. Backorder refers to an order that cannot be immediately fulfilled due to stock depletion. Multiple classification techniques, including Balanced Bagging Classifiers, Fuzzy Logic, Variational Autoencoder - Generative Adversarial Networks, and Multi-layer Perceptron classifiers, are assessed in this work using performance evaluation metrics such as ROC-AUC and PR-AUC. Moreover, this work incorporates a profit function and misclassification costs, considering the financial implications and costs associated with inventory management and backorder handling. The results demonstrate the effectiveness of the predictive model in enhancing inventory system service levels, which leads to customer satisfaction and overall organizational performance. Considering interpretability is a significant aspect of using AI in commercial applications, permutation importance is applied to the selected mo
&lt;/p&gt;</description></item><item><title>MaGNet&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13459</link><description>&lt;p&gt;
&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25972;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Model-Agnostic Graph Neural Network for Integrating Local and Global Information. (arXiv:2309.13459v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13459
&lt;/p&gt;
&lt;p&gt;
MaGNet&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#20197;&#22270;&#20026;&#37325;&#28857;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;GNN&#23384;&#22312;&#20004;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#30001;&#20110;&#40657;&#30418;&#29305;&#24615;&#65292;&#32467;&#26524;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65307;&#26080;&#27861;&#23398;&#20064;&#19981;&#21516;&#39034;&#24207;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MaGNet&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#20174;&#39640;&#38454;&#37051;&#23621;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;MaGNet&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#22270;&#25299;&#25169;&#19979;&#22797;&#26434;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#20272;&#35745;&#27169;&#22411;&#21644;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#37325;&#35201;&#33410;&#28857;&#29305;&#24449;&#30340;&#35299;&#37322;&#27169;&#22411;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;Rademacher&#22797;&#26434;&#24230;&#24314;&#31435;&#20102;MaGNet&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved promising performance in a variety of graph-focused tasks. Despite their success, existing GNNs suffer from two significant limitations: a lack of interpretability in results due to their black-box nature, and an inability to learn representations of varying orders. To tackle these issues, we propose a novel Model-agnostic Graph Neural Network (MaGNet) framework, which is able to sequentially integrate information of various orders, extract knowledge from high-order neighbors, and provide meaningful and interpretable results by identifying influential compact graph structures. In particular, MaGNet consists of two components: an estimation model for the latent representation of complex relationships under graph topology, and an interpretation model that identifies influential nodes, edges, and important node features. Theoretically, we establish the generalization error bound for MaGNet via empirical Rademacher complexity, and showcase its pow
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22534;&#21472;&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36275;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;&#20854;&#21152;&#24378;&#20102;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#24207;&#21015;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24182;&#19981;&#33021;&#26681;&#26412;&#35299;&#20915;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13414</link><description>&lt;p&gt;
&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26159;&#24102;&#26377;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#20840;&#33021;&#36924;&#36817;&#22120;
&lt;/p&gt;
&lt;p&gt;
State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory. (arXiv:2309.13414v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22534;&#21472;&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36275;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;&#20854;&#21152;&#24378;&#20102;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#24207;&#21015;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24182;&#19981;&#33021;&#26681;&#26412;&#35299;&#20915;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#31616;&#21333;&#26377;&#25928;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#24207;&#21015;&#24314;&#27169;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#27839;&#26102;&#38388;&#26041;&#21521;&#32570;&#20047;&#38750;&#32447;&#24615;&#28608;&#27963;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22534;&#21472;&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36275;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#28155;&#21152;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#24207;&#21015;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21487;&#20197;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30475;&#21040;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24182;&#19981;&#26681;&#26412;&#35299;&#20915;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#38382;&#39064;&#12290;&#29702;&#35770;&#32467;&#26524;&#32463;&#36807;&#20102;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the exponential decaying memory issue. Theoretical results are justified by numerical verifications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#38750;&#21516;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#30340;&#32479;&#19968;&#35823;&#24046;&#37327;&#21270;&#26694;&#26550;&#65292;&#24182;&#35299;&#20915;&#20102;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#21306;&#38388;&#20869;&#20849;&#21516;&#37327;&#21270;&#20004;&#20010;&#20272;&#35745;&#35823;&#24046;&#28304;&#65292;&#35813;&#26694;&#26550;&#25581;&#31034;&#20102;&#20043;&#21069;&#38544;&#34255;&#30340;&#35823;&#24046;&#26435;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32622;&#20449;&#21306;&#38388;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13278</link><description>&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;&#24863;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;&#38750;&#21516;&#31574;&#30053;&#21306;&#38388;&#20272;&#35745;&#26041;&#27861;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#35823;&#24046;&#37327;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Distributional Shift-Aware Off-Policy Interval Estimation: A Unified Error Quantification Framework. (arXiv:2309.13278v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#38750;&#21516;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#30340;&#32479;&#19968;&#35823;&#24046;&#37327;&#21270;&#26694;&#26550;&#65292;&#24182;&#35299;&#20915;&#20102;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#21306;&#38388;&#20869;&#20849;&#21516;&#37327;&#21270;&#20004;&#20010;&#20272;&#35745;&#35823;&#24046;&#28304;&#65292;&#35813;&#26694;&#26550;&#25581;&#31034;&#20102;&#20043;&#21069;&#38544;&#34255;&#30340;&#35823;&#24046;&#26435;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32622;&#20449;&#21306;&#38388;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#38480;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#39640;&#32622;&#20449;&#24230;&#38750;&#21516;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#20165;&#21033;&#29992;&#20174;&#26410;&#30693;&#34892;&#20026;&#31574;&#30053;&#39044;&#20808;&#25910;&#38598;&#30340;&#31163;&#32447;&#25968;&#25454;&#20026;&#30446;&#26631;&#31574;&#30053;&#30340;&#20540;&#24314;&#31435;&#19968;&#20010;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#12290;&#35813;&#20219;&#21153;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#22312;CI&#20272;&#35745;&#20013;&#25552;&#20379;&#20840;&#38754;&#19988;&#20005;&#26684;&#30340;&#35823;&#24046;&#37327;&#21270;&#65292;&#24182;&#35299;&#20915;&#30001;&#30446;&#26631;&#31574;&#30053;&#20135;&#29983;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#35813;&#20998;&#24067;&#19982;&#31163;&#32447;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#21463;&#21040;&#21019;&#26032;&#30340;&#32479;&#19968;&#35823;&#24046;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#21306;&#38388;&#20869;&#20849;&#21516;&#37327;&#21270;&#20004;&#20010;&#20272;&#35745;&#35823;&#24046;&#26469;&#28304;&#65306;&#22312;&#24314;&#27169;&#36793;&#38469;&#21270;&#37325;&#35201;&#24615;&#26435;&#37325;&#26102;&#30340;&#35268;&#33539;&#19981;&#20934;&#30830;&#35823;&#24046;&#21644;&#25277;&#26679;&#23548;&#33268;&#30340;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#19968;&#32479;&#19968;&#30340;&#26694;&#26550;&#25581;&#31034;&#20102;&#35823;&#24046;&#20043;&#38388;&#20197;&#21069;&#38544;&#34255;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;CI&#30340;&#32039;&#23494;&#24615;&#12290;&#36890;&#36807;&#20381;&#38752;&#31934;&#24515;&#35774;&#35745;&#30340;&#21028;&#21035;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20811;&#26381;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study high-confidence off-policy evaluation in the context of infinite-horizon Markov decision processes, where the objective is to establish a confidence interval (CI) for the target policy value using only offline data pre-collected from unknown behavior policies. This task faces two primary challenges: providing a comprehensive and rigorous error quantification in CI estimation, and addressing the distributional shift that results from discrepancies between the distribution induced by the target policy and the offline data-generating process. Motivated by an innovative unified error analysis, we jointly quantify the two sources of estimation errors: the misspecification error on modeling marginalized importance weights and the statistical uncertainty due to sampling, within a single interval. This unified framework reveals a previously hidden tradeoff between the errors, which undermines the tightness of the CI. Relying on a carefully designed discriminator function, the proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#65292;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#26799;&#24230;&#19979;&#38477;&#21464;&#31181;&#65292;&#30830;&#23450;&#20102;&#19968;&#20010;&#31283;&#23450;&#24615;&#36793;&#30028;&#65292;&#35813;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.12488</link><description>&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#21644;&#31283;&#23450;&#24615;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization and the Edge of Stability. (arXiv:2309.12488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#65292;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#26799;&#24230;&#19979;&#38477;&#21464;&#31181;&#65292;&#30830;&#23450;&#20102;&#19968;&#20010;&#31283;&#23450;&#24615;&#36793;&#30028;&#65292;&#35813;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;(GD)&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#30340;&#25805;&#20316;&#31526;&#33539;&#25968;&#20250;&#22686;&#38271;&#65292;&#30452;&#21040;&#25509;&#36817;$2/\eta$&#65292;&#20043;&#21518;&#20250;&#22312;&#35813;&#20540;&#21608;&#22260;&#27874;&#21160;&#12290;&#26681;&#25454;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#20108;&#27425;&#36924;&#36817;&#65292;$2/\eta$&#34987;&#31216;&#20026;&#8220;&#31283;&#23450;&#24615;&#36793;&#30028;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#30830;&#23450;&#20102;&#19968;&#20010;&#8220;&#31283;&#23450;&#24615;&#36793;&#30028;&#8221;&#65292;SAM&#26159;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;GD&#21464;&#31181;&#12290;&#19982;GD&#19981;&#21516;&#65292;SAM&#30340;&#31283;&#23450;&#24615;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;&#36890;&#36807;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#23454;&#35777;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;SAM&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#30830;&#23450;&#30340;&#31283;&#23450;&#24615;&#36793;&#30028;&#19978;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\eta$, after which it fluctuates around this value.  The quantity $2/\eta$ has been called the "edge of stability" based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an "edge of stability" for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#24605;&#36335;&#38142;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#36890;&#24120;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#26356;&#21152;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#20855;&#26377;&#26356;&#22823;&#22810;&#26679;&#24615;&#19988;&#24615;&#33021;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#36739;&#22909;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#24605;&#36335;&#38142;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2309.11054</link><description>&lt;p&gt;
&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#24605;&#36335;&#38142;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Design of Chain-of-Thought in Math Problem Solving. (arXiv:2309.11054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#24605;&#36335;&#38142;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#36890;&#24120;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#26356;&#21152;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#20855;&#26377;&#26356;&#22823;&#22810;&#26679;&#24615;&#19988;&#24615;&#33021;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#36739;&#22909;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#24605;&#36335;&#38142;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#36335;&#38142;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#23545;&#35774;&#35745;&#24605;&#36335;&#38142;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32771;&#23519;&#65292;&#27604;&#36739;&#20102;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#21508;&#31181;&#31243;&#24207;&#24605;&#36335;&#38142;&#65292;&#21253;&#25324;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#12289;&#27880;&#37322;&#25551;&#36848;&#31243;&#24207;&#21644;&#38750;&#25551;&#36848;&#31243;&#24207;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#32534;&#31243;&#35821;&#35328;&#23545;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#24433;&#21709;&#65292;&#27604;&#36739;&#20102;Python&#21644;Wolfram&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;GSM8K&#12289;MATHQA&#21644;SVAMP&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#36890;&#24120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20855;&#26377;30B&#21442;&#25968;&#30340;&#26368;&#20339;&#32452;&#21512;&#26126;&#26174;&#36229;&#36807;&#20102;GPT-3.5-turbo&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#22810;&#26679;&#24615;&#65292;&#22240;&#27492;&#36890;&#24120;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#26356;&#22909;&#36873;&#25321;&#27604;Wolfram&#35821;&#35328;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#32771;&#34385;&#22240;&#32032;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem solving. We conduct a comprehensive examination of methods for designing CoT, comparing conventional natural language CoT with various program CoTs, including the self-describing program, the comment-describing program, and the non-describing program. Furthermore, we investigate the impact of programming language on program CoTs, comparing Python and Wolfram Language. Through extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs often have superior effectiveness in math problem solving. Notably, the best performing combination with 30B parameters beats GPT-3.5-turbo by a significant margin. The results show that self-describing program offers greater diversity and thus can generally achieve higher performance. We also find that Python is a better choice of language than Wolfram for program CoTs. The experimental results provide a valuable guideline for future CoT designs that take into acco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#19979;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20219;&#21153;&#22270;&#31163;&#36733;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#24448;&#24448;&#26080;&#27861;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20219;&#21153;&#22270;&#35843;&#24230;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#35745;&#31639;&#33021;&#21147;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.10569</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20219;&#21153;&#22270;&#31163;&#36733;
&lt;/p&gt;
&lt;p&gt;
Task Graph offloading via Deep Reinforcement Learning in Mobile Edge Computing. (arXiv:2309.10569v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#19979;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20219;&#21153;&#22270;&#31163;&#36733;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#24448;&#24448;&#26080;&#27861;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20219;&#21153;&#22270;&#35843;&#24230;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#35745;&#31639;&#33021;&#21147;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20854;&#20013;&#21253;&#21547;&#30340;&#20381;&#36182;&#20219;&#21153;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#36890;&#24120;&#20855;&#26377;&#20302;&#24310;&#36831;&#35201;&#27714;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#24613;&#21095;&#22686;&#21152;&#12290;&#38543;&#30528;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#30340;&#20986;&#29616;&#65292;&#23558;&#24212;&#29992;&#31243;&#24207;&#20219;&#21153;&#21368;&#36733;&#21040;&#37096;&#32626;&#22312;&#31227;&#21160;&#32593;&#32476;&#36793;&#32536;&#30340;&#23567;&#22411;&#35774;&#22791;&#19978;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#29992;&#25143;&#20307;&#39564;&#25104;&#20026;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;MEC&#29615;&#22659;&#26159;&#21160;&#24577;&#30340;&#65292;&#22823;&#22810;&#25968;&#20381;&#36182;&#19987;&#23478;&#30693;&#35782;&#25110;&#20934;&#30830;&#30340;&#20998;&#26512;&#27169;&#22411;&#30340;&#29616;&#26377;&#24037;&#20316;&#22312;&#20219;&#21153;&#22270;&#31163;&#36733;&#26041;&#38754;&#26080;&#27861;&#23436;&#20840;&#36866;&#24212;&#36825;&#31181;&#29615;&#22659;&#21464;&#21270;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#38477;&#20302;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;MEC&#20013;&#30340;&#20219;&#21153;&#22270;&#31163;&#36733;&#65292;&#32771;&#34385;&#21040;&#36793;&#32536;&#35745;&#31639;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#20026;&#20102;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#25105;&#20204;&#23558;&#35745;&#31639;&#31163;&#36733;&#30340;&#20219;&#21153;&#22270;&#35843;&#24230;&#24314;&#27169;&#20026;&#19968;&#20010;Markov&#20915;&#31574;&#36807;&#31243;&#65288;Markov Decision Process&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various mobile applications that comprise dependent tasks are gaining widespread popularity and are increasingly complex. These applications often have low-latency requirements, resulting in a significant surge in demand for computing resources. With the emergence of mobile edge computing (MEC), it becomes the most significant issue to offload the application tasks onto small-scale devices deployed at the edge of the mobile network for obtaining a high-quality user experience. However, since the environment of MEC is dynamic, most existing works focusing on task graph offloading, which rely heavily on expert knowledge or accurate analytical models, fail to fully adapt to such environmental changes, resulting in the reduction of user experience. This paper investigates the task graph offloading in MEC, considering the time-varying computation capabilities of edge computing devices. To adapt to environmental changes, we model the task graph scheduling for computation offloading as a Mark
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#24182;&#34892;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20013;&#22521;&#20859;&#21512;&#20316;&#19982;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#29983;&#24577;&#31995;&#32479;&#24320;&#21457;&#20102;&#20934;&#30830;&#29289;&#29702;&#21644;&#36924;&#30495;&#22270;&#24418;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35757;&#32451;&#21644;&#37096;&#32626;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.10007</link><description>&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#38388;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#19982;&#31454;&#20105;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem. (arXiv:2309.10007v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#24182;&#34892;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20013;&#22521;&#20859;&#21512;&#20316;&#19982;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#29983;&#24577;&#31995;&#32479;&#24320;&#21457;&#20102;&#20934;&#30830;&#29289;&#29702;&#21644;&#36924;&#30495;&#22270;&#24418;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35757;&#32451;&#21644;&#37096;&#32626;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#24182;&#34892;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#22521;&#20859;&#21512;&#20316;&#21644;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20316;&#20026;&#19968;&#20010;&#24037;&#20855;&#65292;&#24320;&#21457;&#20986;&#19982;&#30495;&#23454;&#30340;Nigel&#21644;F1TENTH&#20004;&#31181;&#27604;&#20363;&#33258;&#20027;&#36710;&#36742;&#24179;&#21488;&#20855;&#26377;&#29420;&#29305;&#29305;&#24615;&#21644;&#33021;&#21147;&#30340;&#20934;&#30830;&#29289;&#29702;&#21644;&#36924;&#30495;&#22270;&#24418;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#29983;&#24577;&#31995;&#32479;&#26469;&#35757;&#32451;&#21644;&#37096;&#32626;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#20010;&#20132;&#21449;&#36335;&#21475;&#31359;&#36234;&#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#32452;&#21512;&#20316;&#36710;&#36742;&#65288;Nigel&#65289;&#22312;&#21333;&#20010;&#25110;&#22810;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#20849;&#20139;&#26377;&#38480;&#29366;&#24577;&#20449;&#24687;&#65292;&#37319;&#29992;&#19968;&#31181;&#20844;&#20849;&#31574;&#30053;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#30340;&#22836;&#23545;&#22836;&#33258;&#20027;&#36187;&#36710;&#38382;&#39064;&#65292;&#20351;&#29992;&#21478;&#19968;&#32452;&#36710;&#36742;&#65288;F1TENTH&#65289;&#22312;&#22810;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#37319;&#29992;&#20010;&#20307;&#31574;&#30053;&#26041;&#27861;&#12290;&#22312;&#20219;&#20309;&#19968;&#32452;&#23454;&#39564;&#20013;&#65292;&#37117;&#37319;&#29992;&#20102;&#20998;&#25955;&#23398;&#20064;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a modular and parallelizable multi-agent deep reinforcement learning framework for imbibing cooperative as well as competitive behaviors within autonomous vehicles. We introduce AutoDRIVE Ecosystem as an enabler to develop physically accurate and graphically realistic digital twins of Nigel and F1TENTH, two scaled autonomous vehicle platforms with unique qualities and capabilities, and leverage this ecosystem to train and deploy multi-agent reinforcement learning policies. We first investigate an intersection traversal problem using a set of cooperative vehicles (Nigel) that share limited state information with each other in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial head-to-head autonomous racing problem using a different set of vehicles (F1TENTH) in a multi-agent learning setting using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#28216;&#25103;&#35268;&#21017;&#29983;&#25104;&#30340;&#20154;&#31867;&#28216;&#25103;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#30340;&#35268;&#21017;&#19982;&#20256;&#32479;&#22522;&#32447;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#65292;&#21487;&#33021;&#26356;&#36866;&#21512;&#20154;&#31867;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.09476</link><description>&lt;p&gt;
&#26426;&#26800;&#21270;&#29983;&#25104;&#22120;2.0: &#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#30340;&#28216;&#25103;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules. (arXiv:2309.09476v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#28216;&#25103;&#35268;&#21017;&#29983;&#25104;&#30340;&#20154;&#31867;&#28216;&#25103;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#30340;&#35268;&#21017;&#19982;&#20256;&#32479;&#22522;&#32447;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#65292;&#21487;&#33021;&#26356;&#36866;&#21512;&#20154;&#31867;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#28216;&#25103;&#35774;&#35745;&#65288;AGD&#65289;&#26159;&#30740;&#31350;&#33258;&#21160;&#29983;&#25104;&#28216;&#25103;&#35268;&#21017;&#30340;&#25216;&#26415;&#28216;&#25103;&#30740;&#31350;&#30340;&#19968;&#20010;&#38271;&#26399;&#35838;&#39064;&#12290; AGD&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#20154;&#31867;&#29609;&#23478;&#28216;&#25103;&#30340;&#36817;&#20284;&#65292;&#21487;&#20197;&#26159;&#23458;&#35266;&#20989;&#25968;&#25110;AI&#20195;&#29702;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#37096;&#20998;&#36825;&#20123;&#36817;&#20284;&#22120;&#26159;&#38745;&#24577;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#23427;&#20204;&#19981;&#33021;&#21453;&#26144;&#20154;&#31867;&#29609;&#23478;&#22312;&#28216;&#25103;&#20013;&#30340;&#23398;&#20064;&#21644;&#25552;&#39640;&#33021;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24212;&#29992;&#20110;&#29983;&#25104;&#35268;&#21017;&#30340;&#20154;&#31867;&#28216;&#25103;&#35780;&#20272;&#20013;&#12290;&#25105;&#20204;&#22312;Unity&#20013;&#37325;&#26032;&#21019;&#24314;&#20102;&#32463;&#20856;&#30340;AGD&#29615;&#22659;Mechanic Maker&#20316;&#20026;&#19968;&#20010;&#20840;&#26032;&#30340;&#24320;&#28304;&#29983;&#25104;&#35268;&#21017;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;RL&#19982;A*&#20195;&#29702;&#22522;&#32447;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#35268;&#21017;&#38598;&#65292;&#36825;&#20123;&#35268;&#21017;&#21487;&#33021;&#26356;&#36866;&#21512;&#20154;&#31867;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated game design (AGD), the study of automatically generating game rules, has a long history in technical games research. AGD approaches generally rely on approximations of human play, either objective functions or AI agents. Despite this, the majority of these approximators are static, meaning they do not reflect human player's ability to learn and improve in a game. In this paper, we investigate the application of Reinforcement Learning (RL) as an approximator for human play for rule generation. We recreate the classic AGD environment Mechanic Maker in Unity as a new, open-source rule generation framework. Our results demonstrate that RL produces distinct sets of rules from an A* agent baseline, which may be more usable by humans.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20869;&#23481;&#22686;&#24378;&#21644;&#32423;&#21035;&#20462;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#25216;&#26415;&#65288;&#33258;&#32534;&#30721;&#22120;&#21644;U-net&#65289;&#26469;&#37325;&#24314;&#21644;&#25193;&#23637;&#35270;&#39057;&#28216;&#25103;&#32423;&#21035;&#12290;&#32463;&#36807;&#32508;&#21512;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.09472</link><description>&lt;p&gt;
&#36890;&#36807;&#32423;&#21035;&#20462;&#22797;&#26469;&#37325;&#26500;&#29616;&#26377;&#32423;&#21035;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Existing Levels through Level Inpainting. (arXiv:2309.09472v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20869;&#23481;&#22686;&#24378;&#21644;&#32423;&#21035;&#20462;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#25216;&#26415;&#65288;&#33258;&#32534;&#30721;&#22120;&#21644;U-net&#65289;&#26469;&#37325;&#24314;&#21644;&#25193;&#23637;&#35270;&#39057;&#28216;&#25103;&#32423;&#21035;&#12290;&#32463;&#36807;&#32508;&#21512;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#24050;&#32463;&#20351;&#29992;&#20102;&#36807;&#31243;&#21270;&#20869;&#23481;&#29983;&#25104;&#65288;PCG&#65289;&#21644;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#30340;&#36807;&#31243;&#21270;&#20869;&#23481;&#29983;&#25104;&#65288;PCGML&#65289;&#26469;&#29983;&#25104;&#21508;&#31181;&#28216;&#25103;&#20013;&#30340;&#32423;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20869;&#23481;&#22686;&#24378;&#65292;&#24182;&#23558;&#37325;&#28857;&#25918;&#22312;&#32423;&#21035;&#20462;&#22797;&#30340;&#23376;&#38382;&#39064;&#19978;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#37325;&#24314;&#21644;&#25193;&#23637;&#35270;&#39057;&#28216;&#25103;&#32423;&#21035;&#12290;&#21463;&#22270;&#20687;&#20462;&#22797;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20174;&#36825;&#19968;&#39046;&#22495;&#20013;&#35843;&#25972;&#20102;&#20004;&#31181;&#25216;&#26415;&#20197;&#35299;&#20915;&#25105;&#20204;&#30340;&#29305;&#23450;&#29992;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32423;&#21035;&#20462;&#22797;&#30340;&#26041;&#27861;&#65306;&#33258;&#32534;&#30721;&#22120;&#21644;U-net&#12290;&#36890;&#36807;&#19968;&#20010;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#30456;&#23545;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#32423;&#21035;&#20462;&#22797;&#20219;&#21153;&#25552;&#20379;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#23454;&#38469;&#28436;&#31034;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural Content Generation (PCG) and Procedural Content Generation via Machine Learning (PCGML) have been used in prior work for generating levels in various games. This paper introduces Content Augmentation and focuses on the subproblem of level inpainting, which involves reconstructing and extending video game levels. Drawing inspiration from image inpainting, we adapt two techniques from this domain to address our specific use case. We present two approaches for level inpainting: an Autoencoder and a U-net. Through a comprehensive case study, we demonstrate their superior performance compared to a baseline method and discuss their relative merits. Furthermore, we provide a practical demonstration of both approaches for the level inpainting task and offer insights into potential directions for future research.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36817;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38543;&#26426;&#34987;&#23457;&#26597;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09454</link><description>&lt;p&gt;
&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#26465;&#20214;&#19979;&#28176;&#36817;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#22312;&#34987;&#23457;&#26597;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Asymptotically Efficient Online Learning for Censored Regression Models Under Non-I.I.D Data. (arXiv:2309.09454v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09454
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36817;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38543;&#26426;&#34987;&#23457;&#26597;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28176;&#36817;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#22312;&#38543;&#26426;&#34987;&#23457;&#26597;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#28041;&#21450;&#21040;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#20173;&#32570;&#20047;&#20851;&#20110;&#23398;&#20064;&#31639;&#27861;&#25928;&#29575;&#30340;&#20840;&#38754;&#29702;&#35770;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#22312;&#32447;&#31639;&#27861;&#65292;&#31532;&#19968;&#27493;&#19987;&#27880;&#20110;&#23454;&#29616;&#31639;&#27861;&#25910;&#25947;&#24615;&#65292;&#31532;&#20108;&#27493;&#29992;&#20110;&#25913;&#21892;&#20272;&#35745;&#24615;&#33021;&#12290;&#22312;&#25968;&#25454;&#30340;&#19968;&#33324;&#28608;&#21169;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#26041;&#27861;&#21644;&#23545;&#38789;&#30340;&#26497;&#38480;&#29702;&#35770;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#24378;&#19968;&#33268;&#30340;&#21644;&#28176;&#36817;&#27491;&#24577;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20272;&#35745;&#20540;&#30340;&#21327;&#26041;&#24046;&#22312;&#28176;&#36817;&#19978;&#21487;&#20197;&#36798;&#21040;&#20811;&#25289;&#32654;&#27931;&#30028;&#65292;&#36825;&#24847;&#21619;&#30528;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#21487;&#20197;&#26399;&#26395;&#30340;&#26368;&#22909;&#30340;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#19981;&#20381;&#36182;&#20256;&#32479;&#26041;&#27861;&#32780;&#24471;&#20986;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The asymptotically efficient online learning problem is investigated for stochastic censored regression models, which arise from various fields of learning and statistics but up to now still lacks comprehensive theoretical studies on the efficiency of the learning algorithms. For this, we propose a two-step online algorithm, where the first step focuses on achieving algorithm convergence, and the second step is dedicated to improving the estimation performance. Under a general excitation condition on the data, we show that our algorithm is strongly consistent and asymptotically normal by employing the stochastic Lyapunov function method and limit theories for martingales. Moreover, we show that the covariances of the estimates can achieve the Cramer-Rao (C-R) bound asymptotically, indicating that the performance of the proposed algorithm is the best possible that one can expect in general. Unlike most of the existing works, our results are obtained without resorting to the traditionall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#24182;&#20998;&#37197;&#36739;&#39640;&#30340;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08375</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20248;&#20808;&#32423;&#37325;&#26032;&#21152;&#26435;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Adaptive Priority Reweighing for Generalizing Fairness Improvement. (arXiv:2309.08375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#24182;&#20998;&#37197;&#36739;&#39640;&#30340;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22312;&#20851;&#38190;&#20915;&#31574;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#21628;&#22768;&#36234;&#26469;&#36234;&#22823;&#12290;&#23613;&#31649;&#24050;&#32463;&#36890;&#36807;&#23398;&#20064;&#20844;&#24179;&#32422;&#26463;&#26469;&#25913;&#21892;&#31639;&#27861;&#30340;&#20844;&#24179;&#24615;&#30340;&#21508;&#31181;&#26041;&#24335;&#65292;&#20294;&#23427;&#20204;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#24615;&#33021;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#38656;&#35201;&#19968;&#31181;&#24615;&#33021;&#26377;&#21069;&#26223;&#19988;&#20855;&#26377;&#26356;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#20844;&#24179;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#20998;&#24067;&#20559;&#31227;&#23545;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#25552;&#35758;&#20026;&#27599;&#20010;&#65288;&#23376;&#65289;&#32452;&#20998;&#37197;&#19968;&#20010;&#32479;&#19968;&#30340;&#26435;&#37325;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32454;&#31890;&#24230;&#22320;&#24314;&#27169;&#20102;&#26679;&#26412;&#39044;&#27979;&#19982;&#20915;&#31574;&#36793;&#30028;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#65292;&#24182;&#20998;&#37197;&#36739;&#39640;&#30340;&#26435;&#37325;&#26469;&#25552;&#39640;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing penetration of machine learning applications in critical decision-making areas, calls for algorithmic fairness are more prominent. Although there have been various modalities to improve algorithmic fairness through learning with fairness constraints, their performance does not generalize well in the test set. A performance-promising fair algorithm with better generalizability is needed. This paper proposes a novel adaptive reweighing method to eliminate the impact of the distribution shifts between training and test data on model generalizability. Most previous reweighing methods propose to assign a unified weight for each (sub)group. Rather, our method granularly models the distance from the sample predictions to the decision boundary. Our adaptive reweighing method prioritizes samples closer to the decision boundary and assigns a higher weight to improve the generalizability of fair classifiers. Extensive experiments are performed to validate the generalizability 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#24449;&#24182;&#19981;&#33021;&#25552;&#39640;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#24341;&#20837;&#20102;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23376;&#31354;&#38388;&#21487;&#20197;&#24674;&#22797;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08374</link><description>&lt;p&gt;
&#29702;&#35299;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Understanding the limitations of self-supervised learning for tabular anomaly detection. (arXiv:2309.08374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#24449;&#24182;&#19981;&#33021;&#25552;&#39640;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#24341;&#20837;&#20102;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23376;&#31354;&#38388;&#21487;&#20197;&#24674;&#22797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#25913;&#36827;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#20294;&#34920;&#26684;&#25968;&#25454;&#26159;&#21542;&#21487;&#20197;&#20174;&#20013;&#21463;&#30410;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;26&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#28041;&#21450;&#21508;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#20102;&#35299;&#36825;&#31181;&#24773;&#20917;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#23454;&#65292;&#19982;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#34920;&#31034;&#30456;&#27604;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#24449;&#24182;&#19981;&#33021;&#25552;&#39640;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26159;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#24341;&#20837;&#20102;&#26080;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23376;&#31354;&#38388;&#21487;&#20197;&#24674;&#22797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While self-supervised learning has improved anomaly detection in computer vision and natural language processing, it is unclear whether tabular data can benefit from it. This paper explores the limitations of self-supervision for tabular anomaly detection. We conduct several experiments spanning various pretext tasks on 26 benchmark datasets to understand why this is the case. Our results confirm representations derived from self-supervision do not improve tabular anomaly detection performance compared to using the raw representations of the data. We show this is due to neural networks introducing irrelevant features, which reduces the effectiveness of anomaly detectors. However, we demonstrate that using a subspace of the neural network's representation can recover performance.
&lt;/p&gt;</description></item><item><title>Landscape-Sketch-Step&#26159;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#31105;&#27490;&#30340;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07936</link><description>&lt;p&gt;
Landscape-Sketch-Step: &#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#35299;&#20915;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate Optimization Problems. (arXiv:2309.07936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07936
&lt;/p&gt;
&lt;p&gt;
Landscape-Sketch-Step&#26159;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#31105;&#27490;&#30340;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25104;&#26412;&#20989;&#25968;&#30340;&#35780;&#20272;&#38750;&#24120;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#29978;&#33267;&#31105;&#27490;&#30340;&#22330;&#26223;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;Landscape-Sketch-Step&#65288;LSS&#65289;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20381;&#36182;&#20110;&#20808;&#21069;&#37319;&#26679;&#28857;&#30340;&#21382;&#21490;&#20449;&#24687;&#65292;&#20197;&#26126;&#26234;&#22320;&#36873;&#25321;&#24212;&#35780;&#20272;&#25104;&#26412;&#20989;&#25968;&#30340;&#21442;&#25968;&#20540;&#12290;&#19982;&#22797;&#21046;&#20132;&#25442;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#25152;&#38656;&#30340;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#19982;&#27169;&#25311;&#36864;&#28779;&#26041;&#27861;&#30456;&#24403;&#65292;&#36825;&#22312;&#39640;&#36890;&#37327;&#35745;&#31639;&#25110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#31561;&#29615;&#22659;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#35780;&#20272;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#35201;&#20040;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#25165;&#33021;&#23436;&#25104;&#12290;&#35813;&#26041;&#27861;&#19982;&#26631;&#20934;&#30340;&#20195;&#29702;&#20248;&#21270;&#25216;&#26415;&#20063;&#19981;&#21516;&#65292;&#22240;&#20026;&#23427;&#19981;&#26500;&#24314;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new heuristics for global optimization in scenarios where extensive evaluations of the cost function are expensive, inaccessible, or even prohibitive. The method, which we call Landscape-Sketch-and-Step (LSS), combines Machine Learning, Stochastic Optimization, and Reinforcement Learning techniques, relying on historical information from previously sampled points to make judicious choices of parameter values where the cost function should be evaluated at. Unlike optimization by Replica Exchange Monte Carlo methods, the number of evaluations of the cost function required in this approach is comparable to that used by Simulated Annealing, quality that is especially important in contexts like high-throughput computing or high-performance computing tasks, where evaluations are either computationally expensive or take a long time to be performed. The method also differs from standard Surrogate Optimization techniques, for it does not construct a surrogate model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#24577;&#23398;&#20064;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;NLGAD&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#23545;&#27604;&#23398;&#20064;&#32593;&#32476;&#26469;&#22686;&#24378;&#23398;&#20064;&#27491;&#24120;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#20197;&#25913;&#36827;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06034</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#24577;&#23398;&#20064;&#30340;&#22810;&#23610;&#24230;&#23545;&#27604;&#23398;&#20064;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning. (arXiv:2309.06034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#24577;&#23398;&#20064;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;NLGAD&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#23545;&#27604;&#23398;&#20064;&#32593;&#32476;&#26469;&#22686;&#24378;&#23398;&#20064;&#27491;&#24120;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#20197;&#25913;&#36827;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#39046;&#22495;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22914;&#20309;&#25429;&#25417;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;GAD&#20013;&#33410;&#28857;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#26816;&#27979;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#20219;&#21153;&#30340;&#29305;&#24615;&#20173;&#28982;&#30456;&#23545;&#19981;&#36275;&#12290;GAD&#26088;&#22312;&#35782;&#21035;&#19982;&#22823;&#22810;&#25968;&#33410;&#28857;&#26377;&#25152;&#20559;&#31163;&#30340;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#24456;&#23481;&#26131;&#23398;&#20064;&#21040;&#32452;&#25104;&#22823;&#22810;&#25968;&#26679;&#26412;&#30340;&#27491;&#24120;&#26679;&#26412;&#30340;&#27169;&#24335;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24403;&#24322;&#24120;&#34892;&#20026;&#19982;&#27491;&#24120;&#24615;&#19981;&#21516;&#30340;&#26102;&#20505;&#65292;&#24322;&#24120;&#21487;&#20197;&#24456;&#23481;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#22686;&#24378;&#23398;&#20064;&#27491;&#24120;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#24577;&#23398;&#20064;&#30340;GAD&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#23610;&#24230;&#23545;&#27604;&#23398;&#20064;&#32593;&#32476;&#65288;&#31616;&#31216;NLGAD&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#19981;&#21516;&#23610;&#24230;&#30340;&#23545;&#27604;&#32593;&#32476;&#21021;&#22987;&#21270;&#27169;&#22411;&#65292;&#20197;&#25552;&#20379;&#20805;&#36275;&#21487;&#38752;&#30340;&#27491;&#24120;&#33410;&#28857;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection (GAD) has attracted increasing attention in machine learning and data mining. Recent works have mainly focused on how to capture richer information to improve the quality of node embeddings for GAD. Despite their significant advances in detection performance, there is still a relative dearth of research on the properties of the task. GAD aims to discern the anomalies that deviate from most nodes. However, the model is prone to learn the pattern of normal samples which make up the majority of samples. Meanwhile, anomalies can be easily detected when their behaviors differ from normality. Therefore, the performance can be further improved by enhancing the ability to learn the normal pattern. To this end, we propose a normality learning-based GAD framework via multi-scale contrastive learning networks (NLGAD for abbreviation). Specifically, we first initialize the model with the contrastive networks on different scales. To provide sufficient and reliable normal nod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22411;AI&#39046;&#22495;&#30340;&#26032;&#25216;&#26415;&#36827;&#34892;&#38646;&#26679;&#26412;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#36716;&#21270;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#35821;&#20041;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#24179;&#31283;&#20869;&#23481;&#30340;&#25512;&#33616;&#12290;&#22312;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#26263;&#31034;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.01026</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#26263;&#31034;&#30340;&#38646;&#26679;&#26412;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging. (arXiv:2309.01026v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01026
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22411;AI&#39046;&#22495;&#30340;&#26032;&#25216;&#26415;&#36827;&#34892;&#38646;&#26679;&#26412;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#36716;&#21270;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#35821;&#20041;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#24179;&#31283;&#20869;&#23481;&#30340;&#25512;&#33616;&#12290;&#22312;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#26263;&#31034;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26368;&#26032;&#36827;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25512;&#33616;&#22810;&#27169;&#24577;&#38750;&#24179;&#31283;&#20869;&#23481;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#36755;&#20837;&#28210;&#26579;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#35745;&#31639;&#35821;&#20041;&#23884;&#20837;&#33719;&#21462;&#23427;&#20204;&#30340;&#25968;&#20540;&#34920;&#31034;&#12290;&#19968;&#26086;&#33719;&#24471;&#25152;&#26377;&#20869;&#23481;&#39033;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#36866;&#24403;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#36827;&#34892;&#25512;&#33616;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#26263;&#31034;&#29615;&#22659;&#20013;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36755;&#20837;&#21253;&#25324;&#34920;&#26684;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for zero-shot recommendation of multimodal non-stationary content that leverages recent advancements in the field of generative AI. We propose rendering inputs of different modalities as textual descriptions and to utilize pre-trained LLMs to obtain their numerical representations by computing semantic embeddings. Once unified representations of all content items are obtained, the recommendation can be performed by computing an appropriate similarity metric between them without any additional learning. We demonstrate our approach on a synthetic multimodal nudging environment, where the inputs consist of tabular, textual, and visual data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00079</link><description>&lt;p&gt;
&#20851;&#20110;Adam&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
On the Implicit Bias of Adam. (arXiv:2309.00079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#21518;&#21521;&#35823;&#24046;&#20998;&#26512;&#34987;&#29992;&#26469;&#25214;&#21040;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#12290;&#21457;&#29616;&#26377;&#38480;&#27493;&#38271;&#20250;&#38544;&#24335;&#22320;&#35268;&#33539;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#20986;&#29616;&#22312;ODE&#20013;&#30340;&#39033;&#20250;&#24809;&#32602;&#25439;&#22833;&#26799;&#24230;&#30340;&#20108;&#33539;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#20013;&#26159;&#21542;&#23384;&#22312;&#31867;&#20284;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#20294;&#28041;&#21450;&#30340;&#8220;&#33539;&#25968;&#8221;&#19981;&#21516;&#65306;&#23545;&#24212;&#30340;ODE&#39033;&#35201;&#20040;&#24809;&#32602;&#65288;&#25200;&#21160;&#30340;&#65289;&#25439;&#22833;&#26799;&#24230;&#30340;&#19968;&#33539;&#25968;&#65292;&#35201;&#20040;&#30456;&#21453;&#22320;&#38459;&#27490;&#20854;&#20943;&#23567;&#65288;&#21518;&#19968;&#31181;&#24773;&#20917;&#26159;&#20856;&#22411;&#30340;&#65289;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#22914;&#20309;&#24433;&#21709;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.
&lt;/p&gt;</description></item><item><title>BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16458</link><description>&lt;p&gt;
BioCoder: &#19968;&#31181;&#24102;&#26377;&#19978;&#19979;&#25991;&#35821;&#29992;&#30693;&#35782;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16458
&lt;/p&gt;
&lt;p&gt;
BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#20195;&#30721;&#29983;&#25104;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#36755;&#20986;&#26469;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#29983;&#25104;&#21151;&#33021;&#31243;&#24207;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#37327;&#22823;&#12289;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25805;&#20316;&#21644;&#22797;&#26434;&#30340;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#32780;&#38754;&#20020;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BioCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#19982;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#26377;&#20851;&#65292;BioCoder&#28085;&#30422;&#20102;&#21487;&#33021;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;GitHub&#30340;1026&#20010;Python&#21644;Java&#20989;&#25968;&#21644;1243&#20010;&#26041;&#27861;&#65292;&#20197;&#21450;&#26469;&#33258;Rosalind&#39033;&#30446;&#30340;253&#20010;&#31034;&#20363;&#12290;BioCoder&#36824;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#25105;&#20204;&#24050;&#32463;&#24212;&#29992;&#23427;&#26469;&#35780;&#20272;&#35768;&#22810;&#27169;&#22411;&#65292;&#21253;&#25324;InCoder&#12289;CodeGen&#12289;CodeGen2&#12289;SantaCoder&#12289;StarCoder&#12289;StarCoder+&#12289;InstructCodeT&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27491;&#21017;&#21270;Wasserstein Proximal&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#22122;&#22768;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#30340;&#28508;&#21183;&#20989;&#25968;&#30830;&#23450;&#24615;&#22320;&#36827;&#34892;&#31890;&#23376;&#28436;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#21644;&#36895;&#24230;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.14945</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;Wasserstein Proximals&#23454;&#29616;&#26080;&#22122;&#22768;&#30340;&#25277;&#26679;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals. (arXiv:2308.14945v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27491;&#21017;&#21270;Wasserstein Proximal&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#22122;&#22768;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#30340;&#28508;&#21183;&#20989;&#25968;&#30830;&#23450;&#24615;&#22320;&#36827;&#34892;&#31890;&#23376;&#28436;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#21644;&#36895;&#24230;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#30001;&#28508;&#21183;&#20989;&#25968;&#25511;&#21046;&#30340;&#20998;&#24067;&#25277;&#26679;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;&#22522;&#20110;&#35780;&#20998;&#30340;&#30830;&#23450;&#24615;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#20351;&#24471;&#31890;&#23376;&#30340;&#28436;&#21270;&#21464;&#20026;&#30830;&#23450;&#24615;&#30340;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#28436;&#21270;&#12290;&#35780;&#20998;&#39033;&#30001;&#27491;&#21017;&#21270;&#30340;Wasserstein proximal&#20197;&#38381;&#21512;&#24418;&#24335;&#32473;&#20986;&#65292;&#20351;&#29992;&#37319;&#26679;&#26469;&#36817;&#20284;&#26680;&#21367;&#31215;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#24555;&#36895;&#25910;&#25947;&#65292;&#24182;&#19988;&#19982;&#26410;&#35843;&#25972;Langevin&#31639;&#27861;&#21644;Metropolis&#35843;&#25972;Langevin&#31639;&#27861;&#30456;&#27604;&#65292;&#26174;&#31034;&#20102;&#39640;&#26031;&#20998;&#24067;&#30340;&#28151;&#21512;&#26102;&#38388;&#36793;&#30028;&#30340;&#25913;&#21892;&#32500;&#24230;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20102;&#20108;&#27425;&#28508;&#21183;&#20989;&#25968;&#27599;&#27425;&#36845;&#20195;&#30340;&#20998;&#24067;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#65292;&#34920;&#24449;&#20102;&#26041;&#24046;&#38477;&#20302;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#31890;&#23376;&#30340;&#34892;&#20026;&#26159;&#26377;&#32452;&#32455;&#30340;&#65292;&#20301;&#20110;&#28508;&#21183;&#30340;&#31561;&#20540;&#32447;&#19978;&#12290;&#27492;&#22806;&#65292;&#21518;&#39564;&#22343;&#20540;&#20272;&#35745;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sampling from a distribution governed by a potential function. This work proposes an explicit score-based MCMC method that is deterministic, resulting in a deterministic evolution for particles rather than a stochastic differential equation evolution. The score term is given in closed form by a regularized Wasserstein proximal, using a kernel convolution that is approximated by sampling. We demonstrate fast convergence on various problems and show improved dimensional dependence of mixing time bounds for the case of Gaussian distributions compared to the unadjusted Langevin algorithm (ULA) and the Metropolis-adjusted Langevin algorithm (MALA). We additionally derive closed form expressions for the distributions at each iterate for quadratic potential functions, characterizing the variance reduction. Empirical results demonstrate that the particles behave in an organized manner, lying on level set contours of the potential. Moreover, the posterior mean estimat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#21644;&#26356;&#39640;&#32500;&#24230;&#19978;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13498</link><description>&lt;p&gt;
&#36867;&#31163;&#26679;&#26412;&#38519;&#38449;&#65306;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators. (arXiv:2308.13498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#21644;&#26356;&#39640;&#32500;&#24230;&#19978;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#65288;PaiDEs&#65289;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#20272;&#35745;&#22120;&#21033;&#29992;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#37197;&#23545;&#36317;&#31163;&#26469;&#24314;&#31435;&#29109;&#30340;&#36793;&#30028;&#65292;&#24182;&#23558;&#36825;&#20123;&#36793;&#30028;&#20316;&#20026;&#22522;&#20110;&#20449;&#24687;&#20934;&#21017;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#26368;&#36817;&#22522;&#20110;&#26679;&#26412;&#30340;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#29992;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;PaiDEs&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#65288;&#26368;&#22810;100&#20493;&#65289;&#19978;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#65288;&#26368;&#22810;100&#20493;&#65289;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#26356;&#39640;&#32500;&#24230;&#19978;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#35780;&#20272;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#39564;&#65306;&#19968;&#32500;&#27491;&#24358;&#25968;&#25454;&#65292;&#25670;&#21160;&#29289;&#20307;&#65288;Pendulum-v0&#65289;&#65292;&#36339;&#36291;&#26426;&#22120;&#20154;&#65288;Hopper-v2&#65289;&#65292;&#34434;&#34433;&#26426;&#22120;&#20154;&#65288;Ant-v2&#65289;&#21644;&#20154;&#24418;&#26426;&#22120;&#20154;&#65288;Humanoid-v2&#65289;&#12290;&#23545;&#20110;&#27599;&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#26469;&#23637;&#31034;PaiDEs&#22312;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a novel approach for epistemic uncertainty estimation for ensemble models using pairwise-distance estimators (PaiDEs). These estimators utilize the pairwise-distance between model components to establish bounds on entropy and uses said bounds as estimates for information-based criterion. Unlike recent deep learning methods for epistemic uncertainty estimation, which rely on sample-based Monte Carlo estimators, PaiDEs are able to estimate epistemic uncertainty up to 100$\times$ faster, over a larger space (up to 100$\times$) and perform more accurately in higher dimensions. To validate our approach, we conducted a series of experiments commonly used to evaluate epistemic uncertainty estimation: 1D sinusoidal data, Pendulum-v0, Hopper-v2, Ant-v2 and Humanoid-v2. For each experimental setting, an Active Learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21387;&#32553;&#26426;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#22312;&#25151;&#39076;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#24515;&#24459;&#38388;&#38548;&#24207;&#21015;&#36827;&#34892;&#21387;&#32553;&#36317;&#31163;&#35745;&#31639;&#21644;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#27169;&#22411;&#20248;&#21270;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#25509;&#36817;&#20110;&#26368;&#20339;&#30340;&#25151;&#39076;&#26816;&#27979;&#31639;&#27861;&#12290;&#36825;&#34920;&#26126;gzip&#20998;&#31867;&#22120;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#21644;&#36830;&#32493;&#38543;&#26426;&#24207;&#21015;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.13328</link><description>&lt;p&gt;
&#22522;&#20110;&#21387;&#32553;&#26426;&#30340;&#25151;&#39076;&#26816;&#27979;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Compressor-Based Classification for Atrial Fibrillation Detection. (arXiv:2308.13328v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21387;&#32553;&#26426;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#22312;&#25151;&#39076;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#24515;&#24459;&#38388;&#38548;&#24207;&#21015;&#36827;&#34892;&#21387;&#32553;&#36317;&#31163;&#35745;&#31639;&#21644;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#27169;&#22411;&#20248;&#21270;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#25509;&#36817;&#20110;&#26368;&#20339;&#30340;&#25151;&#39076;&#26816;&#27979;&#31639;&#27861;&#12290;&#36825;&#34920;&#26126;gzip&#20998;&#31867;&#22120;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#21644;&#36830;&#32493;&#38543;&#26426;&#24207;&#21015;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25151;&#39076;&#26159;&#26368;&#24120;&#35265;&#30340;&#24515;&#24459;&#22833;&#24120;&#20043;&#19968;&#65292;&#23545;&#20844;&#20849;&#20581;&#24247;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#33258;&#21160;&#26816;&#27979;&#25151;&#39076;&#21457;&#20316;&#26159;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#26412;&#25991;&#23558;&#26368;&#36817;&#24341;&#20837;&#30340;&#22522;&#20110;&#21387;&#32553;&#26426;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#24212;&#29992;&#20110;&#25151;&#39076;&#26816;&#27979;&#20219;&#21153;&#65288;&#24515;&#24459;&#20043;&#38388;&#30340;&#20108;&#20998;&#31867;&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#916;RR&#21644;RR&#38388;&#26399;&#24207;&#21015;&#24212;&#29992;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65292;k-&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#37197;&#32622;&#65292;&#20197;&#21450;&#26368;&#20339;&#31383;&#21475;&#38271;&#24230;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#65288;&#24179;&#22343;&#25935;&#24863;&#24230;=97.1%&#65292;&#24179;&#22343;&#29305;&#24322;&#24230;=91.7%&#65292;&#26368;&#20339;&#25935;&#24863;&#24230;&#20026;99.8%&#65292;&#26368;&#20339;&#29305;&#24322;&#24230;&#20026;97.6% 5&#25240;&#20132;&#21449;&#39564;&#35777;&#65289;&#12290;&#25152;&#33719;&#24471;&#30340;&#24615;&#33021;&#25509;&#36817;&#20110;&#26368;&#20339;&#30340;&#19987;&#38376;&#30340;&#25151;&#39076;&#26816;&#27979;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;gzip&#20998;&#31867;&#22120;&#65292;&#26368;&#21021;&#29992;&#20110;&#25991;&#26412;&#65292;&#20063;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#21644;&#36830;&#32493;&#38543;&#26426;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Atrial fibrillation (AF) is one of the most common arrhythmias with challenging public health implications. Automatic detection of AF episodes is therefore one of the most important tasks in biomedical engineering. In this paper, we apply the recently introduced method of compressor-based text classification to the task of AF detection (binary classification between heart rhythms). We investigate the normalised compression distance applied to $\Delta$RR and RR-interval sequences, the configuration of the k-Nearest Neighbour classifier, and an optimal window length. We achieve good classification results (avg. sensitivity = 97.1%, avg. specificity = 91.7%, best sensitivity of 99.8%, best specificity of 97.6% with 5-fold cross-validation). Obtained performance is close to the best specialised AF detection algorithms. Our results suggest that gzip classification, originally proposed for texts, is suitable for biomedical data and continuous stochastic sequences in general.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22870;&#21169;&#27169;&#22411;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#24182;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.12030</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#19982;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt-Based Length Controlled Generation with Reinforcement Learning. (arXiv:2308.12030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22870;&#21169;&#27169;&#22411;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#24182;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;GPT-4&#22240;&#20854;&#24778;&#20154;&#30340;&#25913;&#36827;&#21644;&#24615;&#33021;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#25104;&#20026;LLM&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#35805;&#39064;&#65292;&#23427;&#36824;&#20351;&#29992;&#25143;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#22312;&#26356;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#29983;&#25104;&#25152;&#38656;&#38271;&#24230;&#30340;&#21512;&#36866;&#31572;&#26696;&#25110;&#25991;&#31456;&#12290;&#27492;&#22806;&#65292;LLM&#20013;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#38750;&#24120;&#32791;&#26102;&#65292;&#32780;&#25511;&#21046;&#29983;&#25104;&#38271;&#24230;&#30340;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#38480;&#21046;&#38271;&#24230;&#20219;&#24847;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#65292;&#20174;&#32780;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#26469;&#23454;&#29616;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#31867;&#20284;GPT&#30340;LLM&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#25110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22870;&#21169;&#27169;&#22411;&#25552;&#20379;&#22870;&#21169;&#20449;&#21495;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#23545;&#39044;&#23450;&#20041;&#30446;&#26631;&#38271;&#24230;&#36827;&#34892;&#22870;&#21169;&#26469;&#24433;&#21709;LLM&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21435;&#28151;&#28102;&#22120;&#27169;&#22411;FLMD&#65292;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24314;&#27169;&#20013;&#23454;&#29616;&#20102;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#35299;&#20915;&#20102;&#26377;&#20559;&#35265;&#30340;EHR&#20013;&#30340;&#20581;&#24247;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11819</link><description>&lt;p&gt;
&#36890;&#36807;&#21435;&#28151;&#28102;&#22120;&#35299;&#20915;&#26377;&#20559;&#35265;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#20581;&#24247;&#24046;&#24322;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder. (arXiv:2308.11819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11819
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21435;&#28151;&#28102;&#22120;&#27169;&#22411;FLMD&#65292;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24314;&#27169;&#20013;&#23454;&#29616;&#20102;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#35299;&#20915;&#20102;&#26377;&#20559;&#35265;&#30340;EHR&#20013;&#30340;&#20581;&#24247;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#25968;&#25454;&#24314;&#27169;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#19978;&#65292;&#30001;&#20110;EHR&#30340;&#22797;&#26434;&#28508;&#22312;&#32467;&#26500;&#21644;&#28508;&#22312;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#32780;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#20445;&#25345;&#27169;&#22411;&#25972;&#20307;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23569;&#20581;&#24247;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#25429;&#25417;&#21040;&#35266;&#23519;&#25968;&#25454;&#20043;&#22806;&#30340;&#28508;&#22312;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#20844;&#24179;&#32437;&#21521;&#21307;&#30103;&#21435;&#28151;&#28102;&#22120;&#65288;FLMD&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#24314;&#27169;&#12290;&#21463;&#21040;&#21435;&#28151;&#28102;&#22120;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;FLMD&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;FLMD&#25429;&#25417;&#20102;&#27599;&#27425;&#25509;&#35302;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#23376;&#65292;&#36825;&#26377;&#25928;&#22320;&#34920;&#31034;&#20102;&#36229;&#20986;&#35266;&#23519;&#21040;&#30340;EHR&#20043;&#22806;&#30340;&#28508;&#22312;&#21307;&#30103;&#22240;&#32032;&#65292;&#22914;&#24739;&#32773;&#22522;&#22240;&#22411;&#21644;&#29983;&#27963;&#20064;&#24815;&#12290;&#36825;&#20010;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#23376;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fairness issue of clinical data modeling, especially on Electronic Health Records (EHRs), is of utmost importance due to EHR's complex latent structure and potential selection bias. It is frequently necessary to mitigate health disparity while keeping the model's overall accuracy in practice. However, traditional methods often encounter the trade-off between accuracy and fairness, as they fail to capture the underlying factors beyond observed data. To tackle this challenge, we propose a novel model called Fair Longitudinal Medical Deconfounder (FLMD) that aims to achieve both fairness and accuracy in longitudinal Electronic Health Records (EHR) modeling. Drawing inspiration from the deconfounder theory, FLMD employs a two-stage training process. In the first stage, FLMD captures unobserved confounders for each encounter, which effectively represents underlying medical factors beyond observed EHR, such as patient genotypes and lifestyle habits. This unobserved confounder is crucial 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2308.08493</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#26102;&#38388;&#26053;&#34892;&#65306;&#36861;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#26159;&#25351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#29702;&#35299;LLMs&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#26377;&#25928;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#36890;&#36807;&#35782;&#21035;&#20174;&#23567;&#30340;&#38543;&#26426;&#26679;&#26412;&#20013;&#25277;&#21462;&#30340;&#21333;&#20010;&#23454;&#20363;&#20013;&#30340;&#28508;&#22312;&#27745;&#26579;&#65292;&#28982;&#21518;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20272;&#35745;&#21333;&#20010;&#23454;&#20363;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#65306;&#21363;&#19968;&#20010;&#30001;&#25968;&#25454;&#38598;&#21517;&#31216;&#12289;&#20998;&#21306;&#31867;&#22411;&#21644;&#21442;&#32771;&#23454;&#20363;&#30340;&#21021;&#22987;&#37096;&#20998;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#35201;&#27714;LLM&#23436;&#25104;&#23427;&#12290;&#22914;&#26524;LLM&#30340;&#36755;&#20986;&#19982;&#21442;&#32771;&#23454;&#20363;&#30340;&#21518;&#19968;&#37096;&#20998;&#23436;&#20840;&#25110;&#25509;&#36817;&#21305;&#37197;&#65292;&#37027;&#20040;&#35813;&#23454;&#20363;&#34987;&#26631;&#35760;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20102;&#35299;&#25972;&#20010;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24819;&#27861;&#12290;&#31532;&#19968;&#20010;&#24819;&#27861;&#26159;&#26631;&#35760;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#21306;&#65292;&#35813;&#20998;&#21306;&#20013;&#30340;&#23454;&#20363;&#22823;&#22810;&#25968;&#37117;&#34987;&#21028;&#26029;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
&lt;/p&gt;</description></item><item><title>Ada-QPacknet&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21098;&#26525;&#19982;&#20301;&#23485;&#32553;&#20943;&#30340;&#39640;&#25928;&#32487;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#37327;&#21270;&#25216;&#26415;&#29983;&#25104;&#20219;&#21153;&#23376;&#32593;&#32476;&#65292;&#22312;&#21160;&#24577;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#19982;&#28014;&#28857;&#25968;&#23376;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07939</link><description>&lt;p&gt;
Ada-QPacknet -- &#33258;&#36866;&#24212;&#21098;&#26525;&#19982;&#20301;&#23485;&#32553;&#20943;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#32487;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20250;&#36951;&#24536;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ada-QPacknet -- adaptive pruning with bit width reduction as an efficient continual learning method without forgetting. (arXiv:2308.07939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07939
&lt;/p&gt;
&lt;p&gt;
Ada-QPacknet&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21098;&#26525;&#19982;&#20301;&#23485;&#32553;&#20943;&#30340;&#39640;&#25928;&#32487;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#37327;&#21270;&#25216;&#26415;&#29983;&#25104;&#20219;&#21153;&#23376;&#32593;&#32476;&#65292;&#22312;&#21160;&#24577;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#19982;&#28014;&#28857;&#25968;&#23376;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32487;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26159;&#19968;&#20010;&#36807;&#31243;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#29575;&#20173;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#12290;&#26368;&#36817;&#35774;&#35745;&#20102;&#35768;&#22810;CL&#31639;&#27861;&#65292;&#22823;&#37096;&#20998;&#37117;&#23384;&#22312;&#22312;&#21160;&#24577;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#26550;&#26500;&#30340;&#26041;&#27861;Ada-QPacknet&#12290;&#23427;&#36890;&#36807;&#21098;&#26525;&#25552;&#21462;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#12290;&#22522;&#20110;&#26550;&#26500;&#30340;CL&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#23481;&#37327;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37327;&#21270;&#26041;&#27861;&#20943;&#23567;&#20102;&#27169;&#22411;&#30340;&#35268;&#27169;&#12290;&#35813;&#26041;&#27861;&#20943;&#23567;&#20102;&#26435;&#37325;&#26684;&#24335;&#30340;&#20301;&#23485;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#28151;&#21512;8&#20301;&#21644;4&#20301;&#37327;&#21270;&#22312;&#33879;&#21517;&#30340;CL&#22330;&#26223;&#19978;&#23454;&#29616;&#20102;&#19982;&#28014;&#28857;&#25968;&#23376;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#21098;&#26525;&#21644;&#37327;&#21270;&#36825;&#20004;&#31181;&#21387;&#32553;&#25216;&#26415;&#24212;&#29992;&#20110;&#29983;&#25104;&#20219;&#21153;&#23376;&#32593;&#32476;&#30340;CL&#31574;&#30053;&#12290;&#35813;&#31639;&#27861;&#22312;&#33879;&#21517;&#30340;&#24773;&#33410;&#32452;&#21512;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning (CL) is a process in which there is still huge gap between human and deep learning model efficiency. Recently, many CL algorithms were designed. Most of them have many problems with learning in dynamic and complex environments. In this work new architecture based approach Ada-QPacknet is described. It incorporates the pruning for extracting the sub-network for each task. The crucial aspect in architecture based CL methods is theirs capacity. In presented method the size of the model is reduced by efficient linear and nonlinear quantisation approach. The method reduces the bit-width of the weights format. The presented results shows that hybrid 8 and 4-bit quantisation achieves similar accuracy as floating-point sub-network on a well-know CL scenarios. To our knowledge it is the first CL strategy which incorporates both compression techniques pruning and quantisation for generating task sub-networks. The presented algorithm was tested on well-known episode combination
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20613;&#37324;&#21494;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FNO&#65289;&#29992;&#20110;&#23454;&#26102;&#27169;&#25311;3D&#21160;&#24577;&#22478;&#24066;&#24494;&#27668;&#20505;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;FNO&#22312;&#21152;&#36895;&#35299;&#20915;&#22797;&#26434;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#31995;&#32479;&#21160;&#21147;&#23398;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03985</link><description>&lt;p&gt;
&#20613;&#37324;&#21494;&#31070;&#32463;&#36816;&#31639;&#22120;&#29992;&#20110;&#23454;&#26102;&#27169;&#25311;3D&#21160;&#24577;&#22478;&#24066;&#24494;&#27668;&#20505;
&lt;/p&gt;
&lt;p&gt;
Fourier neural operator for real-time simulation of 3D dynamic urban microclimate. (arXiv:2308.03985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20613;&#37324;&#21494;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FNO&#65289;&#29992;&#20110;&#23454;&#26102;&#27169;&#25311;3D&#21160;&#24577;&#22478;&#24066;&#24494;&#27668;&#20505;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;FNO&#22312;&#21152;&#36895;&#35299;&#20915;&#22797;&#26434;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#31995;&#32479;&#21160;&#21147;&#23398;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#22478;&#24066;&#21270;&#20984;&#26174;&#20102;&#22478;&#24066;&#24494;&#27668;&#20505;&#23545;&#20154;&#31867;&#33298;&#36866;&#24230;&#12289;&#20581;&#24247;&#21644;&#24314;&#31569;/&#22478;&#24066;&#33021;&#25928;&#30340;&#37325;&#35201;&#24615;&#12290;&#23427;&#20204;&#23545;&#24314;&#31569;&#35774;&#35745;&#21644;&#22478;&#24066;&#35268;&#21010;&#20135;&#29983;&#37325;&#22823;&#30340;&#29615;&#22659;&#24433;&#21709;&#12290;&#29702;&#35299;&#23616;&#37096;&#24494;&#27668;&#20505;&#23545;&#20110;&#22478;&#24066;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#26377;&#25928;&#23454;&#26045;&#24377;&#24615;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#22478;&#24066;&#24494;&#27668;&#20505;&#38656;&#35201;&#22312;&#35745;&#31639;&#22495;&#20869;&#32771;&#34385;&#22797;&#26434;&#30340;&#23460;&#22806;&#21442;&#25968;&#65292;&#28085;&#30422;&#36739;&#38271;&#26102;&#26399;&#65292;&#24182;&#28085;&#30422;&#22478;&#24066;&#35268;&#27169;&#12290;&#22240;&#27492;&#65292;&#25968;&#20540;&#26041;&#27861;&#22914;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65288;CFD&#65289;&#22312;&#35780;&#20272;&#22478;&#24066;&#24494;&#27668;&#20505;&#30340;&#24433;&#21709;&#26102;&#21464;&#24471;&#35745;&#31639;&#22797;&#26434;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20852;&#36215;&#20026;&#21152;&#36895;&#22797;&#26434;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#31995;&#32479;&#21160;&#21147;&#23398;&#24314;&#27169;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;&#26368;&#36817;&#65292;&#20613;&#37324;&#21494;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FNO&#65289;&#22312;&#21152;&#36895;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global urbanization has underscored the significance of urban microclimates for human comfort, health, and building/urban energy efficiency. They profoundly influence building design and urban planning as major environmental impacts. Understanding local microclimates is essential for cities to prepare for climate change and effectively implement resilience measures. However, analyzing urban microclimates requires considering a complex array of outdoor parameters within computational domains at the city scale over a longer period than indoors. As a result, numerical methods like Computational Fluid Dynamics (CFD) become computationally expensive when evaluating the impact of urban microclimates. The rise of deep learning techniques has opened new opportunities for accelerating the modeling of complex non-linear interactions and system dynamics. Recently, the Fourier Neural Operator (FNO) has been shown to be very promising in accelerating solving the Partial Differential Equations (PDEs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FLIPS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31649;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#21644;&#21442;&#19982;&#32773;&#24322;&#36136;&#24615;&#30340;&#20013;&#38388;&#20214;&#31995;&#32479;&#12290;FLIPS&#36890;&#36807;&#26631;&#31614;&#20998;&#24067;&#32858;&#31867;&#21644;&#26234;&#33021;&#21442;&#19982;&#32773;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#26469;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;FLIPS&#30456;&#27604;&#38543;&#26426;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03901</link><description>&lt;p&gt;
FLIPS: &#20351;&#29992;&#26234;&#33021;&#21442;&#19982;&#32773;&#36873;&#25321;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLIPS: Federated Learning using Intelligent Participant Selection. (arXiv:2308.03901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLIPS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31649;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#21644;&#21442;&#19982;&#32773;&#24322;&#36136;&#24615;&#30340;&#20013;&#38388;&#20214;&#31995;&#32479;&#12290;FLIPS&#36890;&#36807;&#26631;&#31614;&#20998;&#24067;&#32858;&#31867;&#21644;&#26234;&#33021;&#21442;&#19982;&#32773;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#26469;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;FLIPS&#30456;&#27604;&#38543;&#26426;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLIPS&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31649;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#21644;&#21442;&#19982;&#32773;&#24322;&#36136;&#24615;&#30340;&#20013;&#38388;&#20214;&#31995;&#32479;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26631;&#31614;&#20998;&#24067;&#32858;&#31867;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21442;&#19982;&#32773;&#36873;&#25321;&#20013;&#30340;&#22909;&#22788;&#12290;FLIPS&#26681;&#25454;&#25968;&#25454;&#30340;&#26631;&#31614;&#20998;&#24067;&#39044;&#20808;&#23545;&#21442;&#19982;FL&#35757;&#32451;&#20316;&#19994;&#30340;&#21508;&#26041;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#22312;FL&#35757;&#32451;&#26399;&#38388;&#30830;&#20445;&#27599;&#20010;&#32858;&#31867;&#22312;&#34987;&#36873;&#20013;&#30340;&#21442;&#19982;&#32773;&#20013;&#20844;&#24179;&#22320;&#34920;&#31034;&#12290;FLIPS&#21487;&#20197;&#25903;&#25345;&#26368;&#24120;&#35265;&#30340;FL&#31639;&#27861;&#65292;&#21253;&#25324;FedAvg&#65292;FedProx&#65292;FedDyn&#65292;FedOpt&#21644;FedYogi&#12290;&#20026;&#20102;&#31649;&#29702;&#24179;&#21488;&#30340;&#24322;&#26500;&#24615;&#21644;&#21160;&#24577;&#36164;&#28304;&#21487;&#29992;&#24615;&#65292;FLIPS&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#22788;&#29702;&#20998;&#24067;&#24335;&#26234;&#33021;&#31038;&#21306;&#24212;&#29992;&#20013;&#23481;&#37327;&#21464;&#21270;&#30340;&#25302;&#32047;&#31649;&#29702;&#26426;&#21046;&#12290;&#26631;&#31614;&#20998;&#24067;&#12289;&#32858;&#31867;&#21644;&#21442;&#19982;&#32773;&#36873;&#25321;&#30340;&#38544;&#31169;&#36890;&#36807;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;(TEE)&#26469;&#30830;&#20445;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#23558;FLIPS&#19982;&#38543;&#26426;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the design and implementation of FLIPS, a middleware system to manage data and participant heterogeneity in federated learning (FL) training workloads. In particular, we examine the benefits of label distribution clustering on participant selection in federated learning. FLIPS clusters parties involved in an FL training job based on the label distribution of their data apriori, and during FL training, ensures that each cluster is equitably represented in the participants selected. FLIPS can support the most common FL algorithms, including FedAvg, FedProx, FedDyn, FedOpt and FedYogi. To manage platform heterogeneity and dynamic resource availability, FLIPS incorporates a straggler management mechanism to handle changing capacities in distributed, smart community applications. Privacy of label distributions, clustering and participant selection is ensured through a trusted execution environment (TEE). Our comprehensive empirical evaluation compares FLIPS with random p
&lt;/p&gt;</description></item><item><title>Branched Latent Neural Operators (BLNOs) are introduced to learn input-output maps for encoding complex physical processes. BLNOs utilize interpretable latent outputs to enhance learned dynamics and overcome the curse of dimensionality, while also showing excellent generalization properties with small training datasets and short training times. The partial connection structure reduces the number of tunable parameters. BLNOs are proven effective in a challenging biophysically detailed test case.</title><link>http://arxiv.org/abs/2308.02599</link><description>&lt;p&gt;
&#20998;&#25903;&#28508;&#22312;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Branched Latent Neural Operators. (arXiv:2308.02599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02599
&lt;/p&gt;
&lt;p&gt;
Branched Latent Neural Operators (BLNOs) are introduced to learn input-output maps for encoding complex physical processes. BLNOs utilize interpretable latent outputs to enhance learned dynamics and overcome the curse of dimensionality, while also showing excellent generalization properties with small training datasets and short training times. The partial connection structure reduces the number of tunable parameters. BLNOs are proven effective in a challenging biophysically detailed test case.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#20998;&#25903;&#28508;&#22312;&#31070;&#32463;&#31639;&#23376;&#65288;BLNOs&#65289;&#26469;&#23398;&#20064;&#32534;&#30721;&#22797;&#26434;&#29289;&#29702;&#36807;&#31243;&#30340;&#36755;&#20837;-&#36755;&#20986;&#26144;&#23556;&#12290;BLNO&#30001;&#19968;&#20010;&#31616;&#21333;&#32039;&#20945;&#30340;&#21069;&#39304;&#37096;&#20998;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#65292;&#35813;&#32593;&#32476;&#22312;&#32467;&#26500;&#19978;&#23558;&#19981;&#21516;&#22266;&#26377;&#35282;&#33394;&#30340;&#36755;&#20837;&#36827;&#34892;&#35299;&#31163;&#65292;&#20363;&#22914;&#23558;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#38388;&#21464;&#37327;&#19982;&#27169;&#22411;&#21442;&#25968;&#20998;&#31163;&#65292;&#24182;&#23558;&#23427;&#20204;&#36716;&#21270;&#20026;&#24863;&#20852;&#36259;&#30340;&#36890;&#29992;&#39046;&#22495;&#12290;BLNO&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#36755;&#20986;&#22686;&#24378;&#20102;&#23398;&#20064;&#21040;&#30340;&#21160;&#24577;&#65292;&#24182;&#36890;&#36807;&#22312;&#21333;&#20010;&#22788;&#29702;&#22120;&#19978;&#20351;&#29992;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#38469;&#19978;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#35823;&#24046;&#22312;&#27979;&#35797;&#38454;&#27573;&#37319;&#29992;&#30340;&#31163;&#25955;&#21270;&#26041;&#24335;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#21487;&#27604;&#24615;&#12290;&#27492;&#22806;&#65292;&#37096;&#20998;&#36830;&#25509;&#22312;&#20840;&#36830;&#25509;&#32467;&#26500;&#30340;&#22522;&#30784;&#19978;&#26174;&#33879;&#20943;&#23569;&#20102;&#21487;&#35843;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BLNO&#22312;&#28041;&#21450;&#29983;&#29289;&#29289;&#29702;&#32454;&#33410;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26696;&#20363;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. A BLNO is defined by a simple and compact feedforward partially-connected neural network that structurally disentangles inputs with different intrinsic roles, such as the time variable from model parameters of a differential equation, while transferring them into a generic field of interest. BLNOs leverage interpretable latent outputs to enhance the learned dynamics and break the curse of dimensionality by showing excellent generalization properties with small training datasets and short training times on a single processor. Indeed, their generalization error remains comparable regardless of the adopted discretization during the testing phase. Moreover, the partial connections, in place of a fully-connected structure, significantly reduce the number of tunable parameters. We show the capabilities of BLNOs in a challenging test case involving biophysically detailed elect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#28023;&#24213;&#28369;&#32724;&#22120;&#22312;&#19981;&#21487;&#39044;&#27979;&#28023;&#27915;&#29615;&#22659;&#20013;&#27491;&#24120;&#25805;&#20316;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#28369;&#32724;&#22120;&#37096;&#32626;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#12290;&#31639;&#27861;&#33021;&#22815;&#23454;&#26102;&#25552;&#20379;&#24322;&#24120;&#35686;&#25253;&#65292;&#20351;&#39550;&#39542;&#21592;&#33021;&#22815;&#25511;&#21046;&#28369;&#32724;&#22120;&#24182;&#36991;&#20813;&#36827;&#19968;&#27493;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2308.00180</link><description>&lt;p&gt;
&#28023;&#24213;&#28369;&#32724;&#22120;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#39564;&#35777;&#26041;&#27861;&#8212;&#8212;&#22823;&#35268;&#27169;&#37096;&#32626;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Dataset. (arXiv:2308.00180v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#28023;&#24213;&#28369;&#32724;&#22120;&#22312;&#19981;&#21487;&#39044;&#27979;&#28023;&#27915;&#29615;&#22659;&#20013;&#27491;&#24120;&#25805;&#20316;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#28369;&#32724;&#22120;&#37096;&#32626;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#12290;&#31639;&#27861;&#33021;&#22815;&#23454;&#26102;&#25552;&#20379;&#24322;&#24120;&#35686;&#25253;&#65292;&#20351;&#39550;&#39542;&#21592;&#33021;&#22815;&#25511;&#21046;&#28369;&#32724;&#22120;&#24182;&#36991;&#20813;&#36827;&#19968;&#27493;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#19968;&#31181;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#35780;&#20272;&#22312;&#19981;&#21487;&#39044;&#27979;&#30340;&#28023;&#27915;&#29615;&#22659;&#20013;&#28023;&#24213;&#28369;&#32724;&#22120;&#30340;&#27491;&#24120;&#25805;&#20316;&#12290;&#19968;&#26086;&#26816;&#27979;&#21040;&#20219;&#20309;&#24322;&#24120;&#65292;&#21487;&#20197;&#21521;&#28369;&#32724;&#22120;&#39550;&#39542;&#21592;&#25552;&#20379;&#23454;&#26102;&#35686;&#25253;&#65292;&#20351;&#20854;&#33021;&#22815;&#25509;&#31649;&#28369;&#32724;&#22120;&#24182;&#38450;&#27490;&#36827;&#19968;&#27493;&#30340;&#25439;&#23475;&#12290;&#35813;&#26816;&#27979;&#31639;&#27861;&#24212;&#29992;&#20110;&#30001;Skidaway&#28023;&#27915;&#30740;&#31350;&#25152;&#65288;SkIO&#65289;&#21644;&#21335;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#65288;USF&#65289;&#39046;&#23548;&#30340;&#23454;&#38469;&#28369;&#32724;&#22120;&#37096;&#32626;&#20013;&#25910;&#38598;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#12290;&#23601;&#27867;&#21270;&#24615;&#32780;&#35328;&#65292;&#23454;&#39564;&#35780;&#20272;&#21253;&#25324;&#31163;&#32447;&#21644;&#22312;&#32447;&#26816;&#27979;&#27169;&#24335;&#12290;&#31163;&#32447;&#26816;&#27979;&#21033;&#29992;&#23436;&#25972;&#30340;&#22238;&#25910;&#21518;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#30340;&#20449;&#24687;&#65292;&#23545;&#24322;&#24120;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#24182;&#19982;&#39550;&#39542;&#21592;&#26085;&#24535;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#32447;&#26816;&#27979;&#19987;&#27880;&#20110;&#20174;&#28369;&#32724;&#22120;&#20256;&#36755;&#30340;&#23454;&#26102;&#25968;&#25454;&#23376;&#38598;&#12290;&#34429;&#28982;&#23454;&#26102;&#25968;&#25454;&#21487;&#33021;&#19981;&#21253;&#21547;&#19982;&#22238;&#25910;&#21518;&#25968;&#25454;&#19968;&#26679;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20294;&#22312;&#32447;&#26816;&#27979;&#26159;&#23454;&#26102;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper employs an anomaly detection algorithm to assess the normal operation of underwater gliders in unpredictable ocean environments. Real-time alerts can be provided to glider pilots upon detecting any anomalies, enabling them to assume control of the glider and prevent further harm. The detection algorithm is applied to abundant data sets collected in real glider deployments led by the Skidaway Institute of Oceanography (SkIO) and the University of South Florida (USF). Regarding generality, the experimental evaluation is composed of both offline and online detection modes. The offline detection utilizes full post-recovery data sets, which carries high-resolution information, to present detailed analysis of the anomaly and compare it with pilot logs. The online detection focuses on the real-time subsets of data transmitted from the glider at the surfacing events. While the real-time data may not contain as much rich information as the post-recovery data, the online detection is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31639;&#27861;&#23637;&#24320;&#30340;&#26041;&#27861;&#65292;&#23558;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;RLS&#65289;&#21644;&#31561;&#21464;&#33258;&#36866;&#24212;&#28304;&#20998;&#31163;&#65288;EASI&#65289;&#20004;&#31181;&#31639;&#27861;&#36716;&#21270;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#36807;&#31243;&#26377;&#25928;&#22320;&#36827;&#34892;&#28304;&#20449;&#21495;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Stein&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#65288;SURE&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16708</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#33258;&#36866;&#24212;&#28388;&#27874;&#65306;Stein&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Meets Adaptive Filtering: A Stein's Unbiased Risk Estimator Approach. (arXiv:2307.16708v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31639;&#27861;&#23637;&#24320;&#30340;&#26041;&#27861;&#65292;&#23558;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;RLS&#65289;&#21644;&#31561;&#21464;&#33258;&#36866;&#24212;&#28304;&#20998;&#31163;&#65288;EASI&#65289;&#20004;&#31181;&#31639;&#27861;&#36716;&#21270;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#36807;&#31243;&#26377;&#25928;&#22320;&#36827;&#34892;&#28304;&#20449;&#21495;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Stein&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#65288;SURE&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31639;&#27861;&#23637;&#24320;&#30340;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#20004;&#31181;&#33879;&#21517;&#30340;&#33258;&#36866;&#24212;&#28388;&#27874;&#31639;&#27861;&#65292;&#21363;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;RLS&#65289;&#21644;&#31561;&#21464;&#33258;&#36866;&#24212;&#28304;&#20998;&#31163;&#65288;EASI&#65289;&#65292;&#22312;&#28304;&#20272;&#35745;&#21644;&#20998;&#31163;&#30340;&#29615;&#22659;&#20013;&#12290;&#22312;&#23637;&#24320;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;Deep RLS&#21644;Deep EASI&#12290;&#36825;&#20123;&#26550;&#26500;&#23558;&#21407;&#22987;&#31639;&#27861;&#30340;&#36845;&#20195;&#21464;&#25442;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#65292;&#20174;&#32780;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#36807;&#31243;&#26377;&#25928;&#22320;&#36827;&#34892;&#28304;&#20449;&#21495;&#20272;&#35745;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;Stein&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#65288;SURE&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#36825;&#20123;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#36825;&#31181;&#22522;&#20110;SURE&#30340;&#26041;&#27861;&#23545;&#20110;&#22686;&#24378;&#28304;&#20449;&#21495;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits two prominent adaptive filtering algorithms through the lens of algorithm unrolling, namely recursive least squares (RLS) and equivariant adaptive source separation (EASI), in the context of source estimation and separation. Building upon the unrolling methodology, we introduce novel task-based deep learning frameworks, denoted as Deep RLS and Deep EASI. These architectures transform the iterations of the original algorithms into layers of a deep neural network, thereby enabling efficient source signal estimation by taking advantage of a training process. To further enhance performance, we propose training these deep unrolled networks utilizing a loss function grounded on a Stein's unbiased risk estimator (SURE). Our empirical evaluations demonstrate the efficacy of this SURE-based approach for enhanced source signal estimation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;16&#20301;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adam&#20248;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16189</link><description>&lt;p&gt;
&#29992;&#20110;16&#20301;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training. (arXiv:2307.16189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;16&#20301;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adam&#20248;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;16&#20301;&#35745;&#31639;&#20013;&#20351;&#29992;&#27969;&#34892;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;RMSProp&#21644;Adam&#65289;&#26102;&#35266;&#23519;&#21040;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#20986;&#29616;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#21463;&#21040;&#24178;&#25200;&#65292;&#20174;&#32780;&#22952;&#30861;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#37096;&#32626;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21333;&#19968;&#36229;&#21442;&#25968;epsilon&#26159;&#36825;&#31181;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#23545;16&#20301;&#35745;&#31639;&#20013;&#36825;&#20123;&#20248;&#21270;&#22120;&#20013;epsilon&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#32034;&#65292;&#21457;&#29616;&#24494;&#35843;&#20854;&#20540;&#21487;&#20197;&#24674;&#22797;RMSProp&#21644;Adam&#30340;&#21151;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#21033;&#29992;16&#20301;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#34987;&#21457;&#29616;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Adam&#20248;&#21270;&#22120;&#30340;&#26356;&#26032;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#25277;&#35937;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#25277;&#35937;&#30340;&#29992;&#36884;&#21462;&#20915;&#20110;&#20855;&#20307;&#22330;&#26223;&#65292;&#35831;&#27714;&#31616;&#21333;&#30340;&#31895;&#30053;&#25277;&#35937;&#21516;&#26679;&#20250;&#26377;&#20854;&#29992;&#36884;&#65292;&#32780;&#23545;&#20110;&#26356;&#22797;&#26434;</title><link>http://arxiv.org/abs/2307.15546</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#25277;&#35937;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
On the Trade-off Between Efficiency and Precision of Neural Abstraction. (arXiv:2307.15546v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#25277;&#35937;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#25277;&#35937;&#30340;&#29992;&#36884;&#21462;&#20915;&#20110;&#20855;&#20307;&#22330;&#26223;&#65292;&#35831;&#27714;&#31616;&#21333;&#30340;&#31895;&#30053;&#25277;&#35937;&#21516;&#26679;&#20250;&#26377;&#20854;&#29992;&#36884;&#65292;&#32780;&#23545;&#20110;&#26356;&#22797;&#26434;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25277;&#35937;&#26368;&#36817;&#34987;&#24341;&#20837;&#20316;&#20026;&#22797;&#26434;&#38750;&#32447;&#24615;&#21160;&#21147;&#27169;&#22411;&#30340;&#24418;&#24335;&#36817;&#20284;&#12290;&#23427;&#20204;&#21253;&#25324;&#19968;&#20010;&#31070;&#32463;ODE&#21644;&#25277;&#35937;&#31070;&#32463;&#32593;&#32476;&#19982;&#20855;&#20307;&#21160;&#21147;&#27169;&#22411;&#20043;&#38388;&#35823;&#24046;&#30340;&#35777;&#26126;&#19978;&#30028;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#31070;&#32463;&#25277;&#35937;&#20165;&#20197;&#20840;$ReLU$&#28608;&#27963;&#20989;&#25968;&#32452;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#24471;&#21040;&#65292;&#23548;&#33268;&#20855;&#26377;&#20998;&#27573;&#20223;&#23556;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;ODE&#27169;&#22411;&#65292;&#21487;&#20197;&#31561;&#25928;&#22320;&#35299;&#37322;&#20026;&#32447;&#24615;&#28151;&#21512;&#33258;&#21160;&#26426;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25277;&#35937;&#30340;&#25928;&#29992;&#21462;&#20915;&#20110;&#23427;&#30340;&#20351;&#29992;&#65306;&#26576;&#20123;&#24773;&#20917;&#21487;&#33021;&#38656;&#35201;&#23481;&#26131;&#20998;&#26512;&#30340;&#31895;&#30053;&#25277;&#35937;&#65292;&#32780;&#20854;&#20182;&#24773;&#20917;&#21487;&#33021;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#31934;&#32454;&#25277;&#35937;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#26367;&#20195;&#24418;&#29366;&#30340;&#31070;&#32463;&#25277;&#35937;&#65292;&#21363;&#20998;&#27573;&#24120;&#25968;&#25110;&#38750;&#32447;&#24615;&#38750;&#22810;&#39033;&#24335;&#65288;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;sigmoidal&#28608;&#27963;&#20989;&#25968;&#33719;&#24471;&#65289;&#12290;&#25105;&#20204;&#37319;&#29992;&#27491;&#24335;&#30340;&#24402;&#32435;&#32508;&#21512;&#31243;&#24207;&#26469;&#29983;&#25104;&#31070;&#32463;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural abstractions have been recently introduced as formal approximations of complex, nonlinear dynamical models. They comprise a neural ODE and a certified upper bound on the error between the abstract neural network and the concrete dynamical model. So far neural abstractions have exclusively been obtained as neural networks consisting entirely of $ReLU$ activation functions, resulting in neural ODE models that have piecewise affine dynamics, and which can be equivalently interpreted as linear hybrid automata. In this work, we observe that the utility of an abstraction depends on its use: some scenarios might require coarse abstractions that are easier to analyse, whereas others might require more complex, refined abstractions. We therefore consider neural abstractions of alternative shapes, namely either piecewise constant or nonlinear non-polynomial (specifically, obtained via sigmoidal activations). We employ formal inductive synthesis procedures to generate neural abstractions t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.12856</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#35268;&#21010;&#12289;&#38271;&#26399;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#31243;&#24207;&#21512;&#25104;&#33021;&#21147;&#30340;&#29616;&#23454;&#19990;&#30028;WebAgent
&lt;/p&gt;
&lt;p&gt;
A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#20027;Web&#33258;&#21160;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#32593;&#31449;&#19978;&#65292;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#19977;&#20010;&#26041;&#38754;&#30340;&#38480;&#21046;&#65306;&#24320;&#25918;&#39046;&#22495;&#24615;&#12289;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#23545;HTML&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;WebAgent&#36890;&#36807;&#23558;&#25351;&#20196;&#20998;&#35299;&#20026;&#35268;&#33539;&#30340;&#23376;&#25351;&#20196;&#65292;&#23558;&#38271;HTML&#25991;&#26723;&#24635;&#32467;&#20026;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#20174;&#20013;&#29983;&#25104;&#30340;Python&#31243;&#24207;&#23545;&#32593;&#31449;&#36827;&#34892;&#25805;&#20316;&#26469;&#25552;&#21069;&#36827;&#34892;&#35268;&#21010;&#12290;&#25105;&#20204;&#20351;&#29992;Flan-U-PaLM&#35774;&#35745;&#20102;WebAgent&#65292;&#29992;&#20110;&#29983;&#25104;&#26377;&#26681;&#20195;&#30721;&#65292;&#24182;&#20351;&#29992;HTML-T5&#36827;&#34892;&#39044;&#35757;&#32451;LLMs&#65292;&#21033;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#27880;&#24847;&#26426;&#21046;&#20197;&#21450;&#28151;&#21512;&#38271;&#36328;&#24230;&#21435;&#22122;&#30446;&#26631;&#26469;&#36827;&#34892;&#35268;&#21010;&#21644;&#24635;&#32467;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by ov
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#35821;&#35328;&#30456;&#20851;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;CLIP&#27169;&#22411;&#35843;&#25972;&#20026;&#36866;&#29992;&#20110;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#22312;&#21160;&#20316;&#27010;&#24565;&#31354;&#38388;&#20013;&#36827;&#34892;&#33258;&#33976;&#39311;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#21644;&#32447;&#24615;&#25512;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10922</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#30340;&#21160;&#20316;&#27010;&#24565;&#31354;&#38388;&#25913;&#36827;&#35270;&#39057;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Language-based Action Concept Spaces Improve Video Self-Supervised Learning. (arXiv:2307.10922v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10922
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#35821;&#35328;&#30456;&#20851;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;CLIP&#27169;&#22411;&#35843;&#25972;&#20026;&#36866;&#29992;&#20110;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#22312;&#21160;&#20316;&#27010;&#24565;&#31354;&#38388;&#20013;&#36827;&#34892;&#33258;&#33976;&#39311;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#21644;&#32447;&#24615;&#25512;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#24050;&#32463;&#23454;&#29616;&#20102;&#23398;&#20064;&#21487;&#20256;&#36882;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#23569;&#37327;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#35270;&#39057;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#31616;&#21333;&#27493;&#39588;&#65292;&#20351;&#29992;&#19982;&#35821;&#35328;&#30456;&#20851;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#23558;&#22270;&#20687;CLIP&#27169;&#22411;&#35843;&#25972;&#20026;&#35270;&#39057;&#39046;&#22495;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#36866;&#29992;&#20110;&#26102;&#38388;&#24314;&#27169;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#21160;&#20316;&#27010;&#24565;&#31354;&#38388;&#20013;&#20351;&#29992;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#33258;&#33976;&#39311;&#35757;&#32451;&#12290;&#20351;&#29992;&#30456;&#20851;&#25991;&#26412;&#25552;&#31034;&#20174;&#35821;&#35328;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#21508;&#20010;&#21160;&#20316;&#27010;&#24565;&#30340;&#29305;&#24449;&#21521;&#37327;&#26500;&#25104;&#20102;&#36825;&#20010;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#35757;&#32451;&#30446;&#26631;&#65292;&#27010;&#24565;&#33976;&#39311;&#21644;&#27010;&#24565;&#23545;&#40784;&#65292;&#26082;&#20445;&#30041;&#20102;&#21407;&#22987;&#34920;&#31034;&#30340;&#24191;&#27867;&#24615;&#65292;&#21448;&#24378;&#21270;&#20102;&#21160;&#20316;&#21644;&#20854;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#19977;&#20010;&#21160;&#20316;&#35782;&#21035;&#22522;&#20934;&#19978;&#30340;&#38646;&#26679;&#26412;&#21644;&#32447;&#24615;&#25512;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent contrastive language image pre-training has led to learning highly transferable and robust image representations. However, adapting these models to video domains with minimal supervision remains an open problem. We explore a simple step in that direction, using language tied self-supervised learning to adapt an image CLIP model to the video domain. A backbone modified for temporal modeling is trained under self-distillation settings with train objectives operating in an action concept space. Feature vectors of various action concepts extracted from a language encoder using relevant textual prompts construct this space. We introduce two train objectives, concept distillation and concept alignment, that retain generality of original representations while enforcing relations between actions and their attributes. Our approach improves zero-shot and linear probing performance on three action recognition benchmarks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#25237;&#24433;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#40784;&#27425;&#21160;&#21147;&#31995;&#32479;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#31867;&#20284;&#20110;&#32463;&#20856;&#30456;&#20851;&#20998;&#26512;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#36991;&#20813;&#20102;&#30697;&#38453;&#27714;&#36870;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#26696;&#36827;&#19968;&#27493;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09912</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#40784;&#27425;&#21160;&#21147;&#31995;&#32479;&#30340;&#28145;&#24230;&#25237;&#24433;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep projection networks for learning time-homogeneous dynamical systems. (arXiv:2307.09912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09912
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#25237;&#24433;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#40784;&#27425;&#21160;&#21147;&#31995;&#32479;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#31867;&#20284;&#20110;&#32463;&#20856;&#30456;&#20851;&#20998;&#26512;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#36991;&#20813;&#20102;&#30697;&#38453;&#27714;&#36870;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#26696;&#36827;&#19968;&#27493;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#33324;&#30340;&#26102;&#38388;&#40784;&#27425;&#21160;&#21147;&#31995;&#32479;&#65292;&#21253;&#25324;&#31163;&#25955;&#21644;&#36830;&#32493;&#30340;&#65292;&#24182;&#30740;&#31350;&#20102;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#29366;&#24577;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#36825;&#23545;&#20110;&#23398;&#20064;&#31995;&#32479;&#30340;&#21069;&#21521;&#20256;&#36755;&#31639;&#23376;&#33267;&#20851;&#37325;&#35201;&#65292;&#35813;&#31639;&#23376;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#30340;&#29366;&#24577;&#25110;&#21487;&#35266;&#27979;&#37327;&#12290;&#34920;&#31034;&#36890;&#24120;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#19982;&#25237;&#24433;&#31639;&#23376;&#30456;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#31867;&#20284;&#20110;&#32463;&#20856;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#19982;CCA&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#20989;&#25968;&#36991;&#20813;&#20102;&#30697;&#38453;&#27714;&#36870;&#65292;&#22240;&#27492;&#36890;&#24120;&#26356;&#31283;&#23450;&#19988;&#36866;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#20989;&#25968;&#26159;CCA&#30340;&#19968;&#20010;&#32039;&#26494;&#24347;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#26696;&#26469;&#22686;&#24378;&#23427;&#65292;&#19968;&#31181;&#40723;&#21169;&#34920;&#31034;&#30340;&#20998;&#37327;&#27491;&#20132;&#65292;&#32780;&#21478;&#19968;&#31181;&#21033;&#29992;&#20102; Chapman-Kolmogorov &#26041;&#31243;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
We consider the general class of time-homogeneous dynamical systems, both discrete and continuous, and study the problem of learning a meaningful representation of the state from observed data. This is instrumental for the task of learning a forward transfer operator of the system, that in turn can be used for forecasting future states or observables. The representation, typically parametrized via a neural network, is associated with a projection operator and is learned by optimizing an objective function akin to that of canonical correlation analysis (CCA). However, unlike CCA, our objective avoids matrix inversions and therefore is generally more stable and applicable to challenging scenarios. Our objective is a tight relaxation of CCA and we further enhance it by proposing two regularization schemes, one encouraging the orthogonality of the components of the representation while the other exploiting Chapman-Kolmogorov's equation. We apply our method to challenging discrete dynamical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#35299;&#20915;&#20102;Wasserstein GAN&#20013;&#20998;&#21306;&#20989;&#25968;&#26377;&#20559;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#35206;&#30422;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#26469;&#23454;&#29616;&#23545;&#20998;&#21306;&#20989;&#25968;&#30340;&#26080;&#20559;&#20272;&#35745;&#21644;&#29983;&#25104;&#22120;&#29109;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.09882</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adversarial Likelihood Estimation with One-way Flows. (arXiv:2307.09882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#35299;&#20915;&#20102;Wasserstein GAN&#20013;&#20998;&#21306;&#20989;&#25968;&#26377;&#20559;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#35206;&#30422;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#26469;&#23454;&#29616;&#23545;&#20998;&#21306;&#20989;&#25968;&#30340;&#26080;&#20559;&#20272;&#35745;&#21644;&#29983;&#25104;&#22120;&#29109;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#20294;&#26080;&#27861;&#25552;&#20379;&#26679;&#26412;&#21608;&#22260;&#30340;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#27880;&#24847;&#21040;&#22312;&#33021;&#37327;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#26368;&#22823;&#21270;&#23545;&#25968;&#20284;&#28982;&#21487;&#20197;&#23548;&#33268;&#21028;&#21035;&#22120;&#25552;&#20379;&#38750;&#24402;&#19968;&#21270;&#30340;&#23494;&#24230;&#65288;&#36890;&#24120;&#31216;&#20026;&#33021;&#37327;&#65289;&#30340;&#23545;&#25239;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;&#36825;&#19968;&#35266;&#28857;&#65292;&#32467;&#21512;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#19979;&#20869;&#23481;&#65306;1&#65289;Wasserstein GAN&#23545;&#20998;&#21306;&#20989;&#25968;&#36827;&#34892;&#20102;&#26377;&#20559;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26080;&#20559;&#20272;&#35745;&#26041;&#27861;&#65307;2&#65289;&#22312;&#26368;&#20248;&#21270;&#20284;&#28982;&#26102;&#65292;&#24517;&#39035;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#12290;&#36825;&#34987;&#20551;&#35774;&#20250;&#25552;&#20379;&#26356;&#22909;&#30340;&#27169;&#24335;&#35206;&#30422;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#26126;&#30830;&#35745;&#31639;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#12290;&#36825;&#26159;&#35774;&#35745;&#26080;&#20559;&#20272;&#35745;&#20998;&#21306;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#29983;&#25104;&#22120;&#29109;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#29983;&#25104;&#23494;&#24230;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#30340;&#27969;&#32593;&#32476;&#26469;&#33719;&#24471;&#30340;&#65292;&#31216;&#20026;&#21333;&#21521;&#27969;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way 
&lt;/p&gt;</description></item><item><title>&#20999;&#32447;&#27169;&#22411;&#32452;&#21512; (TMC) &#26159;&#19968;&#31181;&#23558;&#29420;&#31435;&#24494;&#35843;&#30340;&#27169;&#22411;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22686;&#37327;&#23398;&#20064;&#12289;&#38598;&#25104;&#21644;&#21462;&#28040;&#23398;&#20064;&#12290;&#36890;&#36807;&#26631;&#37327;&#32452;&#21512;&#26041;&#24335;&#36827;&#34892;&#32452;&#21512;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#24182;&#38477;&#20302;&#20102;&#25512;&#29702;&#25104;&#26412;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#38646;&#25104;&#26412;&#24536;&#35760;&#32452;&#25104;&#27169;&#22411;&#65292;&#19981;&#21463;&#39034;&#24207;&#20559;&#24046;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22312;&#32852;&#21512;&#25968;&#25454;&#19978;&#24182;&#34892;&#25191;&#34892;&#12290;&#22312;&#20219;&#21153;&#22686;&#37327;&#12289;&#31867;&#21035;&#22686;&#37327;&#21644;&#25968;&#25454;&#22686;&#37327;&#35774;&#32622;&#20013;&#65292;TMC&#20960;&#20046;&#22312;&#27599;&#20010;&#26041;&#26696;&#19978;&#22343;&#20248;&#20110;&#26368;&#36817;&#21457;&#34920;&#30340;&#25345;&#32493;&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08114</link><description>&lt;p&gt;
&#20999;&#32447;&#27169;&#22411;&#32452;&#21512;&#29992;&#20110;&#38598;&#25104;&#21644;&#25345;&#32493;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Tangent Model Composition for Ensembling and Continual Fine-tuning. (arXiv:2307.08114v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08114
&lt;/p&gt;
&lt;p&gt;
&#20999;&#32447;&#27169;&#22411;&#32452;&#21512; (TMC) &#26159;&#19968;&#31181;&#23558;&#29420;&#31435;&#24494;&#35843;&#30340;&#27169;&#22411;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22686;&#37327;&#23398;&#20064;&#12289;&#38598;&#25104;&#21644;&#21462;&#28040;&#23398;&#20064;&#12290;&#36890;&#36807;&#26631;&#37327;&#32452;&#21512;&#26041;&#24335;&#36827;&#34892;&#32452;&#21512;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#24182;&#38477;&#20302;&#20102;&#25512;&#29702;&#25104;&#26412;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#38646;&#25104;&#26412;&#24536;&#35760;&#32452;&#25104;&#27169;&#22411;&#65292;&#19981;&#21463;&#39034;&#24207;&#20559;&#24046;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22312;&#32852;&#21512;&#25968;&#25454;&#19978;&#24182;&#34892;&#25191;&#34892;&#12290;&#22312;&#20219;&#21153;&#22686;&#37327;&#12289;&#31867;&#21035;&#22686;&#37327;&#21644;&#25968;&#25454;&#22686;&#37327;&#35774;&#32622;&#20013;&#65292;TMC&#20960;&#20046;&#22312;&#27599;&#20010;&#26041;&#26696;&#19978;&#22343;&#20248;&#20110;&#26368;&#36817;&#21457;&#34920;&#30340;&#25345;&#32493;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#32447;&#27169;&#22411;&#32452;&#21512; (TMC) &#26159;&#19968;&#31181;&#23558;&#29420;&#31435;&#24494;&#35843;&#30340;&#32452;&#25104;&#27169;&#22411;&#32467;&#21512;&#22312;&#39044;&#35757;&#32451;&#28857;&#21608;&#22260;&#30340;&#26041;&#27861;&#12290;&#32452;&#25104;&#27169;&#22411;&#26159;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#20851;&#30340;&#20999;&#32447;&#21521;&#37327;&#65292;&#21487;&#20197;&#36890;&#36807;&#21152;&#27861;&#12289;&#32553;&#25918;&#25110;&#20943;&#27861;&#26469;&#25903;&#25345;&#22686;&#37327;&#23398;&#20064;&#12289;&#38598;&#25104;&#25110;&#21462;&#28040;&#23398;&#20064;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#32452;&#25104;&#27169;&#22411;&#36890;&#36807;&#26631;&#37327;&#32452;&#21512;&#30340;&#26041;&#24335;&#36827;&#34892;&#32452;&#21512;&#65292;&#23558;&#38598;&#25104;&#30340;&#25104;&#26412;&#38477;&#20302;&#21040;&#21333;&#20010;&#27169;&#22411;&#30340;&#27700;&#24179;&#12290;&#19982;&#38750;&#32447;&#24615;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#30456;&#27604;&#65292;TMC&#25552;&#39640;&#20102;4.2%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#23558;&#25512;&#29702;&#25104;&#26412;&#38477;&#20302;&#20102;2.5&#20493;&#33267;10&#20493;&#65292;&#19982;&#32452;&#25104;&#27169;&#22411;&#30340;&#25968;&#37327;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#27599;&#20010;&#32452;&#25104;&#27169;&#22411;&#21487;&#20197;&#20197;&#38646;&#25104;&#26412;&#24536;&#35760;&#65292;&#23545;&#25512;&#29702;&#32467;&#26524;&#27809;&#26377;&#21097;&#20313;&#24433;&#21709;&#12290;&#24403;&#29992;&#20110;&#25345;&#32493;&#24494;&#35843;&#26102;&#65292;TMC&#19981;&#21463;&#39034;&#24207;&#20559;&#24046;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#32852;&#21512;&#25968;&#25454;&#19978;&#24182;&#34892;&#25191;&#34892;&#12290;&#22312;&#20219;&#21153;&#22686;&#37327;&#12289;&#31867;&#21035;&#22686;&#37327;&#21644;&#25968;&#25454;&#22686;&#37327;&#35774;&#32622;&#20013;&#65292;TMC&#20960;&#20046;&#22312;&#27599;&#20010;&#26041;&#26696;&#19978;&#22343;&#20248;&#20110;&#26368;&#36817;&#21457;&#34920;&#30340;&#25345;&#32493;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tangent Model Composition (TMC) is a method to combine component models independently fine-tuned around a pre-trained point. Component models are tangent vectors to the pre-trained model that can be added, scaled, or subtracted to support incremental learning, ensembling, or unlearning. Component models are composed at inference time via scalar combination, reducing the cost of ensembling to that of a single model. TMC improves accuracy by 4.2% compared to ensembling non-linearly fine-tuned models at a 2.5x to 10x reduction of inference cost, growing linearly with the number of component models. Each component model can be forgotten at zero cost, with no residual effect on the resulting inference. When used for continual fine-tuning, TMC is not constrained by sequential bias and can be executed in parallel on federated data. TMC outperforms recently published continual fine-tuning methods almost uniformly on each setting -- task-incremental, class-incremental, and data-incremental -- o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#26679;&#26412;&#19978;&#30340;&#27867;&#21270;&#19982;&#35760;&#24518;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;&#26354;&#29575;&#30340;&#26679;&#26412;&#36890;&#24120;&#26159;&#20855;&#26377;&#26631;&#31614;&#38169;&#35823;&#25110;&#20914;&#31361;&#30340;&#38271;&#23614;&#26679;&#26412;&#65292;&#24182;&#22312;CIFAR100&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22833;&#36133;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#37096;&#20998;&#26679;&#26412;&#36827;&#34892;&#38543;&#26426;&#26631;&#31614;&#38169;&#35823;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26354;&#29575;&#25490;&#24207;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20986;&#36825;&#20123;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.05831</link><description>&lt;p&gt;
&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#35270;&#35282;&#25581;&#31034;&#35760;&#24518;&#21270;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Memorization Through the Lens of Curvature of Loss Function Around Samples. (arXiv:2307.05831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#26679;&#26412;&#19978;&#30340;&#27867;&#21270;&#19982;&#35760;&#24518;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;&#26354;&#29575;&#30340;&#26679;&#26412;&#36890;&#24120;&#26159;&#20855;&#26377;&#26631;&#31614;&#38169;&#35823;&#25110;&#20914;&#31361;&#30340;&#38271;&#23614;&#26679;&#26412;&#65292;&#24182;&#22312;CIFAR100&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22833;&#36133;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#37096;&#20998;&#26679;&#26412;&#36827;&#34892;&#38543;&#26426;&#26631;&#31614;&#38169;&#35823;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26354;&#29575;&#25490;&#24207;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20986;&#36825;&#20123;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#36807;&#22810;&#65292;&#24456;&#23481;&#26131;&#36807;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#12290;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#21487;&#20197;&#23436;&#20840;&#35760;&#24518;&#35757;&#32451;&#38598;&#65292;&#21363;&#20351;&#26631;&#31614;&#26159;&#38543;&#26426;&#30340;&#12290;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#35757;&#32451;&#26679;&#26412;&#21608;&#22260;&#30340;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#20316;&#20026;&#35760;&#24518;&#21270;&#31243;&#24230;&#30340;&#24230;&#37327;&#65292;&#23545;&#25152;&#26377;&#35757;&#32451;&#36718;&#27425;&#36827;&#34892;&#24179;&#22343;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26469;&#30740;&#31350;&#24120;&#35265;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#26679;&#26412;&#30340;&#27867;&#21270;&#19982;&#35760;&#24518;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#21487;&#35270;&#21270;&#20855;&#26377;&#26368;&#39640;&#25439;&#22833;&#26354;&#29575;&#30340;&#26679;&#26412;&#65292;&#21457;&#29616;&#23427;&#20204;&#36890;&#24120;&#26159;&#38271;&#23614;&#26679;&#26412;&#12289;&#26631;&#31614;&#38169;&#35823;&#25110;&#20914;&#31361;&#26679;&#26412;&#12290;&#36825;&#31181;&#20998;&#26512;&#24110;&#21161;&#25105;&#20204;&#22312;CIFAR100&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22833;&#36133;&#27169;&#22411;&#65292;&#21363;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#37325;&#22797;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#38543;&#26426;&#38169;&#35823;&#21270;&#23569;&#37327;&#26679;&#26412;&#30340;&#26631;&#31614;&#26469;&#20154;&#20026;&#22320;&#32473;&#25968;&#25454;&#38598;&#24341;&#20837;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#23637;&#31034;&#20102;&#25353;&#26354;&#29575;&#25490;&#24207;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#20986;&#26631;&#31614;&#38169;&#35823;&#26679;&#26412;&#30340;&#39640;AUROC&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are overparametrized and easily overfit the datasets they train on. In the extreme case, it is shown that they can memorize a training set with fully randomized labels. We propose using the curvature of loss function around the training sample as a measure of its memorization, averaged over all training epochs. We use this to study the generalization versus memorization properties of different samples in popular image datasets. We visualize samples with the highest curvature of loss around them, and show that these visually correspond to long-tailed, mislabeled or conflicting samples. This analysis helps us find a, to the best of our knowledge, novel failure model on the CIFAR100 dataset, that of duplicated images with different labels. We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show that sorting by curvature yields high AUROC values for identifying the mislabeled samples.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#24212;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#20013;&#65292;&#36890;&#36807;&#39044;&#27979;&#24615;&#33021;&#24182;&#21482;&#27169;&#25311;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#27169;&#25311;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#24230;&#33410;&#32422;&#35745;&#31639;&#36164;&#28304;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#20302;&#25104;&#26412;&#26597;&#35810;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#30446;&#26631;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#36991;&#20813;&#20102;&#23545;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35201;&#27714;&#12290;&#22312;&#27969;&#20307;&#21160;&#21147;&#23398;&#39046;&#22495;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.04536</link><description>&lt;p&gt;
DADO -- &#29992;&#20110;&#28145;&#24230;&#20027;&#21160;&#35774;&#35745;&#20248;&#21270;&#30340;&#20302;&#25104;&#26412;&#26597;&#35810;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
DADO -- Low-Cost Query Strategies for Deep Active Design Optimization. (arXiv:2307.04536v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04536
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#24212;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#20013;&#65292;&#36890;&#36807;&#39044;&#27979;&#24615;&#33021;&#24182;&#21482;&#27169;&#25311;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#27169;&#25311;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#24230;&#33410;&#32422;&#35745;&#31639;&#36164;&#28304;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#20302;&#25104;&#26412;&#26597;&#35810;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#30446;&#26631;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#36991;&#20813;&#20102;&#23545;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35201;&#27714;&#12290;&#22312;&#27969;&#20307;&#21160;&#21147;&#23398;&#39046;&#22495;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#32463;&#39564;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#24212;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#39046;&#22495;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#26114;&#36149;&#30340;&#25968;&#20540;&#27169;&#25311;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#20248;&#21270;&#32467;&#26500;&#32452;&#20214;&#30340;&#35774;&#35745;&#65292;&#20854;&#20013;&#24418;&#29366;&#30001;&#19968;&#32452;&#21442;&#25968;&#25551;&#36848;&#12290;&#22914;&#26524;&#25105;&#20204;&#21487;&#20197;&#22522;&#20110;&#36825;&#20123;&#21442;&#25968;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#20165;&#32771;&#34385;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#27169;&#25311;&#65292;&#23601;&#26377;&#24040;&#22823;&#30340;&#33410;&#30465;&#35745;&#31639;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#33258;&#25105;&#20248;&#21270;&#30340;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#22810;&#30446;&#26631;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#26041;&#27861;&#65292;&#26131;&#20110;&#24212;&#29992;&#65292;&#22312;&#38543;&#26426;&#25277;&#26679;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#35268;&#36991;&#20102;&#23545;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#27969;&#20307;&#21160;&#21147;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31574;&#30053;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#30830;&#23450;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#35780;&#20272;&#32467;&#26524;&#31361;&#20986;&#20102;&#25105;&#20204;&#30340;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
In this experience report, we apply deep active learning to the field of design optimization to reduce the number of computationally expensive numerical simulations. We are interested in optimizing the design of structural components, where the shape is described by a set of parameters. If we can predict the performance based on these parameters and consider only the promising candidates for simulation, there is an enormous potential for saving computing power. We present two selection strategies for self-optimization to reduce the computational cost in multi-objective design optimization problems. Our proposed methodology provides an intuitive approach that is easy to apply, offers significant improvements over random sampling, and circumvents the need for uncertainty estimation. We evaluate our strategies on a large dataset from the domain of fluid dynamics and introduce two new evaluation metrics to determine the model's performance. Findings from our evaluation highlights the effec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26657;&#20934;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#26041;&#27861;&#25913;&#36827;&#22522;&#20110;&#35780;&#20998;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#20998;&#31867;&#22120;&#20316;&#20026;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#24182;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04081</link><description>&lt;p&gt;
&#33258;&#26657;&#20934;&#20998;&#31867;&#22120;&#24341;&#23548;&#19979;&#65292;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#22522;&#20110;&#35780;&#20998;&#30340;&#26465;&#20214;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance. (arXiv:2307.04081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26657;&#20934;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#26041;&#27861;&#25913;&#36827;&#22522;&#20110;&#35780;&#20998;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#20998;&#31867;&#22120;&#20316;&#20026;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#24182;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#65292;&#33021;&#22815;&#36798;&#21040;&#39046;&#20808;&#30340;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#24050;&#32463;&#23558;SGMs&#25193;&#23637;&#21040;&#22788;&#29702;&#31867;&#26465;&#20214;&#29983;&#25104;&#65292;&#36890;&#36807;&#23558;&#26080;&#26465;&#20214;&#30340;SGM&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;SGMs&#22312;&#35757;&#32451;&#25968;&#37327;&#36739;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#26102;&#24182;&#19981;&#24635;&#33021;&#23454;&#29616;&#20934;&#30830;&#30340;&#26465;&#20214;&#29983;&#25104;&#12290;&#25105;&#20204;&#35748;&#20026;&#38382;&#39064;&#26681;&#28304;&#22312;&#20110;&#20998;&#31867;&#22120;&#30340;&#19981;&#21487;&#38752;&#26799;&#24230;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#35753;&#20998;&#31867;&#22120;&#33258;&#26657;&#20934;&#26469;&#25913;&#36827;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;SGMs&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#33021;&#37327;&#27169;&#22411;&#30340;&#21407;&#21017;&#23558;&#20998;&#31867;&#22120;&#36716;&#21270;&#20026;&#26080;&#26465;&#20214;SGM&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#37319;&#29992;&#29616;&#26377;&#30340;&#26080;&#26465;&#20214;SGM&#25439;&#22833;&#20989;&#25968;&#26469;&#20351;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#26657;&#20934;&#20998;&#31867;&#22120;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#26465;&#20214;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based Generative Models (SGMs) are a popular family of deep generative models that achieves leading image generation quality. Earlier studies have extended SGMs to tackle class-conditional generation by coupling an unconditional SGM with the guidance of a trained classifier. Nevertheless, such classifier-guided SGMs do not always achieve accurate conditional generation, especially when trained with fewer labeled data. We argue that the issue is rooted in unreliable gradients of the classifier and the inability to fully utilize unlabeled data during training. We then propose to improve classifier-guided SGMs by letting the classifier calibrate itself. Our key idea is to use principles from energy-based models to convert the classifier as another view of the unconditional SGM. Then, existing loss for the unconditional SGM can be adopted to calibrate the classifier using both labeled and unlabeled data. Empirical results validate that the proposed approach significantly improves the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;EffUNet&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24314;&#31569;&#21644;&#36947;&#36335;&#20998;&#21106;&#30340;&#26032;&#26550;&#26500;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#22312;&#39532;&#33832;&#35832;&#22622;&#24030;&#24314;&#31569;&#29289;&#21644;&#36947;&#36335;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.03980</link><description>&lt;p&gt;
&#20351;&#29992;EffUNet&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24314;&#31569;&#21644;&#36947;&#36335;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Building and Road Segmentation Using EffUNet and Transfer Learning Approach. (arXiv:2307.03980v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;EffUNet&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24314;&#31569;&#21644;&#36947;&#36335;&#20998;&#21106;&#30340;&#26032;&#26550;&#26500;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#22312;&#39532;&#33832;&#35832;&#22622;&#24030;&#24314;&#31569;&#29289;&#21644;&#36947;&#36335;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#20013;&#65292;&#20102;&#35299;&#22478;&#24066;&#23545;&#35937;&#65288;&#22914;&#20379;&#27700;&#12289;&#38081;&#36335;&#32447;&#12289;&#30005;&#21147;&#32447;&#36335;&#12289;&#24314;&#31569;&#29289;&#12289;&#36947;&#36335;&#31561;&#65289;&#30340;&#20449;&#24687;&#23545;&#22478;&#24066;&#35268;&#21010;&#26159;&#24517;&#35201;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25919;&#31574;&#21046;&#23450;&#32773;&#38656;&#35201;&#20102;&#35299;&#36825;&#20123;&#23545;&#35937;&#30340;&#20998;&#24067;&#12289;&#20301;&#32622;&#21644;&#23481;&#37327;&#65292;&#20197;&#20570;&#20986;&#26377;&#24433;&#21709;&#21147;&#30340;&#20915;&#31574;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#21355;&#26143;&#21644;&#26080;&#20154;&#26426;&#25293;&#25668;&#30340;&#33322;&#31354;&#22270;&#20687;&#23545;&#24314;&#31569;&#29289;&#21644;&#36947;&#36335;&#36827;&#34892;&#20998;&#21106;&#12290;&#35768;&#22810;&#19981;&#21516;&#30340;&#26550;&#26500;&#24050;&#32463;&#34987;&#25552;&#20986;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#20854;&#20013;UNet&#26159;&#20854;&#20013;&#20043;&#19968;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;&#26032;&#25552;&#20986;&#30340;EfficientNetV2&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#30340;&#32534;&#30721;&#22120;&#21644;UNet&#35299;&#30721;&#22120;&#26500;&#24314;&#20998;&#21106;&#22270;&#30340;&#26032;&#26550;&#26500;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#39532;&#33832;&#35832;&#22622;&#24030;&#24314;&#31569;&#29289;&#21644;&#36947;&#36335;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#22522;&#20934;&#20998;&#25968;&#65292;&#20998;&#21035;&#20026;0.8365&#21644;0.9153&#30340;mIOU&#12290;
&lt;/p&gt;
&lt;p&gt;
In city, information about urban objects such as water supply, railway lines, power lines, buildings, roads, etc., is necessary for city planning. In particular, information about the spread of these objects, locations and capacity is needed for the policymakers to make impactful decisions. This thesis aims to segment the building and roads from the aerial image captured by the satellites and UAVs. Many different architectures have been proposed for the semantic segmentation task and UNet being one of them. In this thesis, we propose a novel architecture based on Google's newly proposed EfficientNetV2 as an encoder for feature extraction with UNet decoder for constructing the segmentation map. Using this approach we achieved a benchmark score for the Massachusetts Building and Road dataset with an mIOU of 0.8365 and 0.9153 respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#20102;&#38050;&#38081;&#34920;&#38754;&#31895;&#31961;&#24230;&#21442;&#25968;&#30340;&#35745;&#31639;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#22312;&#32447;&#27979;&#37327;&#36716;&#21270;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.03723</link><description>&lt;p&gt;
&#20351;&#29992;&#28608;&#20809;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38050;&#38081;&#34920;&#38754;&#31895;&#31961;&#24230;&#21442;&#25968;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Steel Surface Roughness Parameter Calculations Using Lasers and Machine Learning Models. (arXiv:2307.03723v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#20102;&#38050;&#38081;&#34920;&#38754;&#31895;&#31961;&#24230;&#21442;&#25968;&#30340;&#35745;&#31639;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#22312;&#32447;&#27979;&#37327;&#36716;&#21270;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#24102;&#38050;&#34920;&#38754;&#32441;&#29702;&#23545;&#20110;&#28385;&#36275;&#38208;&#38156;&#21644;&#36711;&#21046;&#24037;&#33402;&#20013;&#23458;&#25143;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#29983;&#20135;&#21518;&#30340;&#27979;&#38024;&#27979;&#37327;&#65292;&#32780;&#22312;&#32447;&#25216;&#26415;&#21017;&#25552;&#20379;&#20102;&#23545;&#25972;&#20010;&#24102;&#38050;&#36827;&#34892;&#38750;&#25509;&#35302;&#21644;&#23454;&#26102;&#27979;&#37327;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#20934;&#30830;&#27979;&#37327;&#23545;&#20110;&#20854;&#22312;&#21046;&#36896;&#27969;&#31243;&#20013;&#30340;&#26377;&#25928;&#21033;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#20934;&#30830;&#30340;&#22312;&#32447;&#27979;&#37327;&#20351;&#24471;&#22312;&#29983;&#20135;&#36807;&#31243;&#20013;&#33021;&#22815;&#23454;&#26102;&#35843;&#25972;&#21046;&#36896;&#21152;&#24037;&#21442;&#25968;&#65292;&#30830;&#20445;&#20135;&#21697;&#36136;&#37327;&#30340;&#19968;&#33268;&#24615;&#21644;&#36711;&#26426;&#30340;&#38381;&#29615;&#25511;&#21046;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25913;&#36827;&#22312;&#32447;&#27979;&#37327;&#36716;&#21270;&#20026;&#26356;&#20934;&#30830;&#30340;Ra&#34920;&#38754;&#31895;&#31961;&#24230;&#25351;&#26631;&#12290;&#36890;&#36807;&#27604;&#36739;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#21644;&#38750;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#19982;&#38381;&#21512;&#24418;&#24335;&#36716;&#21270;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#25552;&#39640;&#34920;&#38754;&#31895;&#31961;&#24230;&#21442;&#25968;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Control of surface texture in strip steel is essential to meet customer requirements during galvanizing and temper rolling processes. Traditional methods rely on post-production stylus measurements, while on-line techniques offer non-contact and real-time measurements of the entire strip. However, ensuring accurate measurement is imperative for their effective utilization in the manufacturing pipeline. Moreover, accurate on-line measurements enable real-time adjustments of manufacturing processing parameters during production, ensuring consistent quality and the possibility of closed-loop control of the temper mill. In this study, we leverage state-of-the-art machine learning models to enhance the transformation of on-line measurements into significantly a more accurate Ra surface roughness metric. By comparing a selection of data-driven approaches, including both deep learning and non-deep learning methods, to the close-form transformation, we evaluate their potential for improving su
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#31561;&#28183;&#24615;&#30340;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#20102;&#26032;&#39062;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65292;&#22312;&#39640;&#32500;&#37319;&#26679;&#20013;&#34920;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02037</link><description>&lt;p&gt;
&#26080;&#31561;&#28183;&#24615;&#30340;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#65306;&#19968;&#31181;&#36870;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach. (arXiv:2307.02037v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#31561;&#28183;&#24615;&#30340;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#20102;&#26032;&#39062;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65292;&#22312;&#39640;&#32500;&#37319;&#26679;&#20013;&#34920;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#36890;&#24120;&#21462;&#20915;&#20110;&#25193;&#25955;&#36335;&#24452;&#19978;&#24471;&#20998;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#37325;&#28857;&#20851;&#27880;&#25193;&#25955;&#27169;&#22411;&#21450;&#20854;&#29983;&#25104;&#39640;&#36136;&#37327;&#25968;&#25454;&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#36890;&#36807;&#36870;&#25193;&#25955;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#23545;&#37319;&#26679;&#25991;&#29486;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#26680;&#30340;&#20998;&#35299;&#23558;&#24471;&#20998;&#20272;&#35745;&#36716;&#21270;&#20026;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#20272;&#35745;&#36741;&#21161;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#36870;&#25193;&#25955;&#36807;&#31243;&#21487;&#20197;&#20135;&#29983;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24635;&#21464;&#24046;&#36317;&#31163;&#19979;&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#31561;&#28183;&#24615;&#20381;&#36182;&#24615;&#30456;&#23545;&#36739;&#20302;&#65292;&#27604;&#20256;&#32479;&#30340;MCMC&#25216;&#26415;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#39640;&#32500;&#37319;&#26679;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficacy of modern generative models is commonly contingent upon the precision of score estimation along the diffusion path, with a focus on diffusion models and their ability to generate high-quality data samples. This study delves into the potentialities of posterior sampling through reverse diffusion. An examination of the sampling literature reveals that score estimation can be transformed into a mean estimation problem via the decomposition of the transition kernel. By estimating the mean of the auxiliary distribution, the reverse diffusion process can give rise to a novel posterior sampling algorithm, which diverges from traditional gradient-based Markov Chain Monte Carlo (MCMC) methods. We provide the convergence analysis in total variation distance and demonstrate that the isoperimetric dependency of the proposed algorithm is comparatively lower than that observed in conventional MCMC techniques, which justifies the superior performance for high dimensional sampling with er
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Seq2Peak&#26694;&#26550;&#65292;&#38024;&#23545;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;&#39640;&#24230;&#38750;&#24179;&#31283;&#24615;&#21644;&#24615;&#33021;&#35780;&#20272;&#38382;&#39064;&#65292;&#25104;&#21151;&#32553;&#23567;&#20102;&#22312;&#24120;&#35268;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.01597</link><description>&lt;p&gt;
&#22312;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#20013;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;: Seq2Peak&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework. (arXiv:2307.01597v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Seq2Peak&#26694;&#26550;&#65292;&#38024;&#23545;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;&#39640;&#24230;&#38750;&#24179;&#31283;&#24615;&#21644;&#24615;&#33021;&#35780;&#20272;&#38382;&#39064;&#65292;&#25104;&#21151;&#32553;&#23567;&#20102;&#22312;&#24120;&#35268;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#65288;PHSF&#65289;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24120;&#35268;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;PHSF&#20013;&#21364;&#38590;&#20197;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;&#36825;&#21487;&#33021;&#24402;&#22240;&#20110;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#20013;&#39640;&#24230;&#38750;&#24179;&#31283;&#24615;&#30340;&#25361;&#25112;&#65292;&#20351;&#24471;&#30452;&#25509;&#39044;&#27979;&#27604;&#26631;&#20934;&#30340;TSF&#26356;&#21152;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25163;&#21160;&#20174;&#24120;&#35268;&#39044;&#27979;&#32467;&#26524;&#20013;&#25552;&#21462;&#26368;&#22823;&#20540;&#20250;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#27169;&#22411;&#20250;&#26368;&#23567;&#21270;&#24179;&#22343;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Seq2Peak&#65292;&#19968;&#20010;&#19987;&#20026;PHSF&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#22312;TSF&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;Seq2Peak&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;CyclicNorm&#27969;&#31243;&#26469;&#20943;&#36731;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#33258;&#30001;&#23792;&#20540;&#23567;&#26102;&#35299;&#30721;&#22120;&#65292;&#37319;&#29992;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#21033;&#29992;&#21407;&#22987;&#24207;&#21015;&#21644;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peak-Hour Series Forecasting (PHSF) is a crucial yet underexplored task in various domains. While state-of-the-art deep learning models excel in regular Time Series Forecasting (TSF), they struggle to achieve comparable results in PHSF. This can be attributed to the challenges posed by the high degree of non-stationarity in peak-hour series, which makes direct forecasting more difficult than standard TSF. Additionally, manually extracting the maximum value from regular forecasting results leads to suboptimal performance due to models minimizing the mean deficit. To address these issues, this paper presents Seq2Peak, a novel framework designed specifically for PHSF tasks, bridging the performance gap observed in TSF models. Seq2Peak offers two key components: the CyclicNorm pipeline to mitigate the non-stationarity issue, and a simple yet effective trainable-parameter-free peak-hour decoder with a hybrid loss function that utilizes both the original series and peak-hour series as superv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#30340;&#24213;&#23618;&#32553;&#25918;&#23450;&#24459;&#21644;&#26222;&#36866;&#32479;&#35745;&#32467;&#26500;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#31867;&#27604;&#20026;&#29289;&#29702;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#29305;&#24449;-&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20540;&#32479;&#35745;&#37327;&#30340;&#35268;&#24459;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26080;&#20851;&#38543;&#26426;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#38271;&#31243;&#30456;&#20851;&#24615;&#23436;&#20840;&#24674;&#22797;&#32553;&#25918;&#34892;&#20026;&#12290;&#21516;&#26102;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37117;&#23646;&#20110;&#28151;&#27788;&#31995;&#32479;&#65292;&#24182;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#21363;&#21487;&#20307;&#29616;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#32479;&#35745;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.14975</link><description>&lt;p&gt;
&#22797;&#26434;&#25968;&#25454;&#38598;&#30340;&#24213;&#23618;&#32553;&#25918;&#23450;&#24459;&#21644;&#26222;&#36866;&#32479;&#35745;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets. (arXiv:2306.14975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#30340;&#24213;&#23618;&#32553;&#25918;&#23450;&#24459;&#21644;&#26222;&#36866;&#32479;&#35745;&#32467;&#26500;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#31867;&#27604;&#20026;&#29289;&#29702;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#29305;&#24449;-&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20540;&#32479;&#35745;&#37327;&#30340;&#35268;&#24459;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26080;&#20851;&#38543;&#26426;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#38271;&#31243;&#30456;&#20851;&#24615;&#23436;&#20840;&#24674;&#22797;&#32553;&#25918;&#34892;&#20026;&#12290;&#21516;&#26102;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37117;&#23646;&#20110;&#28151;&#27788;&#31995;&#32479;&#65292;&#24182;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#21363;&#21487;&#20307;&#29616;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#32479;&#35745;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#25968;&#25454;&#38598;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#20013;&#37117;&#20986;&#29616;&#30340;&#26222;&#36941;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#31867;&#27604;&#20026;&#29289;&#29702;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#24037;&#20855;&#25581;&#31034;&#20854;&#24213;&#23618;&#32467;&#26500;&#12290;&#25105;&#20204;&#37325;&#28857;&#20998;&#26512;&#20102;&#29305;&#24449;-&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#20998;&#26512;&#20102;&#20854;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20540;&#32479;&#35745;&#37327;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#32467;&#26524;&#26159;&#65306;(i) &#22823;&#37096;&#20998;&#29305;&#24449;&#20540;&#21576;&#29616;&#30340;&#24130;&#24459;&#32553;&#25918;&#22312;&#26080;&#30456;&#20851;&#38543;&#26426;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;(ii) &#36890;&#36807;&#31616;&#21333;&#22320;&#24341;&#20837;&#38271;&#31243;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#23436;&#20840;&#24674;&#22797;&#36825;&#31181;&#32553;&#25918;&#34892;&#20026;&#21040;&#21512;&#25104;&#25968;&#25454;&#20013;&#65292;(iii) &#20174;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#35282;&#24230;&#30475;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#23646;&#20110;&#21516;&#19968;&#20010;&#26222;&#36866;&#24615;&#31867;&#21035;&#65292;&#37117;&#26159;&#28151;&#27788;&#31995;&#32479;&#32780;&#38750;&#21487;&#31215;&#31995;&#32479;&#65292;(iv) &#39044;&#26399;&#30340;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#32479;&#35745;&#34892;&#20026;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23601;&#24050;&#32463;&#22312;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20013;&#24471;&#21040;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study universal traits which emerge both in real-world complex datasets, as well as in artificially generated ones. Our approach is to analogize data to a physical system and employ tools from statistical physics and Random Matrix Theory (RMT) to reveal their underlying structure. We focus on the feature-feature covariance matrix, analyzing both its local and global eigenvalue statistics. Our main observations are: (i) The power-law scalings that the bulk of its eigenvalues exhibit are vastly different for uncorrelated random data compared to real-world data, (ii) this scaling behavior can be completely recovered by introducing long range correlations in a simple way to the synthetic data, (iii) both generated and real-world datasets lie in the same universality class from the RMT perspective, as chaotic rather than integrable systems, (iv) the expected RMT statistical behavior already manifests for empirical covariance matrices at dataset sizes significantly smaller than those conv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26497;&#38480;&#65292;&#24182;&#25506;&#35752;&#20102;MLP&#30456;&#36739;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#26088;&#22312;&#25512;&#36827;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#21644;&#23454;&#36341;&#30340;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.13575</link><description>&lt;p&gt;
MLP&#30340;&#35268;&#27169;&#21270;&#65306;&#24402;&#32435;&#20559;&#24046;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Scaling MLPs: A Tale of Inductive Bias. (arXiv:2306.13575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26497;&#38480;&#65292;&#24182;&#25506;&#35752;&#20102;MLP&#30456;&#36739;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#26088;&#22312;&#25512;&#36827;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#21644;&#23454;&#36341;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#22522;&#26412;&#30340;&#26500;&#24314;&#22359;&#8212;&#8212;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26497;&#38480;&#12290;MLP&#30340;&#23454;&#39564;&#24615;&#27934;&#35265;&#22312;&#22810;&#20010;&#26041;&#38754;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we revisit the most fundamental building block in deep learning, the multi-layer perceptron (MLP), and study the limits of its performance on vision tasks. Empirical insights into MLPs are important for multiple reasons. (1) Given the recent narrative "less inductive bias is better", popularized due to transformers eclipsing convolutional models, it is natural to explore the limits of this hypothesis. To that end, MLPs offer an ideal test bed, being completely free of any inductive bias. (2) MLPs have almost exclusively been the main protagonist in the deep learning theory literature due to their mathematical simplicity, serving as a proxy to explain empirical phenomena observed for more complex architectures. Surprisingly, experimental datapoints for MLPs are very difficult to find in the literature, especially when coupled with large pre-training protocols. This discrepancy between practice and theory is worrying: Do MLPs reflect the empirical advances exhibited by pract
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#36793;&#38469;&#30697;&#38453;&#20998;&#35299;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#26469;&#22686;&#24191;&#21644;&#32454;&#21270;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#30340;&#35780;&#32423;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33258;&#25105;&#35757;&#32451;&#26469;&#35780;&#20272;&#35780;&#20998;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22686;&#24191;&#31574;&#30053;&#26469;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13050</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#25968;&#25454;&#22686;&#24191;&#65306;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#36793;&#38469;&#30697;&#38453;&#20998;&#35299;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data augmentation for recommender system: A semi-supervised approach using maximum margin matrix factorization. (arXiv:2306.13050v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#36793;&#38469;&#30697;&#38453;&#20998;&#35299;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#26469;&#22686;&#24191;&#21644;&#32454;&#21270;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#30340;&#35780;&#32423;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33258;&#25105;&#35757;&#32451;&#26469;&#35780;&#20272;&#35780;&#20998;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22686;&#24191;&#31574;&#30053;&#26469;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#24050;&#25104;&#20026;&#25512;&#33616;&#31995;&#32479;&#24320;&#21457;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#20854;&#20013;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#36807;&#21435;&#21916;&#22909;&#21644;&#20854;&#20182;&#29992;&#25143;&#30340;&#21487;&#29992;&#20559;&#22909;&#20449;&#24687;&#39044;&#27979;&#20854;&#23545;&#26032;&#29289;&#21697;&#30340;&#35780;&#20998;&#12290;&#23613;&#31649;CF&#26041;&#27861;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#24615;&#33021;&#36890;&#24120;&#21463;&#35266;&#23519;&#21040;&#30340;&#26465;&#30446;&#30340;&#31232;&#30095;&#24615;&#30340;&#26497;&#22823;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#26368;&#22823;&#36793;&#38469;&#30697;&#38453;&#20998;&#35299;&#65288;MMMF&#65289;&#30340;&#25968;&#25454;&#22686;&#24191;&#21644;&#32454;&#21270;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#26159;&#24191;&#27867;&#25509;&#21463;&#30340;&#29992;&#20110;&#35780;&#32423;&#39044;&#27979;&#30340;CF&#25216;&#26415;&#65292;&#20043;&#21069;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#21033;&#29992;CF&#31639;&#27861;&#30340;&#22266;&#26377;&#29305;&#24615;&#26469;&#35780;&#20272;&#21333;&#20010;&#35780;&#20998;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#35780;&#32423;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#20551;&#35774;&#20219;&#20309;CF&#31639;&#27861;&#30340;&#39044;&#27979;&#20302;&#32622;&#20449;&#24230;&#26159;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#26576;&#20123;&#19981;&#36275;&#65292;&#22240;&#27492;&#65292;&#36890;&#36807;&#37319;&#29992;&#31995;&#32479;&#30340;&#25968;&#25454;&#22686;&#24191;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) has become a popular method for developing recommender systems (RS) where ratings of a user for new items is predicted based on her past preferences and available preference information of other users. Despite the popularity of CF-based methods, their performance is often greatly limited by the sparsity of observed entries. In this study, we explore the data augmentation and refinement aspects of Maximum Margin Matrix Factorization (MMMF), a widely accepted CF technique for the rating predictions, which have not been investigated before. We exploit the inherent characteristics of CF algorithms to assess the confidence level of individual ratings and propose a semi-supervised approach for rating augmentation based on self-training. We hypothesize that any CF algorithm's predictions with low confidence are due to some deficiency in the training data and hence, the performance of the algorithm can be improved by adopting a systematic data augmentation strategy
&lt;/p&gt;</description></item><item><title>SPRINT &#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#37325;&#26631;&#35760;&#21450;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#39044;&#35757;&#32451;&#25152;&#38656;&#30340;&#20154;&#21147;&#65292;&#21516;&#26102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.11886</link><description>&lt;p&gt;
SPRINT&#65306;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196; relabeling &#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#31574;&#30053;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling. (arXiv:2306.11886v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11886
&lt;/p&gt;
&lt;p&gt;
SPRINT &#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#37325;&#26631;&#35760;&#21450;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#39044;&#35757;&#32451;&#25152;&#38656;&#30340;&#20154;&#21147;&#65292;&#21516;&#26102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#26426;&#22120;&#20154;&#31574;&#30053;&#24182;&#36171;&#20104;&#20016;&#23500;&#30340;&#25216;&#33021;&#38598;&#21512;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#19979;&#28216;&#20219;&#21153;&#30340;&#23398;&#20064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23450;&#20041;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20294;&#36825;&#38656;&#35201;&#20154;&#20026;&#22320;&#27880;&#37322;&#25968;&#21313;&#19975;&#20010;&#25351;&#20196;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SPRINT&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#22823;&#22823;&#20943;&#23569;&#39044;&#35757;&#32451;&#22810;&#26679;&#30340;&#25216;&#33021;&#25152;&#38656;&#30340;&#20154;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#26680;&#24515;&#24819;&#27861;&#26469;&#33258;&#21160;&#25193;&#23637;&#22522;&#30784;&#39044;&#35757;&#32451;&#20219;&#21153;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#25351;&#20196;&#37325;&#26631;&#35760;&#21644;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20132;&#21449;&#36712;&#36857;&#25216;&#33021;&#38142;&#25509;&#12290;&#22240;&#27492;&#65292;SPRINT &#39044;&#35757;&#32451;&#21487;&#20197;&#20026;&#26426;&#22120;&#20154;&#35013;&#22791;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#12290;&#22312;&#23478;&#24237;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#21416;&#25151;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPRINT &#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training robot policies with a rich set of skills can substantially accelerate the learning of downstream tasks. Prior works have defined pre-training tasks via natural language instructions, but doing so requires tedious human annotation of hundreds of thousands of instructions. Thus, we propose SPRINT, a scalable offline policy pre-training approach which substantially reduces the human effort needed for pre-training a diverse set of skills. Our method uses two core ideas to automatically expand a base set of pre-training tasks: instruction relabeling via large language models and cross-trajectory skill chaining through offline reinforcement learning. As a result, SPRINT pre-training equips robots with a much richer repertoire of skills. Experimental results in a household simulator and on a real robot kitchen manipulation task show that SPRINT leads to substantially faster learning of new long-horizon tasks than previous pre-training approaches. Website at https://clvrai.com/spr
&lt;/p&gt;</description></item><item><title>phi-1&#26159;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31934;&#24515;&#35757;&#32451;&#21644;&#20248;&#21270;&#65292;&#23613;&#31649;&#35268;&#27169;&#30456;&#23545;&#36739;&#23567;&#65292;&#20294;&#22312;&#20934;&#30830;&#29575;&#21644;&#26032;&#30340;&#24615;&#36136;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.11644</link><description>&lt;p&gt;
&#25945;&#31185;&#20070;&#26159;&#20320;&#38656;&#35201;&#30340;&#20840;&#37096;&#12290; (arXiv:2306.11644v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Textbooks Are All You Need. (arXiv:2306.11644v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11644
&lt;/p&gt;
&lt;p&gt;
phi-1&#26159;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31934;&#24515;&#35757;&#32451;&#21644;&#20248;&#21270;&#65292;&#23613;&#31649;&#35268;&#27169;&#30456;&#23545;&#36739;&#23567;&#65292;&#20294;&#22312;&#20934;&#30830;&#29575;&#21644;&#26032;&#30340;&#24615;&#36136;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;phi-1&#65292;&#20854;&#20307;&#31215;&#26126;&#26174;&#23567;&#20110;&#31454;&#20105;&#27169;&#22411;&#65306;phi-1&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#25317;&#26377;13&#20159;&#20010;&#21442;&#25968;&#65292;&#22312;8&#20010;A100&#19978;&#36827;&#34892;&#20102;4&#22825;&#30340;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#32593;&#32476;&#30340;&#8220;&#25945;&#31185;&#20070;&#36136;&#37327;&#8221;&#25968;&#25454;&#65288;60&#20159;&#20010;&#26631;&#35760;&#65289;&#21644;&#20351;&#29992;GPT-3.5&#21512;&#25104;&#29983;&#25104;&#30340;&#25945;&#31185;&#20070;&#21644;&#32451;&#20064;&#65288;10&#20159;&#20010;&#26631;&#35760;&#65289;&#12290;&#23613;&#31649;&#35268;&#27169;&#23567;&#65292;phi-1&#22312;HumanEval&#19978;&#30340;pass@1&#20934;&#30830;&#29575;&#20026;50.6&#65285;&#65292;&#22312;MBPP&#19978;&#20026;55.5&#65285;&#12290;&#19982;&#25105;&#20204;&#22312;&#32534;&#30721;&#32451;&#20064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#20043;&#21069;&#30340;&#27169;&#22411; phi-1-base &#21644;&#20855;&#26377;&#30456;&#21516;&#27969;&#31243;&#30340;350M&#21442;&#25968;&#30340;&#36739;&#23567;&#27169;&#22411; phi-1-small &#30456;&#27604;&#65292;&#23427;&#36824;&#23637;&#29616;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#26032;&#30340;&#24615;&#36136;&#65292;phi-1-small &#22312; HumanEval &#19978;&#20173;&#36798;&#21040;45&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#23450;&#20301;&#36793;&#38469;&#23450;&#20215;&#65288;LMP&#65289;&#26102;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#20010;&#30005;&#21147;&#32593;&#32476;&#19978;&#20855;&#26377;&#20934;&#30830;&#19988;&#31283;&#20581;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.10080</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20934;&#23454;&#26102;&#23450;&#20301;&#36793;&#38469;&#23450;&#20215;&#26041;&#27861;&#65306;&#21487;&#34892;&#24615;&#21644;&#31283;&#20581;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AI Driven Near Real-time Locational Marginal Pricing Method: A Feasibility and Robustness Study. (arXiv:2306.10080v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#23450;&#20301;&#36793;&#38469;&#23450;&#20215;&#65288;LMP&#65289;&#26102;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#20010;&#30005;&#21147;&#32593;&#32476;&#19978;&#20855;&#26377;&#20934;&#30830;&#19988;&#31283;&#20581;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20215;&#26684;&#39044;&#27979;&#23545;&#20110;&#24066;&#22330;&#21442;&#19982;&#32773;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#20248;&#21270;&#20182;&#20204;&#30340;&#25805;&#20316;&#35745;&#21010;&#21644;&#31454;&#26631;&#31574;&#30053;&#65292;&#29305;&#21035;&#26159;&#22312;&#24403;&#21069;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#32463;&#20856;&#26041;&#27861;&#39044;&#27979;&#30005;&#21147;&#20215;&#26684;&#21464;&#24471;&#26356;&#21152;&#27874;&#21160;&#21644;&#19981;&#21487;&#39044;&#27979;&#12290;&#23450;&#20301;&#36793;&#38469;&#23450;&#20215;&#65288;LMP&#65289;&#23450;&#20215;&#26426;&#21046;&#22312;&#35768;&#22810;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#20013;&#20351;&#29992;&#65292;&#20854;&#20013;&#20256;&#32479;&#26041;&#27861;&#21033;&#29992;&#26368;&#20248;&#28526;&#27969;&#65288;OPF&#65289;&#27714;&#35299;&#22120;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22411;&#30005;&#21147;&#32593;&#32476;&#65292;&#36825;&#20010;&#36807;&#31243;&#21464;&#24471;&#32791;&#26102;&#19988;&#35745;&#31639;&#23494;&#38598;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#39044;&#27979;&#21487;&#20197;&#20026;LMP&#39044;&#27979;&#25552;&#20379;&#39640;&#25928;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#21487;&#20877;&#29983;&#33021;&#28304;&#31561;&#38388;&#27463;&#24615;&#26469;&#28304;&#30340;&#33021;&#28304;&#24066;&#22330;&#20013;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#20010;&#30005;&#21147;&#32593;&#32476;&#19978;&#39044;&#27979;LMP&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#22810;&#31181;&#24773;&#26223;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;LMP&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ML&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#19988;&#31283;&#20581;&#30340;LMP&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate price predictions are essential for market participants in order to optimize their operational schedules and bidding strategies, especially in the current context where electricity prices become more volatile and less predictable using classical approaches. The Locational Marginal Pricing (LMP) pricing mechanism is used in many modern power markets, where the traditional approach utilizes optimal power flow (OPF) solvers. However, for large electricity grids this process becomes prohibitively time-consuming and computationally intensive. Machine learning (ML) based predictions could provide an efficient tool for LMP prediction, especially in energy markets with intermittent sources like renewable energy. This study evaluates the performance of popular machine learning and deep learning models in predicting LMP on multiple electricity grids. The accuracy and robustness of these models in predicting LMP is assessed considering multiple scenarios. The results show that ML models 
&lt;/p&gt;</description></item><item><title>MUBen&#35780;&#20272;&#19981;&#21516;&#39592;&#24178;&#21644;UQ&#27169;&#22411;&#32452;&#21512;&#23545;&#20998;&#23376;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#36807;&#25311;&#21512;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10060</link><description>&lt;p&gt;
MUBen&#65306;&#35780;&#20272;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MUBen: Benchmarking the Uncertainty of Pre-Trained Models for Molecular Property Prediction. (arXiv:2306.10060v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10060
&lt;/p&gt;
&lt;p&gt;
MUBen&#35780;&#20272;&#19981;&#21516;&#39592;&#24178;&#21644;UQ&#27169;&#22411;&#32452;&#21512;&#23545;&#20998;&#23376;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#36807;&#25311;&#21512;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#20110;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20998;&#23376;&#25968;&#25454;&#30340;&#22823;&#22411;Transformer&#27169;&#22411;&#22312;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#24494;&#35843;&#26399;&#38388;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#23545;&#27979;&#35797;&#25968;&#25454;&#30340;&#36807;&#24230;&#33258;&#20449;&#39044;&#27979;&#33853;&#22312;&#20102;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#39044;&#27979;&#26657;&#20934;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;UQ&#26041;&#27861;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#26041;&#27861;&#37117;&#20250;&#23548;&#33268;&#24615;&#33021;&#25913;&#21892;&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#20351;&#29992;UQ&#26469;&#25913;&#21892;&#20998;&#23376;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20294;&#36873;&#25321;&#36866;&#21512;&#30340;&#39592;&#24178;&#21644;UQ&#26041;&#27861;&#20197;&#21487;&#38752;&#22320;&#20272;&#35745;&#20998;&#23376;&#19981;&#30830;&#23450;&#24615;&#30340;&#36807;&#31243;&#20173;&#28982;&#26159;&#26410;&#32463;&#25506;&#32034;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MUBen&#65292;&#35780;&#20272;&#19981;&#21516;&#30340;&#39592;&#24178;&#21644;UQ&#27169;&#22411;&#32452;&#21512;&#65292;&#20197;&#37327;&#21270;&#23427;&#20204;&#22312;&#23646;&#24615;&#39044;&#27979;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24494;&#35843;&#20351;&#29992;&#19981;&#21516;&#20998;&#23376;&#25551;&#36848;&#31526;&#30340;&#21508;&#31181;&#39592;&#24178;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Transformer models pre-trained on massive unlabeled molecular data have shown great success in predicting molecular properties. However, these models can be prone to overfitting during fine-tuning, resulting in over-confident predictions on test data that fall outside of the training distribution. To address this issue, uncertainty quantification (UQ) methods can be used to improve the models' calibration of predictions. Although many UQ approaches exist, not all of them lead to improved performance. While some studies have used UQ to improve molecular pre-trained models, the process of selecting suitable backbone and UQ methods for reliable molecular uncertainty estimation remains underexplored. To address this gap, we present MUBen, which evaluates different combinations of backbone and UQ models to quantify their performance for both property prediction and uncertainty estimation. By fine-tuning various backbone molecular representation models using different molecular descrip
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#35774;&#35745;&#20102;&#38750;&#28176;&#36827;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#20004;&#31181;&#20027;&#27969;&#37319;&#26679;&#22120;&#30340;&#26032;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#39640;&#20102;&#24635;&#27493;&#25968;&#19982;&#25910;&#25947;&#36895;&#24230;&#30340;&#27604;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.09251</link><description>&lt;p&gt;
&#38754;&#21521;&#25193;&#25955;&#24335;&#29983;&#25104;&#27169;&#22411;&#30340;&#38750;&#28176;&#36827;&#24555;&#36895;&#25910;&#25947;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models. (arXiv:2306.09251v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09251
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#35774;&#35745;&#20102;&#38750;&#28176;&#36827;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#20004;&#31181;&#20027;&#27969;&#37319;&#26679;&#22120;&#30340;&#26032;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#39640;&#20102;&#24635;&#27493;&#25968;&#19982;&#25910;&#25947;&#36895;&#24230;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21453;&#36716;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#36807;&#31243;&#23558;&#22122;&#38899;&#36716;&#21270;&#20026;&#26032;&#25968;&#25454;&#23454;&#20363;&#65292;&#22312;&#24403;&#20195;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#24050;&#25104;&#20026;&#22522;&#30707;&#12290;&#34429;&#28982;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#29616;&#22312;&#24050;&#34987;&#24191;&#27867;&#35748;&#21487;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#19981;&#22815;&#25104;&#29087;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#38750;&#28176;&#36827;&#29702;&#35770;&#65292;&#20197;&#29702;&#35299;&#31163;&#25955;&#26102;&#38388;&#19979;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#20551;&#35774;&#21487;&#20197;&#33719;&#24471;&#65288;Stein&#65289;&#24471;&#20998;&#20989;&#25968;&#30340;&#21487;&#38752;&#20272;&#35745;&#12290;&#38024;&#23545;&#19968;&#31181;&#27969;&#34892;&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;&#22120;&#65288;&#22522;&#20110;&#27010;&#29575;&#27969;ODE&#65289;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19982; $T$&#65288;&#24635;&#27493;&#25968;&#65289;&#25104;&#27604;&#20363;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25913;&#36827;&#20102;&#36807;&#21435;&#30340;&#32467;&#26524;&#65307;&#23545;&#20110;&#21478;&#19968;&#31181;&#20027;&#27969;&#30340;&#38543;&#26426;&#37319;&#26679;&#22120;&#65288;&#21363;&#19968;&#31181;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#65289;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#20010;&#19982; $1/\sqrt{T}$ &#25104;&#27604;&#20363;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#29702;&#35770;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#23545;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#21482;&#20316;&#20986;&#26368;&#23567;&#30340;&#20551;&#35774;&#65288;&#20363;&#22914;&#65292;&#27809;&#26377;&#24179;&#28369;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling. While their practical power has now been widely recognized, the theoretical underpinnings remain far from mature. In this work, we develop a suite of non-asymptotic theory towards understanding the data generation process of diffusion models in discrete time, assuming access to reliable estimates of the (Stein) score functions. For a popular deterministic sampler (based on the probability flow ODE), we establish a convergence rate proportional to $1/T$ (with $T$ the total number of steps), improving upon past results; for another mainstream stochastic sampler (i.e., a type of the denoising diffusion probabilistic model (DDPM)), we derive a convergence rate proportional to $1/\sqrt{T}$, matching the state-of-the-art theory. Our theory imposes only minimal assumptions on the target data distribution (e.g., no smoot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SGD-like&#31639;&#27861;&#65292;&#27880;&#20837;&#38543;&#26426;&#22122;&#22768;&#24182;&#21033;&#29992;&#20998;&#24067;&#23545;&#31216;&#24615;&#26469;&#20943;&#23569;&#26041;&#24046;&#65292;&#20197;&#23547;&#25214;&#20855;&#26377;&#20302;&#28023;&#26862;&#30697;&#38453;&#36857;&#30340;&#24179;&#22374;&#26497;&#23567;&#20540;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#29575;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.08553</link><description>&lt;p&gt;
&#22122;&#22768;&#31283;&#23450;&#20248;&#21270;&#23545;&#20110;&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#29575;&#30340;&#24179;&#22374;&#26497;&#23567;&#20540;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Noise Stability Optimization for Flat Minima with Optimal Convergence Rates. (arXiv:2306.08553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SGD-like&#31639;&#27861;&#65292;&#27880;&#20837;&#38543;&#26426;&#22122;&#22768;&#24182;&#21033;&#29992;&#20998;&#24067;&#23545;&#31216;&#24615;&#26469;&#20943;&#23569;&#26041;&#24046;&#65292;&#20197;&#23547;&#25214;&#20855;&#26377;&#20302;&#28023;&#26862;&#30697;&#38453;&#36857;&#30340;&#24179;&#22374;&#26497;&#23567;&#20540;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#29575;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#21152;&#20837;&#21152;&#26435;&#25200;&#21160;&#26469;&#25214;&#21040;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#12290;&#32473;&#23450;&#19968;&#20010;&#38750;&#20984;&#20989;&#25968;$f:\mathbb{R}^d\rightarrow \mathbb{R}$&#21644;&#19968;&#20010;$d$&#32500;&#20998;&#24067;$\mathcal{P}$&#65292;&#25105;&#20204;&#25200;&#21160;$f$&#30340;&#26435;&#37325;&#65292;&#24182;&#23450;&#20041;$F(W)=\mathbb{E}[f({W+U})]$&#65292;&#20854;&#20013;$U$&#26159;&#19968;&#20010;&#20174;$\mathcal{P}$&#20013;&#38543;&#26426;&#25277;&#21462;&#30340;&#26679;&#26412;&#12290;&#36825;&#20010;&#36807;&#31243;&#36890;&#36807;$f$&#30340;&#28023;&#26862;&#30697;&#38453;&#30340;&#36857;&#26469;&#35825;&#23548;&#27491;&#21017;&#21270;&#65292;&#20197;&#36866;&#24212;&#20110;&#23567;&#30340;&#12289;&#21508;&#21521;&#21516;&#24615;&#30340;&#39640;&#26031;&#25200;&#21160;&#12290;&#22240;&#27492;&#65292;&#21152;&#26435;&#25200;&#21160;&#30340;&#20989;&#25968;&#20559;&#21521;&#20110;&#24102;&#26377;&#20302;&#28023;&#26862;&#30697;&#38453;&#36857;&#30340;&#26497;&#23567;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;SGD&#30340;&#31639;&#27861;&#65292;&#22312;&#35745;&#31639;&#26799;&#24230;&#20043;&#21069;&#27880;&#20837;&#38543;&#26426;&#22122;&#22768;&#65292;&#21516;&#26102;&#21033;&#29992;$\mathcal{P}$&#30340;&#23545;&#31216;&#24615;&#26469;&#20943;&#23569;&#26041;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We consider finding flat, local minimizers by adding average weight perturbations. Given a nonconvex function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ and a $d$-dimensional distribution $\mathcal{P}$ which is symmetric at zero, we perturb the weight of $f$ and define $F(W) = \mathbb{E}[f({W + U})]$, where $U$ is a random sample from $\mathcal{P}$. This injection induces regularization through the Hessian trace of $f$ for small, isotropic Gaussian perturbations. Thus, the weight-perturbed function biases to minimizers with low Hessian trace. Several prior works have studied settings related to this weight-perturbed function by designing algorithms to improve generalization. Still, convergence rates are not known for finding minima under the average perturbations of the function $F$. This paper considers an SGD-like algorithm that injects random noise before computing gradients while leveraging the symmetry of $\mathcal{P}$ to reduce variance. We then provide a rigorous analysis, showing
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MMASD&#30340;&#33258;&#38381;&#30151;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#33258;&#27835;&#30103;&#24178;&#39044;&#12290;&#23427;&#21253;&#25324;&#20174;32&#21517;&#33258;&#38381;&#30151;&#24739;&#20799;&#30340;&#24178;&#39044;&#24405;&#38899;&#20013;&#20998;&#27573;&#30340;1,315&#20010;&#25968;&#25454;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#21253;&#21547;&#22235;&#31181;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.08243</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#38381;&#30151;&#24178;&#39044;&#20998;&#26512;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;MMASD&#12290;
&lt;/p&gt;
&lt;p&gt;
MMASD: A Multimodal Dataset for Autism Intervention Analysis. (arXiv:2306.08243v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08243
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MMASD&#30340;&#33258;&#38381;&#30151;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#33258;&#27835;&#30103;&#24178;&#39044;&#12290;&#23427;&#21253;&#25324;&#20174;32&#21517;&#33258;&#38381;&#30151;&#24739;&#20799;&#30340;&#24178;&#39044;&#24405;&#38899;&#20013;&#20998;&#27573;&#30340;1,315&#20010;&#25968;&#25454;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#21253;&#21547;&#22235;&#31181;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#26159;&#19968;&#31181;&#21457;&#32946;&#24615;&#30142;&#30149;&#65292;&#20854;&#29305;&#24449;&#26159;&#37325;&#22823;&#30340;&#31038;&#20132;&#27807;&#36890;&#38556;&#30861;&#21644;&#22256;&#38590;&#30340;&#30693;&#35273;&#21644;&#34920;&#36798;&#27807;&#36890;&#25552;&#31034;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#20419;&#36827;&#33258;&#38381;&#30151;&#30740;&#31350;&#21644;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#20110;&#29305;&#23450;&#20998;&#26512;&#65292;&#24182;&#22312;&#33258;&#38381;&#30151;&#31038;&#21306;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#36825;&#38480;&#21046;&#20102;&#30001;&#20110;&#25968;&#25454;&#20849;&#20139;&#22797;&#26434;&#24615;&#32780;&#36328;&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#24320;&#28304;&#25968;&#25454;&#38598;MMASD&#20316;&#20026;&#22810;&#27169;&#24335;ASD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#33258;&#24739;&#26377;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#28216;&#25103;&#27835;&#30103;&#24178;&#39044;&#12290;MMASD&#21253;&#25324;32&#21517;&#24739;&#26377;ASD&#30340;&#20799;&#31461;&#30340;&#25968;&#25454;&#65292;&#20197;&#21450;&#20174;100&#22810;&#23567;&#26102;&#30340;&#24178;&#39044;&#24405;&#38899;&#20013;&#20998;&#27573;&#30340;1,315&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#20026;&#20419;&#36827;&#20844;&#20849;&#35775;&#38382;&#65292;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#21253;&#21547;&#22235;&#31181;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#30340;&#25968;&#25454;&#65306;&#65288;1&#65289;&#20809;&#27969;&#65292;&#65288;2&#65289;2D&#39592;&#26550;&#65292;&#65288;3&#65289;3D&#39592;&#26550;&#21644;&#65288;4&#65289;&#20020;&#24202;&#21307;&#29983;ASD&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autism spectrum disorder (ASD) is a developmental disorder characterized by significant social communication impairments and difficulties perceiving and presenting communication cues. Machine learning techniques have been broadly adopted to facilitate autism studies and assessments. However, computational models are primarily concentrated on specific analysis and validated on private datasets in the autism community, which limits comparisons across models due to privacy-preserving data sharing complications. This work presents a novel privacy-preserving open-source dataset, MMASD as a MultiModal ASD benchmark dataset, collected from play therapy interventions of children with Autism. MMASD includes data from 32 children with ASD, and 1,315 data samples segmented from over 100 hours of intervention recordings. To promote public access, each data sample consists of four privacy-preserving modalities of data: (1) optical flow, (2) 2D skeleton, (3) 3D skeleton, and (4) clinician ASD evalua
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.06599</link><description>&lt;p&gt;
&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;(Variational Imbalanced Regression)
&lt;/p&gt;
&lt;p&gt;
Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#26102;&#65292;&#29616;&#26377;&#30340;&#22238;&#24402;&#27169;&#22411;&#24448;&#24448;&#22312;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#65292;&#23427;&#19981;&#20165;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#33258;&#28982;&#22320;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#19982;&#20856;&#22411;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20551;&#35774;I.I.D.&#34920;&#31034;&#65288;&#25968;&#25454;&#28857;&#30340;&#34920;&#31034;&#19981;&#30452;&#25509;&#21463;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;VIR&#20511;&#29992;&#20855;&#26377;&#31867;&#20284;&#22238;&#24402;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#35745;&#31639;&#28508;&#22312;&#34920;&#31034;&#30340;&#21464;&#20998;&#20998;&#24067;&#65307;&#27492;&#22806;&#65292;&#19981;&#21516;&#20110;&#20135;&#29983;&#28857;&#20272;&#35745;&#30340;&#30830;&#23450;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292; VIR&#39044;&#27979;&#25972;&#20010;&#27491;&#24577;&#21453;-&#20285;&#29595;&#20998;&#24067;&#24182;&#35843;&#33410;&#30456;&#20851;&#32852;&#30340;&#20849;&#36717;&#20998;&#24067;&#65292;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#26045;&#21152;&#27010;&#29575;&#37325;&#26032;&#21152;&#26435;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
&lt;/p&gt;</description></item><item><title>Push&#26159;&#19968;&#20010;&#24182;&#21457;&#27010;&#29575;&#32534;&#31243;&#24211;&#65292;&#29992;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#65292;&#21487;&#20197;&#22312;&#22810;GPU&#30828;&#20214;&#19978;&#25191;&#34892;BDL&#25512;&#29702;&#31639;&#27861;&#12290;&#35813;&#24211;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#31890;&#23376;&#65292;&#24182;&#20801;&#35768;&#31890;&#23376;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#21644;&#21508;&#31181;&#21442;&#25968;&#26356;&#26032;&#65292;&#31616;&#21270;&#20102;BDL&#23454;&#39564;&#21644;&#25193;&#23637;&#31890;&#23376;&#25805;&#20316;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.06528</link><description>&lt;p&gt;
Push: &#24182;&#21457;&#27010;&#29575;&#32534;&#31243;&#29992;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Push: Concurrent Probabilistic Programming for Bayesian Deep Learning. (arXiv:2306.06528v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06528
&lt;/p&gt;
&lt;p&gt;
Push&#26159;&#19968;&#20010;&#24182;&#21457;&#27010;&#29575;&#32534;&#31243;&#24211;&#65292;&#29992;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#65292;&#21487;&#20197;&#22312;&#22810;GPU&#30828;&#20214;&#19978;&#25191;&#34892;BDL&#25512;&#29702;&#31639;&#27861;&#12290;&#35813;&#24211;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#31890;&#23376;&#65292;&#24182;&#20801;&#35768;&#31890;&#23376;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#21644;&#21508;&#31181;&#21442;&#25968;&#26356;&#26032;&#65292;&#31616;&#21270;&#20102;BDL&#23454;&#39564;&#21644;&#25193;&#23637;&#31890;&#23376;&#25805;&#20316;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Push&#30340;&#24211;&#65292;&#37319;&#29992;&#27010;&#29575;&#32534;&#31243;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#12290;&#35813;&#24211;&#21487;&#22312;&#22810;GPU&#30828;&#20214;&#19978;&#24182;&#21457;&#25191;&#34892;BDL&#25512;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;Push&#24341;&#20837;&#20102;&#19968;&#31181;&#25277;&#35937;&#65292;&#23558;&#36755;&#20837;NN&#34920;&#31034;&#20026;&#19968;&#20010;&#31890;&#23376;&#12290;Push&#20351;&#24471;&#21019;&#24314;&#31890;&#23376;&#21464;&#24471;&#23481;&#26131;&#65292;&#20197;&#20415;&#20110;&#22797;&#21046;&#21644;&#31890;&#23376;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#20197;&#23454;&#29616;&#21508;&#31181;&#21442;&#25968;&#26356;&#26032;&#65292;&#21253;&#25324;&#24120;&#35265;&#30340;BDL&#31639;&#27861;&#12290;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;Push&#38477;&#20302;&#36827;&#34892;BDL&#23454;&#39564;&#30340;&#38376;&#27099;&#65292;&#36890;&#36807;&#31616;&#21270;&#22312;&#22810;GPU&#19978;&#25193;&#23637;&#31890;&#23376;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21033;&#29992;&#21333;&#33410;&#28857;&#22810;GPU&#35774;&#22791;&#36827;&#34892;&#35270;&#35273;&#21644;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#20219;&#21153;&#26102;&#30340;&#31890;&#23376;&#25193;&#23637;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a library called Push that takes a probabilistic programming approach to Bayesian deep learning (BDL). This library enables concurrent execution of BDL inference algorithms on multi-GPU hardware for neural network (NN) models. To accomplish this, Push introduces an abstraction that represents an input NN as a particle. Push enables easy creation of particles so that an input NN can be replicated and particles can communicate asynchronously so that a variety of parameter updates can be expressed, including common BDL algorithms. Our hope is that Push lowers the barrier to experimenting with BDL by streamlining the scaling of particles across GPUs. We evaluate the scaling behavior of particles on single-node multi-GPU devices on vision and scientific machine learning (SciML) tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#65292;&#36890;&#36807;&#26799;&#24230;&#27969;&#35757;&#32451;&#19968;&#20010;&#23485;&#24230;&#20219;&#24847;&#30340;&#19968;&#23618;ReLU&#32593;&#32476;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#21333;&#20010;&#31070;&#32463;&#20803;&#24182;&#25910;&#25947;&#21040;&#38646;&#35823;&#24046;&#65292;&#21516;&#26102;&#38544;&#24335;&#20559;&#21521;&#20110;&#26368;&#23567;&#21270;&#32593;&#32476;&#21442;&#25968;&#30340;&#31209;&#12290;&#36825;&#23545;&#20110;&#30456;&#20851;&#30340;&#35757;&#32451;&#25968;&#25454;&#28857;&#26159;&#26377;&#25928;&#30340;&#65292;&#19982;&#20043;&#21069;&#30740;&#31350;&#27491;&#20132;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34917;&#20805;&#20102;&#24444;&#27492;&#12290;</title><link>http://arxiv.org/abs/2306.06479</link><description>&lt;p&gt;
&#36890;&#36807;&#27973;&#23618;ReLU&#32593;&#32476;&#23398;&#20064;&#31070;&#32463;&#20803;&#65306;&#23545;&#30456;&#20851;&#36755;&#20837;&#30340;&#21160;&#24577;&#21644;&#38544;&#24335;&#20559;&#24046;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Learning a Neuron by a Shallow ReLU Network: Dynamics and Implicit Bias for Correlated Inputs. (arXiv:2306.06479v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06479
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#65292;&#36890;&#36807;&#26799;&#24230;&#27969;&#35757;&#32451;&#19968;&#20010;&#23485;&#24230;&#20219;&#24847;&#30340;&#19968;&#23618;ReLU&#32593;&#32476;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#21333;&#20010;&#31070;&#32463;&#20803;&#24182;&#25910;&#25947;&#21040;&#38646;&#35823;&#24046;&#65292;&#21516;&#26102;&#38544;&#24335;&#20559;&#21521;&#20110;&#26368;&#23567;&#21270;&#32593;&#32476;&#21442;&#25968;&#30340;&#31209;&#12290;&#36825;&#23545;&#20110;&#30456;&#20851;&#30340;&#35757;&#32451;&#25968;&#25454;&#28857;&#26159;&#26377;&#25928;&#30340;&#65292;&#19982;&#20043;&#21069;&#30740;&#31350;&#27491;&#20132;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34917;&#20805;&#20102;&#24444;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#23398;&#20064;&#21333;&#20010;&#31070;&#32463;&#20803;&#30340;&#22522;&#26412;&#22238;&#24402;&#20219;&#21153;&#65292;&#36890;&#36807;&#20174;&#23567;&#30340;&#21021;&#22987;&#20540;&#26799;&#24230;&#27969;&#35757;&#32451;&#20219;&#24847;&#23485;&#24230;&#30340;&#19968;&#23618;&#38544;&#34255;&#23618;ReLU&#32593;&#32476;&#20250;&#25910;&#25947;&#21040;&#38646;&#35823;&#24046;&#65292;&#24182;&#19988;&#22312;&#38544;&#24335;&#19978;&#20559;&#21521;&#20110;&#26368;&#23567;&#21270;&#32593;&#32476;&#21442;&#25968;&#30340;&#31209;&#12290;&#20551;&#35774;&#35757;&#32451;&#28857;&#19982;&#25945;&#24072;&#31070;&#32463;&#20803;&#30456;&#20851;&#65292;&#25105;&#20204;&#34917;&#20805;&#20102;&#20808;&#21069;&#32771;&#34385;&#27491;&#20132;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22522;&#20110;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#27599;&#20010;&#38544;&#34255;&#31070;&#32463;&#20803;&#21160;&#24577;&#30340;&#35814;&#32454;&#38750;&#28176;&#36827;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#24182;&#34920;&#24449;&#20102;&#36825;&#31181;&#24773;&#20917;&#19979;&#26368;&#23567;&#31209;&#20869;&#25554;&#32593;&#32476;&#19982;&#26368;&#23567;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#21306;&#21035;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove that, for the fundamental regression task of learning a single neuron, training a one-hidden layer ReLU network of any width by gradient flow from a small initialisation converges to zero loss and is implicitly biased to minimise the rank of network parameters. By assuming that the training points are correlated with the teacher neuron, we complement previous work that considered orthogonal datasets. Our results are based on a detailed non-asymptotic analysis of the dynamics of each hidden neuron throughout the training. We also show and characterise a surprising distinction in this setting between interpolator networks of minimal rank and those of minimal Euclidean norm. Finally we perform a range of numerical experiments, which corroborate our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#32593;&#32476;&#26469;&#37325;&#24314;&#38050;&#29748;&#28436;&#22863;&#20013;&#20154;&#31867;&#34920;&#29616;&#21147;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36716;&#24405;&#20048;&#35889;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25972;&#21512;&#38050;&#29748;&#23478;&#36523;&#20221;&#20197;&#25511;&#21046;&#37319;&#26679;&#36807;&#31243;&#65292;&#24182;&#25104;&#21151;&#29983;&#25104;&#20102;&#39640;&#24230;&#31867;&#20154;&#30340;&#38050;&#29748;&#34920;&#28436;&#12290;</title><link>http://arxiv.org/abs/2306.06040</link><description>&lt;p&gt;
&#21033;&#29992;Transformer&#32593;&#32476;&#37325;&#24314;&#38050;&#29748;&#28436;&#22863;&#20013;&#20154;&#31867;&#34920;&#29616;&#21147;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Human Expressiveness in Piano Performances with a Transformer Network. (arXiv:2306.06040v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#32593;&#32476;&#26469;&#37325;&#24314;&#38050;&#29748;&#28436;&#22863;&#20013;&#20154;&#31867;&#34920;&#29616;&#21147;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36716;&#24405;&#20048;&#35889;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25972;&#21512;&#38050;&#29748;&#23478;&#36523;&#20221;&#20197;&#25511;&#21046;&#37319;&#26679;&#36807;&#31243;&#65292;&#24182;&#25104;&#21151;&#29983;&#25104;&#20102;&#39640;&#24230;&#31867;&#20154;&#30340;&#38050;&#29748;&#34920;&#28436;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#35745;&#31639;&#26041;&#27861;&#25429;&#25417;&#20154;&#31867;&#38899;&#20048;&#28436;&#22863;&#20013;&#22797;&#26434;&#24494;&#22937;&#30340;&#34920;&#29616;&#21147;&#21464;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#23618;&#21452;&#21521;Transformer&#32534;&#30721;&#22120;&#37325;&#24314;&#38050;&#29748;&#28436;&#22863;&#20013;&#30340;&#20154;&#31867;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22823;&#37327;&#20934;&#30830;&#25429;&#25417;&#21644;&#24471;&#20998;&#23545;&#40784;&#30340;&#28436;&#22863;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#29616;&#26377;&#36716;&#24405;&#27169;&#22411;&#33719;&#21462;&#30340;&#36716;&#24405;&#20048;&#35889;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#38050;&#29748;&#23478;&#36523;&#20221;&#20197;&#25511;&#21046;&#37319;&#26679;&#36807;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#27169;&#25311;&#19981;&#21516;&#38050;&#29748;&#23478;&#34920;&#29616;&#21147;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#29983;&#25104;&#30340;&#34920;&#29616;&#21147;&#28436;&#22863;&#30340;&#32479;&#35745;&#20998;&#26512;&#21644;&#21548;&#21147;&#27979;&#35797;&#23545;&#31995;&#32479;&#36827;&#34892;&#35780;&#20272;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20174;&#36716;&#24405;&#30340;&#20048;&#35889;&#20013;&#29983;&#25104;&#31867;&#20154;&#38050;&#29748;&#28436;&#22863;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#21516;&#26102;&#20805;&#20998;&#21644;&#19968;&#33268;&#22320;&#37325;&#24314;&#20102;&#20154;&#31867;&#34920;&#29616;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capturing intricate and subtle variations in human expressiveness in music performance using computational approaches is challenging. In this paper, we propose a novel approach for reconstructing human expressiveness in piano performance with a multi-layer bi-directional Transformer encoder. To address the needs for large amounts of accurately captured and score-aligned performance data in training neural networks, we use transcribed scores obtained from an existing transcription model to train our model. We integrate pianist identities to control the sampling process and explore the ability of our system to model variations in expressiveness for different pianists. The system is evaluated through statistical analysis of generated expressive performances and a listening test. Overall, the results suggest that our method achieves state-of-the-art in generating human-like piano performances from transcribed scores, while fully and consistently reconstructing human expressiveness poses fu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#25551;&#36848;&#20013;&#24341;&#20837;&#25200;&#21160;&#26469;&#22686;&#24378;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#26377;&#25928;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#22120;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05079</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25552;&#21319;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robustness of AI Offensive Code Generators via Data Augmentation. (arXiv:2306.05079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#25551;&#36848;&#20013;&#24341;&#20837;&#25200;&#21160;&#26469;&#22686;&#24378;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#26377;&#25928;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#22120;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25200;&#21160;&#28155;&#21152;&#21040;&#23433;&#20840;&#24615;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#30340;&#20195;&#30721;&#25551;&#36848;&#20013;&#30340;&#26041;&#27861;&#65292;&#21363;&#26469;&#33258;&#21892;&#24847;&#24320;&#21457;&#32773;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#65288;NL&#65289;&#65292;&#24182;&#20998;&#26512;&#20102;&#25200;&#21160;&#22914;&#20309;&#20197;&#21450;&#22312;&#20160;&#20040;&#31243;&#24230;&#19978;&#24433;&#21709;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NL&#25551;&#36848;&#20013;&#30340;&#25200;&#21160;&#39640;&#24230;&#24433;&#21709;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26041;&#27861;&#25191;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#21363;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a method to add perturbations to the code descriptions, i.e., new inputs in natural language (NL) from well-intentioned developers, in the context of security-oriented code, and analyze how and to what extent perturbations affect the performance of AI offensive code generators. Our experiments show that the performance of the code generators is highly affected by perturbations in the NL descriptions. To enhance the robustness of the code generators, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#30340;&#20934;&#30830;&#12289;&#36731;&#37327;&#32423;&#12289;&#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#8212;&#8212;SeaLog&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#31181;&#22522;&#20110; Trie &#32467;&#26500;&#30340;&#21160;&#24577;&#22686;&#38271;&#26816;&#27979;&#20195;&#29702;&#65292;&#21487;&#20197;&#25509;&#25910;&#20154;&#31867;&#19987;&#23478;&#21453;&#39304;&#65292;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#26085;&#24535;&#25968;&#25454;&#20013;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#25163;&#21160;&#39564;&#35777;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.05032</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#24102;&#26377;&#19987;&#23478;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Scalable and Adaptive Log-based Anomaly Detection with Expert in the Loop. (arXiv:2306.05032v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#30340;&#20934;&#30830;&#12289;&#36731;&#37327;&#32423;&#12289;&#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#8212;&#8212;SeaLog&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#31181;&#22522;&#20110; Trie &#32467;&#26500;&#30340;&#21160;&#24577;&#22686;&#38271;&#26816;&#27979;&#20195;&#29702;&#65292;&#21487;&#20197;&#25509;&#25910;&#20154;&#31867;&#19987;&#23478;&#21453;&#39304;&#65292;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#26085;&#24535;&#25968;&#25454;&#20013;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#25163;&#21160;&#39564;&#35777;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#26085;&#24535;&#22312;&#32500;&#25252;&#36719;&#20214;&#31995;&#32479;&#21487;&#38752;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#24050;&#26377;&#30740;&#31350;&#25506;&#32034;&#20102;&#33258;&#21160;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#26102;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30001;&#20110;&#39640;&#36164;&#28304;&#28040;&#32791;&#21644;&#32570;&#20047;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#26085;&#24535;&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#32780;&#38754;&#20020;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#65292;&#36731;&#37327;&#32423;&#21644;&#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#8212;&#8212;SeaLog&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110; Trie &#30340;&#26816;&#27979;&#20195;&#29702; (TDA)&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#21160;&#24577;&#22686;&#38271;&#30340; Trie &#32467;&#26500;&#36827;&#34892;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;&#12290;&#20026;&#20102;&#22686;&#24378; TDA &#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#26085;&#24535;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#20351;&#20854;&#33021;&#22815;&#20174;&#19987;&#23478;&#33719;&#24471;&#21453;&#39304;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#20195;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914; ChatGPT&#65292;&#21487;&#20197;&#25552;&#20379;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#24403;&#19968;&#33268;&#24615;&#30340;&#21453;&#39304;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#20943;&#23569;&#25163;&#21160;&#39564;&#35777;&#30340;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#22823;&#35268;&#27169;&#29615;&#22659;&#19979;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
System logs play a critical role in maintaining the reliability of software systems. Fruitful studies have explored automatic log-based anomaly detection and achieved notable accuracy on benchmark datasets. However, when applied to large-scale cloud systems, these solutions face limitations due to high resource consumption and lack of adaptability to evolving logs. In this paper, we present an accurate, lightweight, and adaptive log-based anomaly detection framework, referred to as SeaLog. Our method introduces a Trie-based Detection Agent (TDA) that employs a lightweight, dynamically-growing trie structure for real-time anomaly detection. To enhance TDA's accuracy in response to evolving log data, we enable it to receive feedback from experts. Interestingly, our findings suggest that contemporary large language models, such as ChatGPT, can provide feedback with a level of consistency comparable to human experts, which can potentially reduce manual verification efforts. We extensively 
&lt;/p&gt;</description></item><item><title>GPT-FL&#26159;&#19968;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#24182;&#32467;&#21512;&#31169;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#23458;&#25143;&#31471;&#37319;&#26679;&#25928;&#29575;&#31561;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#22312;FL&#35757;&#32451;&#20013;&#65292;&#30001;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#19979;&#28216;&#27169;&#22411;&#23545;&#20110;&#25511;&#21046;&#26799;&#24230;&#22810;&#26679;&#24615;&#30340;&#26041;&#21521;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02210</link><description>&lt;p&gt;
GPT-FL: &#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GPT-FL: Generative Pre-trained Model-Assisted Federated Learning. (arXiv:2306.02210v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02210
&lt;/p&gt;
&lt;p&gt;
GPT-FL&#26159;&#19968;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#24182;&#32467;&#21512;&#31169;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#23458;&#25143;&#31471;&#37319;&#26679;&#25928;&#29575;&#31561;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#22312;FL&#35757;&#32451;&#20013;&#65292;&#30001;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#19979;&#28216;&#27169;&#22411;&#23545;&#20110;&#25511;&#21046;&#26799;&#24230;&#22810;&#26679;&#24615;&#30340;&#26041;&#21521;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-FL&#65292;&#19968;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26694;&#26550;&#12290;GPT-FL&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#36825;&#20123;&#29983;&#25104;&#30340;&#25968;&#25454;&#29992;&#20110;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#19979;&#28216;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#26631;&#20934;FL&#26694;&#26550;&#19979;&#20351;&#29992;&#31169;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-FL&#22312;&#27169;&#22411;&#27979;&#35797;&#20934;&#30830;&#24615;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#23458;&#25143;&#31471;&#37319;&#26679;&#25928;&#29575;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;FL&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#28040;&#34701;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;FL&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#30001;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#19979;&#28216;&#27169;&#22411;&#23545;&#20110;&#25511;&#21046;&#26799;&#24230;&#22810;&#26679;&#24615;&#30340;&#26041;&#21521;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36825;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23545;&#35266;&#23519;&#21040;&#30340;GPT-FL&#30340;&#26174;&#33879;&#20934;&#30830;&#24615;&#25552;&#21319;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#30446;&#26631;&#25968;&#25454;&#26159;&#21542;&#22312;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#39046;&#22495;&#20869;&#25110;&#22806;&#65292;GPT-FL&#22987;&#32456;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose GPT-FL, a generative pre-trained model-assisted federated learning (FL) framework. At its core, GPT-FL leverages generative pre-trained models to generate diversified synthetic data. These generated data are used to train a downstream model on the server, which is then fine-tuned with private client data under the standard FL framework. We show that GPT-FL consistently outperforms state-of-the-art FL methods in terms of model test accuracy, communication efficiency, and client sampling efficiency. Through comprehensive ablation analysis, we discover that the downstream model generated by synthetic data plays a crucial role in controlling the direction of gradient diversity during FL training, which enhances convergence speed and contributes to the notable accuracy boost observed with GPT-FL. Also, regardless of whether the target data falls within or outside the domain of the pre-trained generative model, GPT-FL consistently achieves significant performance gai
&lt;/p&gt;</description></item><item><title>PAGAR&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;IRL-based IL&#20013;&#22870;&#21169;&#19981;&#23545;&#40784;&#38382;&#39064;&#30340;&#21322;&#30417;&#30563;&#22870;&#21169;&#35774;&#35745;&#26041;&#27861;&#65292;&#22312;&#22797;&#26434;IL&#20219;&#21153;&#21644;&#38646;-shot IL&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2306.01731</link><description>&lt;p&gt;
PAGAR: &#29992;&#20027;&#35282;-&#21453;&#27966;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;&#39535;&#26381;&#36870;&#24378;&#21270;&#23398;&#20064;&#22312;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#19981;&#23545;&#40784;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
PAGAR: Taming Reward Misalignment in Inverse Reinforcement Learning-Based Imitation Learning with Protagonist Antagonist Guided Adversarial Reward. (arXiv:2306.01731v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01731
&lt;/p&gt;
&lt;p&gt;
PAGAR&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;IRL-based IL&#20013;&#22870;&#21169;&#19981;&#23545;&#40784;&#38382;&#39064;&#30340;&#21322;&#30417;&#30563;&#22870;&#21169;&#35774;&#35745;&#26041;&#27861;&#65292;&#22312;&#22797;&#26434;IL&#20219;&#21153;&#21644;&#38646;-shot IL&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#27169;&#20223;&#23398;&#20064;(imitation learning, IL)&#31639;&#27861;&#20351;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;(inverse reinforcement learning, IRL)&#26469;&#25512;&#26029;&#19987;&#23478;&#20197;&#38544;&#24335;&#26041;&#24335;&#20248;&#21270;&#30340;&#28508;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#22522;&#20110;&#20854;&#23637;&#31034;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#25512;&#26029;&#30340;&#22870;&#21169;&#19982;&#30495;&#23454;&#20219;&#21153;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#21487;&#33021;&#23548;&#33268;&#20219;&#21153;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20027;&#35282;-&#21453;&#27966;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;(PAGAR)&#65292;&#36825;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#22870;&#21169;&#35774;&#35745;&#33539;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;IRL-based IL&#20013;&#30340;&#22870;&#21169;&#19981;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20505;&#36873;&#22870;&#21169;&#20989;&#25968;&#28385;&#36275;&#30340;&#26465;&#20214;&#65292;PAGAR&#33021;&#22815;&#20445;&#35777;&#20135;&#29983;&#19968;&#20010;&#22312;&#24213;&#23618;&#20219;&#21153;&#20013;&#25104;&#21151;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22312;&#31574;&#30053;&#21644;&#31163;&#31574;&#30053;&#26041;&#27861;&#26469;&#22312;IRL-based IL&#20013;&#23454;&#26045;PAGAR&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22797;&#26434;&#30340;IL&#20219;&#21153;&#21644;&#26377;&#38480;&#28436;&#31034;&#30340;&#36801;&#31227;&#29615;&#22659;&#30340;&#38646;-shot IL&#20219;&#21153;&#19978;&#20248;&#20110;&#31454;&#20105;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many imitation learning (IL) algorithms employ inverse reinforcement learning (IRL) to infer the underlying reward function that an expert is implicitly optimizing for, based on their demonstrated behaviors. However, a misalignment between the inferred reward and the true task objective can result in task failures. In this paper, we introduce Protagonist Antagonist Guided Adversarial Reward (PAGAR), a semi-supervised reward design paradigm to tackle this reward misalignment problem in IRL-based IL. We identify the conditions on the candidate reward functions under which PAGAR can guarantee to induce a policy that succeeds in the underlying task. Furthermore, we present a practical on-and-off policy approach to implement PAGAR in IRL-based IL. Experimental results show that our algorithm outperforms competitive baselines on complex IL tasks and zero-shot IL tasks in transfer environments with limited demonstrations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01631</link><description>&lt;p&gt;
Gode -- &#23558;&#29983;&#29289;&#21270;&#23398;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#21040;&#20998;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#20013;
&lt;/p&gt;
&lt;p&gt;
Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#30340;&#20934;&#30830;&#39044;&#27979;&#23545;&#20110;&#20419;&#36827;&#21019;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#21457;&#23637;&#21644;&#29702;&#35299;&#21270;&#23398;&#29289;&#36136;&#21644;&#29983;&#29289;&#31995;&#32479;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#20010;&#20998;&#23376;&#32467;&#26500;&#30340;&#22270;&#34920;&#31034;&#19982;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889; (KG) &#30340;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#36827;&#34892;&#38598;&#25104;&#12290;&#36890;&#36807;&#38598;&#25104;&#20004;&#20010;&#32423;&#21035;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#20998;&#23376;&#32423;&#21644; KG &#32423;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#24615;&#33021;&#35780;&#20272;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312; 11 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#24494;&#35843;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26410;&#30693;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#30340;&#20882;&#21517;&#32773;&#35782;&#21035;&#12290;&#36890;&#36807;&#24341;&#20837;&#24378;&#20581;&#30340;&#35828;&#35805;&#20154;&#29305;&#23450;&#38408;&#20540;&#25216;&#26415;&#21644;&#31471;&#21040;&#31471;&#20803;&#23398;&#20064;&#65292;&#23558;&#20882;&#21517;&#32773;&#26816;&#27979;&#20174;&#26410;&#30693;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#20998;&#31163;&#20986;&#26469;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#27880;&#20876;&#35828;&#35805;&#20154;&#30340;&#35805;&#35821;&#26469;&#23398;&#20064;&#26816;&#27979;&#20882;&#21517;&#32773;&#65292;&#30456;&#27604;&#22522;&#32447;&#25552;&#39640;&#20102;10%&#12290;</title><link>http://arxiv.org/abs/2306.00952</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#26410;&#30693;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#30340;&#31471;&#21040;&#31471;&#20882;&#21517;&#32773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Framework for End-to-End Imposter Identification in Unseen Speaker Recognition. (arXiv:2306.00952v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26410;&#30693;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#30340;&#20882;&#21517;&#32773;&#35782;&#21035;&#12290;&#36890;&#36807;&#24341;&#20837;&#24378;&#20581;&#30340;&#35828;&#35805;&#20154;&#29305;&#23450;&#38408;&#20540;&#25216;&#26415;&#21644;&#31471;&#21040;&#31471;&#20803;&#23398;&#20064;&#65292;&#23558;&#20882;&#21517;&#32773;&#26816;&#27979;&#20174;&#26410;&#30693;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#20998;&#31163;&#20986;&#26469;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#27880;&#20876;&#35828;&#35805;&#20154;&#30340;&#35805;&#35821;&#26469;&#23398;&#20064;&#26816;&#27979;&#20882;&#21517;&#32773;&#65292;&#30456;&#27604;&#22522;&#32447;&#25552;&#39640;&#20102;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#34987;&#37096;&#32626;&#22312;&#22810;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#24448;&#24448;&#19982;&#23427;&#20204;&#25152;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#23454;&#39564;&#23460;&#26465;&#20214;&#19981;&#21516;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#20351;&#29992;&#22266;&#23450;&#38408;&#20540;&#65288;&#20351;&#29992;EER&#24230;&#37327;&#35745;&#31639;&#65289;&#36827;&#34892;&#26410;&#30693;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#30340;&#20882;&#21517;&#32773;&#35782;&#21035;&#26102;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#24378;&#20581;&#30340;&#35828;&#35805;&#20154;&#29305;&#23450;&#38408;&#20540;&#25216;&#26415;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#21463;&#26368;&#36817;&#22312;&#35828;&#35805;&#20154;&#39564;&#35777;&#20013;&#20351;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20882;&#21517;&#32773;&#26816;&#27979;&#30340;&#31471;&#21040;&#31471;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#20882;&#21517;&#32773;&#26816;&#27979;&#38382;&#39064;&#20174;&#26410;&#30693;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#20998;&#31163;&#20986;&#26469;&#12290;&#22240;&#27492;&#65292;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#19968;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#26816;&#27979;&#20882;&#21517;&#32773;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#27880;&#20876;&#35828;&#35805;&#20154;&#30340;&#35805;&#35821;&#26469;&#23398;&#20064;&#26816;&#27979;&#20882;&#21517;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;VoxCeleb1&#12289;VCTK&#21644;FFSVC 2022&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#27604;&#22522;&#32447;&#25552;&#39640;&#20102;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
Speaker identification systems are deployed in diverse environments, often different from the lab conditions on which they are trained and tested. In this paper, first, we show the problem of generalization using fixed thresholds (computed using EER metric) for imposter identification in unseen speaker recognition and then introduce a robust speaker-specific thresholding technique for better performance. Secondly, inspired by the recent use of meta-learning techniques in speaker verification, we propose an end-to-end meta-learning framework for imposter detection which decouples the problem of imposter detection from unseen speaker identification. Thus, unlike most prior works that use some heuristics to detect imposters, the proposed network learns to detect imposters by leveraging the utterances of the enrolled speakers. Furthermore, we show the efficacy of the proposed techniques on VoxCeleb1, VCTK and the FFSVC 2022 datasets, beating the baselines by up to 10%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;RKHS&#36924;&#36817;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#22909;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#23545;&#20110;&#20219;&#24847;&#32534;&#30721;&#22120;&#65292;&#22686;&#24191;&#20989;&#25968;&#36136;&#37327;&#30340;&#25552;&#21319;&#21487;&#20197;&#25552;&#39640;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#25209;&#37327;&#24402;&#19968;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.00788</link><description>&lt;p&gt;
&#36890;&#36807;RKHS&#36924;&#36817;&#29702;&#35299;&#22522;&#20110;&#22686;&#24191;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation. (arXiv:2306.00788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;RKHS&#36924;&#36817;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#22909;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#23545;&#20110;&#20219;&#24847;&#32534;&#30721;&#22120;&#65292;&#22686;&#24191;&#20989;&#25968;&#36136;&#37327;&#30340;&#25552;&#21319;&#21487;&#20197;&#25552;&#39640;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#25209;&#37327;&#24402;&#19968;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22909;&#30340;&#25968;&#25454;&#22686;&#24378;&#26159;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;&#22914;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65289;&#23454;&#29616;&#32463;&#39564;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#65292;&#20294;&#20854;&#22312;&#23398;&#20064;&#22909;&#30340;&#34920;&#31034;&#26041;&#38754;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36924;&#36817;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#39030;&#37096;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#27934;&#23519;&#21147;&#23545;&#22522;&#20110;&#22686;&#24191;&#30340;&#39044;&#35757;&#32451;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#12290;&#25105;&#20204;&#20174;&#20445;&#25345;&#31561;&#36317;&#30340;&#23646;&#24615;&#20986;&#21457;&#65292;&#36825;&#26159;&#30001;&#22686;&#24378;&#32473;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#20851;&#38190;&#20960;&#20309;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20027;&#35201;&#23450;&#29702;&#20026;&#20219;&#24847;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#25509;&#36817;&#32039;&#23494;&#30340;&#19978;&#38480;&#65292;&#29992;&#20110;&#20272;&#35745;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#20043;&#19978;&#25311;&#21512;&#32447;&#24615;&#25506;&#27979;&#22120;&#32780;&#20135;&#29983;&#30340;&#20272;&#35745;&#35823;&#24046;&#21644;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;RKHS&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#20027;&#35201;&#23450;&#29702;&#34920;&#26126;&#65292;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;RKHS&#20989;&#25968;&#20219;&#24847;&#31934;&#30830;&#22320;&#36924;&#36817;&#22686;&#24191;&#20989;&#25968;&#12290;&#36825;&#20010;&#32467;&#26524;&#24847;&#21619;&#30528;&#65292;&#38543;&#30528;&#22686;&#24191;&#20989;&#25968;&#36136;&#37327;&#30340;&#25552;&#39640;&#65292;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#33021;&#21147;&#20063;&#20250;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25209;&#37327;&#24402;&#19968;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Good data augmentation is one of the key factors that lead to the empirical success of self-supervised representation learning such as contrastive learning and masked language modeling, yet theoretical understanding of its role in learning good representations remains limited. Recent work has built the connection between self-supervised learning and approximating the top eigenspace of a graph Laplacian operator. Learning a linear probe on top of such features can naturally be connected to RKHS regression. In this work, we use this insight to perform a statistical analysis of augmentation-based pretraining. We start from the isometry property, a key geometric characterization of the target function given by the augmentation. Our first main theorem provides, for an arbitrary encoder, near tight bounds for both the estimation error incurred by fitting the linear probe on top of the encoder, and the approximation error entailed by the fitness of the RKHS the encoder learns. Our second main
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#21457;&#29616;&#36890;&#36807;&#19968;&#20010;&#26126;&#30830;&#30340;&#20934;&#32447;&#24615;&#37319;&#26679;&#36712;&#36857;&#21644;&#21478;&#19968;&#20010;&#38544;&#24335;&#30340;&#21435;&#22122;&#36712;&#36857;&#24179;&#28369;&#36830;&#25509;&#20102;&#25968;&#25454;&#20998;&#24067;&#21644;&#22122;&#22768;&#20998;&#24067;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;ODE&#30340;&#26368;&#20248;&#37319;&#26679;&#21644;&#32463;&#20856;&#30340;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#20043;&#38388;&#30340;&#29702;&#35770;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.19947</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#20960;&#20309;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Geometric Perspective on Diffusion Models. (arXiv:2305.19947v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#21457;&#29616;&#36890;&#36807;&#19968;&#20010;&#26126;&#30830;&#30340;&#20934;&#32447;&#24615;&#37319;&#26679;&#36712;&#36857;&#21644;&#21478;&#19968;&#20010;&#38544;&#24335;&#30340;&#21435;&#22122;&#36712;&#36857;&#24179;&#28369;&#36830;&#25509;&#20102;&#25968;&#25454;&#20998;&#24067;&#21644;&#22122;&#22768;&#20998;&#24067;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;ODE&#30340;&#26368;&#20248;&#37319;&#26679;&#21644;&#32463;&#20856;&#30340;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#20043;&#38388;&#30340;&#29702;&#35770;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#21644;&#24555;&#36895;&#37319;&#26679;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#26159;&#20351;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#26469;&#25551;&#36848;&#25968;&#25454;&#25200;&#21160;&#21644;&#29983;&#25104;&#24314;&#27169;&#65292;&#20197;&#23454;&#29616;&#32479;&#19968;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20960;&#20010;&#26377;&#36259;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#20026;&#20854;&#37319;&#26679;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#20180;&#32454;&#26816;&#26597;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#24046;&#29190;&#28856;SDE&#21450;&#20854;&#20445;&#25345;&#36793;&#38469;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#29992;&#20110;&#37319;&#26679;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#20998;&#24067;&#21644;&#22122;&#22768;&#20998;&#24067;&#36890;&#36807;&#19968;&#20010;&#26126;&#30830;&#30340;&#20934;&#32447;&#24615;&#37319;&#26679;&#36712;&#36857;&#21644;&#21478;&#19968;&#20010;&#38544;&#24335;&#30340;&#21435;&#22122;&#36712;&#36857;&#24179;&#28369;&#36830;&#25509;&#65292;&#21363;&#20351;&#22312;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#20063;&#25910;&#25947;&#26356;&#24555;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#36215;&#22522;&#20110;ODE&#30340;&#26368;&#20248;&#37319;&#26679;&#21644;&#32463;&#20856;&#30340;&#22343;&#20540;&#28418;&#31227;&#65288;&#23547;&#25214;&#27169;&#24335;&#65289;&#31639;&#27861;&#20043;&#38388;&#30340;&#29702;&#35770;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed significant progress in developing efficient training and fast sampling approaches for diffusion models. A recent remarkable advancement is the use of stochastic differential equations (SDEs) to describe data perturbation and generative modeling in a unified mathematical framework. In this paper, we reveal several intriguing geometric structures of diffusion models and contribute a simple yet powerful interpretation to their sampling dynamics. Through carefully inspecting a popular variance-exploding SDE and its marginal-preserving ordinary differential equation (ODE) for sampling, we discover that the data distribution and the noise distribution are smoothly connected with an explicit, quasi-linear sampling trajectory, and another implicit denoising trajectory, which even converges faster in terms of visual quality. We also establish a theoretical relationship between the optimal ODE-based sampling and the classic mean-shift (mode-seeking) algorithm, with w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19358</link><description>&lt;p&gt;
&#31283;&#20581;&#30340;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stable Anisotropic Regularization. (arXiv:2305.19358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#27169;&#22411;&#28608;&#27963;&#30340;&#23646;&#24615;&#24050;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#25991;&#29486;&#26222;&#36941;&#35748;&#20026;LLMs&#34920;&#31034;&#30001;&#23569;&#25968;&#20855;&#26377;&#26497;&#39640;&#26041;&#24046;&#21644;&#24133;&#24230;&#30340;&#8220;&#24322;&#24120;&#32500;&#24230;&#8221;&#20027;&#23548;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#20960;&#39033;&#30740;&#31350;&#35797;&#22270;&#20943;&#36731;&#36825;&#20123;&#24322;&#24120;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#36843;&#20351;LLMs&#25104;&#20026;&#21508;&#21521;&#21516;&#24615;&#65288;&#21363;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25152;&#26377;&#32500;&#24230;&#20855;&#26377;&#22343;&#21248;&#26041;&#24046;&#65289;&#30340;&#12290;&#21508;&#21521;&#21516;&#24615;&#34987;&#35748;&#20026;&#26159;LLMs&#30340;&#19968;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#26356;&#21152;&#36148;&#36817;&#20154;&#31867;&#30452;&#35273;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;NLP&#20013;&#21508;&#21521;&#21516;&#24615;&#30340;&#35768;&#22810;&#35266;&#28857;&#37117;&#26159;&#22522;&#20110;&#23884;&#20837;&#30340;&#24179;&#22343;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#26368;&#36817;&#24050;&#32463;&#34920;&#26126;&#36825;&#26159;&#19968;&#31181;&#26377;&#32570;&#38519;&#30340;&#21508;&#21521;&#21516;&#24615;&#24230;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-STAR&#65306;&#22522;&#20110;IsoScore$^{\star}$&#30340;&#31283;&#23450;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many of the claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to incre
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;PAC-Bayes&#35757;&#32451;&#26694;&#26550;&#65292;&#26080;&#38656;&#39069;&#22806;&#27491;&#21017;&#21270;&#21644;&#32593;&#26684;&#25628;&#32034;&#35843;&#25972;&#36229;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#33021;&#21147;&#24182;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.19243</link><description>&lt;p&gt;
Auto-tune: &#31070;&#32463;&#32593;&#32476;&#30340;&#20808;&#39564;&#19982;&#21518;&#39564;PAC-Bayes&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Auto-tune: PAC-Bayes Optimization over Prior and Posterior for Neural Networks. (arXiv:2305.19243v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19243
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;PAC-Bayes&#35757;&#32451;&#26694;&#26550;&#65292;&#26080;&#38656;&#39069;&#22806;&#27491;&#21017;&#21270;&#21644;&#32593;&#26684;&#25628;&#32034;&#35843;&#25972;&#36229;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#33021;&#21147;&#24182;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25110;Adam&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#21450;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22914;&#26435;&#37325;&#34928;&#20943;&#12289;Dropout&#25110;&#22122;&#22768;&#27880;&#20837;&#12290;&#36890;&#36807;&#32593;&#26684;&#25628;&#32034;&#35843;&#25972;&#25968;&#37327;&#20247;&#22810;&#30340;&#36229;&#21442;&#25968;&#25165;&#33021;&#36798;&#21040;&#26368;&#20248;&#27867;&#21270;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#65292;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#39564;&#35777;&#25968;&#25454;&#38598;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20999;&#23454;&#21487;&#34892;&#30340;PAC-Bayes&#35757;&#32451;&#26694;&#26550;&#65292;&#20960;&#20046;&#26159;&#26080;&#38656;&#35843;&#25972;&#65292;&#20063;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#65292;&#32780;&#22312;&#23436;&#25104;&#32593;&#26684;&#25628;&#32034;&#21644;&#21152;&#20837;&#39069;&#22806;&#27491;&#21017;&#21270;&#21518;&#65292;&#36798;&#21040;&#20102;&#19982;SGD/Adam&#21487;&#27604;&#36739;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23637;&#31034;&#20102;PAC&#35757;&#32451;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely recognized that the generalization ability of neural networks can be greatly enhanced through carefully designing the training procedure. The current state-of-the-art training approach involves utilizing stochastic gradient descent (SGD) or Adam optimization algorithms along with a combination of additional regularization techniques such as weight decay, dropout, or noise injection. Optimal generalization can only be achieved by tuning a multitude of hyperparameters through grid search, which can be time-consuming and necessitates additional validation datasets. To address this issue, we introduce a practical PAC-Bayes training framework that is nearly tuning-free and requires no additional regularization while achieving comparable testing performance to that of SGD/Adam after a complete grid search and with extra regularizations. Our proposed algorithm demonstrates the remarkable potential of PAC training to achieve state-of-the-art performance on deep neural networks wit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#21453;&#39304;&#30340;RLHF&#12290;</title><link>http://arxiv.org/abs/2305.18505</link><description>&lt;p&gt;
&#22914;&#20309;&#26377;&#25928;&#22320;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Query Human Feedback Efficiently in RL?. (arXiv:2305.18505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18505
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#21453;&#39304;&#30340;RLHF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#33539;&#20363;&#65292;&#22312;&#27492;&#33539;&#20363;&#19979;&#65292;RL&#20195;&#29702;&#23398;&#20064;&#20351;&#29992;&#23545;&#36712;&#36857;&#30340;&#25104;&#23545;&#20248;&#20808;&#32423;&#21453;&#39304;&#26469;&#26368;&#20248;&#21270;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#26126;&#30830;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#23613;&#31649;RLHF&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#29992;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#23454;&#35777;&#30740;&#31350;&#24182;&#26410;&#35299;&#20915;&#22914;&#20309;&#39640;&#25928;&#37319;&#26679;&#36712;&#36857;&#23545;&#20197;&#26597;&#35810;&#20154;&#31867;&#21453;&#39304;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#25506;&#32034;&#24615;&#36712;&#36857;&#65292;&#22312;&#25910;&#38598;&#20219;&#20309;&#20154;&#31867;&#21453;&#39304;&#20043;&#21069;&#65292;&#20351;&#23398;&#20064;&#38544;&#34255;&#30340;&#22870;&#21169;&#20989;&#25968;&#26356;&#21152;&#20934;&#30830;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#25991;&#29486;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#22522;&#20110;&#20559;&#22909;&#27169;&#22411;&#19979;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#25152;&#38656;&#30340;&#20154;&#31867;&#21453;&#39304;&#26356;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#32435;&#20837;&#32447;&#24615;&#21644;&#20302;&#31209;MDPs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#30340;&#21453;&#39304;&#30340;RLHF&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#20248;&#21270;&#20855;&#26377;&#26377;&#38480;&#21453;&#39304;&#30340;&#20219;&#21153;&#26102;&#33719;&#24471;&#25506;&#32034;&#24615;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While RLHF has demonstrated practical success in fine-tuning language models, existing empirical work does not address the challenge of how to efficiently sample trajectory pairs for querying human feedback. In this study, we propose an efficient sampling approach to acquiring exploratory trajectories that enable accurate learning of hidden reward functions before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing literature. Specifically, our framework can incorporate linear and low-rank MDPs. Additionally, we investigate RLHF with action-based comparison feedback and introduce an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.18436</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#23454;&#29616;&#26368;&#20248;K&#22343;&#20540;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming. (arXiv:2305.18436v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
K&#22343;&#20540;&#32858;&#31867;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#27169;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#26494;&#24347;&#26368;&#36817;&#34987;&#25552;&#20986;&#29992;&#20110;&#35299;&#20915;K&#22343;&#20540;&#20248;&#21270;&#38382;&#39064;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#20294;&#23454;&#29616;SDP&#27714;&#35299;&#22120;&#30340;&#24040;&#22823;&#25104;&#26412;&#20351;&#24471;&#36825;&#20123;&#20445;&#35777;&#26080;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#34987;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#32570;&#20047;&#22362;&#23454;&#30340;&#32479;&#35745;&#22522;&#30784;&#25110;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;NMF&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#20984;Burer-Monteiro&#20998;&#35299;&#26041;&#27861;&#35299;&#20915;&#21322;&#23450;&#35268;&#21010;&#26494;&#24347;&#30340;K&#22343;&#20540;&#20844;&#24335;&#30340;&#38750;&#36127;&#20302;&#31209;&#38480;&#21046;&#12290;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#65292;&#21516;&#26102;&#20063;&#20139;&#26377;&#19982;SDP&#30456;&#21516;&#30340;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;NMF&#31639;&#27861;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;SDP&#27714;&#35299;&#22120;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
$K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the $K$-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed $K$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#21450;&#20854;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#21160;&#24577;&#19979;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#22823;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#26469;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#25214;&#21040;&#20102;&#22810;&#20010;&#21644;&#21333;&#19968;&#26041;&#21521;&#30340;&#26368;&#20339;&#25209;&#37327;&#22823;&#23567;&#65292;&#26377;&#21161;&#20110;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#21644;&#26041;&#21521;&#30340;&#19987;&#19994;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.18270</link><description>&lt;p&gt;
&#23398;&#20064;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#27425;(&#24040;&#22823;)&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Two-Layer Neural Networks, One (Giant) Step at a Time. (arXiv:2305.18270v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#21450;&#20854;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#21160;&#24577;&#19979;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#22823;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#26469;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#25214;&#21040;&#20102;&#22810;&#20010;&#21644;&#21333;&#19968;&#26041;&#21521;&#30340;&#26368;&#20339;&#25209;&#37327;&#22823;&#23567;&#65292;&#26377;&#21161;&#20110;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#21644;&#26041;&#21521;&#30340;&#19987;&#19994;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#30740;&#31350;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#22823;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#26377;&#21161;&#20110;&#22312;&#26680;&#24515;&#33539;&#22260;&#20043;&#22806;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25209;&#37327;&#22823;&#23567;&#21644;&#22810;&#20010;(&#20294;&#26377;&#38480;&#30340;)&#27493;&#39588;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21333;&#27493;&#39588;&#36807;&#31243;&#65292;&#21457;&#29616;&#25209;&#37327;&#22823;&#23567;&#20026;$n=O(d)$&#21487;&#20197;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#20294;&#21482;&#36866;&#21512;&#23398;&#20064;&#21333;&#19968;&#26041;&#21521;&#25110;&#21333;&#32034;&#24341;&#27169;&#22411;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;$n=O(d^2)$&#23545;&#20110;&#23398;&#20064;&#22810;&#20010;&#26041;&#21521;&#21644;&#19987;&#19994;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#8220;&#30828;&#8221;&#26041;&#21521;&#32570;&#20047;&#21069;$\ell$&#20010;Hermite&#31995;&#25968;&#65292;&#20173;&#26410;&#34987;&#21457;&#29616;&#65292;&#24182;&#19988;&#38656;&#35201;&#25209;&#37327;&#22823;&#23567;&#20026;$n=O(d^\ell)$&#25165;&#33021;&#34987;&#26799;&#24230;&#19979;&#38477;&#25429;&#33719;&#12290;&#32463;&#36807;&#20960;&#27425;&#36845;&#20195;&#65292;&#24773;&#20917;&#21457;&#29983;&#21464;&#21270;&#65306;&#25209;&#37327;&#22823;&#23567;&#20026;$n=O(d)$&#36275;&#20197;&#23398;&#20064;&#26032;&#30340;&#30446;&#26631;&#26041;&#21521;&#65292;&#36825;&#20123;&#26041;&#21521;&#22312;Hermite&#22522;&#30784;&#19978;&#32447;&#24615;&#36830;&#25509;&#21040;&#20043;&#21069;&#23398;&#20064;&#30340;&#26041;&#21521;&#25152;&#28085;&#30422;&#30340;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the training dynamics of shallow neural networks, investigating the conditions under which a limited number of large batch gradient descent steps can facilitate feature learning beyond the kernel regime. We compare the influence of batch size and that of multiple (but finitely many) steps. Our analysis of a single-step process reveals that while a batch size of $n = O(d)$ enables feature learning, it is only adequate for learning a single direction, or a single-index model. In contrast, $n = O(d^2)$ is essential for learning multiple directions and specialization. Moreover, we demonstrate that ``hard'' directions, which lack the first $\ell$ Hermite coefficients, remain unobserved and require a batch size of $n = O(d^\ell)$ for being captured by gradient descent. Upon iterating a few steps, the scenario changes: a batch-size of $n = O(d)$ is enough to learn new target directions spanning the subspace linearly connected in the Hermite basis to the previously learned directions,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;, &#36890;&#36807;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#21644;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#37117;&#33021;&#22815;&#23454;&#29616;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.16610</link><description>&lt;p&gt;
&#23398;&#20064;&#21333;&#35843;&#21338;&#24328;&#30340;&#25237;&#30707;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Slingshot Approach to Learning in Monotone Games. (arXiv:2305.16610v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;, &#36890;&#36807;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#21644;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#37117;&#33021;&#22815;&#23454;&#29616;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#36981;&#24490;&#27491;&#21017;&#21270;&#39046;&#23548;&#32773;&#31639;&#27861;&#21363;&#20351;&#22312;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#20013;&#20063;&#26080;&#27861;&#25910;&#25947;&#21040;&#22343;&#34913;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#20048;&#35266;&#29256;&#26412;&#24182;&#20855;&#26377;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#26080;&#22122;&#22768;&#30340;&#26799;&#24230;&#21453;&#39304;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#25200;&#21160;&#25110;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#12290;&#36825;&#31181;&#25200;&#21160;&#26377;&#21161;&#20110;&#23558;&#24403;&#21069;&#31574;&#30053;&#25289;&#21521;&#19968;&#20010;&#38170;&#23450;&#31574;&#30053;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#25237;&#30707;&#32034;&#8221;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26694;&#26550;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#33719;&#24471;&#38752;&#36817;&#22343;&#34913;&#28857;&#30340;&#31283;&#23450;&#28857;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23450;&#26399;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#21644;&#24403;&#21069;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#35299;&#37322;&#20026;&#36817;&#31471;p
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the problem of computing equilibria in monotone games. The traditional Follow the Regularized Leader algorithms fail to converge to an equilibrium even in two-player zero-sum games. Although optimistic versions of these algorithms have been proposed with last-iterate convergence guarantees, they require noiseless gradient feedback. To overcome this limitation, we present a novel framework that achieves last-iterate convergence even in the presence of noise. Our key idea involves perturbing or regularizing the payoffs or utilities of the games. This perturbation serves to pull the current strategy to an anchored strategy, which we refer to as a {\it slingshot} strategy. First, we establish the convergence rates of our framework to a stationary point near an equilibrium, regardless of the presence or absence of noise. Next, we introduce an approach to periodically update the slingshot strategy with the current strategy. We interpret this approach as a proximal p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPTIMUS&#30340;&#26032;&#22411;&#27169;&#20223;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#20223;TAMP&#20195;&#29702;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#21160;&#20316;&#36716;&#25442;&#22120;&#31574;&#30053;&#12290;OPTIMUS&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#27169;&#20223;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;TAMP&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#24615;&#33021;&#20248;&#36234;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.16309</link><description>&lt;p&gt;
&#29992;&#35270;&#35273;&#21160;&#20316;&#36716;&#25442;&#22120;&#27169;&#25311;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Imitating Task and Motion Planning with Visuomotor Transformers. (arXiv:2305.16309v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPTIMUS&#30340;&#26032;&#22411;&#27169;&#20223;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#20223;TAMP&#20195;&#29702;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#21160;&#20316;&#36716;&#25442;&#22120;&#31574;&#30053;&#12290;OPTIMUS&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#27169;&#20223;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;TAMP&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#24615;&#33021;&#20248;&#36234;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#35757;&#32451;&#26426;&#22120;&#20154;&#25805;&#20316;&#31574;&#30053;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#32780;&#26080;&#38656;&#25163;&#21160;&#32534;&#31243;&#25110;&#35797;&#38169;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#22914;&#20154;&#24037;&#30417;&#30563;&#65292;&#22240;&#20026;&#32791;&#26102;&#21644;&#21171;&#21160;&#23494;&#38598;&#32780;&#38590;&#20197;&#25193;&#23637;&#12290;&#30456;&#21453;&#65292;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#21487;&#20197;&#33258;&#20027;&#22320;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#22810;&#26679;&#21270;&#28436;&#31034;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;TAMP&#30417;&#30563;&#21592;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19982;&#28789;&#27963;&#30340;Transformer&#27169;&#22411;&#30456;&#32467;&#21512;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#24378;&#22823;&#33539;&#20363;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPTIMUS&#30340;&#26032;&#22411;&#27169;&#20223;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#20223;TAMP&#20195;&#29702;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#21160;&#20316;&#36716;&#25442;&#22120;&#31574;&#30053;&#12290;OPTIMUS&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#27169;&#20223;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;TAMP&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#24615;&#33021;&#20248;&#36234;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#35774;&#35745;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Imitation learning is a powerful tool for training robot manipulation policies, allowing them to learn from expert demonstrations without manual programming or trial-and-error. However, common methods of data collection, such as human supervision, scale poorly, as they are time-consuming and labor-intensive. In contrast, Task and Motion Planning (TAMP) can autonomously generate large-scale datasets of diverse demonstrations. In this work, we show that the combination of large-scale datasets generated by TAMP supervisors and flexible Transformer models to fit them is a powerful paradigm for robot manipulation. To that end, we present a novel imitation learning system called OPTIMUS that trains large-scale visuomotor Transformer policies by imitating a TAMP agent. OPTIMUS introduces a pipeline for generating TAMP data that is specifically curated for imitation learning and can be used to train performant transformer-based policies. In this paper, we present a thorough study of the design
&lt;/p&gt;</description></item><item><title>C-MCTS &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26377;&#32422;&#26463;&#30340;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23433;&#20840;&#35780;&#21028;&#22120;&#36827;&#34892;&#25104;&#26412;&#20272;&#35745;&#65292;&#24182;&#22312;&#37096;&#32626;&#26399;&#38388;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#26356;&#39640;&#25928;&#30340;&#35268;&#21010;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2305.16209</link><description>&lt;p&gt;
C-MCTS: &#23433;&#20840;&#35268;&#21010;&#19982;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
C-MCTS: Safe Planning with Monte Carlo Tree Search. (arXiv:2305.16209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16209
&lt;/p&gt;
&lt;p&gt;
C-MCTS &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26377;&#32422;&#26463;&#30340;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23433;&#20840;&#35780;&#21028;&#22120;&#36827;&#34892;&#25104;&#26412;&#20272;&#35745;&#65292;&#24182;&#22312;&#37096;&#32626;&#26399;&#38388;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#26356;&#39640;&#25928;&#30340;&#35268;&#21010;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#21487;&#20197;&#35299;&#20915;&#21463;&#32422;&#26463;&#30340;&#23433;&#20840;&#20915;&#31574;&#38382;&#39064;&#12290;&#23613;&#31649;CMDP&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;MCTS&#31561;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#31639;&#27861;&#26469;&#35299;&#20915;CMDP&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#22312;&#25104;&#26412;&#26041;&#38754;&#20445;&#23432;&#34892;&#20107;&#65292;&#36890;&#36807;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#25104;&#26412;&#20272;&#35745;&#26469;&#36991;&#20813;&#36829;&#21453;&#32422;&#26463;&#65292;&#20294;&#36825;&#31181;&#20272;&#35745;&#23384;&#22312;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;MCTS&#65288;C-MCTS&#65289;&#65292;&#23427;&#20351;&#29992;&#20808;&#21069;&#22312;&#20195;&#29702;&#37096;&#32626;&#20043;&#21069;&#36890;&#36807;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#35757;&#32451;&#30340;&#23433;&#20840;&#35780;&#21028;&#22120;&#26469;&#20272;&#35745;&#25104;&#26412;&#12290;&#22312;&#37096;&#32626;&#26399;&#38388;&#65292;&#35780;&#21028;&#22120;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#12290;C-MCTS&#28385;&#36275;&#25104;&#26412;&#32422;&#26463;&#65292;&#20294;&#25805;&#20316;&#25509;&#36817;&#32422;&#26463;&#36793;&#30028;&#65292;&#27604;&#20197;&#24448;&#30340;&#24037;&#20316;&#33719;&#24471;&#26356;&#39640;&#30340;&#22870;&#21169;&#12290;&#20316;&#20026;&#19968;&#20010;&#24456;&#22909;&#30340;&#21103;&#20135;&#21697;&#65292;&#36825;&#20010;&#35268;&#21010;&#22120;&#22312;&#35268;&#21010;&#27493;&#39588;&#26041;&#38754;&#26356;&#21152;&#39640;&#25928;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#27169;&#22411;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
The Constrained Markov Decision Process (CMDP) formulation allows to solve safety-critical decision making tasks that are subject to constraints. While CMDPs have been extensively studied in the Reinforcement Learning literature, little attention has been given to sampling-based planning algorithms such as MCTS for solving them. Previous approaches perform conservatively with respect to costs as they avoid constraint violations by using Monte Carlo cost estimates that suffer from high variance. We propose Constrained MCTS (C-MCTS), which estimates cost using a safety critic that is trained with Temporal Difference learning in an offline phase prior to agent deployment. The critic limits exploration by pruning unsafe trajectories within MCTS during deployment. C-MCTS satisfies cost constraints but operates closer to the constraint boundary, achieving higher rewards than previous work. As a nice byproduct, the planner is more efficient w.r.t. planning steps. Most importantly, under model
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34987;&#21160;&#23398;&#20064;&#65292;&#22312;&#26234;&#33021;&#20307;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#33324;&#21270;&#30340;&#20027;&#21160;&#22240;&#26524;&#31574;&#30053;&#65292;&#29992;&#20110;&#30830;&#23450;&#21644;&#20351;&#29992;&#22240;&#26524;&#20851;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#25512;&#26029;&#21644;&#20351;&#29992;&#20174;&#26410;&#20986;&#29616;&#30340;&#22240;&#26524;&#38142;&#25509;&#65292;&#24182;&#23558;&#23454;&#39564;&#31574;&#30053;&#25512;&#24191;&#21040;&#20174;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#21464;&#37327;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.16183</link><description>&lt;p&gt;
&#22312;&#26234;&#33021;&#20307;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#21160;&#23398;&#20064;&#20027;&#21160;&#22240;&#26524;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Passive learning of active causal strategies in agents and language models. (arXiv:2305.16183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16183
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34987;&#21160;&#23398;&#20064;&#65292;&#22312;&#26234;&#33021;&#20307;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#33324;&#21270;&#30340;&#20027;&#21160;&#22240;&#26524;&#31574;&#30053;&#65292;&#29992;&#20110;&#30830;&#23450;&#21644;&#20351;&#29992;&#22240;&#26524;&#20851;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#25512;&#26029;&#21644;&#20351;&#29992;&#20174;&#26410;&#20986;&#29616;&#30340;&#22240;&#26524;&#38142;&#25509;&#65292;&#24182;&#23558;&#23454;&#39564;&#31574;&#30053;&#25512;&#24191;&#21040;&#20174;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#21464;&#37327;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34987;&#21160;&#25968;&#25454;&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#21040;&#20851;&#20110;&#22240;&#26524;&#20851;&#31995;&#21644;&#23454;&#39564;&#30340;&#20160;&#20040;&#20449;&#24687;&#65311;&#37492;&#20110;&#34987;&#21160;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#20351;&#29992;&#31561;&#20132;&#20114;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#21151;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#24456;&#37325;&#35201;&#12290;&#34987;&#21160;&#23398;&#20064;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32431;&#31929;&#30340;&#34987;&#21160;&#23398;&#20064;&#23454;&#38469;&#19978;&#33021;&#22815;&#35753;&#26234;&#33021;&#20307;&#23398;&#20064;&#21040;&#19968;&#33324;&#21270;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#30830;&#23450;&#21644;&#20351;&#29992;&#22240;&#26524;&#20851;&#31995;&#32467;&#26500;&#65292;&#21482;&#35201;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#24178;&#39044;&#12290;&#25105;&#20204;&#22312;&#24418;&#24335;&#19978;&#35828;&#26126;&#20102;&#39318;&#20808;&#36827;&#34892;&#23454;&#39564;&#65292;&#28982;&#21518;&#23547;&#27714;&#30446;&#26631;&#30340;&#31574;&#30053;&#33021;&#22815;&#21407;&#21017;&#19978;&#20351;&#34987;&#21160;&#23398;&#20064;&#23454;&#29616;&#19968;&#33324;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#22312;&#27979;&#35797;&#26102;&#33021;&#22815;&#25512;&#26029;&#21644;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#20174;&#26410;&#20986;&#29616;&#30340;&#22240;&#26524;&#38142;&#25509;&#65307;&#36825;&#20123;&#26234;&#33021;&#20307;&#36824;&#33021;&#22815;&#23558;&#23454;&#39564;&#31574;&#30053;&#25512;&#24191;&#21040;&#20174;&#26410;&#22312;&#35757;&#32451;&#20013;&#35266;&#23519;&#21040;&#30340;&#26032;&#21464;&#37327;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#34987;&#21160;&#25968;&#25454;&#20013;&#19968;&#33324;&#21270;&#22240;&#26524;&#24178;&#39044;&#21644;&#21033;&#29992;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.15852</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#65306;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#24187;&#24819;&#30340;&#25991;&#26412;&#12290;&#33258;&#30456;&#30683;&#30462;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24187;&#35273;&#24418;&#24335;&#65292;&#25351;&#30340;&#26159;&#35821;&#35328;&#27169;&#22411;&#22312;&#21516;&#19968;&#35821;&#22659;&#20013;&#29983;&#25104;&#20004;&#20010;&#30683;&#30462;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#38024;&#23545;&#26368;&#20808;&#36827;&#12289;&#32463;&#36807;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#33258;&#30456;&#30683;&#30462;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12289;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26377;&#25928;&#22320;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#33879;&#21517;&#30340;&#36824;&#26159;&#19981;&#22826;&#20986;&#21517;&#30340;&#35805;&#39064;&#65292;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#30456;&#30683;&#30462;&#37117;&#32463;&#24120;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify self-contradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the sentence pairs generated by itself. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#26550;&#26500;&#21387;&#32553;&#26041;&#27861;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#25928;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22359;&#21024;&#38500;&#30693;&#35782;&#25552;&#21462;SDMs&#65288;BK-SDMs&#65289;&#26041;&#27861;&#65292;&#22312;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#25968;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#37327;&#21270;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;MAC&#21644;&#24310;&#36831;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#19982;&#20351;&#29992;&#26356;&#22810;&#36164;&#28304;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15798</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#26550;&#26500;&#21387;&#32553;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Architectural Compression of Text-to-Image Diffusion Models. (arXiv:2305.15798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#26550;&#26500;&#21387;&#32553;&#26041;&#27861;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#25928;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22359;&#21024;&#38500;&#30693;&#35782;&#25552;&#21462;SDMs&#65288;BK-SDMs&#65289;&#26041;&#27861;&#65292;&#22312;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#25968;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#37327;&#21270;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;MAC&#21644;&#24310;&#36831;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#19982;&#20351;&#29992;&#26356;&#22810;&#36164;&#28304;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65288;SDMs&#65289;&#20013;&#20986;&#33394;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#32467;&#26524;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36817;&#26399;&#20851;&#20110;&#39640;&#25928;SDMs&#30340;&#30740;&#31350;&#23558;&#37325;&#28857;&#25918;&#22312;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#30340;&#25968;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#37327;&#21270;&#19978;&#12290;&#19982;&#36825;&#20123;&#26041;&#21521;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22359;&#21024;&#38500;&#30693;&#35782;&#25552;&#21462;SDMs&#65288;BK-SDMs&#65289;&#65292;&#24378;&#35843;&#20102;&#32463;&#20856;&#26550;&#26500;&#21387;&#32553;&#22312;&#36890;&#29992;T2I&#21512;&#25104;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#20174;SDMs&#30340;U-Net&#20013;&#21024;&#38500;&#20102;&#20960;&#20010;&#27531;&#24046;&#21644;&#27880;&#24847;&#21147;&#22359;&#65292;&#20351;&#21442;&#25968;&#25968;&#37327;&#12289;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#30340;MAC&#21644;&#24310;&#36831;&#20943;&#23569;&#20102;&#36229;&#36807;30&#65285;&#12290;&#25105;&#20204;&#22312;&#21333;&#20010;A100 GPU&#19978;&#20165;&#20351;&#29992;0.22M LAION&#23545;&#36827;&#34892;&#33976;&#39311;&#39044;&#35757;&#32451;&#65288;&#23569;&#20110;&#20840;&#20307;&#35757;&#32451;&#23545;&#30340;0.1&#65285;&#65289;&#12290;&#23613;&#31649;&#20351;&#29992;&#26377;&#38480;&#30340;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#32039;&#20945;&#22411;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20256;&#36882;&#30340;&#30693;&#35782;&#27169;&#20223;&#21407;&#22987;SDM&#65292;&#24182;&#22312;&#23545;&#25239;&#36739;&#22823;&#30340;&#22810;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exceptional text-to-image (T2I) generation results of Stable Diffusion models (SDMs) come with substantial computational demands. To resolve this issue, recent research on efficient SDMs has prioritized reducing the number of sampling steps and utilizing network quantization. Orthogonal to these directions, this study highlights the power of classical architectural compression for general-purpose T2I synthesis by introducing block-removed knowledge-distilled SDMs (BK-SDMs). We eliminate several residual and attention blocks from the U-Net of SDMs, obtaining over a 30% reduction in the number of parameters, MACs per sampling step, and latency. We conduct distillation-based pretraining with only 0.22M LAION pairs (fewer than 0.1% of the full training pairs) on a single A100 GPU. Despite being trained with limited resources, our compact models can imitate the original SDM by benefiting from transferred knowledge and achieve competitive results against larger multi-billion parameter models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15611</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#35282;&#24230;&#21078;&#26512;&#29983;&#29289;&#25968;&#25454;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#65306;&#35266;&#28857;&#21644;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#26159;&#21542;&#20855;&#26377;&#20174;&#23567;&#22270;&#20013;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#25512;&#24191;&#21040;&#21516;&#19968;&#39046;&#22495;&#30340;&#22823;&#22270;&#20013;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#23588;&#20854;&#26159;&#24230;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#20013;&#65292;&#24230;&#25968;&#26159;&#26377;&#30028;&#30340;&#65292;&#22240;&#27492;&#24230;&#20998;&#24067;&#30340;&#20559;&#31227;&#24456;&#23567;&#12290;&#21363;&#20351;&#24230;&#20998;&#24067;&#20559;&#31227;&#24456;&#23567;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GNNs&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#26263;&#31034;&#26377;&#20854;&#20182;&#21407;&#22240;&#12290;&#20107;&#23454;&#19978;&#65292;&#20197;&#24448;&#23545;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#21508;&#31181;&#22270;&#23610;&#23544;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#31867;&#22411;&#21644;&#23646;&#24615;&#30340;&#25506;&#32034;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#20998;&#26512;&#22823;&#22810;&#38598;&#20013;&#22312;&#31354;&#38388;&#39046;&#22495;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#37319;&#29992;&#35889;&#35282;&#24230;&#21435;&#30740;&#31350;GNNs&#22312;&#29983;&#29289;&#22270;&#25968;&#25454;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#27169;&#25311;&#21508;&#31181;&#31867;&#22411;&#30340;&#24230;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#27979;&#35797;GNNs &#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38500;&#20102;&#24230;&#20998;&#24067;&#20559;&#31227;&#22806;&#65292;GNNs &#36824;&#23545;&#22270;&#22823;&#23567;&#21464;&#21270;&#24341;&#36215;&#30340;&#35889;&#20998;&#24067;&#20559;&#31227;&#24456;&#25935;&#24863;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;GNN&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#20855;&#26377;&#23610;&#23544;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20851;&#20110;GNNs&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#30340;&#26032;&#35266;&#28857;&#21644;&#23454;&#36341;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#27934;&#23519;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24102;&#26377;&#25311;&#23545;&#25239;&#25200;&#21160;&#21644;&#26102;&#21464;&#23545;&#25239;&#24615;&#36172;&#21338;&#25439;&#22833;&#20989;&#25968;&#30340;LQR&#21644;LQG&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#35760;&#24518;&#30340;&#36172;&#21338;&#20984;&#20248;&#21270;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.15352</link><description>&lt;p&gt;
&#24102;&#26377;&#25311;&#23545;&#25239;&#25200;&#21160;&#21644;&#26102;&#21464;&#23545;&#25239;&#24615;&#36172;&#21338;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#20248;&#29575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Optimal Rates for Bandit Nonstochastic Control. (arXiv:2305.15352v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24102;&#26377;&#25311;&#23545;&#25239;&#25200;&#21160;&#21644;&#26102;&#21464;&#23545;&#25239;&#24615;&#36172;&#21338;&#25439;&#22833;&#20989;&#25968;&#30340;LQR&#21644;LQG&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#35760;&#24518;&#30340;&#36172;&#21338;&#20984;&#20248;&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#21644;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#65288;LQG&#65289;&#25511;&#21046;&#26159;&#26368;&#20248;&#25511;&#21046;&#20013;&#22522;&#30784;&#19988;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#25311;&#23545;&#25239;&#25200;&#21160;&#21644;&#26102;&#21464;&#23545;&#25239;&#24615;&#36172;&#21338;&#25439;&#22833;&#20989;&#25968;&#30340;LQR&#21644;LQG&#38382;&#39064;&#12290;&#24050;&#30693;&#30340;&#26368;&#20339;&#20122;&#32447;&#24615;&#36951;&#25022;&#31639;&#27861;~\cite{gradu2020non}&#22312;&#26102;&#38388;&#27178;&#36328;&#24230;&#19978;&#20855;&#26377;$T^{\frac{3}{4}}$&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20854;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26159;&#21542;&#21487;&#20197;&#36798;&#21040;$\sqrt{T}$&#30340;&#32039;&#33268;&#29575;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#22238;&#31572;&#32943;&#23450;&#22320;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#20110;&#24050;&#30693;&#21644;&#26410;&#30693;&#31995;&#32479;&#37117;&#33021;&#36798;&#21040;&#26368;&#20248;&#65288;&#38500;&#23545;&#25968;&#22240;&#23376;&#22806;&#65289;&#36951;&#25022;&#30340;&#36172;&#21338;LQR&#21644;LQG&#31639;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26159;&#19968;&#31181;&#20855;&#26377;&#35760;&#24518;&#30340;&#36172;&#21338;&#20984;&#20248;&#21270;&#26032;&#26041;&#26696;&#65292;&#36825;&#20855;&#26377;&#29420;&#31435;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Quadratic Regulator (LQR) and Linear Quadratic Gaussian (LQG) control are foundational and extensively researched problems in optimal control. We investigate LQR and LQG problems with semi-adversarial perturbations and time-varying adversarial bandit loss functions. The best-known sublinear regret algorithm of~\cite{gradu2020non} has a $T^{\frac{3}{4}}$ time horizon dependence, and its authors posed an open question about whether a tight rate of $\sqrt{T}$ could be achieved. We answer in the affirmative, giving an algorithm for bandit LQR and LQG which attains optimal regret (up to logarithmic factors) for both known and unknown systems. A central component of our method is a new scheme for bandit convex optimization with memory, which is of independent interest.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#29305;&#24449;&#20272;&#35745;&#25968;&#25454;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#35813;&#26041;&#27861;&#19982;&#26377;&#30417;&#30563;&#24230;&#37327;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#21322;&#30417;&#30563;&#21644;&#24863;&#30693;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#20572;&#27490;&#20934;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.15016</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#24182;&#24212;&#29992;&#21040;LLMs&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
An Unsupervised Method for Estimating Class Separability of Datasets with Application to LLMs Fine-Tuning. (arXiv:2305.15016v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15016
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#29305;&#24449;&#20272;&#35745;&#25968;&#25454;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#35813;&#26041;&#27861;&#19982;&#26377;&#30417;&#30563;&#24230;&#37327;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#21322;&#30417;&#30563;&#21644;&#24863;&#30693;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#20572;&#27490;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#29305;&#24449;&#26469;&#20272;&#35745;&#25968;&#25454;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#26631;&#31614;&#20449;&#24687;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#23545;&#20960;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20272;&#35745;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#19982;&#38656;&#35201;&#26631;&#31614;&#20449;&#24687;&#30340;&#30417;&#30563;&#24230;&#37327;&#65288;&#22914;Fisher&#21028;&#21035;&#27604;&#29575;&#65288;FDR&#65289;&#21644;&#20998;&#31867;&#22120;&#30340;&#20132;&#21449;&#39564;&#35777;&#65289;&#20043;&#38388;&#23384;&#22312;&#28165;&#26224;&#30340;&#30456;&#20851;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#36825;&#21487;&#20197;&#23454;&#29616;&#20174;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22914;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#24863;&#30693;&#23398;&#20064;&#12290;&#24403;&#25105;&#20204;&#26377;&#38480;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#21644;&#30456;&#23545;&#36739;&#22823;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#26469;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#26102;&#65292;&#36825;&#23558;&#29305;&#21035;&#26377;&#29992;&#12290;&#35813;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#30340;&#35774;&#32622;&#19979;&#65292;&#36890;&#36807;&#30417;&#27979;&#23884;&#20837;&#31354;&#38388;&#27969;&#24418;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#21644;&#33258;&#21160;&#20572;&#27490;&#20934;&#21017;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an unsupervised method that leverages topological characteristics of data manifolds to estimate class separability of the data without requiring labels. Experiments conducted in this paper on several datasets demonstrate a clear correlation and consistency between the class separability estimated by the proposed method with supervised metrics like Fisher Discriminant Ratio~(FDR) and cross-validation of a classifier, which both require labels. This can enable implementing learning paradigms aimed at learning from both labeled and unlabeled data, like semi-supervised and transductive learning. This would be particularly useful when we have limited labeled data and a relatively large unlabeled dataset that can be used to enhance the learning process. The proposed method is implemented for language model fine-tuning with automated stopping criterion by monitoring class separability of the embedding-space manifold in an unsupervised setting. The proposed methodology has 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#20272;&#35745;&#38544;&#24335;&#22870;&#21169;&#20197;&#21450;&#22312;&#32622;&#20449;&#38598;&#21608;&#22260;&#35299;&#20915;&#35268;&#21010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#20351;&#29992;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#23398;&#20064;&#20219;&#20309;&#30446;&#26631;&#31574;&#30053;&#30340;&#26032;&#20445;&#35777;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21333;&#31574;&#30053;&#38598;&#20013;&#31995;&#25968;&#26469;&#34913;&#37327;&#30446;&#26631;&#31574;&#30053;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.14816</link><description>&lt;p&gt;
&#20855;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#21487;&#35777;&#26126;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provable Offline Reinforcement Learning with Human Feedback. (arXiv:2305.14816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#20272;&#35745;&#38544;&#24335;&#22870;&#21169;&#20197;&#21450;&#22312;&#32622;&#20449;&#38598;&#21608;&#22260;&#35299;&#20915;&#35268;&#21010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#20351;&#29992;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#23398;&#20064;&#20219;&#20309;&#30446;&#26631;&#31574;&#30053;&#30340;&#26032;&#20445;&#35777;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21333;&#31574;&#30053;&#38598;&#20013;&#31995;&#25968;&#26469;&#34913;&#37327;&#30446;&#26631;&#31574;&#30053;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#21453;&#39304;&#26159;&#20197;&#36712;&#36857;&#23545;&#20043;&#38388;&#30340;&#20559;&#22909;&#24418;&#24335;&#25552;&#20379;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#65288;1&#65289;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#20174;&#31163;&#32447;&#25968;&#25454;&#20272;&#35745;&#38544;&#24335;&#22870;&#21169;&#65292;&#21644;&#65288;2&#65289;&#22312;MLE&#21608;&#22260;&#30340;&#32622;&#20449;&#38598;&#19978;&#35299;&#20915;&#20998;&#24067;&#40065;&#26834;&#30340;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#36890;&#29992;&#30340;&#22870;&#21169;&#35774;&#32622;&#65292;&#20854;&#20013;&#22870;&#21169;&#21487;&#20197;&#22312;&#25972;&#20010;&#36712;&#36857;&#19978;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#20445;&#35777;&#65292;&#21482;&#35201;&#30446;&#26631;&#31574;&#30053;&#34987;&#31163;&#32447;&#25968;&#25454;&#35206;&#30422;&#65292;&#25105;&#20204;&#23601;&#21487;&#20197;&#20351;&#29992;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20219;&#20309;&#30446;&#26631;&#31574;&#30053;&#12290;&#20026;&#20102;&#34913;&#37327;&#30446;&#26631;&#31574;&#30053;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21333;&#31574;&#30053;&#38598;&#20013;&#31995;&#25968;&#65292;&#21487;&#20197;&#36890;&#36807;&#27599;&#20010;&#36712;&#36857;&#30340;&#38598;&#20013;&#31995;&#25968;&#19978;&#30028;&#26469;&#19978;&#30028;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the problem of offline reinforcement learning with human feedback where feedback is available in the form of preference between trajectory pairs rather than explicit rewards. Our proposed algorithm consists of two main steps: (1) estimate the implicit reward using Maximum Likelihood Estimation (MLE) with general function approximation from offline data and (2) solve a distributionally robust planning problem over a confidence set around the MLE. We consider the general reward setting where the reward can be defined over the whole trajectory and provide a novel guarantee that allows us to learn any target policy with a polynomial number of samples, as long as the target policy is covered by the offline data. This guarantee is the first of its kind with general function approximation. To measure the coverage of the target policy, we introduce a new single-policy concentrability coefficient, which can be upper bounded by the per-trajectory concentrability coe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21464;&#23384;&#20648;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#36807;&#31243; (CMANPs) &#21450;&#20854;&#27880;&#24847;&#21147;&#22359; CMAB&#65292;&#33021;&#22312;&#24120;&#25968;&#20869;&#23384;&#19979;&#36827;&#34892;&#26465;&#20214;&#12289;&#26597;&#35810;&#21644;&#26356;&#26032;&#25805;&#20316;&#65292;&#24182;&#22312;&#20803;&#22238;&#24402;&#21644;&#23569;&#26679;&#26412;&#22238;&#24402;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14567</link><description>&lt;p&gt;
&#19981;&#21464;&#23384;&#20648;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Constant Memory Attentive Neural Processes. (arXiv:2305.14567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14567
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21464;&#23384;&#20648;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#36807;&#31243; (CMANPs) &#21450;&#20854;&#27880;&#24847;&#21147;&#22359; CMAB&#65292;&#33021;&#22312;&#24120;&#25968;&#20869;&#23384;&#19979;&#36827;&#34892;&#26465;&#20214;&#12289;&#26597;&#35810;&#21644;&#26356;&#26032;&#25805;&#20316;&#65292;&#24182;&#22312;&#20803;&#22238;&#24402;&#21644;&#23569;&#26679;&#26412;&#22238;&#24402;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36807;&#31243; (Neural Processes, NPs) &#26159;&#20272;&#35745;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;NPs &#21253;&#21547;&#19968;&#20010;&#32534;&#30721;&#26465;&#20214;&#25968;&#25454;&#38598;&#30340;&#26465;&#20214;&#38454;&#27573;&#12289;&#19968;&#20010;&#20351;&#29992;&#32534;&#30721;&#39044;&#27979;&#30340;&#26597;&#35810;&#38454;&#27573;&#21644;&#19968;&#20010;&#20351;&#29992;&#26032;&#25968;&#25454;&#28857;&#26356;&#26032;&#32534;&#30721;&#30340;&#26356;&#26032;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#20869;&#23384;&#65292;&#36825;&#20010;&#20869;&#23384;&#38543;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21576;&#32447;&#24615;&#25110;&#20108;&#27425;&#20989;&#25968;&#22686;&#38271;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21464;&#23384;&#20648;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#36807;&#31243; (Constant Memory Attentive Neural Processes, CMANPs)&#65292;&#23427;&#30340;&#26465;&#20214;&#12289;&#26597;&#35810;&#21644;&#26356;&#26032;&#38454;&#27573;&#22343;&#21482;&#38656;&#35201;&#24120;&#25968;&#20869;&#23384;&#12290;&#22312;&#26500;&#24314; CMANPs &#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36890;&#29992;&#27880;&#24847;&#21147;&#22359;&#65292;&#31216;&#20026; Constant Memory Attention Block (CMAB)&#65292;&#23427;&#21487;&#20197;&#22312;&#24120;&#25968;&#20869;&#23384;&#20013;&#35745;&#31639;&#36755;&#20986;&#24182;&#36827;&#34892;&#24120;&#25968;&#30340;&#26356;&#26032;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CMANPs &#22312;&#20803;&#22238;&#24402;&#21644;&#23569;&#26679;&#26412;&#22238;&#24402;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#20445;&#25345;&#24120;&#25968;&#20869;&#23384;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Processes (NPs) are efficient methods for estimating predictive uncertainties. NPs comprise of a conditioning phase where a context dataset is encoded, a querying phase where the model makes predictions using the context dataset encoding, and an updating phase where the model updates its encoding with newly received datapoints. However, state-of-the-art methods require additional memory which scales linearly or quadratically with the size of the dataset, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant which only requires constant memory for the conditioning, querying, and updating phases. In building CMANPs, we propose Constant Memory Attention Block (CMAB), a novel general-purpose attention block that can compute its output in constant memory and perform updates in constant computation. Empirically, we show CMANPs achieve state-of-the-art results on meta-regression an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#25391;&#24133;&#30340;&#20809; plethysmography (PPG) &#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35265;&#24615;&#22270;&#21644;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;&#23545;&#24515;&#29575;&#21644;&#34880;&#31649;&#32769;&#21270;&#31561;&#29983;&#29289;&#29305;&#24449;&#30340;&#31283;&#20581;&#20272;&#35745;&#21644;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.14062</link><description>&lt;p&gt;
&#19981;&#20381;&#36182;&#20110;&#25391;&#24133;&#30340;&#20809; plethysmography (PPG) &#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65306;&#36890;&#36807;&#21487;&#35265;&#24615;&#22270;&#21644;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning. (arXiv:2305.14062v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#25391;&#24133;&#30340;&#20809; plethysmography (PPG) &#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35265;&#24615;&#22270;&#21644;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;&#23545;&#24515;&#29575;&#21644;&#34880;&#31649;&#32769;&#21270;&#31561;&#29983;&#29289;&#29305;&#24449;&#30340;&#31283;&#20581;&#20272;&#35745;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#20307;&#31215;&#25551;&#35760;&#27861; (PPG) &#26159;&#20351;&#29992;&#20809;&#27979;&#37327;&#34880;&#28082;&#20307;&#31215;&#30340;&#21464;&#21270;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#26159;&#22823;&#22810;&#25968;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#29305;&#24449;&#12290;PPG&#20449;&#21495;&#33021;&#22815;&#25552;&#20379;&#23545;&#20154;&#20307;&#24490;&#29615;&#31995;&#32479;&#30340;&#27934;&#23519;&#65292;&#24182;&#21487;&#29992;&#20110;&#25552;&#21462;&#21508;&#31181;&#29983;&#29289;&#29305;&#24449;&#65292;&#20363;&#22914;&#24515;&#29575;&#21644;&#34880;&#31649;&#32769;&#21270;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#65292;&#20294;&#35768;&#22810;&#31639;&#27861;&#23384;&#22312;&#38480;&#21046;&#65292;&#21253;&#25324;&#36807;&#22810;&#22320;&#20381;&#36182;&#20154;&#24037;&#26657;&#20934;&#12289;&#39640;&#20449;&#21495;&#36136;&#37327;&#35201;&#27714;&#21644;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22270;&#35770;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#30340;PPG&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#25391;&#24133;&#26080;&#20851;&#24182;&#19988;&#23545;&#20223;&#23556;&#21464;&#25442;&#19981;&#21464;&#12290;&#23427;&#36824;&#38656;&#35201;&#26368;&#23569;&#30340;&#39044;&#22788;&#29702;&#65292;&#36890;&#36807;RGB&#36890;&#36947;&#34701;&#21512;&#20449;&#24687;&#65292;&#24182;&#22312;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#31283;&#20581;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;VGTL-net&#22312;&#34880;&#31649;&#32769;&#21270;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#31283;&#20581;&#30340;&#20272;&#35745;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) refers to the measurement of variations in blood volume using light and is a feature of most wearable devices. The PPG signals provide insight into the body's circulatory system and can be employed to extract various bio-features, such as heart rate and vascular ageing. Although several algorithms have been proposed for this purpose, many exhibit limitations, including heavy reliance on human calibration, high signal quality requirements, and a lack of generalisation. In this paper, we introduce a PPG signal processing framework that integrates graph theory and computer vision algorithms, to provide an analysis framework which is amplitude-independent and invariant to affine transformations. It also requires minimal preprocessing, fuses information through RGB channels and exhibits robust generalisation across tasks and datasets. The proposed VGTL-net achieves state-of-the-art performance in the prediction of vascular ageing and demonstrates robust estimation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290;&#22312;Common Voice&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23454;&#29616;18% WER&#12290;&#32780;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13330</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised ASR via Cross-Lingual Pseudo-Labeling. (arXiv:2305.13330v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290;&#22312;Common Voice&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23454;&#29616;18% WER&#12290;&#32780;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#38750;&#37197;&#23545;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#26469;&#35757;&#32451;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#20551;&#23450;&#19981;&#33021;&#20351;&#29992;&#20219;&#20309;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21363;&#20351;&#27809;&#26377;&#32473;&#23450;&#35821;&#35328;&#30340;&#20219;&#20309;&#26631;&#27880;&#38899;&#39057;&#65292;&#20063;&#22987;&#32456;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#30340;&#23383;&#31526;&#32423;&#22768;&#23398;&#27169;&#22411;&#65288;AM&#65289;&#65292;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290; &#36825;&#37324;&#65292;&#8220;&#26080;&#30417;&#30563;&#8221;&#24847;&#21619;&#30528;&#27809;&#26377;&#21487;&#29992;&#20110;&#30446;&#26631;&#35821;&#35328;&#30340;&#26631;&#27880;&#38899;&#39057;&#12290;&#26412;&#25991;&#30340;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#65288;i&#65289;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;AM&#29983;&#25104;&#8220;&#30446;&#26631;&#8221;&#35821;&#35328;&#30340;&#20266;&#26631;&#31614;&#65288;PLs&#65289;&#65307;&#65288;ii&#65289;&#20351;&#29992;&#8220;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#8221;&#38480;&#21046;&#36825;&#20123;PLs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Common Voice&#19978;&#38750;&#24120;&#26377;&#25928;&#65306;&#20363;&#22914;&#65292;&#23558;&#33521;&#35821;AM&#20256;&#36882;&#21040;&#26031;&#29926;&#24076;&#37324;&#35821;&#21487;&#20197;&#23454;&#29616;18&#65285;&#30340;WER&#12290; &#23427;&#36824;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#20110;&#23383;&#31526;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that it is possible to train an $\textit{unsupervised}$ automatic speech recognition (ASR) system using only unpaired audio and text. Existing unsupervised ASR methods assume that no labeled data can be used for training. We argue that even if one does not have any labeled audio for a given language, there is $\textit{always}$ labeled data available for other languages. We show that it is possible to use character-level acoustic models (AMs) from other languages to bootstrap an $\textit{unsupervised}$ AM in a new language. Here, "unsupervised" means no labeled audio is available for the $\textit{target}$ language. Our approach is based on two key ingredients: (i) generating pseudo-labels (PLs) of the $\textit{target}$ language using some $\textit{other}$ language AM and (ii) constraining these PLs with a $\textit{target language model}$. Our approach is effective on Common Voice: e.g. transfer of English AM to Swahili achieves 18% WER. It also outperforms characte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#19979;&#28216;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#26377;&#25928;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.13301</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Diffusion Models with Reinforcement Learning. (arXiv:2305.13301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#19979;&#28216;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#26377;&#25928;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#28789;&#27963;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#30340;&#36817;&#20284;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#26696;&#20363;&#24182;&#19981;&#20851;&#27880;&#20284;&#28982;&#65292;&#32780;&#26159;&#20851;&#27880;&#20154;&#31867;&#24863;&#30693;&#30340;&#22270;&#20687;&#36136;&#37327;&#25110;&#33647;&#29289;&#25928;&#21147;&#31561;&#19979;&#28216;&#30446;&#26631;&#12290;&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#27492;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23558;&#21435;&#22122;&#35270;&#20026;&#22810;&#27493;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#19968;&#31867;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#30456;&#23545;&#20110;&#26367;&#20195;&#30340;&#22870;&#21169;&#21152;&#26435;&#20284;&#28982;&#26041;&#27861;&#26356;&#20026;&#26377;&#25928;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;DDPO&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;DDPO&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;&#21453;&#39304;&#30340;&#25552;&#31034;-&#22270;&#20687;&#23545;&#40784;&#26041;&#24335;&#26469;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCare&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#26469;&#25913;&#36827;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#21307;&#30103;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12788</link><description>&lt;p&gt;
GraphCare: &#20351;&#29992;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#25552;&#21319;&#21307;&#30103;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs. (arXiv:2305.12788v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCare&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#26469;&#25913;&#36827;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#21307;&#30103;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#65292;&#20294;&#23558;&#21307;&#23398;&#30693;&#35782;&#25972;&#21512;&#21040;&#39044;&#27979;&#21644;&#20915;&#31574;&#20013;&#20197;&#25552;&#39640;&#25928;&#26524;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#26159;&#22240;&#20026;&#20010;&#24615;&#21270;&#39044;&#27979;&#38656;&#35201;&#20010;&#24615;&#21270;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#65292;&#32780;&#20174;&#24739;&#32773;EHR&#25968;&#25454;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\textsc{GraphCare}&#30340;&#24320;&#25918;&#24335;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#26469;&#25913;&#36827;&#22522;&#20110;EHR&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#21644;&#22806;&#37096;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#26500;&#24314;&#20010;&#20307;&#21270;&#30340;&#24739;&#32773;&#30693;&#35782;&#22270;&#35889;&#65292;&#28982;&#21518;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;Bi-attention AugmenTed (BAT)&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#36827;&#34892;&#21307;&#30103;&#39044;&#27979;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;MIMIC-III&#21644;MIMIC-IV&#19978;&#65292;\textsc{GraphCare}&#22312;&#22235;&#20010;&#20851;&#38190;&#30340;&#21307;&#30103;&#39044;&#27979;&#20219;&#21153;&#19978;&#22343;&#36229;&#36807;&#20102;&#22522;&#20934;&#32447;&#65306;&#27515;&#20129;&#29575;&#12289;&#20877;&#20837;&#38498;&#29575;&#12289;&#20303;&#38498;&#22825;&#25968;&#21644;&#33647;&#29289;&#25512;&#33616;&#12290;&#22312;MIMIC-III&#19978;&#65292;&#23427;&#23558;AUROC&#25552;&#39640;&#20102;17.6%&#21644;6.6%&#65292;&#23558;F1&#24471;&#20998;&#25552;&#39640;&#20102;7.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical predictive models often rely on patients' electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge graphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose \textsc{GraphCare}, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, \textsc{GraphCare} surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6\% and 6.6\% for mortality and readmission, and F1-score by 7.9\% 
&lt;/p&gt;</description></item><item><title>HighLight&#26159;&#19968;&#31181;&#39640;&#25928;&#28789;&#27963;&#30340;DNN&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#31232;&#30095;&#24615;&#23454;&#29616;&#20102;&#23545;DNN&#30340;&#26377;&#25928;&#21152;&#36895;&#65292;&#20248;&#21270;&#20102;&#33021;&#32791;&#21644;&#24310;&#36831;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.12718</link><description>&lt;p&gt;
HighLight: &#39640;&#25928;&#28789;&#27963;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#19982;&#20998;&#23618;&#32467;&#26500;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
HighLight: Efficient and Flexible DNN Acceleration with Hierarchical Structured Sparsity. (arXiv:2305.12718v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12718
&lt;/p&gt;
&lt;p&gt;
HighLight&#26159;&#19968;&#31181;&#39640;&#25928;&#28789;&#27963;&#30340;DNN&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#31232;&#30095;&#24615;&#23454;&#29616;&#20102;&#23545;DNN&#30340;&#26377;&#25928;&#21152;&#36895;&#65292;&#20248;&#21270;&#20102;&#33021;&#32791;&#21644;&#24310;&#36831;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21508;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20248;&#21270;&#25216;&#26415;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#29616;&#20195;DNN&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#21487;&#20197;&#26159;&#31264;&#23494;&#30340;&#25110;&#31232;&#30095;&#30340;&#65292;&#20854;&#31232;&#30095;&#31243;&#24230;&#21508;&#19981;&#30456;&#21516;&#12290;&#20026;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#30828;&#20214;&#24615;&#33021;&#20043;&#38388;&#25214;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#26435;&#34913;&#65292;&#29702;&#24819;&#30340;DNN&#21152;&#36895;&#22120;&#24212;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;DNN&#30340;&#31232;&#30095;&#24615;&#36716;&#21270;&#20026;&#33021;&#37327;&#21644;/&#25110;&#24310;&#36831;&#30340;&#38477;&#20302;&#32780;&#19981;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#22797;&#26434;&#24615;&#24320;&#38144;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20998;&#23618;&#32467;&#26500;&#31232;&#30095;&#24615;&#65288;HSS&#65289;, &#36890;&#36807;&#23558;&#22810;&#20010;&#31616;&#21333;&#31232;&#30095;&#27169;&#24335;&#23618;&#32423;&#32452;&#21512;&#26469;&#31995;&#32479;&#22320;&#34920;&#31034;&#19981;&#21516;&#30340;&#31232;&#30095;&#31243;&#24230;&#12290;&#22240;&#27492;&#65292;HSS&#31616;&#21270;&#20102;&#24213;&#23618;&#30828;&#20214;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#25903;&#25345;&#31616;&#21333;&#31232;&#30095;&#27169;&#24335;&#65292;&#36825;&#26174;&#33879;&#20943;&#23569;&#20102;&#31232;&#30095;&#21152;&#36895;&#30340;&#24320;&#38144;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#22312;&#36825;&#31181;&#26426;&#20250;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#39640;&#25928;&#28789;&#27963;&#30340;&#21152;&#36895;&#22120;&#65292;&#21629;&#21517;&#20026;HighLight&#65292;&#29992;&#20110;&#21152;&#36895;DNN&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to complex interactions among various deep neural network (DNN) optimization techniques, modern DNNs can have weights and activations that are dense or sparse with diverse sparsity degrees. To offer a good trade-off between accuracy and hardware performance, an ideal DNN accelerator should have high flexibility to efficiently translate DNN sparsity into reductions in energy and/or latency without incurring significant complexity overhead.  This paper introduces hierarchical structured sparsity (HSS), with the key insight that we can systematically represent diverse sparsity degrees by having them hierarchically composed from multiple simple sparsity patterns. As a result, HSS simplifies the underlying hardware since it only needs to support simple sparsity patterns; this significantly reduces the sparsity acceleration overhead, which improves efficiency. Motivated by such opportunities, we propose a simultaneously efficient and flexible accelerator, named HighLight, to accelerate D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11854</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577; Web &#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027; Web &#23548;&#33322;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20381;&#36182;&#25968;&#21313;&#20159;&#27425;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;&#24615;&#20132;&#20114;&#21644;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#21033;&#29992;&#26469;&#33258;&#20016;&#23500;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33073;&#26426;&#35757;&#32451;&#65292;&#29992;&#20110;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340; Web &#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;&#65292; WebGUM&#65292;&#23427;&#35266;&#23519;&#20102;&#32593;&#39029;&#25130;&#22270;&#21644; HTML &#39029;&#38754;&#65292;&#24182;&#36755;&#20986; Web &#23548;&#33322;&#25805;&#20316;&#65292;&#22914;&#21333;&#20987;&#21644;&#36755;&#20837;&#12290;WebGUM &#26159;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#22312;&#22823;&#37327;&#30340;&#28436;&#31034;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312; MiniWoB &#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#36229;&#36807;&#20043;&#21069;&#26368;&#20339;&#33073;&#26426;&#26041;&#27861; 31.9% &#20197;&#19978;&#65292;&#25509;&#36817;&#23454;&#29616;&#22312;&#32447;&#20132;&#20114;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diff-Pruning&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;Taylor&#23637;&#24320;&#36807;&#31243;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#65292;&#20174;&#32780;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#24615;&#33021;&#31283;&#23450;&#65292;&#24182;&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.10924</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#26500;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Structural Pruning for Diffusion Models. (arXiv:2305.10924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diff-Pruning&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;Taylor&#23637;&#24320;&#36807;&#31243;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#65292;&#20174;&#32780;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#24615;&#33021;&#31283;&#23450;&#65292;&#24182;&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPM&#65289;&#30340;&#36716;&#22411;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#36890;&#24120;&#28041;&#21450;&#21040;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26399;&#38388;&#37117;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-Pruning&#65292;&#19968;&#31181;&#19987;&#20026;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;Diff-Pruning&#30340;&#26412;&#36136;&#26159;&#36890;&#36807;&#21098;&#26525;&#26102;&#38388;&#27493;&#38271;&#30340;Taylor&#23637;&#24320;&#65292;&#22312;&#36807;&#28388;&#25481;&#26080;&#36129;&#29486;&#25193;&#25955;&#27493;&#39588;&#21644;&#25972;&#21512;&#26377;&#20449;&#24687;&#30340;&#26799;&#24230;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#31361;&#20986;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20004;&#20010;&#20027;&#35201;&#20248;&#28857;&#65306;1&#65289;&#25928;&#29575;&#65306;&#23427;&#21487;&#20197;&#20197;&#21407;&#22987;&#35757;&#32451;&#25237;&#20837;&#30340;&#20165;10&#65285;&#21040;20&#65285;&#30340;&#20195;&#20215;&#23454;&#29616;&#32422;50&#65285;&#30340;FLOPs&#20943;&#23569;; 2&#65289;&#19968;&#33268;&#24615;: &#21098;&#26525;&#21518;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#25928;&#26524;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#24403;&#65292;&#19981;&#20250;&#24433;&#21709;&#29983;&#25104;&#24314;&#27169;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative modeling has recently undergone remarkable advancements, primarily propelled by the transformative implications of Diffusion Probabilistic Models (DPMs). The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present Diff-Pruning, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. The essence of Diff-Pruning is encapsulated in a Taylor expansion over pruned timesteps, a process that disregards non-contributory diffusion steps and ensembles informative gradients to identify important weights. Our empirical assessment, undertaken across four diverse datasets highlights two primary benefits of our proposed method: 1) Efficiency: it enables approximately a 50% reduction in FLOPs at a mere 10% to 20% of the original training expenditure; 2) Consistency: the pruned diffusio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;SAMA&#65292;&#36890;&#36807;&#25552;&#21069;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#20998;&#35299;&#26469;&#35299;&#20915;ASG&#26041;&#27861;&#23384;&#22312;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#21644;&#29983;&#25104;&#38750;&#23454;&#38469;&#20219;&#21153;&#22870;&#21169;&#30340;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10865</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#23545;&#40784;&#20219;&#21153;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning. (arXiv:2305.10865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;SAMA&#65292;&#36890;&#36807;&#25552;&#21069;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#20998;&#35299;&#26469;&#35299;&#20915;ASG&#26041;&#27861;&#23384;&#22312;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#21644;&#29983;&#25104;&#38750;&#23454;&#38469;&#20219;&#21153;&#22870;&#21169;&#30340;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#22411;MARL&#20013;&#30340;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#30528;&#37325;&#20110;&#36866;&#24403;&#30340;&#20449;&#29992;&#20998;&#37197;&#12290;&#33258;&#21160;&#23376;&#30446;&#26631;&#29983;&#25104;&#65288;ASG&#65289;&#26159;&#26368;&#36817;&#20986;&#29616;&#30340;&#19968;&#31181;&#21487;&#34892;&#30340;MARL&#26041;&#27861;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#22312;&#20869;&#22312;&#39537;&#21160;&#30340;&#22686;&#24378;&#23398;&#20064;&#20013;&#21033;&#29992;&#23376;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20174;&#31232;&#30095;&#22870;&#21169;&#20013;&#36827;&#34892;&#22797;&#26434;&#20219;&#21153;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26080;&#30097;&#38656;&#35201;&#22823;&#37327;&#30340;&#22521;&#35757;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;"&#35299;&#32806;"&#20915;&#31574;&#26041;&#27861;&#65292;&#21363;&#22312;MARL&#20013;&#30340;&#35821;&#20041;&#23545;&#40784;&#20219;&#21153;&#20998;&#35299;&#65288;SAMA&#65289;&#65292;&#21463;&#21040;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difficulty of appropriately assigning credit is particularly heightened in cooperative MARL with sparse reward, due to the concurrent time and structural scales involved. Automatic subgoal generation (ASG) has recently emerged as a viable MARL approach inspired by utilizing subgoals in intrinsically motivated reinforcement learning. However, end-to-end learning of complex task planning from sparse rewards without prior knowledge, undoubtedly requires massive training samples. Moreover, the diversity-promoting nature of existing ASG methods can lead to the "over-representation" of subgoals, generating numerous spurious subgoals of limited relevance to the actual task reward and thus decreasing the sample efficiency of the algorithm. To address this problem and inspired by the disentangled representation learning, we propose a novel "disentangled" decision-making method, Semantically Aligned task decomposition in MARL (SAMA), that prompts pretrained language models with chain-of-thou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#35780;&#20272;&#24230;&#37327;&#32508;&#36848;&#65292;&#20174;&#32780;&#20026;&#26631;&#20934;&#21270;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#36129;&#29486;&#21147;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.10616</link><description>&lt;p&gt;
CNN &#21387;&#32553;&#30340;&#35780;&#20272;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Evaluation Metrics for CNNs Compression. (arXiv:2305.10616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#35780;&#20272;&#24230;&#37327;&#32508;&#36848;&#65292;&#20174;&#32780;&#20026;&#26631;&#20934;&#21270;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#36129;&#29486;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#33268;&#21147;&#20110;&#24320;&#21457;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#25216;&#26415;&#65292;&#20294;&#31038;&#21306;&#20284;&#20046;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#21387;&#32553;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#35782;&#21035;&#19981;&#21516;&#24212;&#29992;&#31243;&#24207;&#30340;&#26368;&#21512;&#36866;&#30340;&#21387;&#32553;&#25216;&#26415;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#35780;&#20272;&#24230;&#37327;&#30340;&#32508;&#36848;&#26469;&#20026;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#26631;&#20934;&#21270;&#36129;&#29486;&#12290;&#36825;&#20123;&#24230;&#37327;&#24050;&#34987;&#23454;&#29616;&#21040;NetZIP&#65292;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#22522;&#20934;&#20043;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#19968;&#20123;&#34987;&#23457;&#26597;&#30340;&#24230;&#37327;&#65292;&#20998;&#21035;&#32858;&#28966;&#20110;&#30446;&#26631;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#36793;&#32536;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a lot of research effort devoted by researcher into developing different techniques for neural networks compression, yet the community seems to lack standardised ways of evaluating and comparing between different compression techniques, which is key to identifying the most suitable compression technique for different applications. In this paper we contribute towards standardisation of neural network compression by providing a review of evaluation metrics. These metrics have been implemented into NetZIP, a standardised neural network compression bench. We showcase some of the metrics reviewed using three case studies focusing on object classification, object detection, and edge devices.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#22788;&#29702;&#26497;&#31471;&#27668;&#21160;&#23398;&#38382;&#39064;&#30340;&#22522;&#26412;&#29289;&#29702;&#26426;&#21046;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08024</link><description>&lt;p&gt;
&#38477;&#32500;&#22788;&#29702;&#26497;&#31471;&#27668;&#21160;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Grasping Extreme Aerodynamics on a Low-Dimensional Manifold. (arXiv:2305.08024v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08024
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#22788;&#29702;&#26497;&#31471;&#27668;&#21160;&#23398;&#38382;&#39064;&#30340;&#22522;&#26412;&#29289;&#29702;&#26426;&#21046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33322;&#31354;&#22120;&#25191;&#34892;&#24191;&#27867;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#36816;&#36755;&#12289;&#22269;&#38450;&#12289;&#30417;&#35270;&#21644;&#25937;&#25588;&#12290;&#36825;&#20123;&#39134;&#26426;&#21487;&#20197;&#22312;&#24179;&#38745;&#30340;&#26465;&#20214;&#19979;&#39134;&#34892;&#65292;&#20294;&#36991;&#20813;&#22312;&#31364;&#35895;&#65292;&#23665;&#21306;&#21644;&#33337;&#33334;&#23614;&#36857;&#31561;&#22810;&#39118;&#21306;&#25805;&#20316;&#12290;&#20854;&#20013;&#65292;&#23567;&#22411;&#39134;&#26426;&#23588;&#20854;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#27668;&#27969;&#24178;&#25200;&#12290;&#38543;&#30528;&#20840;&#29699;&#21464;&#26262;&#20351;&#26497;&#31471;&#22825;&#27668;&#36234;&#26469;&#36234;&#39057;&#32321;&#65292;&#39044;&#35745;&#39134;&#26426;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#36739;&#23567;&#30340;&#39134;&#26426;&#65292;&#20250;&#36935;&#21040;&#22823;&#35268;&#27169;&#30340;&#22823;&#27668;&#24178;&#25200;&#65292;&#20294;&#20173;&#24212;&#25191;&#34892;&#31283;&#23450;&#30340;&#39134;&#34892;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20960;&#20046;&#27809;&#26377;&#29702;&#35770;&#26469;&#25551;&#36848;&#24378;&#28872;&#30340;&#27668;&#27969;&#23545;&#39134;&#34892;&#22120;&#30340;&#24433;&#21709;&#12290;&#26356;&#20026;&#22797;&#26434;&#30340;&#26159;&#65292;&#32764;&#38754;&#36935;&#21040;&#27668;&#27969;&#30340;&#21442;&#25968;&#31354;&#38388;&#38750;&#24120;&#22823;&#65292;&#32780;&#19981;&#21516;&#21442;&#25968;&#32452;&#21512;&#20043;&#38388;&#30340;&#28065;&#26059;&#27668;&#27969;&#21644;&#32764;&#30340;&#30456;&#20114;&#20316;&#29992;&#20284;&#20046;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#28857;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#30340;&#22522;&#26412;&#29289;&#29702;&#26426;&#21046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern air vehicles perform a wide range of operations, including transportation, defense, surveillance, and rescue. These aircraft can fly in calm conditions but avoid operations in gusty environments, which are seen in urban canyons, over mountainous terrains, and in ship wakes. Smaller aircraft are especially prone to such gust disturbances. With extreme weather becoming ever more frequent due to global warming, it is anticipated that aircraft, especially those that are smaller in size, encounter large-scale atmospheric disturbances and still be expected to manage stable flight. However, there exists virtually no foundation to describe the influence of extreme vortical gusts on flying bodies. To compound on this difficult problem, there is an enormous parameter space for gusty conditions wings encounter. While the interaction between the vortical gusts and wings is seemingly complex and different for each combination of gust parameters, we show in this study that the fundamental phy
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#33258;&#28982;&#22320;&#23548;&#33268;&#27491;&#21017;&#21270;&#30340;&#21464;&#20998;&#26041;&#27861;&#65288;RED-Diff&#65289;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#21453;&#38382;&#39064;&#12290;&#21152;&#26435;&#26426;&#21046;&#21487;&#20197;&#34913;&#37327;&#19981;&#21516;&#26102;&#38388;&#27493;&#38271;&#30340;&#21435;&#22122;&#22120;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.04391</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#27169;&#22411;&#35299;&#20915;&#21453;&#38382;&#39064;&#30340;&#21464;&#20998;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Variational Perspective on Solving Inverse Problems with Diffusion Models. (arXiv:2305.04391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04391
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#33258;&#28982;&#22320;&#23548;&#33268;&#27491;&#21017;&#21270;&#30340;&#21464;&#20998;&#26041;&#27861;&#65288;RED-Diff&#65289;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#21453;&#38382;&#39064;&#12290;&#21152;&#26435;&#26426;&#21046;&#21487;&#20197;&#34913;&#37327;&#19981;&#21516;&#26102;&#38388;&#27493;&#38271;&#30340;&#21435;&#22122;&#22120;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#35270;&#35273;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#30340;&#20851;&#38190;&#25903;&#26609;&#20043;&#19968;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#24212;&#29992;&#26159;&#36890;&#36807;&#21333;&#20010;&#25193;&#25955;&#20808;&#39564;&#26222;&#36941;&#35299;&#20915;&#19981;&#21516;&#30340;&#21453;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#20026;&#27599;&#20010;&#20219;&#21153;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25193;&#25955;&#36807;&#31243;&#30340;&#38750;&#32447;&#24615;&#21644;&#36845;&#20195;&#24615;&#36136;&#20351;&#24471;&#21518;&#39564;&#38590;&#20197;&#22788;&#29702;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#65292;&#26088;&#22312;&#36817;&#20284;&#30495;&#23454;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#65288;RED-Diff&#65289;&#33258;&#28982;&#22320;&#23548;&#33268;&#27491;&#21017;&#21270;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#26102;&#38388;&#27493;&#38271;&#30340;&#21435;&#22122;&#22120;&#21516;&#26102;&#23545;&#22270;&#20687;&#26045;&#21152;&#19981;&#21516;&#30340;&#32467;&#26500;&#32422;&#26463;&#12290;&#20026;&#20102;&#34913;&#37327;&#19981;&#21516;&#26102;&#38388;&#27493;&#38271;&#30340;&#21435;&#22122;&#22120;&#30340;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#21495;-t&#30340;&#21152;&#26435;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a key pillar of foundation models in visual domains. One of their critical applications is to universally solve different downstream inverse tasks via a single diffusion prior without re-training for each task. Most inverse tasks can be formulated as inferring a posterior distribution over data (e.g., a full image) given a measurement (e.g., a masked image). This is however challenging in diffusion models since the nonlinear and iterative nature of the diffusion process renders the posterior intractable. To cope with this challenge, we propose a variational approach that by design seeks to approximate the true posterior distribution. We show that our approach naturally leads to regularization by denoising diffusion process (RED-Diff) where denoisers at different timesteps concurrently impose different structural constraints over the image. To gauge the contribution of denoisers from different timesteps, we propose a weighting mechanism based on signal-t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#37319;&#26679;&#12289;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#30340;&#20248;&#21270;&#21450;&#21160;&#24577;&#36873;&#25321;&#26080;&#26631;&#35760;&#25968;&#25454;&#31561;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02614</link><description>&lt;p&gt;
&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#21450;&#20248;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling. (arXiv:2305.02614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#37319;&#26679;&#12289;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#30340;&#20248;&#21270;&#21450;&#21160;&#24577;&#36873;&#25321;&#26080;&#26631;&#35760;&#25968;&#25454;&#31561;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#23547;&#25214;&#40657;&#31665;&#20989;&#25968;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#34429;&#28982;&#40657;&#31665;&#20989;&#25968;&#30340;&#35780;&#20272;&#25104;&#26412;&#24448;&#24448;&#24456;&#39640;&#65292;&#20294;&#20943;&#23569;&#26114;&#36149;&#26631;&#35760;&#25968;&#25454;&#30340;&#20351;&#29992;&#26159;&#29702;&#24819;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#65292;&#21033;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;BO&#29615;&#22659;&#19979;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#20854;&#20013;&#65292;&#20851;&#38190;&#22312;&#20110;&#36873;&#25321;&#39564;&#35777;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20197;&#25552;&#39640;BO&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#20248;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#37319;&#26679;&#65292;&#25105;&#20204;&#37319;&#29992;&#40657;&#31665;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#65292;&#23558;&#20854;&#20248;&#21270;&#20026;&#25152;&#37319;&#29992;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#12290;&#26356;&#36827;&#19968;&#27493;&#65292;&#36890;&#36807;&#20174;&#21160;&#24577;&#36866;&#24212;&#30340;&#26497;&#20540;&#20998;&#24067;&#20013;&#36873;&#25321;&#26410;&#26631;&#31614;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;BO&#30340;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;BO&#26041;&#27861;&#22312;&#23398;&#20064;&#21518;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;&#20351;&#20854;&#21487;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a powerful tool for seeking the global optimum of black-box functions. While evaluations of the black-box functions can be highly costly, it is desirable to reduce the use of expensive labeled data. For the first time, we introduce a teacher-student model to exploit semi-supervised learning that can make use of large amounts of unlabelled data under the context of BO. Importantly, we show that the selection of the validation and unlabeled data is key to the performance of BO. To optimize the sampling of unlabeled data, we employ a black-box parameterized sampling distribution optimized as part of the employed bi-level optimization framework. Taking one step further, we demonstrate that the performance of BO can be further improved by selecting unlabeled data from a dynamically fitted extreme value distribution. Our BO method operates in a learned latent space with reduced dimensionality, making it scalable to high-dimensional problems. The proposed approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DP-ICL&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#20445;&#35777;&#19979;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;&#32463;&#36807;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#19982;&#38750;&#31169;&#26377;ICL&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2305.01639</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private In-Context Learning. (arXiv:2305.01639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DP-ICL&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#20445;&#35777;&#19979;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;&#32463;&#36807;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#19982;&#38750;&#31169;&#26377;ICL&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#20351;&#29992;&#31169;&#26377;&#25968;&#25454;&#22686;&#24378;LLM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"DP-ICL"&#26469;&#23454;&#29616;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#20445;&#35777;&#12290;DP-ICL&#36890;&#36807;&#20351;&#29992;"report-noisy-max"&#26426;&#21046;&#22312;&#31034;&#20363;&#38598;&#21512;&#19978;&#24314;&#31435;&#22024;&#26434;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#31169;&#26377;&#25512;&#26029;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;DP-ICL&#65292;&#21457;&#29616;&#20854;&#19982;&#38750;&#31169;&#26377;ICL&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;(&lt;2%&#38477;&#32423;)&#12290;
&lt;/p&gt;
&lt;p&gt;
An important question in deploying large language models (LLMs) is how to augment LLMs with private data. We propose Differentially Private In-context Learning (DP-ICL) to enable LLMs to adapt to new tasks while maintaining privacy guarantees. DP-ICL performs private inference by establishing noisy consensus over an ensemble of exemplars using the Report-Noisy-Max mechanism. We evaluate DP-ICL on four benchmarks and find that it achieves comparable performance (&lt;2\% degradation) with non-private ICL.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.14343</link><description>&lt;p&gt;
&#23454;&#29616;&#39640;&#25928;&#21644;&#20840;&#38754;&#30340;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#24211;&#21644;&#24615;&#33021;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark. (arXiv:2304.14343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#26029;&#25512;&#36827;&#21644;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#30340;&#31215;&#32047;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#39046;&#22495;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#65292;&#21253;&#25324;&#24320;&#25918;&#25968;&#25454;&#20197;&#21508;&#31181;&#26684;&#24335;&#23384;&#22312;&#65292;&#20351;&#29992;&#22256;&#38590;&#65292;&#26497;&#23569;&#25968;&#35770;&#25991;&#20844;&#24320;&#20854;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#21450;&#24320;&#28304;&#27169;&#22411;&#32463;&#24120;&#20351;&#29992;&#19981;&#21516;&#30340;&#26694;&#26550;&#21644;&#24179;&#21488;&#65292;&#20351;&#24471;&#27604;&#36739;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36843;&#20999;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#23454;&#26045;&#21644;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32508;&#21512;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#23454;&#39564;&#24037;&#20855;&#21644;&#19968;&#20010;&#26041;&#20415;&#30340;&#24320;&#21457;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#24211;&#20013;&#65292;&#25105;&#20204;&#24050;&#32463;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#65292;&#21253;&#25324;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#24230;&#37327;&#65292;&#20197;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#32479;&#19968;&#24211;&#21644;&#22522;&#20934;&#30340;&#26377;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Total-Recon&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20174;&#38271;&#30340;&#21333;&#30446;RGBD&#35270;&#39057;&#20013;&#23454;&#29616;&#20809;&#23454;&#24863;&#30340;&#21487;&#21464;&#24418;&#22330;&#26223;&#37325;&#24314;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#23454;&#20307;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2304.12317</link><description>&lt;p&gt;
Total-Recon: &#21487;&#21464;&#24418;&#22330;&#26223;&#37325;&#24314;&#29992;&#20110;&#23454;&#20307;&#35270;&#35282;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Total-Recon: Deformable Scene Reconstruction for Embodied View Synthesis. (arXiv:2304.12317v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Total-Recon&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20174;&#38271;&#30340;&#21333;&#30446;RGBD&#35270;&#39057;&#20013;&#23454;&#29616;&#20809;&#23454;&#24863;&#30340;&#21487;&#21464;&#24418;&#22330;&#26223;&#37325;&#24314;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#23454;&#20307;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#21487;&#21464;&#24418;&#22330;&#26223;&#30340;&#21333;&#30446;&#35270;&#39057;&#20013;&#21512;&#25104;&#23454;&#20307;&#35270;&#35282;&#30340;&#20219;&#21153;&#12290;&#32473;&#23450;&#19968;&#20010;&#38271;&#36798;&#19968;&#20998;&#38047;&#30340;RGBD&#35270;&#39057;&#65292;&#25293;&#25668;&#20102;&#20154;&#19982;&#20182;&#20204;&#23456;&#29289;&#20114;&#21160;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#20174;&#22330;&#26223;&#20013;&#30340;&#36816;&#21160;&#27966;&#29983;&#20986;&#26032;&#30340;&#30456;&#26426;&#36712;&#36857;&#65292;&#28210;&#26579;&#20986;&#35813;&#22330;&#26223;&#65306;(1)&#27169;&#25311;&#30446;&#26631;&#28436;&#21592;&#35270;&#35282;&#30340;&#33258;&#25105;&#30456;&#26426;&#21644;(2)&#36319;&#38543;&#28436;&#21592;&#30340;&#31532;&#19977;&#20154;&#31216;&#30456;&#26426;&#12290;&#26500;&#24314;&#36825;&#26679;&#30340;&#31995;&#32479;&#38656;&#35201;&#37325;&#24314;&#27599;&#20010;&#28436;&#21592;&#30340;&#26681;-&#36523;&#20307;&#21644;&#20851;&#33410;&#36816;&#21160;&#65292;&#20197;&#21450;&#25903;&#25345;&#33258;&#30001;&#35270;&#28857;&#21512;&#25104;&#30340;&#22330;&#26223;&#34920;&#31034;&#12290;&#38271;&#35270;&#39057;&#26356;&#26377;&#21487;&#33021;&#20174;&#19981;&#21516;&#30340;&#35270;&#35282;&#25429;&#25417;&#21040;&#22330;&#26223;&#65288;&#26377;&#21161;&#20110;&#37325;&#24314;&#65289;&#65292;&#20294;&#20063;&#26356;&#26377;&#21487;&#33021;&#21253;&#21547;&#36739;&#22823;&#30340;&#36816;&#21160;&#65288;&#20351;&#37325;&#24314;&#21464;&#24471;&#22797;&#26434;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Total-Recon&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20174;&#38271;&#30340;&#21333;&#30446;RGBD&#35270;&#39057;&#20013;&#23454;&#29616;&#20809;&#23454;&#24863;&#30340;&#21487;&#21464;&#24418;&#22330;&#26223;&#37325;&#24314;&#26041;&#27861;&#12290;&#20851;&#38190;&#26159;&#65292;&#20026;&#20102;&#36866;&#24212;&#38271;&#35270;&#39057;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22330;&#26223;&#20998;&#23618;&#20998;&#35299;&#20026;&#32972;&#26223;&#21644;&#21069;&#26223;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#28145;&#24230;&#20272;&#35745;&#25216;&#26415;&#36827;&#34892;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the task of embodied view synthesis from monocular videos of deformable scenes. Given a minute-long RGBD video of people interacting with their pets, we render the scene from novel camera trajectories derived from the in-scene motion of actors: (1) egocentric cameras that simulate the point of view of a target actor and (2) 3rd-person cameras that follow the actor. Building such a system requires reconstructing the root-body and articulated motion of every actor, as well as a scene representation that supports free-viewpoint synthesis. Longer videos are more likely to capture the scene from diverse viewpoints (which helps reconstruction) but are also more likely to contain larger motions (which complicates reconstruction). To address these challenges, we present Total-Recon, the first method to photorealistically reconstruct deformable scenes from long monocular RGBD videos. Crucially, to scale to long videos, our method hierarchically decomposes the scene into the backgroun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#31995;&#25968;&#26469;&#21152;&#36895;&#27969;&#34892;&#30340;&#22522;&#20110;&#21453;&#21521;ODE&#35299;&#31639;&#30340;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20248;&#21270;&#26041;&#27861;&#20026;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#65288;IIA&#65289;&#65292;&#22312;&#27599;&#20010;&#21453;&#21521;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#24314;&#35758;&#38024;&#23545;&#26576;&#20123;&#36873;&#25321;&#30340;&#31995;&#25968;&#26368;&#23567;&#21270;MSE&#20989;&#25968;&#65292;&#32473;&#23450;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#22312;&#19968;&#25209;&#26679;&#26412;&#19978;&#36827;&#34892;&#29305;&#23450;&#25968;&#37327;&#30340;&#31070;&#32463;&#21151;&#33021;&#35780;&#20272;&#65288;NFEs&#65289;&#19968;&#27425;IIA&#36807;&#31243;&#21363;&#21487;&#33719;&#24471;&#26368;&#20339;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.11328</link><description>&lt;p&gt;
&#36890;&#36807;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation. (arXiv:2304.11328v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#31995;&#25968;&#26469;&#21152;&#36895;&#27969;&#34892;&#30340;&#22522;&#20110;&#21453;&#21521;ODE&#35299;&#31639;&#30340;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20248;&#21270;&#26041;&#27861;&#20026;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#65288;IIA&#65289;&#65292;&#22312;&#27599;&#20010;&#21453;&#21521;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#24314;&#35758;&#38024;&#23545;&#26576;&#20123;&#36873;&#25321;&#30340;&#31995;&#25968;&#26368;&#23567;&#21270;MSE&#20989;&#25968;&#65292;&#32473;&#23450;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#22312;&#19968;&#25209;&#26679;&#26412;&#19978;&#36827;&#34892;&#29305;&#23450;&#25968;&#37327;&#30340;&#31070;&#32463;&#21151;&#33021;&#35780;&#20272;&#65288;NFEs&#65289;&#19968;&#27425;IIA&#36807;&#31243;&#21363;&#21487;&#33719;&#24471;&#26368;&#20339;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#37319;&#26679;&#31574;&#30053;&#23581;&#35797;&#26377;&#25928;&#22320;&#35299;&#20915;&#21453;&#21521;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#12290;&#25152;&#24471;ODE&#27714;&#35299;&#22120;&#30340;&#31995;&#25968;&#30001;ODE&#20844;&#24335;&#65292;&#21453;&#21521;&#31163;&#25955;&#30340;&#26102;&#38388;&#27493;&#38271;&#21644;&#20351;&#29992;&#30340;ODE&#26041;&#27861;&#39044;&#20808;&#30830;&#23450;&#12290;&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#20248;&#21270;&#26576;&#20123;&#31995;&#25968;&#26469;&#21152;&#36895;&#20960;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;ODE&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#20248;&#21270;&#26041;&#27861;&#20026;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#65288;IIA&#65289;&#12290;&#22312;&#27599;&#20010;&#21453;&#21521;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#24314;&#35758;&#38024;&#23545;&#26576;&#20123;&#36873;&#25321;&#30340;&#31995;&#25968;&#26368;&#23567;&#21270;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20989;&#25968;&#12290;&#36890;&#36807;&#24212;&#29992;&#19968;&#32452;&#32454;&#31890;&#24230;&#26102;&#38388;&#27493;&#38271;&#30340;&#21407;&#22987;ODE&#27714;&#35299;&#22120;&#26500;&#36896;MSE&#65292;&#20174;&#21407;&#29702;&#19978;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#31215;&#20998;&#36924;&#36817;&#65292;&#20197;&#39044;&#27979;&#19979;&#19968;&#20010;&#25193;&#25955;&#38544;&#34255;&#29366;&#24577;&#65292;&#32473;&#23450;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#22312;&#19968;&#25209;&#26679;&#26412;&#19978;&#36827;&#34892;&#29305;&#23450;&#25968;&#37327;&#30340;&#31070;&#32463;&#21151;&#33021;&#35780;&#20272;&#65288;NFEs&#65289;&#19968;&#27425;IIA&#36807;&#31243;&#21363;&#21487;&#33719;&#24471;&#26368;&#20339;&#35299;
&lt;/p&gt;
&lt;p&gt;
One popular diffusion-based sampling strategy attempts to solve the reverse ordinary differential equations (ODEs) effectively. The coefficients of the obtained ODE solvers are pre-determined by the ODE formulation, the reverse discrete timesteps, and the employed ODE methods. In this paper, we consider accelerating several popular ODE-based sampling processes by optimizing certain coefficients via improved integration approximation (IIA). At each reverse timestep, we propose to minimize a mean squared error (MSE) function with respect to certain selected coefficients. The MSE is constructed by applying the original ODE solver for a set of fine-grained timesteps which in principle provides a more accurate integration approximation in predicting the next diffusion hidden state. Given a pre-trained diffusion model, the procedure for IIA for a particular number of neural functional evaluations (NFEs) only needs to be conducted once over a batch of samples. The obtained optimal solutions f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#30340;&#20445;&#24207;&#26426;&#21046;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25351;&#25968;&#26063;&#20998;&#24067;&#20197;&#25552;&#39640;&#21516;&#34892;&#35780;&#23457;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20316;&#32773;&#30340;&#21516;&#34892;&#35780;&#20998;&#21487;&#20197;&#36739;&#20934;&#30830;&#22320;&#22312;&#19981;&#38656;&#35201;&#30693;&#36947;&#20855;&#20307;&#20998;&#24067;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2304.11160</link><description>&lt;p&gt;
&#21033;&#29992;&#20445;&#24207;&#26426;&#21046;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;
&lt;/p&gt;
&lt;p&gt;
The Isotonic Mechanism for Exponential Family Estimation. (arXiv:2304.11160v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#30340;&#20445;&#24207;&#26426;&#21046;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25351;&#25968;&#26063;&#20998;&#24067;&#20197;&#25552;&#39640;&#21516;&#34892;&#35780;&#23457;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20316;&#32773;&#30340;&#21516;&#34892;&#35780;&#20998;&#21487;&#20197;&#36739;&#20934;&#30830;&#22320;&#22312;&#19981;&#38656;&#35201;&#30693;&#36947;&#20855;&#20307;&#20998;&#24067;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#25193;&#23637;&#20445;&#24207;&#26426;&#21046;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25351;&#25968;&#26063;&#20998;&#24067;&#20197;&#25552;&#39640;&#21516;&#34892;&#35780;&#23457;&#30340;&#36136;&#37327;&#12290;&#35813;&#26426;&#21046;&#21487;&#29983;&#25104;&#19982;&#21407;&#22987;&#35780;&#20998;&#25509;&#36817;&#30340;&#35843;&#25972;&#20998;&#25968;&#65292;&#24182;&#31526;&#21512;&#20316;&#32773;&#25351;&#23450;&#30340;&#25490;&#21517;&#35201;&#27714;&#65292;&#24471;&#21040;&#24191;&#27867;&#30340;&#25351;&#25968;&#26063;&#20998;&#24067;&#24212;&#29992;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#30693;&#36947;&#20855;&#20307;&#30340;&#20998;&#24067;&#24418;&#24335;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19968;&#23450;&#30340;&#25351;&#25968;&#26063;&#20998;&#24067;&#19979;&#65292;&#22914;&#26524;&#20316;&#32773;&#30340;&#25928;&#29992;&#20989;&#25968;&#37319;&#29992;&#31616;&#21333;&#30340;&#20984;&#21487;&#21152;&#20989;&#25968;&#65292;&#21017;&#28608;&#21169;&#20316;&#32773;&#25552;&#20379;&#20934;&#30830;&#30340;&#25490;&#21517;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2023, the International Conference on Machine Learning (ICML) required authors with multiple submissions to rank their submissions based on perceived quality. In this paper, we aim to employ these author-specified rankings to enhance peer review in machine learning and artificial intelligence conferences by extending the Isotonic Mechanism (Su, 2021, 2022) to exponential family distributions. This mechanism generates adjusted scores closely align with the original scores while adhering to author-specified rankings. Despite its applicability to a broad spectrum of exponential family distributions, this mechanism's implementation does not necessitate knowledge of the specific distribution form. We demonstrate that an author is incentivized to provide accurate rankings when her utility takes the form of a convex additive function of the adjusted review scores. For a certain subclass of exponential family distributions, we prove that the author reports truthfully only if the question in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21463;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#36825;&#20010;&#26041;&#27861;&#37319;&#29992;&#33258;&#21160;&#24494;&#20998;&#30340;ODE&#34920;&#36798;&#30001;&#21487;&#23398;&#20064;&#30340;&#27721;&#23494;&#23572;&#39039;&#23433;&#25490;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#29615;&#22659;&#65292;&#22312;&#38376;&#25511;&#21046;&#21644;&#27721;&#23494;&#23572;&#39039;&#21442;&#25968;&#30340;&#23398;&#20064;&#20013;&#36890;&#36807;&#31995;&#32479;&#20132;&#20114;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#27604;&#26631;&#20934;&#22522;&#20110;&#27169;&#22411;&#33258;&#30001;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#65292;&#36866;&#29992;&#20110;&#22122;&#22768;&#26102;&#21464;&#38376;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.09718</link><description>&lt;p&gt;
&#22522;&#20110;&#26679;&#26412;&#25928;&#29575;&#30340;&#27169;&#22411;&#39537;&#21160;&#37327;&#23376;&#25511;&#21046;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-efficient Model-based Reinforcement Learning for Quantum Control. (arXiv:2304.09718v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21463;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#36825;&#20010;&#26041;&#27861;&#37319;&#29992;&#33258;&#21160;&#24494;&#20998;&#30340;ODE&#34920;&#36798;&#30001;&#21487;&#23398;&#20064;&#30340;&#27721;&#23494;&#23572;&#39039;&#23433;&#25490;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#29615;&#22659;&#65292;&#22312;&#38376;&#25511;&#21046;&#21644;&#27721;&#23494;&#23572;&#39039;&#21442;&#25968;&#30340;&#23398;&#20064;&#20013;&#36890;&#36807;&#31995;&#32479;&#20132;&#20114;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#27604;&#26631;&#20934;&#22522;&#20110;&#27169;&#22411;&#33258;&#30001;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#65292;&#36866;&#29992;&#20110;&#22122;&#22768;&#26102;&#21464;&#38376;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22122;&#22768;&#26102;&#21464;&#38376;&#20248;&#21270;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20248;&#20110;&#22522;&#20110;&#27169;&#22411;&#33258;&#30001;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#26679;&#26412;&#22797;&#26434;&#24230;&#26159;&#25511;&#21046;&#22120;&#19982;&#29289;&#29702;&#31995;&#32479;&#20132;&#20114;&#30340;&#27425;&#25968;&#12290;&#20511;&#21161;&#19968;&#20010;&#24402;&#32435;&#20559;&#32622;&#65292;&#21463;&#26368;&#36817;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#24494;&#30340;ODE&#65292;&#20854;&#30001;&#21487;&#23398;&#20064;&#30340;&#27721;&#23494;&#23572;&#39039;&#23433;&#25490;&#21442;&#25968;&#21270;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#36817;&#20284;&#29615;&#22659;&#65292;&#20854;&#26102;&#21464;&#37096;&#20998;&#65288;&#21253;&#25324;&#25511;&#21046;&#65289;&#23436;&#20840;&#24050;&#30693;&#12290;&#25511;&#21046;&#22120;&#21644;&#36830;&#32493;&#26102;&#22495;&#29420;&#31435;&#21442;&#25968;&#30340;&#27721;&#23494;&#23572;&#39039;&#23398;&#20064;&#26159;&#36890;&#36807;&#19982;&#31995;&#32479;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#30340;&#12290;&#22312;&#30495;&#23454;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#22312;&#20934;&#22791;&#19968;&#20123;&#26631;&#20934;&#21333;&#37327;&#23376;&#38376;&#30340;&#38381;&#21512;&#21644;&#24320;&#25918;&#31995;&#32479;&#21160;&#24577;&#26102;&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#19982;&#26631;&#20934;&#27169;&#22411;&#33258;&#30001;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;&#20855;&#26377;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#65292;&#36825;&#21253;&#25324;&#21333;&#27425;&#27979;&#37327;&#12289;&#20219;&#24847;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#25130;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model-based reinforcement learning (RL) approach for noisy time-dependent gate optimization with improved sample complexity over model-free RL. Sample complexity is the number of controller interactions with the physical system. Leveraging an inductive bias, inspired by recent advances in neural ordinary differential equations (ODEs), we use an auto-differentiable ODE parametrised by a learnable Hamiltonian ansatz to represent the model approximating the environment whose time-dependent part, including the control, is fully known. Control alongside Hamiltonian learning of continuous time-independent parameters is addressed through interactions with the system. We demonstrate an order of magnitude advantage in the sample complexity of our method over standard model-free RL in preparing some standard unitary gates with closed and open system dynamics, in realistic numerical experiments incorporating single shot measurements, arbitrary Hilbert space truncations and uncertaint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#20998;&#23618;&#24314;&#27169;&#65292;&#21160;&#24577;&#24179;&#34913;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#65292;&#20197;&#26356;&#22909;&#22320;&#26597;&#35810;&#25968;&#25454;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.07665</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#20998;&#23618;&#24314;&#27169;&#20013;&#20027;&#21160;&#23398;&#20064;&#22238;&#24402;&#20013;&#30340;&#21160;&#24577;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling. (arXiv:2304.07665v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#20998;&#23618;&#24314;&#27169;&#65292;&#21160;&#24577;&#24179;&#34913;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#65292;&#20197;&#26356;&#22909;&#22320;&#26597;&#35810;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#26368;&#20855;&#20449;&#24687;&#30340;&#23454;&#39564;&#20197;&#23398;&#20064;&#26410;&#30693;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20998;&#23618;&#26041;&#27861;&#26469;&#21160;&#24577;&#24179;&#34913;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#65292;&#20197;&#26356;&#22909;&#22320;&#26597;&#35810;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning provides a framework to adaptively sample the most informative experiments towards learning an unknown black-box function. Various approaches of active learning have been proposed in the literature, however, they either focus on exploration or exploitation in the design space. Methods that do consider exploration-exploitation simultaneously employ fixed or ad-hoc measures to control the trade-off that may not be optimal. In this paper, we develop a Bayesian hierarchical approach to dynamically balance the exploration-exploitation trade-off as more data points are queried. We subsequently formulate an approximate Bayesian computation approach based on the linear dependence of data samples in the feature space to sample from the posterior distribution of the trade-off parameter obtained from the Bayesian hierarchical model. Simulated and real-world examples show the proposed approach achieves at least 6% and 11% average improvement when compared to pure exploration and ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; OKRidge &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31232;&#30095;&#25511;&#21046;&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#27714;&#35299;&#31232;&#30095;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#24555;&#36895;&#24615;&#65292;&#21644;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#26377;&#30528;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06686</link><description>&lt;p&gt;
OKRidge: &#29992;&#20110;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637; k &#31232;&#30095;&#23725;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
OKRidge: Scalable Optimal k-Sparse Ridge Regression for Learning Dynamical Systems. (arXiv:2304.06686v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; OKRidge &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31232;&#30095;&#25511;&#21046;&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#27714;&#35299;&#31232;&#30095;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#24555;&#36895;&#24615;&#65292;&#21644;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#26377;&#30528;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#21363;&#65292;&#30830;&#23450;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31232;&#30095;&#25511;&#21046;&#26041;&#31243;&#65292;&#36890;&#36807;&#27714;&#35299;&#31232;&#30095;&#23725;&#22238;&#24402;&#38382;&#39064;&#21487;&#20197;&#35777;&#26126;&#26368;&#20248;&#24615;&#65292;&#20197;&#30830;&#23450;&#39537;&#21160;&#22522;&#30784;&#21160;&#24577;&#30340;&#39033;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; OKRidge &#30340;&#24555;&#36895;&#31639;&#27861;&#65292;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#19979;&#30028;&#35745;&#31639;&#26041;&#27861;&#65292;&#28041;&#21450;&#38797;&#28857;&#20844;&#24335;&#65292;&#28982;&#21518;&#20351;&#29992;&#32447;&#24615;&#31995;&#32479;&#25110;&#22522;&#20110; ADMM &#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#65292;&#20854;&#20013;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#21478;&#19968;&#20010;&#32447;&#24615;&#31995;&#32479;&#21644;&#21333;&#35843;&#22238;&#24402;&#38382;&#39064;&#26469;&#26377;&#25928;&#22320;&#35745;&#31639;&#36817;&#31471;&#31639;&#23376;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21160;&#25105;&#20204;&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#27874;&#26463;&#25628;&#32034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#19988;&#36816;&#34892;&#26102;&#38388;&#27604;&#21830;&#19994;&#27714;&#35299;&#22120; Gurobi &#35299;&#20915;&#30340;&#29616;&#26377; MIP&#20844;&#24335;&#36816;&#34892;&#26102;&#38388;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider an important problem in scientific discovery, identifying sparse governing equations for nonlinear dynamical systems. This involves solving sparse ridge regression problems to provable optimality in order to determine which terms drive the underlying dynamics. We propose a fast algorithm, OKRidge, for sparse ridge regression, using a novel lower bound calculation involving, first, a saddle point formulation, and from there, either solving (i) a linear system or (ii) using an ADMM-based approach, where the proximal operators can be efficiently evaluated by solving another linear system and an isotonic regression problem. We also propose a method to warm-start our solver, which leverages a beam search. Experimentally, our methods attain provable optimality with run times that are orders of magnitude faster than those of the existing MIP formulations solved by the commercial solver Gurobi.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#33021;&#37327;&#22522;&#30784;&#27169;&#22411;&#21644;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;2D&#24773;&#26223;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06094</link><description>&lt;p&gt;
&#33021;&#37327;&#24341;&#23548;&#30340;&#29109;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Energy-guided Entropic Neural Optimal Transport. (arXiv:2304.06094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#33021;&#37327;&#22522;&#30784;&#27169;&#22411;&#21644;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;2D&#24773;&#26223;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#30784;&#27169;&#22411;&#65288;EBMs&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#24050;&#32463;&#26377;&#25968;&#21313;&#24180;&#30340;&#21382;&#21490;&#12290;&#33258;&#20004;&#21315;&#24180;&#20195;&#36215;&#65292;&#19968;&#30452;&#26377;&#24456;&#22810;&#39640;&#25928;&#30340;&#26041;&#27861;&#36890;&#36807;&#33021;&#37327;&#21183;&#65288;&#38750;&#24402;&#19968;&#21270;&#30340;&#20284;&#28982;&#20989;&#25968;&#65289;&#26469;&#35299;&#20915;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#31070;&#32463;OT&#27714;&#35299;&#22120;&#65292;&#21463;&#21040;&#30340;&#25506;&#32034;&#35201;&#23569;&#24471;&#22810;&#65292;&#20165;&#26377;&#19968;&#20123;&#36817;&#26399;&#30340;&#30740;&#31350;&#65288;&#19981;&#21253;&#25324;&#21033;&#29992;OT&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#38382;&#39064;&#30340;WGAN&#26041;&#27861;&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24357;&#21512;&#20102;EBMs&#21644;&#29109;&#27491;&#21017;&#21270;OT&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#21033;&#29992;&#21069;&#32773;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#25216;&#26415;&#25913;&#36827;&#26469;&#20016;&#23500;&#21518;&#32773;&#12290;&#25105;&#20204;&#22312;2D&#24773;&#26223;&#21644;&#26631;&#20934;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#31616;&#21333;&#36215;&#35265;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#31616;&#30701;&#21644;&#38271;&#36305;&#30340;EBMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models (EBMs) are known in the Machine Learning community for the decades. Since the seminal works devoted to EBMs dating back to the noughties there have been appearing a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in particular, neural OT solvers is much less explored and limited by few recent works (excluding WGAN based approaches which utilize OT as a loss function and do not model OT maps themselves). In our work, we bridge the gap between EBMs and Entropy-regularized OT. We present the novel methodology which allows utilizing the recent developments and technical improvements of the former in order to enrich the latter. We validate the applicability of our method on toy 2D scenarios as well as standard unpaired image-to-image translation problems. For the sake of simplicity, we choose simple short- and long- run EBMs as a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#35780;&#20272;&#65292;&#25214;&#21040;&#20102;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.02858</link><description>&lt;p&gt;
&#38754;&#21521;&#31867;&#21035;&#19981;&#22343;&#38382;&#39064;&#30340;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#27169;&#22411;&#32508;&#36848;&#65306;&#32452;&#21512;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation. (arXiv:2304.02858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#35780;&#20272;&#65292;&#25214;&#21040;&#20102;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65288;CI&#65289;&#26159;&#25351;&#23646;&#20110;&#19968;&#20010;&#31867;&#30340;&#35266;&#27979;&#20540;&#25968;&#37327;&#20302;&#20110;&#20854;&#20182;&#31867;&#30340;&#25968;&#37327;&#12290;&#38598;&#25104;&#23398;&#20064;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#19968;&#20123;&#31574;&#30053;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22686;&#24378;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#24320;&#21457;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#12290;&#26412;&#25991;&#23545;&#29992;&#20110;&#35299;&#20915;&#22522;&#20934;CI&#38382;&#39064;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35745;&#31639;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;CI&#38382;&#39064;&#30340;10&#20010;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;10&#20010;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#25552;&#39640;&#20998;&#31867;&#25928;&#26524;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance (CI) in classification problems arises when the number of observations belonging to one class is lower than the other classes. Ensemble learning that combines multiple models to obtain a robust model has been prominently used with data augmentation methods to address class imbalance problems. In the last decade, a number of strategies have been added to enhance ensemble learning and data augmentation methods, along with new methods such as generative adversarial networks (GANs). A combination of these has been applied in many studies, but the true rank of different combinations would require a computational review. In this paper, we present a computational review to evaluate data augmentation and ensemble learning methods used to address prominent benchmark CI problems. We propose a general framework that evaluates 10 data augmentation and 10 ensemble learning methods for CI problems. Our objective was to identify the most effective combination for improving classificat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEENN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26102;&#38388;&#27493;&#25968;&#36827;&#34892;&#32454;&#31890;&#24230;&#35843;&#25972;&#65292;&#20197;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#24182;&#25552;&#39640;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;SEENN&#36798;&#21040;&#20102;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24230;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01230</link><description>&lt;p&gt;
SEENN: &#23454;&#29616;&#26102;&#38388;&#32534;&#30721;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SEENN: Towards Temporal Spiking Early-Exit Neural Networks. (arXiv:2304.01230v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEENN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26102;&#38388;&#27493;&#25968;&#36827;&#34892;&#32454;&#31890;&#24230;&#35843;&#25972;&#65292;&#20197;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#24182;&#25552;&#39640;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;SEENN&#36798;&#21040;&#20102;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24230;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22240;&#20854;&#29983;&#29289;&#23398;&#29305;&#24615;&#25104;&#20026;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#26367;&#20195;&#21697;&#65292;&#26368;&#36817;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;SNNs&#26082;&#36153;&#29992;&#25928;&#30410;&#21448;&#26131;&#20110;&#37096;&#32626;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#29992;&#20108;&#36827;&#21046;&#33033;&#20914;&#20197;&#31354;&#38388;&#21644;&#26102;&#38388;&#26041;&#24335;&#22788;&#29702;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;SNNs&#20013;&#30340;&#20449;&#24687;&#23481;&#37327;&#21463;&#21040;&#26102;&#38388;&#27493;&#39588;&#25968;&#37327;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;SNNs&#20013;&#26102;&#38388;&#27493;&#39588;&#25968;&#37327;&#30340;&#32454;&#31890;&#24230;&#35843;&#25972;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#27493;&#25968;&#35270;&#20026;&#19968;&#20010;&#21464;&#37327;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#36755;&#20837;&#26679;&#26412;&#26469;&#20943;&#23569;&#20887;&#20313;&#26102;&#38388;&#27493;&#39588;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;&#26089;&#26399;&#36864;&#20986;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SEENN&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;&#36866;&#24403;&#30340;&#26102;&#38388;&#27493;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SEENN-I&#65292;&#23427;&#20351;&#29992;&#32622;&#20449;&#24230;&#38408;&#20540;&#26469;&#36807;&#28388;&#19981;&#30830;&#23450;&#30340;&#39044;&#27979;&#65292;&#20197;&#21450;SEENN-II&#65292;&#23427;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30830;&#23450;&#26102;&#38388;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;SEENN&#27604;&#20256;&#32479;SNNs&#26356;&#26377;&#25928;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have recently become more popular as a biologically plausible substitute for traditional Artificial Neural Networks (ANNs). SNNs are cost-efficient and deployment-friendly because they process input in both spatial and temporal manners using binary spikes. However, we observe that the information capacity in SNNs is affected by the number of timesteps, leading to an accuracy-efficiency tradeoff. In this work, we study a fine-grained adjustment of the number of timesteps in SNNs. Specifically, we treat the number of timesteps as a variable conditioned on different input samples to reduce redundant timesteps for certain data. We call our method Spiking Early-Exit Neural Networks (SEENNs). To determine the appropriate number of timesteps, we propose SEENN-I which uses a confidence score thresholding to filter out the uncertain predictions, and SEENN-II which determines the number of timesteps by reinforcement learning. Moreover, we demonstrate that SEENN is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#20013;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#27880; Kullback - Leibler &#25955;&#24230;&#20013;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39057;&#32321;&#25628;&#32034;&#27491;&#21017;&#21270;&#30340;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.16721</link><description>&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#20877;&#25506;&#65306;Kullback - Leibler &#25955;&#24230;&#20013;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#21644;&#24615;&#33021;&#20445;&#35777;&#30340;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Maximum likelihood method revisited: Gauge symmetry in Kullback -- Leibler divergence and performance-guaranteed regularization. (arXiv:2303.16721v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#20013;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#27880; Kullback - Leibler &#25955;&#24230;&#20013;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39057;&#32321;&#25628;&#32034;&#27491;&#21017;&#21270;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#26159;&#20272;&#35745;&#25968;&#25454;&#32972;&#21518;&#27010;&#29575;&#30340;&#26368;&#30693;&#21517;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#33719;&#24471;&#19982;&#32463;&#39564;&#20998;&#24067;&#26368;&#25509;&#36817;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#12290;&#28982;&#21518;&#65292;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#36807;&#24230;&#25509;&#36817;&#38169;&#35823;&#30340;&#27010;&#29575;&#65292;&#20294;&#26159;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#30693;&#20043;&#29978;&#23569;&#12290;&#27491;&#21017;&#21270;&#30340;&#24605;&#24819;&#31867;&#20284;&#20110;&#32416;&#38169;&#20195;&#30721;&#65292;&#36890;&#36807;&#23558;&#27425;&#20248;&#35299;&#19982;&#38169;&#35823;&#25509;&#25910;&#21040;&#30340;&#20195;&#30721;&#28151;&#21512;&#65292;&#33719;&#24471;&#26368;&#20248;&#35299;&#30721;&#12290;&#32416;&#38169;&#20195;&#30721;&#20013;&#30340;&#26368;&#20248;&#35299;&#30721;&#26159;&#22522;&#20110;&#35268;&#33539;&#23545;&#31216;&#24615;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#20851;&#27880; Kullback - Leibler &#25955;&#24230;&#20013;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#20013;&#30340;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#27491;&#21017;&#21270;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#26368;&#20248;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#39057;&#32321;&#25628;&#32034;&#27491;&#21017;&#21270;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The maximum likelihood method is the best-known method for estimating the probabilities behind the data. However, the conventional method obtains the probability model closest to the empirical distribution, resulting in overfitting. Then regularization methods prevent the model from being excessively close to the wrong probability, but little is known systematically about their performance. The idea of regularization is similar to error-correcting codes, which obtain optimal decoding by mixing suboptimal solutions with an incorrectly received code. The optimal decoding in error-correcting codes is achieved based on gauge symmetry. We propose a theoretically guaranteed regularization in the maximum likelihood method by focusing on a gauge symmetry in Kullback -- Leibler divergence. In our approach, we obtain the optimal model without the need to search for hyperparameters frequently appearing in regularization.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#20869;&#37096;&#27979;&#37327;&#20013;&#37325;&#26500;&#26925;&#22278;&#38382;&#39064;&#20013;&#30340;&#23548;&#30005;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#36830;&#32493;&#21644;&#32463;&#39564;&#25439;&#22833;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#25968;&#25454;&#22122;&#22768;&#30340;&#20248;&#24322;&#31283;&#23450;&#24615;&#20197;&#21450;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16454</link><description>&lt;p&gt;
&#28151;&#21512;&#26368;&#23567;&#20108;&#20056;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#37096;&#27979;&#37327;&#19979;&#30340;&#23548;&#30005;&#29575;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Conductivity Imaging from Internal Measurements with Mixed Least-Squares Deep Neural Networks. (arXiv:2303.16454v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#20869;&#37096;&#27979;&#37327;&#20013;&#37325;&#26500;&#26925;&#22278;&#38382;&#39064;&#20013;&#30340;&#23548;&#30005;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#36830;&#32493;&#21644;&#32463;&#39564;&#25439;&#22833;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#25968;&#25454;&#22122;&#22768;&#30340;&#20248;&#24322;&#31283;&#23450;&#24615;&#20197;&#21450;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#19968;&#20010;&#20869;&#37096;&#27979;&#37327;&#20013;&#37325;&#26500;&#26925;&#22278;&#38382;&#39064;&#20013;&#30340;&#23548;&#30005;&#29575;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#25511;&#21046;&#26041;&#31243;&#30340;&#28151;&#21512;&#25913;&#36896;&#65292;&#24182;&#21033;&#29992;&#26631;&#20934;&#30340;&#26368;&#23567;&#20108;&#20056;&#30446;&#26631;&#20989;&#25968;&#21516;&#26102;&#36817;&#20284;&#23548;&#30005;&#29575;&#21644;&#36890;&#37327;&#65292;&#20197;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#35797;&#25506;&#20989;&#25968;&#12290;&#25105;&#20204;&#23545;&#36830;&#32493;&#21644;&#32463;&#39564;&#25439;&#22833;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#21253;&#25324;&#36890;&#36807;&#22122;&#22768;&#27700;&#24179;&#12289;&#21508;&#31181;&#24809;&#32602;&#21442;&#25968;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21442;&#25968;&#65288;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#21442;&#25968;&#36793;&#30028;&#65289;&#26174;&#24335;&#22320;&#20272;&#35745;&#35823;&#24046;&#30340;&#20005;&#26684;&#20272;&#35745;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#35813;&#26041;&#27861;&#30340;&#19981;&#21516;&#29305;&#28857;&#65292;&#20363;&#22914;&#23545;&#20110;&#25968;&#25454;&#22122;&#22768;&#30340;&#20248;&#24322;&#31283;&#23450;&#24615;&#20197;&#21450;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we develop a novel approach using deep neural networks to reconstruct the conductivity distribution in elliptic problems from one internal measurement. The approach is based on a mixed reformulation of the governing equation and utilizes the standard least-squares objective to approximate the conductivity and flux simultaneously, with deep neural networks as ansatz functions. We provide a thorough analysis of the neural network approximations for both continuous and empirical losses, including rigorous error estimates that are explicit in terms of the noise level, various penalty parameters and neural network architectural parameters (depth, width and parameter bound). We also provide extensive numerical experiments in two- and multi-dimensions to illustrate distinct features of the approach, e.g., excellent stability with respect to data noise and capability of solving high-dimensional problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#30450;&#30446;&#38450;&#24481;&#26694;&#26550;&#65288;BDMAE&#65289;&#65292;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#38450;&#24481;&#30450;&#30446;&#21518;&#38376;&#25915;&#20987;&#65292;&#19981;&#38656;&#35201;&#39564;&#35777;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#36890;&#36807;&#27979;&#35797;&#22270;&#20687;&#21644; MAE &#36824;&#21407;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#26631;&#31614;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.15564</link><description>&lt;p&gt;
&#25513;&#30721;&#36824;&#21407;&#25216;&#26415;&#65306;&#21033;&#29992;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#22312;&#27979;&#35797;&#26102;&#38450;&#24481;&#30450;&#30446;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder. (arXiv:2303.15564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#30450;&#30446;&#38450;&#24481;&#26694;&#26550;&#65288;BDMAE&#65289;&#65292;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#38450;&#24481;&#30450;&#30446;&#21518;&#38376;&#25915;&#20987;&#65292;&#19981;&#38656;&#35201;&#39564;&#35777;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#36890;&#36807;&#27979;&#35797;&#22270;&#20687;&#21644; MAE &#36824;&#21407;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#26631;&#31614;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#20250;&#36890;&#36807;&#22312;&#22270;&#20687;&#19978;&#21472;&#21152;&#29305;&#27530;&#30340;&#35302;&#21457;&#22120;&#26469;&#24694;&#24847;&#25805;&#32437;&#27169;&#22411;&#34892;&#20026;&#65292;&#36825;&#31216;&#20026;&#21518;&#38376;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#19968;&#20123;&#39564;&#35777;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#20363;&#22914;&#24403;&#27169;&#22411;&#20316;&#20026;&#20113;&#26381;&#21153;&#25552;&#20379;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#33268;&#21147;&#20110;&#27979;&#35797;&#26102;&#30340;&#30450;&#30446;&#21518;&#38376;&#38450;&#24481;&#23454;&#36341;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#40657;&#30418;&#27169;&#22411;&#12290;&#27599;&#20010;&#27979;&#35797;&#22270;&#20687;&#30340;&#30495;&#23454;&#26631;&#31614;&#38656;&#35201;&#20174;&#21487;&#30097;&#27169;&#22411;&#30340;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#24674;&#22797;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#20687;&#31354;&#38388;&#20013;&#21551;&#21457;&#24335;&#35302;&#21457;&#22120;&#25628;&#32034;&#19981;&#36866;&#29992;&#20110;&#22797;&#26434;&#35302;&#21457;&#22120;&#25110;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#29255;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#36890;&#29992;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#30450;&#30446;&#38450;&#24481;&#26694;&#26550;&#65288;BDMAE&#65289;&#65292;&#36890;&#36807;&#27979;&#35797;&#22270;&#20687;&#21644; MAE &#36824;&#21407;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#26631;&#31614;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are vulnerable to backdoor attacks, where an adversary maliciously manipulates the model behavior through overlaying images with special triggers. Existing backdoor defense methods often require accessing a few validation data and model parameters, which are impractical in many real-world applications, e.g., when the model is provided as a cloud service. In this paper, we address the practical task of blind backdoor defense at test time, in particular for black-box models. The true label of every test image needs to be recovered on the fly from the hard label predictions of a suspicious model. The heuristic trigger search in image space, however, is not scalable to complex triggers or high image resolution. We circumvent such barrier by leveraging generic image generation models, and propose a framework of Blind Defense with Masked AutoEncoder (BDMAE). It uses the image structural similarity and label consistency between the test image and MAE restorations to detec
&lt;/p&gt;</description></item><item><title>ID3PM&#26041;&#27861;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#30340;&#38543;&#26426;&#24615;&#36136;&#26469;&#20135;&#29983;&#40657;&#30418;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#39640;&#24230;&#21487;&#25511;&#30340;&#21453;&#28436;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#19988;&#22810;&#26679;&#30340;&#36755;&#20986;&#65292;&#36866;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13006</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#21487;&#25511;&#22320;&#21453;&#21521;&#40657;&#30418;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Controllable Inversion of Black-Box Face-Recognition Models via Diffusion. (arXiv:2303.13006v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13006
&lt;/p&gt;
&lt;p&gt;
ID3PM&#26041;&#27861;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#30340;&#38543;&#26426;&#24615;&#36136;&#26469;&#20135;&#29983;&#40657;&#30418;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#39640;&#24230;&#21487;&#25511;&#30340;&#21453;&#28436;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#19988;&#22810;&#26679;&#30340;&#36755;&#20986;&#65292;&#36866;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#23558;&#20154;&#33080;&#22270;&#20687;&#23884;&#20837;&#20302;&#32500;&#36523;&#20221;&#21521;&#37327;&#20013;&#65292;&#21253;&#21547;&#36523;&#20221;&#29305;&#24449;&#30340;&#25277;&#35937;&#32534;&#30721;&#65292;&#36825;&#20123;&#29305;&#24449;&#20801;&#35768;&#21306;&#20998;&#20010;&#20307;&#12290;&#25105;&#20204;&#38754;&#20020;&#30528;&#22312;&#27809;&#26377;&#23436;&#20840;&#27169;&#22411;&#35775;&#38382;&#30340;&#24773;&#20917;&#19979;&#65288;&#21363;&#40657;&#30418;&#35774;&#32622;&#19979;&#65289;&#21453;&#21521;&#39044;&#35757;&#32451;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;&#24050;&#32463;&#26377;&#35768;&#22810;&#26041;&#27861;&#25552;&#20986;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20005;&#37325;&#30340;&#32570;&#38519;&#65292;&#22914;&#32570;&#20047;&#29616;&#23454;&#36755;&#20986;&#12289;&#25512;&#29702;&#26102;&#38388;&#38271;&#20197;&#21450;&#23545;&#25968;&#25454;&#38598;&#21644;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#21487;&#35775;&#38382;&#24615;&#26377;&#24378;&#28872;&#30340;&#35201;&#27714;&#12290;&#36890;&#36807;&#23545;&#40657;&#30418;&#21453;&#28436;&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#28431;&#27934;&#30340;&#33258;&#28982;&#28044;&#29616;&#65292;&#24182;&#19988;&#21363;&#20351;&#27809;&#26377;&#36523;&#20221;&#29305;&#24449;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#21453;&#21521;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#36523;&#20221;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;ID3PM&#65289;&#65292;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#38543;&#26426;&#24615;&#36136;&#26469;&#20135;&#29983;&#40657;&#30418;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#39640;&#24230;&#21487;&#25511;&#30340;&#21453;&#28436;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#19988;&#22810;&#26679;&#30340;&#36755;&#20986;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face recognition models embed a face image into a low-dimensional identity vector containing abstract encodings of identity-specific facial features that allow individuals to be distinguished from one another. We tackle the challenging task of inverting the latent space of pre-trained face recognition models without full model access (i.e. black-box setting). A variety of methods have been proposed in literature for this task, but they have serious shortcomings such as a lack of realistic outputs, long inference times, and strong requirements for the data set and accessibility of the face recognition model. Through an analysis of the black-box inversion problem, we show that the conditional diffusion model loss naturally emerges and that we can effectively sample from the inverse distribution even without an identity-specific loss. Our method, named identity denoising diffusion probabilistic model (ID3PM), leverages the stochastic nature of the denoising diffusion process to produce hi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20248;&#21270;&#20989;&#25968;&#36229;&#27700;&#24179;&#38598;&#22312;&#32593;&#32476;&#31867;&#31574;&#30053;&#21644;&#34920;&#26684;&#24335;&#19979;&#22987;&#32456;&#26159;&#36830;&#36890;&#30340;&#65292;&#24182;&#24212;&#29992;&#27492;&#32467;&#26524;&#23548;&#20986;&#20102;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.12981</link><description>&lt;p&gt;
(&#28145;&#24230;)&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36830;&#25509;&#36229;&#27700;&#24179;&#38598;&#21450;&#20854;&#22312;&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Connected Superlevel Set in (Deep) Reinforcement Learning and its Application to Minimax Theorems. (arXiv:2303.12981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20248;&#21270;&#20989;&#25968;&#36229;&#27700;&#24179;&#38598;&#22312;&#32593;&#32476;&#31867;&#31574;&#30053;&#21644;&#34920;&#26684;&#24335;&#19979;&#22987;&#32456;&#26159;&#36830;&#36890;&#30340;&#65292;&#24182;&#24212;&#29992;&#27492;&#32467;&#26524;&#23548;&#20986;&#20102;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#30340;&#20248;&#21270;&#20989;&#25968;&#22270;&#26223;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31574;&#30053;&#21442;&#25968;&#30340;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36229;&#27700;&#24179;&#38598;&#65292;&#22312;&#32593;&#32476;&#31867;&#31574;&#30053;&#21644;&#34920;&#26684;&#24335;&#19979;&#22987;&#32456;&#26159;&#36830;&#36890;&#30340;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22870;&#21169;&#20316;&#20026;&#36229;&#27700;&#24179;&#38598;&#30340;&#20989;&#25968;&#28385;&#36275;&#26356;&#24378;&#30340;&#8220;&#31561;&#36830;&#36890;&#8221;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#36825;&#20123;&#36229;&#27700;&#24179;&#38598;&#30340;&#36830;&#36890;&#24615;&#32467;&#26524;&#65292;&#23548;&#20986;&#20102;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20219;&#20309;&#19968;&#20010;&#22312;&#19968;&#20391;&#20026;&#20984;&#30340;&#65292;&#21478;&#19968;&#20391;&#20026;&#31561;&#36830;&#36890;&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#37117;&#26377;&#32435;&#20160;&#22343;&#34913;&#12290;&#36825;&#20123;&#32467;&#35770;&#26159;&#26032;&#39062;&#32780;&#19988;&#20043;&#21069;&#26410;&#30693;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this paper is to improve the understanding of the optimization landscape for policy optimization problems in reinforcement learning. Specifically, we show that the superlevel set of the objective function with respect to the policy parameter is always a connected set both in the tabular setting and under policies represented by a class of neural networks. In addition, we show that the optimization objective as a function of the policy parameter and reward satisfies a stronger "equiconnectedness" property. To our best knowledge, these are novel and previously unknown discoveries.  We present an application of the connectedness of these superlevel sets to the derivation of minimax theorems for robust reinforcement learning. We show that any minimax optimization program which is convex on one side and is equiconnected on the other side observes the minimax equality (i.e. has a Nash equilibrium). We find that this exact structure is exhibited by an interesting robust reinforceme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#26816;&#27979;&#20986;&#22810;&#31181;&#25104;&#20687;&#21644;&#29983;&#29289;&#21464;&#37327;&#20013;&#30340;&#24322;&#24120;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#30740;&#31350;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#30142;&#30149;&#12290;</title><link>http://arxiv.org/abs/2303.12706</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#36328;&#22810;&#31181;&#25104;&#20687;&#27169;&#24577;&#36827;&#34892;&#35268;&#33539;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities. (arXiv:2303.12706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#26816;&#27979;&#20986;&#22810;&#31181;&#25104;&#20687;&#21644;&#29983;&#29289;&#21464;&#37327;&#20013;&#30340;&#24322;&#24120;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#30740;&#31350;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#30142;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24120;&#35265;&#31070;&#32463;&#30142;&#30149;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#30142;&#30149;&#24322;&#36136;&#24615;&#65292;&#21253;&#25324;&#30149;&#22240;&#12289;&#31070;&#32463;&#25104;&#20687;&#29305;&#24449;&#12289;&#21512;&#24182;&#30151;&#25110;&#22522;&#22240;&#21464;&#24322;&#30340;&#24046;&#24322;&#12290;&#35268;&#33539;&#24314;&#27169;&#24050;&#25104;&#20026;&#30740;&#31350;&#36825;&#31181;&#20154;&#32676;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#20854;&#20013;&#23545;&#29983;&#29702;&#31995;&#32479;&#30340;&#8220;&#27491;&#24120;&#8221;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#20010;&#20307;&#23618;&#38754;&#19978;&#26816;&#27979;&#19982;&#30142;&#30149;&#30149;&#29702;&#30456;&#20851;&#30340;&#20559;&#24046;&#12290;&#23545;&#20110;&#35768;&#22810;&#24322;&#36136;&#24615;&#30142;&#30149;&#65292;&#25105;&#20204;&#39044;&#35745;&#20250;&#35266;&#23519;&#21040;&#22810;&#31181;&#31070;&#32463;&#25104;&#20687;&#21644;&#29983;&#29289;&#21464;&#37327;&#30340;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#35268;&#33539;&#27169;&#22411;&#20027;&#35201;&#26159;&#20026;&#20102;&#30740;&#31350;&#21333;&#19968;&#25104;&#20687;&#27169;&#24577;&#32780;&#24320;&#21457;&#30340;&#12290;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#22810;&#20010;&#27169;&#24577;&#30340;&#21464;&#37327;&#20013;&#32858;&#21512;&#24322;&#24120;&#24615;&#65292;&#24182;&#19988;&#27604;&#21333;&#27169;&#24335;&#22522;&#32447;&#26356;&#33021;&#26816;&#27979;&#21040;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#26816;&#27979;T1&#21644;DTI&#25968;&#25454;&#20013;&#30340;&#20010;&#20307;&#23618;&#38754;&#20559;&#24046;&#30340;&#22810;&#27169;&#24577;VAE&#35268;&#33539;&#27169;&#22411;&#12290;&#19982;&#21333;&#27169;&#24335;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#26816;&#27979;&#21040;&#36731;&#24230;&#35748;&#30693;&#21463;&#25439;&#30340;&#21463;&#35797;&#32773;&#20013;&#30340;&#20559;&#24046;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#29992;&#20110;&#30142;&#30149;&#24322;&#36136;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges of studying common neurological disorders is disease heterogeneity including differences in causes, neuroimaging characteristics, comorbidities, or genetic variation. Normative modelling has become a popular method for studying such cohorts where the 'normal' behaviour of a physiological system is modelled and can be used at subject level to detect deviations relating to disease pathology. For many heterogeneous diseases, we expect to observe abnormalities across a range of neuroimaging and biological variables. However, thus far, normative models have largely been developed for studying a single imaging modality. We aim to develop a multi-modal normative modelling framework where abnormality is aggregated across variables of multiple modalities and is better able to detect deviations than uni-modal baselines. We propose two multi-modal VAE normative models to detect subject level deviations across T1 and DTI data. Our proposed models were better able to detect di
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#26368;&#22823;&#21270;&#20581;&#24247;&#32467;&#26524;&#21644;&#27835;&#30103;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DecompPI&#30340;&#26032;&#31639;&#27861;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12206</link><description>&lt;p&gt;
&#34892;&#20026;&#20581;&#24247;&#20010;&#24615;&#21270;&#20171;&#20837;&#30340;&#25919;&#31574;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization for Personalized Interventions in Behavioral Health. (arXiv:2303.12206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12206
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#26368;&#22823;&#21270;&#20581;&#24247;&#32467;&#26524;&#21644;&#27835;&#30103;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DecompPI&#30340;&#26032;&#31639;&#27861;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#23450;&#20041;&#65306;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#65292;&#36890;&#36807;&#25945;&#32946;&#65292;&#28608;&#21169;&#65292;&#25552;&#37266;&#21644;&#22806;&#23637;&#65292;&#26377;&#26395;&#26174;&#30528;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20171;&#20837;&#20855;&#26377;&#25104;&#26412;&#21644;&#33021;&#21147;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#24739;&#32773;&#20010;&#24615;&#21270;&#20171;&#20837;&#20197;&#26368;&#22823;&#21270;&#26576;&#31181;&#38271;&#26399;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#26041;&#27861;/&#32467;&#26524;&#65306;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26469;&#33258;&#22686;&#24378;&#23398;&#20064;&#25991;&#29486;&#30340;&#36890;&#29992;&#26080;&#27169;&#22411;&#26041;&#27861;&#23545;&#20110;&#21307;&#30103;&#24212;&#29992;&#26469;&#35828;&#36807;&#20110;&#25968;&#25454;&#23494;&#38598;&#65292;&#32780;&#26356;&#31616;&#21333;&#30340;&#36172;&#33218;&#38382;&#39064;&#26041;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24573;&#30053;&#20102;&#38271;&#26399;&#24739;&#32773;&#21160;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;DecompPI&#65292;&#23427;&#36817;&#20284;&#20110;&#19968;&#27493;&#25919;&#31574;&#36845;&#20195;&#12290;&#23454;&#29616;DecompPI&#21482;&#38656;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#31181;&#33258;&#28982;&#30340;&#32467;&#26500;&#20551;&#35774;&#19979;&#65292;DecompPI&#21487;&#20197;&#33719;&#24471;&#31639;&#27861;&#22797;&#26434;&#24230;&#30340;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;
Problem definition: Behavioral health interventions, delivered through digital platforms, have the potential to significantly improve health outcomes, through education, motivation, reminders, and outreach. We study the problem of optimizing personalized interventions for patients to maximize some long-term outcome, in a setting where interventions are costly and capacity-constrained.  Methodology/results: This paper provides a model-free approach to solving this problem. We find that generic model-free approaches from the reinforcement learning literature are too data intensive for healthcare applications, while simpler bandit approaches make progress at the expense of ignoring long-term patient dynamics. We present a new algorithm we dub DecompPI that approximates one step of policy iteration. Implementing DecompPI simply consists of a prediction task from offline data, alleviating the need for online experimentation. Theoretically, we show that under a natural set of structural assu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#20266;&#30417;&#30563;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#26080;&#30417;&#30563;&#22270;&#29255;&#21040;&#22270;&#29255;&#32763;&#35793;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.10310</link><description>&lt;p&gt;
&#20266;&#30417;&#30563;&#24230;&#37327;&#65306;&#22312;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013;&#35780;&#20272;&#26080;&#30417;&#30563;&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks. (arXiv:2303.10310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#20266;&#30417;&#30563;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#26080;&#30417;&#30563;&#22270;&#29255;&#21040;&#22270;&#29255;&#32763;&#35793;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#39640;&#25928;&#24615;&#21462;&#20915;&#20110;&#35775;&#38382;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#30340;&#30456;&#21516;&#39046;&#22495;&#19978;&#27979;&#35797;&#25968;&#25454;&#12290;&#24403;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26032;&#25968;&#25454;&#26102;&#65292;&#20998;&#31867;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25910;&#38598;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#24182;&#20174;&#22836;&#35757;&#32451;&#26032;&#20998;&#31867;&#22120;&#32791;&#26102;&#12289;&#26114;&#36149;&#65292;&#26377;&#26102;&#26159;&#19981;&#21487;&#34892;&#25110;&#19981;&#21487;&#33021;&#30340;&#12290;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#22270;&#20687;&#23545;&#22270;&#20687; (UI2I) &#32763;&#35793;&#27169;&#22411;&#23558;&#36755;&#20837;&#22270;&#20687;&#20174;&#26410;&#26631;&#35760;&#30340;&#22495;&#36716;&#25442;&#20026;&#26631;&#35760;&#22495;&#26469;&#22788;&#29702;&#36825;&#20010;&#25968;&#25454;&#22495;&#28418;&#31227;&#38382;&#39064;&#12290;&#36825;&#20123;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#38382;&#39064;&#22312;&#20110;&#23427;&#20204;&#26159;&#26080;&#30417;&#30563;&#30340;&#12290;&#30001;&#20110;&#32570;&#23569;&#27880;&#37322;&#65292;&#26080;&#27861;&#20351;&#29992;&#20256;&#32479;&#30340;&#30417;&#30563;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#32763;&#35793;&#27169;&#22411;&#20197;&#36873;&#25321;&#26368;&#20339;&#30340;&#26816;&#26597;&#28857;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20266;&#30417;&#30563;&#24230;&#37327;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013; UI2I &#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to classify images accurately and efficiently is dependent on having access to large labeled datasets and testing on data from the same domain that the model is trained on. Classification becomes more challenging when dealing with new data from a different domain, where collecting a large labeled dataset and training a new classifier from scratch is time-consuming, expensive, and sometimes infeasible or impossible. Cross-domain classification frameworks were developed to handle this data domain shift problem by utilizing unsupervised image-to-image (UI2I) translation models to translate an input image from the unlabeled domain to the labeled domain. The problem with these unsupervised models lies in their unsupervised nature. For lack of annotations, it is not possible to use the traditional supervised metrics to evaluate these translation models to pick the best-saved checkpoint model. In this paper, we introduce a new method called Pseudo Supervised Metrics that was desig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24863;&#30693;&#12290;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.09874</link><description>&lt;p&gt;
&#22270;&#20687;&#32479;&#35745;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#20851;&#32852;&#20851;&#31995;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Disentangling the Link Between Image Statistics and Human Perception. (arXiv:2303.09874v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24863;&#30693;&#12290;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;20&#19990;&#32426;50&#24180;&#20195;&#65292;&#38669;&#21202;&#26031;&#24052;&#27931;&#21644;&#24343;&#38647;&#24503;&#38463;&#29305;&#32435;&#22827;&#25552;&#20986;&#20102;&#24863;&#23448;&#31995;&#32479;&#21644;&#23427;&#20204;&#22914;&#20309;&#36866;&#24212;&#29615;&#22659;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#26089;&#26399;&#35270;&#35273;&#30340;&#36827;&#21270;&#26159;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20256;&#36882;&#20851;&#20110;&#36755;&#20837;&#20449;&#21495;&#30340;&#20449;&#24687;&#12290;&#25353;&#29031;&#39321;&#20892;&#30340;&#23450;&#20041;&#65292;&#36825;&#20123;&#20449;&#24687;&#26159;&#36890;&#36807;&#33258;&#28982;&#22330;&#26223;&#20013;&#25293;&#25668;&#30340;&#22270;&#20687;&#30340;&#27010;&#29575;&#26469;&#25551;&#36848;&#30340;&#12290;&#30001;&#20110;&#35745;&#31639;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#20197;&#21069;&#26080;&#27861;&#30452;&#25509;&#20934;&#30830;&#22320;&#39044;&#27979;&#22270;&#20687;&#30340;&#27010;&#29575;&#12290;&#23613;&#31649;&#36825;&#31181;&#24819;&#27861;&#30340;&#25506;&#32034;&#26159;&#38388;&#25509;&#30340;&#65292;&#20027;&#35201;&#22522;&#20110;&#22270;&#20687;&#23494;&#24230;&#30340;&#36807;&#24230;&#31616;&#21270;&#27169;&#22411;&#25110;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#37325;&#29616;&#21508;&#31181;&#29983;&#29702;&#21644;&#24515;&#29702;&#29289;&#29702;&#29616;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#30830;&#23450;&#30693;&#35273;&#28789;&#25935;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;&#20154;&#31867;&#24847;&#35265;&#30456;&#20851;&#24615;&#24456;&#39640;&#30340;&#22270;&#20687;&#36136;&#37327;&#25351;&#26631;&#20316;&#20026;&#20154;&#31867;&#35270;&#35273;&#30340;&#20195;&#29702;&#65292;&#20197;&#21450;&#19968;&#20010;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#30452;&#25509;&#20272;&#35745;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26681;&#25454;Barlow&#21644;Attneave&#29702;&#35770;&#39044;&#27979;&#30340;&#22270;&#20687;&#32479;&#35745;&#19982;&#20154;&#31867;&#30693;&#35273;&#20043;&#38388;&#23384;&#22312;&#31995;&#32479;&#24615;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#26469;&#35828;&#26126;&#36825;&#19968;&#21457;&#29616;&#65292;&#36825;&#26159;&#36890;&#36807;&#35270;&#35273;&#25628;&#32034;&#23454;&#39564;&#27979;&#37327;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the 1950s Horace Barlow and Fred Attneave suggested a connection between sensory systems and how they are adapted to the environment: early vision evolved to maximise the information it conveys about incoming signals. Following Shannon's definition, this information was described using the probability of the images taken from natural scenes. Previously, direct accurate predictions of image probabilities were not possible due to computational limitations. Despite the exploration of this idea being indirect, mainly based on oversimplified models of the image density or on system design methods, these methods had success in reproducing a wide range of physiological and psychophysical phenomena. In this paper, we directly evaluate the probability of natural images and analyse how it may determine perceptual sensitivity. We employ image quality metrics that correlate well with human opinion as a surrogate of human vision, and an advanced generative model to directly estimate the probabil
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#31354;&#38388;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#27979;&#37327;&#20998;&#32452;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#30340;&#36829;&#35268;&#24773;&#20917;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#26816;&#26597;&#32452;&#38388;&#27495;&#35270;&#30340;&#21407;&#22240;&#65292;&#25552;&#39640;&#20102;&#23457;&#35745;&#20844;&#24179;&#24615;&#30340;&#25935;&#24863;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.08040</link><description>&lt;p&gt;
&#12298;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#26816;&#26597;&#21592;&#65306;&#36890;&#36807;&#35299;&#37322;&#31354;&#38388;&#36827;&#34892;&#20844;&#24179;&#23457;&#26680;&#12299;
&lt;/p&gt;
&lt;p&gt;
Demographic Parity Inspector: Fairness Audits via the Explanation Space. (arXiv:2303.08040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08040
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#31354;&#38388;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#27979;&#37327;&#20998;&#32452;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#30340;&#36829;&#35268;&#24773;&#20917;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#26816;&#26597;&#32452;&#38388;&#27495;&#35270;&#30340;&#21407;&#22240;&#65292;&#25552;&#39640;&#20102;&#23457;&#35745;&#20844;&#24179;&#24615;&#30340;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#20855;&#26377;&#26368;&#22909;&#30340;&#24847;&#22270;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20063;&#21487;&#33021;&#24310;&#32493;&#12289;&#25918;&#22823;&#29978;&#33267;&#21019;&#36896;&#31038;&#20250;&#20559;&#35265;&#12290;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27495;&#35270;&#24615;&#65288;&#38750;&#27495;&#35270;&#24615;&#65289;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#27495;&#35270;&#25928;&#26524;&#30340;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20195;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#27979;&#37327;&#20998;&#32452;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#30340;&#36829;&#35268;&#24773;&#20917;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#26816;&#26597;&#32452;&#38388;&#27495;&#35270;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#24819;&#65292;&#21363;&#22522;&#20110;&#35299;&#37322;&#31354;&#38388;&#23545;&#27169;&#22411;&#23545;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20381;&#36182;&#24230;&#36827;&#34892;&#27979;&#37327;&#65292;&#35299;&#37322;&#31354;&#38388;&#26159;&#19968;&#31181;&#25552;&#20379;&#27604;&#36755;&#20837;&#25968;&#25454;&#25110;&#39044;&#27979;&#20998;&#24067;&#30340;&#21407;&#22987;&#31354;&#38388;&#26356;&#25935;&#24863;&#23457;&#35745;&#30340;&#20449;&#24687;&#31354;&#38388;&#65292;&#20174;&#32780;&#20801;&#35768;&#26029;&#35328;&#29702;&#35770;&#19978;&#30340;&#20154;&#21475;&#32479;&#35745;&#23457;&#26680;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#23398;&#20998;&#26512;&#12289;&#21512;&#25104;&#26679;&#20363;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;Pytorch&#23454;&#29616;&#21644;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even if deployed with the best intentions, machine learning methods can perpetuate, amplify or even create social biases. Measures of (un-)fairness have been proposed as a way to gauge the (non-)discriminatory nature of machine learning models. However, proxies of protected attributes causing discriminatory effects remain challenging to address. In this work, we propose a new algorithmic approach that measures group-wise demographic parity violations and allows us to inspect the causes of inter-group discrimination. Our method relies on the novel idea of measuring the dependence of a model on the protected attribute based on the explanation space, an informative space that allows for more sensitive audits than the primary space of input data or prediction distributions, and allowing for the assertion of theoretical demographic parity auditing guarantees. We provide a mathematical analysis, synthetic examples, and experimental evaluation of real-world data. We release an open-source Pyt
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#32602;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#37096;&#20998;&#32447;&#24615;Cox&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#32954;&#30284;&#24739;&#32773;&#30340;CT&#25195;&#25551;&#20013;&#20998;&#26512;&#27515;&#20129;&#39118;&#38505;&#12290;&#35813;&#27169;&#22411;&#33021;&#26377;&#25928;&#22320;&#25972;&#21512;&#24050;&#30693;&#21644;&#26032;&#20852;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#35299;&#20915;&#20102;&#21442;&#25968;&#32500;&#24230;&#36229;&#20986;&#26679;&#26412;&#22823;&#23567;&#21644;&#38750;&#21442;&#25968;&#24314;&#27169;&#20013;&#32500;&#24230;&#28798;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.05341</link><description>&lt;p&gt;
&#21033;&#29992;&#32602;&#20989;&#25968;&#30340;&#28145;&#24230;&#37096;&#20998;&#32447;&#24615;Cox&#27169;&#22411;&#21450;&#20854;&#22312;&#32954;&#30284;&#24739;&#32773;CT&#25195;&#25551;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Penalized Deep Partially Linear Cox Models with Application to CT Scans of Lung Cancer Patients. (arXiv:2303.05341v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05341
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#32602;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#37096;&#20998;&#32447;&#24615;Cox&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#32954;&#30284;&#24739;&#32773;&#30340;CT&#25195;&#25551;&#20013;&#20998;&#26512;&#27515;&#20129;&#39118;&#38505;&#12290;&#35813;&#27169;&#22411;&#33021;&#26377;&#25928;&#22320;&#25972;&#21512;&#24050;&#30693;&#21644;&#26032;&#20852;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#35299;&#20915;&#20102;&#21442;&#25968;&#32500;&#24230;&#36229;&#20986;&#26679;&#26412;&#22823;&#23567;&#21644;&#38750;&#21442;&#25968;&#24314;&#27169;&#20013;&#32500;&#24230;&#28798;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#20840;&#29699;&#30284;&#30151;&#27515;&#20129;&#29575;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#31361;&#20986;&#20102;&#29702;&#35299;&#20854;&#27515;&#20129;&#39118;&#38505;&#23545;&#35774;&#35745;&#26377;&#25928;&#30340;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#27835;&#30103;&#30340;&#37325;&#35201;&#24615;&#12290;&#22269;&#23478;&#32954;&#37096;&#31579;&#26597;&#35797;&#39564;&#65288;NLST&#65289;&#37319;&#29992;&#20102;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#32441;&#29702;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;CT&#25195;&#25551;&#19978;&#32441;&#29702;&#27169;&#24335;&#30340;&#23458;&#35266;&#27979;&#37327;&#65292;&#29992;&#20110;&#37327;&#21270;&#32954;&#30284;&#24739;&#32773;&#30340;&#27515;&#20129;&#39118;&#38505;&#12290;&#37096;&#20998;&#32447;&#24615;Cox&#27169;&#22411;&#36890;&#36807;&#23558;&#39118;&#38505;&#20989;&#25968;&#20998;&#35299;&#20026;&#21442;&#25968;&#21644;&#38750;&#21442;&#25968;&#20998;&#37327;&#65292;&#25104;&#20026;&#29983;&#23384;&#20998;&#26512;&#20013;&#22791;&#21463;&#38738;&#30544;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#24050;&#30693;&#39118;&#38505;&#22240;&#32032;&#65288;&#22914;&#24180;&#40836;&#21644;&#20020;&#24202;&#21464;&#37327;&#65289;&#21644;&#26032;&#20852;&#39118;&#38505;&#22240;&#32032;&#65288;&#22914;&#22270;&#20687;&#29305;&#24449;&#65289;&#25972;&#21512;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#12290;&#28982;&#32780;&#65292;&#24403;&#21442;&#25968;&#20998;&#37327;&#30340;&#32500;&#24230;&#36229;&#36807;&#26679;&#26412;&#22823;&#23567;&#26102;&#65292;&#27169;&#22411;&#25311;&#21512;&#21464;&#24471;&#22256;&#38590;&#65292;&#32780;&#38750;&#21442;&#25968;&#24314;&#27169;&#21017;&#38754;&#20020;&#32500;&#24230;&#28798;&#38590;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32602;&#20989;&#25968;&#28145;&#24230;&#37096;&#20998;&#32447;&#24615;Cox&#27169;&#22411;&#65288;Penali
&lt;/p&gt;
&lt;p&gt;
Lung cancer is a leading cause of cancer mortality globally, highlighting the importance of understanding its mortality risks to design effective patient-centered therapies. The National Lung Screening Trial (NLST) employed computed tomography texture analysis, which provides objective measurements of texture patterns on CT scans, to quantify the mortality risks of lung cancer patients. Partially linear Cox models have gained popularity for survival analysis by dissecting the hazard function into parametric and nonparametric components, allowing for the effective incorporation of both well-established risk factors (such as age and clinical variables) and emerging risk factors (e.g., image features) within a unified framework. However, when the dimension of parametric components exceeds the sample size, the task of model fitting becomes formidable, while nonparametric modeling grapples with the curse of dimensionality. We propose a novel Penalized Deep Partially Linear Cox Model (Penali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20849;&#23398;&#20064;&#35268;&#21010;&#21644;&#25511;&#21046;&#31574;&#30053;&#26469;&#35299;&#20915;&#24102;&#26377;&#22797;&#26434;&#36923;&#36753;&#32422;&#26463;&#30340;&#39640;&#32500;&#24230;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24615;&#26469;&#35757;&#32451;&#20986;&#39640;&#36136;&#37327;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#38271;&#26399;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#36335;&#24452;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01346</link><description>&lt;p&gt;
&#21463;&#21487;&#24494;&#20998;&#36923;&#36753;&#32422;&#26463;&#30340;&#20849;&#23398;&#20064;&#35268;&#21010;&#19982;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Co-learning Planning and Control Policies Constrained by Differentiable Logic Specifications. (arXiv:2303.01346v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20849;&#23398;&#20064;&#35268;&#21010;&#21644;&#25511;&#21046;&#31574;&#30053;&#26469;&#35299;&#20915;&#24102;&#26377;&#22797;&#26434;&#36923;&#36753;&#32422;&#26463;&#30340;&#39640;&#32500;&#24230;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24615;&#26469;&#35757;&#32451;&#20986;&#39640;&#36136;&#37327;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#38271;&#26399;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#36335;&#24452;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#32508;&#21512;&#35268;&#21010;&#19982;&#25511;&#21046;&#31574;&#30053;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#20294;&#22797;&#26434;&#30340;&#36923;&#36753;&#32422;&#26463;&#21644;&#39640;&#32500;&#24230;&#30340;&#26426;&#22120;&#20154;&#21160;&#21147;&#23398;&#20351;&#20854;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#23398;&#20064;&#35268;&#21010;&#21644;&#25511;&#21046;&#31574;&#30053;&#26469;&#35299;&#20915;&#24102;&#26377;&#22797;&#26434;&#36923;&#36753;&#32422;&#26463;&#30340;&#39640;&#32500;&#24230;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#35757;&#32451;&#20986;&#39640;&#36136;&#37327;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#20174;&#22320;&#22270;&#22270;&#20687;&#20013;&#25552;&#21462;&#22797;&#26434;&#35268;&#33539;&#24182;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#19981;&#21516;&#22320;&#22270;&#24067;&#23616;&#30340;&#38271;&#26399;&#26426;&#22120;&#20154;&#36816;&#21160;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#23637;&#31034;&#20102;&#22312;&#39640;&#32500;&#24230;&#25511;&#21046;&#21644;&#36991;&#20813;&#27425;&#20248;&#31574;&#30053;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#27169;&#25311;&#39640;&#32500;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing planning and control policies in robotics is a fundamental task, further complicated by factors such as complex logic specifications and high-dimensional robot dynamics. This paper presents a novel reinforcement learning approach to solving high-dimensional robot navigation tasks with complex logic specifications by co-learning planning and control policies. Notably, this approach significantly reduces the sample complexity in training, allowing us to train high-quality policies with much fewer samples compared to existing reinforcement learning algorithms. In addition, our methodology streamlines complex specification extraction from map images and enables the efficient generation of long-horizon robot motion paths across different map layouts. Moreover, our approach also demonstrates capabilities for high-dimensional control and avoiding suboptimal policies via policy alignment. The efficacy of our approach is demonstrated through experiments involving simulated high-dim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#8220;ConLinUCB&#8221;&#26469;&#35299;&#20915;&#23545;&#35805;&#24335;&#24773;&#22659;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20449;&#24687;&#25972;&#21512;&#21644;&#25506;&#32034;&#24615;&#20851;&#38190;&#35789;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#20197;&#21152;&#36895;&#29992;&#25143;&#20559;&#22909;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.00315</link><description>&lt;p&gt;
&#23545;&#35805;&#24335;&#24773;&#22659;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#39640;&#25928;&#25506;&#32034;&#24615;&#20851;&#38190;&#35789;&#36873;&#25321;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Efficient Explorative Key-term Selection Strategies for Conversational Contextual Bandits. (arXiv:2303.00315v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#8220;ConLinUCB&#8221;&#26469;&#35299;&#20915;&#23545;&#35805;&#24335;&#24773;&#22659;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20449;&#24687;&#25972;&#21512;&#21644;&#25506;&#32034;&#24615;&#20851;&#38190;&#35789;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#20197;&#21152;&#36895;&#29992;&#25143;&#20559;&#22909;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#24773;&#22659;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36890;&#36807;&#20598;&#23572;&#35810;&#38382;&#26174;&#24335;&#21453;&#39304;&#20013;&#30340;&#20851;&#38190;&#35789;&#26469;&#21152;&#24555;&#23398;&#20064;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#20851;&#20110;&#20851;&#38190;&#35789;&#23618;&#38754;&#30340;&#23545;&#35805;&#21644;&#33218;&#32423;&#25512;&#33616;&#30340;&#20449;&#24687;&#27809;&#26377;&#34987;&#22949;&#21892;&#22320;&#32467;&#21512;&#36215;&#26469;&#20197;&#21152;&#36895;&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#37325;&#35201;&#30340;&#26159;&#38382;&#19968;&#20123;&#25506;&#32034;&#24615;&#30340;&#20851;&#38190;&#35789;&#65292;&#20197;&#36805;&#36895;&#20102;&#35299;&#29992;&#25143;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#20852;&#36259;&#65292;&#20174;&#32780;&#21152;&#36895;&#29992;&#25143;&#20559;&#22909;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#36825;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#20174;&#26410;&#34987;&#32771;&#34385;&#36807;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#8220;ConLinUCB&#8221;&#36825;&#19968;&#23545;&#35805;&#24335;&#20915;&#31574;&#36807;&#31243;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#20851;&#38190;&#35789;&#23618;&#38754;&#21644;&#33218;&#32423;&#21453;&#39304;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#19968;&#27493;&#20272;&#35745;&#29992;&#25143;&#20559;&#22909;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#20004;&#31181;&#24102;&#26377;&#25506;&#32034;&#24615;&#20851;&#38190;&#35789;&#36873;&#25321;&#31574;&#30053;&#30340;&#20915;&#31574;&#36807;&#31243;&#31639;&#27861;&#65292;&#21363;ConLinUCB-BS&#21644;ConLinUCB-MCR&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational contextual bandits elicit user preferences by occasionally querying for explicit feedback on key-terms to accelerate learning. However, there are aspects of existing approaches which limit their performance. First, information gained from key-term-level conversations and arm-level recommendations is not appropriately incorporated to speed up learning. Second, it is important to ask explorative key-terms to quickly elicit the user's potential interests in various domains to accelerate the convergence of user preference estimation, which has never been considered in existing works. To tackle these issues, we first propose ``ConLinUCB", a general framework for conversational bandits with better information incorporation, combining arm-level and key-term-level feedback to estimate user preference in one step at each time. Based on this framework, we further design two bandit algorithms with explorative key-term selection strategies, ConLinUCB-BS and ConLinUCB-MCR. We prove t
&lt;/p&gt;</description></item><item><title>EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.14838</link><description>&lt;p&gt;
EvoPrompting: &#36866;&#29992;&#20110;&#20195;&#30721;&#32423;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v1 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14838
&lt;/p&gt;
&lt;p&gt;
EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#25104;&#23601;&#65292;&#25105;&#20204;&#25506;&#32034;&#23558;LM&#20316;&#20026;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#30340;&#20351;&#29992;&#12290;&#23613;&#31649;NAS&#20173;&#28982;&#36807;&#20110;&#22256;&#38590;&#65292;&#20197;&#33267;&#20110;&#20165;&#20165;&#36890;&#36807;&#25552;&#31034;&#23601;&#38590;&#20197;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36827;&#21270;&#25552;&#31034;&#24037;&#31243;&#19982;&#36719;&#25552;&#31034;&#35843;&#25972;&#30340;&#32452;&#21512;&#65292;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;EvoPrompting&#30340;&#26041;&#27861;&#65292;&#22987;&#32456;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#21270;&#19988;&#24615;&#33021;&#39640;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;EvoPrompting&#22312;MNIST-1D&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20854;&#20013;EvoPrompting&#20135;&#29983;&#30340;&#21367;&#31215;&#26550;&#26500;&#21464;&#20307;&#22312;&#20934;&#30830;&#29575;&#21644;&#27169;&#22411;&#22823;&#23567;&#26041;&#38754;&#22343;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#35774;&#35745;&#30340;&#26550;&#26500;&#21644;&#22825;&#30495;&#30340;&#23569;&#25968;&#20808;&#23548;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#22312;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#25628;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;EvoPrompting&#33021;&#22815;&#35774;&#35745;&#20986;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26356;&#22909;&#30340;&#26032;&#39062;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 ou
&lt;/p&gt;</description></item><item><title>HUST&#36724;&#25215;&#26159;&#19968;&#20010;&#23454;&#29992;&#30340;&#29699;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;90&#20010;&#24102;&#26377;6&#31181;&#25925;&#38556;&#31867;&#22411;&#65288;&#20869;&#37096;&#35010;&#32441;&#12289;&#22806;&#37096;&#35010;&#32441;&#12289;&#29699;&#20307;&#35010;&#32441;&#21644;&#23427;&#20204;&#30340;2&#31181;&#32452;&#21512;&#65289;&#30340;5&#31181;&#19981;&#21516;&#31867;&#22411;&#36724;&#25215;&#30340;&#25391;&#21160;&#25968;&#25454;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#20197;&#21450;&#20808;&#36827;&#30340;&#38750;&#30417;&#30563;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#23545;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#20934;&#30830;&#29575;&#21487;&#36798;&#21040;100%&#65292;&#22312;&#38750;&#30417;&#30563;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#20934;&#30830;&#29575;&#20026;60-80%&#12290;</title><link>http://arxiv.org/abs/2302.12533</link><description>&lt;p&gt;
HUST&#36724;&#25215;&#65306;&#19968;&#20010;&#23454;&#29992;&#30340;&#29699;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HUST bearing: a practical dataset for ball bearing fault diagnosis. (arXiv:2302.12533v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12533
&lt;/p&gt;
&lt;p&gt;
HUST&#36724;&#25215;&#26159;&#19968;&#20010;&#23454;&#29992;&#30340;&#29699;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;90&#20010;&#24102;&#26377;6&#31181;&#25925;&#38556;&#31867;&#22411;&#65288;&#20869;&#37096;&#35010;&#32441;&#12289;&#22806;&#37096;&#35010;&#32441;&#12289;&#29699;&#20307;&#35010;&#32441;&#21644;&#23427;&#20204;&#30340;2&#31181;&#32452;&#21512;&#65289;&#30340;5&#31181;&#19981;&#21516;&#31867;&#22411;&#36724;&#25215;&#30340;&#25391;&#21160;&#25968;&#25454;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#20197;&#21450;&#20808;&#36827;&#30340;&#38750;&#30417;&#30563;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#23545;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#20934;&#30830;&#29575;&#21487;&#36798;&#21040;100%&#65292;&#22312;&#38750;&#30417;&#30563;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#20934;&#30830;&#29575;&#20026;60-80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;HUST&#36724;&#25215;&#30340;&#23454;&#29992;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#32452;&#19981;&#21516;&#29699;&#36724;&#25215;&#30340;&#25391;&#21160;&#25968;&#25454;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;5&#31181;&#31867;&#22411;&#36724;&#25215;&#30340;90&#20010;&#21407;&#22987;&#25391;&#21160;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#20869;&#37096;&#35010;&#32441;&#12289;&#22806;&#37096;&#35010;&#32441;&#12289;&#29699;&#20307;&#35010;&#32441;&#20197;&#21450;&#23427;&#20204;&#30340;2&#31181;&#32452;&#21512;&#22312;&#20869;&#30340;6&#31181;&#32570;&#38519;&#31867;&#22411;&#65292;&#20197;&#21450;3&#20010;&#24037;&#20316;&#26465;&#20214;&#19979;&#30340;&#37319;&#26679;&#29575;&#20026;51,200&#27425;/&#31186;&#12290;&#25105;&#20204;&#22312;&#24341;&#20837;&#30340;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#20102;&#21253;&#32476;&#20998;&#26512;&#21644;&#38454;&#27425;&#36319;&#36394;&#20998;&#26512;&#65292;&#20197;&#36827;&#34892;&#25968;&#25454;&#30340;&#21021;&#27493;&#35780;&#20272;&#12290;&#20351;&#29992;&#19981;&#21516;&#22495;&#20013;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#31181;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#26469;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#36724;&#25215;&#25925;&#38556;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#20856;&#22411;&#30340;&#20808;&#36827;&#38750;&#30417;&#30563;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35266;&#23519;&#25968;&#25454;&#38598;&#21508;&#37096;&#20998;&#20043;&#38388;&#30340;&#30693;&#35782;&#21487;&#36801;&#31227;&#24615;&#12290;&#22312;&#25968;&#25454;&#38598;&#19978;&#32463;&#36807;&#23454;&#39564;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;&#36798;&#21040;100%&#30340;&#19981;&#21516;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#38750;&#30417;&#30563;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;60-80%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a practical dataset named HUST bearing, that provides a large set of vibration data on different ball bearings. This dataset contains 90 raw vibration data of 6 types of defects (inner crack, outer crack, ball crack, and their 2-combinations) on 5 types of bearing at 3 working conditions with the sample rate of 51,200 samples per second. We established the envelope analysis and order tracking analysis on the introduced dataset to allow an initial evaluation of the data. A number of classical machine learning classification methods are used to identify bearing faults of the dataset using features in different domains. The typical advanced unsupervised transfer learning algorithms also perform to observe the transferability of knowledge among parts of the dataset. The experimental results of examined methods on the dataset gain divergent accuracy up to 100% on classification task and 60-80% on unsupervised transfer learning task.
&lt;/p&gt;</description></item><item><title>mSAM&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32858;&#21512;&#23545;&#25239;&#24615;&#25200;&#21160;&#24471;&#21040;&#30340;&#26356;&#26032;&#65292;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24179;&#30340;&#26497;&#23567;&#20540;&#28857;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09693</link><description>&lt;p&gt;
mSAM: &#24494;&#25209;&#37327;&#24179;&#22343;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
mSAM: Micro-Batch-Averaged Sharpness-Aware Minimization. (arXiv:2302.09693v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09693
&lt;/p&gt;
&lt;p&gt;
mSAM&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32858;&#21512;&#23545;&#25239;&#24615;&#25200;&#21160;&#24471;&#21040;&#30340;&#26356;&#26032;&#65292;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24179;&#30340;&#26497;&#23567;&#20540;&#28857;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#36807;&#21442;&#25968;&#21270;&#30340;&#65292;&#19981;&#21516;&#30340;&#26497;&#20540;&#21487;&#33021;&#23548;&#33268;&#24191;&#27867;&#21464;&#21270;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;Sharpness-Aware Minimization&#65292;SAM&#65289;&#25216;&#26415;&#20462;&#25913;&#20102;&#22522;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26397;&#30528;&#26356;&#24179;&#30340;&#26497;&#23567;&#20540;&#28857;&#21069;&#36827;&#65292;&#36825;&#34987;&#35748;&#20026;&#33021;&#22815;&#23637;&#29616;&#20986;&#22686;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;SAM&#21464;&#20307;&#65292;&#21363;&#24494;&#25209;&#37327;SAM&#65288;mSAM&#65289;&#12290;&#36825;&#31181;&#21464;&#20307;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#23545;&#26469;&#33258;&#22810;&#20010;&#20998;&#29255;&#65288;&#24494;&#25209;&#37327;&#65289;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#24471;&#21040;&#30340;&#26356;&#26032;&#36827;&#34892;&#32858;&#21512;&#12290;&#25105;&#20204;&#23558;&#26368;&#36817;&#24320;&#21457;&#21644;&#30740;&#31350;&#30340;&#29992;&#20110;&#24179;&#22374;&#24615;&#20998;&#26512;&#30340;&#36890;&#29992;&#26694;&#26550;&#25193;&#23637;&#21040;&#29702;&#35770;&#19978;&#35777;&#26126;SAM&#23454;&#29616;&#20102;&#27604;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26356;&#24179;&#30340;&#26497;&#23567;&#20540;&#28857;&#65292;&#32780;mSAM&#27604;SAM&#23454;&#29616;&#20102;&#26356;&#21152;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#35777;&#35780;&#20272;&#20197;&#39564;&#35777;&#36825;&#19968;&#29702;&#35770;&#36827;&#23637;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;mSAM&#21487;&#20197;&#34987;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep learning models are over-parameterized, where different optima can result in widely varying generalization performance. The Sharpness-Aware Minimization (SAM) technique modifies the fundamental loss function that steers gradient descent methods toward flatter minima, which are believed to exhibit enhanced generalization prowess. Our study delves into a specific variant of SAM known as micro-batch SAM (mSAM). This variation involves aggregating updates derived from adversarial perturbations across multiple shards (micro-batches) of a mini-batch during training. We extend a recently developed and well-studied general framework for flatness analysis to theoretically show that SAM achieves flatter minima than SGD, and mSAM achieves even flatter minima than SAM. We provide a thorough empirical evaluation of various image classification and natural language processing tasks to substantiate this theoretical advancement. We also show that contrary to previous work, mSAM can be impl
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#28151;&#21512;&#25240;&#25187;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#29702;&#35770;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#22312;&#28151;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#20381;&#36182;&#20110;&#24635;&#21464;&#24322;&#28151;&#21512;&#26102;&#38388;&#12289;&#25240;&#25187;&#22240;&#23376;&#21644;&#35299;&#35823;&#24046;&#23481;&#24525;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.07477</link><description>&lt;p&gt;
&#23545;&#20110;&#28151;&#21512;&#25240;&#25187;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal Sample Complexity of Reinforcement Learning for Mixing Discounted Markov Decision Processes. (arXiv:2302.07477v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07477
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#28151;&#21512;&#25240;&#25187;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#29702;&#35770;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#22312;&#28151;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#20381;&#36182;&#20110;&#24635;&#21464;&#24322;&#28151;&#21512;&#26102;&#38388;&#12289;&#25240;&#25187;&#22240;&#23376;&#21644;&#35299;&#35823;&#24046;&#23481;&#24525;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#34920;&#26684;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;&#20110;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#26368;&#22823;&#21270;&#26080;&#31351;&#26102;&#38388;&#25240;&#25187;&#22870;&#21169;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#29702;&#35770;&#12290;&#22312;&#36825;&#31181;&#35774;&#23450;&#19979;&#65292;&#24050;&#32463;&#20026;&#34920;&#26684;&#22411;&#38382;&#39064;&#24320;&#21457;&#20102;&#26368;&#20248;&#26368;&#22351;&#24773;&#20917;&#22797;&#26434;&#24230;&#32467;&#26524;&#65292;&#23548;&#33268;&#26679;&#26412;&#22797;&#26434;&#24230;&#20381;&#36182;&#20110;&#25240;&#25187;&#31995;&#25968;$\gamma$&#21644;&#35299;&#35823;&#24046;&#23481;&#24525;&#24230;$\epsilon$&#30340;&#24418;&#24335;&#20026;$\tilde \Theta((1-\gamma)^{-3}\epsilon^{-2})$&#65292;&#20854;&#20013;$\gamma$&#34920;&#31034;&#25240;&#25187;&#22240;&#23376;&#65292;$\epsilon$&#20026;&#35299;&#35823;&#24046;&#23481;&#24525;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#24212;&#29992;&#20013;&#65292;&#26368;&#20248;&#31574;&#30053;&#65288;&#25110;&#25152;&#26377;&#31574;&#30053;&#65289;&#20250;&#20135;&#29983;&#28151;&#21512;&#12290;&#25105;&#20204;&#30830;&#23450;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#20026;$\tilde \Theta(t_{\text{mix}}(1-\gamma)^{-2}\epsilon^{-2})$&#65292;&#20854;&#20013;$t_{\text{mix}}$&#26159;&#24635;&#21464;&#24322;&#28151;&#21512;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#20877;&#29983;&#22411;&#24605;&#24819;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#24605;&#24819;&#23545;&#20110;&#30740;&#31350;&#19968;&#33324;&#29366;&#24577;&#31354;&#38388;MDPs&#30340;RL&#38382;&#39064;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the optimal sample complexity theory of tabular reinforcement learning (RL) for maximizing the infinite horizon discounted reward in a Markov decision process (MDP). Optimal worst-case complexity results have been developed for tabular RL problems in this setting, leading to a sample complexity dependence on $\gamma$ and $\epsilon$ of the form $\tilde \Theta((1-\gamma)^{-3}\epsilon^{-2})$, where $\gamma$ denotes the discount factor and $\epsilon$ is the solution error tolerance. However, in many applications of interest, the optimal policy (or all policies) induces mixing. We establish that in such settings, the optimal sample complexity dependence is $\tilde \Theta(t_{\text{mix}}(1-\gamma)^{-2}\epsilon^{-2})$, where $t_{\text{mix}}$ is the total variation mixing time. Our analysis is grounded in regeneration-type ideas, which we believe are of independent interest, as they can be used to study RL problems for general state space MDPs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AdaptSim&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#39537;&#21160;&#30340;&#36866;&#24212;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#21040;&#23454;&#38469;&#36716;&#25442;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#30446;&#26631;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#24615;&#33021;&#26469;&#35299;&#20915;&#20223;&#30495;&#21644;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2302.04903</link><description>&lt;p&gt;
AdaptSim: &#29992;&#20110;&#27169;&#25311;&#21040;&#23454;&#38469;&#36716;&#25442;&#30340;&#20219;&#21153;&#39537;&#21160;&#20223;&#30495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
AdaptSim: Task-Driven Simulation Adaptation for Sim-to-Real Transfer. (arXiv:2302.04903v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04903
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AdaptSim&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#39537;&#21160;&#30340;&#36866;&#24212;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#21040;&#23454;&#38469;&#36716;&#25442;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#30446;&#26631;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#24615;&#33021;&#26469;&#35299;&#20915;&#20223;&#30495;&#21644;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#21442;&#25968;&#35774;&#32622;&#65288;&#22914;&#25509;&#35302;&#27169;&#22411;&#21644;&#29289;&#20307;&#20960;&#20309;&#36817;&#20284;&#65289;&#23545;&#20110;&#35757;&#32451;&#33021;&#22815;&#20174;&#20223;&#30495;&#36716;&#25442;&#21040;&#30495;&#23454;&#29615;&#22659;&#37096;&#32626;&#30340;&#31283;&#20581;&#26426;&#22120;&#20154;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#24120;&#25163;&#24037;&#21046;&#20316;&#21442;&#25968;&#20998;&#24067;&#65288;&#22495;&#38543;&#26426;&#21270;&#65289;&#65292;&#25110;&#32773;&#35782;&#21035;&#26368;&#33021;&#21305;&#37197;&#30495;&#23454;&#29615;&#22659;&#21160;&#21147;&#23398;&#30340;&#21442;&#25968;&#65288;&#31995;&#32479;&#35782;&#21035;&#65289;&#12290;&#28982;&#32780;&#65292;&#20223;&#30495;&#19982;&#29616;&#23454;&#20043;&#38388;&#24448;&#24448;&#23384;&#22312;&#30528;&#26080;&#27861;&#28040;&#38500;&#30340;&#24046;&#36317;&#65306;&#35797;&#22270;&#22312;&#25152;&#26377;&#29366;&#24577;&#21644;&#20219;&#21153;&#19979;&#21305;&#37197;&#20223;&#30495;&#21644;&#29616;&#23454;&#30340;&#21160;&#21147;&#23398;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#20063;&#19981;&#19968;&#23450;&#33021;&#22815;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaptSim&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#39537;&#21160;&#30340;&#36866;&#24212;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#21040;&#23454;&#38469;&#36716;&#25442;&#65292;&#20854;&#30446;&#26631;&#26159;&#20248;&#21270;&#30446;&#26631;&#65288;&#30495;&#23454;&#65289;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;&#21305;&#37197;&#20223;&#30495;&#21644;&#29616;&#23454;&#20043;&#38388;&#30340;&#21160;&#21147;&#23398;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#20013;&#20803;&#23398;&#20064;&#19968;&#20010;&#36866;&#24212;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation parameter settings such as contact models and object geometry approximations are critical to training robust robotic policies capable of transferring from simulation to real-world deployment. Previous approaches typically handcraft distributions over such parameters (domain randomization), or identify parameters that best match the dynamics of the real environment (system identification). However, there is often an irreducible gap between simulation and reality: attempting to match the dynamics between simulation and reality across all states and tasks may be infeasible and may not lead to policies that perform well in reality for a specific task. Addressing this issue, we propose AdaptSim, a new task-driven adaptation framework for sim-to-real transfer that aims to optimize task performance in target (real) environments -- instead of matching dynamics between simulation and reality. First, we meta-learn an adaptation policy in simulation using reinforcement learning for adj
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26102;&#38388;&#23610;&#24230;&#28201;&#24230;&#32553;&#25918;&#65288;LHTS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#28201;&#24230;&#32553;&#25918;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;LHTS&#21487;&#20197;&#20248;&#21270;&#26679;&#26412;&#30340;&#38271;&#26102;&#38388;&#23610;&#24230;&#20284;&#28982;&#65292;&#24182;&#19988;&#22312;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21644;&#23383;&#31526;/&#35821;&#35328;&#33258;&#22238;&#24402;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.03686</link><description>&lt;p&gt;
&#38271;&#26102;&#38388;&#23610;&#24230;&#28201;&#24230;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Long Horizon Temperature Scaling. (arXiv:2302.03686v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03686
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26102;&#38388;&#23610;&#24230;&#28201;&#24230;&#32553;&#25918;&#65288;LHTS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#28201;&#24230;&#32553;&#25918;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;LHTS&#21487;&#20197;&#20248;&#21270;&#26679;&#26412;&#30340;&#38271;&#26102;&#38388;&#23610;&#24230;&#20284;&#28982;&#65292;&#24182;&#19988;&#22312;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21644;&#23383;&#31526;/&#35821;&#35328;&#33258;&#22238;&#24402;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28201;&#24230;&#32553;&#25918;&#26159;&#19968;&#31181;&#35843;&#33410;&#27169;&#22411;&#20998;&#24067;&#38160;&#24230;&#30340;&#24120;&#29992;&#25216;&#26415;&#12290;&#23427;&#24191;&#27867;&#24212;&#29992;&#20110;&#37319;&#26679;&#21487;&#33021;&#30340;&#29983;&#25104;&#29289;&#21644;&#26657;&#20934;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#29978;&#33267;&#22312;&#35768;&#22810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37096;&#32626;&#20013;&#20316;&#20026;&#21487;&#25511;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#20381;&#36182;&#20110;&#36138;&#23146;&#22320;&#20248;&#21270;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#30701;&#35270;&#28201;&#24230;&#32553;&#25918;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#38271;&#26102;&#38388;&#23610;&#24230;&#28201;&#24230;&#32553;&#25918;&#65288;LHTS&#65289;&#65292;&#29992;&#20110;&#20174;&#28201;&#24230;&#32553;&#25918;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;LHTS&#19982;&#25152;&#26377;&#22522;&#20110;&#20284;&#28982;&#30340;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20248;&#21270;&#26679;&#26412;&#30340;&#38271;&#26102;&#38388;&#23610;&#24230;&#20284;&#28982;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#28201;&#24230;&#30456;&#20851;&#30340;LHTS&#30446;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19968;&#31995;&#21015;&#28201;&#24230;&#19978;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#20855;&#26377;&#21487;&#25511;&#38271;&#26102;&#38388;&#23610;&#24230;&#28201;&#24230;&#21442;&#25968;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21644;&#23383;&#31526;/&#35821;&#35328;&#33258;&#22238;&#24402;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;LHTS&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#30456;&#27604;&#20110;&#30701;&#35270;&#28201;&#24230;&#32553;&#25918;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temperature scaling is a popular technique for tuning the sharpness of a model distribution. It is used extensively for sampling likely generations and calibrating model uncertainty, and even features as a controllable parameter to many large language models in deployment. However, autoregressive models rely on myopic temperature scaling that greedily optimizes the next token. To address this, we propose Long Horizon Temperature Scaling (LHTS), a novel approach for sampling from temperature-scaled joint distributions. LHTS is compatible with all likelihood-based models, and optimizes for the long horizon likelihood of samples. We derive a temperature-dependent LHTS objective, and show that finetuning a model on a range of temperatures produces a single model capable of generation with a controllable long horizon temperature parameter. We experiment with LHTS on image diffusion models and character/language autoregressive models, demonstrating advantages over myopic temperature scaling 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#35299;&#37322;&#20102;Weisfeiler-Lehman&#36317;&#31163;&#65292;&#24182;&#23558;&#20854;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#32852;&#31995;&#65292;&#23545;&#20110;&#29702;&#35299;&#32593;&#32476;&#30340;Lipschitz&#24615;&#36136;&#21644;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2302.00713</link><description>&lt;p&gt;
Weisfeiler-Lehman&#36317;&#31163;&#65306;&#37325;&#26032;&#35299;&#37322;&#21644;&#19982;GNN&#30340;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
The Weisfeiler-Lehman Distance: Reinterpretation and Connection with GNNs. (arXiv:2302.00713v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#35299;&#37322;&#20102;Weisfeiler-Lehman&#36317;&#31163;&#65292;&#24182;&#23558;&#20854;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#32852;&#31995;&#65292;&#23545;&#20110;&#29702;&#35299;&#32593;&#32476;&#30340;Lipschitz&#24615;&#36136;&#21644;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#36807;&#31243;&#30340;&#27010;&#24565;&#65292;&#23545;Chen&#31561;&#20154;&#65288;2022&#24180;&#65289;&#25552;&#20986;&#30340;&#25152;&#35859;&#30340;Weisfeiler-Lehman&#65288;WL&#65289;&#36317;&#31163;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#35299;&#37322;&#12290;WL&#36317;&#31163;&#26088;&#22312;&#27604;&#36739;&#20855;&#26377;&#33410;&#28857;&#29305;&#24449;&#30340;&#22270;&#24418;&#65292;&#20855;&#26377;&#19982;&#32463;&#20856;&#30340;Weisfeiler-Lehman&#22270;&#21516;&#26500;&#27979;&#35797;&#30456;&#21516;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#24182;&#19982;Gromov-Wasserstein&#36317;&#31163;&#26377;&#30528;&#28145;&#20837;&#30340;&#32852;&#31995;&#12290;&#36825;&#31181;&#26032;&#30340;&#35299;&#37322;&#23558;WL&#36317;&#31163;&#19982;&#29992;&#20110;&#38543;&#26426;&#36807;&#31243;&#36317;&#31163;&#30340;&#25991;&#29486;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#20351;&#24471;&#23545;&#36317;&#31163;&#30340;&#35299;&#37322;&#26356;&#21152;&#31616;&#27905;&#26131;&#25026;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;WL&#36317;&#31163;&#19982;&#26576;&#20123;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;WL&#36317;&#31163;&#23545;&#20110;&#29702;&#35299;&#36825;&#20123;&#32593;&#32476;&#30340;Lipschitz&#24615;&#36136;&#21644;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel interpretation of the so-called Weisfeiler-Lehman (WL) distance, introduced by Chen et al. (2022), using concepts from stochastic processes. The WL distance aims at comparing graphs with node features, has the same discriminative power as the classic Weisfeiler-Lehman graph isomorphism test and has deep connections to the Gromov-Wasserstein distance. This new interpretation connects the WL distance to the literature on distances for stochastic processes, which also makes the interpretation of the distance more accessible and intuitive. We further explore the connections between the WL distance and certain Message Passing Neural Networks, and discuss the implications of the WL distance for understanding the Lipschitz property and the universal approximation results for these networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22270;&#21367;&#31215;&#32593;&#32476;&#20013;&#25552;&#20379;&#20102;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#21487;&#36716;&#31227;&#24615;&#30028;&#38480;&#65292;&#26080;&#38656;&#21442;&#32771;&#20219;&#20309;&#38480;&#21046;&#23545;&#35937;&#25110;&#32479;&#35745;&#20998;&#24067;&#65292;&#21487;&#22788;&#29702;&#26080;&#21521;&#22270;&#21644;&#26377;&#21521;&#22270;&#12290;&#33410;&#28857;&#32423;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#19982;&#36807;&#28388;&#22120;&#30340;&#8220;&#20805;&#20998;&#65288;&#35889;&#65289;&#35206;&#30422;&#8221;&#23646;&#24615;&#26377;&#20851;&#65292;&#36793;&#32423;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#19982;Lipschitz&#24120;&#25968;&#21644;&#26032;&#24341;&#20837;&#30340;&#36807;&#28388;&#22120;&#21322;&#33539;&#25968;&#26377;&#20851;&#12290;&#36890;&#36807;&#25968;&#23398;&#29289;&#29702;&#24037;&#20855;&#33719;&#24471;&#20102;&#20851;&#20110;&#25299;&#25169;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22270;&#31895;&#31890;&#21270;&#36807;&#31243;&#20013;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2301.11443</link><description>&lt;p&gt;
&#26080;&#38480;&#31283;&#23450;&#24615;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Limitless stability for Graph Convolutional Networks. (arXiv:2301.11443v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22270;&#21367;&#31215;&#32593;&#32476;&#20013;&#25552;&#20379;&#20102;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#21487;&#36716;&#31227;&#24615;&#30028;&#38480;&#65292;&#26080;&#38656;&#21442;&#32771;&#20219;&#20309;&#38480;&#21046;&#23545;&#35937;&#25110;&#32479;&#35745;&#20998;&#24067;&#65292;&#21487;&#22788;&#29702;&#26080;&#21521;&#22270;&#21644;&#26377;&#21521;&#22270;&#12290;&#33410;&#28857;&#32423;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#19982;&#36807;&#28388;&#22120;&#30340;&#8220;&#20805;&#20998;&#65288;&#35889;&#65289;&#35206;&#30422;&#8221;&#23646;&#24615;&#26377;&#20851;&#65292;&#36793;&#32423;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#19982;Lipschitz&#24120;&#25968;&#21644;&#26032;&#24341;&#20837;&#30340;&#36807;&#28388;&#22120;&#21322;&#33539;&#25968;&#26377;&#20851;&#12290;&#36890;&#36807;&#25968;&#23398;&#29289;&#29702;&#24037;&#20855;&#33719;&#24471;&#20102;&#20851;&#20110;&#25299;&#25169;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22270;&#31895;&#31890;&#21270;&#36807;&#31243;&#20013;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#22270;&#21367;&#31215;&#32593;&#32476;&#25552;&#20379;&#20102;&#20005;&#26684;&#12289;&#26032;&#39062;&#19988;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#21487;&#36716;&#31227;&#24615;&#30028;&#38480; - &#19981;&#38656;&#35201;&#23545;&#20219;&#20309;&#28508;&#22312;&#38480;&#21046;&#23545;&#35937;&#25110;&#32479;&#35745;&#20998;&#24067;&#36827;&#34892;&#21442;&#32771;&#12290;&#20851;&#38190;&#26159;&#65292;&#25152;&#20351;&#29992;&#30340;&#22270;&#31227;&#20301;&#36816;&#31639;&#31526;&#65288;GSO&#65289;&#19981;&#19968;&#23450;&#26159;&#27491;&#35268;&#30340;&#65292;&#21487;&#20197;&#22788;&#29702;&#26080;&#21521;&#22270;&#21644;&#26377;&#21521;&#22270;&#12290;&#33410;&#28857;&#32423;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#19982;&#27599;&#23618;&#36807;&#28388;&#22120;&#20013;&#30340;&#8220;&#20805;&#20998;&#65288;&#35889;&#65289;&#35206;&#30422;&#8221;&#23646;&#24615;&#26377;&#20851;&#12290;&#36793;&#32423;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#19982;Lipschitz&#24120;&#25968;&#21644;&#26032;&#24341;&#20837;&#30340;&#36807;&#28388;&#22120;&#21322;&#33539;&#25968;&#26377;&#20851;&#12290;&#36890;&#36807;&#26368;&#36817;&#24320;&#21457;&#30340;&#22522;&#20110;&#25968;&#23398;&#29289;&#29702;&#24037;&#20855;&#33719;&#24471;&#20102;&#20851;&#20110;&#25299;&#25169;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#32467;&#26524;&#12290;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#32780;&#26032;&#39062;&#30340;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#21482;&#26377;&#22312;GSO&#26159;t&#26102;&#65292;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#22270;&#31895;&#31890;&#21270;&#36807;&#31243;&#20013;&#65288;&#36890;&#36807;&#29992;&#21333;&#20010;&#33410;&#28857;&#26367;&#25442;&#24378;&#36830;&#36890;&#23376;&#22270;&#65289;&#26159;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work establishes rigorous, novel and widely applicable stability guarantees and transferability bounds for graph convolutional networks -without reference to any underlying limit object or statistical distribution. Crucially, utilized graph-shift operators (GSOs) are not necessarily assumed to be normal, allowing for the treatment of networks on both undirected- and for the first time also directed graphs. Stability to node-level perturbations is related to an 'adequate (spectral) covering' property of the filters in each layer. Stability to edge-level perturbations is related to Lipschitz constants and newly introduced semi-norms of filters. Results on stability to topological perturbations are obtained through recently developed mathematical-physics based tools. As an important and novel example, it is showcased that graph convolutional networks are stable under graph-coarse-graining procedures (replacing strongly-connected sub-graphs by single nodes) precisely if the GSO is t
&lt;/p&gt;</description></item><item><title>MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11259</link><description>&lt;p&gt;
&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#29983;&#25104;&#19982;&#33258;&#25105;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11259
&lt;/p&gt;
&lt;p&gt;
MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#29983;&#25104;&#24050;&#32463;&#21463;&#21040;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#20854;&#38761;&#26032;&#20102;&#31185;&#23398;&#23478;&#35774;&#35745;&#20998;&#23376;&#32467;&#26500;&#30340;&#26041;&#24335;&#65292;&#24182;&#20026;&#21270;&#23398;&#21644;&#33647;&#29289;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#20998;&#23376;&#29983;&#25104;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#29983;&#25104;&#35821;&#27861;&#25110;&#21270;&#23398;&#23384;&#22312;&#32570;&#38519;&#30340;&#20998;&#23376;&#65292;&#29421;&#31364;&#30340;&#39046;&#22495;&#19987;&#27880;&#20197;&#21450;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#25110;&#22806;&#37096;&#20998;&#23376;&#25968;&#25454;&#24211;&#32780;&#38480;&#21046;&#20102;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MolGen&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20998;&#23376;&#35821;&#35328;&#27169;&#22411;&#12290;MolGen&#36890;&#36807;&#37325;&#26500;&#19968;&#20159;&#22810;&#20010;&#20998;&#23376;SELFIES&#33719;&#24471;&#20102;&#22266;&#26377;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#20419;&#36827;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21453;&#39304;&#33539;&#24335;&#65292;&#21551;&#21457;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#26368;&#32456;&#19979;&#28216;&#30446;&#26631;&#23545;&#40784;&#65292;&#26377;&#21161;&#20110;&#26356;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MolGen&#22312;&#21270;&#23398;&#26377;&#25928;&#24615;&#65292;&#22810;&#26679;&#24615;&#65292;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of molecules with desired properties has gained tremendous popularity, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face numerous challenges such as the generation of syntactically or chemically flawed molecules, narrow domain focus, and limitations in creating diverse and directionally feasible molecules due to a dearth of annotated data or external molecular databases. To this end, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. MolGen acquires intrinsic structural and grammatical insights by reconstructing over 100 million molecular SELFIES, while facilitating knowledge transfer between different domains through domain-agnostic molecular prefix tuning. Moreover, we present a self-feedback paradigm that inspires the pre-trained model to align with the ulti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#20581;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#20803;&#31574;&#30053;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#29615;&#22659;&#21644;&#20219;&#21153;&#21464;&#21270;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#36807;&#37319;&#26679;&#26356;&#38590;&#30340;&#20219;&#21153;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.11147</link><description>&lt;p&gt;
&#21051;&#33510;&#35757;&#32451;&#65292;&#36731;&#26494;&#25112;&#26007;&#65306;&#24378;&#20581;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Train Hard, Fight Easy: Robust Meta Reinforcement Learning. (arXiv:2301.11147v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#20581;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#20803;&#31574;&#30053;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#29615;&#22659;&#21644;&#20219;&#21153;&#21464;&#21270;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#36807;&#37319;&#26679;&#26356;&#38590;&#30340;&#20219;&#21153;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38754;&#20020;&#30528;&#29615;&#22659;&#12289;&#20219;&#21153;&#25110;&#23458;&#25143;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20803;&#24378;&#21270;&#23398;&#20064;(MRL)&#36890;&#36807;&#23398;&#20064;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#20803;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26631;&#20934;&#30340;MRL&#26041;&#27861;&#20248;&#21270;&#20219;&#21153;&#30340;&#24179;&#22343;&#22238;&#25253;&#65292;&#20294;&#22312;&#39640;&#39118;&#38505;&#25110;&#39640;&#38590;&#24230;&#30340;&#20219;&#21153;&#20013;&#24448;&#24448;&#32467;&#26524;&#19981;&#20339;&#12290;&#30001;&#20110;&#20107;&#20808;&#19981;&#30693;&#36947;&#27979;&#35797;&#20219;&#21153;&#65292;&#36825;&#38480;&#21046;&#20102;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20855;&#26377;&#21487;&#25511;&#40065;&#26834;&#24615;&#27700;&#24179;&#30340;&#24378;&#20581;MRL&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;MRL&#26694;&#26550;&#20013;&#65292;&#26799;&#24230;&#20559;&#24046;&#28040;&#22833;&#20102;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;Robust Meta RL&#31639;&#27861;&#65288;RoML&#65289;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#25968;&#25454;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;RoML&#26159;&#19968;&#20010;&#20803;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35782;&#21035;&#21644;&#36807;&#37319;&#26679;&#26356;&#38590;&#30340;&#20219;&#21153;&#26469;&#29983;&#25104;&#20219;&#20309;&#32473;&#23450;MRL&#31639;&#27861;&#30340;&#24378;&#20581;&#29256;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RoML&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#24378;&#20581;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge of reinforcement learning (RL) in real-world applications is the variation between environments, tasks or clients. Meta-RL (MRL) addresses this issue by learning a meta-policy that adapts to new tasks. Standard MRL methods optimize the average return over tasks, but often suffer from poor results in tasks of high risk or difficulty. This limits system reliability since test tasks are not known in advance. In this work, we define a robust MRL objective with a controlled robustness level. Optimization of analogous robust objectives in RL is known to lead to both *biased gradients* and *data inefficiency*. We prove that the gradient bias disappears in our proposed MRL framework. The data inefficiency is addressed via the novel Robust Meta RL algorithm (RoML). RoML is a meta-algorithm that generates a robust version of any given MRL algorithm, by identifying and over-sampling harder tasks throughout training. We demonstrate that RoML achieves robust returns on multiple na
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36830;&#32493;&#21160;&#20316;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21307;&#30103;&#24212;&#29992;&#20013;&#30830;&#23450;&#26368;&#20339;&#21058;&#37327;&#27700;&#24179;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.08940</link><description>&lt;p&gt;
&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#30340;&#20934;&#26368;&#20248;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Quasi-optimal Reinforcement Learning with Continuous Actions. (arXiv:2301.08940v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36830;&#32493;&#21160;&#20316;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21307;&#30103;&#24212;&#29992;&#20013;&#30830;&#23450;&#26368;&#20339;&#21058;&#37327;&#27700;&#24179;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#38656;&#35201;&#22312;&#36830;&#32493;&#21160;&#20316;&#29615;&#22659;&#20013;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#21307;&#30103;&#27835;&#30103;&#26041;&#26696;&#30340;&#24320;&#21457;&#20013;&#65292;&#30830;&#23450;&#26368;&#20339;&#21058;&#37327;&#27700;&#24179;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#65292;&#27969;&#34892;&#30340;&#26080;&#31351;&#25903;&#25345;&#38543;&#26426;&#31574;&#30053;&#65288;&#20363;&#22914;&#39640;&#26031;&#31574;&#30053;&#65289;&#21487;&#33021;&#20250;&#20998;&#37197;&#36807;&#39640;&#30340;&#21058;&#37327;&#65292;&#20005;&#37325;&#21361;&#23475;&#24739;&#32773;&#12290;&#22240;&#27492;&#65292;&#24341;&#23548;&#19968;&#20010;&#25903;&#25345;&#20165;&#21253;&#21547;&#36817;&#20284;&#26368;&#20248;&#21160;&#20316;&#30340;&#31574;&#30053;&#31867;&#21035;&#65292;&#24182;&#32553;&#23567;&#25928;&#26524;&#21644;&#21487;&#38752;&#24615;&#30340;&#21160;&#20316;&#25628;&#32034;&#21306;&#22495;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#20934;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#8221;&#65292;&#35813;&#31639;&#27861;&#22312;&#31163;&#32447;&#31574;&#30053;&#35774;&#32622;&#19979;&#21487;&#20197;&#36731;&#26494;&#20248;&#21270;&#65292;&#24182;&#22312;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#20445;&#35777;&#25910;&#25947;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#19968;&#33268;&#24615;&#12289;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#36866;&#24212;&#24615;&#21644;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#30340;&#27169;&#25311;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world applications of reinforcement learning (RL) require making decisions in continuous action environments. In particular, determining the optimal dose level plays a vital role in developing medical treatment regimes. One challenge in adapting existing RL algorithms to medical applications, however, is that the popular infinite support stochastic policies, e.g., Gaussian policy, may assign riskily high dosages and harm patients seriously. Hence, it is important to induce a policy class whose support only contains near-optimal actions, and shrink the action-searching area for effectiveness and reliability. To achieve this, we develop a novel \emph{quasi-optimal learning algorithm}, which can be easily optimized in off-policy settings with guaranteed convergence under general function approximations. Theoretically, we analyze the consistency, sample complexity, adaptability, and convergence of the proposed algorithm. We evaluate our algorithm with comprehensive simulated expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.05599</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#30340;&#30701;SSVEP&#25968;&#25454;&#25193;&#23637;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Short-length SSVEP data extension by a novel generative adversarial networks based framework. (arXiv:2301.05599v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;SSVEP&#30340;&#33041;&#26426;&#25509;&#21475;&#22240;&#20854;&#39640;&#20449;&#24687;&#20256;&#36755;&#36895;&#29575;&#21644;&#30446;&#26631;&#25968;&#37327;&#21487;&#29992;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#39057;&#29575;&#35782;&#21035;&#26041;&#27861;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#29992;&#25143;&#26657;&#20934;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#25968;&#25454;&#38271;&#24230;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#26469;&#21019;&#24314;&#21512;&#25104;&#30340;&#33041;&#30005;&#25968;&#25454;&#65292;&#26377;&#26395;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GANs&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#29992;&#20110;&#25968;&#25454;&#38271;&#24230;&#25193;&#23637;&#12290;TEGAN&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;&#26032;&#39062;&#30340;U&#22411;&#29983;&#25104;&#22120;&#26550;&#26500;&#21644;&#19968;&#20010;&#36741;&#21161;&#20998;&#31867;&#22120;&#21152;&#20837;&#21040;&#32593;&#32476;&#32467;&#26500;&#20013;&#65292;TEGAN&#21487;&#20197;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#20135;&#29983;&#26377;&#26465;&#20214;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#39057;&#29575;&#35782;&#21035;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;TEGAN&#29983;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;TEGAN&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;TEGAN&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#65292;&#20943;&#23569;&#25152;&#38656;&#30340;&#26657;&#20934;&#26102;&#38388;&#24182;&#25913;&#21892;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steady-state visual evoked potentials (SSVEPs) based brain-computer interface (BCI) has received considerable attention due to its high information transfer rate (ITR) and available quantity of targets. However, the performance of frequency identification methods heavily hinges on the amount of user calibration data and data length, which hinders the deployment in real-world applications. Recently, generative adversarial networks (GANs)-based data generation methods have been widely adopted to create synthetic electroencephalography (EEG) data, holds promise to address these issues. In this paper, we proposed a GAN-based end-to-end signal transformation network for data length extension, termed as TEGAN. TEGAN transforms short-length SSVEP signals into long-length artificial SSVEP signals. By incorporating a novel U-Net generator architecture and an auxiliary classifier into the network architecture, the TEGAN could produce conditioned features in the synthetic data. Additionally, we i
&lt;/p&gt;</description></item><item><title>&#40664;&#40664;&#26432;&#25163;&#26159;&#19968;&#31181;&#38544;&#34109;&#30340;&#12289;&#26080;&#26631;&#31614;&#30340;&#12289;&#40657;&#30418;&#23376;&#21518;&#38376;&#25915;&#20987;&#65292;&#23427;&#20351;&#29992;&#20102;&#38544;&#34109;&#30340;&#27602;&#29289;&#21644;&#35302;&#21457;&#22120;&#65292;&#22312;&#26080;&#26631;&#31614;&#25915;&#20987;&#20013;&#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#36890;&#36807;&#28176;&#21464;&#23545;&#40784;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;MNIST&#12289;CIFAR10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.02615</link><description>&lt;p&gt;
&#40664;&#40664;&#26432;&#25163;: &#19968;&#31181;&#38544;&#34109;&#30340;&#12289;&#26080;&#26631;&#31614;&#30340;&#12289;&#40657;&#30418;&#23376;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack. (arXiv:2301.02615v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02615
&lt;/p&gt;
&lt;p&gt;
&#40664;&#40664;&#26432;&#25163;&#26159;&#19968;&#31181;&#38544;&#34109;&#30340;&#12289;&#26080;&#26631;&#31614;&#30340;&#12289;&#40657;&#30418;&#23376;&#21518;&#38376;&#25915;&#20987;&#65292;&#23427;&#20351;&#29992;&#20102;&#38544;&#34109;&#30340;&#27602;&#29289;&#21644;&#35302;&#21457;&#22120;&#65292;&#22312;&#26080;&#26631;&#31614;&#25915;&#20987;&#20013;&#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#36890;&#36807;&#28176;&#21464;&#23545;&#40784;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;MNIST&#12289;CIFAR10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#27745;&#26579;&#25915;&#20987;&#23545;&#31070;&#32463;&#32593;&#32476;&#26500;&#25104;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#23485;&#26494;&#30340;&#23041;&#32961;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#40664;&#40664;&#26432;&#25163;&#30340;&#26032;&#22411;&#25915;&#20987;&#65292;&#22312;&#26080;&#26631;&#31614;&#30340;&#40657;&#30418;&#23376;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#20351;&#29992;&#38544;&#34109;&#30340;&#27602;&#29289;&#21644;&#35302;&#21457;&#22120;&#65292;&#24182;&#19988;&#32988;&#36807;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26080;&#26631;&#31614;&#25915;&#20987;&#20013;&#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#20316;&#20026;&#35302;&#21457;&#22120;&#30340;&#26041;&#27861;&#65292;&#22312;&#27602;&#26631;&#31614;&#35774;&#32622;&#19979;&#30340;&#25104;&#21151;&#26696;&#20363;&#20043;&#21518;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#22825;&#30495;&#30340;&#36866;&#24212;&#26041;&#27861;&#30340;&#25104;&#21151;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#38656;&#35201;&#28176;&#21464;&#23545;&#40784;&#20197;&#30830;&#20445;&#39640;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#23545;MNIST&#12289;CIFAR10&#21644;&#19968;&#20010;&#32553;&#23567;&#29256;&#30340;ImageNet&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor poisoning attacks pose a well-known risk to neural networks. However, most studies have focused on lenient threat models. We introduce Silent Killer, a novel attack that operates in clean-label, black-box settings, uses a stealthy poison and trigger and outperforms existing methods. We investigate the use of universal adversarial perturbations as triggers in clean-label attacks, following the success of such approaches under poison-label settings. We analyze the success of a naive adaptation and find that gradient alignment for crafting the poison is required to ensure high success rates. We conduct thorough experiments on MNIST, CIFAR10, and a reduced version of ImageNet and achieve state-of-the-art results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#36807;&#31243;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#33719;&#21462;&#19968;&#20010;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#36890;&#29992;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#36739;&#20026;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.01913</link><description>&lt;p&gt;
&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#23398;&#20064;&#36890;&#29992;&#30340;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learning a Generic Value-Selection Heuristic Inside a Constraint Programming Solver. (arXiv:2301.01913v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#36807;&#31243;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#33719;&#21462;&#19968;&#20010;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#36890;&#29992;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#36739;&#20026;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#35268;&#21010;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#30340;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#12290;&#27714;&#35299;&#22120;&#20013;&#30340;&#37325;&#35201;&#35774;&#35745;&#36873;&#25321;&#26159;&#20998;&#25903;&#21551;&#21457;&#24335;&#65292;&#23427;&#20204;&#26088;&#22312;&#22312;&#26368;&#30701;&#30340;&#26102;&#38388;&#20869;&#23547;&#25214;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#20123;&#21551;&#21457;&#24335;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#24182;&#38656;&#35201;&#38382;&#39064;&#29305;&#23450;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#28608;&#21457;&#20102;&#35768;&#22810;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#23398;&#20064;&#39640;&#25928;&#21551;&#21457;&#24335;&#30340;&#21162;&#21147;&#65292;&#32780;&#26080;&#38656;&#19987;&#23478;&#24178;&#39044;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#26377;&#20960;&#31181;&#36890;&#29992;&#30340;&#21464;&#37327;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#36890;&#29992;&#30340;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#36873;&#25321;&#21364;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#36890;&#29992;&#30340;&#23398;&#20064;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#36807;&#31243;&#21487;&#20197;&#29992;&#20110;&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#33719;&#24471;&#19968;&#20010;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#36825;&#24471;&#30410;&#20110;&#28145;&#24230;Q&#23398;&#20064;&#31639;&#27861;&#21644;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Constraint programming is known for being an efficient approach for solving combinatorial problems. Important design choices in a solver are the branching heuristics, which are designed to lead the search to the best solutions in a minimum amount of time. However, developing these heuristics is a time-consuming process that requires problem-specific expertise. This observation has motivated many efforts to use machine learning to automatically learn efficient heuristics without expert intervention. To the best of our knowledge, it is still an open research question. Although several generic variable-selection heuristics are available in the literature, the options for a generic value-selection heuristic are more scarce. In this paper, we propose to tackle this issue by introducing a generic learning procedure that can be used to obtain a value-selection heuristic inside a constraint programming solver. This has been achieved thanks to the combination of a deep Q-learning algorithm, a t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#39118;&#38505;&#22235;&#26041;&#29702;&#35770;&#65292;&#30740;&#31350;&#20102;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;SVR&#30340;&#20004;&#31181;&#24418;&#24335;&#23545;&#24212;&#20110;&#31561;&#25928;&#35823;&#24046;&#24230;&#37327;&#30340;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#21152;&#19978;&#27491;&#21017;&#21270;&#24809;&#32602;&#39033;&#12290;&#36890;&#36807;&#26500;&#36896;&#22522;&#26412;&#39118;&#38505;&#22235;&#26041;&#26694;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SVR&#26159;&#23545;&#20004;&#20010;&#23545;&#31216;&#26465;&#20214;&#20998;&#20301;&#25968;&#30340;&#24179;&#22343;&#25968;&#30340;&#28176;&#36817;&#26080;&#20559;&#20272;&#35745;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;$\varepsilon$-SVR&#21644;$\nu$-SVR&#22312;&#19968;&#33324;&#38543;&#26426;&#29615;&#22659;&#19979;&#30340;&#31561;&#20215;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.09178</link><description>&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;: &#39118;&#38505;&#22235;&#26041;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Support Vector Regression: Risk Quadrangle Framework. (arXiv:2212.09178v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#39118;&#38505;&#22235;&#26041;&#29702;&#35770;&#65292;&#30740;&#31350;&#20102;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;SVR&#30340;&#20004;&#31181;&#24418;&#24335;&#23545;&#24212;&#20110;&#31561;&#25928;&#35823;&#24046;&#24230;&#37327;&#30340;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#21152;&#19978;&#27491;&#21017;&#21270;&#24809;&#32602;&#39033;&#12290;&#36890;&#36807;&#26500;&#36896;&#22522;&#26412;&#39118;&#38505;&#22235;&#26041;&#26694;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SVR&#26159;&#23545;&#20004;&#20010;&#23545;&#31216;&#26465;&#20214;&#20998;&#20301;&#25968;&#30340;&#24179;&#22343;&#25968;&#30340;&#28176;&#36817;&#26080;&#20559;&#20272;&#35745;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;$\varepsilon$-SVR&#21644;$\nu$-SVR&#22312;&#19968;&#33324;&#38543;&#26426;&#29615;&#22659;&#19979;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22522;&#26412;&#30340;&#39118;&#38505;&#22235;&#26041;&#29702;&#35770;&#30340;&#32972;&#26223;&#19979;&#30740;&#31350;&#20102;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#65292;&#35813;&#29702;&#35770;&#23558;&#20248;&#21270;&#12289;&#39118;&#38505;&#31649;&#29702;&#21644;&#32479;&#35745;&#20272;&#35745;&#32852;&#31995;&#36215;&#26469;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;SVR&#30340;&#20004;&#31181;&#24418;&#24335;&#65292;$\varepsilon$-SVR&#21644;$\nu$-SVR&#65292;&#37117;&#23545;&#24212;&#20110;&#31561;&#25928;&#35823;&#24046;&#24230;&#37327;&#65288;&#20998;&#21035;&#20026;Vapnik&#35823;&#24046;&#21644;CVaR&#33539;&#25968;&#65289;&#30340;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#21152;&#19978;&#27491;&#21017;&#21270;&#24809;&#32602;&#39033;&#12290;&#36825;&#20123;&#35823;&#24046;&#24230;&#37327;&#21448;&#23450;&#20041;&#20102;&#30456;&#24212;&#30340;&#39118;&#38505;&#22235;&#26041;&#26694;&#12290;&#36890;&#36807;&#26500;&#36896;&#19982;SVR&#23545;&#24212;&#30340;&#22522;&#26412;&#39118;&#38505;&#22235;&#26041;&#26694;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SVR&#26159;&#20004;&#20010;&#23545;&#31216;&#26465;&#20214;&#20998;&#20301;&#25968;&#30340;&#24179;&#22343;&#25968;&#30340;&#28176;&#36817;&#26080;&#20559;&#20272;&#35745;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#19968;&#33324;&#38543;&#26426;&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;$\varepsilon$-SVR&#21644;$\nu$-SVR&#30340;&#31561;&#20215;&#24615;&#12290;&#27492;&#22806;&#65292;SVR&#34987;&#34920;&#36848;&#20026;&#24102;&#26377;&#27491;&#21017;&#21270;&#24809;&#32602;&#39033;&#30340;&#27491;&#21017;&#20559;&#31163;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25512;&#23548;&#20102;&#22312;&#39118;&#38505;&#22235;&#26041;&#26694;&#26550;&#20013;&#30340;SVR&#30340;&#23545;&#20598;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates Support Vector Regression (SVR) in the context of the fundamental risk quadrangle theory, which links optimization, risk management, and statistical estimation. It is shown that both formulations of SVR, $\varepsilon$-SVR and $\nu$-SVR, correspond to the minimization of equivalent error measures (Vapnik error and CVaR norm, respectively) with a regularization penalty. These error measures, in turn, define the corresponding risk quadrangles. By constructing the fundamental risk quadrangle, which corresponds to SVR, we show that SVR is the asymptotically unbiased estimator of the average of two symmetric conditional quantiles. Further, we prove the equivalence of the $\varepsilon$-SVR and $\nu$-SVR in a general stochastic setting. Additionally, SVR is formulated as a regular deviation minimization problem with a regularization penalty. Finally, the dual formulation of SVR in the risk quadrangle framework is derived.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#27969;&#20998;&#26512;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#28431;&#27934;&#26816;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#20102;DeepDFA&#26694;&#26550;&#21644;&#23884;&#20837;&#25216;&#26415;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#20195;&#30721;&#35821;&#20041;&#30340;&#26356;&#39640;&#25928;&#25429;&#25417;&#65292;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#22312;&#28431;&#27934;&#26816;&#27979;&#20013;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#24615;&#33021;&#12290;DeepDFA&#35757;&#32451;&#26102;&#38388;&#21482;&#38656;9&#20998;&#38047;&#65292;&#19988;&#36229;&#36807;&#20102;&#25152;&#26377;&#38750;transformer&#22522;&#32447;&#27169;&#22411;75&#20493;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.08108</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#27969;&#20998;&#26512;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#39640;&#25928;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection. (arXiv:2212.08108v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#27969;&#20998;&#26512;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#28431;&#27934;&#26816;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#20102;DeepDFA&#26694;&#26550;&#21644;&#23884;&#20837;&#25216;&#26415;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#20195;&#30721;&#35821;&#20041;&#30340;&#26356;&#39640;&#25928;&#25429;&#25417;&#65292;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#22312;&#28431;&#27934;&#26816;&#27979;&#20013;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#24615;&#33021;&#12290;DeepDFA&#35757;&#32451;&#26102;&#38388;&#21482;&#38656;9&#20998;&#38047;&#65292;&#19988;&#36229;&#36807;&#20102;&#25152;&#26377;&#38750;transformer&#22522;&#32447;&#27169;&#22411;75&#20493;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28431;&#27934;&#26816;&#27979;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19968;&#20123;&#30740;&#31350;&#20013;&#36229;&#36807;&#20102;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#26368;&#39640;&#24615;&#33021;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;token&#30340;transformer&#27169;&#22411;&#65292;&#36825;&#19981;&#26159;&#25429;&#25417;&#28431;&#27934;&#26816;&#27979;&#25152;&#38656;&#30340;&#20195;&#30721;&#35821;&#20041;&#26368;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#31243;&#24207;&#20998;&#26512;&#25216;&#26415;&#65292;&#22914;&#25968;&#25454;&#27969;&#20998;&#26512;&#65292;&#21487;&#20197;&#26681;&#25454;&#20854;&#26681;&#26412;&#21407;&#22240;&#26816;&#27979;&#20986;&#35768;&#22810;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#27492;&#31867;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#28431;&#27934;&#26816;&#27979;&#31639;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#28431;&#27934;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DeepDFA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#27969;&#20998;&#26512;&#21551;&#21457;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;&#21644;&#19968;&#31181;&#23884;&#20837;&#25216;&#26415;&#65292;&#21487;&#20197;&#20351;&#22270;&#23398;&#20064;&#27169;&#25311;&#25968;&#25454;&#27969;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DeepDFA&#26082;&#20855;&#26377;&#24615;&#33021;&#21448;&#20855;&#26377;&#25928;&#29575;&#12290;DeepDFA&#36229;&#36807;&#20102;&#25152;&#26377;&#38750;transformer&#22522;&#32447;&#27169;&#22411;&#12290;&#23427;&#30340;&#35757;&#32451;&#26102;&#38388;&#21482;&#38656;9&#20998;&#38047;&#65292;&#27604;&#20855;&#26377;&#26368;&#39640;&#24615;&#33021;&#30340;&#22522;&#32447;&#27169;&#22411;&#24555;75&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based vulnerability detection has shown great performance and, in some studies, outperformed static analysis tools. However, the highest-performing approaches use token-based transformer models, which are not the most efficient to capture code semantics required for vulnerability detection. Classical program analysis techniques such as dataflow analysis can detect many types of bugs based on their root causes. In this paper, we propose to combine such causal-based vulnerability detection algorithms with deep learning, aiming to achieve more efficient and effective vulnerability detection. Specifically, we designed DeepDFA, a dataflow analysis-inspired graph learning framework and an embedding technique that enables graph learning to simulate dataflow computation. We show that DeepDFA is both performant and efficient. DeepDFA outperformed all non-transformer baselines. It was trained in 9 minutes, 75x faster than the highest-performing baseline model. When using only 50+ v
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#23384;&#22312;&#38750;&#39640;&#26031;&#22122;&#22768;&#24773;&#20917;&#19979;&#22833;&#25928;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#33021;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#27491;&#30830;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#36890;&#36807;&#22810;&#20010;&#20363;&#23376;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.15498</link><description>&lt;p&gt;
&#20855;&#26377;&#26410;&#30693;&#27979;&#37327;&#22122;&#22768;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks with unknown measurement noise. (arXiv:2211.15498v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15498
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#23384;&#22312;&#38750;&#39640;&#26031;&#22122;&#22768;&#24773;&#20917;&#19979;&#22833;&#25928;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#33021;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#27491;&#30830;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#36890;&#36807;&#22810;&#20010;&#20363;&#23376;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26159;&#19968;&#31181;&#26082;&#33021;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#21448;&#33021;&#35782;&#21035;&#20559;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#30456;&#20851;&#30340;&#30740;&#31350;&#37117;&#20551;&#35774;&#25968;&#25454;&#26159;&#26080;&#22122;&#22768;&#30340;&#65292;&#25110;&#32773;&#26159;&#21463;&#24369;&#39640;&#26031;&#22122;&#22768;&#27745;&#26579;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;PINN&#26694;&#26550;&#22312;&#38750;&#39640;&#26031;&#22122;&#22768;&#24773;&#20917;&#19979;&#22833;&#25928;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#26681;&#26412;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#33021;&#37327;&#27169;&#22411;(Energy-Based Model, EBM)&#26469;&#23398;&#20064;&#27491;&#30830;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#20363;&#23376;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples.
&lt;/p&gt;</description></item><item><title>ARISE&#26159;&#19968;&#20010;&#22522;&#20110;&#23646;&#24615;&#32593;&#32476;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#23376;&#32467;&#26500;&#26469;&#25552;&#39640;&#25299;&#25169;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.15255</link><description>&lt;p&gt;
ARISE&#65306;&#36890;&#36807;&#23376;&#32467;&#26500;&#35748;&#30693;&#22312;&#23646;&#24615;&#32593;&#32476;&#19978;&#36827;&#34892;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ARISE: Graph Anomaly Detection on Attributed Networks via Substructure Awareness. (arXiv:2211.15255v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15255
&lt;/p&gt;
&lt;p&gt;
ARISE&#26159;&#19968;&#20010;&#22522;&#20110;&#23646;&#24615;&#32593;&#32476;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#23376;&#32467;&#26500;&#26469;&#25552;&#39640;&#25299;&#25169;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23646;&#24615;&#32593;&#32476;&#19978;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#24341;&#36215;&#20102;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#38500;&#20102;&#23646;&#24615;&#24322;&#24120;&#22806;&#65292;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#36824;&#26088;&#22312;&#26816;&#27979;&#34920;&#29616;&#20986;&#38598;&#20307;&#24322;&#24120;&#34892;&#20026;&#30340;&#20855;&#26377;&#21487;&#30097;&#25299;&#25169;&#24322;&#24120;&#30340;&#33410;&#28857;&#12290;&#32039;&#23494;&#36830;&#25509;&#30340;&#19981;&#30456;&#20851;&#33410;&#28857;&#32452;&#22312;&#32593;&#32476;&#20013;&#24418;&#25104;&#24322;&#24120;&#23494;&#38598;&#30340;&#23376;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#36890;&#36807;&#35782;&#21035;&#36825;&#31181;&#38598;&#20307;&#27169;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#25299;&#25169;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#30340;&#20107;&#23454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#32593;&#32476;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#23376;&#32467;&#26500;&#35748;&#30693;&#65288;&#31616;&#31216;ARISE&#65289;&#12290;&#19982;&#20197;&#24448;&#30340;&#31639;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#35782;&#21035;&#22270;&#20013;&#30340;&#23376;&#32467;&#26500;&#20197;&#36776;&#21035;&#24322;&#24120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21306;&#22495;&#25552;&#26696;&#27169;&#22359;&#65292;&#20197;&#21457;&#29616;&#32593;&#32476;&#20013;&#30340;&#39640;&#23494;&#24230;&#23376;&#32467;&#26500;&#20316;&#20026;&#21487;&#30097;&#21306;&#22495;&#12290;&#33410;&#28857;&#23545;&#30456;&#20284;&#24230;&#30340;&#24179;&#22343;&#20540;&#21487;&#34987;&#35270;&#20026;&#23376;&#32467;&#26500;&#20869;&#33410;&#28857;&#30340;&#25299;&#25169;&#24322;&#24120;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, graph anomaly detection on attributed networks has attracted growing attention in data mining and machine learning communities. Apart from attribute anomalies, graph anomaly detection also aims at suspicious topological-abnormal nodes that exhibit collective anomalous behavior. Closely connected uncorrelated node groups form uncommonly dense substructures in the network. However, existing methods overlook that the topology anomaly detection performance can be improved by recognizing such a collective pattern. To this end, we propose a new graph anomaly detection framework on attributed networks via substructure awareness (ARISE for abbreviation). Unlike previous algorithms, we focus on the substructures in the graph to discern abnormalities. Specifically, we establish a region proposal module to discover high-density substructures in the network as suspicious regions. The average node-pair similarity can be regarded as the topology anomaly degree of nodes within substructures
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#65292;&#21487;&#20197;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#27491;&#30830;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#36991;&#20813;&#27425;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2211.13723</link><description>&lt;p&gt;
&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#26469;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Multi-task Learning via Seeking Task-based Flat Regions. (arXiv:2211.13723v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13723
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#65292;&#21487;&#20197;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#27491;&#30830;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#36991;&#20813;&#27425;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#19988;&#24378;&#22823;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#39592;&#24178;&#23398;&#20064;&#22810;&#20010;&#30446;&#26631;&#12290;&#19982;&#21333;&#29420;&#35757;&#32451;&#20219;&#21153;&#30456;&#27604;&#65292;MTL&#26174;&#30528;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#26469;&#28508;&#22312;&#22320;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#23427;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#65292;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#12290;&#20854;&#20013;&#65292;MTL&#30340;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#38598;&#20013;&#22312;&#25805;&#32437;&#20219;&#21153;&#26799;&#24230;&#20197;&#25512;&#23548;&#20986;&#23545;&#25152;&#26377;&#20219;&#21153;&#26377;&#30410;&#30340;&#26368;&#32456;&#26799;&#24230;&#19979;&#38477;&#26041;&#21521;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#22312;&#23454;&#38469;&#38382;&#39064;&#19978;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#32780;&#19981;&#20351;&#29992;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;&#26631;&#20934;&#35757;&#32451;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#26368;&#23567;&#21270;&#32463;&#39564;&#25439;&#22833;&#65292;&#24456;&#23481;&#26131;&#36973;&#21463;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning (MTL) is a widely-used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions on real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#30340;&#38750;IID&#25968;&#25454;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#19987;&#19994;&#21270;&#20027;&#21160;&#25277;&#26679;&#21644;&#30693;&#35782;&#34917;&#20607;&#32852;&#37030;&#26356;&#26032;&#26469;&#35299;&#20915;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.13579</link><description>&lt;p&gt;
&#30693;&#35782;&#24863;&#30693;&#30340;&#38750;IID&#25968;&#25454;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Aware Federated Active Learning with Non-IID Data. (arXiv:2211.13579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#30340;&#38750;IID&#25968;&#25454;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#19987;&#19994;&#21270;&#20027;&#21160;&#25277;&#26679;&#21644;&#30693;&#35782;&#34917;&#20607;&#32852;&#37030;&#26356;&#26032;&#26469;&#35299;&#20915;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#26412;&#22320;&#25968;&#25454;&#26631;&#31614;&#30340;&#26114;&#36149;&#27880;&#37322;&#25104;&#26412;&#20173;&#28982;&#26159;&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#30340;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#22312;&#26377;&#38480;&#27880;&#37322;&#39044;&#31639;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20197;&#20998;&#25955;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#39640;&#25928;&#22320;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#12290;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#26381;&#21153;&#22120;&#31471;&#20840;&#23616;&#27169;&#22411;&#30340;&#20027;&#21160;&#25277;&#26679;&#30446;&#26631;&#19982;&#24322;&#27493;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#12290;&#24403;&#25968;&#25454;&#22312;&#26412;&#22320;&#23458;&#25143;&#31471;&#20043;&#38388;&#20998;&#24067;&#38750;IID&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#30340;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;(KAFAL)&#26041;&#27861;&#65292;&#23427;&#21253;&#25324;&#30693;&#35782;&#19987;&#19994;&#21270;&#20027;&#21160;&#25277;&#26679;(KSAS)&#21644;&#30693;&#35782;&#34917;&#20607;&#32852;&#37030;&#26356;&#26032;(KCFU)&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning enables multiple decentralized clients to learn collaboratively without sharing the local training data. However, the expensive annotation cost to acquire data labels on local clients remains an obstacle in utilizing local data. In this paper, we propose a federated active learning paradigm to efficiently learn a global model with limited annotation budget while protecting data privacy in a decentralized learning way. The main challenge faced by federated active learning is the mismatch between the active sampling goal of the global model on the server and that of the asynchronous local clients. This becomes even more significant when data is distributed non-IID across local clients. To address the aforementioned challenge, we propose Knowledge-Aware Federated Active Learning (KAFAL), which consists of Knowledge-Specialized Active Sampling (KSAS) and Knowledge-Compensatory Federated Update (KCFU). KSAS is a novel active sampling method tailored for the federated acti
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;&#65288;HELM&#65289;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#22330;&#26223;&#21644;&#24230;&#37327;&#36827;&#34892;&#20998;&#31867;&#24182;&#37319;&#29992;&#22810;&#24230;&#37327;&#26041;&#27861;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.09110</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Holistic Evaluation of Language Models. (arXiv:2211.09110v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09110
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;&#65288;HELM&#65289;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#22330;&#26223;&#21644;&#24230;&#37327;&#36827;&#34892;&#20998;&#31867;&#24182;&#37319;&#29992;&#22810;&#24230;&#37327;&#26041;&#27861;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#27491;&#22312;&#25104;&#20026;&#20960;&#20046;&#25152;&#26377;&#20027;&#35201;&#35821;&#35328;&#25216;&#26415;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#38480;&#21046;&#21644;&#39118;&#38505;&#24182;&#19981;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;&#65288;HELM&#65289;&#65292;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#24863;&#20852;&#36259;&#30340;&#28508;&#22312;&#22330;&#26223;&#65288;&#21363;&#29992;&#20363;&#65289;&#21644;&#24230;&#37327;&#65288;&#21363;&#26399;&#26395;&#65289;&#30340;&#24191;&#38420;&#31354;&#38388;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#20010;&#23485;&#27867;&#30340;&#23376;&#38598;&#65292;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#21644;&#21487;&#34892;&#24615;&#65292;&#27880;&#24847;&#21040;&#20102;&#32570;&#22833;&#25110;&#26410;&#20805;&#20998;&#20195;&#34920;&#30340;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#20026;&#34987;&#24573;&#35270;&#30340;&#33521;&#35821;&#26041;&#35328;&#36827;&#34892;&#38382;&#31572;&#65292;&#29992;&#20110;&#21487;&#20449;&#24230;&#30340;&#24230;&#37327;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#24230;&#37327;&#26041;&#27861;&#65306;&#25105;&#20204;&#20998;&#21035;&#38024;&#23545;&#27599;&#20010;&#26680;&#24515;&#22330;&#26223;&#27979;&#37327;&#20102;&#20934;&#30830;&#24230;&#12289;&#26657;&#20934;&#24230;&#12289;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#26377;&#27602;&#24615;&#21644;&#25928;&#29575;&#36825;7&#20010;&#24230;&#37327;&#25351;&#26631;&#65288;&#22312;87.5%&#30340;&#26102;&#38388;&#20869;&#65289;&#12290;&#36825;&#30830;&#20445;&#20102;&#20934;&#30830;&#24230;&#20197;&#22806;&#30340;&#24230;&#37327;&#19981;&#20250;&#34987;&#24573;&#35270;&#65292;&#24182;&#19988;&#26435;&#34913;&#28165;&#26224;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;7&#20010;&#38024;&#23545;&#24615;&#35780;&#20272;&#65292;&#22522;&#20110;26&#20010;&#38024;&#23545;&#24615;&#22330;&#26223;&#65292;&#20197;&#20998;&#26512;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze speci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#22411;&#29289;&#29702;&#30693;&#35782;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;(APINN)&#65292;&#37319;&#29992;&#36719;&#39046;&#22495;&#20998;&#35299;&#21644;&#21442;&#25968;&#20849;&#20139;&#65292;&#36890;&#36807;&#38376;&#25511;&#32593;&#32476;&#21021;&#22987;&#21270;&#21644;&#19968;&#33324;&#39046;&#22495;&#21644;&#20989;&#25968;&#20998;&#35299;&#26469;&#25913;&#36827;&#20102;&#25193;&#23637;&#29289;&#29702;&#30693;&#35782;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;(XPINN)&#21644;&#22522;&#26412;&#29289;&#29702;&#30693;&#35782;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;(PINN)&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.08939</link><description>&lt;p&gt;
&#22686;&#24378;&#22411;&#29289;&#29702;&#30693;&#35782;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476; (APINNs)&#65306;&#22522;&#20110;&#38376;&#25511;&#32593;&#32476;&#30340;&#36719;&#39046;&#22495;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Augmented Physics-Informed Neural Networks (APINNs): A gating network-based soft domain decomposition methodology. (arXiv:2211.08939v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#22411;&#29289;&#29702;&#30693;&#35782;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;(APINN)&#65292;&#37319;&#29992;&#36719;&#39046;&#22495;&#20998;&#35299;&#21644;&#21442;&#25968;&#20849;&#20139;&#65292;&#36890;&#36807;&#38376;&#25511;&#32593;&#32476;&#21021;&#22987;&#21270;&#21644;&#19968;&#33324;&#39046;&#22495;&#21644;&#20989;&#25968;&#20998;&#35299;&#26469;&#25913;&#36827;&#20102;&#25193;&#23637;&#29289;&#29702;&#30693;&#35782;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;(XPINN)&#21644;&#22522;&#26412;&#29289;&#29702;&#30693;&#35782;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;(PINN)&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#22411;&#29289;&#29702;&#30693;&#35782;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476; (APINN)&#65292;&#37319;&#29992;&#36719;&#21487;&#35757;&#32451;&#30340;&#39046;&#22495;&#20998;&#35299;&#21644;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#25193;&#23637;&#29289;&#29702;&#30693;&#35782;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476; (XPINN) &#21644;&#22522;&#26412;&#29289;&#29702;&#30693;&#35782;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476; (PINN) &#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#38376;&#25511;&#32593;&#32476;&#26469;&#27169;&#25311; XPINN &#30340;&#30828;&#20998;&#35299;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#24494;&#35843;&#20197;&#21457;&#29616;&#26356;&#22909;&#30340;&#20998;&#21306;&#12290;APINN&#30340;&#36755;&#20986;&#26159;&#20960;&#20010;&#23376;&#32593;&#32476;&#30340;&#26435;&#37325;&#24179;&#22343;&#20540;&#12290;APINN&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#30028;&#38754;&#26465;&#20214;&#65292;&#24182;&#19988;&#20854;&#23376;&#32593;&#32476;&#21487;&#20197;&#21033;&#29992;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20854;&#23376;&#22495;&#20013;&#30340;&#19968;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#12290;&#26368;&#21518;&#65292;&#27599;&#20010;&#23376;&#32593;&#32476;&#20849;&#20139;&#19968;&#37096;&#20998;&#20849;&#21516;&#21442;&#25968;&#65292;&#20197;&#25429;&#25417;&#27599;&#20010;&#20998;&#35299;&#20989;&#25968;&#20013;&#30340;&#30456;&#20284;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#32993;&#31561;&#20154;[2021]&#30340;PINN&#27867;&#21270;&#29702;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;APINN&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#38376;&#25511;&#32593;&#32476;&#21021;&#22987;&#21270;&#21644;&#19968;&#33324;&#39046;&#22495;&#21644;&#20989;&#25968;&#20998;&#35299;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;APINN&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the augmented physics-informed neural network (APINN), which adopts soft and trainable domain decomposition and flexible parameter sharing to further improve the extended PINN (XPINN) as well as the vanilla PINN methods. In particular, a trainable gate network is employed to mimic the hard decomposition of XPINN, which can be flexibly fine-tuned for discovering a potentially better partition. It weight-averages several sub-nets as the output of APINN. APINN does not require complex interface conditions, and its sub-nets can take advantage of all training samples rather than just part of the training data in their subdomains. Lastly, each sub-net shares part of the common parameters to capture the similar components in each decomposed function. Furthermore, following the PINN generalization theory in Hu et al. [2021], we show that APINN can improve generalization by proper gate network initialization and general domain &amp; function decomposition. Extensive experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#25968;&#25454;&#27745;&#26579;&#21518;&#38376;&#25915;&#20987;&#23545;&#23545;&#27604;&#23398;&#20064;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CorruptEncoder&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#29702;&#35770;&#23548;&#21521;&#30340;&#26041;&#24335;&#21019;&#24314;&#20248;&#21270;&#30340;&#27745;&#26579;&#36755;&#20837;&#65292;&#22823;&#24133;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CorruptEncoder&#26159;&#39318;&#20010;&#20165;&#38656;&#35201;&#23569;&#37327;&#22270;&#20687;&#21644;&#27745;&#26579;&#27604;&#20363;&#21363;&#21487;&#36798;&#21040;90%&#20197;&#19978;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23616;&#37096;&#35009;&#21098;&#30340;&#38450;&#24481;&#31574;&#30053;&#26469;&#24212;&#23545;&#25968;&#25454;&#27745;&#26579;&#21518;&#38376;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2211.08229</link><description>&lt;p&gt;
CorruptEncoder&#65306;&#22522;&#20110;&#25968;&#25454;&#27745;&#26579;&#30340;&#23545;&#27604;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning. (arXiv:2211.08229v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#25968;&#25454;&#27745;&#26579;&#21518;&#38376;&#25915;&#20987;&#23545;&#23545;&#27604;&#23398;&#20064;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CorruptEncoder&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#29702;&#35770;&#23548;&#21521;&#30340;&#26041;&#24335;&#21019;&#24314;&#20248;&#21270;&#30340;&#27745;&#26579;&#36755;&#20837;&#65292;&#22823;&#24133;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CorruptEncoder&#26159;&#39318;&#20010;&#20165;&#38656;&#35201;&#23569;&#37327;&#22270;&#20687;&#21644;&#27745;&#26579;&#27604;&#20363;&#21363;&#21487;&#36798;&#21040;90%&#20197;&#19978;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23616;&#37096;&#35009;&#21098;&#30340;&#38450;&#24481;&#31574;&#30053;&#26469;&#24212;&#23545;&#25968;&#25454;&#27745;&#26579;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#20351;&#29992;&#26080;&#26631;&#31614;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#36890;&#29992;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#22270;&#20687;&#25110;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#23545;&#27604;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#22522;&#20110;&#25968;&#25454;&#27745;&#26579;&#30340;&#21518;&#38376;&#25915;&#20987;&#65288;DPBA&#65289;&#30340;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#21521;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#34987;&#27745;&#26579;&#30340;&#36755;&#20837;&#26469;&#21518;&#38376;&#21270;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DPBA&#30340;&#25928;&#26524;&#26377;&#38480;&#12290;&#26412;&#25991;&#39318;&#20808;&#20998;&#26512;&#29616;&#26377;&#25915;&#20987;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CorruptEncoder&#30340;&#26032;&#22411;DPBA&#26469;&#23545;&#25239;&#23545;&#27604;&#23398;&#20064;&#12290;CorruptEncoder&#20351;&#29992;&#29702;&#35770;&#23548;&#21521;&#30340;&#26041;&#27861;&#21019;&#24314;&#26368;&#20248;&#30340;&#27745;&#26579;&#36755;&#20837;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CorruptEncoder&#22312;&#25915;&#20987;&#25928;&#26524;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;DPBA&#12290;&#23588;&#20854;&#26159;&#65292;CorruptEncoder&#26159;&#39318;&#20010;&#20165;&#38656;&#35201;&#23569;&#37327;&#65288;3&#20010;&#65289;&#21442;&#32771;&#22270;&#20687;&#21644;&#23567;&#35268;&#27169;&#27745;&#26579;&#27604;&#20363;&#65288;0.5%&#65289;&#21363;&#21487;&#36798;&#21040;90%&#20197;&#19978;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;DPBA&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23616;&#37096;&#35009;&#21098;&#30340;&#38450;&#24481;&#31574;&#30053;&#26469;&#25269;&#24481;DPBA&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#31574;&#30053;&#33021;&#26377;&#25928;&#25269;&#24481;DPBA&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning (CL) pre-trains general-purpose encoders using an unlabeled pre-training dataset, which consists of images or image-text pairs. CL is vulnerable to data poisoning based backdoor attacks (DPBAs), in which an attacker injects poisoned inputs into the pre-training dataset so the encoder is backdoored. However, existing DPBAs achieve limited effectiveness. In this work, we take the first step to analyze the limitations of existing attacks and propose new DPBAs called CorruptEncoder to CL. CorruptEncoder uses a theory-guided method to create optimal poisoned inputs to maximize attack effectiveness. Our experiments show that CorruptEncoder substantially outperforms existing DPBAs. In particular, CorruptEncoder is the first DPBA that achieves more than 90% attack success rates with only a few (3) reference images and a small poisoning ratio (0.5%). Moreover, we also propose a defense, called localized cropping, to defend against DPBAs. Our results show that our defense ca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#22810;&#27169;&#24577;&#21151;&#33021;&#22270;&#27169;&#22411;&#20272;&#35745;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#36716;&#25442;&#31639;&#23376;&#21644;&#28508;&#22312;&#22270;&#26469;&#22635;&#34917;&#24403;&#21069;&#31185;&#23398;&#26041;&#27861;&#22312;&#20272;&#35745;&#22810;&#27169;&#24577;&#21151;&#33021;&#25968;&#25454;&#22270;&#27169;&#22411;&#26041;&#38754;&#30340;&#31354;&#30333;</title><link>http://arxiv.org/abs/2210.17237</link><description>&lt;p&gt;
&#28508;&#22312;&#22810;&#27169;&#24577;&#21151;&#33021;&#22270;&#27169;&#22411;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Latent Multimodal Functional Graphical Model Estimation. (arXiv:2210.17237v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#22810;&#27169;&#24577;&#21151;&#33021;&#22270;&#27169;&#22411;&#20272;&#35745;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#36716;&#25442;&#31639;&#23376;&#21644;&#28508;&#22312;&#22270;&#26469;&#22635;&#34917;&#24403;&#21069;&#31185;&#23398;&#26041;&#27861;&#22312;&#20272;&#35745;&#22810;&#27169;&#24577;&#21151;&#33021;&#25968;&#25454;&#22270;&#27169;&#22411;&#26041;&#38754;&#30340;&#31354;&#30333;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#21516;&#22810;&#27169;&#24577;&#21151;&#33021;&#25968;&#25454;&#37319;&#38598;&#26159;&#19968;&#31181;&#29616;&#20195;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#36817;&#22312;&#31070;&#32463;&#23398;&#21644;&#29983;&#29289;&#31185;&#23398;&#20013;&#30340;&#24037;&#31243;&#31361;&#30772;&#65292;&#21487;&#20197;&#21516;&#26102;&#20174;&#21516;&#19968;&#20027;&#20307;&#20013;&#27979;&#37327;&#26469;&#33258;&#22810;&#31181;&#27169;&#24335;&#30340;&#21151;&#33021;&#25968;&#25454;&#12290;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#30340;&#19968;&#20010;&#37325;&#35201;&#21160;&#26426;&#26159;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#20449;&#21495;&#26469;&#21457;&#29616;&#28508;&#22312;&#30340;&#36830;&#25509;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#31185;&#23398;&#20852;&#36259;&#65292;&#20294;&#22312;&#20272;&#35745;&#22810;&#27169;&#24577;&#21151;&#33021;&#25968;&#25454;&#19979;&#30340;&#22270;&#27169;&#22411;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#35782;&#21035;&#20174;&#35266;&#27979;&#31354;&#38388;&#21040;&#28508;&#22312;&#31354;&#38388;&#30340;&#31639;&#23376;&#26144;&#23556;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#21516;&#26102;&#20272;&#35745;&#36716;&#25442;&#31639;&#23376;&#21644;&#28508;&#22312;&#22270;&#12290;&#36825;&#20010;&#20272;&#35745;&#22120;&#22522;&#20110;&#20559;&#30456;&#20851;&#31639;&#23376;&#65292;&#25105;&#20204;&#20174;&#22810;&#20803;&#21040;&#21151;&#33021;&#35774;&#32622;&#20013;&#20005;&#26684;&#25512;&#24191;&#20102;&#23427;&#12290;&#25105;&#20204;&#30340;&#31243;&#24207;&#26159;pr&#23553;&#38381;&#30340;
&lt;/p&gt;
&lt;p&gt;
Joint multimodal functional data acquisition, where functional data from multiple modes are measured simultaneously from the same subject, has emerged as an exciting modern approach enabled by recent engineering breakthroughs in the neurological and biological sciences. One prominent motivation to acquire such data is to enable new discoveries of the underlying connectivity by combining multimodal signals. Despite the scientific interest, there remains a gap in principled statistical methods for estimating the graph underlying multimodal functional data. To this end, we propose a new integrative framework that models the data generation process and identifies operators mapping from the observation space to the latent space. We then develop an estimator that simultaneously estimates the transformation operators and the latent graph. This estimator is based on the partial correlation operator, which we rigorously extend from the multivariate to the functional setting. Our procedure is pr
&lt;/p&gt;</description></item><item><title>Auxo &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20855;&#26377;&#30456;&#20284;&#25968;&#25454;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#36827;&#34892;&#20998;&#32452;&#65292;&#36880;&#27493;&#35782;&#21035;&#22823;&#35268;&#27169;&#12289;&#20302;&#21487;&#29992;&#24615;&#21644;&#36164;&#28304;&#21463;&#38480;&#30340;FL&#20154;&#32676;&#20013;&#30340;&#36825;&#20123;&#20998;&#32452;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#22914;&#20309;&#35757;&#32451;&#29305;&#23450;&#20998;&#32452;&#30340;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.16656</link><description>&lt;p&gt;
Auxo: &#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#23458;&#25143;&#31471;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Auxo: Efficient Federated Learning via Scalable Client Clustering. (arXiv:2210.16656v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16656
&lt;/p&gt;
&lt;p&gt;
Auxo &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20855;&#26377;&#30456;&#20284;&#25968;&#25454;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#36827;&#34892;&#20998;&#32452;&#65292;&#36880;&#27493;&#35782;&#21035;&#22823;&#35268;&#27169;&#12289;&#20302;&#21487;&#29992;&#24615;&#21644;&#36164;&#28304;&#21463;&#38480;&#30340;FL&#20154;&#32676;&#20013;&#30340;&#36825;&#20123;&#20998;&#32452;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#22914;&#20309;&#35757;&#32451;&#29305;&#23450;&#20998;&#32452;&#30340;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#33539;&#24335;&#65292;&#23427;&#20351;&#24471;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#33021;&#22815;&#22312;&#19981;&#21521;&#19968;&#20010;&#36923;&#36753;&#19978;&#38598;&#20013;&#30340;&#26381;&#21153;&#22120;&#36879;&#38706;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;ML&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#24322;&#26500;&#35774;&#22791;&#23481;&#37327;&#22806;&#65292;FL&#21442;&#19982;&#32773;&#24448;&#24448;&#22312;&#20854;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#20123;&#24046;&#24322;&#19981;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;(Non-IID)&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#24037;&#20316;&#38024;&#23545;&#30001;&#20110;&#23458;&#25143;&#31471;&#24322;&#36136;&#24615;&#24341;&#36215;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#12289;&#26368;&#32456;&#20934;&#30830;&#24230;&#20302;&#21644;&#20559;&#24046;&#31561;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20123;&#28857;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#26377;&#32479;&#35745;&#30456;&#20284;&#25968;&#25454;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#36827;&#34892;&#20998;&#32452;&#26469;&#35299;&#20915;&#36825;&#31181;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Auxo&#26469;&#36880;&#27493;&#35782;&#21035;&#22823;&#35268;&#27169;&#12289;&#20302;&#21487;&#29992;&#24615;&#21644;&#36164;&#28304;&#21463;&#38480;&#30340;FL&#20154;&#32676;&#20013;&#30340;&#36825;&#20123;&#20998;&#32452;&#12290;&#28982;&#21518;&#65292;Auxo&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#22914;&#20309;&#35757;&#32451;&#29305;&#23450;&#20998;&#32452;&#30340;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#24182;&#30830;&#20445;&#36164;&#28304;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging machine learning (ML) paradigm that enables heterogeneous edge devices to collaboratively train ML models without revealing their raw data to a logically centralized server. However, beyond the heterogeneous device capacity, FL participants often exhibit differences in their data distributions, which are not independent and identically distributed (Non-IID). Many existing works present point solutions to address issues like slow convergence, low final accuracy, and bias in FL, all stemming from client heterogeneity. In this paper, we explore an additional layer of complexity to mitigate such heterogeneity by grouping clients with statistically similar data distributions (cohorts). We propose Auxo to gradually identify such cohorts in large-scale, low-availability, and resource-constrained FL populations. Auxo then adaptively determines how to train cohort-specific models in order to achieve better model performance and ensure resource efficiency. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#35299;&#20915;&#22823;&#26102;&#38388;&#38271;&#24230;&#19979;&#30340;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#19988;&#20854;&#35745;&#31639;&#25104;&#26412;&#19981;&#21253;&#25324;&#20381;&#36182;&#20110;&#26102;&#38388;&#38271;&#24230;&#30340;&#39033;&#12290;</title><link>http://arxiv.org/abs/2210.07513</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#36830;&#32493;&#26102;&#38388;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Continuous-in-time Limit for Bayesian Bandits. (arXiv:2210.07513v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#35299;&#20915;&#22823;&#26102;&#38388;&#38271;&#24230;&#19979;&#30340;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#19988;&#20854;&#35745;&#31639;&#25104;&#26412;&#19981;&#21253;&#25324;&#20381;&#36182;&#20110;&#26102;&#38388;&#38271;&#24230;&#30340;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#36125;&#21494;&#26031;&#35774;&#32622;&#19979;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#23558;&#36172;&#21338;&#26426;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#23547;&#25214;&#26368;&#20248;&#31574;&#30053;&#20197;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#12290;&#38754;&#23545;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#65292;&#24403;&#38382;&#39064;&#30340;&#26102;&#38388;&#38271;&#24230;&#25110;&#33218;&#25968;&#36739;&#22823;&#26102;&#65292;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#36866;&#24403;&#30340;&#37325;&#32553;&#25918;&#19979;&#65292;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#38382;&#39064;&#25910;&#25947;&#20110;&#19968;&#20010;&#36830;&#32493;&#30340;&#21704;&#23494;&#23572;&#39039; - &#38597;&#21508;&#27604; - &#36125;&#23572;&#26364;&#65288;HJB&#65289;&#26041;&#31243;&#12290;&#23545;&#20110;&#24120;&#35265;&#30340;&#19968;&#20123;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#21487;&#20197;&#26126;&#30830;&#33719;&#24471;&#26497;&#38480;HJB&#26041;&#31243;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#26080;&#27861;&#26126;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35299;&#20915;HJB&#26041;&#31243;&#30340;&#25968;&#23383;&#26041;&#27861;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#35299;&#20915;&#22823;&#26102;&#38388;&#38271;&#24230;&#19979;&#30340;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#19981;&#21253;&#25324;&#20381;&#36182;&#20110;&#26102;&#38388;&#38271;&#24230;&#30340;&#39033;&#65292;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits the bandit problem in the Bayesian setting. The Bayesian approach formulates the bandit problem as an optimization problem, and the goal is to find the optimal policy which minimizes the Bayesian regret. One of the main challenges facing the Bayesian approach is that computation of the optimal policy is often intractable, especially when the length of the problem horizon or the number of arms is large. In this paper, we first show that under a suitable rescaling, the Bayesian bandit problem converges toward a continuous Hamilton-Jacobi-Bellman (HJB) equation. The optimal policy for the limiting HJB equation can be explicitly obtained for several common bandit problems, and we give numerical methods to solve the HJB equation when an explicit solution is not available. Based on these results, we propose an approximate Bayes-optimal policy for solving Bayesian bandit problems with large horizons. Our method has the added benefit that its computational cost does not inc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#26469;&#22312;&#36830;&#32493;&#39046;&#22495;&#20013;&#24212;&#29992;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#26469;&#23454;&#29616;&#20998;&#31867;&#27861;&#32467;&#26500;&#30340;&#32435;&#20837;&#12290;</title><link>http://arxiv.org/abs/2210.01672</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#23558;&#26426;&#22120;&#20154;&#20998;&#31867;&#24102;&#20837;&#36830;&#32493;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Bringing robotics taxonomies to continuous domains via GPLVM on hyperbolic manifolds. (arXiv:2210.01672v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#26469;&#22312;&#36830;&#32493;&#39046;&#22495;&#20013;&#24212;&#29992;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#26469;&#23454;&#29616;&#20998;&#31867;&#27861;&#32467;&#26500;&#30340;&#32435;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20998;&#31867;&#34987;&#29992;&#20316;&#23558;&#20154;&#31867;&#30340;&#31227;&#21160;&#21644;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;&#26041;&#24335;&#36827;&#34892;&#39640;&#23618;&#27425;&#30340;&#20998;&#23618;&#25277;&#35937;&#12290;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#20998;&#26512;&#25235;&#21462;&#12289;&#25805;&#32437;&#25216;&#33021;&#21644;&#20840;&#36523;&#25903;&#25745;&#23039;&#21183;&#38750;&#24120;&#26377;&#29992;&#12290;&#23613;&#31649;&#25105;&#20204;&#22312;&#35774;&#35745;&#23618;&#27425;&#32467;&#26500;&#21644;&#22522;&#30784;&#31867;&#21035;&#26041;&#38754;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#24212;&#29992;&#39046;&#22495;&#30340;&#20351;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#32570;&#20047;&#22635;&#34917;&#20998;&#31867;&#23618;&#32423;&#32467;&#26500;&#21644;&#19982;&#20854;&#31867;&#21035;&#30456;&#20851;&#32852;&#30340;&#39640;&#32500;&#24322;&#26500;&#25968;&#25454;&#20043;&#38388;&#24046;&#36317;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26032;&#39062;&#30340;&#39640;&#26031;&#36807;&#31243;&#21452;&#26354;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#23558;&#20998;&#31867;&#27861;&#32467;&#26500;&#32435;&#20837;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic taxonomies serve as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite substantial efforts devoted to design their hierarchy and underlying categories, their use in application fields remains limited. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. We achieve this by formulating a novel Gaussian process hyperbolic latent variable model that incorporates the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We validate our model on three different robotics taxonomies to lear
&lt;/p&gt;</description></item><item><title>NAG-GS&#26159;&#19968;&#31181;&#21322;&#38544;&#24335;&#12289;&#21152;&#36895;&#21644;&#31283;&#20581;&#30340;&#38543;&#26426;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#36895;&#30340;&#31867;Nesterov&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#21644;&#21322;&#38544;&#24335;Gauss-Seidel&#31867;&#22411;&#31163;&#25955;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;&#20108;&#27425;&#20989;&#25968;&#26368;&#23567;&#21270;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#31454;&#20105;&#24615;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14937</link><description>&lt;p&gt;
NAG-GS: &#21322;&#38544;&#24335;&#12289;&#21152;&#36895;&#21644;&#31283;&#20581;&#30340;&#38543;&#26426;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
NAG-GS: Semi-Implicit, Accelerated and Robust Stochastic Optimizer. (arXiv:2209.14937v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14937
&lt;/p&gt;
&lt;p&gt;
NAG-GS&#26159;&#19968;&#31181;&#21322;&#38544;&#24335;&#12289;&#21152;&#36895;&#21644;&#31283;&#20581;&#30340;&#38543;&#26426;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#36895;&#30340;&#31867;Nesterov&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#21644;&#21322;&#38544;&#24335;Gauss-Seidel&#31867;&#22411;&#31163;&#25955;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;&#20108;&#27425;&#20989;&#25968;&#26368;&#23567;&#21270;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#31454;&#20105;&#24615;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#32463;&#20856;&#30340;SGD&#21487;&#20197;&#35299;&#37322;&#20026;&#38543;&#26426;&#26799;&#24230;&#27969;&#30340;&#31163;&#25955;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#31283;&#20581;&#19988;&#21152;&#36895;&#30340;&#38543;&#26426;&#20248;&#21270;&#22120;&#65292;&#23427;&#20381;&#36182;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#65288;1&#65289;&#21152;&#36895;&#30340;&#31867;Nesterov&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#21644;&#65288;2&#65289;&#20854;&#21322;&#38544;&#24335;Gauss-Seidel&#31867;&#22411;&#31163;&#25955;&#21270;&#12290;&#39318;&#20808;&#65292;&#22312;&#20108;&#27425;&#20989;&#25968;&#26368;&#23567;&#21270;&#30340;&#24773;&#20917;&#19979;&#24191;&#27867;&#30740;&#31350;&#20102;&#25152;&#24471;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#20180;&#32454;&#20998;&#26512;&#25105;&#20204;&#26041;&#27861;&#30340;&#25152;&#26377;&#36229;&#21442;&#25968;&#30456;&#23545;&#20110;&#36845;&#20195;&#30697;&#38453;&#21644;&#31283;&#24577;&#19979;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35889;&#21322;&#24452;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#20248;&#21270;&#30340;&#23398;&#20064;&#29575;&#65292;&#20197;&#20445;&#35777;NAG-GS&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;NAG-GS&#22312;&#31454;&#20105;&#24615;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical machine learning models such as deep neural networks are usually trained by using Stochastic Gradient Descent-based (SGD) algorithms. The classical SGD can be interpreted as a discretization of the stochastic gradient flow. In this paper we propose a novel, robust and accelerated stochastic optimizer that relies on two key elements: (1) an accelerated Nesterov-like Stochastic Differential Equation (SDE) and (2) its semi-implicit Gauss-Seidel type discretization. The convergence and stability of the obtained method, referred to as NAG-GS, are first studied extensively in the case of the minimization of a quadratic function. This analysis allows us to come up with an optimal learning rate in terms of the convergence rate while ensuring the stability of NAG-GS. This is achieved by the careful analysis of the spectral radius of the iteration matrix and the covariance matrix at stationarity with respect to all hyperparameters of our method. Further, we show that NAG- GS is competi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;APE&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;Swarm&#33322;&#22825;&#22120;&#19978;&#30340;&#31561;&#31163;&#23376;&#20307;&#27873;&#25351;&#25968;&#12290;&#35813;&#27169;&#22411;&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545;&#36196;&#36947;&#31561;&#31163;&#23376;&#20307;&#27873;&#30340;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2209.13482</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;Shapley&#20540;&#39044;&#27979;&#32676;&#38598;&#36196;&#36947;&#31561;&#31163;&#23376;&#20307;&#27873;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Predicting Swarm Equatorial Plasma Bubbles via Machine Learning and Shapley Values. (arXiv:2209.13482v2 [physics.space-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;APE&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;Swarm&#33322;&#22825;&#22120;&#19978;&#30340;&#31561;&#31163;&#23376;&#20307;&#27873;&#25351;&#25968;&#12290;&#35813;&#27169;&#22411;&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545;&#36196;&#36947;&#31561;&#31163;&#23376;&#20307;&#27873;&#30340;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;APE&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;Swarm&#33322;&#22825;&#22120;&#19978;&#30340;&#30005;&#31163;&#23618;&#27873;&#25351;&#25968;&#65288;IBI&#65289;&#12290;IBI&#26159;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#25200;&#21160;&#19982;&#30913;&#22330;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65288;$R^2$&#65289;&#65292;&#20854;&#26469;&#28304;&#21487;&#20197;&#26159;&#36196;&#36947;&#31561;&#31163;&#23376;&#20307;&#27873;&#65288;EPB&#65289;&#12290;EPB&#24050;&#32463;&#30740;&#31350;&#20102;&#22810;&#24180;&#65292;&#20294;&#20854;&#22825;&#22825;&#21464;&#21270;&#20351;&#24471;&#23545;&#20854;&#36827;&#34892;&#39044;&#27979;&#25104;&#20026;&#19968;&#39033;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;IBI&#12290;&#25105;&#20204;&#20351;&#29992;2014-22&#24180;&#30340;&#25968;&#25454;&#65292;&#20998;&#36776;&#29575;&#20026;1&#31186;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#19968;&#20010;&#20855;&#26377;&#30456;&#24212;EPB $R^2$&#65288;0-1&#65289;&#26631;&#31614;&#30340;6&#32500;&#31354;&#38388;&#12290;APE&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#23637;&#31034;&#20102;&#20998;&#25968;&#12289;&#20851;&#32852;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#20998;&#21035;&#20026;0.96&#12289;0.98&#21644;0.08&#12290;&#35813;&#27169;&#22411;&#22312;&#26085;&#33853;&#21518;&#12289;&#32654;&#27954;/&#22823;&#35199;&#27915;&#22320;&#21306;&#12289;&#26149;&#31179;&#20998;&#21644;&#22826;&#38451;&#27963;&#21160;&#36739;&#39640;&#26102;&#34920;&#29616;&#26368;&#20339;&#12290;&#36825;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;EPB&#26159;&#26368;&#37325;&#35201;&#30340;&#31561;&#31163;&#23376;&#20307;&#29289;&#29702;&#29616;&#35937;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study we present AI Prediction of Equatorial Plasma Bubbles (APE), a machine learning model that can accurately predict the Ionospheric Bubble Index (IBI) on the Swarm spacecraft. IBI is a correlation ($R^2$) between perturbations in plasma density and the magnetic field, whose source can be Equatorial Plasma Bubbles (EPBs). EPBs have been studied for a number of years, but their day-to-day variability has made predicting them a considerable challenge. We build an ensemble machine learning model to predict IBI. We use data from 2014-22 at a resolution of 1sec, and transform it from a time-series into a 6-dimensional space with a corresponding EPB $R^2$ (0-1) acting as the label. APE performs well across all metrics, exhibiting a skill, association and root mean squared error score of 0.96, 0.98 and 0.08 respectively. The model performs best post-sunset, in the American/Atlantic sector, around the equinoxes, and when solar activity is high. This is promising because EPBs are mos
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#26368;&#36817;&#21457;&#24067;&#30340;&#33016;&#37096;X&#20809;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#20559;&#20506;&#39118;&#38505;&#65292;&#24182;&#21457;&#29616;&#22312;&#29983;&#29289;&#24615;&#21035;&#21644;&#31181;&#26063;&#20043;&#38388;&#23384;&#22312;&#20122;&#32452;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2209.02965</link><description>&lt;p&gt;
&#33016;&#37096;X&#20809;&#28145;&#24230;&#23398;&#20064;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#20559;&#20506;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Risk of Bias in Chest Radiography Deep Learning Foundation Models. (arXiv:2209.02965v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#26368;&#36817;&#21457;&#24067;&#30340;&#33016;&#37096;X&#20809;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#20559;&#20506;&#39118;&#38505;&#65292;&#24182;&#21457;&#29616;&#22312;&#29983;&#29289;&#24615;&#21035;&#21644;&#31181;&#26063;&#20043;&#38388;&#23384;&#22312;&#20122;&#32452;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#20998;&#26512;&#26368;&#36817;&#21457;&#24067;&#30340;&#33016;&#37096;X&#20809;&#22522;&#30784;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#20559;&#20506;&#65292;&#21487;&#33021;&#23548;&#33268;&#22312;&#29983;&#29289;&#24615;&#21035;&#21644;&#31181;&#26063;&#20043;&#38388;&#23384;&#22312;&#20122;&#32452;&#24615;&#33021;&#24046;&#36317;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#26412;&#22238;&#39038;&#24615;&#30740;&#31350;&#20351;&#29992;CheXpert&#25968;&#25454;&#38598;&#20013;&#33258;2002&#24180;10&#26376;&#33267;2017&#24180;7&#26376;&#26399;&#38388;&#25910;&#38598;&#30340;42,884&#21517;&#24739;&#32773;&#65288;&#24180;&#40836;&#24179;&#22343;&#20026;63&#23681;&#65292;&#26631;&#20934;&#20559;&#24046;&#20026;17&#23681;&#65307;&#30007;&#24615;23,623&#20154;&#65292;&#22899;&#24615;19,261&#20154;&#65289;&#30340;127,118&#24352;&#33016;&#37096;X&#20809;&#12290;&#20351;&#29992;&#38477;&#32500;&#26041;&#27861;&#21644;&#20004;&#26679;&#26412;Kolmogorov-Smirnov&#26816;&#39564;&#26816;&#27979;&#33016;&#37096;X&#20809;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#30784;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#30340;&#29305;&#24449;&#20013;&#30340;&#20998;&#24067;&#20559;&#24046;&#65292;&#20197;&#30830;&#23450;&#26159;&#21542;&#23384;&#22312;&#20559;&#20506;&#12290;&#28982;&#21518;&#36827;&#34892;&#20840;&#38754;&#30340;&#30142;&#30149;&#26816;&#27979;&#24615;&#33021;&#20998;&#26512;&#65292;&#23558;&#29305;&#24449;&#20013;&#30340;&#20219;&#20309;&#20559;&#20506;&#19982;&#24739;&#32773;&#20122;&#32452;&#30340;&#20998;&#31867;&#24615;&#33021;&#24046;&#24322;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To analyze a recently published chest radiography foundation model for the presence of biases that could lead to subgroup performance disparities across biological sex and race.  Materials and Methods: This retrospective study used 127,118 chest radiographs from 42,884 patients (mean age, 63 [SD] 17 years; 23,623 male, 19,261 female) from the CheXpert dataset collected between October 2002 and July 2017. To determine the presence of bias in features generated by a chest radiography foundation model and baseline deep learning model, dimensionality reduction methods together with two-sample Kolmogorov-Smirnov tests were used to detect distribution shifts across sex and race. A comprehensive disease detection performance analysis was then performed to associate any biases in the features to specific disparities in classification performance across patient subgroups.  Results: Ten out of twelve pairwise comparisons across biological sex and race showed statistically significant di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#20869;&#37096;&#24230;&#37327;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2209.02935</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#32858;&#31867;&#20934;&#30830;&#24230;&#65306;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Normalised clustering accuracy: An asymmetric external cluster validity measure. (arXiv:2209.02935v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#20869;&#37096;&#24230;&#37327;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#19968;&#20010;&#26368;&#22909;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#25105;&#20204;&#20173;&#28982;&#24076;&#26395;&#33021;&#22815;&#21306;&#20998;&#20986;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#32858;&#31867;&#31639;&#27861;&#20351;&#29992;&#20869;&#37096;&#25110;&#22806;&#37096;&#26377;&#25928;&#24230;&#37327;&#36827;&#34892;&#35780;&#20272;&#12290;&#20869;&#37096;&#24230;&#37327;&#37327;&#21270;&#25152;&#24471;&#20998;&#21306;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#20363;&#22914;&#65292;&#31751;&#32039;&#23494;&#24230;&#30340;&#24179;&#22343;&#31243;&#24230;&#25110;&#28857;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#20419;&#20351;&#30340;&#32858;&#31867;&#26377;&#26102;&#21487;&#33021;&#26159;&#26080;&#24847;&#20041;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22806;&#37096;&#24230;&#37327;&#23558;&#31639;&#27861;&#30340;&#36755;&#20986;&#19982;&#30001;&#19987;&#23478;&#25552;&#20379;&#30340;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24120;&#29992;&#30340;&#32463;&#20856;&#20998;&#21306;&#30456;&#20284;&#24615;&#35780;&#20998;&#65292;&#20363;&#22914;&#35268;&#33539;&#21270;&#20114;&#20449;&#24687;&#12289;Fowlkes-Mallows&#25110;&#35843;&#25972;&#20848;&#24503;&#25351;&#25968;&#65292;&#32570;&#23569;&#19968;&#20123;&#21487;&#21462;&#30340;&#23646;&#24615;&#65292;&#20363;&#22914;&#65292;&#23427;&#20204;&#19981;&#33021;&#27491;&#30830;&#35782;&#21035;&#26368;&#22351;&#24773;&#20917;&#65292;&#20063;&#19981;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is no, nor will there ever be, single best clustering algorithm, but we would still like to be able to distinguish between methods which work well on certain task types and those that systematically underperform. Clustering algorithms are traditionally evaluated using either internal or external validity measures. Internal measures quantify different aspects of the obtained partitions, e.g., the average degree of cluster compactness or point separability. Yet, their validity is questionable, because the clusterings they promote can sometimes be meaningless. External measures, on the other hand, compare the algorithms' outputs to the reference, ground truth groupings that are provided by experts. In this paper, we argue that the commonly-used classical partition similarity scores, such as the normalised mutual information, Fowlkes-Mallows, or adjusted Rand index, miss some desirable properties, e.g., they do not identify worst-case scenarios correctly or are not easily interpretab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;&#36793;&#32536;&#27169;&#22411;&#30340;&#26412;&#22320;&#30693;&#35782;&#25972;&#21512;&#20026;&#40065;&#26834;&#30340;&#20840;&#23616;&#30693;&#35782;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#27169;&#22411;&#30693;&#35782;&#34701;&#21512;&#65292;&#24182;&#22312;&#20445;&#25345;&#27169;&#22411;&#24322;&#36136;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.07978</link><description>&lt;p&gt;
&#25552;&#21319;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;&#21644;&#22810;&#27169;&#22411;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Enhancing Heterogeneous Federated Learning with Knowledge Extraction and Multi-Model Fusion. (arXiv:2208.07978v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;&#36793;&#32536;&#27169;&#22411;&#30340;&#26412;&#22320;&#30693;&#35782;&#25972;&#21512;&#20026;&#40065;&#26834;&#30340;&#20840;&#23616;&#30693;&#35782;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#27169;&#22411;&#30693;&#35782;&#34701;&#21512;&#65292;&#24182;&#22312;&#20445;&#25345;&#27169;&#22411;&#24322;&#36136;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#19981;&#35775;&#38382;&#25935;&#24863;&#25968;&#25454;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#34429;&#28982;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#24615;&#65292;&#20294;&#30001;&#20110;&#20381;&#36182;&#20110;&#32858;&#21512;&#26041;&#27861;&#65292;&#26080;&#27861;&#31649;&#29702;&#27169;&#22411;&#24322;&#36136;&#24615;&#65292;&#24182;&#36896;&#25104;&#39640;&#36890;&#20449;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65292;&#23558;&#26469;&#33258;&#36793;&#32536;&#27169;&#22411;&#30340;&#26412;&#22320;&#30693;&#35782;&#25972;&#21512;&#20026;&#40065;&#26834;&#30340;&#20840;&#23616;&#30693;&#35782;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#27169;&#22411;&#30693;&#35782;&#34701;&#21512;&#65292;&#24182;&#22312;&#20445;&#25345;&#27169;&#22411;&#24322;&#36136;&#24615;&#30340;&#21516;&#26102;&#37096;&#32626;&#36164;&#28304;&#24863;&#30693;&#30340;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24322;&#26500;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#25913;&#21892;&#20102;&#36890;&#20449;&#25104;&#26412;&#21644;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;ResNet-32&#30340;&#36890;&#20449;&#25104;&#26412;&#38477;&#20302;&#20102;&#26368;&#22810;50\&#65285;&#65292;&#23558;VGG-11&#30340;&#36890;&#20449;&#25104;&#26412;&#38477;&#20302;&#20102;&#26368;&#22810;10&#20493;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concerned with user data privacy, this paper presents a new federated learning (FL) method that trains machine learning models on edge devices without accessing sensitive data. Traditional FL methods, although privacy-protective, fail to manage model heterogeneity and incur high communication costs due to their reliance on aggregation methods. To address this limitation, we propose a resource-aware FL method that aggregates local knowledge from edge models and distills it into robust global knowledge through knowledge distillation. This method allows efficient multi-model knowledge fusion and the deployment of resource-aware models while preserving model heterogeneity. Our method improves communication cost and performance in heterogeneous data and models compared to existing FL algorithms. Notably, it reduces the communication cost of ResNet-32 by up to 50\% and VGG-11 by up to 10$\times$ while delivering superior performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#36866;&#24212;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#33258;&#36866;&#24212;&#30340;&#30417;&#30563;&#20449;&#24687;&#26469;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20869;&#37096;&#20998;&#24067;&#27867;&#21270;&#19982;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.09380</link><description>&lt;p&gt;
&#30417;&#30563;&#36866;&#24212;&#24615;&#24179;&#34913;&#20869;&#37096;&#20998;&#24067;&#27867;&#21270;&#19982;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Supervision Adaptation Balancing In-distribution Generalization and Out-of-distribution Detection. (arXiv:2206.09380v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#36866;&#24212;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#33258;&#36866;&#24212;&#30340;&#30417;&#30563;&#20449;&#24687;&#26469;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20869;&#37096;&#20998;&#24067;&#27867;&#21270;&#19982;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#21644;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#21487;&#33021;&#23548;&#33268;&#32593;&#32476;&#30340;&#20998;&#24067;&#33030;&#24369;&#24615;&#65292;&#36827;&#32780;&#23548;&#33268;&#23545;OOD&#26679;&#26412;&#30340;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#23569;OOD&#26679;&#26412;&#65292;&#26080;&#27861;&#20805;&#20998;&#32422;&#26463;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21253;&#25324;&#22312;&#35757;&#32451;&#20013;&#28155;&#21152;&#39069;&#22806;&#30340;OOD&#26679;&#26412;&#65292;&#24182;&#20026;&#20854;&#20998;&#37197;&#25163;&#21160;&#23450;&#20041;&#30340;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20570;&#27861;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#26631;&#27880;&#65292;&#23545;ID&#20998;&#31867;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20998;&#24067;&#33030;&#24369;&#24615;&#23545;&#20110;&#38750;IID&#28145;&#24230;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#36890;&#36807;&#24179;&#34913;ID&#27867;&#21270;&#21644;OOD&#26816;&#27979;&#26469;&#23454;&#29616;&#23545;OOD&#23481;&#24525;&#30340;ID&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#30417;&#30563;&#36866;&#24212;&#24615;&#8221;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;OOD&#26679;&#26412;&#29983;&#25104;&#33258;&#36866;&#24212;&#30340;&#30417;&#30563;&#20449;&#24687;&#65292;&#20351;&#20854;&#26356;&#20860;&#23481;ID&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discrepancy between in-distribution (ID) and out-of-distribution (OOD) samples can lead to \textit{distributional vulnerability} in deep neural networks, which can subsequently lead to high-confidence predictions for OOD samples. This is mainly due to the absence of OOD samples during training, which fails to constrain the network properly. To tackle this issue, several state-of-the-art methods include adding extra OOD samples to training and assign them with manually-defined labels. However, this practice can introduce unreliable labeling, negatively affecting ID classification. The distributional vulnerability presents a critical challenge for non-IID deep learning, which aims for OOD-tolerant ID classification by balancing ID generalization and OOD detection. In this paper, we introduce a novel \textit{supervision adaptation} approach to generate adaptive supervision information for OOD samples, making them more compatible with ID samples. Firstly, we measure the dependency betw
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#26694;&#26550;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#22312;&#27599;&#36718;&#20013;&#25512;&#26029;&#20854;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#21644;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#23558;&#20854;&#33976;&#39311;&#20026;&#21333;&#19968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21457;&#36865;&#32473;&#26381;&#21153;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#20272;&#35745;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#26377;&#38480;&#25968;&#25454;&#29615;&#22659;&#19979;&#21462;&#24471;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2206.07562</link><description>&lt;p&gt;
&#36890;&#36807;&#33976;&#39311;&#39044;&#27979;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Uncertainty via Distilled Predictive Distributions. (arXiv:2206.07562v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#26694;&#26550;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#22312;&#27599;&#36718;&#20013;&#25512;&#26029;&#20854;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#21644;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#23558;&#20854;&#33976;&#39311;&#20026;&#21333;&#19968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21457;&#36865;&#32473;&#26381;&#21153;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#20272;&#35745;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#26377;&#38480;&#25968;&#25454;&#29615;&#22659;&#19979;&#21462;&#24471;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#20272;&#35745;&#27169;&#22411;/&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#20026;&#23458;&#25143;&#31471;&#27169;&#22411;&#20351;&#29992;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#26368;&#23567;&#21270;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#24573;&#30053;&#20102;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#32771;&#34385;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#20197;&#29992;&#20110;&#35832;&#22914;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#21644;&#24207;&#36143;&#20915;&#31574;&#20219;&#21153;&#65288;&#22914;&#20027;&#21160;&#23398;&#20064;&#65289;&#31561;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#25512;&#26029;&#20854;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#21644;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65288;PPD&#65289;&#65292;&#23558;PPD&#33976;&#39311;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#35813;&#32593;&#32476;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#12290;&#19982;&#26368;&#36817;&#19968;&#20123;&#36125;&#21494;&#26031;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#35201;&#27714;&#21457;&#36865;&#25152;&#26377;&#21407;&#22987;&#25968;&#25454;&#33267;&#26381;&#21153;&#22120;&#65292;&#20445;&#25252;&#20102;&#23458;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing federated learning methods are unable to estimate model/predictive uncertainty since the client models are trained using the standard loss function minimization approach which ignores such uncertainties. In many situations, however, especially in limited data settings, it is beneficial to take into account the uncertainty in the model parameters at each client as it leads to more accurate predictions and also because reliable estimates of uncertainty can be used for tasks, such as out-of-distribution (OOD) detection, and sequential decision-making tasks, such as active learning. We present a framework for federated learning with uncertainty where, in each round, each client infers the posterior distribution over its parameters as well as the posterior predictive distribution (PPD), distills the PPD into a single deep neural network, and sends this network to the server. Unlike some of the recent Bayesian approaches to federated learning, our approach does not require send
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#33410;&#28857;&#20998;&#31867;&#20013;&#25554;&#20540;&#24102;&#38480;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;GNN&#32467;&#26500;&#20197;&#30456;&#21516;&#30340;&#31934;&#24230;&#25554;&#20540;&#24102;&#38480;&#20989;&#25968;&#25152;&#38656;&#30340;&#26435;&#37325;&#27604;&#20351;&#29992;&#23436;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#23569;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2206.05904</link><description>&lt;p&gt;
GNN&#22312;&#25512;&#24191;&#24102;&#38480;&#20989;&#25968;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#27604;NN&#26356;&#21152;&#26126;&#26174;
&lt;/p&gt;
&lt;p&gt;
Superiority of GNN over NN in generalizing bandlimited functions. (arXiv:2206.05904v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#33410;&#28857;&#20998;&#31867;&#20013;&#25554;&#20540;&#24102;&#38480;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;GNN&#32467;&#26500;&#20197;&#30456;&#21516;&#30340;&#31934;&#24230;&#25554;&#20540;&#24102;&#38480;&#20989;&#25968;&#25152;&#38656;&#30340;&#26435;&#37325;&#27604;&#20351;&#29992;&#23436;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#23569;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20197;&#20854;&#25972;&#21512;&#22270;&#24418;&#20449;&#24687;&#30340;&#33021;&#21147;&#34987;&#24191;&#27867;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#20165;&#38024;&#23545;&#22270;&#32423;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#33410;&#28857;&#32423;&#20219;&#21153;&#65292;&#20363;&#22914;&#33410;&#28857;&#20998;&#31867;&#65292;&#20854;&#20013;&#35797;&#22270;&#20174;&#35266;&#23519;&#21040;&#30340;&#33410;&#28857;&#26631;&#31614;&#20013;&#25554;&#20540;&#20986;&#32570;&#22833;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#25152;&#36848;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23427;&#23454;&#36136;&#19978;&#26159;&#19968;&#20010;&#20989;&#25968;&#25554;&#20540;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;GNN&#25554;&#20540;$\mathbb{R}^d$&#20013;&#24102;&#38480;&#20989;&#25968;&#25152;&#38656;&#30340;&#26435;&#37325;&#21644;&#23618;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;GNN&#26550;&#26500;&#20197;$\epsilon$-&#36817;&#20284;&#31163;&#25955;&#24102;&#38480;&#20449;&#21495;&#20165;&#38656;&#35201;$O((\log \epsilon^{-1})^{d})$&#20010;&#26435;&#37325;&#65292;&#36825;&#27604;&#20351;&#29992;&#23436;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#24471;&#21040;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#25152;&#38656;&#26435;&#37325;&#23569;&#24471;&#22810; - &#29305;&#21035;&#22320;&#65292;&#20351;&#29992;&#20351;&#29992;$O((\log \epsilon^{-1})^{d})$&#20010;&#26679;&#26412;&#26469;&#35757;&#32451;GNN&#20197;$\epsilon$-&#36924;&#36817;&#24102;&#38480;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network (GNN) with its ability to integrate graph information has been widely used for data analyses. However, the expressive power of GNN has only been studied for graph-level tasks but not for node-level tasks, such as node classification, where one tries to interpolate missing nodal labels from the observed ones. In this paper, we study the expressive power of GNN for the said classification task, which is in essence a function interpolation problem. Explicitly, we derive the number of weights and layers needed for a GNN to interpolate a band-limited function in $\mathbb{R}^d$. Our result shows that, the number of weights needed to $\epsilon$-approximate a bandlimited function using the GNN architecture is much fewer than the best known one using a fully connected neural network (NN) - in particular, one only needs $O((\log \epsilon^{-1})^{d})$ weights using a GNN trained by $O((\log \epsilon^{-1})^{d})$ samples to $\epsilon$-approximate a discretized bandlimited signal
&lt;/p&gt;</description></item><item><title>&#31934;&#39311;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#40657;&#30418;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#31934;&#39311;&#21040;&#20915;&#31574;&#26641;&#20013;&#26469;&#20419;&#36827;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#30693;&#35782;&#31934;&#39311;&#30340;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#24182;&#19988;&#22312;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#26465;&#20214;&#19979;&#21487;&#20197;&#26377;&#25928;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.04661</link><description>&lt;p&gt;
&#31934;&#39311;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Distillation Decision Tree. (arXiv:2206.04661v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04661
&lt;/p&gt;
&lt;p&gt;
&#31934;&#39311;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#40657;&#30418;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#31934;&#39311;&#21040;&#20915;&#31574;&#26641;&#20013;&#26469;&#20419;&#36827;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#30693;&#35782;&#31934;&#39311;&#30340;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#24182;&#19988;&#22312;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#26465;&#20214;&#19979;&#21487;&#20197;&#26377;&#25928;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#39044;&#27979;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#23427;&#20204;&#32463;&#24120;&#38754;&#20020;&#25209;&#35780;&#21644;&#25361;&#25112;&#12290;&#30683;&#30462;&#30340;&#26159;&#65292;&#23427;&#20204;&#24378;&#22823;&#30340;&#39044;&#27979;&#33021;&#21147;&#34920;&#26126;&#23545;&#24213;&#23618;&#25968;&#25454;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#24847;&#21619;&#30528;&#37325;&#35201;&#30340;&#35299;&#37322;&#28508;&#21147;&#12290;&#20511;&#21161;&#30693;&#35782;&#31934;&#39311;&#30340;&#26032;&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31934;&#39311;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#20851;&#20110;&#25968;&#25454;&#30340;&#30693;&#35782;&#20174;&#40657;&#30418;&#27169;&#22411;&#31934;&#39311;&#21040;&#20915;&#31574;&#26641;&#20013;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#23545;&#40657;&#30418;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#30693;&#35782;&#31934;&#39311;&#36807;&#31243;&#26500;&#24314;&#30340;DDT&#30340;&#21487;&#35299;&#37322;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20854;&#32467;&#26500;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#20026;DDT&#30340;&#32467;&#26500;&#31283;&#23450;&#24615;&#24314;&#31435;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#35777;&#26126;&#20854;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#21487;&#20197;&#23454;&#29616;&#32467;&#26500;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#31639;&#27861;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Machine learning models, particularly the black-box models, are widely favored for their outstanding predictive capabilities. However, they often face scrutiny and criticism due to the lack of interpretability. Paradoxically, their strong predictive capabilities suggest a deep understanding about the underlying data, implying significant potential for interpretation. Leveraging the emerging concept of knowledge distillation, we introduced the method of distillation decision tree (DDT). This method enables the distillation of knowledge about the data from a black-box model into a decision tree, thereby facilitating the interpretation of the black-box model. Constructed through the knowledge distillation process, the interpretability of DDT relies significantly on the stability of its structure. We establish the theoretical foundations for the structural stability of DDT, demonstrating that its structure can achieve stability under mild assumptions. Furthermore, we develop algorithms for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#21516;&#24577;&#22270;&#30340;&#35299;&#32806;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;DSSL&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#27169;&#25311;&#33410;&#28857;&#21644;&#38142;&#25509;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23558;&#19981;&#21516;&#37051;&#22495;&#20043;&#38388;&#30340;&#19981;&#21516;&#28508;&#22312;&#35821;&#20041;&#35299;&#32806;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#35813;&#26694;&#26550;&#23545;&#32534;&#30721;&#22120;&#19981;&#25935;&#24863;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39044;&#21046;&#30340;&#22686;&#24378;&#65292;&#23545;&#19981;&#21516;&#30340;&#22270;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.03601</link><description>&lt;p&gt;
&#38750;&#21516;&#24577;&#22270;&#30340;&#35299;&#32806;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decoupled Self-supervised Learning for Non-Homophilous Graphs. (arXiv:2206.03601v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#21516;&#24577;&#22270;&#30340;&#35299;&#32806;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;DSSL&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#27169;&#25311;&#33410;&#28857;&#21644;&#38142;&#25509;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23558;&#19981;&#21516;&#37051;&#22495;&#20043;&#38388;&#30340;&#19981;&#21516;&#28508;&#22312;&#35821;&#20041;&#35299;&#32806;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#35813;&#26694;&#26550;&#23545;&#32534;&#30721;&#22120;&#19981;&#25935;&#24863;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39044;&#21046;&#30340;&#22686;&#24378;&#65292;&#23545;&#19981;&#21516;&#30340;&#22270;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#19978;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#37117;&#20551;&#35774;&#22270;&#26159;&#21516;&#36136;&#30340;&#65292;&#21363;&#36830;&#25509;&#30340;&#33410;&#28857;&#36890;&#24120;&#23646;&#20110;&#21516;&#19968;&#31867;&#25110;&#20855;&#26377;&#30456;&#20284;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#65292;&#36825;&#31181;&#21516;&#36136;&#24615;&#30340;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#32806;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;DSSL&#65289;&#26694;&#26550;&#12290;DSSL&#36890;&#36807;&#20174;&#35821;&#20041;&#32467;&#26500;&#30340;&#28508;&#21464;&#37327;&#24314;&#27169;&#20013;&#27169;&#25311;&#33410;&#28857;&#21644;&#38142;&#25509;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23558;&#19981;&#21516;&#37051;&#22495;&#20043;&#38388;&#30340;&#19981;&#21516;&#28508;&#22312;&#35821;&#20041;&#35299;&#32806;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#30340;DSSL&#26694;&#26550;&#23545;&#32534;&#30721;&#22120;&#19981;&#25935;&#24863;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39044;&#21046;&#30340;&#22686;&#24378;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#22270;&#12290;&#20026;&#20102;&#26377;&#25928;&#20248;&#21270;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#33258;&#30417;&#30563;&#30446;&#26631;&#30340;&#35777;&#25454;&#19979;&#30028;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#21464;&#20998;&#29305;&#24615;&#30340;&#21487;&#25193;&#23637;&#35757;&#32451;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of conducting self-supervised learning for node representation learning on graphs. Most existing self-supervised learning methods assume the graph is homophilous, where linked nodes often belong to the same class or have similar features. However, such assumptions of homophily do not always hold in real-world graphs. We address this problem by developing a decoupled self-supervised learning (DSSL) framework for graph neural networks. DSSL imitates a generative process of nodes and links from latent variable modeling of the semantic structure, which decouples different underlying semantics between different neighborhoods into the self-supervised learning process. Our DSSL framework is agnostic to the encoders and does not need prefabricated augmentations, thus is flexible to different graphs. To effectively optimize the framework, we derive the evidence lower bound of the self-supervised objective and develop a scalable training algorithm with variational 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#34109;&#33258;&#22238;&#24402;&#27969;&#21644;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#23545;&#24212;&#20110;&#26680;&#24515;&#31185;&#23398;&#21442;&#25968;&#30340;&#36793;&#38469;&#21518;&#39564;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36793;&#38469;&#24211;&#23572;&#24052;&#20811;-&#21202;&#24067;&#21202;&#25955;&#24230;&#12289;&#36793;&#38469;&#36125;&#21494;&#26031;&#27169;&#22411;&#32500;&#24230;&#12289;&#20284;&#28982;&#20989;&#25968;&#27169;&#25311;&#21644;&#20808;&#39564;&#27169;&#25311;&#31561;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2205.12841</link><description>&lt;p&gt;
&#29992;&#20154;&#36896;&#40644;&#27833;&#20174;&#21518;&#39564;&#26679;&#26412;&#20013;&#21435;&#38500;&#33026;&#32938;
&lt;/p&gt;
&lt;p&gt;
Removing the fat from your posterior samples with margarine. (arXiv:2205.12841v3 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#34109;&#33258;&#22238;&#24402;&#27969;&#21644;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#23545;&#24212;&#20110;&#26680;&#24515;&#31185;&#23398;&#21442;&#25968;&#30340;&#36793;&#38469;&#21518;&#39564;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36793;&#38469;&#24211;&#23572;&#24052;&#20811;-&#21202;&#24067;&#21202;&#25955;&#24230;&#12289;&#36793;&#38469;&#36125;&#21494;&#26031;&#27169;&#22411;&#32500;&#24230;&#12289;&#20284;&#28982;&#20989;&#25968;&#27169;&#25311;&#21644;&#20808;&#39564;&#27169;&#25311;&#31561;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20998;&#26512;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#23431;&#23449;&#23398;&#39046;&#22495;&#30340;&#19981;&#21487;&#25110;&#32570;&#24037;&#20855;&#65292;&#21253;&#25324;&#24341;&#21147;&#27874;&#30740;&#31350;&#12289;&#23431;&#23449;&#24494;&#27874;&#32972;&#26223;&#21644;&#23431;&#23449;&#40654;&#26126;&#26102;&#26399;&#30340;21&#21400;&#31859;&#20449;&#21495;&#31561;&#29616;&#35937;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#22797;&#26434;&#27169;&#22411;&#19982;&#25551;&#36848;&#20851;&#38190;&#23431;&#23449;&#23398;&#21644;&#22825;&#20307;&#29289;&#29702;&#20449;&#21495;&#20197;&#21450;&#21508;&#31181;&#27745;&#26579;&#20449;&#21495;&#21644;&#20202;&#22120;&#25928;&#24212;&#30340;'&#24178;&#25200;&#21442;&#25968;'&#25311;&#21512;&#21040;&#25968;&#25454;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#34109;&#33258;&#22238;&#24402;&#27969;&#21644;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#26469;&#23398;&#20064;&#23545;&#24212;&#20110;&#26680;&#24515;&#31185;&#23398;&#21442;&#25968;&#30340;&#36793;&#38469;&#21518;&#39564;&#23494;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36793;&#38469;&#25110;&#8220;&#26080;&#24178;&#25200;&#8221;&#30340;&#21518;&#39564;&#20998;&#24067;&#21450;&#20854;&#30456;&#20851;&#30340;&#20284;&#28982;&#20989;&#25968;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#35745;&#31639;&#20197;&#21069;&#38590;&#20197;&#22788;&#29702;&#30340;&#36793;&#38469;&#24211;&#23572;&#24052;&#20811;-&#21202;&#24067;&#21202;&#25955;&#24230;&#21644;&#36793;&#38469;&#36125;&#21494;&#26031;&#27169;&#22411;&#32500;&#24230;&#65292;&#20284;&#28982;&#20989;&#25968;&#27169;&#25311;&#21644;&#20808;&#39564;&#27169;&#25311;&#12290;&#25105;&#20204;&#20351;&#29992;&#29609;&#20855;&#20363;&#23376;&#21644;&#23454;&#38469;&#26696;&#20363;&#20998;&#21035;&#23637;&#31034;&#20102;&#27599;&#20010;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian analysis has become an indispensable tool across many different cosmological fields including the study of gravitational waves, the Cosmic Microwave Background and the 21-cm signal from the Cosmic Dawn among other phenomena. The method provides a way to fit complex models to data describing key cosmological and astrophysical signals and a whole host of contaminating signals and instrumental effects modelled with 'nuisance parameters'. In this paper, we summarise a method that uses Masked Autoregressive Flows and Kernel Density Estimators to learn marginal posterior densities corresponding to core science parameters. We find that the marginal or 'nuisance-free' posteriors and the associated likelihoods have an abundance of applications including; the calculation of previously intractable marginal Kullback-Leibler divergences and marginal Bayesian Model Dimensionalities, likelihood emulation and prior emulation. We demonstrate each application using toy examples, examples from t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#39034;&#24207;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#36319;&#36394;&#38750;&#24179;&#31283;&#20449;&#36947;&#22122;&#22768;&#30340;LDPC&#30721;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22312;5G&#34892;&#39542;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20248;&#20110;&#20855;&#26377;&#22266;&#23450;&#20449;&#36947;&#22122;&#22768;&#30693;&#35782;&#30340;LDPC&#30721;&#12290;</title><link>http://arxiv.org/abs/2204.07037</link><description>&lt;p&gt;
LDPC&#30721;&#65306;&#20351;&#29992;&#39034;&#24207;&#21464;&#20998;&#36125;&#21494;&#26031;&#20272;&#35745;&#36319;&#36394;&#38750;&#24179;&#31283;&#20449;&#36947;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
LDPC codes: tracking non-stationary channel noise using sequential variational Bayesian estimates. (arXiv:2204.07037v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07037
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#39034;&#24207;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#36319;&#36394;&#38750;&#24179;&#31283;&#20449;&#36947;&#22122;&#22768;&#30340;LDPC&#30721;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22312;5G&#34892;&#39542;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20248;&#20110;&#20855;&#26377;&#22266;&#23450;&#20449;&#36947;&#22122;&#22768;&#30693;&#35782;&#30340;LDPC&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#36319;&#36394;LDPC&#30721;&#20013;&#38750;&#24179;&#31283;&#20449;&#22122;&#27604;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#36890;&#29992;&#30340;&#32858;&#31751;&#22270;&#26500;&#36896;&#31639;&#27861;&#8212;&#8212;&#20998;&#23618;&#26641;&#36816;&#34892;&#20132;&#38598;&#23646;&#24615;&#65288;LTRIP&#65289;&#31639;&#27861;&#23558;LDPC&#30721;&#34920;&#31034;&#20026;&#32858;&#31751;&#22270;&#12290;&#20449;&#36947;&#22122;&#22768;&#20272;&#35745;&#22120;&#26159;&#19968;&#20010;&#20840;&#23616;Gamma&#32858;&#22242;&#65292;&#25105;&#20204;&#23558;&#20854;&#25193;&#23637;&#20197;&#20801;&#35768;&#36125;&#21494;&#26031;&#36319;&#36394;&#38750;&#24179;&#31283;&#22122;&#22768;&#21464;&#21270;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#30340;5G&#34892;&#39542;&#27979;&#35797;&#25968;&#25454;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#36319;&#36394;&#38750;&#24179;&#31283;&#20449;&#36947;&#22122;&#22768;&#65292;&#20248;&#20110;&#20855;&#26377;&#22266;&#23450;&#23454;&#38469;&#24179;&#22343;&#20449;&#36947;&#22122;&#22768;&#30693;&#35782;&#30340;LDPC&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a sequential Bayesian learning method for tracking non-stationary signal-to-noise ratios in LDPC codes using probabilistic graphical models. We represent the LDPC code as a cluster graph using a general purpose cluster graph construction algorithm called the layered trees running intersection property (LTRIP) algorithm. The channel noise estimator is a global Gamma cluster, which we extend to allow for Bayesian tracking of non-stationary noise variation. We evaluate our proposed model on real-world 5G drive test data. Our results show that our model is capable of tracking non-stationary channel noise, which outperforms an LDPC code with a fixed knowledge of the actual average channel noise.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;LDPC&#30721;&#30340;&#32858;&#31867;&#22270;&#21644;&#22240;&#23376;&#22270;&#34920;&#31034;&#65292;&#32467;&#26524;&#26174;&#31034;&#32858;&#31867;&#22270;&#34920;&#31034;&#20248;&#20110;&#20256;&#32479;&#30340;&#22240;&#23376;&#22270;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2204.06350</link><description>&lt;p&gt;
LDPC&#30721;&#65306;&#27604;&#36739;&#32858;&#31867;&#22270;&#21644;&#22240;&#23376;&#22270;
&lt;/p&gt;
&lt;p&gt;
LDPC codes: comparing cluster graphs to factor graphs. (arXiv:2204.06350v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;LDPC&#30721;&#30340;&#32858;&#31867;&#22270;&#21644;&#22240;&#23376;&#22270;&#34920;&#31034;&#65292;&#32467;&#26524;&#26174;&#31034;&#32858;&#31867;&#22270;&#34920;&#31034;&#20248;&#20110;&#20256;&#32479;&#30340;&#22240;&#23376;&#22270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;LDPC&#30721;&#30340;&#32858;&#31867;&#22270;&#21644;&#22240;&#23376;&#22270;&#34920;&#31034;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;&#22312;&#27010;&#29575;&#22270;&#27169;&#22411;&#20013;&#65292;&#32858;&#31867;&#22270;&#20445;&#30041;&#20102;&#25512;&#29702;&#36807;&#31243;&#20013;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#26377;&#29992;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#22312;&#35745;&#31639;&#25104;&#26412;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#36793;&#38469;&#27010;&#29575;&#20934;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#20248;&#21183;&#22312;LDPC&#30721;&#30340;&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#34920;&#26126;&#32858;&#31867;&#22270;&#34920;&#31034;&#20248;&#20110;&#20256;&#32479;&#30340;&#22240;&#23376;&#22270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comparison study between a cluster and factor graph representation of LDPC codes. In probabilistic graphical models, cluster graphs retain useful dependence between random variables during inference, which are advantageous in terms of computational cost, convergence speed, and accuracy of marginal probabilities. This study investigates these benefits in the context of LDPC codes and shows that a cluster graph representation outperforms the traditional factor graph representation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21015;&#20030;&#38382;&#39064;&#23454;&#20363;&#24635;&#25439;&#22833;&#20989;&#25968;&#30340;&#37096;&#20998;&#26469;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20986;&#24863;&#30693;ERM&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#22810;&#21442;&#25968;&#32452;&#21512;&#31639;&#27861;&#26063;&#30340;&#35745;&#31639;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.03569</link><description>&lt;p&gt;
&#22522;&#20110;&#36755;&#20986;&#24863;&#30693;ERM&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Output-sensitive ERM-based techniques for data-driven algorithm design. (arXiv:2204.03569v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21015;&#20030;&#38382;&#39064;&#23454;&#20363;&#24635;&#25439;&#22833;&#20989;&#25968;&#30340;&#37096;&#20998;&#26469;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20986;&#24863;&#30693;ERM&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#22810;&#21442;&#25968;&#32452;&#21512;&#31639;&#27861;&#26063;&#30340;&#35745;&#31639;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#26159;&#19968;&#31181;&#26377;&#28508;&#21147;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36229;&#20986;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#20855;&#26377;&#21487;&#35843;&#21442;&#25968;&#30340;&#31639;&#27861;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#26159;&#20026;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#30340;&#32452;&#21512;&#31639;&#27861;&#26063;&#35774;&#35745;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#12290;&#24403;&#22266;&#23450;&#38382;&#39064;&#23454;&#20363;&#24182;&#21464;&#21270;&#21442;&#25968;&#26102;&#65292;"&#23545;&#20598;"&#25439;&#22833;&#20989;&#25968;&#36890;&#24120;&#20855;&#26377;&#20998;&#27573;&#21487;&#20998;&#35299;&#30340;&#32467;&#26500;&#65292;&#21363;&#38500;&#20102;&#26576;&#20123;&#23574;&#38160;&#30340;&#36716;&#25442;&#36793;&#30028;&#22806;&#37117;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21015;&#20030;&#19968;&#32452;&#38382;&#39064;&#23454;&#20363;&#30340;&#24635;&#25439;&#22833;&#20989;&#25968;&#30340;&#37096;&#20998;&#26469;&#24320;&#23637;&#25216;&#26415;&#30740;&#31350;&#65292;&#20197;&#24320;&#21457;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#30340;&#39640;&#25928;ERM&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#23454;&#38469;&#20986;&#29616;&#30340;&#37096;&#20998;&#25968;&#30446;&#25104;&#27604;&#20363;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#37096;&#20998;&#25968;&#30446;&#30340;&#26368;&#22351;&#24773;&#20917;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20004;&#20010;&#26032;&#39062;&#30340;&#35201;&#32032; - &#19968;&#31181;&#29992;&#20110;&#26522;&#20030;&#30001;&#19968;&#32452;&#36229;&#24179;&#38754;&#35825;&#23548;&#30340;&#22810;&#38754;&#20307;&#30340;&#36755;&#20986;&#24863;&#30693;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven algorithm design is a promising, learning-based approach for beyond worst-case analysis of algorithms with tunable parameters. An important open problem is the design of computationally efficient data-driven algorithms for combinatorial algorithm families with multiple parameters. As one fixes the problem instance and varies the parameters, the "dual" loss function typically has a piecewise-decomposable structure, i.e. is well-behaved except at certain sharp transition boundaries. In this work we initiate the study of techniques to develop efficient ERM learning algorithms for data-driven algorithm design by enumerating the pieces of the sum dual loss functions for a collection of problem instances. The running time of our approach scales with the actual number of pieces that appear as opposed to worst case upper bounds on the number of pieces. Our approach involves two novel ingredients -- an output-sensitive algorithm for enumerating polytopes induced by a set of hyperpla
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#19968;&#20010;&#20809;&#23398;&#27169;&#25311;&#29615;&#22659;&#65292;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#25511;&#21046;&#31639;&#27861;&#65292;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#22797;&#26434;&#20809;&#23398;&#25511;&#21046;&#29615;&#22659;&#20013;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2203.12114</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20809;&#23398;&#25511;&#21046;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
An Optical Control Environment for Benchmarking Reinforcement Learning Algorithms. (arXiv:2203.12114v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#19968;&#20010;&#20809;&#23398;&#27169;&#25311;&#29615;&#22659;&#65292;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#25511;&#21046;&#31639;&#27861;&#65292;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#22797;&#26434;&#20809;&#23398;&#25511;&#21046;&#29615;&#22659;&#20013;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26377;&#28508;&#21147;&#35299;&#20915;&#21508;&#31181;&#31185;&#23398;&#38382;&#39064;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#22522;&#20110;&#25511;&#21046;&#22120;&#30340;&#20809;&#23398;&#27169;&#25311;&#29615;&#22659;&#12290;&#35813;&#29615;&#22659;&#25429;&#25417;&#20102;&#20809;&#23398;&#31995;&#32479;&#22266;&#26377;&#30340;&#38750;&#20984;&#24615;&#12289;&#38750;&#32447;&#24615;&#21644;&#26102;&#21464;&#22122;&#22768;&#30340;&#26412;&#36136;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#30495;&#23454;&#30340;&#35774;&#32622;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25552;&#20986;&#30340;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#22522;&#20934;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23548;&#33322;&#22797;&#26434;&#20809;&#23398;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#25511;&#21046;&#31639;&#27861;&#12290;&#35770;&#25991;&#20195;&#30721;&#21487;&#22312;https://github.com/Walleclipse/Reinforcement-Learning-Pulse-Stacking&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning has the potential to address various scientific problems. In this paper, we implement an optics simulation environment for reinforcement learning based controllers. The environment captures the essence of nonconvexity, nonlinearity, and time-dependent noise inherent in optical systems, offering a more realistic setting. Subsequently, we provide the benchmark results of several reinforcement learning algorithms on the proposed simulation environment. The experimental findings demonstrate the superiority of off-policy reinforcement learning approaches over traditional control algorithms in navigating the intricacies of complex optical control environments. The code of the paper is available at https://github.com/Walleclipse/Reinforcement-Learning-Pulse-Stacking.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#25022;&#30340;&#35838;&#31243;&#26041;&#27861;&#65292;&#23558;&#29615;&#22659;&#35774;&#35745;&#20316;&#20026;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#28216;&#25103;&#36827;&#34892;&#65292;&#20197;&#20135;&#29983;&#23398;&#29983;&#26234;&#33021;&#20307;&#33021;&#21147;&#21069;&#27839;&#30340;&#29615;&#22659;&#23454;&#20363;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#36827;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#20851;&#21345;&#12290;</title><link>http://arxiv.org/abs/2203.01302</link><description>&lt;p&gt;
&#29992;&#36951;&#25022;&#22522;&#20110;&#29615;&#22659;&#35774;&#35745;&#26469;&#28436;&#36827;&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Evolving Curricula with Regret-Based Environment Design. (arXiv:2203.01302v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#25022;&#30340;&#35838;&#31243;&#26041;&#27861;&#65292;&#23558;&#29615;&#22659;&#35774;&#35745;&#20316;&#20026;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#28216;&#25103;&#36827;&#34892;&#65292;&#20197;&#20135;&#29983;&#23398;&#29983;&#26234;&#33021;&#20307;&#33021;&#21147;&#21069;&#27839;&#30340;&#29615;&#22659;&#23454;&#20363;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#36827;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#20851;&#21345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#35757;&#32451;&#20855;&#26377;&#26222;&#36941;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#40065;&#26834;&#24615;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#26159;&#20351;&#29992;&#35838;&#31243;&#12290;&#19968;&#31867;&#26041;&#27861;&#23558;&#29615;&#22659;&#35774;&#35745;&#35270;&#20026;&#23398;&#29983;&#21644;&#25945;&#24072;&#20043;&#38388;&#30340;&#19968;&#20010;&#28216;&#25103;&#65292;&#21033;&#29992;&#22522;&#20110;&#36951;&#25022;&#30340;&#30446;&#26631;&#26469;&#20135;&#29983;&#23398;&#29983;&#26234;&#33021;&#20307;&#33021;&#21147;&#21069;&#27839;&#30340;&#29615;&#22659;&#23454;&#20363;&#65288;&#25110;&#20851;&#21345;&#65289;&#12290;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#24182;&#22312;&#24179;&#34913;&#29366;&#24577;&#19979;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#20294;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#32463;&#24120;&#38590;&#20197;&#25214;&#21040;&#26377;&#25928;&#30340;&#20851;&#21345;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36827;&#21270;&#26041;&#27861;&#33268;&#21147;&#20110;&#36880;&#27493;&#25913;&#21464;&#29615;&#22659;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#28508;&#22312;&#30340;&#26080;&#38480;&#23398;&#20064;&#65292;&#20294;&#36890;&#24120;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#25022;&#30340;&#35838;&#31243;&#26041;&#27861;&#65292;&#20197;&#21512;&#29702;&#12289;&#26377;&#21407;&#21017;&#22320;&#21033;&#29992;&#36827;&#21270;&#30340;&#21147;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#36890;&#36807;&#32534;&#36753;&#20851;&#21345;&#36880;&#28176;&#22797;&#21512;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It remains a significant challenge to train generally capable agents with reinforcement learning (RL). A promising avenue for improving the robustness of RL agents is through the use of curricula. One such class of methods frames environment design as a game between a student and a teacher, using regret-based objectives to produce environment instantiations (or levels) at the frontier of the student agent's capabilities. These methods benefit from their generality, with theoretical guarantees at equilibrium, yet they often struggle to find effective levels in challenging design spaces. By contrast, evolutionary approaches seek to incrementally alter environment complexity, resulting in potentially open-ended learning, but often rely on domain-specific heuristics and vast amounts of computational resources. In this paper we propose to harness the power of evolution in a principled, regret-based curriculum. Our approach, which we call Adversarially Compounding Complexity by Editing Level
&lt;/p&gt;</description></item><item><title>&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#21512;&#20316;&#26469;&#25552;&#39640;&#27599;&#20010;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;DDAL&#26469;&#25903;&#25345;&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2202.05135</link><description>&lt;p&gt;
&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Group-Agent Reinforcement Learning. (arXiv:2202.05135v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05135
&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#21512;&#20316;&#26469;&#25552;&#39640;&#27599;&#20010;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;DDAL&#26469;&#25903;&#25345;&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#22810;&#20010;&#22320;&#29702;&#20998;&#24067;&#30340;&#20195;&#29702;&#36827;&#34892;&#21512;&#20316;&#24615;&#30340;&#20010;&#21035;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#21487;&#20197;&#20026;&#27599;&#20010;&#20195;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#24102;&#26469;&#24456;&#22823;&#30340;&#22909;&#22788;&#12290;&#19982;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#19981;&#21516;&#65292;MARL&#20013;&#22810;&#20010;&#20195;&#29702;&#20849;&#21516;&#23384;&#22312;&#20110;&#19968;&#20010;&#29615;&#22659;&#20013;&#65292;&#24182;&#19988;&#38656;&#35201;&#23398;&#20064;&#22914;&#20309;&#21512;&#20316;&#25110;&#31454;&#20105;&#12290;&#22312;&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#33258;&#24049;&#30340;&#29615;&#22659;&#65292;&#24182;&#19988;&#21482;&#19982;&#20854;&#20182;&#20195;&#29702;&#36827;&#34892;&#36890;&#20449;&#20197;&#20998;&#20139;&#30693;&#35782;&#65292;&#27809;&#26377;&#21512;&#20316;&#25110;&#31454;&#20105;&#34892;&#20026;&#20316;&#20026;&#23398;&#20064;&#32467;&#26524;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#31181;&#24773;&#26223;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20854;&#27010;&#24565;&#21487;&#20197;&#24212;&#29992;&#20110;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#23578;&#26410;&#24456;&#22909;&#29702;&#35299;&#21644;&#34920;&#36848;&#12290;&#20316;&#20026;&#39318;&#27425;&#23581;&#35797;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20316;&#20026;&#23545;&#21333;&#20010;&#20195;&#29702;&#21644;&#22810;&#20010;&#20195;&#29702;&#31995;&#32479;&#30340;&#31532;&#19977;&#31867;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#34920;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;DDAL&#65288;&#20998;&#25955;&#24335;&#20998;&#24067;&#24335;&#24322;&#27493;&#23398;&#20064;&#65289;&#65292;&#19987;&#20026;&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
It can largely benefit the reinforcement learning (RL) process of each agent if multiple geographically distributed agents perform their separate RL tasks cooperatively. Different from multi-agent reinforcement learning (MARL) where multiple agents are in a common environment and should learn to cooperate or compete with each other, in this case each agent has its separate environment and only communicates with others to share knowledge without any cooperative or competitive behaviour as a learning outcome. In fact, this scenario exists widely in real life whose concept can be utilised in many applications, but is not well understood yet and not well formulated. As the first effort, we propose group-agent system for RL as a formulation of this scenario and the third type of RL system with respect to single-agent and multi-agent systems. We then propose a distributed RL framework called DDAL (Decentralised Distributed Asynchronous Learning) designed for group-agent reinforcement learnin
&lt;/p&gt;</description></item><item><title>GLISp-r&#26159;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#21644;&#36164;&#28304;&#21208;&#25506;&#65292;&#36845;&#20195;&#22320;&#25552;&#20986;&#26032;&#30340;&#26679;&#26412;&#19982;&#26368;&#20339;&#26657;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2202.01125</link><description>&lt;p&gt;
GLISp-r&#65306;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#22522;&#20110;&#20559;&#22909;&#30340;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
GLISp-r: A preference-based optimization algorithm with convergence guarantees. (arXiv:2202.01125v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01125
&lt;/p&gt;
&lt;p&gt;
GLISp-r&#26159;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#21644;&#36164;&#28304;&#21208;&#25506;&#65292;&#36845;&#20195;&#22320;&#25552;&#20986;&#26032;&#30340;&#26679;&#26412;&#19982;&#26368;&#20339;&#26657;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#20248;&#21270;&#31639;&#27861;&#26159;&#19968;&#31181;&#36845;&#20195;&#36807;&#31243;&#65292;&#20165;&#22522;&#20110;&#19981;&#21516;&#35843;&#35856;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#23547;&#27714;&#20915;&#31574;&#21521;&#37327;&#30340;&#26368;&#20248;&#26657;&#20934;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#20154;&#20026;&#20915;&#31574;&#32773;&#34920;&#36798;&#23545;&#20004;&#20010;&#26657;&#20934;&#65288;&#26679;&#26412;&#65289;&#20043;&#38388;&#30340;&#20559;&#22909;&#65292;&#24378;&#35843;&#21738;&#20010;&#26657;&#20934;&#65288;&#22914;&#26524;&#26377;&#65289;&#20248;&#20110;&#21478;&#19968;&#20010;&#12290;&#20248;&#21270;&#36807;&#31243;&#24517;&#39035;&#20351;&#29992;&#35266;&#23519;&#21040;&#30340;&#20559;&#22909;&#26469;&#25214;&#21040;&#20915;&#31574;&#32773;&#26368;&#21916;&#27426;&#30340;&#20915;&#31574;&#21521;&#37327;&#35843;&#35856;&#65292;&#21516;&#26102;&#36824;&#35201;&#26368;&#23567;&#21270;&#27604;&#36739;&#30340;&#27425;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#25928;&#29992;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#38416;&#36848;&#22522;&#20110;&#20559;&#22909;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GLISp-r&#65292;&#36825;&#26159;&#26368;&#36817;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#20248;&#21270;&#36807;&#31243;GLISp&#30340;&#25193;&#23637;&#12290;&#21518;&#32773;&#20351;&#29992;&#24452;&#21521;&#22522;&#20989;&#25968;&#26367;&#20195;&#27169;&#22411;&#26469;&#25551;&#36848;&#20915;&#31574;&#32773;&#30340;&#21916;&#22909;&#12290;GLISp-r&#36890;&#36807;&#22312;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#24320;&#21457;&#21644;&#36164;&#28304;&#21208;&#25506;&#20043;&#38388;&#26435;&#34913;&#65292;&#36845;&#20195;&#22320;&#25552;&#20986;&#26032;&#30340;&#26679;&#26412;&#19982;&#26368;&#20339;&#26657;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference-based optimization algorithms are iterative procedures that seek the optimal calibration of a decision vector based only on comparisons between couples of different tunings. At each iteration, a human decision-maker expresses a preference between two calibrations (samples), highlighting which one, if any, is better than the other. The optimization procedure must use the observed preferences to find the tuning of the decision vector that is most preferred by the decision-maker, while also minimizing the number of comparisons. In this work, we formulate the preference-based optimization problem from a utility theory perspective. Then, we propose GLISp-r, an extension of a recent preference-based optimization procedure called GLISp. The latter uses a Radial Basis Function surrogate to describe the tastes of the decision-maker. Iteratively, GLISp proposes new samples to compare with the best calibration available by trading off exploitation of the surrogate model and exploration
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Kriging&#29702;&#35770;&#30340;&#22810;&#23618;&#27425;&#38543;&#26426;&#20248;&#21270;&#22635;&#34917;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#12289;&#26356;&#24555;&#36895;&#21644;&#26356;&#31283;&#23450;&#22320;&#22788;&#29702;&#22823;&#35268;&#27169;&#21307;&#30103;&#25968;&#25454;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#25968;&#20540;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2110.09680</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21307;&#30103;&#25968;&#25454;&#35760;&#24405;&#20013;&#30340;&#22810;&#23618;&#27425;&#38543;&#26426;&#20248;&#21270;&#22635;&#34917;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multilevel Stochastic Optimization for Imputation in Massive Medical Data Records. (arXiv:2110.09680v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.09680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Kriging&#29702;&#35770;&#30340;&#22810;&#23618;&#27425;&#38543;&#26426;&#20248;&#21270;&#22635;&#34917;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#12289;&#26356;&#24555;&#36895;&#21644;&#26356;&#31283;&#23450;&#22320;&#22788;&#29702;&#22823;&#35268;&#27169;&#21307;&#30103;&#25968;&#25454;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#25968;&#20540;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#21644;&#20998;&#26512;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26368;&#36817;&#22312;&#30740;&#31350;&#21644;&#21457;&#23637;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#35782;&#21040;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#22823;&#37327;&#32570;&#22833;&#30340;&#25968;&#20540;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Kriging&#29702;&#35770;&#30340;&#25968;&#23398;&#21407;&#21017;&#38543;&#26426;&#20248;&#21270;&#22635;&#34917;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22635;&#34917;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20854;&#35745;&#31639;&#25104;&#26412;&#21644;&#28508;&#22312;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#20250;&#23548;&#33268;&#26114;&#36149;&#21644;/&#25110;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#21487;&#33021;&#38480;&#21046;&#20854;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#26368;&#36817;&#24320;&#21457;&#30340;&#22810;&#23618;&#27425;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#21307;&#30103;&#35760;&#24405;&#20013;&#30340;&#22635;&#34917;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#35745;&#31639;&#24212;&#29992;&#25968;&#23398;&#25216;&#26415;&#65292;&#24182;&#20855;&#26377;&#39640;&#31934;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#26368;&#20339;&#32447;&#24615;&#26080;&#20559;&#39044;&#27979;&#22120;&#65288;BLUP&#65289;&#65292;&#35813;&#22810;&#23618;&#27425;&#24418;&#24335;&#21270;&#26159;&#31934;&#30830;&#30340;&#65292;&#32780;&#19988;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#25968;&#20540;&#31283;&#23450;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration and analysis of massive datasets has recently generated increasing interest in the research and development communities. It has long been a recognized problem that many datasets contain significant levels of missing numerical data. We introduce a mathematically principled stochastic optimization imputation method based on the theory of Kriging. This is shown to be a powerful method for imputation. However, its computational effort and potential numerical instabilities produce costly and/or unreliable predictions, potentially limiting its use on large scale datasets. In this paper, we apply a recently developed multi-level stochastic optimization approach to the problem of imputation in massive medical records. The approach is based on computational applied mathematics techniques and is highly accurate. In particular, for the Best Linear Unbiased Predictor (BLUP) this multi-level formulation is exact, and is also significantly faster and more numerically stable. This permits
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#24418;&#29366;&#20248;&#21270;&#65292;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#24037;&#19994;&#35774;&#35745;&#24615;&#33021;&#26102;&#65292;&#35299;&#20915;&#20102;&#24418;&#29366;&#20559;&#31163;&#35757;&#32451;&#38598;&#26102;&#39044;&#27979;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#39640;&#20102;&#32467;&#26524;&#24418;&#29366;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2109.13337</link><description>&lt;p&gt;
DEBOSH: &#28145;&#24230;&#36125;&#21494;&#26031;&#24418;&#29366;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
DEBOSH: Deep Bayesian Shape Optimization. (arXiv:2109.13337v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.13337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#24418;&#29366;&#20248;&#21270;&#65292;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#24037;&#19994;&#35774;&#35745;&#24615;&#33021;&#26102;&#65292;&#35299;&#20915;&#20102;&#24418;&#29366;&#20559;&#31163;&#35757;&#32451;&#38598;&#26102;&#39044;&#27979;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#39640;&#20102;&#32467;&#26524;&#24418;&#29366;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21487;&#20197;&#24555;&#36895;&#20934;&#30830;&#22320;&#39044;&#27979;&#24037;&#19994;&#35774;&#35745;&#30340;&#24615;&#33021;&#65292;&#24182;&#29992;&#20110;&#26377;&#25928;&#20248;&#21270;&#20854;&#24418;&#29366;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20805;&#20998;&#25506;&#32034;&#24418;&#29366;&#31354;&#38388;&#65292;&#36890;&#24120;&#38656;&#35201;&#32771;&#34385;&#19982;&#35757;&#32451;&#38598;&#26126;&#26174;&#20559;&#31163;&#30340;&#24418;&#29366;&#12290;&#23545;&#20110;&#36825;&#20123;&#24773;&#20917;&#65292;GNN&#30340;&#39044;&#27979;&#21464;&#24471;&#19981;&#21487;&#38752;&#65292;&#20294;&#36825;&#36890;&#24120;&#34987;&#24573;&#35270;&#12290;&#38024;&#23545;&#20381;&#36182;&#39640;&#26031;&#36807;&#31243;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#36890;&#36807;&#21033;&#29992;&#20854;&#35780;&#20272;&#33258;&#36523;&#31934;&#24230;&#30340;&#33021;&#21147;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#20272;&#35745;&#20854;&#19981;&#30830;&#23450;&#24615;&#30340;&#26631;&#20934;&#26041;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#35745;&#31639;&#37327;&#22823;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24418;&#29366;&#20248;&#21270;&#30340;&#26032;&#39062;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#23427;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;BO&#65292;&#24182;&#25552;&#39640;&#20102;&#32467;&#26524;&#24418;&#29366;&#30340;&#36136;&#37327;&#65292;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) can predict the performance of an industrial design quickly and accurately and be used to optimize its shape effectively. However, to fully explore the shape space, one must often consider shapes deviating significantly from the training set. For these, GNN predictions become unreliable, something that is often ignored. For optimization techniques relying on Gaussian Processes, Bayesian Optimization (BO) addresses this issue by exploiting their ability to assess their own accuracy. Unfortunately, this is harder to do when using neural networks because standard approaches to estimating their uncertainty can entail high computational loads and reduced model accuracy. Hence, we propose a novel uncertainty-based method tailored to shape optimization. It enables effective BO and increases the quality of the resulting shapes beyond that of state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31232;&#30095;&#21152;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;(SLR)&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#27169;&#22411;&#21644;&#27714;&#35299;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2109.12701</link><description>&lt;p&gt;
&#31232;&#30095;&#21152;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;: &#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization Approach. (arXiv:2109.12701v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31232;&#30095;&#21152;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;(SLR)&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#27169;&#22411;&#21644;&#27714;&#35299;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31232;&#30095;&#21152;&#20302;&#31209;&#20998;&#35299;&#38382;&#39064;(SLR)&#65292;&#21363;&#23558;&#25439;&#22351;&#30340;&#25968;&#25454;&#30697;&#38453;&#20998;&#35299;&#20026;&#21253;&#21547;&#22522;&#26412;&#30495;&#20540;&#30340;&#20302;&#31209;&#30697;&#38453;&#21644;&#21253;&#21547;&#25200;&#21160;&#30340;&#31232;&#30095;&#30697;&#38453;&#12290; SLR&#26159;&#36816;&#31609;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#22522;&#30784;&#38382;&#39064;&#65292;&#22312;&#25968;&#25454;&#21387;&#32553;&#12289;&#28508;&#22312;&#35821;&#20041;&#32034;&#24341;&#12289;&#21327;&#21516;&#36807;&#28388;&#21644;&#21307;&#23398;&#25104;&#20687;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#20102;&#20132;&#26367;&#26368;&#23567;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#20197;&#21450;&#26032;&#30340;&#21322;&#23450;&#26494;&#24347;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#23450;&#20041;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#20984;&#26494;&#24347;&#26469;&#35299;&#20915;&#23567;&#35268;&#27169;&#30340;SLR&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915; $n=10000$ &#30340;&#38382;&#39064;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the Sparse Plus Low-Rank decomposition problem (SLR), which is the problem of decomposing a corrupted data matrix into a sparse matrix of perturbations plus a low-rank matrix containing the ground truth. SLR is a fundamental problem in Operations Research and Machine Learning which arises in various applications, including data compression, latent semantic indexing, collaborative filtering, and medical imaging. We introduce a novel formulation for SLR that directly models its underlying discreteness. For this formulation, we develop an alternating minimization heuristic that computes high-quality solutions and a novel semidefinite relaxation that provides meaningful bounds for the solutions returned by our heuristic. We also develop a custom branch-and-bound algorithm that leverages our heuristic and convex relaxations to solve small instances of SLR to certifiable (near) optimality. Given an input $n$-by-$n$ matrix, our heuristic scales to solve instances where $n=10000$ in m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#20174;&#32463;&#39564;&#12289;&#20849;&#35782;&#21644;&#26368;&#20339;&#23454;&#36341;&#20013;&#25552;&#28860;&#20986;&#30340;&#25351;&#23548;&#21407;&#21017;&#65292;&#26088;&#22312;&#24341;&#39046;&#21307;&#23398;&#24433;&#20687;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;&#20449;&#20219;&#12289;&#23433;&#20840;&#24615;&#21644;&#24212;&#29992;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2109.09658</link><description>&lt;p&gt;
FUTURE-AI:&#21307;&#23398;&#24433;&#20687;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#20849;&#35782;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging. (arXiv:2109.09658v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.09658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#20174;&#32463;&#39564;&#12289;&#20849;&#35782;&#21644;&#26368;&#20339;&#23454;&#36341;&#20013;&#25552;&#28860;&#20986;&#30340;&#25351;&#23548;&#21407;&#21017;&#65292;&#26088;&#22312;&#24341;&#39046;&#21307;&#23398;&#24433;&#20687;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;&#20449;&#20219;&#12289;&#23433;&#20840;&#24615;&#21644;&#24212;&#29992;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#20020;&#24202;&#31995;&#32479;&#29983;&#25104;&#30340;&#22823;&#37327;&#25968;&#25454;&#30340;&#32467;&#21512;&#65292;&#25512;&#21160;&#20102;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#25972;&#20010;&#20215;&#20540;&#38142;&#19978;&#30340;&#25104;&#20687;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#22270;&#20687;&#37325;&#24314;&#12289;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12289;&#22522;&#20110;&#22270;&#20687;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#12290;&#23613;&#31649;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#25104;&#21151;&#24182;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#35768;&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#25285;&#24515;&#25104;&#20687;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#35748;&#20026;&#20854;&#22797;&#26434;&#12289;&#19981;&#36879;&#26126;&#12289;&#38590;&#20197;&#29702;&#35299;&#12289;&#38590;&#20197;&#24212;&#29992;&#21644;&#38590;&#20197;&#22312;&#20851;&#38190;&#20020;&#24202;&#24212;&#29992;&#20013;&#24314;&#31435;&#20449;&#20219;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#25285;&#24551;&#21644;&#39118;&#38505;&#65292;&#20294;&#30446;&#21069;&#23578;&#27809;&#26377;&#20855;&#20307;&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#26469;&#24341;&#23548;&#26410;&#26469;&#21307;&#23398;&#24433;&#20687;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#20197;&#22686;&#21152;&#20449;&#20219;&#12289;&#23433;&#20840;&#24615;&#21644;&#37319;&#29992;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#31215;&#32047;&#30340;&#32463;&#39564;&#12289;&#20849;&#35782;&#21644;&#26368;&#20339;&#23454;&#36341;&#20013;&#31934;&#36873;&#20986;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in artificial intelligence (AI) combined with the extensive amount of data generated by today's clinical systems, has led to the development of imaging AI solutions across the whole value chain of medical imaging, including image reconstruction, medical image segmentation, image-based diagnosis and treatment planning. Notwithstanding the successes and future potential of AI in medical imaging, many stakeholders are concerned of the potential risks and ethical implications of imaging AI solutions, which are perceived as complex, opaque, and difficult to comprehend, utilise, and trust in critical clinical applications. Despite these concerns and risks, there are currently no concrete guidelines and best practices for guiding future AI developments in medical imaging towards increased trust, safety and adoption. To bridge this gap, this paper introduces a careful selection of guiding principles drawn from the accumulated experiences, consensus, and best practices f
&lt;/p&gt;</description></item><item><title>Comfetch&#26159;&#19968;&#20010;&#36890;&#36807;&#20351;&#29992;&#33609;&#22270;&#30340;&#31616;&#21270;&#34920;&#31034;&#24418;&#24335;&#65292;&#20801;&#35768;&#36164;&#28304;&#21463;&#38480;&#30340;&#23458;&#25143;&#31471;&#36827;&#34892;&#22823;&#35268;&#27169;&#32593;&#32476;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2109.08346</link><description>&lt;p&gt;
Comfetch: &#36890;&#36807;&#33609;&#22270;&#22312;&#21463;&#38480;&#23458;&#25143;&#31471;&#19978;&#24320;&#23637;&#22823;&#35268;&#27169;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Comfetch: Federated Learning of Large Networks on Constrained Clients via Sketching. (arXiv:2109.08346v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.08346
&lt;/p&gt;
&lt;p&gt;
Comfetch&#26159;&#19968;&#20010;&#36890;&#36807;&#20351;&#29992;&#33609;&#22270;&#30340;&#31616;&#21270;&#34920;&#31034;&#24418;&#24335;&#65292;&#20801;&#35768;&#36164;&#28304;&#21463;&#38480;&#30340;&#23458;&#25143;&#31471;&#36827;&#34892;&#22823;&#35268;&#27169;&#32593;&#32476;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#36793;&#32536;&#19978;&#36827;&#34892;&#31169;&#23494;&#21644;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#22312;&#38598;&#20013;&#24335;FL&#20013;&#65292;&#20840;&#23616;&#26550;&#26500;&#30340;&#21442;&#25968;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#30001;&#20013;&#22830;&#26381;&#21153;&#22120;/&#25511;&#21046;&#22120;&#32500;&#25252;&#21644;&#20998;&#21457;&#32473;&#23458;&#25143;&#31471;&#65292;&#21518;&#32773;&#26681;&#25454;&#26412;&#22320;&#20248;&#21270;&#21521;&#26381;&#21153;&#22120;&#20256;&#36755;&#27169;&#22411;&#26356;&#26032;&#65288;&#26799;&#24230;&#65289;&#12290;&#23613;&#31649;&#35768;&#22810;&#24037;&#20316;&#37117;&#33268;&#21147;&#20110;&#20943;&#23569;&#26799;&#24230;&#20256;&#36755;&#30340;&#36890;&#20449;&#22797;&#26434;&#24615;&#65292;&#20294;&#32477;&#22823;&#22810;&#25968;&#22522;&#20110;&#21387;&#32553;&#30340;&#31639;&#27861;&#37117;&#20551;&#35774;&#27599;&#20010;&#21442;&#19982;&#30340;&#23458;&#25143;&#31471;&#33021;&#22815;&#19979;&#36733;&#21644;&#35757;&#32451;&#24403;&#21069;&#21644;&#23436;&#25972;&#30340;&#21442;&#25968;&#38598;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#19968;&#20010;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#22240;&#20026;&#36739;&#23567;&#30340;&#23458;&#25143;&#31471;&#65288;&#22914;&#31227;&#21160;&#35774;&#22791;&#65289;&#21487;&#33021;&#20855;&#26377;&#36164;&#28304;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26032;&#31639;&#27861;Comfetch&#65292;&#23427;&#20801;&#35768;&#23458;&#25143;&#31471;&#20351;&#29992;&#20840;&#23616;&#26550;&#26500;&#30340;&#31616;&#21270;&#34920;&#31034;&#26469;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#65292;&#36890;&#36807;&#35745;&#25968;&#33609;&#22270;&#20943;&#23569;&#20102;&#26412;&#22320;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#20197;&#21450;&#21452;&#21521;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a popular paradigm for private and collaborative model training on the edge. In centralized FL, the parameters of a global architecture (such as a deep neural network) are maintained and distributed by a central server/controller to clients who transmit model updates (gradients) back to the server based on local optimization. While many efforts have focused on reducing the communication complexity of gradient transmission, the vast majority of compression-based algorithms assume that each participating client is able to download and train the current and full set of parameters, which may not be a practical assumption depending on the resource constraints of smaller clients such as mobile devices. In this work, we propose a simple yet effective novel algorithm, Comfetch, which allows clients to train large networks using reduced representations of the global architecture via the count sketch, which reduces local computational and memory costs along with bi-dir
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2106.02626</link><description>&lt;p&gt;
&#32422;&#26463;&#36164;&#28304;&#19979;&#31070;&#32463;&#27169;&#22359;&#19987;&#19994;&#21270;&#30340;&#21160;&#21147;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dynamics of specialization in neural modules under resource constraints. (arXiv:2106.02626v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#22823;&#33041;&#22312;&#32467;&#26500;&#21644;&#21151;&#33021;&#19978;&#39640;&#24230;&#27169;&#22359;&#21270;&#65292;&#20294;&#26368;&#36817;&#30340;&#35777;&#25454;&#20351;&#19968;&#20123;&#20154;&#23545;&#20004;&#31181;&#27169;&#22359;&#21270;&#30340;&#31243;&#24230;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#27979;&#35797;&#32467;&#26500;&#27169;&#22359;&#21270;&#26159;&#21542;&#36275;&#20197;&#20445;&#35777;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#21457;&#29616;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#65292;&#38500;&#38750;&#22312;&#26497;&#31471;&#27700;&#24179;&#19978;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#29615;&#22659;&#21644;&#32593;&#32476;&#30340;&#21738;&#20123;&#29305;&#24449;&#20250;&#23548;&#33268;&#19987;&#19994;&#21270;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29609;&#20855;&#29615;&#22659;&#12289;&#20219;&#21153;&#21644;&#32593;&#32476;&#65292;&#20197;&#31934;&#30830;&#25511;&#21046;&#26465;&#20214;&#65292;&#24182;&#34920;&#26126;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20960;&#20010;&#19981;&#21516;&#30340;&#19987;&#19994;&#21270;&#24230;&#37327;&#25351;&#26631;&#32473;&#20986;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#65288;1&#65289;&#19987;&#19994;&#21270;&#21482;&#33021;&#22312;&#29615;&#22659;&#20013;&#37027;&#20123;&#21487;&#20197;&#26126;&#30830;&#20998;&#31163;&#30340;&#29305;&#24449;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;2&#65289;&#19987;&#19994;&#21270;&#26356;&#23481;&#26131;&#22312;&#32593;&#32476;&#36164;&#28304;&#21463;&#21040;&#24378;&#28872;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;3&#65289;&#36825;&#20123;&#21457;&#29616;&#22312; qualitatively &#19978;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has long been believed that the brain is highly modular both in terms of structure and function, although recent evidence has led some to question the extent of both types of modularity. We used artificial neural networks to test the hypothesis that structural modularity is sufficient to guarantee functional specialization, and find that in general, this doesn't necessarily hold except at extreme levels. We then systematically tested which features of the environment and network do lead to the emergence of specialization. We used a simple toy environment, task and network, allowing us precise control, and show that in this setup, several distinct measures of specialization give qualitatively similar results. We further find that (1) specialization can only emerge in environments where features of that environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#23457;&#20102;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;MDL&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#22797;&#26434;&#24230;&#19981;&#20165;&#21462;&#20915;&#20110;&#21442;&#25968;&#25968;&#37327;&#65292;&#36824;&#19982;&#35774;&#35745;&#30697;&#38453;&#25110;&#26680;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#21644;&#20449;&#22122;&#27604;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2006.10189</link><description>&lt;p&gt;
&#37325;&#23457;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Revisiting minimum description length complexity in overparameterized models. (arXiv:2006.10189v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.10189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#23457;&#20102;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;MDL&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#22797;&#26434;&#24230;&#19981;&#20165;&#21462;&#20915;&#20110;&#21442;&#25968;&#25968;&#37327;&#65292;&#36824;&#19982;&#35774;&#35745;&#30697;&#38453;&#25110;&#26680;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#21644;&#20449;&#22122;&#27604;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#24230;&#26159;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#27010;&#24565;&#65292;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;&#27867;&#21270;&#24615;&#33021;&#30340;&#20449;&#24687;&#12290;&#22312;&#20302;&#32500;&#24230;&#24773;&#20917;&#19979;&#65292;&#21442;&#25968;&#25968;&#37327;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#26159;&#25104;&#21151;&#30340;&#65292;&#20294;&#22312;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#65292;&#24403;&#21442;&#25968;&#25968;&#37327;&#36229;&#36807;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26102;&#65292;&#20854;&#21512;&#29702;&#24615;&#19981;&#36275;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Rissanen&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#65288;MDL&#65289;&#21407;&#29702;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#29992;&#20110;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#22522;&#20110;MDL&#30340;&#22797;&#26434;&#24230;&#65288;MDL-COMP&#65289;&#12290;MDL-COMP&#36890;&#36807;&#23545;&#19968;&#20010;&#33391;&#22909;&#30340;Ridge&#20272;&#35745;&#31867;&#25152;&#24341;&#36215;&#30340;&#32534;&#30721;&#32780;&#23450;&#20041;&#20986;&#26469;&#30340;&#26368;&#20248;&#24615;&#20934;&#21017;&#12290;&#25105;&#20204;&#23545;&#32447;&#24615;&#27169;&#22411;&#21644;&#26680;&#26041;&#27861;&#30340;MDL-COMP&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29702;&#35770;&#21051;&#30011;&#65292;&#24182;&#34920;&#26126;&#23427;&#19981;&#20165;&#26159;&#21442;&#25968;&#25968;&#37327;&#30340;&#20989;&#25968;&#65292;&#32780;&#26159;&#35774;&#35745;&#25110;&#26680;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#21644;&#20449;&#22122;&#27604;&#30340;&#20989;&#25968;&#12290;&#23545;&#20110;&#20855;&#26377;n&#20010;&#35266;&#27979;&#20540;&#65292;d&#20010;&#21442;&#25968;&#21644;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#39640;&#26031;&#39044;&#27979;&#22240;&#23376;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;MDL-COMP&#30340;&#23610;&#24230;&#26159;&#32447;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complexity is a fundamental concept underlying statistical learning theory that aims to inform generalization performance. Parameter count, while successful in low-dimensional settings, is not well-justified for overparameterized settings when the number of parameters is more than the number of training samples. We revisit complexity measures based on Rissanen's principle of minimum description length (MDL) and define a novel MDL-based complexity (MDL-COMP) that remains valid for overparameterized models. MDL-COMP is defined via an optimality criterion over the encodings induced by a good Ridge estimator class. We provide an extensive theoretical characterization of MDL-COMP for linear models and kernel methods and show that it is not just a function of parameter count, but rather a function of the singular values of the design or the kernel matrix and the signal-to-noise ratio. For a linear model with $n$ observations, $d$ parameters, and i.i.d. Gaussian predictors, MDL-COMP scales li
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Transformer&#35757;&#32451;&#30340;&#22256;&#38590;&#12290;&#20182;&#20204;&#21457;&#29616;&#19981;&#24179;&#34913;&#30340;&#26799;&#24230;&#19981;&#26159;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#32780;&#26159;&#27599;&#19968;&#23618;&#30340;&#25918;&#22823;&#25928;&#24212;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#12290;&#20182;&#20204;&#35266;&#23519;&#21040;&#36731;&#37327;&#32423;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#27169;&#22411;&#28508;&#21147;&#65292;&#23548;&#33268;&#34920;&#29616;&#36739;&#24046;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2004.08249</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#35757;&#32451;&#30340;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Understanding the Difficulty of Training Transformers. (arXiv:2004.08249v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.08249
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Transformer&#35757;&#32451;&#30340;&#22256;&#38590;&#12290;&#20182;&#20204;&#21457;&#29616;&#19981;&#24179;&#34913;&#30340;&#26799;&#24230;&#19981;&#26159;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#32780;&#26159;&#27599;&#19968;&#23618;&#30340;&#25918;&#22823;&#25928;&#24212;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#12290;&#20182;&#20204;&#35266;&#23519;&#21040;&#36731;&#37327;&#32423;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#27169;&#22411;&#28508;&#21147;&#65292;&#23548;&#33268;&#34920;&#29616;&#36739;&#24046;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#38656;&#35201;&#35774;&#35745;&#20808;&#36827;&#30340;&#20248;&#21270;&#22120;&#21644;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#30340;&#38750;&#24179;&#20961;&#24037;&#20316;&#65288;&#20363;&#22914;&#65292;&#20256;&#32479;&#30340;SGD&#26080;&#27861;&#26377;&#25928;&#35757;&#32451;Transformer&#65289;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#30340;&#35282;&#24230;&#29702;&#35299;$\textit{&#20160;&#20040;&#20351;&#24471;Transformer&#30340;&#35757;&#32451;&#21464;&#24471;&#22256;&#38590;}$&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#19981;&#24179;&#34913;&#30340;&#26799;&#24230;&#24182;&#19981;&#26159;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#24433;&#21709;&#35757;&#32451;&#30340;&#25918;&#22823;&#25928;&#24212;--&#23545;&#20110;&#22810;&#23618;Transformer&#27169;&#22411;&#20013;&#30340;&#27599;&#19968;&#23618;&#65292;&#23427;&#23545;&#20854;&#27531;&#24046;&#20998;&#25903;&#30340;&#20381;&#36182;&#31243;&#24230;&#36739;&#39640;&#65292;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#65292;&#22240;&#20026;&#23427;&#25918;&#22823;&#20102;&#23567;&#30340;&#21442;&#25968;&#25200;&#21160;&#65288;&#20363;&#22914;&#21442;&#25968;&#26356;&#26032;&#65289;&#65292;&#24182;&#23548;&#33268;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#26174;&#33879;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36731;&#37327;&#32423;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#24182;&#23548;&#33268;&#34920;&#29616;&#36739;&#24046;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Admin&#65288;$\textbf{Ad}$aptive &#37325;&#36848;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding designing cutting-edge optimizers and learning rate schedulers carefully (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand $\textit{what complicates Transformer training}$ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially -- for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin ($\textbf{Ad}$aptive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#22270;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#23450;&#20041;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#21367;&#31215;&#21644;&#27744;&#21270;&#25805;&#20316;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#31616;&#21333;&#20219;&#21153;&#19978;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2002.09285</link><description>&lt;p&gt;
&#19968;&#20010;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#22270;&#31354;&#38388;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Convolutional Neural Network into graph space. (arXiv:2002.09285v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.09285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#22270;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#23450;&#20041;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#21367;&#31215;&#21644;&#27744;&#21270;&#25805;&#20316;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#31616;&#21333;&#20219;&#21153;&#19978;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#22312;&#20960;&#21313;&#24180;&#30340;&#21457;&#23637;&#20013;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;CNN&#22312;&#26500;&#24314;&#26102;&#34987;&#38480;&#21046;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#36827;&#34892;&#25805;&#20316;&#12290;&#22240;&#20026;&#21367;&#31215;&#26159;&#38024;&#23545;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#23450;&#20041;&#30340;&#20449;&#21495;&#25805;&#20316;&#12290;&#36825;&#38480;&#21046;&#20102;&#28145;&#24230;&#23398;&#20064;&#20027;&#35201;&#29992;&#20110;&#27431;&#20960;&#37324;&#24471;&#23450;&#20041;&#30340;&#25968;&#25454;&#65292;&#22914;&#22768;&#38899;&#25110;&#22270;&#20687;&#12290;&#32780;&#23454;&#38469;&#19978;&#65292;&#35768;&#22810;&#35745;&#31639;&#26426;&#24212;&#29992;&#39046;&#22495;(&#21253;&#25324;&#32593;&#32476;&#20998;&#26512;&#12289;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#12289;&#21270;&#23398;&#20449;&#24687;&#23398;&#25110;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;)&#28041;&#21450;&#38750;&#27431;&#20960;&#37324;&#24471;&#23450;&#20041;&#30340;&#25968;&#25454;&#65292;&#22914;&#22270;&#12289;&#32593;&#32476;&#25110;&#27969;&#24418;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#30452;&#25509;&#22312;&#22270;&#31354;&#38388;&#20013;&#23450;&#20041;&#12290;&#21367;&#31215;&#21644;&#27744;&#21270;&#25805;&#20316;&#22312;&#22270;&#22495;&#20013;&#23450;&#20041;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#21453;&#21521;&#20256;&#25773;&#29615;&#22659;&#19979;&#20351;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#31616;&#21333;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#19988;&#22312;&#22270;&#22495;&#21464;&#21270;&#26041;&#38754;&#34920;&#29616;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs), in a few decades, have outperformed the existing state of the art methods in classification context. However, in the way they were formalised, CNNs are bound to operate on euclidean spaces. Indeed, convolution is a signal operation that are defined on euclidean spaces. This has restricted deep learning main use to euclidean-defined data such as sound or image. And yet, numerous computer application fields (among which network analysis, computational social science, chemo-informatics or computer graphics) induce non-euclideanly defined data such as graphs, networks or manifolds. In this paper we propose a new convolution neural network architecture, defined directly into graph space. Convolution and pooling operators are defined in graph domain. We show its usability in a back-propagation context. Experimental results show that our model performance is at state of the art level on simple tasks. It shows robustness with respect to graph domain change
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35299;&#20915;&#33258;&#21160;&#27979;&#35797;&#20013;&#30340;&#27979;&#35797;&#31070;&#35861;&#38382;&#39064;&#12290;&#20316;&#32773;&#37319;&#29992;&#23545;&#25191;&#34892;&#36712;&#36857;&#36827;&#34892;&#26631;&#35760;&#21644;&#35757;&#32451;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#21306;&#20998;&#36890;&#36807;&#21644;&#22833;&#36133;&#25191;&#34892;&#30340;&#36816;&#34892;&#26102;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#36890;&#29992;&#12289;&#21487;&#25193;&#23637;&#21644;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2001.02444</link><description>&lt;p&gt;
&#23398;&#20064;&#32534;&#30721;&#21644;&#20998;&#31867;&#27979;&#35797;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Learning to Encode and Classify Test Executions. (arXiv:2001.02444v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.02444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35299;&#20915;&#33258;&#21160;&#27979;&#35797;&#20013;&#30340;&#27979;&#35797;&#31070;&#35861;&#38382;&#39064;&#12290;&#20316;&#32773;&#37319;&#29992;&#23545;&#25191;&#34892;&#36712;&#36857;&#36827;&#34892;&#26631;&#35760;&#21644;&#35757;&#32451;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#21306;&#20998;&#36890;&#36807;&#21644;&#22833;&#36133;&#25191;&#34892;&#30340;&#36816;&#34892;&#26102;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#36890;&#29992;&#12289;&#21487;&#25193;&#23637;&#21644;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30830;&#23450;&#27979;&#35797;&#25191;&#34892;&#30340;&#27491;&#30830;&#24615;&#34987;&#31216;&#20026;&#27979;&#35797;&#31070;&#35861;&#38382;&#39064;&#65292;&#26159;&#33258;&#21160;&#21270;&#27979;&#35797;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20197;&#19968;&#31181;&#36890;&#29992;&#12289;&#21487;&#25193;&#23637;&#21644;&#20934;&#30830;&#30340;&#26041;&#24335;&#35299;&#20915;&#27979;&#35797;&#31070;&#35861;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26469;&#23398;&#20064;&#27979;&#35797;&#25191;&#34892;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#23545;&#25191;&#34892;&#36712;&#36857;&#30340;&#23569;&#37096;&#20998;&#36827;&#34892;&#26631;&#35760;&#65292;&#20197;&#20854;&#36890;&#36807;&#19982;&#22833;&#36133;&#30340;&#32467;&#35770;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#26631;&#35760;&#30340;&#36712;&#36857;&#26469;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#21306;&#20998;&#32473;&#23450;&#31243;&#24207;&#30340;&#36890;&#36807;&#19982;&#22833;&#36133;&#25191;&#34892;&#30340;&#36816;&#34892;&#26102;&#27169;&#24335;&#12290;&#25105;&#20204;&#26500;&#24314;&#36825;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#28041;&#21450;&#20197;&#19979;&#27493;&#39588;&#65292;1. &#20026;&#31243;&#24207;&#23433;&#35013;&#35760;&#24405;&#25191;&#34892;&#36712;&#36857;&#65292;&#20316;&#20026;&#26041;&#27861;&#35843;&#29992;&#21644;&#20840;&#23616;&#29366;&#24577;&#30340;&#24207;&#21015;&#65292;2. &#23545;&#25191;&#34892;&#36712;&#36857;&#30340;&#23569;&#37096;&#20998;&#36827;&#34892;&#26631;&#35760;&#65292;3. &#35774;&#35745;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#29992;&#20110;&#23558;&#25191;&#34892;&#36712;&#36857;&#20013;&#30340;&#20449;&#24687;&#23884;&#20837;&#21040;&#22266;&#23450;&#38271;&#24230;&#30340;&#21521;&#37327;&#20013;&#65292;4. &#35774;&#35745;&#19968;&#20010;&#20351;&#29992;&#36712;&#36857;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge of automatically determining the correctness of test executions is referred to as the test oracle problem and is one of the key remaining issues for automated testing. The goal in this paper is to solve the test oracle problem in a way that is general, scalable and accurate. To achieve this, we use supervised learning over test execution traces. We label a small fraction of the execution traces with their verdict of pass or fail. We use the labelled traces to train a neural network (NN) model to learn to distinguish runtime patterns for passing versus failing executions for a given program. Our approach for building this NN model involves the following steps, 1. Instrument the program to record execution traces as sequences of method invocations and global state, 2. Label a small fraction of the execution traces with their verdicts, 3. Designing a NN component that embeds information in execution traces to fixed length vectors, 4. Design a NN model that uses the trace inf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#32463;&#20856;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#25511;&#21046;&#25216;&#26415;&#21644;&#26368;&#36817;&#22312;&#20248;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23637;&#31034;&#20102;&#22312;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#25511;&#21046;&#21644;&#33258;&#36866;&#24212;&#21160;&#24577;&#39044;&#27979;&#30340;&#31639;&#27861;&#21457;&#23637;&#20013;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#21463;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#21644;&#38236;&#20687;&#19979;&#38477;&#21551;&#21457;&#30340;&#19968;&#38454;&#33258;&#36866;&#24212;&#27861;&#21017;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#27861;&#21017;&#22312;&#23384;&#22312;&#22810;&#31181;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#21160;&#24577;&#26102;&#38544;&#24335;&#27491;&#21017;&#21270;&#20102;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/1912.13154</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#21442;&#25968;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#39044;&#27979;&#20013;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#21644;&#21160;&#37327;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Implicit Regularization and Momentum Algorithms in Nonlinearly Parameterized Adaptive Control and Prediction. (arXiv:1912.13154v7 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.13154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#32463;&#20856;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#25511;&#21046;&#25216;&#26415;&#21644;&#26368;&#36817;&#22312;&#20248;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23637;&#31034;&#20102;&#22312;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#25511;&#21046;&#21644;&#33258;&#36866;&#24212;&#21160;&#24577;&#39044;&#27979;&#30340;&#31639;&#27861;&#21457;&#23637;&#20013;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#21463;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#21644;&#38236;&#20687;&#19979;&#38477;&#21551;&#21457;&#30340;&#19968;&#38454;&#33258;&#36866;&#24212;&#27861;&#21017;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#27861;&#21017;&#22312;&#23384;&#22312;&#22810;&#31181;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#21160;&#24577;&#26102;&#38544;&#24335;&#27491;&#21017;&#21270;&#20102;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#30340;&#24182;&#21457;&#23398;&#20064;&#21644;&#25511;&#21046;&#21160;&#24577;&#31995;&#32479;&#26159;&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#20027;&#39064;&#12290;&#23613;&#31649;&#33258;&#36866;&#24212;&#25511;&#21046;&#26159;&#19968;&#20010;&#24050;&#32463;&#24314;&#31435;&#36215;&#26469;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#21644;&#20016;&#23500;&#29702;&#35770;&#30340;&#39046;&#22495;&#65292;&#20294;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#33258;&#36866;&#24212;&#21160;&#24577;&#39044;&#27979;&#30340;&#21457;&#23637;&#20027;&#35201;&#22260;&#32469;&#30528;&#19968;&#20123;&#20851;&#38190;&#31639;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#32463;&#20856;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#25511;&#21046;&#25216;&#26415;&#21644;&#26368;&#36817;&#22312;&#20248;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#20043;&#38388;&#30340;&#23494;&#20999;&#32852;&#31995;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#25511;&#21046;&#21644;&#33258;&#36866;&#24212;&#21160;&#24577;&#39044;&#27979;&#30340;&#31639;&#27861;&#21457;&#23637;&#20013;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#21463;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#21644;&#38236;&#20687;&#19979;&#38477;&#21551;&#21457;&#30340;&#19968;&#38454;&#33258;&#36866;&#24212;&#27861;&#21017;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#23384;&#22312;&#22810;&#31181;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#21160;&#24577;&#26102;&#65292;&#36825;&#20123;&#38750;&#27431;&#20960;&#37324;&#24503;&#33258;&#36866;&#24212;&#27861;&#21017;&#38544;&#24335;&#27491;&#21017;&#21270;&#20102;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26045;&#21152;&#30340;&#23616;&#37096;&#20960;&#20309;&#24615;&#36136;&#21487;&#20197;&#29992;&#26469;&#36873;&#25321;&#21442;&#25968;&#21521;&#37327; - &#22312;&#21487;&#33021;&#23454;&#29616;&#23436;&#32654;&#36319;&#36394;&#25110;&#39044;&#27979;&#30340;&#35768;&#22810;&#21442;&#25968;&#21521;&#37327;&#20013;&#36873;&#25321;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stable concurrent learning and control of dynamical systems is the subject of adaptive control. Despite being an established field with many practical applications and a rich theory, much of the development in adaptive control for nonlinear systems revolves around a few key algorithms. By exploiting strong connections between classical adaptive nonlinear control techniques and recent progress in optimization and machine learning, we show that there exists considerable untapped potential in algorithm development for both adaptive nonlinear control and adaptive dynamics prediction. We begin by introducing first-order adaptation laws inspired by natural gradient descent and mirror descent. We prove that when there are multiple dynamics consistent with the data, these non-Euclidean adaptation laws implicitly regularize the learned model. Local geometry imposed during learning thus may be used to select parameter vectors -- out of the many that will achieve perfect tracking or prediction --
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#26679;&#25277;&#26679;&#30340;CNN&#23376;&#37319;&#26679;&#25216;&#26415;&#65292;&#36890;&#36807;&#23376;&#37319;&#26679;&#23618;&#26174;&#33879;&#22686;&#21152;&#29305;&#24449;&#22270;&#20445;&#30041;&#30340;&#20449;&#24687;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#31895;&#31961;&#30340;&#29305;&#24449;&#22270;&#26159;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#24615;&#33021;&#30340;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/1805.10766</link><description>&lt;p&gt;
&#29992;&#22810;&#26679;&#25277;&#26679;&#26377;&#25928;&#22320;&#25552;&#39640;CNN&#29305;&#24449;&#22270;&#30340;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improving the Resolution of CNN Feature Maps Efficiently with Multisampling. (arXiv:1805.10766v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1805.10766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#26679;&#25277;&#26679;&#30340;CNN&#23376;&#37319;&#26679;&#25216;&#26415;&#65292;&#36890;&#36807;&#23376;&#37319;&#26679;&#23618;&#26174;&#33879;&#22686;&#21152;&#29305;&#24449;&#22270;&#20445;&#30041;&#30340;&#20449;&#24687;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#31895;&#31961;&#30340;&#29305;&#24449;&#22270;&#26159;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#24615;&#33021;&#30340;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#26032;&#30340;CNN&#23376;&#37319;&#26679;&#25216;&#26415;&#31867;&#21035;&#65292;&#31216;&#20026;&#22810;&#26679;&#25277;&#26679;&#65292;&#36890;&#36807;&#23376;&#37319;&#26679;&#23618;&#22823;&#22823;&#22686;&#21152;&#20102;&#29305;&#24449;&#22270;&#20445;&#30041;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#20854;&#20013;&#19968;&#20010;&#26041;&#27861;&#31216;&#20026;&#26041;&#26684;&#23376;&#37319;&#26679;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#26550;&#26500;&#65288;&#22914;DenseNet&#21644;ResNet&#65289;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#21442;&#25968;&#65292;&#24182;&#19988;&#38750;&#24120;&#26174;&#33879;&#22320;&#25552;&#39640;&#20102;&#26576;&#20123;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;ImageNet&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#26080;&#38656;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#24615;&#36136;&#26377;&#20102;&#19968;&#20123;&#21487;&#33021;&#30340;&#35266;&#23519;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#31895;&#31961;&#30340;&#29305;&#24449;&#22270;&#26159;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#24615;&#33021;&#30340;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a new class of subsampling techniques for CNNs, termed multisampling, that significantly increases the amount of information kept by feature maps through subsampling layers. One version of our method, which we call checkered subsampling, significantly improves the accuracy of state-of-the-art architectures such as DenseNet and ResNet without any additional parameters and, remarkably, improves the accuracy of certain pretrained ImageNet models without any training or fine-tuning. We glean possible insight into the nature of data augmentations and demonstrate experimentally that coarse feature maps are bottlenecking the performance of neural networks in image classification.
&lt;/p&gt;</description></item></channel></rss>