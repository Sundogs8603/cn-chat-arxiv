<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;FP8&#33258;&#21160;&#28151;&#21512;&#31934;&#24230;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.18313</link><description>&lt;p&gt;
FP8-LM&#65306;&#35757;&#32451;FP8&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FP8-LM: Training FP8 Large Language Models. (arXiv:2310.18313v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;FP8&#33258;&#21160;&#28151;&#21512;&#31934;&#24230;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;FP8&#20302;&#27604;&#29305;&#25968;&#25454;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#22312;LLM&#35757;&#32451;&#20013;&#65292;&#22823;&#22810;&#25968;&#21464;&#37327;&#65288;&#22914;&#26799;&#24230;&#21644;&#20248;&#21270;&#22120;&#29366;&#24577;&#65289;&#21487;&#20197;&#20351;&#29992;&#20302;&#31934;&#24230;&#25968;&#25454;&#26684;&#24335;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#25913;&#21464;&#36229;&#21442;&#25968;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FP8&#33258;&#21160;&#28151;&#21512;&#31934;&#24230;&#26694;&#26550;&#29992;&#20110;&#35757;&#32451;LLMs&#12290;&#35813;&#26694;&#26550;&#20026;LLM&#30340;&#28151;&#21512;&#31934;&#24230;&#21644;&#20998;&#24067;&#24335;&#24182;&#34892;&#35757;&#32451;&#25552;&#20379;&#20102;&#19977;&#20010;&#32423;&#21035;&#30340;FP8&#21033;&#29992;&#12290;&#23427;&#36880;&#27493;&#24341;&#20837;8&#20301;&#26799;&#24230;&#65292;&#20248;&#21270;&#22120;&#29366;&#24577;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;H100 GPU&#24179;&#21488;&#19978;&#35757;&#32451;GPT-175B&#27169;&#22411;&#26399;&#38388;&#65292;&#25105;&#20204;&#30340;FP8&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#26694;&#26550;&#19981;&#20165;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;42%&#30340;&#30495;&#23454;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#65292;&#32780;&#19988;&#27604;&#24191;&#27867;&#37319;&#29992;&#30340;BF16&#26694;&#26550;&#65288;&#21363;Megatron-LM&#65289;&#36816;&#34892;&#36895;&#24230;&#24555;64%&#65292;&#27604;Nvidia Transformer Engine&#24555;17%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 42% reduction in real memory usage but also ran 64% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 17%. This l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Gen2Sim&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;3D&#36164;&#20135;&#12289;&#20219;&#21153;&#25551;&#36848;&#12289;&#20219;&#21153;&#20998;&#35299;&#21644;&#22870;&#21169;&#20989;&#25968;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#30340;&#25216;&#33021;&#23398;&#20064;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#27969;&#31243;&#26377;&#21161;&#20110;&#35299;&#20915;&#20154;&#20026;&#21442;&#19982;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#23398;&#20064;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#20013;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.18308</link><description>&lt;p&gt;
Gen2Sim&#65306;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#25193;&#23637;&#26426;&#22120;&#20154;&#23398;&#20064;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models. (arXiv:2310.18308v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Gen2Sim&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;3D&#36164;&#20135;&#12289;&#20219;&#21153;&#25551;&#36848;&#12289;&#20219;&#21153;&#20998;&#35299;&#21644;&#22870;&#21169;&#20989;&#25968;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#30340;&#25216;&#33021;&#23398;&#20064;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#27969;&#31243;&#26377;&#21161;&#20110;&#35299;&#20915;&#20154;&#20026;&#21442;&#19982;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#23398;&#20064;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#20013;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#38656;&#35201;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#23398;&#20064;&#21508;&#31181;&#25805;&#32437;&#25216;&#33021;&#12290;&#30446;&#21069;&#30340;&#26426;&#22120;&#20154;&#35757;&#32451;&#27969;&#31243;&#20381;&#36182;&#20110;&#20154;&#31867;&#25552;&#20379;&#36816;&#21160;&#31034;&#33539;&#25110;&#32534;&#31243;&#20223;&#30495;&#29615;&#22659;&#24182;&#20026;&#24378;&#21270;&#23398;&#20064;&#32534;&#20889;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#31181;&#20154;&#20026;&#21442;&#19982;&#26159;&#25193;&#23637;&#26426;&#22120;&#20154;&#23398;&#20064;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#30340;&#37325;&#35201;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Generation to Simulation&#65288;Gen2Sim&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#35821;&#35328;&#21644;&#35270;&#35273;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#26469;&#25193;&#23637;&#20223;&#30495;&#20013;&#26426;&#22120;&#20154;&#25216;&#33021;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#23558;&#24320;&#25918;&#19990;&#30028;&#30340;&#20108;&#32500;&#29289;&#20307;&#20013;&#24515;&#22270;&#20687;&#25552;&#21319;&#21040;&#19977;&#32500;&#65292;&#24182;&#26597;&#35810;LLMs&#26469;&#30830;&#23450;&#21512;&#29702;&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#29983;&#25104;&#29992;&#20110;&#20223;&#30495;&#30340;&#19977;&#32500;&#36164;&#20135;&#12290;&#32473;&#23450;&#29983;&#25104;&#21644;&#20154;&#31867;&#24320;&#21457;&#30340;&#36164;&#20135;&#30340;URDF&#25991;&#20214;&#65292;&#25105;&#20204;&#20351;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;LLMs&#23558;&#36825;&#20123;&#26144;&#23556;&#21040;&#30456;&#20851;&#30340;&#20219;&#21153;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalist robot manipulators need to learn a wide variety of manipulation skills across diverse environments. Current robot training pipelines rely on humans to provide kinesthetic demonstrations or to program simulation environments and to code up reward functions for reinforcement learning. Such human involvement is an important bottleneck towards scaling up robot learning across diverse tasks and environments. We propose Generation to Simulation (Gen2Sim), a method for scaling up robot skill learning in simulation by automating generation of 3D assets, task descriptions, task decompositions and reward functions using large pre-trained generative models of language and vision. We generate 3D assets for simulation by lifting open-world 2D object-centric images to 3D using image diffusion models and querying LLMs to determine plausible physics parameters. Given URDF files of generated and human-developed assets, we chain-of-thought prompt LLMs to map these to relevant task description
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#26469;&#25913;&#21892;&#20809;&#35889;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.18306</link><description>&lt;p&gt;
&#30417;&#30563;&#21644;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Supervised and Penalized Baseline Correction. (arXiv:2310.18306v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#26469;&#25913;&#21892;&#20809;&#35889;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#35889;&#27979;&#37327;&#21487;&#20197;&#26174;&#31034;&#30001;&#21560;&#25910;&#21644;&#25955;&#23556;&#25104;&#20998;&#28151;&#21512;&#24341;&#36215;&#30340;&#25197;&#26354;&#20809;&#35889;&#24418;&#29366;&#12290;&#36825;&#20123;&#25197;&#26354;&#65288;&#25110;&#22522;&#32447;&#65289;&#36890;&#24120;&#34920;&#29616;&#20026;&#38750;&#24658;&#23450;&#20559;&#31227;&#25110;&#20302;&#39057;&#25391;&#33633;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#22522;&#32447;&#21487;&#33021;&#23545;&#20998;&#26512;&#21644;&#23450;&#37327;&#32467;&#26524;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22522;&#32447;&#26657;&#27491;&#26159;&#19968;&#20010;&#28085;&#30422;&#20102;&#39044;&#22788;&#29702;&#26041;&#27861;&#30340;&#24635;&#31216;&#65292;&#36890;&#36807;&#33719;&#21462;&#22522;&#32447;&#20809;&#35889;&#65288;&#19981;&#38656;&#35201;&#30340;&#25197;&#26354;&#65289;&#24182;&#36890;&#36807;&#24046;&#24322;&#21270;&#21435;&#38500;&#25197;&#26354;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#21363;&#20351;&#21487;&#29992;&#20998;&#26512;&#29289;&#27987;&#24230;&#25110;&#32773;&#23427;&#20204;&#23545;&#35266;&#23519;&#21040;&#30340;&#20809;&#35889;&#21464;&#24322;&#26377;&#37325;&#35201;&#36129;&#29486;&#65292;&#20063;&#27809;&#26377;&#21033;&#29992;&#23427;&#20204;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#65289;&#24182;&#23545;&#20854;&#36827;&#34892;&#20462;&#25913;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#23558;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#24615;&#33021;&#65292;&#21253;&#25324;&#32463;&#20856;&#21463;&#32602;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectroscopic measurements can show distorted spectra shapes arising from a mixture of absorbing and scattering contributions. These distortions (or baselines) often manifest themselves as non-constant offsets or low-frequency oscillations. As a result, these baselines can adversely affect analytical and quantitative results. Baseline correction is an umbrella term where one applies pre-processing methods to obtain baseline spectra (the unwanted distortions) and then remove the distortions by differencing. However, current state-of-the art baseline correction methods do not utilize analyte concentrations even if they are available, or even if they contribute significantly to the observed spectral variability. We examine a class of state-of-the-art methods (penalized baseline correction) and modify them such that they can accommodate a priori analyte concentration such that prediction can be enhanced. Performance will be access on two near infra-red data sets across both classical penal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.18304</link><description>&lt;p&gt;
&#23398;&#20064;&#38750;&#31283;&#24577;&#26465;&#20214;&#19979;&#30340;&#31283;&#23450;&#24615;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Stability Principle for Learning under Non-Stationarity. (arXiv:2310.18304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#26469;&#36873;&#25321;&#19968;&#20010;&#22238;&#28335;&#31383;&#21475;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#23558;&#32047;&#31215;&#20559;&#24046;&#20445;&#25345;&#22312;&#19982;&#38543;&#26426;&#35823;&#24046;&#30456;&#23545;&#21487;&#25509;&#21463;&#30340;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#26410;&#30693;&#38750;&#31283;&#23450;&#24615;&#30340;&#36866;&#24212;&#24615;&#12290;&#24403;&#20154;&#21475;&#25439;&#22833;&#20989;&#25968;&#24378;&#20984;&#25110;&#20165;&#28385;&#36275;Lipschitz&#26465;&#20214;&#26102;&#65292;&#36951;&#25022;&#30028;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#65292;&#20165;&#21463;&#23545;&#25968;&#22240;&#23376;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26680;&#24515;&#26159;&#20004;&#20010;&#26032;&#39062;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#20989;&#25968;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#23558;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#20026;&#20934;&#31283;&#24577;&#29255;&#27573;&#30340;&#20998;&#21106;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a versatile framework for statistical learning in non-stationary environments. In each time period, our approach applies a stability principle to select a look-back window that maximizes the utilization of historical data while keeping the cumulative bias within an acceptable range relative to the stochastic error. Our theory showcases the adaptability of this approach to unknown non-stationarity. The regret bound is minimax optimal up to logarithmic factors when the population losses are strongly convex, or Lipschitz only. At the heart of our analysis lie two novel components: a measure of similarity between functions and a segmentation technique for dividing the non-stationary data sequence into quasi-stationary pieces.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21487;&#35843;&#30340;&#20998;&#31867;&#25439;&#22833;&#35299;&#20915;&#20102;GAN&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;CPE&#25439;&#22833;GAN&#21644;&#26368;&#23567;&#21270;f-&#25955;&#24230;&#30340;f-GAN&#20043;&#38388;&#30340;&#21452;&#21521;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.18291</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#35843;&#30340;&#20998;&#31867;&#25439;&#22833;&#35299;&#20915; GAN &#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing GAN Training Instabilities via Tunable Classification Losses. (arXiv:2310.18291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21487;&#35843;&#30340;&#20998;&#31867;&#25439;&#22833;&#35299;&#20915;&#20102;GAN&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;CPE&#25439;&#22833;GAN&#21644;&#26368;&#23567;&#21270;f-&#25955;&#24230;&#30340;f-GAN&#20043;&#38388;&#30340;&#21452;&#21521;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20316;&#20026;&#29983;&#25104;&#22120;&#65288;G&#65289;&#21644;&#37492;&#21035;&#22120;&#65288;D&#65289;&#20043;&#38388;&#30340;&#38646;&#21644;&#21338;&#24328;&#27169;&#22411;&#65292;&#20801;&#35768;&#20197;&#24418;&#24335;&#20445;&#35777;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31867;&#27010;&#29575;&#20272;&#35745;&#65288;CPE&#65289;&#25439;&#22833;&#26469;&#37325;&#26032;&#23450;&#20041;GAN&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CPE&#25439;&#22833;GAN&#21644;&#26368;&#23567;&#21270;f-&#25955;&#24230;&#30340;f-GAN&#20043;&#38388;&#23384;&#22312;&#21452;&#21521;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25152;&#26377;&#23545;&#31216;f-&#25955;&#24230;&#22312;&#25910;&#25947;&#24615;&#19978;&#31561;&#20215;&#12290;&#22312;&#26377;&#38480;&#26679;&#26412;&#21644;&#27169;&#22411;&#23481;&#37327;&#35774;&#23450;&#19979;&#65292;&#25105;&#20204;&#23450;&#20041;&#24182;&#33719;&#24471;&#20102;&#20851;&#20110;&#20272;&#35745;&#21644;&#27867;&#21270;&#35823;&#24046;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#29305;&#21270;&#21040;&#20351;&#29992;&#945;-loss&#23450;&#20041;&#30340;&#945;-GAN&#65292;&#23427;&#26159;&#19968;&#20010;&#30001;&#945;&#65288;0&#65292;&#8734;]&#21442;&#25968;&#21270;&#30340;&#21487;&#35843;CPE&#25439;&#22833;&#30340;&#23478;&#26063;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31867;&#21452;&#30446;&#26631;GAN&#26469;&#35299;&#20915;GAN&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#945;-loss&#26469;&#23545;&#27599;&#20010;&#29609;&#23478;&#30340;&#30446;&#26631;&#24314;&#27169;&#65292;&#20197;&#33719;&#24471;(&#945;_D&#65292;&#945;_G)-GAN&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20135;&#29983;&#30340;&#38750;&#38646;&#21644;&#28216;&#25103;&#31616;&#21270;&#20026;&#26368;&#23567;&#21270;f-&#25955;&#24230;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs), modeled as a zero-sum game between a generator (G) and a discriminator (D), allow generating synthetic data with formal guarantees. Noting that D is a classifier, we begin by reformulating the GAN value function using class probability estimation (CPE) losses. We prove a two-way correspondence between CPE loss GANs and $f$-GANs which minimize $f$-divergences. We also show that all symmetric $f$-divergences are equivalent in convergence. In the finite sample and model capacity setting, we define and obtain bounds on estimation and generalization errors. We specialize these results to $\alpha$-GANs, defined using $\alpha$-loss, a tunable CPE loss family parametrized by $\alpha\in(0,\infty]$. We next introduce a class of dual-objective GANs to address training instabilities of GANs by modeling each player's objective using $\alpha$-loss to obtain $(\alpha_D,\alpha_G)$-GANs. We show that the resulting non-zero sum game simplifies to minimizing an $f$
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21152;&#36895;&#23547;&#25214;&#24378;&#24230;&#21644;&#21487;&#25345;&#32493;&#24615;&#28151;&#20957;&#22303;&#37197;&#26041;&#65292;&#36890;&#36807;&#30456;&#23545;&#36739;&#23569;&#30340;&#27979;&#37327;&#26377;&#25928;&#39044;&#27979;&#28151;&#20957;&#22303;&#24378;&#24230;&#65292;&#24182;&#23558;&#23547;&#25214;&#21487;&#25345;&#32493;&#28151;&#20957;&#22303;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18288</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#23454;&#29616;&#21487;&#25345;&#32493;&#28151;&#20957;&#22303;
&lt;/p&gt;
&lt;p&gt;
Sustainable Concrete via Bayesian Optimization. (arXiv:2310.18288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18288
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21152;&#36895;&#23547;&#25214;&#24378;&#24230;&#21644;&#21487;&#25345;&#32493;&#24615;&#28151;&#20957;&#22303;&#37197;&#26041;&#65292;&#36890;&#36807;&#30456;&#23545;&#36739;&#23569;&#30340;&#27979;&#37327;&#26377;&#25928;&#39044;&#27979;&#28151;&#20957;&#22303;&#24378;&#24230;&#65292;&#24182;&#23558;&#23547;&#25214;&#21487;&#25345;&#32493;&#28151;&#20957;&#22303;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#20843;&#20998;&#20043;&#19968;&#30340;&#20108;&#27687;&#21270;&#30899;&#25490;&#25918;&#21487;&#20197;&#24402;&#22240;&#20110;&#27700;&#27877;&#30340;&#29983;&#20135;&#65292;&#27700;&#27877;&#26159;&#28151;&#20957;&#22303;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20063;&#26159;&#25968;&#25454;&#20013;&#24515;&#24314;&#35774;&#20013;&#20108;&#27687;&#21270;&#30899;&#25490;&#25918;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20302;&#30899;&#28151;&#20957;&#22303;&#37197;&#26041;&#23545;&#21487;&#25345;&#32493;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23581;&#35797;&#26032;&#30340;&#28151;&#20957;&#22303;&#37197;&#26041;&#38750;&#24120;&#32791;&#26102;&#21644;&#21171;&#21160;&#23494;&#38598;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#31561;&#24453;&#35760;&#24405;&#28151;&#20957;&#22303;&#30340;28&#22825;&#25239;&#21387;&#24378;&#24230;&#65292;&#32780;&#36825;&#20010;&#37327;&#30340;&#27979;&#37327;&#26080;&#27861;&#21152;&#36895;&#12290;&#22240;&#27492;&#65292;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#22914;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#21487;&#20197;&#21152;&#36895;&#23547;&#25214;&#24378;&#24230;&#21644;&#21487;&#25345;&#32493;&#24615;&#28151;&#20957;&#22303;&#37197;&#26041;&#30340;&#25628;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#27493;&#39588;&#65292;&#36890;&#36807;&#30456;&#23545;&#36739;&#23569;&#30340;&#27979;&#37327;&#65292;&#20351;&#28151;&#20957;&#22303;&#24378;&#24230;&#36866;&#21512;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#65307;&#23558;&#23547;&#25214;&#21487;&#25345;&#32493;&#28151;&#20957;&#22303;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65307;&#24182;&#21033;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Eight percent of global carbon dioxide emissions can be attributed to the production of cement, the main component of concrete, which is also the dominant source of CO2 emissions in the construction of data centers. The discovery of lower-carbon concrete formulae is therefore of high significance for sustainability. However, experimenting with new concrete formulae is time consuming and labor intensive, as one usually has to wait to record the concrete's 28-day compressive strength, a quantity whose measurement can by its definition not be accelerated. This provides an opportunity for experimental design methodology like Bayesian Optimization (BO) to accelerate the search for strong and sustainable concrete formulae. Herein, we 1) propose modeling steps that make concrete strength amenable to be predicted accurately by a Gaussian process model with relatively few measurements, 2) formulate the search for sustainable concrete as a multi-objective optimization problem, and 3) leverage th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ESCFR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#26368;&#20248;&#36816;&#36755;&#22312;&#22240;&#26524;&#24615;&#32972;&#26223;&#19979;&#35299;&#20915;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20013;&#30340;&#23567;&#25209;&#37327;&#37319;&#26679;&#25928;&#24212;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#21464;&#37327;&#25928;&#24212;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18286</link><description>&lt;p&gt;
&#29992;&#20110;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#26368;&#20248;&#36816;&#36755;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Treatment Effect Estimation. (arXiv:2310.18286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ESCFR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#26368;&#20248;&#36816;&#36755;&#22312;&#22240;&#26524;&#24615;&#32972;&#26223;&#19979;&#35299;&#20915;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20013;&#30340;&#23567;&#25209;&#37327;&#37319;&#26679;&#25928;&#24212;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#21464;&#37327;&#25928;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23384;&#22312;&#27835;&#30103;&#36873;&#25321;&#20559;&#24046;&#65292;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23545;&#19981;&#21516;&#22788;&#29702;&#32452;&#30340;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26410;&#33021;&#35299;&#20915;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#23567;&#25209;&#37327;&#37319;&#26679;&#25928;&#24212;&#65288;MSE&#65289;&#65292;&#21363;&#22312;&#20855;&#26377;&#19981;&#24179;&#34913;&#32467;&#26524;&#21644;&#24322;&#24120;&#20540;&#30340;&#38750;&#29702;&#24819;&#23567;&#25209;&#37327;&#20013;&#23548;&#33268;&#38169;&#35823;&#23545;&#40784;&#65307;&#65288;2&#65289;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#21464;&#37327;&#25928;&#24212;&#65288;UCE&#65289;&#65292;&#21363;&#30001;&#20110;&#24573;&#30053;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#21464;&#37327;&#32780;&#23548;&#33268;&#30340;&#19981;&#20934;&#30830;&#24046;&#24322;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25972;&#20307;&#31354;&#38388;&#21453;&#20107;&#23454;&#22238;&#24402;&#65288;ESCFR&#65289;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#23427;&#26159;&#22312;&#22240;&#26524;&#24615;&#32972;&#26223;&#19979;&#36816;&#29992;&#26368;&#20248;&#36816;&#36755;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22522;&#20110;&#38543;&#26426;&#26368;&#20248;&#36816;&#36755;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26494;&#24347;&#30340;&#20445;&#25345;&#36136;&#37327;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#35299;&#20915;MSE&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22788;&#29702;UCE&#30340;&#36817;&#22240;&#20107;&#23454;&#32467;&#26524;&#27491;&#21017;&#21270;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating conditional average treatment effect from observational data is highly challenging due to the existence of treatment selection bias. Prevalent methods mitigate this issue by aligning distributions of different treatment groups in the latent space. However, there are two critical problems that these methods fail to address: (1) mini-batch sampling effects (MSE), which causes misalignment in non-ideal mini-batches with outcome imbalance and outliers; (2) unobserved confounder effects (UCE), which results in inaccurate discrepancy calculation due to the neglect of unobserved confounders. To tackle these problems, we propose a principled approach named Entire Space CounterFactual Regression (ESCFR), which is a new take on optimal transport in the context of causality. Specifically, based on the framework of stochastic optimal transport, we propose a relaxed mass-preserving regularizer to address the MSE issue and design a proximal factual outcome regularizer to handle the UCE is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#32676;&#20307;&#25552;&#31034;&#23454;&#29616;&#33719;&#21462;&#36890;&#29992;&#30693;&#35782;&#21644;&#20010;&#24615;&#21270;&#30693;&#35782;&#65292;&#20197;&#35757;&#32451;&#36866;&#24212;&#19981;&#21516;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.18285</link><description>&lt;p&gt;
&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#19982;&#32676;&#20307;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Federated Learning with Group-Aware Prompt Tuning. (arXiv:2310.18285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#32676;&#20307;&#25552;&#31034;&#23454;&#29616;&#33719;&#21462;&#36890;&#29992;&#30693;&#35782;&#21644;&#20010;&#24615;&#21270;&#30693;&#35782;&#65292;&#20197;&#35757;&#32451;&#36866;&#24212;&#19981;&#21516;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20419;&#20351;&#23427;&#20204;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#23427;&#20204;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#20855;&#26377;&#19981;&#21516;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#22330;&#26223;&#12290;&#20026;&#20102;&#28385;&#36275;FL&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#38656;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#24341;&#20837;&#20102;&#21516;&#26102;&#23398;&#20064;&#20849;&#20139;&#21644;&#32676;&#20307;&#25552;&#31034;&#30340;&#27010;&#24565;&#65292;&#33021;&#22815;&#21516;&#26102;&#33719;&#21462;&#36890;&#29992;&#30693;&#35782;&#21644;&#32676;&#20307;&#29305;&#23450;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25552;&#31034;&#36873;&#25321;&#27169;&#22359;&#20026;&#27599;&#20010;&#36755;&#20837;&#20998;&#37197;&#20010;&#24615;&#21270;&#30340;&#32676;&#20307;&#25552;&#31034;&#65292;&#20351;&#20840;&#23616;&#27169;&#22411;&#19982;&#27599;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#19981;&#21516;&#30340;&#26412;&#22320;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26412;&#22320;&#24494;&#35843;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25645;&#24314;&#20102;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved remarkable success in various machine-learning tasks, prompting their widespread adoption. In this paper, we explore their application in the context of federated learning (FL), with a particular focus on heterogeneous scenarios where individual clients possess diverse local datasets. To meet the computational and communication demands of FL, we leverage pre-trained Transformers and use an efficient prompt-tuning strategy. Our strategy introduces the concept of learning both shared and group prompts, enabling the acquisition of universal knowledge and group-specific knowledge simultaneously. Additionally, a prompt selection module assigns personalized group prompts to each input, aligning the global model with the data distribution of each client. This approach allows us to train a single global model that can automatically adapt to various local client data distributions without requiring local fine-tuning. In this way, our proposed method effectively bridge
&lt;/p&gt;</description></item><item><title>LipSim&#26159;&#19968;&#20010;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#30693;&#35273;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;1-Lipschitz&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39592;&#24178;&#65292;&#25552;&#20379;&#20102;&#22260;&#32469;&#24230;&#37327;&#26041;&#27861;&#30340;&#38450;&#25252;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.18274</link><description>&lt;p&gt;
LipSim: &#19968;&#31181;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#30693;&#35273;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LipSim: A Provably Robust Perceptual Similarity Metric. (arXiv:2310.18274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18274
&lt;/p&gt;
&lt;p&gt;
LipSim&#26159;&#19968;&#20010;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#30693;&#35273;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;1-Lipschitz&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39592;&#24178;&#65292;&#25552;&#20379;&#20102;&#22260;&#32469;&#24230;&#37327;&#26041;&#27861;&#30340;&#38450;&#25252;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20110;&#24320;&#21457;&#21644;&#24212;&#29992;&#30693;&#35273;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#20687;&#32032;&#32423;&#24230;&#37327;&#26041;&#27861;&#65292;&#30693;&#35273;&#24230;&#37327;&#26041;&#27861;&#22312;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#19968;&#33268;&#24615;&#21644;&#20316;&#20026;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#20195;&#29702;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30693;&#35273;&#24230;&#37327;&#26041;&#27861;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#25915;&#20987;&#33030;&#24369;&#24615;&#30340;&#20851;&#27880;&#20063;&#26085;&#30410;&#22686;&#38271;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;ViT&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#26368;&#26032;&#30693;&#35273;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LipSim&#65288;Lipschitz&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65289;&#30340;&#40065;&#26834;&#30693;&#35273;&#30456;&#20284;&#24230;&#24230;&#37327;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;1-Lipschitz&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39592;&#24178;&#65292;LipSim&#25552;&#20379;&#20102;&#22260;&#32469;&#30528;&#24230;&#37327;&#26041;&#27861;&#30340;&#38450;&#25252;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen growing interest in developing and applying perceptual similarity metrics. Research has shown the superiority of perceptual metrics over pixel-wise metrics in aligning with human perception and serving as a proxy for the human visual system. On the other hand, as perceptual metrics rely on neural networks, there is a growing concern regarding their resilience, given the established vulnerability of neural networks to adversarial attacks. It is indeed logical to infer that perceptual metrics may inherit both the strengths and shortcomings of neural networks. In this work, we demonstrate the vulnerability of state-of-the-art perceptual similarity metrics based on an ensemble of ViT-based feature extractors to adversarial attacks. We then propose a framework to train a robust perceptual similarity metric called LipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging 1-Lipschitz neural networks as the backbone, LipSim provides guarded areas aroun
&lt;/p&gt;</description></item><item><title>PlantPlotGAN&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#29992;&#20110;&#26893;&#29289;&#30149;&#23475;&#39044;&#27979;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#30340;&#22810;&#20809;&#35889;&#21306;&#22495;&#22270;&#20687;&#26469;&#22686;&#21152;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18268</link><description>&lt;p&gt;
PlantPlotGAN:&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#26893;&#29289;&#30149;&#23475;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PlantPlotGAN: A Physics-Informed Generative Adversarial Network for Plant Disease Prediction. (arXiv:2310.18268v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18268
&lt;/p&gt;
&lt;p&gt;
PlantPlotGAN&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#29992;&#20110;&#26893;&#29289;&#30149;&#23475;&#39044;&#27979;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#30340;&#22810;&#20809;&#35889;&#21306;&#22495;&#22270;&#20687;&#26469;&#22686;&#21152;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#26893;&#29289;&#22253;&#26519;&#23545;&#20110;&#20892;&#20316;&#29289;&#31649;&#29702;&#21644;&#20135;&#20986;&#20581;&#24247;&#30340;&#25910;&#25104;&#33267;&#20851;&#37325;&#35201;&#12290;&#26080;&#20154;&#26426;&#24050;&#34987;&#29992;&#20110;&#25910;&#38598;&#22810;&#20809;&#35889;&#22270;&#20687;&#26469;&#36827;&#34892;&#30417;&#27979;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#38656;&#35201;&#30417;&#27979;&#30340;&#38754;&#31215;&#21644;&#39134;&#34892;&#30340;&#38480;&#21046;&#65292;&#26893;&#29289;&#30149;&#23475;&#20449;&#21495;&#21482;&#26377;&#22312;&#26893;&#29289;&#29983;&#38271;&#30340;&#21518;&#26399;&#38454;&#27573;&#20197;&#21450;&#30149;&#23475;&#24050;&#32463;&#25193;&#25955;&#21040;&#26893;&#29289;&#22253;&#26519;&#30340;&#22823;&#37096;&#20998;&#21306;&#22495;&#26102;&#25165;&#20250;&#21464;&#24471;&#26126;&#26174;&#12290;&#36825;&#31181;&#26377;&#38480;&#37327;&#30340;&#30456;&#20851;&#25968;&#25454;&#20351;&#24471;&#39044;&#27979;&#27169;&#22411;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#31639;&#27861;&#38590;&#20197;&#26377;&#25928;&#22320;&#20174;&#19981;&#24179;&#34913;&#25110;&#19981;&#29616;&#23454;&#30340;&#22686;&#24191;&#25968;&#25454;&#38598;&#20013;&#25512;&#24191;&#20986;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PlantPlotGAN&#65292;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#21019;&#24314;&#20855;&#26377;&#30495;&#23454;&#26893;&#34987;&#25351;&#25968;&#30340;&#21512;&#25104;&#22810;&#20809;&#35889;&#21306;&#22495;&#22270;&#20687;&#12290;&#36825;&#20123;&#25351;&#25968;&#20316;&#20026;&#30142;&#30149;&#26816;&#27979;&#30340;&#20195;&#29702;&#65292;&#24182;&#29992;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#24110;&#21161;&#22686;&#21152;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#25104;&#30340;&#22270;&#20687;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring plantations is crucial for crop management and producing healthy harvests. Unmanned Aerial Vehicles (UAVs) have been used to collect multispectral images that aid in this monitoring. However, given the number of hectares to be monitored and the limitations of flight, plant disease signals become visually clear only in the later stages of plant growth and only if the disease has spread throughout a significant portion of the plantation. This limited amount of relevant data hampers the prediction models, as the algorithms struggle to generalize patterns with unbalanced or unrealistic augmented datasets effectively. To address this issue, we propose PlantPlotGAN, a physics-informed generative model capable of creating synthetic multispectral plot images with realistic vegetation indices. These indices served as a proxy for disease detection and were used to evaluate if our model could help increase the accuracy of prediction models. The results demonstrate that the synthetic im
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23547;&#25214;&#36817;&#20284;&#26368;&#20248;&#30340;&#39044;&#22788;&#29702;&#22120;&#26469;&#35299;&#20915;&#32447;&#24615;&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#23545;&#35282;&#39044;&#22788;&#29702;&#32467;&#26524;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#36890;&#29992;&#21322;/shear programming&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18265</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#21322;&#23450;&#35268;&#21010;&#29992;&#20110;&#24674;&#22797;&#32467;&#26500;&#21270;&#39044;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Structured Semidefinite Programming for Recovering Structured Preconditioners. (arXiv:2310.18265v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23547;&#25214;&#36817;&#20284;&#26368;&#20248;&#30340;&#39044;&#22788;&#29702;&#22120;&#26469;&#35299;&#20915;&#32447;&#24615;&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#23545;&#35282;&#39044;&#22788;&#29702;&#32467;&#26524;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#36890;&#29992;&#21322;/shear programming&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23547;&#25214;&#36817;&#20284;&#26368;&#20248;&#30340;&#39044;&#22788;&#29702;&#22120;&#26469;&#35299;&#20915;&#32447;&#24615;&#31995;&#32479;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#22522;&#26412;&#39044;&#22788;&#29702;&#21644;&#32447;&#24615;&#31995;&#32479;&#27714;&#35299;&#38382;&#39064;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#32473;&#23450;&#27491;&#23450;&#30697;&#38453;$\mathbf{K} \in \mathbb{R}^{d \times d}$&#65292;&#20854;&#20013;$\mathrm{nnz}(\mathbf{K})$&#20026;&#38750;&#38646;&#20803;&#32032;&#30340;&#25968;&#37327;&#65292;&#23427;&#21487;&#20197;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;$\widetilde{O}(\mathrm{nnz}(\mathbf{K}) \cdot \mathrm{poly}(\kappa^\star,\epsilon^{-1}))$&#20869;&#35745;&#31639;$\epsilon$-&#26368;&#20248;&#30340;&#23545;&#35282;&#39044;&#22788;&#29702;&#22120;&#65292;&#20854;&#20013;$\kappa^\star$&#26159;&#32553;&#25918;&#21518;&#30697;&#38453;&#30340;&#26368;&#20248;&#26465;&#20214;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#32473;&#23450;$\mathbf{M} \in \mathbb{R}^{d \times d}$&#65292;&#23427;&#26159;&#19968;&#20010;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#20266;&#36870;&#30697;&#38453;&#25110;&#32773;&#19968;&#20010;&#24120;&#25968;&#35889;&#36924;&#36817;&#65292;&#21487;&#20197;&#22312;$\widetilde{O}(d^2)$&#30340;&#26102;&#38388;&#20869;&#27714;&#35299;&#22312;$\mathbf{M}$&#19978;&#30340;&#32447;&#24615;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#23545;&#35282;&#39044;&#22788;&#29702;&#32467;&#26524;&#25913;&#36827;&#20102;&#36890;&#36807;&#36890;&#29992;&#21322;/shear programming&#26041;&#27861;&#33719;&#24471;&#30340;$\Omega(d^{3.5})$&#30340;&#26368;&#20808;&#36827;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a general framework for finding approximately-optimal preconditioners for solving linear systems. Leveraging this framework we obtain improved runtimes for fundamental preconditioning and linear system solving problems including the following. We give an algorithm which, given positive definite $\mathbf{K} \in \mathbb{R}^{d \times d}$ with $\mathrm{nnz}(\mathbf{K})$ nonzero entries, computes an $\epsilon$-optimal diagonal preconditioner in time $\widetilde{O}(\mathrm{nnz}(\mathbf{K}) \cdot \mathrm{poly}(\kappa^\star,\epsilon^{-1}))$, where $\kappa^\star$ is the optimal condition number of the rescaled matrix. We give an algorithm which, given $\mathbf{M} \in \mathbb{R}^{d \times d}$ that is either the pseudoinverse of a graph Laplacian matrix or a constant spectral approximation of one, solves linear systems in $\mathbf{M}$ in $\widetilde{O}(d^2)$ time. Our diagonal preconditioning results improve state-of-the-art runtimes of $\Omega(d^{3.5})$ attained by general-purpose sem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Neural k-Opt&#30340;&#23398;&#20064;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#36335;&#24452;&#38382;&#39064;&#12290;&#36890;&#36807;&#28789;&#27963;&#30340;k-opt&#20132;&#25442;&#21644;&#33258;&#20027;&#30340;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#21306;&#22495;&#25506;&#32034;&#65292;&#35813;&#31639;&#27861;&#22312;&#26053;&#34892;&#21830;&#38382;&#39064;&#21644;&#26377;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18264</link><description>&lt;p&gt;
&#23398;&#20064;&#25628;&#32034;&#20855;&#26377;&#28789;&#27963;&#31070;&#32463;k-Opt&#30340;&#36335;&#24452;&#38382;&#39064;&#30340;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt. (arXiv:2310.18264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Neural k-Opt&#30340;&#23398;&#20064;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#36335;&#24452;&#38382;&#39064;&#12290;&#36890;&#36807;&#28789;&#27963;&#30340;k-opt&#20132;&#25442;&#21644;&#33258;&#20027;&#30340;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#21306;&#22495;&#25506;&#32034;&#65292;&#35813;&#31639;&#27861;&#22312;&#26053;&#34892;&#21830;&#38382;&#39064;&#21644;&#26377;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#36335;&#24452;&#38382;&#39064;&#30340;&#23398;&#20064;&#25628;&#32034;&#65288;L2S&#65289;&#27714;&#35299;&#22120;Neural k-Opt&#65288;NeuOpt&#65289;&#12290;&#23427;&#22522;&#20110;&#23450;&#21046;&#30340;&#21160;&#20316;&#22240;&#23376;&#20998;&#35299;&#26041;&#27861;&#21644;&#23450;&#21046;&#30340;&#21452;&#27969;&#24490;&#29615;&#35299;&#30721;&#22120;&#65292;&#23398;&#20064;&#25191;&#34892;&#28789;&#27963;&#30340;k-opt&#20132;&#25442;&#12290;&#20316;&#20026;&#32469;&#36807;&#32431;&#21487;&#34892;&#24615;&#25513;&#30422;&#26041;&#26696;&#24182;&#23454;&#29616;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#21306;&#22495;&#30340;&#33258;&#20027;&#25506;&#32034;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Guided Infeasible Region Exploration&#65288;GIRE&#65289;&#26041;&#26696;&#65292;&#23427;&#20026;NeuOpt&#31574;&#30053;&#32593;&#32476;&#34917;&#20805;&#20102;&#19982;&#21487;&#34892;&#24615;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#22870;&#21169;&#22609;&#36896;&#26356;&#26377;&#25928;&#22320;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;NeuOpt&#37197;&#22791;&#20102;Dynamic Data Augmentation&#65288;D2A&#65289;&#65292;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36827;&#34892;&#26356;&#22810;&#26679;&#21270;&#30340;&#25628;&#32034;&#12290;&#22312;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#21644;&#26377;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;CVRP&#65289;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;NeuOpt&#19981;&#20165;&#26126;&#26174;&#36229;&#36807;&#29616;&#26377;&#30340;&#65288;&#22522;&#20110;&#25513;&#30721;&#30340;&#65289;L2S&#27714;&#35299;&#22120;&#65292;&#32780;&#19988;&#36824;&#23637;&#31034;&#20102;&#22312;&#23398;&#20064;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Neural k-Opt (NeuOpt), a novel learning-to-search (L2S) solver for routing problems. It learns to perform flexible k-opt exchanges based on a tailored action factorization method and a customized recurrent dual-stream decoder. As a pioneering work to circumvent the pure feasibility masking scheme and enable the autonomous exploration of both feasible and infeasible regions, we then propose the Guided Infeasible Region Exploration (GIRE) scheme, which supplements the NeuOpt policy network with feasibility-related features and leverages reward shaping to steer reinforcement learning more effectively. Additionally, we equip NeuOpt with Dynamic Data Augmentation (D2A) for more diverse searches during inference. Extensive experiments on the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that our NeuOpt not only significantly outstrips existing (masking-based) L2S solvers, but also showcases superiority over the learning-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MIM-GAN&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21010;&#20998;&#20026;&#23376;&#24207;&#21015;&#65292;&#21033;&#29992;LSTM&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#25429;&#25417;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#25351;&#25968;&#20449;&#24687;&#24230;&#37327;&#26469;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#35299;&#21644;&#27169;&#22411;&#23849;&#28291;&#12290;</title><link>http://arxiv.org/abs/2310.18257</link><description>&lt;p&gt;
&#22522;&#20110;MIM-GAN&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MIM-GAN-based Anomaly Detection for Multivariate Time Series Data. (arXiv:2310.18257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MIM-GAN&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21010;&#20998;&#20026;&#23376;&#24207;&#21015;&#65292;&#21033;&#29992;LSTM&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#25429;&#25417;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#25351;&#25968;&#20449;&#24687;&#24230;&#37327;&#26469;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#35299;&#21644;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#24433;&#21709;&#24322;&#24120;&#26816;&#27979;&#29983;&#25104;&#26679;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MIM-GAN&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21010;&#20998;&#20026;&#23376;&#24207;&#21015;&#65292;&#28982;&#21518;&#21033;&#29992;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#21644;&#27169;&#22411;&#23849;&#28291;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25351;&#25968;&#20449;&#24687;&#24230;&#37327;&#21040;GAN&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#12290;&#27492;&#22806;&#65292;&#36824;&#32771;&#34385;&#20102;&#19968;&#20010;&#30001;&#21028;&#21035;&#24615;&#21644;&#37325;&#26500;&#25439;&#22833;&#32452;&#25104;&#30340;&#21028;&#21035;&#37325;&#26500;&#24471;&#20998;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;MIM-GAN-based&#24322;&#24120;&#26816;&#27979;&#36991;&#20813;&#20102;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
The loss function of Generative adversarial network(GAN) is an important factor that affects the quality and diversity of the generated samples for anomaly detection. In this paper, we propose an unsupervised multiple time series anomaly detection algorithm based on the GAN with message importance measure(MIM-GAN). In particular, the time series data is divided into subsequences using a sliding window. Then a generator and a discriminator designed based on the Long Short-Term Memory (LSTM) are employed to capture the temporal correlations of the time series data. To avoid the local optimal solution of loss function and the model collapse, we introduce an exponential information measure into the loss function of GAN. Additionally, a discriminant reconstruction score consisting on discrimination and reconstruction loss is taken into account. The global optimal solution for the loss function is derived and the model collapse is proved to be avoided in our proposed MIM-GAN-based anomaly de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;GuDA&#65289;&#29992;&#20110;&#25552;&#39640;&#28436;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18247</link><description>&lt;p&gt;
&#20026;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#25552;&#20379;&#25351;&#23548;&#24615;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Guided Data Augmentation for Offline Reinforcement Learning and Imitation Learning. (arXiv:2310.18247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18247
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;GuDA&#65289;&#29992;&#20110;&#25552;&#39640;&#28436;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#23398;&#20064;&#26159;&#19968;&#31181;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#19987;&#23478;&#32423;&#28436;&#31034;&#30340;&#38590;&#24230;&#38480;&#21046;&#20102;&#28436;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65306;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#25910;&#38598;&#36890;&#24120;&#24456;&#26114;&#36149;&#65292;&#24182;&#19988;&#28436;&#31034;&#30340;&#36136;&#37327;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#28436;&#31034;&#32773;&#30340;&#33021;&#21147;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#19968;&#20123;&#24037;&#20316;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#24265;&#20215;&#29983;&#25104;&#39069;&#22806;&#30340;&#28436;&#31034;&#25968;&#25454;&#65292;&#20294;&#22823;&#22810;&#25968;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20197;&#38543;&#26426;&#26041;&#24335;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#26368;&#32456;&#20135;&#29983;&#39640;&#24230;&#27425;&#20248;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;GuDA&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22686;&#24378;&#25968;&#25454;&#12290;GuDA&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#65292;&#34429;&#28982;&#28436;&#31034;&#21160;&#20316;&#24207;&#21015;&#21487;&#33021;&#24456;&#38590;&#23637;&#31034;&#20135;&#29983;&#19987;&#23478;&#25968;&#25454;&#25152;&#38656;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#20294;&#29992;&#25143;&#32463;&#24120;&#21487;&#20197;&#36731;&#26494;&#22320;&#36776;&#21035;&#20986;&#22686;&#24378;&#36712;&#36857;&#27573;&#34920;&#31034;&#30340;&#20219;&#21153;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#29992;&#25143;&#21487;&#20197;&#26045;&#21152;&#19968;&#31995;&#21015;s
&lt;/p&gt;
&lt;p&gt;
Learning from demonstration (LfD) is a popular technique that uses expert demonstrations to learn robot control policies. However, the difficulty in acquiring expert-quality demonstrations limits the applicability of LfD methods: real-world data collection is often costly, and the quality of the demonstrations depends greatly on the demonstrator's abilities and safety concerns. A number of works have leveraged data augmentation (DA) to inexpensively generate additional demonstration data, but most DA works generate augmented data in a random fashion and ultimately produce highly suboptimal data. In this work, we propose Guided Data Augmentation (GuDA), a human-guided DA framework that generates expert-quality augmented data. The key insight of GuDA is that while it may be difficult to demonstrate the sequence of actions required to produce expert data, a user can often easily identify when an augmented trajectory segment represents task progress. Thus, the user can impose a series of s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35843;&#33410;&#30340;&#38544;&#31169;&#24230;&#37327;$\alpha$-&#20114;&#20449;&#24687;&#65292;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#20849;&#20139;&#20013;&#33021;&#22815;&#29983;&#25104;&#20248;&#36234;&#30340;&#27169;&#22411;&#20197;&#26377;&#25928;&#38459;&#25376;&#25915;&#20987;&#32773;&#65292;&#36890;&#36807;&#25805;&#32437;&#21407;&#22987;&#25968;&#25454;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#36890;&#29992;&#23545;&#25239;&#24615;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#38544;&#31169;&#24230;&#37327;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#39564;&#35777;&#20102;$\alpha$-&#20114;&#20449;&#24687;&#30340;&#21151;&#33021;&#65292;&#24182;&#35780;&#20272;&#20102;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.18241</link><description>&lt;p&gt;
$\alpha$-&#20114;&#20449;&#24687;&#65306;&#19968;&#20010;&#21487;&#35843;&#33410;&#30340;&#38544;&#31169;&#24230;&#37327;&#29992;&#20110;&#25968;&#25454;&#20849;&#20139;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
$\alpha$-Mutual Information: A Tunable Privacy Measure for Privacy Protection in Data Sharing. (arXiv:2310.18241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35843;&#33410;&#30340;&#38544;&#31169;&#24230;&#37327;$\alpha$-&#20114;&#20449;&#24687;&#65292;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#20849;&#20139;&#20013;&#33021;&#22815;&#29983;&#25104;&#20248;&#36234;&#30340;&#27169;&#22411;&#20197;&#26377;&#25928;&#38459;&#25376;&#25915;&#20987;&#32773;&#65292;&#36890;&#36807;&#25805;&#32437;&#21407;&#22987;&#25968;&#25454;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#36890;&#29992;&#23545;&#25239;&#24615;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#38544;&#31169;&#24230;&#37327;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#39564;&#35777;&#20102;$\alpha$-&#20114;&#20449;&#24687;&#30340;&#21151;&#33021;&#65292;&#24182;&#35780;&#20272;&#20102;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;Arimoto&#30340;$\alpha$-&#20114;&#20449;&#24687;&#20316;&#20026;&#21487;&#35843;&#33410;&#30340;&#38544;&#31169;&#24230;&#37327;&#65292;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#21457;&#24067;&#29615;&#22659;&#20013;&#26088;&#22312;&#38450;&#27490;&#21521;&#25915;&#20987;&#32773;&#25259;&#38706;&#31169;&#23494;&#25968;&#25454;&#12290;&#36890;&#36807;&#23545;&#38544;&#31169;&#24230;&#37327;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#22312;&#21508;&#31181;&#24615;&#33021;&#32500;&#24230;&#19978;&#26377;&#25928;&#38459;&#25376;&#25915;&#20987;&#32773;&#30340;&#20248;&#36234;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25197;&#26354;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#36890;&#36807;&#25805;&#32437;&#21407;&#22987;&#25968;&#25454;&#26469;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#12290;&#25197;&#26354;&#24230;&#37327;&#26159;&#26681;&#25454;&#29305;&#23450;&#23454;&#39564;&#30340;&#25968;&#25454;&#32467;&#26500;&#26469;&#30830;&#23450;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#19968;&#20010;&#21253;&#21547;&#21457;&#24067;&#32773;&#21644;&#23545;&#25163;&#30340;&#36890;&#29992;&#23545;&#25239;&#24615;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#34920;&#36798;&#24335;&#20013;&#30340;&#38382;&#39064;&#65292;&#36825;&#20004;&#32773;&#37117;&#26159;&#20197;&#30456;&#21453;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#26412;&#30740;&#31350;&#23545;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;$\alpha$-&#20114;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23450;&#21046;&#27169;&#22411;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#24182;&#23558;&#20854;&#19982;&#20114;&#20449;&#24687;&#20316;&#20026;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper adopts Arimoto's $\alpha$-Mutual Information as a tunable privacy measure, in a privacy-preserving data release setting that aims to prevent disclosing private data to adversaries. By fine-tuning the privacy metric, we demonstrate that our approach yields superior models that effectively thwart attackers across various performance dimensions. We formulate a general distortion-based mechanism that manipulates the original data to offer privacy protection. The distortion metrics are determined according to the data structure of a specific experiment. We confront the problem expressed in the formulation by employing a general adversarial deep learning framework that consists of a releaser and an adversary, trained with opposite goals. This study conducts empirical experiments on images and time-series data to verify the functionality of $\alpha$-Mutual Information. We evaluate the privacy-utility trade-off of customized models and compare them to mutual information as the basel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25913;&#21892;&#38271;&#23614;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#35757;&#32451;&#22270;&#20687;&#19981;&#21253;&#21547;&#26080;&#20851;&#32972;&#26223;&#26102;&#65292;&#37325;&#26032;&#37319;&#26679;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#22312;&#20854;&#20182;&#22330;&#26223;&#19979;&#21487;&#33021;&#20250;&#23548;&#33268;&#23398;&#20064;&#21040;&#19982;&#30446;&#26631;&#26631;&#31614;&#26080;&#20851;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18236</link><description>&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25913;&#21892;&#38271;&#23614;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Re-sampling Helps for Long-Tail Learning?. (arXiv:2310.18236v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25913;&#21892;&#38271;&#23614;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#35757;&#32451;&#22270;&#20687;&#19981;&#21253;&#21547;&#26080;&#20851;&#32972;&#26223;&#26102;&#65292;&#37325;&#26032;&#37319;&#26679;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#22312;&#20854;&#20182;&#22330;&#26223;&#19979;&#21487;&#33021;&#20250;&#23548;&#33268;&#23398;&#20064;&#21040;&#19982;&#30446;&#26631;&#26631;&#31614;&#26080;&#20851;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#26497;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#38271;&#23614;&#23398;&#20064;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#65292;&#21482;&#26377;&#23569;&#25968;&#31867;&#21035;&#65288;&#31216;&#20026;&#22836;&#37096;&#31867;&#21035;&#65289;&#20855;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#20854;&#20313;&#31867;&#21035;&#65288;&#31216;&#20026;&#23614;&#37096;&#31867;&#21035;&#65289;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#24456;&#23569;&#35265;&#12290;&#37325;&#26032;&#37319;&#26679;&#26159;&#19968;&#31181;&#32463;&#20856;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;&#65292;&#22312;&#29616;&#20195;&#38271;&#23614;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#37325;&#26032;&#37319;&#26679;&#23545;&#24615;&#33021;&#30340;&#25552;&#21319;&#24494;&#19981;&#36275;&#36947;&#12290;&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#30740;&#31350;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#35757;&#32451;&#22270;&#20687;&#19981;&#21253;&#21547;&#35821;&#20041;&#19978;&#26080;&#20851;&#30340;&#32972;&#26223;&#26102;&#65292;&#37325;&#26032;&#37319;&#26679;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#20182;&#22330;&#26223;&#19979;&#65292;&#23427;&#21487;&#33021;&#20250;&#23398;&#20064;&#21040;&#19982;&#30446;&#26631;&#26631;&#31614;&#26080;&#20851;&#30340;&#24847;&#22806;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#21516;&#36136;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#19968;&#20010;&#21253;&#21547;&#26080;&#20851;&#32972;&#26223;&#65292;&#21478;&#19968;&#20010;&#19981;&#21253;&#21547;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-tail learning has received significant attention in recent years due to the challenge it poses with extremely imbalanced datasets. In these datasets, only a few classes (known as the head classes) have an adequate number of training samples, while the rest of the classes (known as the tail classes) are infrequent in the training data. Re-sampling is a classical and widely used approach for addressing class imbalance issues. Unfortunately, recent studies claim that re-sampling brings negligible performance improvements in modern long-tail learning tasks. This paper aims to investigate this phenomenon systematically. Our research shows that re-sampling can considerably improve generalization when the training images do not contain semantically irrelevant contexts. In other scenarios, however, it can learn unexpected spurious correlations between irrelevant contexts and target labels. We design experiments on two homogeneous datasets, one containing irrelevant context and the other n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18235</link><description>&lt;p&gt;
Davidsonian&#22330;&#26223;&#22270;&#65306;&#25913;&#36827;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#24544;&#23454;&#24230;&#30340;&#24378;&#22823;&#26041;&#27861;&#26159;&#22522;&#20110;QG/A&#65288;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#65289;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#31572;&#26696;&#19982;&#22522;&#20110;&#25552;&#31034;&#30340;&#31572;&#26696;&#22312;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#19968;&#33268;&#24615;&#23545;&#36755;&#20986;&#22270;&#20687;&#36827;&#34892;&#35780;&#20998;&#12290;&#36825;&#31181;&#35780;&#20272;&#33258;&#28982;&#19978;&#21462;&#20915;&#20110;&#24213;&#23618;QG&#21644;QA&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QG/A&#24037;&#20316;&#20013;&#30340;&#20960;&#20010;&#21487;&#38752;&#24615;&#25361;&#25112;&#65306;&#65288;a&#65289;QG&#38382;&#39064;&#24212;&#23562;&#37325;&#25552;&#31034;&#65288;&#36991;&#20813;&#24187;&#35273;&#12289;&#37325;&#22797;&#21644;&#36951;&#28431;&#65289;&#21644;&#65288;b&#65289;VQA&#31572;&#26696;&#24212;&#19968;&#33268;&#65288;&#19981;&#20250;&#22312;&#22270;&#20687;&#20013;&#23459;&#31216;&#27809;&#26377;&#25705;&#25176;&#36710;&#65292;&#21516;&#26102;&#22768;&#31216;&#25705;&#25176;&#36710;&#26159;&#34013;&#33394;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#65292;&#36825;&#20010;&#21463;&#24418;&#24335;&#35821;&#20041;&#21551;&#21457;&#30340;&#23454;&#35777;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#36716;&#25442;&#39640;&#26031;&#36807;&#31243;&#65288;DTGPs&#65289;&#30340;&#36716;&#25442;&#39640;&#26031;&#36807;&#31243;&#65288;TGPs&#65289;&#30340;&#25512;&#24191;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20018;&#32852;&#23618;&#32423;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;&#30456;&#23545;&#20110;TGPs&#21644;DGPs&#30340;&#28789;&#27963;&#24615;&#22686;&#24378;&#12290;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#25512;&#29702;&#65292;&#21487;&#20197;&#36817;&#20284;&#25152;&#38656;&#30340;&#35745;&#31639;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#31616;&#21333;&#30452;&#25509;&#30340;&#25512;&#29702;&#31639;&#27861;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.18230</link><description>&lt;p&gt;
&#28145;&#24230;&#36716;&#25442;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep Transformed Gaussian Processes. (arXiv:2310.18230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#36716;&#25442;&#39640;&#26031;&#36807;&#31243;&#65288;DTGPs&#65289;&#30340;&#36716;&#25442;&#39640;&#26031;&#36807;&#31243;&#65288;TGPs&#65289;&#30340;&#25512;&#24191;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20018;&#32852;&#23618;&#32423;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;&#30456;&#23545;&#20110;TGPs&#21644;DGPs&#30340;&#28789;&#27963;&#24615;&#22686;&#24378;&#12290;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#25512;&#29702;&#65292;&#21487;&#20197;&#36817;&#20284;&#25152;&#38656;&#30340;&#35745;&#31639;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#31616;&#21333;&#30452;&#25509;&#30340;&#25512;&#29702;&#31639;&#27861;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#25442;&#39640;&#26031;&#36807;&#31243;&#65288;TGPs&#65289;&#26159;&#36890;&#36807;&#20351;&#29992;&#21487;&#36870;&#36716;&#25442;&#20174;&#20808;&#39564;&#36807;&#31243;&#65288;&#36890;&#24120;&#26159;&#39640;&#26031;&#36807;&#31243;&#65289;&#20013;&#36716;&#25442;&#26679;&#26412;&#26469;&#25351;&#23450;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#22522;&#26412;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#30340;&#23618;&#32423;&#20018;&#32852;&#26500;&#36896;&#30340;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#65288;DGPs&#65289;&#30456;&#27604;&#65292;TGPs&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#36716;&#25442;&#39640;&#26031;&#36807;&#31243;&#65288;DTGPs&#65289;&#30340;TGP&#25512;&#24191;&#65292;&#23427;&#36981;&#24490;&#20018;&#32852;&#38543;&#26426;&#36807;&#31243;&#23618;&#30340;&#36235;&#21183;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#22810;&#23618;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#19968;&#23618;&#37117;&#26159;&#19968;&#20010;TGP&#12290;&#36825;&#31181;&#25512;&#24191;&#24847;&#21619;&#30528;&#30456;&#23545;&#20110;TGPs&#21644;DGPs&#37117;&#25552;&#39640;&#20102;&#28789;&#27963;&#24615;&#12290;&#22312;&#36825;&#26679;&#30340;&#27169;&#22411;&#20013;&#36827;&#34892;&#31934;&#30830;&#25512;&#29702;&#26159;&#22256;&#38590;&#30340;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#21464;&#20998;&#25512;&#29702;&#26469;&#36817;&#20284;&#25152;&#38656;&#30340;&#35745;&#31639;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#27969;&#34892;&#30340;DSVI&#25512;&#29702;&#31639;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformed Gaussian Processes (TGPs) are stochastic processes specified by transforming samples from the joint distribution from a prior process (typically a GP) using an invertible transformation; increasing the flexibility of the base process.  Furthermore, they achieve competitive results compared with Deep Gaussian Processes (DGPs), which are another generalization constructed by a hierarchical concatenation of GPs. In this work, we propose a generalization of TGPs named Deep Transformed Gaussian Processes (DTGPs), which follows the trend of concatenating layers of stochastic processes. More precisely, we obtain a multi-layer model in which each layer is a TGP. This generalization implies an increment of flexibility with respect to both TGPs and DGPs. Exact inference in such a model is intractable. However, we show that one can use variational inference to approximate the required computations yielding a straightforward extension of the popular DSVI inference algorithm Salimbeni e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TBDLNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#22810;&#33647;&#32784;&#33647;&#21644;&#33647;&#29289;&#25935;&#24863;&#32467;&#26680;&#30149;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ResNet50&#25552;&#21462;&#29305;&#24449;&#65292;&#37319;&#29992;&#19977;&#20010;&#38543;&#26426;&#21270;&#31070;&#32463;&#32593;&#32476;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#30340;&#26041;&#24335;&#36827;&#34892;&#38598;&#25104;&#65292;TBDLNet&#36798;&#21040;&#20102;&#39640;&#20934;&#30830;&#29575;&#12289;&#29305;&#24322;&#24615;&#12289;&#31934;&#30830;&#24230;&#12289;&#25935;&#24863;&#24230;&#21644;F1-score&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#21450;&#26089;&#26816;&#27979;&#21040;&#22810;&#33647;&#32784;&#33647;&#24615;&#32954;&#32467;&#26680;&#65292;&#24110;&#21161;&#21450;&#26102;&#35843;&#25972;&#27835;&#30103;&#35745;&#21010;&#24182;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.18222</link><description>&lt;p&gt;
TBDLNet&#65306;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#22810;&#33647;&#32784;&#33647;&#21644;&#33647;&#29289;&#25935;&#24863;&#32467;&#26680;&#30149;&#30340;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TBDLNet: a network for classifying multidrug-resistant and drug-sensitive tuberculosis. (arXiv:2310.18222v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TBDLNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#22810;&#33647;&#32784;&#33647;&#21644;&#33647;&#29289;&#25935;&#24863;&#32467;&#26680;&#30149;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ResNet50&#25552;&#21462;&#29305;&#24449;&#65292;&#37319;&#29992;&#19977;&#20010;&#38543;&#26426;&#21270;&#31070;&#32463;&#32593;&#32476;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#30340;&#26041;&#24335;&#36827;&#34892;&#38598;&#25104;&#65292;TBDLNet&#36798;&#21040;&#20102;&#39640;&#20934;&#30830;&#29575;&#12289;&#29305;&#24322;&#24615;&#12289;&#31934;&#30830;&#24230;&#12289;&#25935;&#24863;&#24230;&#21644;F1-score&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#21450;&#26089;&#26816;&#27979;&#21040;&#22810;&#33647;&#32784;&#33647;&#24615;&#32954;&#32467;&#26680;&#65292;&#24110;&#21161;&#21450;&#26102;&#35843;&#25972;&#27835;&#30103;&#35745;&#21010;&#24182;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;TBDLNet&#24212;&#29992;&#20110;&#35782;&#21035;CT&#22270;&#20687;&#65292;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#22810;&#33647;&#32784;&#33647;&#21644;&#33647;&#29289;&#25935;&#24863;&#32467;&#26680;&#30149;&#12290;&#36873;&#25321;&#39044;&#35757;&#32451;&#30340;ResNet50&#26469;&#25552;&#21462;&#29305;&#24449;&#12290;&#37319;&#29992;&#19977;&#20010;&#38543;&#26426;&#21270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#30340;&#26041;&#24335;&#24212;&#29992;&#19977;&#20010;RNN&#30340;&#38598;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#36873;&#25321;&#20102;&#20116;&#20010;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#20998;&#21035;&#26159;&#20934;&#30830;&#29575;&#12289;&#25935;&#24863;&#24230;&#12289;&#31934;&#30830;&#24230;&#12289;F1-score&#21644;&#29305;&#24322;&#24615;&#12290;TBDLNet&#20998;&#21035;&#36798;&#21040;&#20102;0.9822&#30340;&#20934;&#30830;&#29575;&#12289;0.9815&#30340;&#29305;&#24322;&#24615;&#12289;0.9823&#30340;&#31934;&#30830;&#24230;&#12289;0.9829&#30340;&#25935;&#24863;&#24230;&#21644;0.9826&#30340;F1-score&#12290;TBDLNet&#36866;&#29992;&#20110;&#20998;&#31867;&#22810;&#33647;&#32784;&#33647;&#32467;&#26680;&#30149;&#21644;&#33647;&#29289;&#25935;&#24863;&#32467;&#26680;&#30149;&#12290;&#23427;&#33021;&#23613;&#26089;&#26816;&#27979;&#21040;&#22810;&#33647;&#32784;&#33647;&#24615;&#32954;&#32467;&#26680;&#65292;&#26377;&#21161;&#20110;&#21450;&#26102;&#35843;&#25972;&#27835;&#30103;&#26041;&#26696;&#65292;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes applying a novel deep-learning model, TBDLNet, to recognize CT images to classify multidrug-resistant and drug-sensitive tuberculosis automatically. The pre-trained ResNet50 is selected to extract features. Three randomized neural networks are used to alleviate the overfitting problem. The ensemble of three RNNs is applied to boost the robustness via majority voting. The proposed model is evaluated by five-fold cross-validation. Five indexes are selected in this paper, which are accuracy, sensitivity, precision, F1-score, and specificity. The TBDLNet achieves 0.9822 accuracy, 0.9815 specificity, 0.9823 precision, 0.9829 sensitivity, and 0.9826 F1-score, respectively. The TBDLNet is suitable for classifying multidrug-resistant tuberculosis and drug-sensitive tuberculosis. It can detect multidrug-resistant pulmonary tuberculosis as early as possible, which helps to adjust the treatment plan in time and improve the treatment effect.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20840;&#21306;&#22495;&#30340;&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#27169;&#24335;&#65292;&#24182;&#37319;&#29992;&#20102;&#21306;&#22495;&#20013;&#31435;&#30340;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#20219;&#20309;&#22320;&#21306;&#65292;&#21253;&#25324;&#26410;&#30693;&#22320;&#21306;&#12290;</title><link>http://arxiv.org/abs/2310.18215</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#20840;&#21306;&#22495;&#30340;&#20132;&#21449;&#22320;&#21306;&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
One Model Fits All: Cross-Region Taxi-Demand Forecasting. (arXiv:2310.18215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20840;&#21306;&#22495;&#30340;&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#27169;&#24335;&#65292;&#24182;&#37319;&#29992;&#20102;&#21306;&#22495;&#20013;&#31435;&#30340;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#20219;&#20309;&#22320;&#21306;&#65292;&#21253;&#25324;&#26410;&#30693;&#22320;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25171;&#36710;&#26381;&#21153;&#38656;&#27714;&#30340;&#22686;&#38271;&#65292;&#23545;&#20934;&#30830;&#30340;&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#21152;&#12290;&#29616;&#26377;&#31995;&#32479;&#20165;&#38480;&#20110;&#29305;&#23450;&#22320;&#21306;&#65292;&#32570;&#20047;&#23545;&#26410;&#30693;&#22320;&#21306;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#20102;&#19968;&#31181;&#21306;&#22495;&#20013;&#31435;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#22320;&#21306;&#65292;&#21253;&#25324;&#26410;&#30693;&#22320;&#21306;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#36755;&#20837;&#29305;&#24449;&#20998;&#24320;&#20026;&#21306;&#22495;&#29305;&#23450;&#21644;&#21306;&#22495;&#20013;&#31435;&#30340;&#37096;&#20998;&#12290;&#21306;&#22495;&#20013;&#31435;&#29305;&#24449;&#26377;&#21161;&#20110;&#20132;&#21449;&#22320;&#21306;&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#21516;&#22478;&#24066;&#22320;&#21306;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31995;&#32479;&#22312;&#20934;&#30830;&#39044;&#27979;&#20986;&#31199;&#36710;&#38656;&#27714;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#29978;&#33267;&#22312;&#20197;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#22320;&#21306;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing demand for ride-hailing services has led to an increasing need for accurate taxi demand prediction. Existing systems are limited to specific regions, lacking generalizability to unseen areas. This paper presents a novel taxi demand forecasting system that leverages a graph neural network to capture spatial dependencies and patterns in urban environments. Additionally, the proposed system employs a region-neutral approach, enabling it to train a model that can be applied to any region, including unseen regions. To achieve this, the framework incorporates the power of Variational Autoencoder to disentangle the input features into region-specific and region-neutral components. The region-neutral features facilitate cross-region taxi demand predictions, allowing the model to generalize well across different urban areas. Experimental results demonstrate the effectiveness of the proposed system in accurately forecasting taxi demand, even in previously unobserved regions, thus sho
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#36229;&#21442;&#25968;&#23545;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23545;&#19981;&#21516;&#22797;&#26434;&#32423;&#21035;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#21457;&#29616;&#36229;&#21442;&#25968;&#36873;&#25321;&#21516;&#26679;&#23545;&#31639;&#27861;&#36873;&#25321;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.18212</link><description>&lt;p&gt;
&#29992;&#20110;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#31639;&#27861;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness of Algorithms for Causal Structure Learning to Hyperparameter Choice. (arXiv:2310.18212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18212
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#36229;&#21442;&#25968;&#23545;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23545;&#19981;&#21516;&#22797;&#26434;&#32423;&#21035;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#21457;&#29616;&#36229;&#21442;&#25968;&#36873;&#25321;&#21516;&#26679;&#23545;&#31639;&#27861;&#36873;&#25321;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30001;&#20110;&#32467;&#26500;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#29305;&#24615;&#65292;&#36229;&#21442;&#25968;&#35843;&#25972;&#21487;&#20197;&#22312;&#20219;&#20309;&#31639;&#27861;&#20013;&#20135;&#29983;&#20840;&#29699;&#39046;&#20808;&#21644;&#31967;&#31957;&#30340;&#39044;&#27979;&#34920;&#29616;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20351;&#29992;&#29305;&#23450;&#31639;&#27861;&#30340;&#40664;&#35748;&#20540;&#65292;&#20154;&#20204;&#32463;&#24120;&#24573;&#35270;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#22823;&#37327;&#20851;&#20110;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#24615;&#33021;&#35780;&#20272;&#30340;&#30740;&#31350;&#65292;&#20294;&#36229;&#21442;&#25968;&#22914;&#20309;&#24433;&#21709;&#21333;&#20010;&#31639;&#27861;&#20197;&#21450;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#36229;&#21442;&#25968;&#23545;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20219;&#21153;&#30340;&#24433;&#21709;&#26469;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#22797;&#26434;&#32423;&#21035;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#19968;&#20123;&#24320;&#21019;&#24615;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#31639;&#27861;&#30340;&#36873;&#25321;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#21516;&#26679;&#23545;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20219;&#21153;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameters play a critical role in machine learning. Hyperparameter tuning can make the difference between state-of-the-art and poor prediction performance for any algorithm, but it is particularly challenging for structure learning due to its unsupervised nature. As a result, hyperparameter tuning is often neglected in favour of using the default values provided by a particular implementation of an algorithm. While there have been numerous studies on performance evaluation of causal discovery algorithms, how hyperparameters affect individual algorithms, as well as the choice of the best algorithm for a specific problem, has not been studied in depth before. This work addresses this gap by investigating the influence of hyperparameters on causal structure learning tasks. Specifically, we perform an empirical evaluation of hyperparameter selection for some seminal learning algorithms on datasets of varying levels of complexity. We find that, while the choice of algorithm remains cr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#22270;&#23884;&#20837;&#65292;&#24182;&#35774;&#35745;&#20102;&#23545;&#40784;&#24230;&#37327;&#21644;&#22343;&#21248;&#24615;&#24230;&#37327;&#26469;&#35299;&#20915;&#22270;&#39046;&#22495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#20960;&#20309;&#32467;&#26500;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18209</link><description>&lt;p&gt;
&#36229;&#21367;&#26354;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#23545;&#40784;&#21644;&#22806;&#22771;&#21516;&#26500;&#24615;
&lt;/p&gt;
&lt;p&gt;
Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning. (arXiv:2310.18209v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18209
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#22270;&#23884;&#20837;&#65292;&#24182;&#35774;&#35745;&#20102;&#23545;&#40784;&#24230;&#37327;&#21644;&#22343;&#21248;&#24615;&#24230;&#37327;&#26469;&#35299;&#20915;&#22270;&#39046;&#22495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#20960;&#20309;&#32467;&#26500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#30410;&#30340;&#33258;&#30417;&#30563;&#22270;&#34920;&#31034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#23545;&#27604;&#23398;&#20064;&#30340;&#23884;&#20837;&#34987;&#25490;&#21015;&#22312;&#19968;&#20010;&#36229;&#29699;&#38754;&#19978;&#65292;&#20174;&#32780;&#20351;&#24471;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#20313;&#24358;&#36317;&#31163;&#27979;&#37327;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#39046;&#22495;&#30340;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#65292;&#22914;&#22270;&#24418;&#65292;&#23637;&#29616;&#20102;&#39640;&#24230;&#38750;&#27431;&#20960;&#37324;&#24503;&#30340;&#29305;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#22270;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#25429;&#25417;&#23618;&#27425;&#25968;&#25454;&#19981;&#21464;&#24615;&#20449;&#24687;&#30340;&#23545;&#40784;&#24230;&#37327;&#65292;&#21516;&#26102;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22343;&#21248;&#24230;&#37327;&#26469;&#38450;&#27490;&#25152;&#35859;&#30340;&#32500;&#24230;&#22604;&#38519;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#24517;&#39035;&#35299;&#20915;&#19982;&#26641;&#30340;&#23646;&#24615;&#30456;&#20851;&#30340;&#21494;&#23376;&#21644;&#39640;&#24230;&#23618;&#38754;&#30340;&#22343;&#21248;&#24615;&#65292;&#32780;&#22312;&#21452;&#26354;&#27969;&#24418;&#30340;&#29615;&#22659;&#31354;&#38388;&#20013;&#65292;&#36825;&#20123;&#27010;&#24565;&#36716;&#21270;&#20026;&#23545;&#21516;&#26500;&#24615;&#30340;&#26045;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning good self-supervised graph representations that are beneficial to downstream tasks is challenging. Among a variety of methods, contrastive learning enjoys competitive performance. The embeddings of contrastive learning are arranged on a hypersphere that enables the Cosine distance measurement in the Euclidean space. However, the underlying structure of many domains such as graphs exhibits highly non-Euclidean latent geometry. To this end, we propose a novel contrastive learning framework to learn high-quality graph embedding. Specifically, we design the alignment metric that effectively captures the hierarchical data-invariant information, as well as we propose a substitute of uniformity metric to prevent the so-called dimensional collapse. We show that in the hyperbolic space one has to address the leaf- and height-level uniformity which are related to properties of trees, whereas in the ambient space of the hyperbolic manifold, these notions translate into imposing an isotro
&lt;/p&gt;</description></item><item><title>ArcheType&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.18208</link><description>&lt;p&gt;
ArcheType&#65306;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models. (arXiv:2310.18208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18208
&lt;/p&gt;
&lt;p&gt;
ArcheType&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35821;&#20041;&#21015;&#31867;&#22411;&#27880;&#37322;&#65288;CTA&#65289;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#32570;&#28857;&#65306;&#23427;&#20204;&#20381;&#36182;&#20110;&#22312;&#35757;&#32451;&#26102;&#22266;&#23450;&#30340;&#35821;&#20041;&#31867;&#22411;&#65307;&#38656;&#35201;&#22823;&#37327;&#30340;&#27599;&#20010;&#31867;&#22411;&#30340;&#35757;&#32451;&#26679;&#26412;&#24182;&#20135;&#29983;&#22823;&#37327;&#36816;&#34892;&#26102;&#25512;&#26029;&#25104;&#26412;&#65307;&#21363;&#20351;&#31867;&#22411;&#20445;&#25345;&#19981;&#21464;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20063;&#21487;&#33021;&#22312;&#35780;&#20272;&#26032;&#25968;&#25454;&#38598;&#26102;&#19979;&#38477;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;CTA&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ArcheType&#65292;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#37319;&#26679;&#12289;&#25552;&#31034;&#24207;&#21015;&#21270;&#12289;&#27169;&#22411;&#26597;&#35810;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#20174;&#32780;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#20840;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#35299;&#20915;&#21015;&#31867;&#22411;&#27880;&#37322;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#21035;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20986;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#25552;&#20379;&#20102;&#26368;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type and incur large run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve column type annotation problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes new state-of-the-art performance on both zero-shot and fine-tuned C
&lt;/p&gt;</description></item><item><title>VeLO&#26159;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#35757;&#32451;&#36890;&#29992;&#8220;&#22522;&#30784;&#8221;&#20248;&#21270;&#22120;&#30340;&#23581;&#35797;&#65292;&#20294;&#25105;&#20204;&#30340;&#35780;&#20272;&#21457;&#29616;&#23427;&#38656;&#35201;&#38382;&#39064;&#29305;&#23450;&#30340;&#35843;&#20248;&#65292;&#24182;&#19981;&#19968;&#23450;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#35757;&#32451;&#35823;&#24046;&#38477;&#20302;&#36895;&#24230;&#65292;&#36825;&#23545;&#20110;VeLO&#30340;&#36890;&#29992;&#24615;&#21644;&#22521;&#35757;&#25237;&#36164;&#30340;&#20215;&#20540;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;</title><link>http://arxiv.org/abs/2310.18191</link><description>&lt;p&gt;
&#32553;&#25918;&#23398;&#20064;&#20248;&#21270;&#22120;&#26159;&#21542;&#20540;&#24471;&#65311;&#35780;&#20272; VeLO &#30340; 4000 &#20010; TPU &#26376;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months. (arXiv:2310.18191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18191
&lt;/p&gt;
&lt;p&gt;
VeLO&#26159;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#35757;&#32451;&#36890;&#29992;&#8220;&#22522;&#30784;&#8221;&#20248;&#21270;&#22120;&#30340;&#23581;&#35797;&#65292;&#20294;&#25105;&#20204;&#30340;&#35780;&#20272;&#21457;&#29616;&#23427;&#38656;&#35201;&#38382;&#39064;&#29305;&#23450;&#30340;&#35843;&#20248;&#65292;&#24182;&#19981;&#19968;&#23450;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#35757;&#32451;&#35823;&#24046;&#38477;&#20302;&#36895;&#24230;&#65292;&#36825;&#23545;&#20110;VeLO&#30340;&#36890;&#29992;&#24615;&#21644;&#22521;&#35757;&#25237;&#36164;&#30340;&#20215;&#20540;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102; VeLO&#65288;&#19975;&#33021;&#23398;&#20064;&#20248;&#21270;&#22120;&#65289;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#35757;&#32451;&#36890;&#29992;&#8220;&#22522;&#30784;&#8221;&#20248;&#21270;&#22120;&#30340;&#23581;&#35797;&#12290;VeLO &#20351;&#29992;&#36229;&#36807; 4000 &#20010; TPU &#26376;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#30446;&#26631;&#26159;&#20135;&#29983;&#19968;&#20010;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#38382;&#39064;&#24182;&#19988;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#36229;&#36807; Adam &#31561;&#34892;&#19994;&#26631;&#20934;&#30340;&#20248;&#21270;&#22120;&#12290;&#25105;&#20204;&#23545; MLCommons &#20248;&#21270;&#22120;&#22522;&#20934;&#22871;&#20214;&#29420;&#31435;&#35780;&#20272;&#20102; VeLO&#12290;&#25105;&#20204;&#21457;&#29616;&#19982;&#21021;&#27493;&#22768;&#26126;&#30456;&#21453;&#65306;&#65288;1&#65289;VeLO&#26377;&#19968;&#20010;&#20851;&#38190;&#30340;&#36229;&#21442;&#25968;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#38382;&#39064;&#36827;&#34892;&#35843;&#25972;&#65292;&#65288;2&#65289;VeLO&#22312;&#25214;&#21040;&#30340;&#35299;&#30340;&#36136;&#37327;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#65292;&#65288;3&#65289;VeLO&#22312;&#38477;&#20302;&#35757;&#32451;&#35823;&#24046;&#19978;&#24182;&#19981;&#27604;&#31454;&#20105;&#20248;&#21270;&#22120;&#26356;&#24555;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#23545; VeLO &#30340;&#36890;&#29992;&#24615;&#21644;&#22521;&#35757;&#25237;&#36164;&#30340;&#20215;&#20540;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze VeLO (versatile learned optimizer), the largest scale attempt to train a general purpose "foundational" optimizer to date. VeLO was trained on thousands of machine learning tasks using over 4000 TPU months with the goal of producing an optimizer capable of generalizing to new problems while being hyperparameter free, and outperforming industry standards such as Adam. We independently evaluate VeLO on the MLCommons optimizer benchmark suite. We find that, contrary to initial claims: (1) VeLO has a critical hyperparameter that needs problem-specific tuning, (2) VeLO does not necessarily outperform competitors in quality of solution found, and (3) VeLO is not faster than competing optimizers at reducing the training loss. These observations call into question VeLO's generality and the value of the investment in training it.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38543;&#26426;&#21270;&#26080;&#27169;&#22411;&#31639;&#27861;RandQL&#65292;&#29992;&#20110;&#20943;&#23567;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#36951;&#25022;&#12290;RandQL&#36890;&#36807;&#23398;&#20064;&#29575;&#38543;&#26426;&#21270;&#23454;&#29616;&#20048;&#35266;&#25506;&#32034;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.18186</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#21518;&#39564;&#37319;&#26679;&#30340;&#27169;&#22411;&#33258;&#30001;&#38543;&#26426;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Model-free Posterior Sampling via Learning Rate Randomization. (arXiv:2310.18186v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38543;&#26426;&#21270;&#26080;&#27169;&#22411;&#31639;&#27861;RandQL&#65292;&#29992;&#20110;&#20943;&#23567;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#36951;&#25022;&#12290;RandQL&#36890;&#36807;&#23398;&#20064;&#29575;&#38543;&#26426;&#21270;&#23454;&#29616;&#20048;&#35266;&#25506;&#32034;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#21270;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;Randomized Q-learning&#65288;&#31616;&#31216;RandQL&#65289;&#65292;&#29992;&#20110;&#20943;&#23567;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#30340;&#36951;&#25022;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;RandQL&#26159;&#31532;&#19968;&#20010;&#21487;&#34892;&#30340;&#27169;&#22411;&#33258;&#30001;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;RandQL&#22312;&#34920;&#26684;&#21644;&#38750;&#34920;&#26684;&#24230;&#37327;&#31354;&#38388;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;&#22312;&#34920;&#26684;MDPs&#20013;&#65292;RandQL&#23454;&#29616;&#20102;&#19968;&#20010;&#36951;&#25022;&#30028;&#30340;&#39034;&#24207;&#20026;$\widetilde{\mathcal{O}}(\sqrt{H^{5}SAT})$&#65292;&#20854;&#20013;$H$&#26159;&#35745;&#21010;&#30340;&#26102;&#38388;&#38271;&#24230;&#65292;$S$&#26159;&#29366;&#24577;&#25968;&#65292;$A$&#26159;&#21160;&#20316;&#25968;&#65292;$T$&#26159;&#22238;&#21512;&#25968;&#12290;&#23545;&#20110;&#24230;&#37327;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#65292;RandQL&#23454;&#29616;&#20102;&#19968;&#20010;&#36951;&#25022;&#30028;&#30340;&#39034;&#24207;&#20026;$\widetilde{\mathcal{O}}(H^{5/2} T^{(d_z+1)/(d_z+2)})$&#65292;&#20854;&#20013;$d_z$&#34920;&#31034;&#32553;&#25918;&#32500;&#24230;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;RandQL&#23454;&#29616;&#20102;&#20048;&#35266;&#25506;&#32034;&#65292;&#32780;&#19981;&#20351;&#29992;&#22870;&#21169;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#23398;&#20064;&#29575;&#38543;&#26426;&#21270;&#30340;&#26032;&#24605;&#24819;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;RandQL&#22312;&#22522;&#32447;&#25506;&#32034;&#19978;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Randomized Q-learning (RandQL), a novel randomized model-free algorithm for regret minimization in episodic Markov Decision Processes (MDPs). To the best of our knowledge, RandQL is the first tractable model-free posterior sampling-based algorithm. We analyze the performance of RandQL in both tabular and non-tabular metric space settings. In tabular MDPs, RandQL achieves a regret bound of order $\widetilde{\mathcal{O}}(\sqrt{H^{5}SAT})$, where $H$ is the planning horizon, $S$ is the number of states, $A$ is the number of actions, and $T$ is the number of episodes. For a metric state-action space, RandQL enjoys a regret bound of order $\widetilde{\mathcal{O}}(H^{5/2} T^{(d_z+1)/(d_z+2)})$, where $d_z$ denotes the zooming dimension. Notably, RandQL achieves optimistic exploration without using bonuses, relying instead on a novel idea of learning rate randomization. Our empirical study shows that RandQL outperforms existing approaches on baseline exploration en
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18168</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25991;&#26412;&#20013;&#26082;&#21253;&#21547;&#20102;&#20107;&#23454;&#65292;&#20063;&#21253;&#21547;&#20102;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#36825;&#20123;&#30456;&#20114;&#30683;&#30462;&#30340;&#25968;&#25454;&#20013;&#36776;&#21035;&#30495;&#23454;&#19982;&#34394;&#20551;&#21527;&#65311;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#19981;&#21516;&#20135;&#29983;&#25991;&#26412;&#30340;&#20010;&#20307;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#26469;&#32858;&#31867;&#30495;&#23454;&#25991;&#26412;&#65306;&#19968;&#32676;&#24456;&#21487;&#33021;&#20135;&#29983;&#30495;&#23454;&#25991;&#26412;&#24182;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#20010;&#20307;&#12290;&#20363;&#22914;&#65292;&#21487;&#20449;&#28304;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#31185;&#23398;&#26399;&#21002;&#36890;&#24120;&#20351;&#29992;&#27491;&#24335;&#30340;&#20889;&#20316;&#39118;&#26684;&#24182;&#25552;&#20986;&#19968;&#33268;&#30340;&#20027;&#24352;&#12290;&#36890;&#36807;&#24314;&#27169;&#36825;&#19968;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#27599;&#20010;&#20010;&#20307;&#29983;&#25104;&#35757;&#32451;&#25991;&#26412;&#30340;&#29305;&#23450;&#19978;&#19979;&#25991;&#20043;&#22806;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#8220;&#32500;&#22522;&#30334;&#31185;&#8221;&#36825;&#20010;&#20010;&#20307;&#22312;&#8220;&#31185;&#23398;&#8221;&#29983;&#25104;&#30340;&#20027;&#39064;&#19978;&#20250;&#34920;&#29616;&#20986;&#30495;&#23454;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20849;&#20139;&#19968;&#20010;&#20154;&#35774;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20004;&#20010;&#35266;&#23519;&#32467;&#26524;&#20026;&#20154;&#35774;&#20551;&#35774;&#25552;&#20379;&#20102;&#35777;&#25454;&#65306;&#65288;1&#65289;&#25105;&#20204;&#21487;&#20197;&#25506;&#27979;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21028;&#26029;&#30495;&#23454;&#24615;&#30340;&#33021;&#21147;&#65307;&#65288;2&#65289;&#27169;&#22411;&#21487;&#20197;&#20174;&#30456;&#20851;&#29305;&#24449;&#20013;&#25512;&#27979;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22522;&#20110;&#26426;&#22120;&#32423;&#21644;&#36827;&#31243;&#32423;&#20998;&#26512;&#30340;&#21160;&#24577;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#25506;&#32034;&#22914;&#20309;&#38548;&#31163;&#24694;&#24847;&#36827;&#31243;&#20197;&#25552;&#39640;&#32593;&#32476;&#23433;&#20840;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#26679;&#26412;&#25191;&#34892;&#20013;&#30340;&#23454;&#38469;&#24773;&#20917;&#21644;&#36164;&#28304;&#21033;&#29992;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.18165</link><description>&lt;p&gt;
&#25552;&#21319;&#20225;&#19994;&#32593;&#32476;&#23433;&#20840;&#65306;&#27604;&#36739;&#22522;&#20110;&#26426;&#22120;&#32423;&#21644;&#36827;&#31243;&#32423;&#20998;&#26512;&#30340;&#21160;&#24577;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Enterprise Network Security: Comparing Machine-Level and Process-Level Analysis for Dynamic Malware Detection. (arXiv:2310.18165v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22522;&#20110;&#26426;&#22120;&#32423;&#21644;&#36827;&#31243;&#32423;&#20998;&#26512;&#30340;&#21160;&#24577;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#25506;&#32034;&#22914;&#20309;&#38548;&#31163;&#24694;&#24847;&#36827;&#31243;&#20197;&#25552;&#39640;&#32593;&#32476;&#23433;&#20840;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#26679;&#26412;&#25191;&#34892;&#20013;&#30340;&#23454;&#38469;&#24773;&#20917;&#21644;&#36164;&#28304;&#21033;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#24694;&#24847;&#36719;&#20214;&#23545;&#20110;&#29702;&#35299;&#24694;&#24847;&#36719;&#20214;&#30340;&#24037;&#20316;&#21407;&#29702;&#24182;&#24320;&#21457;&#36866;&#24403;&#30340;&#26816;&#27979;&#21644;&#39044;&#38450;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#21160;&#24577;&#20998;&#26512;&#21487;&#20197;&#20811;&#26381;&#24120;&#29992;&#20110;&#32469;&#36807;&#38745;&#24577;&#20998;&#26512;&#30340;&#36867;&#36920;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#23545;&#24694;&#24847;&#36719;&#20214;&#36816;&#34892;&#26102;&#27963;&#21160;&#30340;&#27934;&#23519;&#12290;&#35768;&#22810;&#20851;&#20110;&#21160;&#24577;&#20998;&#26512;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#35843;&#26597;&#26426;&#22120;&#32423;&#20449;&#24687;&#65288;&#20363;&#22914;CPU&#12289;&#20869;&#23384;&#12289;&#32593;&#32476;&#20351;&#29992;&#29575;&#65289;&#65292;&#20197;&#30830;&#23450;&#26426;&#22120;&#26159;&#21542;&#36816;&#34892;&#24694;&#24847;&#27963;&#21160;&#12290;&#24694;&#24847;&#26426;&#22120;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#26426;&#22120;&#19978;&#30340;&#25152;&#26377;&#36816;&#34892;&#36827;&#31243;&#20063;&#26159;&#24694;&#24847;&#30340;&#12290;&#22914;&#26524;&#25105;&#20204;&#21487;&#20197;&#38548;&#31163;&#24694;&#24847;&#36827;&#31243;&#32780;&#19981;&#26159;&#38548;&#31163;&#25972;&#20010;&#26426;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#26432;&#27515;&#24694;&#24847;&#36827;&#31243;&#65292;&#32780;&#26426;&#22120;&#21487;&#20197;&#32487;&#32493;&#27491;&#24120;&#24037;&#20316;&#12290;&#21160;&#24577;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30740;&#31350;&#38754;&#20020;&#30340;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#26679;&#26412;&#22312;&#19968;&#20010;&#27809;&#26377;&#20219;&#20309;&#21518;&#21488;&#24212;&#29992;&#31243;&#24207;&#36816;&#34892;&#30340;&#26426;&#22120;&#19978;&#25191;&#34892;&#12290;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#22240;&#20026;&#24694;&#24847;&#36719;&#20214;&#20107;&#20214;&#21457;&#29983;&#26102;&#65292;&#35745;&#31639;&#26426;&#36890;&#24120;&#20250;&#36816;&#34892;&#35768;&#22810;&#33391;&#24615;&#65288;&#21518;&#21488;&#65289;&#24212;&#29992;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Analysing malware is important to understand how malicious software works and to develop appropriate detection and prevention methods. Dynamic analysis can overcome evasion techniques commonly used to bypass static analysis and provide insights into malware runtime activities. Much research on dynamic analysis focused on investigating machine-level information (e.g., CPU, memory, network usage) to identify whether a machine is running malicious activities. A malicious machine does not necessarily mean all running processes on the machine are also malicious. If we can isolate the malicious process instead of isolating the whole machine, we could kill the malicious process, and the machine can keep doing its job. Another challenge dynamic malware detection research faces is that the samples are executed in one machine without any background applications running. It is unrealistic as a computer typically runs many benign (background) applications when a malware incident happens. Our exper
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27604;&#20363;&#32858;&#31867;&#38382;&#39064;&#65292;&#23558;&#20854;&#19982;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#20013;&#30340;&#22810;&#36194;&#23478;&#25237;&#31080;&#39046;&#22495;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#21457;&#29616;&#28385;&#36275;Brill&#21644;Peters&#30340;&#27604;&#20363;&#27010;&#24565;&#30340;&#20219;&#20309;&#32858;&#31867;&#37117;&#33021;&#21516;&#26102;&#33719;&#24471;Chen&#31561;&#20154;&#30340;&#27604;&#20363;&#20844;&#24179;&#24615;&#12289;&#20010;&#20307;&#20844;&#24179;&#24615;&#21644;&#26680;&#24515;&#30340;&#26368;&#20339;&#36817;&#20284;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#26356;&#24378;&#30340;&#27604;&#20363;&#20195;&#34920;&#24615;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#26356;&#24378;&#27010;&#24565;&#23545;&#24212;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2310.18162</link><description>&lt;p&gt;
&#38598;&#32676;&#20013;&#30340;&#27604;&#20363;&#20844;&#24179;&#24615;: &#31038;&#20250;&#36873;&#25321;&#35270;&#35282;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Proportional Fairness in Clustering: A Social Choice Perspective. (arXiv:2310.18162v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18162
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27604;&#20363;&#32858;&#31867;&#38382;&#39064;&#65292;&#23558;&#20854;&#19982;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#20013;&#30340;&#22810;&#36194;&#23478;&#25237;&#31080;&#39046;&#22495;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#21457;&#29616;&#28385;&#36275;Brill&#21644;Peters&#30340;&#27604;&#20363;&#27010;&#24565;&#30340;&#20219;&#20309;&#32858;&#31867;&#37117;&#33021;&#21516;&#26102;&#33719;&#24471;Chen&#31561;&#20154;&#30340;&#27604;&#20363;&#20844;&#24179;&#24615;&#12289;&#20010;&#20307;&#20844;&#24179;&#24615;&#21644;&#26680;&#24515;&#30340;&#26368;&#20339;&#36817;&#20284;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#26356;&#24378;&#30340;&#27604;&#20363;&#20195;&#34920;&#24615;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#26356;&#24378;&#27010;&#24565;&#23545;&#24212;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;Chen&#31561;&#20154;&#30340;&#27604;&#20363;&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#19982;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#20013;&#30340;&#22810;&#36194;&#23478;&#25237;&#31080;&#39046;&#22495;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28385;&#36275;Brill&#21644;Peters&#30340;&#24369;&#27604;&#20363;&#27010;&#24565;&#30340;&#20219;&#20309;&#32858;&#31867;&#21516;&#26102;&#33719;&#24471;&#20102;Chen&#31561;&#20154;&#30340;&#27604;&#20363;&#20844;&#24179;&#24615;&#30340;&#26368;&#20339;&#36817;&#20284;&#65292;&#20063;&#33719;&#24471;&#20102;&#20010;&#20307;&#20844;&#24179;&#24615;&#21644;&#8220;&#26680;&#24515;&#8221;&#30340;&#26368;&#20339;&#36817;&#20284;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#34920;&#26126;&#20219;&#20309;&#23545;&#27604;&#20363;&#20844;&#24179;&#24615;&#30340;&#36817;&#20284;&#20063;&#26159;&#23545;&#20010;&#20307;&#20844;&#24179;&#24615;&#30340;&#36817;&#20284;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#26356;&#24378;&#30340;&#27604;&#20363;&#20195;&#34920;&#24615;&#27010;&#24565;&#65292;&#20854;&#20013;&#20559;&#24046;&#19981;&#20165;&#21457;&#29983;&#22312;&#21333;&#20010;&#20505;&#36873;&#20013;&#24515;&#65292;&#32780;&#26159;&#22810;&#20010;&#20505;&#36873;&#20013;&#24515;&#65292;&#24182;&#23637;&#31034;&#20102;Brill&#21644;Peters&#30340;&#26356;&#24378;&#27604;&#20363;&#27010;&#24565;&#26263;&#31034;&#20102;&#36825;&#20123;&#26356;&#24378;&#20445;&#35777;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the proportional clustering problem of Chen et al. [ICML'19] and relate it to the area of multiwinner voting in computational social choice. We show that any clustering satisfying a weak proportionality notion of Brill and Peters [EC'23] simultaneously obtains the best known approximations to the proportional fairness notion of Chen et al. [ICML'19], but also to individual fairness [Jung et al., FORC'20] and the "core" [Li et al. ICML'21]. In fact, we show that any approximation to proportional fairness is also an approximation to individual fairness and vice versa. Finally, we also study stronger notions of proportional representation, in which deviations do not only happen to single, but multiple candidate centers, and show that stronger proportionality notions of Brill and Peters [EC'23] imply approximations to these stronger guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.18152</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#23646;&#24615;&#22270;&#30340;&#35299;&#32544;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#22312;&#32593;&#32476;&#19978;&#38750;&#24120;&#24120;&#35265;&#65292;&#23545;&#20110;&#35813;&#31867;&#22270;&#65292;&#22914;&#24341;&#29992;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#21644;&#31038;&#20132;&#32593;&#32476;&#30340;&#30740;&#31350;&#22312;&#32593;&#32476;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20165;&#20165;&#20381;&#38752;&#25552;&#31034;&#20449;&#24687;&#26469;&#20256;&#36798;&#22270;&#32467;&#26500;&#20449;&#24687;&#32473;LLMs&#65292;&#22240;&#27492;&#23545;&#20110;TAGs&#20013;&#22797;&#26434;&#30340;&#32467;&#26500;&#20851;&#31995;&#20102;&#35299;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#32544;&#22270;&#25991;&#23398;&#20064;&#22120;&#65288;DGTL&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#22686;&#24378;LLMs&#23545;TAGs&#30340;&#25512;&#29702;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;DGTL&#27169;&#22411;&#36890;&#36807;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#22240;&#32032;&#20013;&#38544;&#34255;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2310.18144</link><description>&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#26469;&#25913;&#36827;&#20869;&#22312;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Intrinsic Exploration by Creating Stationary Objectives. (arXiv:2310.18144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#22870;&#21169;&#36890;&#36807;&#23450;&#20041;&#33258;&#23450;&#20041;&#30340;&#20869;&#22312;&#30446;&#26631;&#26469;&#24341;&#23548;&#38271;&#26399;&#25506;&#32034;&#12290;&#22522;&#20110;&#35745;&#25968;&#30340;&#26041;&#27861;&#20351;&#29992;&#29366;&#24577;&#35775;&#38382;&#39057;&#29575;&#26469;&#33719;&#24471;&#25506;&#32034;&#22870;&#21169;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20219;&#20309;&#20174;&#22522;&#20110;&#35745;&#25968;&#30340;&#26041;&#27861;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#37117;&#26159;&#38750;&#22266;&#23450;&#30340;&#65292;&#22240;&#27492;&#20026;&#20195;&#29702;&#20154;&#26500;&#24314;&#20102;&#19968;&#20010;&#38590;&#20197;&#20248;&#21270;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;&#22686;&#24378;&#29366;&#24577;&#34920;&#31034;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#25506;&#32034;&#30340;&#22266;&#23450;&#30446;&#26631;&#65288;SOFE&#65289;&#26694;&#26550;&#12290;SOFE&#38656;&#35201;&#35782;&#21035;&#19981;&#21516;&#25506;&#32034;&#22870;&#21169;&#30340;&#36275;&#22815;&#32479;&#35745;&#37327;&#65292;&#24182;&#25214;&#21040;&#19968;&#31181;&#23558;&#36825;&#20123;&#32479;&#35745;&#37327;&#39640;&#25928;&#32534;&#30721;&#20316;&#20026;&#28145;&#24230;&#32593;&#32476;&#36755;&#20837;&#30340;&#26041;&#27861;&#12290;SOFE&#22522;&#20110;&#25552;&#20986;&#25193;&#23637;&#29366;&#24577;&#31354;&#38388;&#30340;&#29366;&#24577;&#22686;&#24378;&#65292;&#20294;&#26377;&#24076;&#26395;&#31616;&#21270;&#20195;&#29702;&#30446;&#26631;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SOFE&#25913;&#21892;&#20102;&#25506;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21487;&#21464;&#24418;&#24418;&#29366;&#38598;&#21512;&#30340;&#32534;&#30721;&#21644;&#25805;&#20316;&#12290;&#36890;&#36807;&#35889;&#27744;&#21270;&#25216;&#26415;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#30340;&#28508;&#31354;&#38388;&#65292;&#21487;&#20197;&#23545;&#22810;&#26679;&#21270;&#30340;&#32593;&#26684;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18141</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#34920;&#31034;&#23398;&#20064;&#22312;&#19981;&#21516;&#24418;&#21464;&#24418;&#29366;&#38598;&#21512;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Representation Learning for Diverse Deformable Shape Collections. (arXiv:2310.18141v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21487;&#21464;&#24418;&#24418;&#29366;&#38598;&#21512;&#30340;&#32534;&#30721;&#21644;&#25805;&#20316;&#12290;&#36890;&#36807;&#35889;&#27744;&#21270;&#25216;&#26415;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#30340;&#28508;&#31354;&#38388;&#65292;&#21487;&#20197;&#23545;&#22810;&#26679;&#21270;&#30340;&#32593;&#26684;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32534;&#30721;&#21644;&#25805;&#20316;3D&#34920;&#38754;&#32593;&#26684;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#26469;&#21019;&#24314;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#29992;&#20110;&#22788;&#29702;&#21487;&#21464;&#24418;&#30340;&#24418;&#29366;&#38598;&#21512;&#12290;&#19982;&#20043;&#21069;&#30340;3D&#32593;&#26684;&#33258;&#32534;&#30721;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#30340;&#26465;&#20214;&#19979;&#23545;&#22810;&#26679;&#21270;&#30340;&#32593;&#26684;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#35889;&#27744;&#21270;&#25216;&#26415;&#65292;&#23427;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#28508;&#31354;&#38388;&#65292;&#25171;&#30772;&#20102;&#20256;&#32479;&#32593;&#26684;&#36830;&#36890;&#24615;&#21644;&#24418;&#29366;&#31867;&#21035;&#30340;&#32422;&#26463;&#12290;&#25972;&#20010;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#20989;&#25968;&#26144;&#23556;&#33539;&#24335;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#25552;&#21462;&#19968;&#32452;&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#28857;&#26144;&#23556;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#28857;&#23545;&#28857;&#26144;&#23556;&#26500;&#24314;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#31354;&#38388;&#65292;&#20174;&#32780;&#30830;&#20445;&#20102;&#30452;&#35266;&#35299;&#37322;&#21644;&#29420;&#31435;&#20110;&#32593;&#26684;&#36830;&#36890;&#24615;&#21644;&#24418;&#29366;&#31867;&#21035;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel learning-based method for encoding and manipulating 3D surface meshes. Our method is specifically designed to create an interpretable embedding space for deformable shape collections. Unlike previous 3D mesh autoencoders that require meshes to be in a 1-to-1 correspondence, our approach is trained on diverse meshes in an unsupervised manner. Central to our method is a spectral pooling technique that establishes a universal latent space, breaking free from traditional constraints of mesh connectivity and shape categories. The entire process consists of two stages. In the first stage, we employ the functional map paradigm to extract point-to-point (p2p) maps between a collection of shapes in an unsupervised manner. These p2p maps are then utilized to construct a common latent space, which ensures straightforward interpretation and independence from mesh connectivity and shape category. Through extensive experiments, we demonstrate that our method achieves excellent r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#24182;&#36827;&#34892;&#25512;&#29702;&#26469;&#25351;&#23548;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.18127</link><description>&lt;p&gt;
&#25552;&#38382;&#26356;&#22810;&#65292;&#20102;&#35299;&#26356;&#22810;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#38382;&#39064;&#19982;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models. (arXiv:2310.18127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#24182;&#36827;&#34892;&#25512;&#29702;&#26469;&#25351;&#23548;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23558;&#22522;&#20110;&#34892;&#21160;&#30340;&#31574;&#30053;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#22797;&#26434;&#23454;&#38469;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#26469;&#35828;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#36825;&#20123;&#25552;&#31034;&#26159;&#36890;&#36807;&#24191;&#27867;&#20351;&#29992;&#20154;&#21147;&#25163;&#24037;&#21046;&#20316;&#30340;&#65292;&#23548;&#33268;CoT&#31574;&#30053;&#32463;&#24120;&#26080;&#27861;&#25512;&#24191;&#12290;&#20026;&#20102;&#30830;&#20445;&#20302;&#23618;&#25511;&#21046;&#22120;&#36866;&#24403;&#22320;&#22788;&#29702;CoT&#25512;&#29702;&#65292;&#36824;&#38656;&#35201;&#20154;&#20026;&#20171;&#20837;&#26469;&#24320;&#21457;&#25509;&#22320;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#36808;&#21521;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#24212;&#29992;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#35299;&#20915;&#30340;&#23436;&#20840;&#38598;&#25104;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#21452;&#23618;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#65288;&#25552;&#31034;&#65289;&#65292;&#24182;&#38543;&#21518;&#36827;&#34892;&#25512;&#29702;&#65292;&#25351;&#23548;&#22312;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#19968;&#20010;&#22909;&#30340;&#25552;&#31034;&#24212;&#35813;&#22522;&#20110;&#21382;&#21490;&#30340;&#33258;&#30465;&#24615;&#20462;&#35746;&#26469;&#36827;&#34892;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, these prompts are handcrafted utilizing extensive human labor, resulting in CoT policies that frequently fail to generalize. Human intervention is also required in order to develop grounding functions that ensure low-level controllers appropriately process CoT reasoning. In this paper, we take the first step towards a fully integrated end-to-end framework for task-solving in real settings employing complicated reasoning. To that purpose, we offer a new leader-follower bilevel framework capable of learning to ask relevant questions (prompts) and subsequently undertaking reasoning to guide the learning of actions to be performed in an environment. A good prompt should make introspective revisions based on historical fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#23545;score-matching&#30340;&#20934;&#30830;&#20272;&#35745;&#65292;&#24182;&#22312;&#22240;&#26524;&#21457;&#29616;&#21644;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#25552;&#20379;&#20102;&#32479;&#35745;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.18123</link><description>&lt;p&gt;
score-matching&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65306;&#22240;&#26524;&#21457;&#29616;&#21644;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity Bounds for Score-Matching: Causal Discovery and Generative Modeling. (arXiv:2310.18123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#23545;score-matching&#30340;&#20934;&#30830;&#20272;&#35745;&#65292;&#24182;&#22312;&#22240;&#26524;&#21457;&#29616;&#21644;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#25552;&#20379;&#20102;&#32479;&#35745;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;score-matching&#21450;&#20854;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#30340;&#32479;&#35745;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#26631;&#20934;&#30340;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#23545;&#20998;&#25968;&#20989;&#25968;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;&#22312;&#20551;&#23450;&#20998;&#25968;&#20989;&#25968;&#30340;&#20272;&#35745;&#36275;&#22815;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;&#22522;&#20110;score-matching&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#30340;&#38169;&#35823;&#29575;&#24314;&#31435;&#20102;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;score-matching&#20272;&#35745;&#30340;&#19978;&#30028;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#20063;&#22312;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#20855;&#26377;&#29420;&#31435;&#30340;&#30740;&#31350;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides statistical sample complexity bounds for score-matching and its applications in causal discovery. We demonstrate that accurate estimation of the score function is achievable by training a standard deep ReLU neural network using stochastic gradient descent. We establish bounds on the error rate of recovering causal relationships using the score-matching-based causal discovery method of Rolland et al. [2022], assuming a sufficiently good estimation of the score function. Finally, we analyze the upper bound of score-matching estimation within the score-based generative modeling, which has been applied for causal discovery but is also of independent interest within the domain of generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#25104;&#26412;&#39063;&#31890;&#29289;&#65288;PM&#65289;&#20256;&#24863;&#22120;&#30340;&#20840;&#29699;&#22810;&#21333;&#20803;&#26657;&#20934;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#39063;&#31890;&#29289;&#30417;&#27979;&#31995;&#32479;&#30340;&#37096;&#32626;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37319;&#38598;&#26377;&#38480;&#25968;&#37327;&#30340;&#29289;&#32852;&#32593;AQ&#22810;&#20256;&#24863;&#22120;&#21333;&#20803;&#30340;&#29616;&#22330;&#21709;&#24212;&#21644;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27010;&#24565;&#65292;&#33021;&#22815;&#35299;&#20915;&#29615;&#22659;&#24178;&#25200;&#21644;&#21046;&#36896;&#24046;&#24322;&#23548;&#33268;&#30340;&#26657;&#20934;&#25104;&#26412;&#38382;&#39064;&#65292;&#23454;&#29616;&#20934;&#30830;&#21644;&#26222;&#36941;&#30340;&#31354;&#27668;&#36136;&#37327;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.18118</link><description>&lt;p&gt;
&#19968;&#20010;&#20840;&#29699;&#22810;&#21333;&#20803;&#26657;&#20934;&#26041;&#27861;&#20316;&#20026;&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#39063;&#31890;&#29289;&#30417;&#27979;&#31995;&#32479;&#37096;&#32626;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Global Multi-Unit Calibration as a Method for Large Scale IoT Particulate Matter Monitoring Systems Deployments. (arXiv:2310.18118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#25104;&#26412;&#39063;&#31890;&#29289;&#65288;PM&#65289;&#20256;&#24863;&#22120;&#30340;&#20840;&#29699;&#22810;&#21333;&#20803;&#26657;&#20934;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#39063;&#31890;&#29289;&#30417;&#27979;&#31995;&#32479;&#30340;&#37096;&#32626;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37319;&#38598;&#26377;&#38480;&#25968;&#37327;&#30340;&#29289;&#32852;&#32593;AQ&#22810;&#20256;&#24863;&#22120;&#21333;&#20803;&#30340;&#29616;&#22330;&#21709;&#24212;&#21644;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27010;&#24565;&#65292;&#33021;&#22815;&#35299;&#20915;&#29615;&#22659;&#24178;&#25200;&#21644;&#21046;&#36896;&#24046;&#24322;&#23548;&#33268;&#30340;&#26657;&#20934;&#25104;&#26412;&#38382;&#39064;&#65292;&#23454;&#29616;&#20934;&#30830;&#21644;&#26222;&#36941;&#30340;&#31354;&#27668;&#36136;&#37327;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#21644;&#26377;&#25928;&#30340;&#26657;&#20934;&#26159;&#20302;&#25104;&#26412;&#31354;&#27668;&#36136;&#37327;&#30417;&#27979;&#31995;&#32479;&#30340;&#22522;&#26412;&#35201;&#27714;&#65292;&#23558;&#20351;&#24471;&#22312;&#22478;&#24066;&#20013;&#33021;&#22815;&#36827;&#34892;&#20934;&#30830;&#21644;&#26222;&#36941;&#30340;&#30417;&#27979;&#12290;&#21463;&#29615;&#22659;&#24178;&#25200;&#21644;&#21046;&#36896;&#24046;&#24322;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#35774;&#22791;&#38656;&#35201;&#21253;&#25324;&#20256;&#24863;&#22120;&#29305;&#23450;&#30340;&#21644;&#22797;&#26434;&#30340;&#26657;&#20934;&#27969;&#31243;&#65292;&#20197;&#36798;&#21040;&#36275;&#22815;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#20415;&#20316;&#20026;&#25351;&#31034;&#24615;&#27979;&#37327;&#35774;&#22791;&#37096;&#32626;&#22312;&#31354;&#27668;&#36136;&#37327;&#65288;AQ&#65289;&#30417;&#27979;&#32593;&#32476;&#20013;&#12290;&#27010;&#24565;&#21644;&#20256;&#24863;&#22120;&#28418;&#31227;&#32463;&#24120;&#24378;&#21046;&#36827;&#34892;&#39057;&#32321;&#30340;&#26657;&#20934;&#36807;&#31243;&#12290;&#36825;&#20123;&#38382;&#39064;&#23548;&#33268;&#26080;&#27861;&#25215;&#21463;&#30340;&#26657;&#20934;&#25104;&#26412;&#65292;&#20174;&#32780;&#38459;&#27490;&#20102;&#23427;&#20204;&#22312;&#20851;&#27880;&#20934;&#30830;&#24615;&#26102;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#25104;&#26412;&#39063;&#31890;&#29289;&#65288;PM&#65289;&#20256;&#24863;&#22120;&#30340;&#38646;&#20256;&#36755;&#26679;&#26412;&#30340;&#20840;&#23616;&#26657;&#20934;&#26041;&#27861;&#65292;&#20316;&#20026;&#29289;&#32852;&#32593;AQ&#22810;&#24863;&#30693;&#35774;&#22791;&#30340;&#25216;&#26415;&#23454;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#29289;&#32852;&#32593;AQ&#22810;&#20256;&#24863;&#22120;&#21333;&#20803;&#20013;&#35760;&#24405;&#30340;&#29616;&#22330;&#21709;&#24212;&#21644;&#26426;&#22120;&#23398;&#20064;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scalable and effective calibration is a fundamental requirement for Low Cost Air Quality Monitoring Systems and will enable accurate and pervasive monitoring in cities. Suffering from environmental interferences and fabrication variance, these devices need to encompass sensors specific and complex calibration processes for reaching a sufficient accuracy to be deployed as indicative measurement devices in Air Quality (AQ) monitoring networks. Concept and sensor drift often force calibration process to be frequently repeated. These issues lead to unbearable calibration costs which denies their massive deployment when accuracy is a concern. In this work, We propose a zero transfer samples, global calibration methodology as a technological enabler for IoT AQ multisensory devices which relies on low cost Particulate Matter (PM) sensors. This methodology is based on field recorded responses from a limited number of IoT AQ multisensors units and machine learning concepts and can be universall
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#36716;&#23548;&#24335;&#19968;&#33268;&#25512;&#26029;&#26041;&#27861;&#36827;&#34892;&#19968;&#33268;&#24615;&#26080;&#20998;&#24067;&#20445;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;Polya&#29699;&#27169;&#22411;&#30340;&#32852;&#21512;&#20998;&#24067;&#21644;&#32463;&#39564;&#20998;&#24067;&#20989;&#25968;&#30340;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#25552;&#20379;&#20102;&#33258;&#36866;&#24212;&#24471;&#20998;&#30340;&#21487;&#29992;&#24615;&#21644;&#26356;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18108</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#36866;&#24212;&#24471;&#20998;&#30340;&#36716;&#23548;&#24335;&#19968;&#33268;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Transductive conformal inference with adaptive scores. (arXiv:2310.18108v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18108
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36716;&#23548;&#24335;&#19968;&#33268;&#25512;&#26029;&#26041;&#27861;&#36827;&#34892;&#19968;&#33268;&#24615;&#26080;&#20998;&#24067;&#20445;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;Polya&#29699;&#27169;&#22411;&#30340;&#32852;&#21512;&#20998;&#24067;&#21644;&#32463;&#39564;&#20998;&#24067;&#20989;&#25968;&#30340;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#25552;&#20379;&#20102;&#33258;&#36866;&#24212;&#24471;&#20998;&#30340;&#21487;&#29992;&#24615;&#21644;&#26356;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#25512;&#26029;&#26159;&#19968;&#31181;&#22522;&#26412;&#19988;&#22810;&#29992;&#36884;&#30340;&#24037;&#20855;&#65292;&#20026;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#26080;&#20998;&#24067;&#20445;&#35777;&#12290;&#25105;&#20204;&#32771;&#34385;&#36716;&#23548;&#24335;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#38024;&#23545;$m$&#20010;&#26032;&#26679;&#26412;&#36827;&#34892;&#20915;&#31574;&#65292;&#20135;&#29983;$m$&#20010;&#19968;&#33268;&#25512;&#26029;$p$&#20540;&#12290;&#34429;&#28982;&#32463;&#20856;&#32467;&#26524;&#20165;&#28041;&#21450;&#20854;&#36793;&#38469;&#20998;&#24067;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#23427;&#20204;&#30340;&#32852;&#21512;&#20998;&#24067;&#36981;&#24490;&#19968;&#20010;Polya&#29699;&#27169;&#22411;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#30340;&#32463;&#39564;&#20998;&#24067;&#20989;&#25968;&#30340;&#27987;&#24230;&#19981;&#31561;&#24335;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#20219;&#24847;&#21487;&#20132;&#25442;&#30340;&#24471;&#20998;&#65292;&#21253;&#25324;&#21487;&#20197;&#22312;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#27979;&#35797;+&#26657;&#20934;&#26679;&#26412;&#30340;&#21327;&#21464;&#37327;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#8220;&#33258;&#36866;&#24212;&#8221;&#24471;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#24403;&#21069;&#24863;&#20852;&#36259;&#30340;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65288;&#36716;&#23548;&#24335;&#36801;&#31227;&#23398;&#20064;&#30340;&#21306;&#38388;&#39044;&#27979;&#21644;&#22522;&#20110;&#20004;&#31867;&#20998;&#31867;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#65289;&#25552;&#20379;&#32479;&#19968;&#19988;&#22312;&#27010;&#29575;&#19978;&#30340;&#20445;&#35777;&#26469;&#28436;&#31034;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal inference is a fundamental and versatile tool that provides distribution-free guarantees for many machine learning tasks. We consider the transductive setting, where decisions are made on a test sample of $m$ new points, giving rise to $m$ conformal $p$-values. {While classical results only concern their marginal distribution, we show that their joint distribution follows a P\'olya urn model, and establish a concentration inequality for their empirical distribution function.} The results hold for arbitrary exchangeable scores, including {\it adaptive} ones that can use the covariates of the test+calibration samples at training stage for increased accuracy. We demonstrate the usefulness of these theoretical results through uniform, in-probability guarantees for two machine learning tasks of current interest: interval prediction for transductive transfer learning and novelty detection based on two-class classification.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20808;&#39564;&#21644;&#38750;&#32447;&#24615;&#24322;&#24120;&#20998;&#25968;&#30340;&#23545;&#25239;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;$\beta$-VAE&#30340;&#29983;&#25104;&#31283;&#23450;&#24615;&#21644;GAN&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;$\beta$-VAEGAN&#65292;&#24182;&#23545;&#24322;&#24120;&#20998;&#25968;&#30340;&#32452;&#21512;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#36890;&#36807;&#35757;&#32451;&#26680;&#21270;SVM&#65292;&#32771;&#34385;&#20102;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21516;&#26102;&#21033;&#29992;$\beta$-VAEGAN&#23545;&#39640;&#26031;&#20808;&#39564;&#30340;&#20559;&#24046;&#24418;&#25104;&#20102;&#26032;&#30340;&#24322;&#24120;&#20998;&#25968;&#32452;&#20214;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#19978;&#26377;&#25152;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.18091</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#20808;&#39564;&#21644;&#38750;&#32447;&#24615;&#24322;&#24120;&#20998;&#25968;&#30340;&#23545;&#25239;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adversarial Anomaly Detection using Gaussian Priors and Nonlinear Anomaly Scores. (arXiv:2310.18091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18091
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20808;&#39564;&#21644;&#38750;&#32447;&#24615;&#24322;&#24120;&#20998;&#25968;&#30340;&#23545;&#25239;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;$\beta$-VAE&#30340;&#29983;&#25104;&#31283;&#23450;&#24615;&#21644;GAN&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;$\beta$-VAEGAN&#65292;&#24182;&#23545;&#24322;&#24120;&#20998;&#25968;&#30340;&#32452;&#21512;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#36890;&#36807;&#35757;&#32451;&#26680;&#21270;SVM&#65292;&#32771;&#34385;&#20102;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21516;&#26102;&#21033;&#29992;$\beta$-VAEGAN&#23545;&#39640;&#26031;&#20808;&#39564;&#30340;&#20559;&#24046;&#24418;&#25104;&#20102;&#26032;&#30340;&#24322;&#24120;&#20998;&#25968;&#32452;&#20214;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#19978;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#39057;&#32321;&#19988;&#20851;&#38190;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#33719;&#21462;&#21644;&#26631;&#35760;&#24322;&#24120;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#12290;&#23558;$\beta$&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#29983;&#25104;&#31283;&#23450;&#24615;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#21028;&#21035;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;$\beta$-VAEGAN&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#25105;&#20204;&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#21644;&#37325;&#26500;&#33021;&#21147;&#30340;&#24322;&#24120;&#20998;&#25968;&#30340;&#32452;&#21512;&#26041;&#27861;&#12290;&#29616;&#26377;&#24037;&#20316;&#38598;&#20013;&#20110;&#32447;&#24615;&#32452;&#21512;&#36825;&#20123;&#32452;&#20214;&#26469;&#30830;&#23450;&#25968;&#25454;&#26159;&#21542;&#24322;&#24120;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#30456;&#24212;&#30340;&#38169;&#35823;&#32452;&#20214;&#19978;&#35757;&#32451;&#26680;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#24182;&#32771;&#34385;&#20102;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#36825;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#26356;&#24555;&#30340;&#20248;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;$\beta$-VAEGAN&#23545;&#20110;&#39640;&#26031;&#20808;&#39564;&#30340;&#20559;&#24046;&#24418;&#25104;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24322;&#24120;&#20998;&#25968;&#32452;&#20214;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in imbalanced datasets is a frequent and crucial problem, especially in the medical domain where retrieving and labeling irregularities is often expensive. By combining the generative stability of a $\beta$-variational autoencoder (VAE) with the discriminative strengths of generative adversarial networks (GANs), we propose a novel model, $\beta$-VAEGAN. We investigate methods for composing anomaly scores based on the discriminative and reconstructive capabilities of our model. Existing work focuses on linear combinations of these components to determine if data is anomalous. We advance existing work by training a kernelized support vector machine (SVM) on the respective error components to also consider nonlinear relationships. This improves anomaly detection performance, while allowing faster optimization. Lastly, we use the deviations from the Gaussian prior of $\beta$-VAEGAN to form a novel anomaly score component. In comparison to state-of-the-art work, we improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#27010;&#29575;&#24314;&#27169;&#24341;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23545;&#24615;&#33021;&#12289;&#20449;&#24687;&#21387;&#32553;&#21644;&#36229;&#20986;&#20998;&#24067;&#35782;&#21035;&#28508;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#25581;&#31034;&#20102;&#20449;&#24687;&#21387;&#32553;&#21644;&#20445;&#30041;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.18080</link><description>&lt;p&gt;
&#25581;&#31034;&#27010;&#29575;&#23884;&#20837;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Potential of Probabilistic Embeddings in Self-Supervised Learning. (arXiv:2310.18080v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#27010;&#29575;&#24314;&#27169;&#24341;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23545;&#24615;&#33021;&#12289;&#20449;&#24687;&#21387;&#32553;&#21644;&#36229;&#20986;&#20998;&#24067;&#35782;&#21035;&#28508;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#25581;&#31034;&#20102;&#20449;&#24687;&#21387;&#32553;&#21644;&#20445;&#30041;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#33719;&#21462;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#20026;&#25512;&#21160;&#26426;&#22120;&#23398;&#20064;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#30740;&#31350;&#26041;&#21521;&#26159;&#22312;&#20449;&#24687;&#35770;&#26694;&#26550;&#20869;&#24320;&#21457;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#22312;&#25512;&#23548;&#30446;&#26631;&#26102;&#24448;&#24448;&#20559;&#31163;&#20102;&#38543;&#26426;&#24615;&#20551;&#35774;&#12290;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26126;&#30830;&#22320;&#29992;&#38543;&#26426;&#23884;&#20837;&#26469;&#24314;&#27169;&#34920;&#31034;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#24615;&#33021;&#12289;&#20449;&#24687;&#21387;&#32553;&#21644;&#35782;&#21035;&#36229;&#20986;&#20998;&#24067;&#30340;&#28508;&#21147;&#30340;&#24433;&#21709;&#12290;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#35797;&#22270;&#30740;&#31350;&#27010;&#29575;&#24314;&#27169;&#23545;&#20449;&#24687;&#29942;&#39048;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#22312;&#34920;&#31034;&#31354;&#38388;&#21644;&#25439;&#22833;&#31354;&#38388;&#20013;&#20449;&#24687;&#21387;&#32553;&#21644;&#20449;&#24687;&#20445;&#30041;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#24378;&#35843;&#21306;&#20998;&#36825;&#20004;&#20010;&#31354;&#38388;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#32422;&#26463;&#20854;&#20013;&#19968;&#20010;&#31354;&#38388;&#20250;&#24433;&#21709;&#21040;&#21478;&#19968;&#20010;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, self-supervised learning has played a pivotal role in advancing machine learning by allowing models to acquire meaningful representations from unlabeled data. An intriguing research avenue involves developing self-supervised models within an information-theoretic framework, but many studies often deviate from the stochasticity assumptions made when deriving their objectives. To gain deeper insights into this issue, we propose to explicitly model the representation with stochastic embeddings and assess their effects on performance, information compression and potential for out-of-distribution detection. From an information-theoretic perspective, we seek to investigate the impact of probabilistic modeling on the information bottleneck, shedding light on a trade-off between compression and preservation of information in both representation and loss space. Emphasizing the importance of distinguishing between these two spaces, we demonstrate how constraining one can affect 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30740;&#31350;&#20102;Lipschitz&#21644;H&#246;lder&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20010;&#20805;&#20998;&#26465;&#20214;&#21644;&#23545;&#20877;&#29983;&#26680;&#30340;&#28145;&#24230;&#35843;&#26597;&#12290;&#36825;&#20010;&#24037;&#20316;&#20063;&#26159;&#36825;&#20010;&#20027;&#39064;&#30340;&#26041;&#20415;&#21442;&#32771;&#36164;&#26009;&#12290;</title><link>http://arxiv.org/abs/2310.18078</link><description>&lt;p&gt;
&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;Lipschitz&#21644;H&#246;lder&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
Lipschitz and H\"older Continuity in Reproducing Kernel Hilbert Spaces. (arXiv:2310.18078v1 [math.FA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30740;&#31350;&#20102;Lipschitz&#21644;H&#246;lder&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20010;&#20805;&#20998;&#26465;&#20214;&#21644;&#23545;&#20877;&#29983;&#26680;&#30340;&#28145;&#24230;&#35843;&#26597;&#12290;&#36825;&#20010;&#24037;&#20316;&#20063;&#26159;&#36825;&#20010;&#20027;&#39064;&#30340;&#26041;&#20415;&#21442;&#32771;&#36164;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#20989;&#25968;&#31354;&#38388;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#32479;&#35745;&#23398;&#12289;&#25968;&#20540;&#20998;&#26512;&#21644;&#32431;&#25968;&#23398;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#30001;&#20110;Lipschitz&#21644;H&#246;lder&#36830;&#32493;&#24615;&#26159;&#37325;&#35201;&#30340;&#27491;&#21017;&#24615;&#36136;&#65292;&#22312;&#25554;&#20540;&#12289;&#36924;&#36817;&#21644;&#20248;&#21270;&#38382;&#39064;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;RKHS&#20013;&#36825;&#20123;&#36830;&#32493;&#24615;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20010;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#21450;&#23545;&#24341;&#36215;&#39044;&#23450;Lipschitz&#25110;H&#246;lder&#36830;&#32493;&#24615;&#30340;&#20877;&#29983;&#26680;&#30340;&#28145;&#24230;&#35843;&#26597;&#12290;&#38500;&#20102;&#26032;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#20174;&#25991;&#29486;&#20013;&#24635;&#32467;&#20102;&#30456;&#20851;&#24050;&#30693;&#32467;&#26524;&#65292;&#20351;&#26412;&#24037;&#20316;&#20063;&#25104;&#20026;&#36825;&#20010;&#20027;&#39064;&#30340;&#26041;&#20415;&#21442;&#32771;&#36164;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducing kernel Hilbert spaces (RKHSs) are very important function spaces, playing an important role in machine learning, statistics, numerical analysis and pure mathematics. Since Lipschitz and H\"older continuity are important regularity properties, with many applications in interpolation, approximation and optimization problems, in this work we investigate these continuity notion in RKHSs. We provide several sufficient conditions as well as an in depth investigation of reproducing kernels inducing prescribed Lipschitz or H\"older continuity. Apart from new results, we also collect related known results from the literature, making the present work also a convenient reference on this topic.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#30340;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#26680;&#32479;&#35745;&#23398;&#20064;&#65292;&#21253;&#25324;&#26680;&#30340;&#22343;&#22330;&#26497;&#38480;&#30340;&#29702;&#35770;&#23436;&#21892;&#12289;&#36924;&#36817;&#20197;&#21450;&#25903;&#25345;&#21521;&#37327;&#26426;&#31561;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#22823;&#35268;&#27169;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#24037;&#20855;&#21644;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.18074</link><description>&lt;p&gt;
&#20851;&#20110;&#22343;&#22330;&#26497;&#38480;&#20013;&#22522;&#20110;&#26680;&#30340;&#32479;&#35745;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On kernel-based statistical learning in the mean field limit. (arXiv:2310.18074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18074
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#30340;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#26680;&#32479;&#35745;&#23398;&#20064;&#65292;&#21253;&#25324;&#26680;&#30340;&#22343;&#22330;&#26497;&#38480;&#30340;&#29702;&#35770;&#23436;&#21892;&#12289;&#36924;&#36817;&#20197;&#21450;&#25903;&#25345;&#21521;&#37327;&#26426;&#31561;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#22823;&#35268;&#27169;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#24037;&#20855;&#21644;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#32771;&#34385;&#20102;&#22823;&#37327;&#30340;&#21464;&#37327;&#12290;&#21463;&#20132;&#20114;&#31890;&#23376;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21551;&#21457;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36755;&#20837;&#21464;&#37327;&#25968;&#37327;&#36235;&#20110;&#26080;&#31351;&#22823;&#30340;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#26680;&#21450;&#20854;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#22343;&#22330;&#26497;&#38480;&#65292;&#23436;&#21892;&#20102;&#29616;&#26377;&#29702;&#35770;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19982;&#22343;&#22330;&#26497;&#38480;&#19979;&#36825;&#20123;&#26680;&#30340;&#36924;&#36817;&#30456;&#20851;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#19968;&#20010;&#34920;&#29616;&#23450;&#29702;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#26680;&#24212;&#29992;&#20110;&#22312;&#22343;&#22330;&#26497;&#38480;&#20013;&#30340;&#32479;&#35745;&#23398;&#20064;&#65292;&#37325;&#28857;&#20851;&#27880;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#39564;&#21644;&#26080;&#31351;&#26679;&#26412;&#35299;&#30340;&#22343;&#22330;&#25910;&#25947;&#20197;&#21450;&#30456;&#24212;&#39118;&#38505;&#30340;&#25910;&#25947;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#26680;&#26041;&#27861;&#30340;&#32972;&#26223;&#19979;&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;&#22343;&#22330;&#26497;&#38480;&#65292;&#20026;&#22823;&#35268;&#27169;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#24037;&#20855;&#21644;&#35265;&#35299;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#35774;&#32622;&#23545;&#24212;&#20110;...
&lt;/p&gt;
&lt;p&gt;
In many applications of machine learning, a large number of variables are considered. Motivated by machine learning of interacting particle systems, we consider the situation when the number of input variables goes to infinity. First, we continue the recent investigation of the mean field limit of kernels and their reproducing kernel Hilbert spaces, completing the existing theory. Next, we provide results relevant for approximation with such kernels in the mean field limit, including a representer theorem. Finally, we use these kernels in the context of statistical learning in the mean field limit, focusing on Support Vector Machines. In particular, we show mean field convergence of empirical and infinite-sample solutions as well as the convergence of the corresponding risks. On the one hand, our results establish rigorous mean field limits in the context of kernel methods, providing new theoretical tools and insights for large-scale problems. On the other hand, our setting corresponds
&lt;/p&gt;</description></item><item><title>Therapy&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#25991;&#26412;&#30340;&#20840;&#23616;&#21644;&#26080;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#29983;&#25104;&#25991;&#26412;&#65292;&#19981;&#20381;&#36182;&#20110;&#21021;&#22987;&#26679;&#26412;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#36755;&#20837;&#31354;&#38388;&#20013;&#27169;&#22411;&#34892;&#20026;&#30340;&#20840;&#23616;&#27010;&#35272;&#12290;</title><link>http://arxiv.org/abs/2310.18063</link><description>&lt;p&gt;
&#8220;&#20146;&#29233;&#30340;&#65292;&#21578;&#35785;&#25105;&#20986;&#20102;&#20160;&#20040;&#38382;&#39064;&#8221;&#65292;&#36890;&#36807;&#21512;&#20316;&#29983;&#25104;&#20840;&#23616;&#25991;&#26412;&#36776;&#21035;&#27169;&#22411;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
"Honey, Tell Me What's Wrong", Global Explanation of Textual Discriminative Models through Cooperative Generation. (arXiv:2310.18063v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18063
&lt;/p&gt;
&lt;p&gt;
Therapy&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#25991;&#26412;&#30340;&#20840;&#23616;&#21644;&#26080;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#29983;&#25104;&#25991;&#26412;&#65292;&#19981;&#20381;&#36182;&#20110;&#21021;&#22987;&#26679;&#26412;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#36755;&#20837;&#31354;&#38388;&#20013;&#27169;&#22411;&#34892;&#20026;&#30340;&#20840;&#23616;&#27010;&#35272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#30340;&#26222;&#21450;&#25552;&#39640;&#20102;&#26080;&#27169;&#22411;&#35299;&#37322;&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#36731;&#24494;&#25200;&#21160;&#30495;&#23454;&#23454;&#20363;&#26469;&#21019;&#24314;&#20154;&#24037;&#23454;&#20363;&#65292;&#25429;&#25417;&#27169;&#22411;&#20915;&#31574;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#21021;&#22987;&#25968;&#25454;&#65292;&#24182;&#19988;&#21482;&#25552;&#20379;&#20851;&#20110;&#36825;&#20123;&#21021;&#22987;&#25968;&#25454;&#20915;&#31574;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Therapy&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#25991;&#26412;&#30340;&#20840;&#23616;&#21644;&#26080;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#36755;&#20837;&#25968;&#25454;&#38598;&#12290;Therapy&#36890;&#36807;&#21512;&#20316;&#29983;&#25104;&#65292;&#26681;&#25454;&#20998;&#31867;&#22120;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#29983;&#25104;&#25991;&#26412;&#12290;&#22240;&#20026;&#23427;&#19981;&#20381;&#36182;&#20110;&#21021;&#22987;&#26679;&#26412;&#65292;&#25152;&#20197;&#21363;&#20351;&#25968;&#25454;&#32570;&#22833;&#65288;&#20363;&#22914;&#22240;&#20445;&#23494;&#21407;&#22240;&#65289;&#65292;&#20063;&#33021;&#29983;&#25104;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#19982;&#23558;&#22810;&#20010;&#23616;&#37096;&#35299;&#37322;&#32452;&#21512;&#25104;&#19968;&#20010;&#20840;&#23616;&#35299;&#37322;&#30340;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;Therapy&#25552;&#20379;&#20102;&#23545;&#36755;&#20837;&#31354;&#38388;&#20013;&#27169;&#22411;&#34892;&#20026;&#30340;&#20840;&#23616;&#27010;&#35272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#34429;&#28982;&#19981;&#20351;&#29992;&#36755;&#20837;&#25968;&#25454;&#26469;&#29983;&#25104;&#26679;&#26412;&#65292;&#20294; Therapy &#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ubiquity of complex machine learning has raised the importance of model-agnostic explanation algorithms. These methods create artificial instances by slightly perturbing real instances, capturing shifts in model decisions. However, such methods rely on initial data and only provide explanations of the decision for these. To tackle these problems, we propose Therapy, the first global and model-agnostic explanation method adapted to text which requires no input dataset. Therapy generates texts following the distribution learned by a classifier through cooperative generation. Because it does not rely on initial samples, it allows to generate explanations even when data is absent (e.g., for confidentiality reasons). Moreover, conversely to existing methods that combine multiple local explanations into a global one, Therapy offers a global overview of the model behavior on the input space. Our experiments show that although using no input data to generate samples, Therapy provides insig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26435;&#37325;&#21098;&#35009;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20844;&#20849;&#20449;&#24687;&#23545;&#20840;&#23616;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#28789;&#25935;&#24230;&#30028;&#38480;&#21644;&#22122;&#22768;&#27700;&#24179;&#35843;&#25972;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.18001</link><description>&lt;p&gt;
&#24102;&#26435;&#37325;&#21098;&#35009;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DP-SGD with weight clipping. (arXiv:2310.18001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26435;&#37325;&#21098;&#35009;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20844;&#20849;&#20449;&#24687;&#23545;&#20840;&#23616;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#28789;&#25935;&#24230;&#30028;&#38480;&#21644;&#22122;&#22768;&#27700;&#24179;&#35843;&#25972;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#20182;&#20381;&#36182;&#20110;&#30446;&#26631;&#20989;&#25968;&#20248;&#21270;&#30340;&#26041;&#27861;&#30340;&#39640;&#24230;&#27969;&#34892;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#20851;&#27880;&#65292;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#20026;&#20102;&#22312;&#25552;&#20379;&#26368;&#23567;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#38480;&#21046;&#21442;&#19982;&#32773;&#23558;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#30340;&#28789;&#25935;&#24230;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#26799;&#24230;&#21098;&#35009;&#20135;&#29983;&#30340;&#20559;&#24046;&#12290;&#36890;&#36807;&#21033;&#29992;&#20851;&#20110;&#24403;&#21069;&#20840;&#23616;&#27169;&#22411;&#21450;&#20854;&#22312;&#25628;&#32034;&#39046;&#22495;&#20013;&#20301;&#32622;&#30340;&#20844;&#20849;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#25913;&#36827;&#30340;&#26799;&#24230;&#30028;&#38480;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#28789;&#25935;&#24230;&#30830;&#23450;&#21644;&#22122;&#22768;&#27700;&#24179;&#35843;&#25972;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#22122;&#22768;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, due to the popularity of deep neural networks and other methods whose training typically relies on the optimization of an objective function, and due to concerns for data privacy, there is a lot of interest in differentially private gradient descent methods. To achieve differential privacy guarantees with a minimum amount of noise, it is important to be able to bound precisely the sensitivity of the information which the participants will observe. In this study, we present a novel approach that mitigates the bias arising from traditional gradient clipping. By leveraging public information concerning the current global model and its location within the search domain, we can achieve improved gradient bounds, leading to enhanced sensitivity determinations and refined noise level adjustments. We extend the state of the art algorithms, present improved differential privacy guarantees requiring less noise and present an empirical evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#31181;&#26032;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#20165;&#38656;&#28385;&#36275;$L$-&#24179;&#28369;&#26465;&#20214;&#21644;&#26377;&#30028;&#22122;&#22768;&#26041;&#24046;&#30340;&#20551;&#35774;&#65292;&#26469;&#24357;&#21512;Adam&#25910;&#25947;&#24615;&#30340;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#22312;&#21512;&#29702;&#36873;&#25321;&#30340;&#36229;&#21442;&#25968;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;Adam&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#65292;&#24182;&#19988;&#35777;&#26126;&#23427;&#28385;&#36275;&#19968;&#38454;&#20248;&#21270;&#22120;&#30340;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.17998</link><description>&lt;p&gt;
&#20851;&#20110;Adam&#36845;&#20195;&#22797;&#26434;&#24230;&#19978;&#19979;&#30028;&#30340;&#32553;&#23567;
&lt;/p&gt;
&lt;p&gt;
Closing the Gap Between the Upper Bound and the Lower Bound of Adam's Iteration Complexity. (arXiv:2310.17998v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#31181;&#26032;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#20165;&#38656;&#28385;&#36275;$L$-&#24179;&#28369;&#26465;&#20214;&#21644;&#26377;&#30028;&#22122;&#22768;&#26041;&#24046;&#30340;&#20551;&#35774;&#65292;&#26469;&#24357;&#21512;Adam&#25910;&#25947;&#24615;&#30340;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#22312;&#21512;&#29702;&#36873;&#25321;&#30340;&#36229;&#21442;&#25968;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;Adam&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#65292;&#24182;&#19988;&#35777;&#26126;&#23427;&#28385;&#36275;&#19968;&#38454;&#20248;&#21270;&#22120;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Arjevani&#31561;&#20154;[1]&#22312;$L$-&#24179;&#28369;&#26465;&#20214;&#21644;&#26377;&#30028;&#22122;&#22768;&#26041;&#24046;&#30340;&#20551;&#35774;&#19979;&#65292;&#20026;&#19968;&#38454;&#20248;&#21270;&#38382;&#39064;&#24314;&#31435;&#20102;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#19979;&#30028;&#12290;&#28982;&#32780;&#65292;&#23545;Adam&#25910;&#25947;&#24615;&#30340;&#29616;&#26377;&#25991;&#29486;&#36827;&#34892;&#24443;&#24213;&#22238;&#39038;&#21457;&#29616;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#65306;&#23427;&#20204;&#37117;&#26410;&#36798;&#21040;&#19978;&#36848;&#30340;&#19979;&#30028;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#20986;Adam&#30340;&#26032;&#25910;&#25947;&#20445;&#35777;&#65292;&#20165;&#38656;&#35201;$L$-&#24179;&#28369;&#26465;&#20214;&#21644;&#26377;&#30028;&#22122;&#22768;&#26041;&#24046;&#30340;&#20551;&#35774;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#33539;&#22260;&#12290;&#29305;&#21035;&#26159;&#22312;&#21512;&#29702;&#36873;&#25321;&#30340;&#36229;&#21442;&#25968;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;Adam&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#65292;&#24182;&#19988;&#35777;&#26126;&#23427;&#28385;&#36275;&#19968;&#38454;&#20248;&#21270;&#22120;&#30340;&#19979;&#30028;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20026;Adam&#30340;&#25910;&#25947;&#24615;&#24314;&#31435;&#22914;&#27492;&#32039;&#33268;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#21033;&#29992;&#20102;&#26032;&#39062;&#30340;&#25216;&#24039;&#26469;&#22788;&#29702;&#21160;&#37327;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#20043;&#38388;&#30340;&#20132;&#32455;&#38382;&#39064;&#65292;&#24182;&#23558;&#38477;&#35299;&#24341;&#29702;&#20013;&#30340;&#19968;&#38454;&#39033;&#36716;&#21270;&#20026;&#36739;&#31616;&#21333;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Arjevani et al. [1] established a lower bound of iteration complexity for the first-order optimization under an $L$-smooth condition and a bounded noise variance assumption. However, a thorough review of existing literature on Adam's convergence reveals a noticeable gap: none of them meet the above lower bound. In this paper, we close the gap by deriving a new convergence guarantee of Adam, with only an $L$-smooth condition and a bounded noise variance assumption. Our results remain valid across a broad spectrum of hyperparameters. Especially with properly chosen hyperparameters, we derive an upper bound of the iteration complexity of Adam and show that it meets the lower bound for first-order optimizers. To the best of our knowledge, this is the first to establish such a tight upper bound for Adam's convergence. Our proof utilizes novel techniques to handle the entanglement between momentum and adaptive learning rate and to convert the first-order term in the Descent Lemma t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;CEFL&#30340;&#30899;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#25104;&#26412;&#24863;&#30693;&#31574;&#30053;&#26469;&#20248;&#21270;FL&#27169;&#22411;&#35757;&#32451;&#30340;&#20219;&#24847;&#25104;&#26412;&#24230;&#37327;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#30899;&#25490;&#25918;&#20943;&#23569;93&#65285;&#21644;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;50&#65285;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.17972</link><description>&lt;p&gt;
CEFL&#65306;&#30899;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CEFL: Carbon-Efficient Federated Learning. (arXiv:2310.17972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17972
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;CEFL&#30340;&#30899;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#25104;&#26412;&#24863;&#30693;&#31574;&#30053;&#26469;&#20248;&#21270;FL&#27169;&#22411;&#35757;&#32451;&#30340;&#20219;&#24847;&#25104;&#26412;&#24230;&#37327;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#30899;&#25490;&#25918;&#20943;&#23569;93&#65285;&#21644;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;50&#65285;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35757;&#32451;&#20998;&#24067;&#22312;&#35768;&#22810;&#36793;&#32536;&#35774;&#22791;&#19978;&#65292;&#20197;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#21644;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#30001;&#20110;FL&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#28041;&#21450;&#25968;&#30334;&#19975;&#20010;&#35774;&#22791;&#65292;&#22240;&#27492;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#22240;&#27492;&#20043;&#21069;&#30340;&#24037;&#20316;&#19968;&#30452;&#33268;&#21147;&#20110;&#25552;&#39640;&#20854;&#36164;&#28304;&#25928;&#29575;&#20197;&#20248;&#21270;&#26102;&#38388;&#33267;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#23558;&#25152;&#26377;&#36164;&#28304;&#35270;&#20026;&#30456;&#21516;&#65292;&#32780;&#23454;&#38469;&#19978;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#22823;&#19981;&#30456;&#21516;&#30340;&#25104;&#26412;&#65292;&#36825;&#21453;&#32780;&#28608;&#21457;&#20102;&#20248;&#21270;&#25104;&#26412;&#33267;&#20934;&#30830;&#24615;&#30340;&#21160;&#26426;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;CEFL&#65292;&#23427;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#25104;&#26412;&#24863;&#30693;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#65292;&#22312;&#35757;&#32451;FL&#27169;&#22411;&#26102;&#20248;&#21270;&#20219;&#24847;&#25104;&#26412;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#25193;&#23637;&#24182;&#32467;&#21512;&#20102;&#22522;&#20110;&#25928;&#29992;&#30340;&#23458;&#25143;&#36873;&#25321;&#21644;&#20851;&#38190;&#23398;&#20064;&#26399;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#20351;&#20854;&#20855;&#26377;&#25104;&#26412;&#24863;&#30693;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#30899;&#39640;&#25928;&#30340;FL&#26469;&#28436;&#31034;CEFL&#65292;&#22312;&#36825;&#37324;&#33021;&#28304;&#30340;&#30899;&#24378;&#24230;&#26159;&#25104;&#26412;&#65292;&#24182;&#19988;&#26174;&#31034;&#23427;&#21487;&#20197;&#23558;&#30899;&#25490;&#25918;&#20943;&#23569;93&#65285;&#65292;&#24182;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;50&#65285;&#65292;&#19982;&#38543;&#26426;&#23458;&#25143;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) distributes machine learning (ML) training across many edge devices to reduce data transfer overhead and protect data privacy. Since FL model training may span millions of devices and is thus resource-intensive, prior work has focused on improving its resource efficiency to optimize time-to-accuracy. However, prior work generally treats all resources the same, while, in practice, they may incur widely different costs, which instead motivates optimizing cost-to-accuracy. To address the problem, we design CEFL, which uses adaptive cost-aware client selection policies to optimize an arbitrary cost metric when training FL models. Our policies extend and combine prior work on utility-based client selection and critical learning periods by making them cost-aware. We demonstrate CEFL by designing carbon-efficient FL, where energy's carbon-intensity is the cost, and show that it i) reduces carbon emissions by 93\% and reduces training time by 50% compared to random clie
&lt;/p&gt;</description></item><item><title>&#22312;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#21482;&#20351;&#29992;&#19968;&#31181;&#24179;&#34913;&#31574;&#30053;&#65292;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#29366;&#24577;&#30340;&#25968;&#25454;&#36136;&#37327;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23478;&#26063;&#24335;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;(FamO2O)&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#21516;&#25913;&#36827;&#21644;&#32422;&#26463;&#24378;&#24230;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#29366;&#24577;&#33258;&#36866;&#24212;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.17966</link><description>&lt;p&gt;
&#19968;&#27425;&#35757;&#32451;&#65292;&#33719;&#24471;&#19968;&#20010;&#23478;&#24237;&#65306;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#29366;&#24577;&#33258;&#36866;&#24212;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning. (arXiv:2310.17966v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17966
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#21482;&#20351;&#29992;&#19968;&#31181;&#24179;&#34913;&#31574;&#30053;&#65292;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#29366;&#24577;&#30340;&#25968;&#25454;&#36136;&#37327;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23478;&#26063;&#24335;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;(FamO2O)&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#21516;&#25913;&#36827;&#21644;&#32422;&#26463;&#24378;&#24230;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#29366;&#24577;&#33258;&#36866;&#24212;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#23558;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#35757;&#32451;&#19982;&#22312;&#32447;&#29615;&#22659;&#20013;&#30340;&#24494;&#35843;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#24341;&#20837;&#22312;&#32447;&#24494;&#35843;&#21487;&#33021;&#20250;&#21152;&#21095;&#24050;&#30693;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#23545;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#25919;&#31574;&#25913;&#36827;&#30446;&#26631;&#26045;&#21152;&#31574;&#30053;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#20204;&#36890;&#24120;&#20027;&#24352;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#37319;&#29992;&#19968;&#31181;&#24179;&#34913;&#25919;&#31574;&#25913;&#36827;&#21644;&#32422;&#26463;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19968;&#20992;&#20999;&#30340;&#26041;&#24335;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#25910;&#38598;&#21040;&#30340;&#26679;&#26412;&#65292;&#22240;&#20026;&#19981;&#21516;&#29366;&#24577;&#30340;&#25968;&#25454;&#36136;&#37327;&#21464;&#21270;&#24456;&#22823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23478;&#26063;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;(FamO2O)&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#36171;&#20104;&#29616;&#26377;&#31639;&#27861;&#30830;&#23450;&#29366;&#24577;&#33258;&#36866;&#24212;&#25913;&#36827;-&#32422;&#26463;&#24179;&#34913;&#30340;&#33021;&#21147;&#12290;FamO2O&#21033;&#29992;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#35757;&#32451;&#19968;&#20010;&#23478;&#26063;&#30340;&#31574;&#30053;&#65292;&#27599;&#20010;&#31574;&#30053;&#20855;&#26377;&#19981;&#21516;&#30340;&#25913;&#36827;/&#32422;&#26463;&#24378;&#24230;&#65292;&#21644;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Offline-to-online reinforcement learning (RL) is a training paradigm that combines pre-training on a pre-collected dataset with fine-tuning in an online environment. However, the incorporation of online fine-tuning can intensify the well-known distributional shift problem. Existing solutions tackle this problem by imposing a policy constraint on the policy improvement objective in both offline and online learning. They typically advocate a single balance between policy improvement and constraints across diverse data collections. This one-size-fits-all manner may not optimally leverage each collected sample due to the significant variation in data quality across different states. To this end, we introduce Family Offline-to-Online RL (FamO2O), a simple yet effective framework that empowers existing algorithms to determine state-adaptive improvement-constraint balances. FamO2O utilizes a universal model to train a family of policies with different improvement/constraint intensities, and a
&lt;/p&gt;</description></item><item><title>&#21452;&#36793;&#21024;&#38500;&#21644;&#37325;&#26500;&#65288;DoRaR&#65289;&#26159;&#19968;&#31181;&#20840;&#38754;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#20182;&#30456;&#20851;&#27169;&#22411;&#20869;&#37096;&#20915;&#31574;&#26426;&#21046;&#19981;&#36879;&#26126;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.17945</link><description>&lt;p&gt;
&#19968;&#31181;&#20840;&#38754;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65306;&#21452;&#36793;&#21024;&#38500;&#21644;&#37325;&#26500;&#65288;DoRaR&#65289;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive and Reliable Feature Attribution Method: Double-sided Remove and Reconstruct (DoRaR). (arXiv:2310.17945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17945
&lt;/p&gt;
&lt;p&gt;
&#21452;&#36793;&#21024;&#38500;&#21644;&#37325;&#26500;&#65288;DoRaR&#65289;&#26159;&#19968;&#31181;&#20840;&#38754;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#20182;&#30456;&#20851;&#27169;&#22411;&#20869;&#37096;&#20915;&#31574;&#26426;&#21046;&#19981;&#36879;&#26126;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20869;&#37096;&#20915;&#31574;&#26426;&#21046;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#23545;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#30340;&#20915;&#31574;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#22266;&#26377;&#30340;&#32570;&#28857;&#12290;&#20363;&#22914;&#65292;&#19968;&#31867;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#20266;&#24433;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#30452;&#25509;&#36890;&#36807;&#21407;&#22987;&#35757;&#32451;&#22312;&#33258;&#28982;&#25968;&#25454;&#28857;&#19978;&#30340;&#20998;&#31867;&#22120;&#65292;&#23545;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23631;&#34109;&#36755;&#20837;&#36827;&#34892;&#21453;&#39304;&#12290;&#21478;&#19968;&#31867;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#32852;&#21512;&#35757;&#32451;&#30340;&#29305;&#24449;&#36873;&#25321;&#22120;&#21644;&#39044;&#27979;&#22120;&#26469;&#25214;&#21040;&#35299;&#37322;&#12290;&#34429;&#28982;&#36991;&#20813;&#20102;&#20266;&#24433;&#38382;&#39064;&#65292;&#20294;&#36825;&#20010;&#26032;&#31867;&#21035;&#30340;&#26041;&#27861;&#23384;&#22312;&#35299;&#37322;&#20013;&#30340;&#32534;&#30721;&#39044;&#27979;&#38382;&#39064;&#65288;EPITE&#65289;&#65292;&#20854;&#20013;&#39044;&#27979;&#22120;&#30340;&#20915;&#31574;&#19981;&#20381;&#36182;&#20110;&#29305;&#24449;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#36873;&#25321;&#36825;&#20123;&#29305;&#24449;&#30340;&#36974;&#32617;&#12290;
&lt;/p&gt;
&lt;p&gt;
The limited transparency of the inner decision-making mechanism in deep neural networks (DNN) and other machine learning (ML) models has hindered their application in several domains. In order to tackle this issue, feature attribution methods have been developed to identify the crucial features that heavily influence decisions made by these black box models. However, many feature attribution methods have inherent downsides. For example, one category of feature attribution methods suffers from the artifacts problem, which feeds out-of-distribution masked inputs directly through the classifier that was originally trained on natural data points. Another category of feature attribution method finds explanations by using jointly trained feature selectors and predictors. While avoiding the artifacts problem, this new category suffers from the Encoding Prediction in the Explanation (EPITE) problem, in which the predictor's decisions rely not on the features, but on the masks that selects thos
&lt;/p&gt;</description></item><item><title>&#21487;&#20449;&#20219;&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#26159;&#36793;&#32536;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#38754;&#20020;&#21508;&#31181;&#25361;&#25112;&#65292;&#26412;&#35843;&#26597;&#24635;&#32467;&#20102;&#23545;&#20854;&#30340;&#23450;&#20041;&#12289;&#23646;&#24615;&#12289;&#26694;&#26550;&#12289;&#25216;&#26415;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;6G&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17944</link><description>&lt;p&gt;
&#21487;&#20449;&#20219;&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Edge Machine Learning: A Survey. (arXiv:2310.17944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17944
&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#20219;&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#26159;&#36793;&#32536;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#38754;&#20020;&#21508;&#31181;&#25361;&#25112;&#65292;&#26412;&#35843;&#26597;&#24635;&#32467;&#20102;&#23545;&#20854;&#30340;&#23450;&#20041;&#12289;&#23646;&#24615;&#12289;&#26694;&#26550;&#12289;&#25216;&#26415;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;6G&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35745;&#31639;&#65288;EC&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#34701;&#21512;&#65292;&#21363;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#65288;EML&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#24067;&#24335;&#32593;&#32476;&#36164;&#28304;&#20197;&#21512;&#20316;&#26041;&#24335;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#24050;&#25104;&#20026;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;EML&#38754;&#20020;&#36164;&#28304;&#38480;&#21046;&#12289;&#24322;&#26500;&#32593;&#32476;&#29615;&#22659;&#20197;&#21450;&#19981;&#21516;&#24212;&#29992;&#30340;&#22810;&#26679;&#21270;&#26381;&#21153;&#38656;&#27714;&#31561;&#21508;&#31181;&#25361;&#25112;&#65292;&#36825;&#20123;&#22240;&#32032;&#20849;&#21516;&#24433;&#21709;&#30528;EML&#22312;&#21033;&#30410;&#30456;&#20851;&#32773;&#30524;&#20013;&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#21487;&#20449;&#20219;&#30340;EML&#23450;&#20041;&#12289;&#23646;&#24615;&#12289;&#26694;&#26550;&#12289;&#25216;&#26415;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#20840;&#38754;&#24635;&#32467;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#22312;&#31532;&#20845;&#20195;&#65288;6G&#65289;&#32593;&#32476;&#20013;&#21487;&#20449;&#20219;&#30340;EML&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#37096;&#32626;&#21644;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#30340;&#25361;&#25112;&#30340;&#35282;&#24230;&#35752;&#35770;&#20102;&#21487;&#20449;&#20219;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#20449;&#20219;&#30340;EML&#30340;&#21021;&#27493;&#23450;&#20041;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20851;&#38190;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The convergence of Edge Computing (EC) and Machine Learning (ML), known as Edge Machine Learning (EML), has become a highly regarded research area by utilizing distributed network resources to perform joint training and inference in a cooperative manner. However, EML faces various challenges due to resource constraints, heterogeneous network environments, and diverse service requirements of different applications, which together affect the trustworthiness of EML in the eyes of its stakeholders. This survey provides a comprehensive summary of definitions, attributes, frameworks, techniques, and solutions for trustworthy EML. Specifically, we first emphasize the importance of trustworthy EML within the context of Sixth-Generation (6G) networks. We then discuss the necessity of trustworthiness from the perspective of challenges encountered during deployment and real-world application scenarios. Subsequently, we provide a preliminary definition of trustworthy EML and explore its key attrib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35748;&#20026;Transformers&#26412;&#36136;&#19978;&#26159;&#22270;&#21040;&#22270;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26435;&#37325;&#31561;&#20215;&#20110;&#22270;&#20013;&#30340;&#36793;&#65292;&#24182;&#20351;&#29992;&#22270;&#21040;&#22270;Transformer&#26550;&#26500;&#32467;&#21512;&#26174;&#24335;&#22270;&#21644;&#28508;&#22312;&#22270;&#36827;&#34892;&#38750;&#33258;&#22238;&#24402;&#22270;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#24314;&#27169;&#21508;&#31181;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17936</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#22270;&#21040;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Transformers as Graph-to-Graph Models. (arXiv:2310.17936v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;Transformers&#26412;&#36136;&#19978;&#26159;&#22270;&#21040;&#22270;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26435;&#37325;&#31561;&#20215;&#20110;&#22270;&#20013;&#30340;&#36793;&#65292;&#24182;&#20351;&#29992;&#22270;&#21040;&#22270;Transformer&#26550;&#26500;&#32467;&#21512;&#26174;&#24335;&#22270;&#21644;&#28508;&#22312;&#22270;&#36827;&#34892;&#38750;&#33258;&#22238;&#24402;&#22270;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#24314;&#27169;&#21508;&#31181;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;Transformers&#26412;&#36136;&#19978;&#26159;&#22270;&#21040;&#22270;&#27169;&#22411;&#65292;&#32780;&#24207;&#21015;&#21482;&#26159;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#27880;&#24847;&#21147;&#26435;&#37325;&#22312;&#21151;&#33021;&#19978;&#31561;&#20215;&#20110;&#22270;&#20013;&#30340;&#36793;&#12290;&#25105;&#20204;&#30340;&#22270;&#21040;&#22270;Transformer&#26550;&#26500;&#23558;&#36825;&#31181;&#33021;&#21147;&#26126;&#30830;&#22320;&#20307;&#29616;&#20986;&#26469;&#65292;&#36890;&#36807;&#23558;&#22270;&#30340;&#36793;&#36755;&#20837;&#21040;&#27880;&#24847;&#21147;&#26435;&#37325;&#35745;&#31639;&#20013;&#65292;&#24182;&#20351;&#29992;&#31867;&#20284;&#27880;&#24847;&#21147;&#30340;&#20989;&#25968;&#26469;&#39044;&#27979;&#22270;&#30340;&#36793;&#65292;&#20174;&#32780;&#23558;&#26174;&#24335;&#22270;&#38598;&#25104;&#21040;&#39044;&#35757;&#32451;Transformers&#23398;&#20064;&#30340;&#28508;&#22312;&#22270;&#20013;&#12290;&#28155;&#21152;&#36845;&#20195;&#22270;&#32454;&#21270;&#21487;&#20197;&#20026;&#36755;&#20837;&#12289;&#36755;&#20986;&#21644;&#28508;&#22312;&#22270;&#25552;&#20379;&#32852;&#21512;&#23884;&#20837;&#65292;&#20351;&#24471;&#38750;&#33258;&#22238;&#24402;&#22270;&#39044;&#27979;&#21487;&#20197;&#20248;&#21270;&#23436;&#25972;&#30340;&#22270;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#19987;&#38376;&#30340;&#31649;&#36947;&#25110;&#35299;&#30721;&#31574;&#30053;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26550;&#26500;&#22312;&#24314;&#27169;&#21508;&#31181;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19982;&#39044;&#35757;&#32451;&#23398;&#20064;&#30340;&#28508;&#22312;&#35821;&#35328;&#34920;&#31034;&#38750;&#24120;&#26377;&#25928;&#22320;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We argue that Transformers are essentially graph-to-graph models, with sequences just being a special case. Attention weights are functionally equivalent to graph edges. Our Graph-to-Graph Transformer architecture makes this ability explicit, by inputting graph edges into the attention weight computations and predicting graph edges with attention-like functions, thereby integrating explicit graphs into the latent graphs learned by pretrained Transformers. Adding iterative graph refinement provides a joint embedding of input, output, and latent graphs, allowing non-autoregressive graph prediction to optimise the complete graph without any bespoke pipeline or decoding strategy. Empirical results show that this architecture achieves state-of-the-art accuracies for modelling a variety of linguistic structures, integrating very effectively with the latent linguistic representations learned by pretraining.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#22312;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#26694;&#26550;&#19979;&#65292;&#28145;&#24230;Q-learning&#30340;&#28145;&#24230;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#33021;&#22815;&#25429;&#25417;&#22870;&#21169;&#30340;&#29305;&#27530;&#23646;&#24615;&#12290;&#36825;&#19968;&#32467;&#26524;&#23545;&#35299;&#37322;&#28145;&#24230;Q-learning&#25104;&#21151;&#30340;&#21407;&#22240;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.17915</link><description>&lt;p&gt;
&#35299;&#24320;Q-learning&#20013;&#28145;&#24230;&#30340;&#21147;&#37327;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
Lifting the Veil: Unlocking the Power of Depth in Q-learning. (arXiv:2310.17915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#22312;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#26694;&#26550;&#19979;&#65292;&#28145;&#24230;Q-learning&#30340;&#28145;&#24230;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#33021;&#22815;&#25429;&#25417;&#22870;&#21169;&#30340;&#29305;&#27530;&#23646;&#24615;&#12290;&#36825;&#19968;&#32467;&#26524;&#23545;&#35299;&#37322;&#28145;&#24230;Q-learning&#25104;&#21151;&#30340;&#21407;&#22240;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#25968;&#25454;&#21644;&#20016;&#23500;&#30340;&#35745;&#31639;&#36164;&#28304;&#30340;&#24110;&#21161;&#19979;&#65292;&#28145;&#24230;Q-learning&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#36816;&#31609;&#23398;&#21644;&#31649;&#29702;&#31185;&#23398;&#65292;&#24182;&#22312;&#20247;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12289;&#20379;&#24212;&#38142;&#12289;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;Q-learning&#30340;&#25104;&#21151;&#32570;&#20047;&#22362;&#23454;&#30340;&#29702;&#35770;&#39564;&#35777;&#21644;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#22312;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#26694;&#26550;&#19979;&#65292;&#23454;&#35777;&#22320;&#39564;&#35777;&#28145;&#24230;&#22312;&#28145;&#24230;Q-learning&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#20854;&#33391;&#22909;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#28145;&#24230;Q-learning&#25104;&#21151;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;&#28145;&#23618;&#32593;&#32476;&#65289;&#22312;&#25429;&#25417;&#22870;&#21169;&#30340;&#29305;&#27530;&#23646;&#24615;&#65292;&#21363;&#31354;&#38388;&#31232;&#30095;&#24615;&#21644;&#20998;&#27573;&#24658;&#23450;&#24615;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;&#23427;&#20204;&#30340;&#22823;&#23481;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#25512;&#21160;&#28145;&#24230;Q-learning&#30340;&#39046;&#22495;&#20570;&#20986;&#20102;&#22522;&#30784;&#24615;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the help of massive data and rich computational resources, deep Q-learning has been widely used in operations research and management science and has contributed to great success in numerous applications, including recommender systems, supply chains, games, and robotic manipulation. However, the success of deep Q-learning lacks solid theoretical verification and interpretability. The aim of this paper is to theoretically verify the power of depth in deep Q-learning. Within the framework of statistical learning theory, we rigorously prove that deep Q-learning outperforms its traditional version by demonstrating its good generalization error bound. Our results reveal that the main reason for the success of deep Q-learning is the excellent performance of deep neural networks (deep nets) in capturing the special properties of rewards namely, spatial sparseness and piecewise constancy, rather than their large capacities. In this paper, we make fundamental contributions to the field of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#30693;&#35782;&#26799;&#24230;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#30693;&#35782;&#26799;&#24230;&#65288;iKG&#65289;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#35299;&#20915;&#20102;&#30693;&#35782;&#26799;&#24230;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#22312;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#20013;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#30456;&#27604;&#30693;&#35782;&#26799;&#24230;&#65288;KG&#65289;&#65292;iKG&#26356;&#23481;&#26131;&#25193;&#23637;&#21040;&#20854;&#20182;BAI&#38382;&#39064;&#65292;&#19988;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17901</link><description>&lt;p&gt;
&#25913;&#36827;&#30693;&#35782;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving the Knowledge Gradient Algorithm. (arXiv:2310.17901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#30693;&#35782;&#26799;&#24230;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#30693;&#35782;&#26799;&#24230;&#65288;iKG&#65289;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#35299;&#20915;&#20102;&#30693;&#35782;&#26799;&#24230;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#22312;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#20013;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#30456;&#27604;&#30693;&#35782;&#26799;&#24230;&#65288;KG&#65289;&#65292;iKG&#26356;&#23481;&#26131;&#25193;&#23637;&#21040;&#20854;&#20182;BAI&#38382;&#39064;&#65292;&#19988;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#26799;&#24230;&#65288;KG&#65289;&#31639;&#27861;&#26159;&#19968;&#31181;&#29992;&#20110;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI&#65289;&#38382;&#39064;&#30340;&#27969;&#34892;&#31574;&#30053;&#12290;&#23427;&#24314;&#31435;&#22312;&#19968;&#31181;&#31616;&#21333;&#30340;&#24605;&#24819;&#19978;&#65292;&#21363;&#22987;&#32456;&#36873;&#25321;&#20135;&#29983;&#23545;&#33218;&#30340;&#26368;&#20339;&#22343;&#20540;&#20272;&#35745;&#39044;&#26399;&#19968;&#27493;&#25913;&#36827;&#26368;&#22823;&#30340;&#27979;&#37327;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#31574;&#30053;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#23548;&#33268;&#31639;&#27861;&#22312;&#28176;&#36817;&#19978;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#25509;&#19979;&#26469;&#25552;&#20379;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#36981;&#24490;KG&#30340;&#19968;&#27493;&#21069;&#30651;&#26041;&#24335;&#65292;&#20294;&#36873;&#25321;&#20135;&#29983;&#23545;&#36873;&#25321;&#26368;&#20339;&#33218;&#30340;&#27010;&#29575;&#26368;&#22823;&#30340;&#19968;&#27493;&#25913;&#36827;&#30340;&#27979;&#37327;&#12290;&#26032;&#30340;&#31574;&#30053;&#31216;&#20026;&#25913;&#36827;&#30340;&#30693;&#35782;&#26799;&#24230;&#65288;iKG&#65289;&#12290;&#21487;&#20197;&#35777;&#26126;iKG&#22312;&#28176;&#36817;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19982;KG&#30456;&#27604;&#65292;&#26356;&#23481;&#26131;&#23558;iKG&#25193;&#23637;&#21040;BAI&#30340;&#19981;&#21516;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#949;-&#22909;&#33218;&#35782;&#21035;&#21644;&#21487;&#34892;&#33218;&#35782;&#21035;&#20004;&#20010;&#20363;&#23376;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25968;&#20540;&#23454;&#20363;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;iKG&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The knowledge gradient (KG) algorithm is a popular policy for the best arm identification (BAI) problem. It is built on the simple idea of always choosing the measurement that yields the greatest expected one-step improvement in the estimate of the best mean of the arms. In this research, we show that this policy has limitations, causing the algorithm not asymptotically optimal. We next provide a remedy for it, by following the manner of one-step look ahead of KG, but instead choosing the measurement that yields the greatest one-step improvement in the probability of selecting the best arm. The new policy is called improved knowledge gradient (iKG). iKG can be shown to be asymptotically optimal. In addition, we show that compared to KG, it is easier to extend iKG to variant problems of BAI, with the $\epsilon$-good arm identification and feasible arm identification as two examples. The superior performances of iKG on these problems are further demonstrated using numerical examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65306;&#20998;&#23618;&#29420;&#31435;&#23376;&#27169;&#22411;&#35757;&#32451;&#65288;HIST&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20840;&#23616;&#27169;&#22411;&#21010;&#20998;&#20026;&#19981;&#30456;&#20132;&#30340;&#23376;&#27169;&#22411;&#65292;&#24182;&#22312;&#20998;&#23618;&#32467;&#26500;&#20013;&#20998;&#24067;&#65292;&#20197;&#38477;&#20302;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#23384;&#20648;&#36127;&#25285;&#65292;&#21516;&#26102;&#33410;&#32422;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2310.17890</link><description>&lt;p&gt;
&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23376;&#27169;&#22411;&#21010;&#20998;&#65306;&#31639;&#27861;&#35774;&#35745;&#19982;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Submodel Partitioning in Hierarchical Federated Learning: Algorithm Design and Convergence Analysis. (arXiv:2310.17890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65306;&#20998;&#23618;&#29420;&#31435;&#23376;&#27169;&#22411;&#35757;&#32451;&#65288;HIST&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20840;&#23616;&#27169;&#22411;&#21010;&#20998;&#20026;&#19981;&#30456;&#20132;&#30340;&#23376;&#27169;&#22411;&#65292;&#24182;&#22312;&#20998;&#23618;&#32467;&#26500;&#20013;&#20998;&#24067;&#65292;&#20197;&#38477;&#20302;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#23384;&#20648;&#36127;&#25285;&#65292;&#21516;&#26102;&#33410;&#32422;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#30456;&#36739;&#20256;&#32479;&#30340;&#8220;&#26143;&#22411;&#25299;&#25169;&#8221;&#26550;&#26500;&#30340;&#32852;&#37030;&#23398;&#20064;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#19978;&#35757;&#32451;&#22823;&#35268;&#27169;&#27169;&#22411;&#26102;&#65292;HFL&#20173;&#28982;&#20250;&#23545;&#36793;&#32536;&#35774;&#22791;&#36896;&#25104;&#37325;&#22823;&#30340;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#23384;&#20648;&#36127;&#25285;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#29420;&#31435;&#23376;&#27169;&#22411;&#35757;&#32451;&#65288;HIST&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#20998;&#23618;&#22330;&#26223;&#19979;&#30340;&#36825;&#20123;&#38382;&#39064;&#12290;HIST&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20998;&#23618;&#29256;&#26412;&#30340;&#27169;&#22411;&#21010;&#20998;&#65292;&#21363;&#22312;&#27599;&#19968;&#36718;&#20013;&#23558;&#20840;&#23616;&#27169;&#22411;&#21010;&#20998;&#20026;&#19981;&#30456;&#20132;&#30340;&#23376;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#32454;&#32990;&#20013;&#65292;&#20351;&#24471;&#27599;&#20010;&#32454;&#32990;&#21482;&#36127;&#36131;&#35757;&#32451;&#20840;&#27169;&#22411;&#30340;&#19968;&#20010;&#21010;&#20998;&#12290;&#36825;&#26679;&#27599;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#33410;&#30465;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#21516;&#26102;&#20943;&#36731;&#25972;&#20010;&#20998;&#23618;&#32467;&#26500;&#20013;&#30340;&#36890;&#20449;&#36127;&#36733;&#12290;&#25105;&#20204;&#23545;HIST&#22312;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#19979;&#30340;&#25910;&#25947;&#24615;&#34892;&#20026;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical federated learning (HFL) has demonstrated promising scalability advantages over the traditional "star-topology" architecture-based federated learning (FL). However, HFL still imposes significant computation, communication, and storage burdens on the edge, especially when training a large-scale model over resource-constrained Internet of Things (IoT) devices. In this paper, we propose hierarchical independent submodel training (HIST), a new FL methodology that aims to address these issues in hierarchical settings. The key idea behind HIST is a hierarchical version of model partitioning, where we partition the global model into disjoint submodels in each round, and distribute them across different cells, so that each cell is responsible for training only one partition of the full model. This enables each client to save computation/storage costs while alleviating the communication loads throughout the hierarchy. We characterize the convergence behavior of HIST for non-convex 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;Impressions&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#22270;&#20687;&#30340;&#31526;&#21495;&#23398;&#20197;&#21450;&#29305;&#23450;&#30340;&#35270;&#35273;&#29305;&#24449;&#21644;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24341;&#21457;&#24773;&#24863;&#12289;&#24605;&#32500;&#21644;&#20449;&#24565;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#20687;&#30340;&#24433;&#21709;&#21147;&#19981;&#20165;&#20165;&#22312;&#20110;&#32654;&#23398;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#32780;&#26159;&#19982;&#20854;&#20316;&#20026;&#27807;&#36890;&#34892;&#20026;&#30340;&#25104;&#21151;&#24687;&#24687;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.17887</link><description>&lt;p&gt;
&#21360;&#35937;&#8212;&#8212;&#29702;&#35299;&#35270;&#35273;&#31526;&#21495;&#23398;&#21644;&#32654;&#23398;&#24433;&#21709;&#21147;
&lt;/p&gt;
&lt;p&gt;
Impressions: Understanding Visual Semiotics and Aesthetic Impact. (arXiv:2310.17887v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;Impressions&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#22270;&#20687;&#30340;&#31526;&#21495;&#23398;&#20197;&#21450;&#29305;&#23450;&#30340;&#35270;&#35273;&#29305;&#24449;&#21644;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24341;&#21457;&#24773;&#24863;&#12289;&#24605;&#32500;&#21644;&#20449;&#24565;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#20687;&#30340;&#24433;&#21709;&#21147;&#19981;&#20165;&#20165;&#22312;&#20110;&#32654;&#23398;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#32780;&#26159;&#19982;&#20854;&#20316;&#20026;&#27807;&#36890;&#34892;&#20026;&#30340;&#25104;&#21151;&#24687;&#24687;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#23398;&#24433;&#21709;&#21147;&#26159;&#21542;&#19982;&#32654;&#19981;&#21516;&#65311;&#35270;&#35273;&#26174;&#33879;&#24615;&#26159;&#21542;&#21453;&#26144;&#20102;&#20854;&#26377;&#25928;&#27807;&#36890;&#30340;&#33021;&#21147;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;Impressions&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23427;&#21487;&#20197;&#30740;&#31350;&#22270;&#20687;&#30340;&#31526;&#21495;&#23398;&#65292;&#20197;&#21450;&#29305;&#23450;&#30340;&#35270;&#35273;&#29305;&#24449;&#21644;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24341;&#21457;&#29305;&#23450;&#30340;&#24773;&#24863;&#12289;&#24605;&#32500;&#21644;&#20449;&#24565;&#12290;&#25105;&#20204;&#35748;&#20026;&#22270;&#20687;&#30340;&#24433;&#21709;&#21147;&#19981;&#20165;&#20165;&#22312;&#20110;&#32654;&#23398;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#20110;&#20854;&#20316;&#20026;&#19968;&#31181;&#27807;&#36890;&#34892;&#20026;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#39118;&#26684;&#23545;&#20110;&#24847;&#20041;&#24418;&#25104;&#30340;&#36129;&#29486;&#19982;&#20027;&#39064;&#19968;&#26679;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#22270;&#20687;&#25551;&#36848;&#25968;&#25454;&#38598;&#24182;&#19981;&#36866;&#21512;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#26469;&#27169;&#25311;&#20154;&#31867;&#23545;&#22270;&#20687;&#30340;&#28508;&#22312;&#21360;&#35937;&#25110;&#35299;&#35835;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21463;&#35270;&#35273;&#33402;&#26415;&#20013;&#22270;&#20687;&#20998;&#26512;&#25216;&#26415;&#21551;&#21457;&#30340;&#27880;&#37322;&#20219;&#21153;&#65292;&#25910;&#38598;&#20102;1,440&#20010;&#22270;&#20687;-&#26631;&#39064;&#23545;&#21644;4,320&#20010;&#29420;&#29305;&#30340;&#27880;&#37322;&#65292;&#25506;&#32034;&#24433;&#21709;&#21147;&#12289;&#23454;&#29992;&#22270;&#20687;&#25551;&#36848;&#12289;&#21360;&#35937;&#21644;&#32654;&#23398;&#35774;&#35745;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is aesthetic impact different from beauty? Is visual salience a reflection of its capacity for effective communication? We present Impressions, a novel dataset through which to investigate the semiotics of images, and how specific visual features and design choices can elicit specific emotions, thoughts and beliefs. We posit that the impactfulness of an image extends beyond formal definitions of aesthetics, to its success as a communicative act, where style contributes as much to meaning formation as the subject matter. However, prior image captioning datasets are not designed to empower state-of-the-art architectures to model potential human impressions or interpretations of images. To fill this gap, we design an annotation task heavily inspired by image analysis techniques in the Visual Arts to collect 1,440 image-caption pairs and 4,320 unique annotations exploring impact, pragmatic image description, impressions, and aesthetic design choices. We show that existing multimodal image 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LOOP-MAC&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26469;&#21327;&#35843;&#34394;&#25311;&#30005;&#21378;&#65288;VPP&#65289;&#30340;&#36164;&#20135;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#35270;&#35282;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#22120;&#21152;&#36895;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#27979;&#37327;&#22270;&#26469;&#20445;&#35777;&#26368;&#20339;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.17882</link><description>&lt;p&gt;
&#20026;&#21327;&#35843;&#34394;&#25311;&#30005;&#21378;&#36164;&#20135;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Infused Distributed Optimization for Coordinating Virtual Power Plant Assets. (arXiv:2310.17882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LOOP-MAC&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26469;&#21327;&#35843;&#34394;&#25311;&#30005;&#21378;&#65288;VPP&#65289;&#30340;&#36164;&#20135;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#35270;&#35282;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#22120;&#21152;&#36895;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#27979;&#37327;&#22270;&#26469;&#20445;&#35777;&#26368;&#20339;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#65288;DER&#65289;&#37096;&#32626;&#26085;&#30410;&#22686;&#21152;&#30340;&#20852;&#36259;&#65292;&#34394;&#25311;&#30005;&#21378;&#65288;VPP&#65289;&#24050;&#25104;&#20026;&#27719;&#38598;&#21508;&#31181;DER&#24182;&#20419;&#36827;&#20854;&#21442;&#19982;&#25209;&#21457;&#33021;&#28304;&#24066;&#22330;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#36825;&#20123;VPP&#37096;&#32626;&#24471;&#21040;&#20102;&#32852;&#37030;&#33021;&#28304;&#30417;&#31649;&#22996;&#21592;&#20250;&#31532;2222&#21495;&#21629;&#20196;&#30340;&#25512;&#21160;&#65292;&#35813;&#21629;&#20196;&#20351;DER&#21644;VPP&#22312;&#24066;&#22330;&#39046;&#22495;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#28982;&#32780;&#65292;DER&#30340;&#22810;&#26679;&#24615;&#21644;&#20998;&#25955;&#24615;&#36136;&#32473;VPP&#36164;&#20135;&#30340;&#21487;&#25193;&#23637;&#21327;&#35843;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#25928;&#29575;&#21644;&#36895;&#24230;&#29942;&#39048;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#26469;&#21327;&#35843;VPP&#36164;&#20135;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;LOOP-MAC&#65288;&#20026;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#20248;&#21270;&#36807;&#31243;&#23398;&#20064;&#20248;&#21270;&#65289;&#65292;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#35270;&#35282;&#65292;&#27599;&#20010;VPP&#20195;&#29702;&#31649;&#29702;&#22810;&#20010;DER&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#22120;&#21152;&#36895;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#12290;LOOP-MAC&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#27979;&#37327;&#22270;&#30830;&#20445;&#20102;&#26368;&#20339;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amid the increasing interest in the deployment of Distributed Energy Resources (DERs), the Virtual Power Plant (VPP) has emerged as a pivotal tool for aggregating diverse DERs and facilitating their participation in wholesale energy markets. These VPP deployments have been fueled by the Federal Energy Regulatory Commission's Order 2222, which makes DERs and VPPs competitive across market segments. However, the diversity and decentralized nature of DERs present significant challenges to the scalable coordination of VPP assets. To address efficiency and speed bottlenecks, this paper presents a novel machine learning-assisted distributed optimization to coordinate VPP assets. Our method, named LOOP-MAC(Learning to Optimize the Optimization Process for Multi-agent Coordination), adopts a multi-agent coordination perspective where each VPP agent manages multiple DERs and utilizes neural network approximators to expedite the solution search. The LOOP-MAC method employs a gauge map to guarant
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20122;&#32447;&#24615;&#26102;&#38388;&#30340;&#35889;&#32858;&#31867;&#39044;&#27979;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#24378;&#32858;&#31867;&#29305;&#24615;&#30340;&#22270;&#12290;&#35813;&#39044;&#27979;&#22120;&#33021;&#22815;&#22312;&#20122;&#32447;&#24615;&#26102;&#38388;&#20869;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#26597;&#35810;&#32858;&#31867;&#25104;&#21592;&#65292;&#24182;&#19988;&#19982;&#30495;&#23454;&#32858;&#31867;&#25509;&#36817;&#30340;k-&#20998;&#21306;&#20445;&#25345;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#35813;&#39044;&#27979;&#22120;&#23545;&#20110;&#23569;&#37327;&#30340;&#38543;&#26426;&#36793;&#21024;&#38500;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17878</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#25913;&#36827;&#39044;&#22788;&#29702;&#26102;&#38388;&#30340;&#20122;&#32447;&#24615;&#26102;&#38388;&#35889;&#32858;&#31867;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Sublinear-Time Spectral Clustering Oracle with Improved Preprocessing Time. (arXiv:2310.17878v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20122;&#32447;&#24615;&#26102;&#38388;&#30340;&#35889;&#32858;&#31867;&#39044;&#27979;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#24378;&#32858;&#31867;&#29305;&#24615;&#30340;&#22270;&#12290;&#35813;&#39044;&#27979;&#22120;&#33021;&#22815;&#22312;&#20122;&#32447;&#24615;&#26102;&#38388;&#20869;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#26597;&#35810;&#32858;&#31867;&#25104;&#21592;&#65292;&#24182;&#19988;&#19982;&#30495;&#23454;&#32858;&#31867;&#25509;&#36817;&#30340;k-&#20998;&#21306;&#20445;&#25345;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#35813;&#39044;&#27979;&#22120;&#23545;&#20110;&#23569;&#37327;&#30340;&#38543;&#26426;&#36793;&#21024;&#38500;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#24378;&#32858;&#31867;&#29305;&#24615;&#30340;&#22270;&#30340;&#20122;&#32447;&#24615;&#26102;&#38388;&#35889;&#32858;&#31867;&#39044;&#27979;&#22120;&#30340;&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#22270;&#21253;&#21547;k&#20010;&#28508;&#22312;&#32858;&#31867;&#65292;&#27599;&#20010;&#32858;&#31867;&#30340;&#20869;&#23548;&#32435;&#36739;&#22823;&#65288;&#33267;&#23569;&#20026;&#966;&#65289;&#65292;&#22806;&#23548;&#32435;&#36739;&#23567;&#65288;&#26368;&#22810;&#20026;&#949;&#65289;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23545;&#22270;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#20197;&#20351;&#24471;&#32858;&#31867;&#25104;&#21592;&#26597;&#35810;&#33021;&#22815;&#22312;&#20122;&#32447;&#24615;&#26102;&#38388;&#20869;&#36827;&#34892;&#65292;&#24182;&#19988;&#25152;&#24471;&#21040;&#30340;&#20998;&#21306;&#24212;&#19982;&#25509;&#36817;&#30495;&#23454;&#32858;&#31867;&#30340;k-&#20998;&#21306;&#19968;&#33268;&#12290;&#20043;&#21069;&#30340;&#39044;&#27979;&#22120;&#35201;&#20040;&#20381;&#36182;&#20110;&#20869;&#22806;&#23548;&#32435;&#20043;&#38388;&#26377;&#19968;&#20010;poly(k)log n&#30340;&#24046;&#36317;&#65292;&#35201;&#20040;&#38656;&#35201;&#25351;&#25968;&#32423;&#65288;&#22312;k/&#949;&#19978;&#65289;&#30340;&#39044;&#22788;&#29702;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#25918;&#23485;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#23613;&#31649;&#20250;&#30053;&#24494;&#22686;&#21152;&#38169;&#35823;&#20998;&#31867;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32858;&#31867;&#39044;&#27979;&#22120;&#23545;&#20110;&#23569;&#37327;&#30340;&#38543;&#26426;&#36793;&#21024;&#38500;&#26159;&#40065;&#26834;&#30340;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of designing a sublinear-time spectral clustering oracle for graphs that exhibit strong clusterability. Such graphs contain $k$ latent clusters, each characterized by a large inner conductance (at least $\varphi$) and a small outer conductance (at most $\varepsilon$). Our aim is to preprocess the graph to enable clustering membership queries, with the key requirement that both preprocessing and query answering should be performed in sublinear time, and the resulting partition should be consistent with a $k$-partition that is close to the ground-truth clustering. Previous oracles have relied on either a $\textrm{poly}(k)\log n$ gap between inner and outer conductances or exponential (in $k/\varepsilon$) preprocessing time. Our algorithm relaxes these assumptions, albeit at the cost of a slightly higher misclassification ratio. We also show that our clustering oracle is robust against a few random edge deletions. To validate our theoretical bounds, we conducted exp
&lt;/p&gt;</description></item><item><title>ASPIRO&#26159;&#19968;&#31181;&#33021;&#22312;&#38646;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#30701;&#27169;&#26495;&#21477;&#23376;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31639;&#27861;&#35299;&#26512;&#26816;&#26597;&#12289;LLM&#30340;&#37325;&#26032;&#25552;&#31034;&#20197;&#21450;&#19968;&#33268;&#24615;&#39564;&#35777;&#25351;&#26631;PARENT&#65292;ASPIRO&#25104;&#21151;&#38477;&#20302;&#20102;66%&#30340;&#35299;&#26512;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#22312;&#19982;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#31454;&#20105;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.17877</link><description>&lt;p&gt;
ASPIRO: &#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#32467;&#26500;&#21270;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#38169;&#35823;&#24863;&#30693;&#37325;&#25552;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation. (arXiv:2310.17877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17877
&lt;/p&gt;
&lt;p&gt;
ASPIRO&#26159;&#19968;&#31181;&#33021;&#22312;&#38646;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#30701;&#27169;&#26495;&#21477;&#23376;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31639;&#27861;&#35299;&#26512;&#26816;&#26597;&#12289;LLM&#30340;&#37325;&#26032;&#25552;&#31034;&#20197;&#21450;&#19968;&#33268;&#24615;&#39564;&#35777;&#25351;&#26631;PARENT&#65292;ASPIRO&#25104;&#21151;&#38477;&#20302;&#20102;66%&#30340;&#35299;&#26512;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#22312;&#19982;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#31454;&#20105;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ASPIRO&#65292;&#19968;&#31181;&#22312;&#38646;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#30701;&#27169;&#26495;&#21477;&#23376;&#30340;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20135;&#29983;&#19982;&#23454;&#20307;&#26080;&#20851;&#30340;&#27169;&#26495;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;LLM&#24544;&#23454;&#22320;&#22797;&#21046;&#32473;&#23450;&#30340;&#23454;&#20307;&#65292;&#25110;&#32773;&#25163;&#21160;&#39564;&#35777;/&#21046;&#20316;&#27169;&#26495;&#12290;&#25105;&#20204;&#36890;&#36807;&#31639;&#27861;&#35299;&#26512;&#26816;&#26597;&#21644;PARENT&#25351;&#26631;&#35825;&#23548;&#30340;&#19968;&#33268;&#24615;&#39564;&#35777;&#65292;&#32467;&#21512;LLM&#30340;&#37325;&#26032;&#25552;&#31034;&#65292;&#23454;&#26102;&#35782;&#21035;&#21644;&#32416;&#27491;&#27169;&#26495;&#29983;&#25104;&#38382;&#39064;&#12290;&#22312;DART&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#30452;&#25509;LLM&#36755;&#20986;&#30456;&#27604;&#65292;ASPIRO&#23545;RDF&#19977;&#20803;&#32452;&#30340;&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#26512;&#38169;&#35823;&#29575;&#24179;&#22343;&#38477;&#20302;&#20102;66&#65285;&#12290;&#25105;&#20204;&#22312;Rel2Text&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20339;5&#26679;&#26412;text-davinci-003&#35774;&#32622;&#35780;&#20998;&#20026;BLEU 50.62&#65292;METEOR 45.16&#65292;BLEURT 0.82&#65292;NUBIA 0.87&#21644;PARENT 0.8962&#65292;&#19982;&#26368;&#36817;&#30340;&#31934;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26377;&#20102;&#26377;&#25928;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ASPIRO, an approach for structured data verbalisation into short template sentences in zero to few-shot settings. Unlike previous methods, our approach prompts large language models (LLMs) to directly produce entity-agnostic templates, rather than relying on LLMs to faithfully copy the given example entities, or validating/crafting the templates manually. We incorporate LLM re-prompting, triggered by algorithmic parsing checks, as well as the PARENT metric induced consistency validation to identify and rectify template generation problems in real-time. ASPIRO, compared to direct LLM output, averages 66\% parsing error rate reduction in generated verbalisations of RDF triples on the DART dataset. Our best 5-shot text-davinci-003 setup, scoring BLEU of 50.62, METEOR of 45.16, BLEURT of 0.82, NUBIA of 0.87, and PARENT of 0.8962 on the Rel2Text dataset, competes effectively with recent fine-tuned pre-trained language models.
&lt;/p&gt;</description></item><item><title>&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#21517;&#31639;&#27861;MatchRank&#65292;&#23427;&#22312;&#20505;&#36873;&#20154;&#25353;&#25490;&#21517;&#39034;&#24207;&#34987;&#20154;&#31867;&#20915;&#31574;&#32773;&#35780;&#20272;&#26102;&#65292;&#20135;&#29983;&#26368;&#22823;&#21270;&#22635;&#20805;&#27133;&#20301;&#30340;&#25490;&#21517;&#12290;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#36924;&#36817;&#20445;&#35777;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#23454;&#29616;&#12290; (arXiv:2310.17870v1 [cs.IR])</title><link>http://arxiv.org/abs/2310.17870</link><description>&lt;p&gt;
&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ranking with Slot Constraints. (arXiv:2310.17870v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17870
&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#21517;&#31639;&#27861;MatchRank&#65292;&#23427;&#22312;&#20505;&#36873;&#20154;&#25353;&#25490;&#21517;&#39034;&#24207;&#34987;&#20154;&#31867;&#20915;&#31574;&#32773;&#35780;&#20272;&#26102;&#65292;&#20135;&#29983;&#26368;&#22823;&#21270;&#22635;&#20805;&#27133;&#20301;&#30340;&#25490;&#21517;&#12290;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#36924;&#36817;&#20445;&#35777;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#23454;&#29616;&#12290; (arXiv:2310.17870v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;&#65292;&#36825;&#21487;&#20197;&#29992;&#26469;&#24314;&#27169;&#21508;&#31181;&#24212;&#29992;&#38382;&#39064; - &#20174;&#20855;&#26377;&#19981;&#21516;&#19987;&#19994;&#38480;&#21046;&#27133;&#20301;&#30340;&#22823;&#23398;&#24405;&#21462;&#65292;&#21040;&#22312;&#21307;&#23398;&#35797;&#39564;&#20013;&#26500;&#24314;&#31526;&#21512;&#26465;&#20214;&#30340;&#21442;&#19982;&#32773;&#20998;&#23618;&#38431;&#21015;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#27010;&#29575;&#25490;&#21517;&#21407;&#21017;&#65288;PRP&#65289;&#22312;&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;&#20013;&#21487;&#33021;&#20250;&#38750;&#24120;&#27425;&#20248;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#21517;&#31639;&#27861;&#65292;&#31216;&#20026;MatchRank&#12290;MatchRank&#30340;&#30446;&#26631;&#26159;&#22312;&#20505;&#36873;&#20154;&#25353;&#25490;&#21517;&#39034;&#24207;&#30001;&#20154;&#31867;&#20915;&#31574;&#32773;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#20135;&#29983;&#26368;&#22823;&#21270;&#22635;&#20805;&#27133;&#20301;&#30340;&#25490;&#21517;&#12290;&#36825;&#26679;&#65292;MatchRank&#22312;&#24191;&#20041;&#19978;&#26159;PRP&#30340;&#25512;&#24191;&#65292;&#24403;&#27809;&#26377;&#27133;&#32422;&#26463;&#26102;&#65292;&#23427;&#26159;PRP&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;MatchRank&#20855;&#26377;&#24378;&#22823;&#30340;&#36924;&#36817;&#20445;&#35777;&#65292;&#27809;&#26377;&#20219;&#20309;&#27133;&#20301;&#25110;&#20505;&#36873;&#20154;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#23454;&#29616;MatchRank&#12290;&#38500;&#20102;&#29702;&#35770;&#20445;&#35777;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;MatchRank&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the problem of ranking with slot constraints, which can be used to model a wide range of application problems -- from college admission with limited slots for different majors, to composing a stratified cohort of eligible participants in a medical trial. We show that the conventional Probability Ranking Principle (PRP) can be highly sub-optimal for slot-constrained ranking problems, and we devise a new ranking algorithm, called MatchRank. The goal of MatchRank is to produce rankings that maximize the number of filled slots if candidates are evaluated by a human decision maker in the order of the ranking. In this way, MatchRank generalizes the PRP, and it subsumes the PRP as a special case when there are no slot constraints. Our theoretical analysis shows that MatchRank has a strong approximation guarantee without any independence assumptions between slots or candidates. Furthermore, we show how MatchRank can be implemented efficiently. Beyond the theoretical guarantees, em
&lt;/p&gt;</description></item><item><title>&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#30340;&#20116;&#20010;&#28145;&#24230;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36829;&#21453;&#20102;&#26631;&#20934;&#30340;MIL&#20551;&#35774;&#65292;&#23548;&#33268;&#33021;&#22815;&#23398;&#20064;&#21453;&#30456;&#20851;&#30340;&#23454;&#20363;&#12290;&#36825;&#19968;&#38382;&#39064;&#38656;&#35201;&#36890;&#36807;&#25913;&#36827;&#21644;&#20854;&#20182;&#31574;&#30053;&#26469;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2310.17867</link><description>&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#30340;&#21487;&#37325;&#29616;&#24615;: &#31639;&#27861;&#21333;&#20803;&#27979;&#35797;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests. (arXiv:2310.17867v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17867
&lt;/p&gt;
&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#30340;&#20116;&#20010;&#28145;&#24230;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36829;&#21453;&#20102;&#26631;&#20934;&#30340;MIL&#20551;&#35774;&#65292;&#23548;&#33268;&#33021;&#22815;&#23398;&#20064;&#21453;&#30456;&#20851;&#30340;&#23454;&#20363;&#12290;&#36825;&#19968;&#38382;&#39064;&#38656;&#35201;&#36890;&#36807;&#25913;&#36827;&#21644;&#20854;&#20182;&#31574;&#30053;&#26469;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;(MIL)&#26159;&#20998;&#31867;&#38382;&#39064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#20854;&#20013;&#26377;&#27491;&#36127;&#26631;&#31614;&#21644;&#19968;&#20010;&#36755;&#20837;&#30340;&#8220;&#21253;&#8221;&#65292;&#24403;&#19988;&#20165;&#24403;&#21253;&#20013;&#21253;&#21547;&#19968;&#20010;&#27491;&#20803;&#32032;&#26102;&#65292;&#26631;&#31614;&#20026;&#27491;&#65292;&#21542;&#21017;&#20026;&#36127;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#38656;&#35201;&#23558;&#21253;&#32423;&#26631;&#31614;&#19982;&#23454;&#20363;&#32423;&#20449;&#24687;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#38544;&#21547;&#30528;&#19968;&#20010;&#22240;&#26524;&#20551;&#35774;&#21644;&#20219;&#21153;&#30340;&#19981;&#23545;&#31216;&#24615;&#65288;&#21363;&#65292;&#26080;&#27861;&#20132;&#25442;&#26631;&#31614;&#32780;&#19981;&#25913;&#21464;&#35821;&#20041;&#65289;&#12290;MIL&#38382;&#39064;&#20986;&#29616;&#22312;&#21307;&#30103;&#20445;&#20581;&#65288;&#19968;&#20010;&#24694;&#24615;&#32454;&#32990;&#34920;&#31034;&#30284;&#30151;&#65289;&#65292;&#32593;&#32476;&#23433;&#20840;&#65288;&#19968;&#20010;&#24694;&#24847;&#21487;&#25191;&#34892;&#25991;&#20214;&#20250;&#24863;&#26579;&#35745;&#31639;&#26426;&#65289;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#26368;&#33879;&#21517;&#30340;&#20116;&#20010;&#28145;&#24230;MIL&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#19981;&#31526;&#21512;&#26631;&#20934;&#30340;MIL&#20551;&#35774;&#12290;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#21453;&#30456;&#20851;&#30340;&#23454;&#20363;&#65292;&#21363;&#22312;&#30475;&#21040;&#36127;&#30340;&#21453;&#20363;&#20043;&#21069;&#40664;&#35748;&#20026;&#8220;&#27491;&#8221;&#26631;&#31614;&#65292;&#36825;&#23545;&#20110;&#19968;&#20010;&#27491;&#30830;&#30340;MIL&#27169;&#22411;&#26469;&#35828;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#24576;&#30097;&#25913;&#36827;&#21644;&#20854;&#20182;&#31574;&#30053;&#21487;&#33021;&#20250;&#25913;&#21892;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning (MIL) is a sub-domain of classification problems with positive and negative labels and a "bag" of inputs, where the label is positive if and only if a positive element is contained within the bag, and otherwise is negative. Training in this context requires associating the bag-wide label to instance-level information, and implicitly contains a causal assumption and asymmetry to the task (i.e., you can't swap the labels without changing the semantics). MIL problems occur in healthcare (one malignant cell indicates cancer), cyber security (one malicious executable makes an infected computer), and many other tasks. In this work, we examine five of the most prominent deep-MIL models and find that none of them respects the standard MIL assumption. They are able to learn anti-correlated instances, i.e., defaulting to "positive" labels until seeing a negative counter-example, which should not be possible for a correct MIL model. We suspect that enhancements and othe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#26032;&#39062;&#36125;&#21494;&#26031;&#20266;&#26680;&#24515;&#38598;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#26680;&#24515;&#38598;&#21518;&#39564;&#30340;&#21464;&#20998;&#36817;&#20284;&#24182;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23558;&#20854;&#19982;&#23436;&#25972;&#25968;&#25454;&#21518;&#39564;&#21305;&#37197;&#65292;&#23454;&#29616;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31561;&#39640;&#32500;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17852</link><description>&lt;p&gt;
&#20989;&#25968;&#31354;&#38388;&#36125;&#21494;&#26031;&#20266;&#26680;&#24515;&#38598;&#29992;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Function Space Bayesian Pseudocoreset for Bayesian Neural Networks. (arXiv:2310.17852v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#26032;&#39062;&#36125;&#21494;&#26031;&#20266;&#26680;&#24515;&#38598;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#26680;&#24515;&#38598;&#21518;&#39564;&#30340;&#21464;&#20998;&#36817;&#20284;&#24182;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23558;&#20854;&#19982;&#23436;&#25972;&#25968;&#25454;&#21518;&#39564;&#21305;&#37197;&#65292;&#23454;&#29616;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31561;&#39640;&#32500;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20266;&#26680;&#24515;&#38598;&#26159;&#19968;&#20010;&#32039;&#20945;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24635;&#32467;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#22240;&#27492;&#21487;&#20197;&#20316;&#20026;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#20195;&#29702;&#25968;&#25454;&#38598;&#12290;&#36890;&#24120;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20266;&#26680;&#24515;&#38598;&#21518;&#39564;&#26465;&#20214;&#21644;&#23436;&#25972;&#25968;&#25454;&#38598;&#21518;&#39564;&#26465;&#20214;&#20043;&#38388;&#30340;&#24046;&#24322;&#24230;&#37327;&#26469;&#26500;&#24314;&#36125;&#21494;&#26031;&#20266;&#26680;&#24515;&#38598;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#24046;&#24322;&#24230;&#37327;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20855;&#26377;&#39640;&#32500;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31561;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#26032;&#39062;&#36125;&#21494;&#26031;&#20266;&#26680;&#24515;&#38598;&#26500;&#24314;&#26041;&#27861;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#20197;&#27169;&#22411;&#21442;&#25968;&#65288;&#26435;&#37325;&#65289;&#30340;&#31354;&#38388;&#26500;&#24314;&#21644;&#21305;&#37197;&#26680;&#24515;&#38598;&#21644;&#23436;&#25972;&#25968;&#25454;&#21518;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#26500;&#24314;&#26680;&#24515;&#38598;&#21518;&#39564;&#30340;&#21464;&#20998;&#36817;&#20284;&#65292;&#24182;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23558;&#20854;&#19982;&#23436;&#25972;&#25968;&#25454;&#21518;&#39564;&#21305;&#37197;&#12290;&#36890;&#36807;&#30452;&#25509;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32469;&#36807;&#19968;&#20123;&#35745;&#31639;&#21644;&#35780;&#20272;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Bayesian pseudocoreset is a compact synthetic dataset summarizing essential information of a large-scale dataset and thus can be used as a proxy dataset for scalable Bayesian inference. Typically, a Bayesian pseudocoreset is constructed by minimizing a divergence measure between the posterior conditioning on the pseudocoreset and the posterior conditioning on the full dataset. However, evaluating the divergence can be challenging, particularly for the models like deep neural networks having high-dimensional parameters. In this paper, we propose a novel Bayesian pseudocoreset construction method that operates on a function space. Unlike previous methods, which construct and match the coreset and full data posteriors in the space of model parameters (weights), our method constructs variational approximations to the coreset posterior on a function space and matches it to the full data posterior in the function space. By working directly on the function space, our method could bypass sev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#26469;&#25552;&#21319;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#20808;&#36827;&#27169;&#22411;&#29983;&#25104;&#39640;&#36924;&#30495;&#24230;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#32479;&#35745;&#26041;&#27861;&#38169;&#35823;&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#20294;&#26368;&#32456;&#21487;&#33021;&#20250;&#22686;&#21152;&#25110;&#20572;&#28382;&#12290;</title><link>http://arxiv.org/abs/2310.17848</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#25193;&#23637;&#25552;&#21319;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Boosting Data Analytics With Synthetic Volume Expansion. (arXiv:2310.17848v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#26469;&#25552;&#21319;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#20808;&#36827;&#27169;&#22411;&#29983;&#25104;&#39640;&#36924;&#30495;&#24230;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#32479;&#35745;&#26041;&#27861;&#38169;&#35823;&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#20294;&#26368;&#32456;&#21487;&#33021;&#20250;&#22686;&#21152;&#25110;&#20572;&#28382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20316;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30707;&#65292;&#22312;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#30340;&#26085;&#30410;&#37325;&#35201;&#65292;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#32479;&#35745;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#19982;&#21407;&#22987;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#39640;&#36924;&#30495;&#24230;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#36890;&#36807;&#20808;&#36827;&#27169;&#22411;&#22914;&#34920;&#26684;&#25193;&#25955;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#29983;&#25104;&#65292;&#24182;&#32467;&#21512;&#30456;&#20851;&#30740;&#31350;&#27934;&#23519;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#21457;&#29616;&#26159;&#29983;&#25104;&#25928;&#24212;&#65306;&#32479;&#35745;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#38169;&#35823;&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#30340;&#22686;&#21152;&#19968;&#24320;&#22987;&#20943;&#23569;&#65292;&#20294;&#26368;&#32456;&#21487;&#33021;&#20250;&#22686;&#21152;&#25110;&#20572;&#28382;&#12290;&#36825;&#20010;&#29616;&#35937;&#26681;&#28304;&#20110;&#22797;&#21046;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data generation, a cornerstone of Generative Artificial Intelligence, signifies a paradigm shift in data science by addressing data scarcity and privacy while enabling unprecedented performance. As synthetic data gains prominence, questions arise concerning the accuracy of statistical methods when applied to synthetic data compared to raw data. In this article, we introduce the Synthetic Data Generation for Analytics framework. This framework employs statistical methods on high-fidelity synthetic data generated by advanced models such as tabular diffusion and Generative Pre-trained Transformer models. These models, trained on raw data, are further enhanced with insights from pertinent studies. A significant discovery within this framework is the generational effect: the error of a statistical method on synthetic data initially diminishes with added synthetic data but may eventually increase or plateau. This phenomenon, rooted in the complexities of replicating raw data distri
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#22312;&#32447;&#24066;&#22330;&#65292;&#29992;&#20110;&#36830;&#25509;&#26426;&#22120;&#23398;&#20064;&#30340;&#20379;&#27714;&#21305;&#37197;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20010;&#24066;&#22330;&#35774;&#35745;&#20013;&#30340;&#20004;&#20010;&#26680;&#24515;&#25361;&#25112;&#30340;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.17843</link><description>&lt;p&gt;
&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#32447;&#24066;&#22330;&#65306;&#20174;&#21457;&#29616;&#21040;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
A Data-Centric Online Market for Machine Learning: From Discovery to Pricing. (arXiv:2310.17843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17843
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#22312;&#32447;&#24066;&#22330;&#65292;&#29992;&#20110;&#36830;&#25509;&#26426;&#22120;&#23398;&#20064;&#30340;&#20379;&#27714;&#21305;&#37197;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20010;&#24066;&#22330;&#35774;&#35745;&#20013;&#30340;&#20004;&#20010;&#26680;&#24515;&#25361;&#25112;&#30340;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#21160;&#21147; - &#20016;&#23500;&#21644;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35201;&#23558;&#26426;&#22120;&#23398;&#20064;&#20174;&#23569;&#25968;&#22823;&#22411;&#20844;&#21496;&#20043;&#38388;&#30340;&#31454;&#36187;&#36716;&#21464;&#20026;&#20026;&#20247;&#22810;&#26222;&#36890;&#29992;&#25143;&#30340;&#25968;&#25454;&#20998;&#26512;&#35831;&#27714;&#26381;&#21153;&#30340;&#21487;&#35775;&#38382;&#25216;&#26415;&#65292;&#20173;&#28982;&#23384;&#22312;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#30340;&#19968;&#20010;&#24046;&#36317;&#26159;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#29992;&#25143;&#21487;&#20197;&#20174;&#20854;&#20182;&#25968;&#25454;&#25152;&#26377;&#32773;&#25317;&#26377;&#30340;&#26032;&#25968;&#25454;&#20013;&#21463;&#30410;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#25152;&#26377;&#32773;&#21364;&#22352;&#22312;&#19968;&#22534;&#25968;&#25454;&#19978;&#65292;&#19981;&#30693;&#36947;&#35841;&#21487;&#20197;&#21463;&#30410;&#20110;&#23427;&#12290;&#36825;&#31181;&#24046;&#36317;&#20026;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#36830;&#25509;&#20379;&#27714;&#30340;&#22312;&#32447;&#24066;&#22330;&#21019;&#36896;&#20102;&#26426;&#20250;&#12290;&#34429;&#28982;&#22312;&#32447;&#21305;&#37197;&#24066;&#22330;&#24456;&#24120;&#35265;&#65288;&#20363;&#22914;&#65292;&#25171;&#36710;&#31995;&#32479;&#65289;&#65292;&#20294;&#20026;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#24066;&#22330;&#38754;&#20020;&#35768;&#22810;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#26032;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#35774;&#35745;&#36825;&#26679;&#19968;&#20010;&#24066;&#22330;&#20013;&#30340;&#20004;&#20010;&#26680;&#24515;&#25361;&#25112;&#65306;&#65288;a&#65289;&#20026;&#20102;&#39640;&#25928;&#22320;&#23558;&#38656;&#27714;&#19982;&#20379;&#24212;&#21305;&#37197;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#25968;&#21315;&#20010;&#25968;&#25454;&#27744;&#20013;&#33258;&#21160;&#21457;&#29616;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25152;&#38656;&#30340;&#26377;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data fuels machine learning (ML) - rich and high-quality training data is essential to the success of ML. However, to transform ML from the race among a few large corporations to an accessible technology that serves numerous normal users' data analysis requests, there still exist important challenges. One gap we observed is that many ML users can benefit from new data that other data owners possess, whereas these data owners sit on piles of data without knowing who can benefit from it. This gap creates the opportunity for building an online market that can automatically connect supply with demand. While online matching markets are prevalent (e.g., ride-hailing systems), designing a data-centric market for ML exhibits many unprecedented challenges.  This paper develops new techniques to tackle two core challenges in designing such a market: (a) to efficiently match demand with supply, we design an algorithm to automatically discover useful data for any ML task from a pool of thousands o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20301;&#32622;&#32534;&#30721;&#30340;&#22810;&#20303;&#25143;&#26234;&#33021;&#23478;&#23621;&#23621;&#27665;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22270;&#24418;&#21644;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#23621;&#27665;&#30340;&#36523;&#20221;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20303;&#25143;&#29615;&#22659;&#20013;&#21487;&#20197;&#36798;&#21040;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.17836</link><description>&lt;p&gt;
&#22522;&#20110;&#20301;&#32622;&#32534;&#30721;&#30340;&#22810;&#20303;&#25143;&#26234;&#33021;&#23478;&#23621;&#23621;&#27665;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Positional Encoding-based Resident Identification in Multi-resident Smart Homes. (arXiv:2310.17836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20301;&#32622;&#32534;&#30721;&#30340;&#22810;&#20303;&#25143;&#26234;&#33021;&#23478;&#23621;&#23621;&#27665;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22270;&#24418;&#21644;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#23621;&#27665;&#30340;&#36523;&#20221;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20303;&#25143;&#29615;&#22659;&#20013;&#21487;&#20197;&#36798;&#21040;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23621;&#27665;&#35782;&#21035;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22810;&#20303;&#25143;&#26234;&#33021;&#29615;&#22659;&#20013;&#35782;&#21035;&#23621;&#27665;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#22522;&#20110;&#20301;&#32622;&#32534;&#30721;&#27010;&#24565;&#30340;&#29305;&#24449;&#25552;&#21462;&#27169;&#22411;&#12290;&#29305;&#24449;&#25552;&#21462;&#27169;&#22411;&#23558;&#20303;&#23429;&#20301;&#32622;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#20174;&#26234;&#33021;&#29615;&#22659;&#30340;&#24067;&#23616;&#22270;&#20013;&#26500;&#24314;&#36825;&#20123;&#22270;&#24418;&#12290;Node2Vec&#31639;&#27861;&#34987;&#29992;&#20110;&#23558;&#22270;&#24418;&#36716;&#21270;&#20026;&#39640;&#32500;&#33410;&#28857;&#23884;&#20837;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#65292;&#20351;&#29992;&#33410;&#28857;&#23884;&#20837;&#21644;&#20256;&#24863;&#22120;&#20107;&#20214;&#30340;&#26102;&#38388;&#24207;&#21015;&#26469;&#39044;&#27979;&#23621;&#27665;&#30340;&#36523;&#20221;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#26696;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#22810;&#20303;&#25143;&#29615;&#22659;&#20013;&#30340;&#23621;&#27665;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20998;&#21035;&#36798;&#21040;&#20102;94.5&#65285;&#21644;87.9&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel resident identification framework to identify residents in a multi-occupant smart environment. The proposed framework employs a feature extraction model based on the concepts of positional encoding. The feature extraction model considers the locations of homes as a graph. We design a novel algorithm to build such graphs from layout maps of smart environments. The Node2Vec algorithm is used to transform the graph into high-dimensional node embeddings. A Long Short-Term Memory (LSTM) model is introduced to predict the identities of residents using temporal sequences of sensor events with the node embeddings. Extensive experiments show that our proposed scheme effectively identifies residents in a multi-occupant environment. Evaluation results on two real-world datasets demonstrate that our proposed approach achieves 94.5% and 87.9% accuracy, respectively.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#23616;&#37096;&#27979;&#37327;&#30340;&#28151;&#21512;&#20809;&#23398;&#28237;&#27969;&#27169;&#22411;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#32447;&#23439;&#35266;&#27668;&#35937;&#27169;&#22411;&#21644;&#23616;&#37096;&#35266;&#27979;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#33021;&#21147;&#12290;&#19982;&#22522;&#32447;&#27169;&#22411;&#21644;&#20165;&#21033;&#29992;&#23616;&#37096;&#35266;&#27979;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;&#28151;&#21512;&#27169;&#22411;&#20855;&#26377;&#26356;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17829</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#23616;&#37096;&#27979;&#37327;&#30340;&#28151;&#21512;&#20809;&#23398;&#28237;&#27969;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hybrid Optical Turbulence Models Using Machine Learning and Local Measurements. (arXiv:2310.17829v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17829
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#23616;&#37096;&#27979;&#37327;&#30340;&#28151;&#21512;&#20809;&#23398;&#28237;&#27969;&#27169;&#22411;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#32447;&#23439;&#35266;&#27668;&#35937;&#27169;&#22411;&#21644;&#23616;&#37096;&#35266;&#27979;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#33021;&#21147;&#12290;&#19982;&#22522;&#32447;&#27169;&#22411;&#21644;&#20165;&#21033;&#29992;&#23616;&#37096;&#35266;&#27979;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;&#28151;&#21512;&#27169;&#22411;&#20855;&#26377;&#26356;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#23616;&#37096;&#29615;&#22659;&#20013;&#30340;&#22823;&#27668;&#20809;&#23398;&#28237;&#27969;&#23545;&#20110;&#20272;&#31639;&#33258;&#30001;&#31354;&#38388;&#20809;&#23398;&#31995;&#32479;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#23558;&#22312;&#19968;&#20010;&#29615;&#22659;&#20013;&#24320;&#21457;&#30340;&#23439;&#35266;&#27668;&#35937;&#27169;&#22411;&#24212;&#29992;&#20110;&#26032;&#29615;&#22659;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23439;&#35266;&#27668;&#35937;&#27169;&#22411;&#39044;&#35745;&#20173;&#28982;&#20855;&#26377;&#19968;&#23450;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22522;&#32447;&#23439;&#35266;&#27668;&#35937;&#27169;&#22411;&#19982;&#23616;&#37096;&#35266;&#27979;&#32467;&#21512;&#36215;&#26469;&#65292;&#35757;&#32451;&#28151;&#21512;&#27169;&#22411;&#21487;&#20197;&#25913;&#36827;&#27599;&#20010;&#22522;&#32447;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#23558;&#28151;&#21512;&#27169;&#22411;&#12289;&#36873;&#25321;&#30340;&#22522;&#32447;&#23439;&#35266;&#27668;&#35937;&#27169;&#22411;&#21644;&#20165;&#21033;&#29992;&#23616;&#37096;&#35266;&#27979;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#65292;&#31361;&#20986;&#20102;&#28151;&#21512;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of atmospheric optical turbulence in localized environments is essential for estimating the performance of free-space optical systems. Macro-meteorological models developed to predict turbulent effects in one environment may fail when applied in new environments. However, existing macro-meteorological models are expected to offer some predictive power. Building a new model from locally-measured macro-meteorology and scintillometer readings can require significant time and resources, as well as a large number of observations. These challenges motivate the development of a machine-learning informed hybrid model framework. By combining some baseline macro-meteorological model with local observations, hybrid models were trained to improve upon the predictive power of each baseline model. Comparisons between the performance of the hybrid models, the selected baseline macro-meteorological models, and machine-learning models trained only on local observations highlight pot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#65292;SA-Roundtrip&#65292;&#21487;&#20197;&#36827;&#34892;&#21487;&#25511;&#30340;&#37319;&#26679;&#29983;&#25104;&#65292;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#20869;&#22312;&#32500;&#24230;&#12290;&#22522;&#20110;&#35813;&#20808;&#39564;&#65292;&#32467;&#21512;Hamiltonian Monte Carlo&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#25104;&#20687;&#36870;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#37325;&#24314;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#23545;&#27604;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.17817</link><description>&lt;p&gt;
Bayesian&#25104;&#20687;&#36870;&#38382;&#39064;&#20013;&#30340;SA-Roundtrip&#20808;&#39564;&#21450;HMC-pCN&#37319;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Bayesian imaging inverse problem with SA-Roundtrip prior via HMC-pCN sampler. (arXiv:2310.17817v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#65292;SA-Roundtrip&#65292;&#21487;&#20197;&#36827;&#34892;&#21487;&#25511;&#30340;&#37319;&#26679;&#29983;&#25104;&#65292;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#20869;&#22312;&#32500;&#24230;&#12290;&#22522;&#20110;&#35813;&#20808;&#39564;&#65292;&#32467;&#21512;Hamiltonian Monte Carlo&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#25104;&#20687;&#36870;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#37325;&#24314;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#23545;&#27604;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#19982;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#25104;&#20687;&#36870;&#38382;&#39064;&#27714;&#35299;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20808;&#39564;&#20998;&#24067;&#30340;&#36873;&#25321;&#26159;&#20174;&#21487;&#29992;&#20808;&#39564;&#27979;&#37327;&#20013;&#23398;&#20064;&#30340;&#65292;&#22240;&#27492;&#26159;&#20851;&#20110;&#21487;&#29992;&#20808;&#39564;&#27979;&#37327;&#30340;&#37325;&#35201;&#34920;&#31034;&#23398;&#20064;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;SA-Roundtrip&#65292;&#20197;&#23454;&#29616;&#21487;&#25511;&#30340;&#37319;&#26679;&#29983;&#25104;&#65292;&#24182;&#35782;&#21035;&#20986;&#25968;&#25454;&#30340;&#20869;&#22312;&#32500;&#24230;&#12290;&#35813;&#20808;&#39564;&#22312;&#21452;&#21521;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#23884;&#20837;&#20102;&#33258;&#27880;&#24847;&#21147;&#32467;&#26500;&#12290;&#38543;&#21518;&#65292;&#23558;&#36125;&#21494;&#26031;&#25512;&#26029;&#24212;&#29992;&#20110;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20351;&#29992;&#20855;&#26377;&#39044;&#26465;&#20214;Crank-Nicolson&#31639;&#27861;&#30340;Hamiltonian Monte Carlo (HMC-pCN)&#12290;&#35813;&#31639;&#27861;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#34987;&#35777;&#26126;&#20855;&#26377;&#36941;&#21382;&#24615;&#12290;&#23545;MNIST&#21644;TomoPhantom&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#37325;&#24314;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#23545;&#27604;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference with deep generative prior has received considerable interest for solving imaging inverse problems in many scientific and engineering fields. The selection of the prior distribution is learned from, and therefore an important representation learning of, available prior measurements. The SA-Roundtrip, a novel deep generative prior, is introduced to enable controlled sampling generation and identify the data's intrinsic dimension. This prior incorporates a self-attention structure within a bidirectional generative adversarial network. Subsequently, Bayesian inference is applied to the posterior distribution in the low-dimensional latent space using the Hamiltonian Monte Carlo with preconditioned Crank-Nicolson (HMC-pCN) algorithm, which is proven to be ergodic under specific conditions. Experiments conducted on computed tomography (CT) reconstruction with the MNIST and TomoPhantom datasets reveal that the proposed method outperforms state-of-the-art comparisons, consis
&lt;/p&gt;</description></item><item><title>&#22312;&#26377;&#38480;&#20808;&#39564;&#30693;&#35782;&#19979;&#65292;&#36890;&#36807;&#23616;&#37096;&#20998;&#21306;&#21457;&#29616;&#31639;&#27861;&#65288;LDP&#65289;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#33258;&#21160;&#21464;&#37327;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;LDP&#26681;&#25454;&#19982;&#26333;&#20809;-&#32467;&#26524;&#23545;{X,Y}&#30456;&#20851;&#30340;&#23376;&#38598;&#23558;&#21464;&#37327;&#38598;&#21512;Z&#36827;&#34892;&#20998;&#21306;&#65292;&#24182;&#21306;&#20998;&#28151;&#28102;&#22240;&#32032;&#21644;&#20854;&#20182;&#21464;&#37327;&#31867;&#22411;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#27425;&#20108;&#27425;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.17816</link><description>&lt;p&gt;
Local Discovery by Partitioning: &#22312;&#26377;&#38480;&#20808;&#39564;&#30693;&#35782;&#19979;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs. (arXiv:2310.17816v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17816
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#20808;&#39564;&#30693;&#35782;&#19979;&#65292;&#36890;&#36807;&#23616;&#37096;&#20998;&#21306;&#21457;&#29616;&#31639;&#27861;&#65288;LDP&#65289;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#33258;&#21160;&#21464;&#37327;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;LDP&#26681;&#25454;&#19982;&#26333;&#20809;-&#32467;&#26524;&#23545;{X,Y}&#30456;&#20851;&#30340;&#23376;&#38598;&#23558;&#21464;&#37327;&#38598;&#21512;Z&#36827;&#34892;&#20998;&#21306;&#65292;&#24182;&#21306;&#20998;&#28151;&#28102;&#22240;&#32032;&#21644;&#20854;&#20182;&#21464;&#37327;&#31867;&#22411;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#27425;&#20108;&#27425;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#26377;&#38480;&#20808;&#39564;&#30693;&#35782;&#19979;&#33258;&#21160;&#21464;&#37327;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#20010;{X,Y}&#30340;&#26333;&#20809;-&#32467;&#26524;&#23545;&#21644;&#19968;&#20010;&#26410;&#30693;&#22240;&#26524;&#32467;&#26500;&#30340;&#21464;&#37327;&#38598;&#21512;Z&#65292;&#23616;&#37096;&#20998;&#21306;&#21457;&#29616;&#65288;LDP&#65289;&#31639;&#27861;&#23558;Z&#21010;&#20998;&#25104;&#19982;{X,Y}&#30456;&#20851;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#21015;&#20030;&#20102;&#20219;&#24847;Z&#30340;8&#20010;&#31351;&#20030;&#19988;&#20114;&#19981;&#37325;&#22797;&#30340;&#20998;&#21306;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#20998;&#31867;&#27861;&#21306;&#20998;&#28151;&#28102;&#22240;&#32032;&#21644;&#20854;&#20182;&#21464;&#37327;&#31867;&#22411;&#12290;LDP&#30340;&#21160;&#26426;&#26159;&#26377;&#25928;&#30340;&#35843;&#25972;&#38598;&#35782;&#21035;&#65292;&#20294;&#36991;&#20813;&#20102;&#33258;&#21160;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;&#20013;&#24120;&#35265;&#30340;&#39044;&#22788;&#29702;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;LDP&#23545;&#20110;&#28385;&#36275;&#36275;&#22815;&#22270;&#24418;&#26465;&#20214;&#30340;&#20219;&#20309;Z&#37117;&#36820;&#22238;&#19968;&#20010;&#26377;&#25928;&#30340;&#35843;&#25972;&#38598;&#12290;&#22312;&#26356;&#24378;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#21306;&#26631;&#31614;&#30340;&#28176;&#36817;&#27491;&#30830;&#24615;&#12290;&#24635;&#29420;&#31435;&#24615;&#27979;&#35797;&#22312;|Z|&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#26159;&#20108;&#27425;&#30340;&#65292;&#32463;&#39564;&#19978;&#35266;&#23519;&#21040;&#27425;&#20108;&#27425;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#23545;&#29702;&#35770;&#20445;&#35777;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the problem of automated covariate selection under limited prior knowledge. Given an exposure-outcome pair {X,Y} and a variable set Z of unknown causal structure, the Local Discovery by Partitioning (LDP) algorithm partitions Z into subsets defined by their relation to {X,Y}. We enumerate eight exhaustive and mutually exclusive partitions of any arbitrary Z and leverage this taxonomy to differentiate confounders from other variable types. LDP is motivated by valid adjustment set identification, but avoids the pretreatment assumption commonly made by automated covariate selection methods. We provide theoretical guarantees that LDP returns a valid adjustment set for any Z that meets sufficient graphical conditions. Under stronger conditions, we prove that partition labels are asymptotically correct. Total independence tests is worst-case quadratic in |Z|, with sub-quadratic runtimes observed empirically. We numerically validate our theoretical guarantees on synthetic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#23398;&#20064;&#30340;&#20809;&#35889;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#26435;&#37325;&#30697;&#38453;&#21644;&#26356;&#26032;&#30340;&#35889;&#33539;&#25968;&#32553;&#25918;&#20026;$\sqrt{\texttt{fan-out}/\texttt{fan-in}}$&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#29305;&#24449;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23548;&#20986;&#20102;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#30340;&#25512;&#23548;&#65292;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#29702;&#35299;&#26356;&#21152;&#28145;&#20837;&#12290;</title><link>http://arxiv.org/abs/2310.17813</link><description>&lt;p&gt;
&#19968;&#20010;&#29305;&#24449;&#23398;&#20064;&#30340;&#20809;&#35889;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
A Spectral Condition for Feature Learning. (arXiv:2310.17813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#23398;&#20064;&#30340;&#20809;&#35889;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#26435;&#37325;&#30697;&#38453;&#21644;&#26356;&#26032;&#30340;&#35889;&#33539;&#25968;&#32553;&#25918;&#20026;$\sqrt{\texttt{fan-out}/\texttt{fan-in}}$&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#29305;&#24449;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23548;&#20986;&#20102;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#30340;&#25512;&#23548;&#65292;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#29702;&#35299;&#26356;&#21152;&#28145;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35757;&#32451;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#21160;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#32593;&#32476;&#23485;&#24230;&#19978;&#30340;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23545;&#32593;&#32476;&#30340;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#38750;&#24179;&#20961;&#30340;&#28436;&#21464;&#65292;&#21363;&#29305;&#24449;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#32553;&#25918;&#26435;&#37325;&#30697;&#38453;&#21644;&#26356;&#26032;&#30340;&#35889;&#33539;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#29305;&#24449;&#23398;&#20064;&#65292;&#32553;&#25918;&#31995;&#25968;&#20026;$\sqrt{\texttt{fan-out}/\texttt{fan-in}}$&#65292;&#19982;&#22522;&#20110;Frobenius&#33539;&#25968;&#21644;&#20803;&#32032;&#22823;&#23567;&#30340;&#21551;&#21457;&#24335;&#32553;&#25918;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#20809;&#35889;&#32553;&#25918;&#20998;&#26512;&#36824;&#23548;&#20986;&#20102;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#30340;&#22522;&#26412;&#25512;&#23548;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#35835;&#32773;&#25552;&#20379;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#23398;&#20064;&#30340;&#22362;&#23454;&#27010;&#24565;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The push to train ever larger neural networks has motivated the study of initialization and training at large network width. A key challenge is to scale training so that a network's internal representations evolve nontrivially at all widths, a process known as feature learning. Here, we show that feature learning is achieved by scaling the spectral norm of weight matrices and their updates like $\sqrt{\texttt{fan-out}/\texttt{fan-in}}$, in contrast to widely used but heuristic scalings based on Frobenius norm and entry size. Our spectral scaling analysis also leads to an elementary derivation of \emph{maximal update parametrization}. All in all, we aim to provide the reader with a solid conceptual understanding of feature learning in neural networks.
&lt;/p&gt;</description></item><item><title>Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17807</link><description>&lt;p&gt;
Clover: &#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Clover: Closed-Loop Verifiable Code Generation. (arXiv:2310.17807v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17807
&lt;/p&gt;
&lt;p&gt;
Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#26159;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#36825;&#20010;&#36235;&#21183;&#21487;&#33021;&#20250;&#23548;&#33268;&#35768;&#22810;&#19981;&#33391;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#24895;&#26223;&#65306;Clover&#33539;&#24335;&#65292;&#21363;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#65292;&#23427;&#23558;&#27491;&#30830;&#24615;&#26816;&#26597;&#31616;&#21270;&#20026;&#26356;&#21487;&#35775;&#38382;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#38382;&#39064;&#12290;&#22312;Clover&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26816;&#26597;&#22120;&#65292;&#23427;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#12290;&#35813;&#26816;&#26597;&#22120;&#20351;&#29992;&#20102;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#38598;&#25104;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#28857;&#65292;&#21363;Clover&#22312;&#19968;&#33268;&#24615;&#26816;&#26597;&#26041;&#38754;&#24212;&#35813;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#30001;&#25163;&#24037;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65288;CloverBench&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35843;&#26597;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#27880;&#37322;&#30340;Dafny&#31243;&#24207;&#65292;&#38590;&#24230;&#27700;&#24179;&#19982;&#25945;&#31185;&#20070;&#30456;&#24403;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results sho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;DreamerV3&#30340;&#25216;&#24039;&#24212;&#29992;&#21040;PPO&#20013;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#25216;&#24039;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;PPO&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#24773;&#20917;&#19979;&#36825;&#20123;&#25216;&#24039;&#30340;&#25104;&#21151;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.17805</link><description>&lt;p&gt;
&#36890;&#36807;DreamerV3&#25216;&#24039;&#25552;&#39640;PPO&#30340;&#22870;&#21169;&#35268;&#27169;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks. (arXiv:2310.17805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;DreamerV3&#30340;&#25216;&#24039;&#24212;&#29992;&#21040;PPO&#20013;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#25216;&#24039;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;PPO&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#24773;&#20917;&#19979;&#36825;&#20123;&#25216;&#24039;&#30340;&#25104;&#21151;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#23494;&#38598;&#12289;&#35268;&#33539;&#21270;&#30340;&#29615;&#22659;&#22870;&#21169;&#12290;DreamerV3&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#20123;&#25216;&#24039;&#26469;&#20943;&#36731;&#36825;&#20123;&#38480;&#21046;&#65292;&#20351;&#29992;&#19968;&#32452;&#36229;&#21442;&#25968;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#29366;&#24577;&#12290;&#36825;&#20010;&#32467;&#26524;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#20123;&#25216;&#24039;&#30340;&#26222;&#36866;&#24615;&#30340;&#35752;&#35770;&#65292;&#22240;&#20026;&#23427;&#20204;&#20284;&#20046;&#36866;&#29992;&#20110;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;DreamerV3&#30340;&#25216;&#24039;&#24212;&#29992;&#21040;PPO&#20013;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#22312;&#21407;&#22987;&#24037;&#20316;&#20043;&#22806;&#36827;&#34892;&#36825;&#26679;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25216;&#24039;&#24182;&#19981;&#33021;&#20316;&#20026;&#19968;&#33324;&#30340;&#25913;&#36827;&#36716;&#31227;&#21040;PPO&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#39640;&#36136;&#37327;&#30340;PPO&#21442;&#32771;&#23454;&#29616;&#65292;&#24182;&#22312;Arcade Learning Environment&#21644;DeepMind Control Suite&#19978;&#36827;&#34892;&#20102;&#38271;&#36798;10,000&#20010;A100&#23567;&#26102;&#30340;&#22823;&#37327;&#28040;&#34701;&#30740;&#31350;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#25216;&#24039;&#24182;&#27809;&#26377;&#26222;&#36941;&#36229;&#36807;PPO&#65292;&#20294;&#25105;&#20204;&#30830;&#23450;&#20102;&#23427;&#20204;&#25104;&#21151;&#30340;&#24773;&#20917;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning methods rely heavily on dense, well-normalized environment rewards. DreamerV3 recently introduced a model-based method with a number of tricks that mitigate these limitations, achieving state-of-the-art on a wide range of benchmarks with a single set of hyperparameters. This result sparked discussion about the generality of the tricks, since they appear to be applicable to other reinforcement learning algorithms. Our work applies DreamerV3's tricks to PPO and is the first such empirical study outside of the original work. Surprisingly, we find that the tricks presented do not transfer as general improvements to PPO. We use a high quality PPO reference implementation and present extensive ablation studies totaling over 10,000 A100 hours on the Arcade Learning Environment and the DeepMind Control Suite. Though our experiments demonstrate that these tricks do not generally outperform PPO, we identify cases where they succeed and offer insight into the relations
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#20132;&#20114;&#25193;&#25955;&#36807;&#31243;&#65292;&#29992;&#20110;&#20107;&#20214;&#24207;&#21015;&#39044;&#27979;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#30452;&#25509;&#23398;&#20064;&#22810;&#20010;&#20107;&#20214;&#31867;&#22411;&#21644;&#20004;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#21040;&#36798;&#26102;&#38388;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#32500;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17800</link><description>&lt;p&gt;
&#20107;&#20214;&#24207;&#21015;&#39044;&#27979;&#30340;&#20132;&#20114;&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Interacting Diffusion Processes for Event Sequence Forecasting. (arXiv:2310.17800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#20132;&#20114;&#25193;&#25955;&#36807;&#31243;&#65292;&#29992;&#20110;&#20107;&#20214;&#24207;&#21015;&#39044;&#27979;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#30452;&#25509;&#23398;&#20064;&#22810;&#20010;&#20107;&#20214;&#31867;&#22411;&#21644;&#20004;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#21040;&#36798;&#26102;&#38388;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#32500;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;TPPs&#65289;&#24050;&#25104;&#20026;&#39044;&#27979;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#20013;&#21457;&#29983;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#20027;&#35201;&#26694;&#26550;&#65292;&#20294;&#20854;&#39034;&#24207;&#24615;&#21487;&#33021;&#20250;&#24433;&#21709;&#38271;&#26399;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#32435;&#20837;&#20854;&#20013;&#12290;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#39044;&#27979;&#65292;&#26681;&#25454;&#21382;&#21490;&#20107;&#20214;&#24207;&#21015;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30452;&#25509;&#23398;&#20064;&#22810;&#20010;&#20107;&#20214;&#31867;&#22411;&#21644;&#20004;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#21040;&#36798;&#26102;&#38388;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#32500;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#20004;&#20010;&#25193;&#25955;&#36807;&#31243;&#32452;&#25104;&#65292;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#38388;&#38548;&#65292;&#19968;&#20010;&#29992;&#20110;&#20107;&#20214;&#31867;&#22411;&#12290;&#36825;&#20123;&#36807;&#31243;&#36890;&#36807;&#21508;&#33258;&#30340;&#21435;&#22122;&#20989;&#25968;&#36827;&#34892;&#20132;&#20114;&#65292;&#21487;&#20197;&#25509;&#21463;&#26469;&#33258;&#20004;&#20010;&#36807;&#31243;&#30340;&#20013;&#38388;&#34920;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Temporal Point Processes (TPPs) have emerged as the primary framework for predicting sequences of events that occur at irregular time intervals, but their sequential nature can hamper performance for long-horizon forecasts. To address this, we introduce a novel approach that incorporates a diffusion generative model. The model facilitates sequence-to-sequence prediction, allowing multi-step predictions based on historical event sequences. In contrast to previous approaches, our model directly learns the joint probability distribution of types and inter-arrival times for multiple events. This allows us to fully leverage the high dimensional modeling capability of modern generative models. Our model is composed of two diffusion processes, one for the time intervals and one for the event types. These processes interact through their respective denoising functions, which can take as input intermediate representations from both processes, allowing the model to learn complex interacti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#21644;&#29289;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#38477;&#20302;&#24377;&#22609;&#24615;&#21644;&#26029;&#35010;&#24314;&#27169;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#20851;&#38190;&#21019;&#26032;&#26159;&#36890;&#36807;&#35757;&#32451;&#20302;&#32500;&#31070;&#32463;&#24212;&#21147;&#22330;&#26469;&#23454;&#29616;&#22312;&#20219;&#24847;&#31354;&#38388;&#20301;&#32622;&#19978;&#39640;&#25928;&#35745;&#31639;&#24212;&#21147;&#20540;&#21644;&#20869;&#37096;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#35757;&#32451;&#20102;&#31070;&#32463;&#21464;&#24418;&#21644;&#20223;&#23556;&#22330;&#26469;&#24314;&#31435;&#20302;&#32500;&#27969;&#24418;&#12290;</title><link>http://arxiv.org/abs/2310.17790</link><description>&lt;p&gt;
&#31070;&#32463;&#24212;&#21147;&#22330;&#22312;&#38477;&#38454;&#24377;&#22609;&#24615;&#21644;&#26029;&#35010;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Neural Stress Fields for Reduced-order Elastoplasticity and Fracture. (arXiv:2310.17790v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#21644;&#29289;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#38477;&#20302;&#24377;&#22609;&#24615;&#21644;&#26029;&#35010;&#24314;&#27169;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#20851;&#38190;&#21019;&#26032;&#26159;&#36890;&#36807;&#35757;&#32451;&#20302;&#32500;&#31070;&#32463;&#24212;&#21147;&#22330;&#26469;&#23454;&#29616;&#22312;&#20219;&#24847;&#31354;&#38388;&#20301;&#32622;&#19978;&#39640;&#25928;&#35745;&#31639;&#24212;&#21147;&#20540;&#21644;&#20869;&#37096;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#35757;&#32451;&#20102;&#31070;&#32463;&#21464;&#24418;&#21644;&#20223;&#23556;&#22330;&#26469;&#24314;&#31435;&#20302;&#32500;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#29289;&#29702;&#26694;&#26550;&#29992;&#20110;&#38477;&#38454;&#24314;&#27169;&#24377;&#22609;&#24615;&#21644;&#26029;&#35010;&#12290;&#26368;&#20808;&#36827;&#30340;&#31185;&#23398;&#35745;&#31639;&#27169;&#22411;&#22914;&#26448;&#26009;&#28857;&#27861; (MPM) &#21487;&#20197;&#20934;&#30830;&#27169;&#25311;&#22823;&#21464;&#24418;&#24377;&#22609;&#24615;&#21644;&#26029;&#35010;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#38271;&#36816;&#34892;&#26102;&#38388;&#21644;&#22823;&#20869;&#23384;&#28040;&#32791;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#35745;&#31639;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#21463;&#38480;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#34394;&#25311;&#29616;&#23454;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#38454;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#36890;&#36807;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#35757;&#32451;&#19968;&#31181;&#22522;&#20110;Kirchhoff&#24212;&#21147;&#22330;&#30340;&#20302;&#32500;&#27969;&#24418;&#12290;&#36825;&#31181;&#20302;&#32500;&#31070;&#32463;&#24212;&#21147;&#22330; (NSF) &#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#20219;&#24847;&#31354;&#38388;&#20301;&#32622;&#30340;&#24212;&#21147;&#20540;&#21644;&#23545;&#24212;&#30340;&#20869;&#37096;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#31070;&#32463;&#21464;&#24418;&#21644;&#20223;&#23556;&#22330;&#65292;&#20026;&#21464;&#24418;&#21644;&#20223;&#23556;&#21160;&#37327;&#22330;&#24314;&#31435;&#20302;&#32500;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a hybrid neural network and physics framework for reduced-order modeling of elastoplasticity and fracture. State-of-the-art scientific computing models like the Material Point Method (MPM) faithfully simulate large-deformation elastoplasticity and fracture mechanics. However, their long runtime and large memory consumption render them unsuitable for applications constrained by computation time and memory usage, e.g., virtual reality. To overcome these barriers, we propose a reduced-order framework. Our key innovation is training a low-dimensional manifold for the Kirchhoff stress field via an implicit neural representation. This low-dimensional neural stress field (NSF) enables efficient evaluations of stress values and, correspondingly, internal forces at arbitrary spatial locations. In addition, we also train neural deformation and affine fields to build low-dimensional manifolds for the deformation and affine momentum fields. These neural stress, deformation, and affine f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#20013;&#65292;&#21160;&#21147;&#23398;&#19981;&#21464;&#30340;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#23545;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#26356;&#26032;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#29366;&#24577;-&#21160;&#20316;&#35206;&#30422;&#29575;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.17786</link><description>&lt;p&gt;
&#29702;&#35299;&#20309;&#26102;&#21160;&#21147;&#23398;&#19981;&#21464;&#30340;&#25968;&#25454;&#22686;&#24378;&#23545;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#26356;&#26032;&#26377;&#30410;
&lt;/p&gt;
&lt;p&gt;
Understanding when Dynamics-Invariant Data Augmentations Benefit Model-Free Reinforcement Learning Updates. (arXiv:2310.17786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#20013;&#65292;&#21160;&#21147;&#23398;&#19981;&#21464;&#30340;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#23545;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#26356;&#26032;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#29366;&#24577;-&#21160;&#20316;&#35206;&#30422;&#29575;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#20197;&#20302;&#25104;&#26412;&#20135;&#29983;&#39069;&#22806;&#25968;&#25454;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24448;&#24448;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#23558;&#22686;&#24378;&#25968;&#25454;&#30452;&#25509;&#32435;&#20837;&#27169;&#22411;&#26080;&#20851;&#30340;RL&#26356;&#26032;&#20013;&#30340;&#25928;&#29992;&#65292;&#20294;&#30446;&#21069;&#36824;&#19981;&#22826;&#28165;&#26970;&#29305;&#23450;&#30340;DA&#31574;&#30053;&#20309;&#26102;&#20250;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290;&#26412;&#25991;&#26088;&#22312;&#25214;&#20986;DA&#30340;&#19968;&#33324;&#26041;&#38754;&#65292;&#20197;&#30830;&#23450;&#23548;&#33268;&#35266;&#23519;&#21040;&#30340;&#23398;&#20064;&#25913;&#36827;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20855;&#26377;&#21160;&#21147;&#23398;&#19981;&#21464;&#30340;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#30340;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#19978;&#65292;&#36825;&#26159;&#29702;&#35299;DA&#21450;&#20854;&#19982;RL&#35757;&#32451;&#25972;&#21512;&#30340;&#26356;&#19968;&#33324;&#30340;&#29702;&#35299;&#30340;&#19968;&#20010;&#21021;&#22987;&#27493;&#39588;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#20998;&#31163;&#20102;&#19977;&#20010;&#19982;DA&#30456;&#20851;&#30340;&#26041;&#38754;&#65306;&#29366;&#24577;-&#21160;&#20316;&#35206;&#30422;&#29575;&#65292;&#22870;&#21169;&#23494;&#24230;&#21644;&#27599;&#27425;&#26356;&#26032;&#29983;&#25104;&#30340;&#22686;&#24378;&#36716;&#25442;&#30340;&#25968;&#37327;&#65288;&#22686;&#24378;&#22238;&#25918;&#29575;&#65289;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#24471;&#20986;&#20004;&#20010;&#32467;&#35770;&#65306;(1) &#22686;&#21152;&#29366;&#24577;-&#21160;&#20316;&#35206;&#30422;&#29575;&#21487;&#25913;&#36827;&#23398;&#20064;&#25928;&#26524;&#65307;
&lt;/p&gt;
&lt;p&gt;
Recently, data augmentation (DA) has emerged as a method for leveraging domain knowledge to inexpensively generate additional data in reinforcement learning (RL) tasks, often yielding substantial improvements in data efficiency. While prior work has demonstrated the utility of incorporating augmented data directly into model-free RL updates, it is not well-understood when a particular DA strategy will improve data efficiency. In this paper, we seek to identify general aspects of DA responsible for observed learning improvements. Our study focuses on sparse-reward tasks with dynamics-invariant data augmentation functions, serving as an initial step towards a more general understanding of DA and its integration into RL training. Experimentally, we isolate three relevant aspects of DA: state-action coverage, reward density, and the number of augmented transitions generated per update (the augmented replay ratio). From our experiments, we draw two conclusions: (1) increasing state-action c
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#23398;&#20064;&#19968;&#31995;&#21015;&#21442;&#25968;&#21270;&#25805;&#20316;&#21407;&#35821;&#65292;&#24182;&#21033;&#29992;&#29615;&#22659;&#25913;&#21464;&#29289;&#20307;&#23039;&#24577;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#22312;&#30446;&#26631;&#29289;&#20307;&#25235;&#21462;&#34987;&#29615;&#22659;&#36974;&#25377;&#26102;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.17785</link><description>&lt;p&gt;
&#20351;&#29992;&#21442;&#25968;&#21270;&#25805;&#20316;&#21407;&#35821;&#23398;&#20064;&#22806;&#22312;&#28789;&#24039;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Extrinsic Dexterity with Parameterized Manipulation Primitives. (arXiv:2310.17785v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17785
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#23398;&#20064;&#19968;&#31995;&#21015;&#21442;&#25968;&#21270;&#25805;&#20316;&#21407;&#35821;&#65292;&#24182;&#21033;&#29992;&#29615;&#22659;&#25913;&#21464;&#29289;&#20307;&#23039;&#24577;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#22312;&#30446;&#26631;&#29289;&#20307;&#25235;&#21462;&#34987;&#29615;&#22659;&#36974;&#25377;&#26102;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#30456;&#20851;&#30340;&#26426;&#22120;&#20154;&#25235;&#21462;&#38382;&#39064;&#37117;&#28041;&#21450;&#21040;&#30446;&#26631;&#29289;&#20307;&#65292;&#20854;&#25152;&#26377;&#25235;&#21462;&#37117;&#34987;&#29615;&#22659;&#36974;&#25377;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21333;&#27425;&#25235;&#21462;&#35745;&#21010;&#24635;&#26159;&#22833;&#36133;&#30340;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#39318;&#20808;&#23558;&#29289;&#20307;&#25805;&#20316;&#21040;&#19968;&#20010;&#36866;&#21512;&#36827;&#34892;&#25235;&#21462;&#30340;&#37197;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#31995;&#21015;&#21033;&#29992;&#29615;&#22659;&#25913;&#21464;&#29289;&#20307;&#23039;&#24577;&#30340;&#21160;&#20316;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26469;&#32467;&#21512;&#19968;&#31995;&#21015;&#23398;&#20064;&#21040;&#30340;&#21442;&#25968;&#21270;&#25805;&#20316;&#21407;&#35821;&#12290;&#36890;&#36807;&#23398;&#20064;&#20302;&#32423;&#25805;&#20316;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#29289;&#20307;&#12289;&#22841;&#20855;&#21644;&#29615;&#22659;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26469;&#25511;&#21046;&#29289;&#20307;&#30340;&#29366;&#24577;&#12290;&#22312;&#26080;&#25511;&#21046;&#26465;&#20214;&#19979;&#20998;&#26512;&#35774;&#35745;&#36825;&#26679;&#19968;&#20010;&#22797;&#26434;&#34892;&#20026;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#20998;&#26512;&#26041;&#27861;&#38656;&#35201;&#20934;&#30830;&#24314;&#27169;&#20114;&#21160;&#21644;&#25509;&#35302;&#21160;&#21147;&#23398;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#22312;&#28145;&#24230;&#22270;&#20687;&#19978;&#30452;&#25509;&#25805;&#20316;&#30340;&#20998;&#23618;&#31574;&#30053;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many practically relevant robot grasping problems feature a target object for which all grasps are occluded, e.g., by the environment. Single-shot grasp planning invariably fails in such scenarios. Instead, it is necessary to first manipulate the object into a configuration that affords a grasp. We solve this problem by learning a sequence of actions that utilize the environment to change the object's pose. Concretely, we employ hierarchical reinforcement learning to combine a sequence of learned parameterized manipulation primitives. By learning the low-level manipulation policies, our approach can control the object's state through exploiting interactions between the object, the gripper, and the environment. Designing such a complex behavior analytically would be infeasible under uncontrolled conditions, as an analytic approach requires accurate physical modeling of the interaction and contact dynamics. In contrast, we learn a hierarchical policy model that operates directly on depth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#37329;&#34701;LLMs&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.17784</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Data-Centric Financial Large Language Models. (arXiv:2310.17784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#37329;&#34701;LLMs&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#28508;&#21147;&#65292;&#20294;&#30452;&#25509;&#24212;&#29992;&#20110;&#22797;&#26434;&#39046;&#22495;&#22914;&#37329;&#34701;&#26102;&#21364;&#36935;&#21040;&#22256;&#38590;&#12290;LLMs&#38590;&#20197;&#25512;&#29702;&#21644;&#25972;&#21512;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#20351;LLMs&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#37329;&#34701;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#19981;&#26159;&#19968;&#27425;&#24615;&#32473;LLM&#36127;&#36733;&#36807;&#22810;&#20449;&#24687;&#65292;&#32780;&#26159;&#26356;&#26377;&#25928;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20219;&#21153;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26469;&#21019;&#24314;&#37329;&#34701;LLM&#65288;FLLM&#65289;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#25163;&#21160;&#27880;&#37322;&#30340;&#25104;&#26412;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#24378;&#25512;&#29702;&#65288;AAR&#65289;&#26469;&#20462;&#25913;FLLM&#33258;&#36523;&#36755;&#20986;&#30340;&#20266;&#26631;&#31614;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#21270;FLLM&#19982;AAR&#30456;&#27604;&#65292;&#26174;&#33879;&#20248;&#20110;&#20026;&#21407;&#22987;&#25991;&#26412;&#35774;&#35745;&#30340;&#22522;&#32447;&#37329;&#34701;LLMs&#65292;&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information. We propose a data-centric approach to enable LLMs to better handle financial tasks. Our key insight is that rather than overloading the LLM with everything at once, it is more effective to preprocess and pre-understand the data. We create a financial LLM (FLLM) using multitask prompt-based finetuning to achieve data pre-processing and pre-understanding. However, labeled data is scarce for each task. To overcome manual annotation costs, we employ abductive augmentation reasoning (AAR) to automatically generate training data by modifying the pseudo labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR substantially outperforms baseline financial LLMs designed for raw text, achieving state-of-the-art on financial analysis and interpretation tasks. We 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#27169;&#25311;&#36710;&#36742;&#19982;&#29615;&#22659;&#20197;&#21450;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;&#36825;&#20123;&#24773;&#26223;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2310.17773</link><description>&lt;p&gt;
&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20998;&#31867;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks for Complex Traffic Scenario Classification. (arXiv:2310.17773v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17773
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#27169;&#25311;&#36710;&#36742;&#19982;&#29615;&#22659;&#20197;&#21450;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;&#36825;&#20123;&#24773;&#26223;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22330;&#26223;&#30340;&#27979;&#35797;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#33719;&#21462;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#32479;&#35745;&#26174;&#33879;&#35777;&#25454;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;&#33258;&#21160;&#21270;&#35782;&#21035;&#36825;&#20123;&#24773;&#26223;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#24773;&#26223;&#20998;&#31867;&#26041;&#27861;&#22312;&#22797;&#26434;&#24773;&#26223;&#65288;&#39640;&#36895;&#20844;&#36335;&#65292;&#22478;&#24066;&#65289;&#21644;&#19982;&#20854;&#20182;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#20114;&#21160;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#12290;&#29616;&#26377;&#26041;&#27861;&#27169;&#25311;&#20102;&#36710;&#36742;&#19982;&#20854;&#29615;&#22659;&#30340;&#20851;&#31995;&#65292;&#20294;&#24573;&#30053;&#20102;&#22810;&#36742;&#36710;&#20043;&#38388;&#30340;&#20114;&#21160;&#65288;&#20363;&#22914;&#65292;&#25554;&#20837;&#20999;&#25442;&#65292;&#38745;&#27490;&#21069;&#36710;&#65289;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#27809;&#26377;&#27599;&#24103;&#27880;&#37322;&#26469;&#20934;&#30830;&#23398;&#20064;&#24773;&#26223;&#30340;&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#33021;&#22815;&#27169;&#25311;&#36710;&#36742;&#19982;&#29615;&#22659;&#20197;&#21450;&#20854;&#20182;&#21442;&#19982;&#32773;&#20114;&#21160;&#30340;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#27169;&#25311;&#36825;&#20123;&#24773;&#26223;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;&#25193;&#23637;t
&lt;/p&gt;
&lt;p&gt;
A scenario-based testing approach can reduce the time required to obtain statistically significant evidence of the safety of Automated Driving Systems (ADS). Identifying these scenarios in an automated manner is a challenging task. Most methods on scenario classification do not work for complex scenarios with diverse environments (highways, urban) and interaction with other traffic agents. This is mirrored in their approaches which model an individual vehicle in relation to its environment, but neglect the interaction between multiple vehicles (e.g. cut-ins, stationary lead vehicle). Furthermore, existing datasets lack diversity and do not have per-frame annotations to accurately learn the start and end time of a scenario. We propose a method for complex traffic scenario classification that is able to model the interaction of a vehicle with the environment, as well as other agents. We use Graph Convolutional Networks to model spatial and temporal aspects of these scenarios. Expanding t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#23545;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26368;&#20248;&#20998;&#31867;&#26641;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#23558;&#35813;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#38454;&#27573;&#28151;&#21512;&#25972;&#25968;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#32422;&#26463;&#29983;&#25104;&#30340;&#35299;&#20915;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.17772</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26368;&#20248;&#20998;&#31867;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Classification Trees Robust to Distribution Shifts. (arXiv:2310.17772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#23545;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26368;&#20248;&#20998;&#31867;&#26641;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#23558;&#35813;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#38454;&#27573;&#28151;&#21512;&#25972;&#25968;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#32422;&#26463;&#29983;&#25104;&#30340;&#35299;&#20915;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23398;&#20064;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;/&#37096;&#32626;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20998;&#31867;&#26641;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#32463;&#24120;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#20986;&#29616;&#65292;&#20363;&#22914;&#20844;&#20849;&#21355;&#29983;&#21644;&#31038;&#20250;&#24037;&#20316;&#65292;&#20854;&#20013;&#25968;&#25454;&#36890;&#24120;&#26159;&#36890;&#36807;&#33258;&#25105;&#25253;&#21578;&#30340;&#35843;&#26597;&#25910;&#38598;&#30340;&#65292;&#36825;&#20123;&#35843;&#26597;&#23545;&#38382;&#39064;&#30340;&#34920;&#36848;&#26041;&#24335;&#12289;&#35843;&#26597;&#36827;&#34892;&#30340;&#26102;&#38388;&#21644;&#22320;&#28857;&#12289;&#20197;&#21450;&#21463;&#35775;&#32773;&#19982;&#35843;&#26597;&#21592;&#20998;&#20139;&#20449;&#24687;&#30340;&#33298;&#36866;&#31243;&#24230;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#25972;&#25968;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#30340;&#23398;&#20064;&#26368;&#20248;&#40065;&#26834;&#20998;&#31867;&#26641;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#23398;&#20064;&#26368;&#20248;&#40065;&#26834;&#26641;&#30340;&#38382;&#39064;&#21487;&#20197;&#31561;&#20215;&#22320;&#34920;&#36798;&#20026;&#19968;&#20010;&#20855;&#26377;&#39640;&#24230;&#38750;&#32447;&#24615;&#21644;&#19981;&#36830;&#32493;&#30446;&#26631;&#30340;&#21333;&#38454;&#27573;&#28151;&#21512;&#25972;&#25968;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31561;&#20215;&#22320;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#20004;&#38454;&#27573;&#32447;&#24615;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#20026;&#27492;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#32422;&#26463;&#29983;&#25104;&#30340;&#23450;&#21046;&#35299;&#20915;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning classification trees that are robust to distribution shifts between training and testing/deployment data. This problem arises frequently in high stakes settings such as public health and social work where data is often collected using self-reported surveys which are highly sensitive to e.g., the framing of the questions, the time when and place where the survey is conducted, and the level of comfort the interviewee has in sharing information with the interviewer. We propose a method for learning optimal robust classification trees based on mixed-integer robust optimization technology. In particular, we demonstrate that the problem of learning an optimal robust tree can be cast as a single-stage mixed-integer robust optimization problem with a highly nonlinear and discontinuous objective. We reformulate this problem equivalently as a two-stage linear robust optimization problem for which we devise a tailored solution procedure based on constraint gene
&lt;/p&gt;</description></item><item><title>GROOViST&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#25925;&#20107;&#20013;&#29289;&#20307;&#23450;&#20301;&#30340;&#26032;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#32771;&#34385;&#20102;&#36328;&#27169;&#24577;&#20381;&#36182;&#12289;&#26102;&#38388;&#38169;&#20301;&#21644;&#20154;&#31867;&#23545;&#35270;&#35273;&#23450;&#20301;&#30340;&#30452;&#35273;&#12290;&#36825;&#31181;&#24037;&#20855;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#35780;&#20272;&#21644;&#35299;&#37322;&#27599;&#20010;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.17770</link><description>&lt;p&gt;
GROOViST:&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#25925;&#20107;&#20013;&#29289;&#20307;&#23450;&#20301;&#35780;&#20272;&#30340;&#24230;&#37327;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
GROOViST: A Metric for Grounding Objects in Visual Storytelling. (arXiv:2310.17770v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17770
&lt;/p&gt;
&lt;p&gt;
GROOViST&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#25925;&#20107;&#20013;&#29289;&#20307;&#23450;&#20301;&#30340;&#26032;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#32771;&#34385;&#20102;&#36328;&#27169;&#24577;&#20381;&#36182;&#12289;&#26102;&#38388;&#38169;&#20301;&#21644;&#20154;&#31867;&#23545;&#35270;&#35273;&#23450;&#20301;&#30340;&#30452;&#35273;&#12290;&#36825;&#31181;&#24037;&#20855;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#35780;&#20272;&#21644;&#35299;&#37322;&#27599;&#20010;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#30001;&#19968;&#31995;&#21015;&#22270;&#20687;&#29983;&#25104;&#30340;&#25925;&#20107;&#30340;&#36866;&#24403;&#35780;&#20272;&#65292;&#24517;&#39035;&#32771;&#34385;&#22810;&#20010;&#26041;&#38754;&#65292;&#20363;&#22914;&#36830;&#36143;&#24615;&#65292;&#35821;&#27861;&#27491;&#30830;&#24615;&#21644;&#35270;&#35273;&#23450;&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35780;&#20272;&#23450;&#20301;&#31243;&#24230;&#65292;&#21363;&#25925;&#20107;&#19982;&#22270;&#20687;&#20013;&#26174;&#31034;&#30340;&#23454;&#20307;&#30456;&#20851;&#31243;&#24230;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#21069;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#38024;&#23545;&#27492;&#30446;&#30340;&#35774;&#35745;&#30340;&#25351;&#26631;&#21644;&#38024;&#23545;&#19968;&#33324;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#30340;&#25351;&#26631;&#12290;&#37492;&#20110;&#23427;&#20204;&#23384;&#22312;&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#24037;&#20855;GROOViST&#65292;&#35813;&#24037;&#20855;&#32771;&#34385;&#20102;&#36328;&#27169;&#24577;&#20381;&#36182;&#65292;&#26102;&#38388;&#38169;&#20301;&#65288;&#25925;&#20107;&#20013;&#23454;&#20307;&#20986;&#29616;&#30340;&#39034;&#24207;&#21644;&#22270;&#20687;&#24207;&#21015;&#21487;&#33021;&#19981;&#21305;&#37197;&#65289;&#20197;&#21450;&#20154;&#31867;&#23545;&#35270;&#35273;&#23450;&#20301;&#30340;&#30452;&#35273;&#12290;GROOViST&#30340;&#21478;&#19968;&#20010;&#20248;&#28857;&#26159;&#20854;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#23545;&#27599;&#20010;&#32452;&#20214;&#30340;&#36129;&#29486;&#36827;&#34892;&#35780;&#20272;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A proper evaluation of stories generated for a sequence of images -- the task commonly referred to as visual storytelling -- must consider multiple aspects, such as coherence, grammatical correctness, and visual grounding. In this work, we focus on evaluating the degree of grounding, that is, the extent to which a story is about the entities shown in the images. We analyze current metrics, both designed for this purpose and for general vision-text alignment. Given their observed shortcomings, we propose a novel evaluation tool, GROOViST, that accounts for cross-modal dependencies, temporal misalignments (the fact that the order in which entities appear in the story and the image sequence may not match), and human intuitions on visual grounding. An additional advantage of GROOViST is its modular design, where the contribution of each component can be assessed and interpreted individually.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20010;&#24615;&#21270;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;PERM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#23454;&#29616;&#19981;&#23545;&#21442;&#19982;&#35774;&#22791;&#20849;&#20139;&#30340;&#35745;&#31639;&#36164;&#28304;&#26045;&#21152;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20174;&#24322;&#26500;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20272;&#35745;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#32479;&#35745;&#24046;&#24322;&#26469;&#20010;&#24615;&#21270;&#32858;&#21512;&#26412;&#22320;&#32463;&#39564;&#25439;&#22833;&#65292;&#20174;&#32780;&#20811;&#26381;&#25968;&#25454;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.17761</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#20010;&#24615;&#21270;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distributed Personalized Empirical Risk Minimization. (arXiv:2310.17761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20010;&#24615;&#21270;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;PERM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#23454;&#29616;&#19981;&#23545;&#21442;&#19982;&#35774;&#22791;&#20849;&#20139;&#30340;&#35745;&#31639;&#36164;&#28304;&#26045;&#21152;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20174;&#24322;&#26500;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20272;&#35745;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#32479;&#35745;&#24046;&#24322;&#26469;&#20010;&#24615;&#21270;&#32858;&#21512;&#26412;&#22320;&#32463;&#39564;&#25439;&#22833;&#65292;&#20174;&#32780;&#20811;&#26381;&#25968;&#25454;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20513;&#23548;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65306;&#20010;&#24615;&#21270;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;PERM&#65289;&#65292;&#20197;&#20415;&#22312;&#19981;&#23545;&#21442;&#19982;&#35774;&#22791;&#20849;&#20139;&#30340;&#35745;&#31639;&#36164;&#28304;&#26045;&#21152;&#20005;&#26684;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20174;&#24322;&#26500;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;PERM&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#19982;&#35841;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#20272;&#35745;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#32479;&#35745;&#24046;&#24322;&#26469;&#20010;&#24615;&#21270;&#32858;&#21512;&#26412;&#22320;&#32463;&#39564;&#25439;&#22833;&#65292;&#20174;&#32780;&#33719;&#24471;&#25152;&#26377;&#26412;&#22320;&#20998;&#24067;&#30340;&#26368;&#20339;&#32479;&#35745;&#20934;&#30830;&#24615;&#24182;&#20811;&#26381;&#25968;&#25454;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#23398;&#20064;&#35268;&#27169;&#21270;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#27169;&#22411;&#37325;&#25490;&#21462;&#20195;&#20102;&#26631;&#20934;&#30340;&#27169;&#22411;&#24179;&#22343;&#21270;&#65292;&#20197;&#21516;&#26102;&#20248;&#21270;&#25152;&#26377;&#35774;&#22791;&#30340;PERM&#30446;&#26631;&#12290;&#36825;&#36824;&#20801;&#35768;&#25105;&#20204;&#20026;&#19981;&#21516;&#30340;&#23458;&#25143;&#23398;&#20064;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#65288;&#20363;&#22914;&#20855;&#26377;&#19981;&#21516;&#21442;&#25968;&#25968;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21508;&#20010;&#23458;&#25143;&#30340;&#28508;&#22312;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper advocates a new paradigm Personalized Empirical Risk Minimization (PERM) to facilitate learning from heterogeneous data sources without imposing stringent constraints on computational resources shared by participating devices. In PERM, we aim to learn a distinct model for each client by learning who to learn with and personalizing the aggregation of local empirical losses by effectively estimating the statistical discrepancy among data distributions, which entails optimal statistical accuracy for all local distributions and overcomes the data heterogeneity issue. To learn personalized models at scale, we propose a distributed algorithm that replaces the standard model averaging with model shuffling to simultaneously optimize PERM objectives for all devices. This also allows us to learn distinct model architectures (e.g., neural networks with different numbers of parameters) for different clients, thus confining underlying memory and compute resources of individual clients. W
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20984;&#20248;&#21270;&#20013;&#31639;&#27861;&#30340;&#21487;&#37325;&#29616;&#24615;&#21644;&#26799;&#24230;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;&#20182;&#20204;&#25361;&#25112;&#20102;&#20043;&#21069;&#30340;&#35266;&#28857;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#28369;&#20984;&#20248;&#21270;&#21644;&#24179;&#28369;&#20984;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#30340;&#21487;&#37325;&#29616;&#24615;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#20182;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#19981;&#21516;&#30340;oracle&#35774;&#32622;&#19979;&#65292;&#19982;&#19981;&#21516;&#31867;&#22411;&#30340;oracle&#30456;&#21305;&#37197;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17759</link><description>&lt;p&gt;
&#22312;&#20984;&#20248;&#21270;&#20013;&#30340;&#31639;&#27861;&#21487;&#37325;&#29616;&#24615;&#21644;&#26799;&#24230;&#22797;&#26434;&#24230;&#30340;&#26368;&#20248;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization. (arXiv:2310.17759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17759
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20984;&#20248;&#21270;&#20013;&#31639;&#27861;&#30340;&#21487;&#37325;&#29616;&#24615;&#21644;&#26799;&#24230;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;&#20182;&#20204;&#25361;&#25112;&#20102;&#20043;&#21069;&#30340;&#35266;&#28857;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#28369;&#20984;&#20248;&#21270;&#21644;&#24179;&#28369;&#20984;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#30340;&#21487;&#37325;&#29616;&#24615;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#20182;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#19981;&#21516;&#30340;oracle&#35774;&#32622;&#19979;&#65292;&#19982;&#19981;&#21516;&#31867;&#22411;&#30340;oracle&#30456;&#21305;&#37197;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#21487;&#37325;&#29616;&#24615;&#34913;&#37327;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#31245;&#24494;&#25913;&#21464;&#26102;&#36755;&#20986;&#30340;&#20559;&#24046;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#38454;&#26041;&#27861;&#38656;&#35201;&#22312;&#25910;&#25947;&#36895;&#24230;&#65288;&#26799;&#24230;&#22797;&#26434;&#24230;&#65289;&#21644;&#26356;&#22909;&#30340;&#21487;&#37325;&#29616;&#24615;&#20043;&#38388;&#20570;&#20986;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#36825;&#31181;&#30475;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#23481;&#26131;&#20986;&#38169;&#30340;oracle&#35774;&#32622;&#19979;&#65292;&#23545;&#20110;&#24179;&#28369;&#20984;&#20248;&#21270;&#21644;&#24179;&#28369;&#20984;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#30340;&#21487;&#37325;&#29616;&#24615;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#23588;&#20854;&#26159;&#65292;&#22312;&#19981;&#31934;&#30830;&#30340;&#21021;&#22987;&#21270;oracle&#32473;&#23450;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#21487;&#37325;&#29616;&#24615;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#26799;&#24230;&#22797;&#26434;&#24230;-&#23545;&#20110;&#26368;&#23567;&#21270;&#21644;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#12290;&#23545;&#20110;&#19981;&#31934;&#30830;&#30340;&#26799;&#24230;oracle&#65292;&#25509;&#36817;&#26368;&#20248;&#30340;&#20445;&#35777;&#20063;&#36866;&#29992;&#20110;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;oracle&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#22312;&#21487;&#37325;&#29616;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#37117;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic reproducibility measures the deviation in outputs of machine learning algorithms upon minor changes in the training process. Previous work suggests that first-order methods would need to trade-off convergence rate (gradient complexity) for better reproducibility. In this work, we challenge this perception and demonstrate that both optimal reproducibility and near-optimal convergence guarantees can be achieved for smooth convex minimization and smooth convex-concave minimax problems under various error-prone oracle settings. Particularly, given the inexact initialization oracle, our regularization-based algorithms achieve the best of both worlds optimal reproducibility and near-optimal gradient complexity - for minimization and minimax optimization. With the inexact gradient oracle, the near-optimal guarantees also hold for minimax optimization. Additionally, with the stochastic gradient oracle, we show that stochastic gradient descent ascent is optimal in terms of both re
&lt;/p&gt;</description></item><item><title>PockEngine&#26159;&#19968;&#31181;&#31232;&#30095;&#19988;&#39640;&#25928;&#30340;&#36793;&#32536;&#24494;&#35843;&#24341;&#25806;&#65292;&#25903;&#25345;&#31232;&#30095;&#21453;&#21521;&#20256;&#25773;&#21644;&#32534;&#35793;&#20026;&#20808;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#36793;&#32536;&#35774;&#22791;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#30828;&#20214;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17752</link><description>&lt;p&gt;
PockEngine: &#31232;&#30095;&#19988;&#39640;&#25928;&#30340;&#36793;&#32536;&#24494;&#35843;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
PockEngine: Sparse and Efficient Fine-tuning in a Pocket. (arXiv:2310.17752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17752
&lt;/p&gt;
&lt;p&gt;
PockEngine&#26159;&#19968;&#31181;&#31232;&#30095;&#19988;&#39640;&#25928;&#30340;&#36793;&#32536;&#24494;&#35843;&#24341;&#25806;&#65292;&#25903;&#25345;&#31232;&#30095;&#21453;&#21521;&#20256;&#25773;&#21644;&#32534;&#35793;&#20026;&#20808;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#36793;&#32536;&#35774;&#22791;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#30828;&#20214;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#22791;&#19978;&#30340;&#23398;&#20064;&#21644;&#39640;&#25928;&#24494;&#35843;&#33021;&#22815;&#23454;&#29616;&#25345;&#32493;&#19988;&#38544;&#31169;&#20445;&#25252;&#30340;&#20010;&#24615;&#21270;&#23450;&#21046;&#65288;&#20363;&#22914;&#65292;&#22312;&#20010;&#24615;&#21270;&#25968;&#25454;&#19978;&#26412;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35757;&#32451;&#26694;&#26550;&#26159;&#20026;&#24378;&#22823;&#30340;&#20113;&#26381;&#21153;&#22120;&#35774;&#35745;&#30340;&#65288;&#20363;&#22914;&#65292;GPU&#12289;TPU&#31561;&#65289;&#65292;&#32570;&#20047;&#38024;&#23545;&#36793;&#32536;&#23398;&#20064;&#30340;&#20248;&#21270;&#65292;&#38754;&#20020;&#30528;&#36164;&#28304;&#26377;&#38480;&#21644;&#36793;&#32536;&#30828;&#20214;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;PockEngine&#65306;&#19968;&#31181;&#23567;&#22411;&#12289;&#31232;&#30095;&#19988;&#39640;&#25928;&#30340;&#24341;&#25806;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;PockEngine&#25903;&#25345;&#31232;&#30095;&#21453;&#21521;&#20256;&#25773;&#65306;&#23427;&#20462;&#21098;&#21453;&#21521;&#22270;&#65292;&#24182;&#20351;&#29992;&#32463;&#36807;&#27979;&#37327;&#30340;&#20869;&#23384;&#33410;&#30465;&#21644;&#24310;&#36831;&#38477;&#20302;&#26469;&#31232;&#30095;&#26356;&#26032;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#12290;&#20854;&#27425;&#65292;PockEngine&#20197;&#32534;&#35793;&#20026;&#20808;&#65306;&#25972;&#20010;&#35757;&#32451;&#22270;&#65288;&#21253;&#25324;&#21069;&#21521;&#12289;&#21453;&#21521;&#21644;&#20248;&#21270;&#27493;&#39588;&#65289;&#22312;&#32534;&#35793;&#26102;&#25512;&#23548;&#20986;&#26469;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#36816;&#34892;&#26102;&#24320;&#38144;&#65292;&#24182;&#24102;&#26469;&#20102;&#22270;&#21464;&#25442;&#30340;&#26426;&#20250;&#12290;PockEngine&#36824;&#25972;&#21512;&#20102;&#20016;&#23500;&#30340;&#35757;&#32451;&#24037;&#20855;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-device learning and efficient fine-tuning enable continuous and privacy-preserving customization (e.g., locally fine-tuning large language models on personalized data). However, existing training frameworks are designed for cloud servers with powerful accelerators (e.g., GPUs, TPUs) and lack the optimizations for learning on the edge, which faces challenges of resource limitations and edge hardware diversity. We introduce PockEngine: a tiny, sparse and efficient engine to enable fine-tuning on various edge devices. PockEngine supports sparse backpropagation: it prunes the backward graph and sparsely updates the model with measured memory saving and latency reduction while maintaining the model quality. Secondly, PockEngine is compilation first: the entire training graph (including forward, backward and optimization steps) is derived at compile-time, which reduces the runtime overhead and brings opportunities for graph transformations. PockEngine also integrates a rich set of trainin
&lt;/p&gt;</description></item><item><title>OrionBench&#26159;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#20379;&#20102;&#36890;&#29992;&#25277;&#35937;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21457;&#24067;&#39057;&#32321;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2310.17748</link><description>&lt;p&gt;
&#35753;&#26368;&#32456;&#29992;&#25143;&#25104;&#20026;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#28857;&#65306;OrionBench&#29992;&#20110;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Making the End-User a Priority in Benchmarking: OrionBench for Unsupervised Time Series Anomaly Detection. (arXiv:2310.17748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17748
&lt;/p&gt;
&lt;p&gt;
OrionBench&#26159;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#20379;&#20102;&#36890;&#29992;&#25277;&#35937;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21457;&#24067;&#39057;&#32321;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#24120;&#35265;&#38382;&#39064;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24739;&#32773;&#30417;&#27979;&#12289;&#37329;&#34701;&#20013;&#30340;&#39044;&#27979;&#25110;&#33021;&#28304;&#20013;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#12290;&#36825;&#23548;&#33268;&#20102;&#35768;&#22810;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#27604;&#36739;&#26032;&#24320;&#21457;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#26377;&#38480;&#25968;&#25454;&#38598;&#30340;&#19968;&#27425;&#24615;&#25191;&#34892;&#65292;&#24182;&#19988;&#27604;&#36739;&#20165;&#38480;&#20110;&#23569;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;OrionBench&#8212;&#8212;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#12289;&#25345;&#32493;&#32500;&#25252;&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#29992;&#20110;&#34920;&#31034;&#27169;&#22411;&#30340;&#36890;&#29992;&#25277;&#35937;&#12289;&#28155;&#21152;&#26032;&#30340;&#27969;&#27700;&#32447;&#21644;&#25968;&#25454;&#38598;&#30340;&#21487;&#25193;&#23637;&#24615;&#12289;&#36229;&#21442;&#25968;&#26631;&#20934;&#21270;&#12289;&#27969;&#27700;&#32447;&#39564;&#35777;&#20197;&#21450;&#21457;&#24067;&#22522;&#20934;&#27979;&#35797;&#30340;&#39057;&#32321;&#29256;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;OrionBench&#30340;&#29992;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19977;&#24180;&#26102;&#38388;&#20869;&#21457;&#24067;&#30340;15&#20010;&#29256;&#26412;&#20013;&#27969;&#27700;&#32447;&#30340;&#28436;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection is a prevalent problem in many application domains such as patient monitoring in healthcare, forecasting in finance, or predictive maintenance in energy. This has led to the emergence of a plethora of anomaly detection methods, including more recently, deep learning based methods. Although several benchmarks have been proposed to compare newly developed models, they usually rely on one-time execution over a limited set of datasets and the comparison is restricted to a few models. We propose OrionBench -- a user centric continuously maintained benchmark for unsupervised time series anomaly detection. The framework provides universal abstractions to represent models, extensibility to add new pipelines and datasets, hyperparameter standardization, pipeline verification, and frequent releases with published benchmarks. We demonstrate the usage of OrionBench, and the progression of pipelines across 15 releases published over the course of three years. Moreover,
&lt;/p&gt;</description></item><item><title>BERT-PIN&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;&#26102;&#38388;&#24207;&#21015;&#36127;&#36733;&#26354;&#32447;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#27573;&#12290;&#23427;&#20351;&#29992;&#36127;&#36733;&#21644;&#28201;&#24230;&#26102;&#38388;&#24207;&#21015;&#26354;&#32447;&#20316;&#20026;&#36755;&#20837;&#65292;&#37319;&#29992;Transformer&#27169;&#22411;&#32467;&#26500;&#36827;&#34892;&#26354;&#32447;&#20462;&#22797;&#65292;&#24182;&#36890;&#36807;&#20505;&#36873;&#32773;&#36873;&#25321;&#36807;&#31243;&#29983;&#25104;&#22810;&#20010;&#21487;&#20449;&#24230;&#19981;&#21516;&#30340;&#21487;&#33021;&#30340;&#20462;&#22797;&#25968;&#25454;&#38598;&#12290;&#22312;&#22810;&#20010;MDS&#24674;&#22797;&#21644;&#38656;&#27714;&#21709;&#24212;&#22522;&#32447;&#20272;&#35745;&#31561;&#24212;&#29992;&#20013;&#65292;BERT-PIN&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17742</link><description>&lt;p&gt;
BERT-PIN: &#19968;&#31181;&#22522;&#20110;BERT&#30340;&#26694;&#26550;&#26469;&#24674;&#22797;&#26102;&#38388;&#24207;&#21015;&#36127;&#36733;&#26354;&#32447;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#27573;
&lt;/p&gt;
&lt;p&gt;
BERT-PIN: A BERT-based Framework for Recovering Missing Data Segments in Time-series Load Profiles. (arXiv:2310.17742v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17742
&lt;/p&gt;
&lt;p&gt;
BERT-PIN&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;&#26102;&#38388;&#24207;&#21015;&#36127;&#36733;&#26354;&#32447;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#27573;&#12290;&#23427;&#20351;&#29992;&#36127;&#36733;&#21644;&#28201;&#24230;&#26102;&#38388;&#24207;&#21015;&#26354;&#32447;&#20316;&#20026;&#36755;&#20837;&#65292;&#37319;&#29992;Transformer&#27169;&#22411;&#32467;&#26500;&#36827;&#34892;&#26354;&#32447;&#20462;&#22797;&#65292;&#24182;&#36890;&#36807;&#20505;&#36873;&#32773;&#36873;&#25321;&#36807;&#31243;&#29983;&#25104;&#22810;&#20010;&#21487;&#20449;&#24230;&#19981;&#21516;&#30340;&#21487;&#33021;&#30340;&#20462;&#22797;&#25968;&#25454;&#38598;&#12290;&#22312;&#22810;&#20010;MDS&#24674;&#22797;&#21644;&#38656;&#27714;&#21709;&#24212;&#22522;&#32447;&#20272;&#35745;&#31561;&#24212;&#29992;&#20013;&#65292;BERT-PIN&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;Transformer&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;BERT-PIN&#65292;&#19968;&#31181;&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#36716;&#25442;&#22120;&#65288;BERT&#65289;&#30340;Profile Inpainting Network&#12290;BERT-PIN&#20351;&#29992;&#36127;&#36733;&#21644;&#28201;&#24230;&#26102;&#38388;&#24207;&#21015;&#26354;&#32447;&#20316;&#20026;&#36755;&#20837;&#26469;&#24674;&#22797;&#22810;&#20010;&#32570;&#22833;&#25968;&#25454;&#27573;&#65288;MDSs&#65289;&#12290;&#20026;&#20102;&#37319;&#29992;&#26631;&#20934;&#30340;Transformer&#27169;&#22411;&#32467;&#26500;&#36827;&#34892;&#26354;&#32447;&#20462;&#22797;&#65292;&#25105;&#20204;&#23558;&#36127;&#36733;&#21644;&#28201;&#24230;&#26354;&#32447;&#20998;&#21106;&#20026;&#30452;&#32447;&#27573;&#65292;&#23558;&#27599;&#20010;&#27573;&#20316;&#20026;&#19968;&#20010;&#35789;&#65292;&#25972;&#20010;&#26354;&#32447;&#20316;&#20026;&#19968;&#20010;&#21477;&#23376;&#12290;&#25105;&#20204;&#22312;BERT-PIN&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#20505;&#36873;&#32773;&#36873;&#25321;&#36807;&#31243;&#65292;&#20351;&#20854;&#33021;&#22815;&#20135;&#29983;&#19968;&#31995;&#21015;&#27010;&#29575;&#20998;&#24067;&#65292;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#36825;&#20123;&#27010;&#29575;&#20998;&#24067;&#29983;&#25104;&#22810;&#20010;&#21487;&#20449;&#24230;&#19981;&#21516;&#30340;&#21487;&#33021;&#30340;&#20462;&#22797;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;BERT-PIN&#65292;&#24212;&#29992;&#20110;&#20004;&#20010;&#22330;&#26223;&#65306;&#22810;&#20010;MDS&#30340;&#24674;&#22797;&#21644;&#38656;&#27714;&#21709;&#24212;&#22522;&#32447;&#20272;&#35745;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;BERT-PIN&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the success of the Transformer model in natural language processing and computer vision, this paper introduces BERT-PIN, a Bidirectional Encoder Representations from Transformers (BERT) powered Profile Inpainting Network. BERT-PIN recovers multiple missing data segments (MDSs) using load and temperature time-series profiles as inputs. To adopt a standard Transformer model structure for profile inpainting, we segment the load and temperature profiles into line segments, treating each segment as a word and the entire profile as a sentence. We incorporate a top candidates selection process in BERT-PIN, enabling it to produce a sequence of probability distributions, based on which users can generate multiple plausible imputed data sets, each reflecting different confidence levels. We develop and evaluate BERT-PIN using real-world dataset for two applications: multiple MDSs recovery and demand response baseline estimation. Simulation results show that BERT-PIN outperforms the ex
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;GNN-GMVO&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#20248;&#21270;&#30005;&#23376;&#21830;&#21153;&#20013;&#30456;&#20284;&#21830;&#21697;&#25512;&#33616;&#30340;&#21830;&#21697;&#24635;&#20132;&#26131;&#20215;&#20540;&#65288;GMV&#65289;&#12290;&#23427;&#35299;&#20915;&#20102;&#20256;&#32479;GNN&#26550;&#26500;&#22312;&#20248;&#21270;&#25910;&#20837;&#30456;&#20851;&#30446;&#26631;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;GMV&#26469;&#20445;&#35777;&#25512;&#33616;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.17732</link><description>&lt;p&gt;
GNN-GMVO: &#29992;&#20110;&#20248;&#21270;&#30456;&#20284;&#21830;&#21697;&#25512;&#33616;&#20013;&#30340;&#21830;&#21697;&#24635;&#20132;&#26131;&#20215;&#20540;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GNN-GMVO: Graph Neural Networks for Optimizing Gross Merchandise Value in Similar Item Recommendation. (arXiv:2310.17732v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17732
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;GNN-GMVO&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#20248;&#21270;&#30005;&#23376;&#21830;&#21153;&#20013;&#30456;&#20284;&#21830;&#21697;&#25512;&#33616;&#30340;&#21830;&#21697;&#24635;&#20132;&#26131;&#20215;&#20540;&#65288;GMV&#65289;&#12290;&#23427;&#35299;&#20915;&#20102;&#20256;&#32479;GNN&#26550;&#26500;&#22312;&#20248;&#21270;&#25910;&#20837;&#30456;&#20851;&#30446;&#26631;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;GMV&#26469;&#20445;&#35777;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20284;&#21830;&#21697;&#25512;&#33616;&#26159;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#23427;&#24110;&#21161;&#23458;&#25143;&#22522;&#20110;&#20182;&#20204;&#24863;&#20852;&#36259;&#30340;&#20135;&#21697;&#25506;&#32034;&#30456;&#20284;&#21644;&#30456;&#20851;&#30340;&#26367;&#20195;&#21697;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21487;&#20197;&#29702;&#35299;&#20135;&#21697;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#22914;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#23427;&#20204;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#21644;&#20248;&#21270;&#30456;&#20851;&#24615;&#30340;&#37325;&#28857;&#30456;&#21453;&#65292;&#24403;&#21069;&#30340;GNN&#26550;&#26500;&#24182;&#26410;&#38024;&#23545;&#26368;&#22823;&#21270;&#19982;&#25910;&#20837;&#30456;&#20851;&#30340;&#30446;&#26631;&#65288;&#22914;&#21830;&#21697;&#24635;&#20132;&#26131;&#20215;&#20540;&#65288;GMV&#65289;&#65289;&#36827;&#34892;&#35774;&#35745;&#65292;&#32780;GMV&#26159;&#30005;&#23376;&#21830;&#21153;&#20844;&#21496;&#30340;&#20027;&#35201;&#19994;&#21153;&#25351;&#26631;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#22312;&#22823;&#35268;&#27169;&#30005;&#23376;&#21830;&#21153;&#31995;&#32479;&#20013;&#23450;&#20041;&#20934;&#30830;&#30340;&#36793;&#20851;&#31995;&#23545;&#20110;GNN&#26469;&#35828;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#21830;&#21697;&#20043;&#38388;&#30340;&#20851;&#31995;&#20855;&#26377;&#24322;&#36136;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#31216;&#20026;GNN-GMVO&#30340;&#26032;&#22411;GNN&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#30452;&#25509;&#20248;&#21270;GMV&#65292;&#21516;&#26102;&#20445;&#35777;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Similar item recommendation is a critical task in the e-Commerce industry, which helps customers explore similar and relevant alternatives based on their interested products. Despite the traditional machine learning models, Graph Neural Networks (GNNs), by design, can understand complex relations like similarity between products. However, in contrast to their wide usage in retrieval tasks and their focus on optimizing the relevance, the current GNN architectures are not tailored toward maximizing revenue-related objectives such as Gross Merchandise Value (GMV), which is one of the major business metrics for e-Commerce companies. In addition, defining accurate edge relations in GNNs is non-trivial in large-scale e-Commerce systems, due to the heterogeneity nature of the item-item relationships. This work aims to address these issues by designing a new GNN architecture called GNN-GMVO (Graph Neural Network - Gross Merchandise Value Optimizer). This model directly optimizes GMV while cons
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#38376;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GGNNs&#65289;&#65292;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#20132;&#36890;&#39044;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#35823;&#24046;&#65292;GGNNs&#34987;&#35777;&#26126;&#26159;&#26368;&#26377;&#25928;&#30340;&#36873;&#25321;&#65292;&#23637;&#31034;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17729</link><description>&lt;p&gt;
&#20351;&#29992;&#38376;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#20132;&#36890;&#23494;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Traffic Density Forecasting in Intelligent Transportation Systems Using Gated Graph Neural Networks. (arXiv:2310.17729v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#38376;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GGNNs&#65289;&#65292;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#20132;&#36890;&#39044;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#35823;&#24046;&#65292;GGNNs&#34987;&#35777;&#26126;&#26159;&#26368;&#26377;&#25928;&#30340;&#36873;&#25321;&#65292;&#23637;&#31034;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20132;&#36890;&#39044;&#27979;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#20934;&#30830;&#30340;&#20132;&#36890;&#39044;&#27979;&#23545;&#20110;&#34892;&#31243;&#35268;&#21010;&#12289;&#20132;&#36890;&#25511;&#21046;&#21644;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#31561;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#20132;&#36890;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;GNN&#26550;&#26500;&#65306;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;&#22270;&#37319;&#26679;&#21644;&#32858;&#21512;&#65289;&#21644;&#38376;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#35814;&#32454;&#32771;&#23519;&#20102;&#27599;&#31181;&#26550;&#26500;&#30340;&#26041;&#27861;&#35770;&#65292;&#21253;&#25324;&#23618;&#37197;&#32622;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#38477;&#20302;&#39044;&#27979;&#35823;&#24046;&#65292;&#22312;&#19977;&#31181;&#27169;&#22411;&#20013;&#65292;GGNNs&#34987;&#35777;&#26126;&#26159;&#26368;&#26377;&#25928;&#30340;&#36873;&#25321;&#12290;&#30740;&#31350;&#27010;&#36848;&#20102;&#27599;&#31181;&#26550;&#26500;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#38416;&#26126;&#20102;&#23427;&#20204;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20551;&#35774;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#35265;&#35299;&#65306;GCNs&#26174;&#31034;&#20102;9.10&#30340;RMSE&#21644;8.00&#30340;MAE&#65292;&#32780;GraphSAGE&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study delves into the application of graph neural networks in the realm of traffic forecasting, a crucial facet of intelligent transportation systems. Accurate traffic predictions are vital for functions like trip planning, traffic control, and vehicle routing in such systems. Three prominent GNN architectures Graph Convolutional Networks (Graph Sample and Aggregation) and Gated Graph Neural Networks are explored within the context of traffic prediction. Each architecture's methodology is thoroughly examined, including layer configurations, activation functions,and hyperparameters. The primary goal is to minimize prediction errors, with GGNNs emerging as the most effective choice among the three models. The research outlines outcomes for each architecture, elucidating their predictive performance through root mean squared error and mean absolute error (MAE). Hypothetical results reveal intriguing insights: GCNs display an RMSE of 9.10 and an MAE of 8.00, while GraphSAGE shows impr
&lt;/p&gt;</description></item><item><title>ZeroQuant-HERO&#26159;&#19968;&#31181;&#30828;&#20214;&#22686;&#24378;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#38024;&#23545;W8A8 Transformer&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#65292;&#26088;&#22312;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#65292;&#24182;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#37327;&#21270;&#38382;&#39064;&#21644;&#20869;&#23384;&#21463;&#38480;&#36816;&#31639;&#31526;&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#36824;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#20801;&#35768;&#29305;&#23450;&#27169;&#22359;&#20999;&#25442;&#33267;FP16/BF16&#27169;&#24335;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17723</link><description>&lt;p&gt;
ZeroQuant-HERO: W8A8 Transformer&#30340;&#30828;&#20214;&#22686;&#24378;&#30340;&#20248;&#21270;&#21518;&#35757;&#32451;&#37327;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers. (arXiv:2310.17723v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17723
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-HERO&#26159;&#19968;&#31181;&#30828;&#20214;&#22686;&#24378;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#38024;&#23545;W8A8 Transformer&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#65292;&#26088;&#22312;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#65292;&#24182;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#37327;&#21270;&#38382;&#39064;&#21644;&#20869;&#23384;&#21463;&#38480;&#36816;&#31639;&#31526;&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#36824;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#20801;&#35768;&#29305;&#23450;&#27169;&#22359;&#20999;&#25442;&#33267;FP16/BF16&#27169;&#24335;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25216;&#26415;&#22312;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;ZeroQuant&#65292;&#20026;BERT&#21644;GPT&#31561;&#27169;&#22411;&#25552;&#20379;&#20102;&#21160;&#24577;&#37327;&#21270;&#65292;&#20294;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#20869;&#23384;&#21463;&#38480;&#36816;&#31639;&#31526;&#21644;&#27599;&#20010;&#26631;&#35760;&#30340;&#37327;&#21270;&#22797;&#26434;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#12289;&#23436;&#20840;&#30001;&#30828;&#20214;&#22686;&#24378;&#30340;&#12289;&#32463;&#36807;&#20248;&#21270;&#30340;&#12289;&#21518;&#35757;&#32451;W8A8&#37327;&#21270;&#26694;&#26550;ZeroQuant-HERO&#12290;&#35813;&#26694;&#26550;&#29420;&#29305;&#22320;&#38598;&#25104;&#20102;&#20869;&#23384;&#24102;&#23485;&#21644;&#35745;&#31639;&#23494;&#38598;&#22411;&#36816;&#31639;&#31526;&#65292;&#26088;&#22312;&#23454;&#29616;&#26368;&#20339;&#30828;&#20214;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#20801;&#35768;&#29305;&#23450;&#30340;INT8&#27169;&#22359;&#20999;&#25442;&#21040;FP16/BF16&#27169;&#24335;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization techniques are pivotal in reducing the memory and computational demands of deep neural network inference. Existing solutions, such as ZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook crucial memory-bounded operators and the complexities of per-token quantization. Addressing these gaps, we present a novel, fully hardware-enhanced robust optimized post-training W8A8 quantization framework, ZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and compute-intensive operators, aiming for optimal hardware performance. Additionally, it offers flexibility by allowing specific INT8 modules to switch to FP16/BF16 mode, enhancing accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#36866;&#24212;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;LLaRP&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;LLM&#29992;&#20110;&#25509;&#25910;&#25351;&#20196;&#21644;&#35270;&#35273;&#36755;&#20837;&#65292;&#24182;&#22312;&#29615;&#22659;&#20013;&#30452;&#25509;&#36755;&#20986;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLaRP&#19981;&#20165;&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#22312;&#22823;&#37327;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;LLaRP&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.17722</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Generalizable Policies for Embodied Tasks. (arXiv:2310.17722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#36866;&#24212;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;LLaRP&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;LLM&#29992;&#20110;&#25509;&#25910;&#25351;&#20196;&#21644;&#35270;&#35273;&#36755;&#20837;&#65292;&#24182;&#22312;&#29615;&#22659;&#20013;&#30452;&#25509;&#36755;&#20986;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLaRP&#19981;&#20165;&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#22312;&#22823;&#37327;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;LLaRP&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#35843;&#25972;&#20026;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#35270;&#35273;&#20219;&#21153;&#30340;&#26222;&#36866;&#24615;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;(LLaRP)&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#30340;LLM&#35843;&#25972;&#20026;&#25509;&#25910;&#25991;&#26412;&#25351;&#20196;&#21644;&#35270;&#35273;&#33258;&#25105;&#20013;&#24515;&#35266;&#27979;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#30452;&#25509;&#22312;&#29615;&#22659;&#20013;&#36755;&#20986;&#21160;&#20316;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#35757;&#32451;LLaRP&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#30475;&#21644;&#34892;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLaRP&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;1,000&#20010;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;&#23427;&#30340;&#25104;&#21151;&#29575;&#36798;&#21040;&#20102;42%&#65292;&#26159;&#20854;&#20182;&#24120;&#35265;&#23398;&#20064;&#22522;&#32447;&#25110;&#38646;&#26679;&#26412;&#24212;&#29992;&#30340;1.7&#20493;&#25104;&#21151;&#29575;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#24110;&#21161;&#31038;&#21306;&#30740;&#31350;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#12289;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;AI&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#65292;&#21253;&#25324;150,000&#20010;&#35757;&#32451;&#20219;&#21153;&#21644;1,000&#20010;&#27979;&#35797;&#20219;&#21153;&#65292;&#29992;&#20110;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#37325;&#26032;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35814;&#32454;&#30740;&#31350;&#20102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;MRI&#22270;&#20687;&#36827;&#34892;&#33041;&#32959;&#30244;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;SoftMax&#20998;&#31867;&#22120;&#20197;&#21450;&#32858;&#31867;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#39640;&#20934;&#30830;&#29575;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#33041;&#32959;&#30244;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.17720</link><description>&lt;p&gt;
&#25512;&#36827;&#33041;&#32959;&#30244;&#26816;&#27979;: &#23545;MRI&#22270;&#20687;&#20013;&#30340;CNN&#12289;&#32858;&#31867;&#21644;SoftMax&#20998;&#31867;&#36827;&#34892;&#24443;&#24213;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Advancing Brain Tumor Detection: A Thorough Investigation of CNNs, Clustering, and SoftMax Classification in the Analysis of MRI Images. (arXiv:2310.17720v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35814;&#32454;&#30740;&#31350;&#20102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;MRI&#22270;&#20687;&#36827;&#34892;&#33041;&#32959;&#30244;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;SoftMax&#20998;&#31867;&#22120;&#20197;&#21450;&#32858;&#31867;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#39640;&#20934;&#30830;&#29575;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#33041;&#32959;&#30244;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32959;&#30244;&#30001;&#20110;&#39640;&#21457;&#30149;&#29575;&#21644;&#20840;&#24180;&#40836;&#32452;&#30340;&#39640;&#27515;&#20129;&#29575;&#65292;&#26159;&#20840;&#29699;&#38754;&#20020;&#30340;&#37325;&#22823;&#20581;&#24247;&#25361;&#25112;&#12290;&#21450;&#26089;&#21457;&#29616;&#33041;&#32959;&#30244;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#21644;&#24739;&#32773;&#30340;&#39044;&#21518;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#22270;&#20687;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;&#35814;&#23613;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#33041;&#32959;&#30244;&#30340;&#26816;&#27979;&#12290;&#23558;&#21253;&#21547;&#20581;&#24247;&#20154;&#21644;&#24739;&#26377;&#33041;&#32959;&#30244;&#30340;&#24739;&#32773;&#30340;MRI&#25195;&#25551;&#25968;&#25454;&#38598;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#36755;&#20837;&#21040;CNN&#26550;&#26500;&#20013;&#12290;&#37319;&#29992;SoftMax&#20840;&#36830;&#25509;&#23618;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#36798;&#21040;&#20102;98%&#30340;&#20934;&#30830;&#29575;&#12290;&#20026;&#20102;&#35780;&#20272;CNN&#30340;&#24615;&#33021;&#65292;&#36824;&#20351;&#29992;&#20102;&#20854;&#20182;&#20004;&#31181;&#20998;&#31867;&#22120;&#65292;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;RBF&#65289;&#21644;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;98.24%&#21644;95.64%&#30340;&#20934;&#30830;&#29575;&#12290;&#26412;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#32858;&#31867;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#39640;&#20102;CNN&#30340;&#20934;&#30830;&#29575;&#12290;&#22312;&#20934;&#30830;&#29575;&#20043;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#25935;&#24863;&#24230;&#12289;&#29305;&#24322;&#24230;&#21644;&#31934;&#30830;&#24230;&#31561;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain tumors pose a significant global health challenge due to their high prevalence and mortality rates across all age groups. Detecting brain tumors at an early stage is crucial for effective treatment and patient outcomes. This study presents a comprehensive investigation into the use of Convolutional Neural Networks (CNNs) for brain tumor detection using Magnetic Resonance Imaging (MRI) images. The dataset, consisting of MRI scans from both healthy individuals and patients with brain tumors, was processed and fed into the CNN architecture. The SoftMax Fully Connected layer was employed to classify the images, achieving an accuracy of 98%. To evaluate the CNN's performance, two other classifiers, Radial Basis Function (RBF) and Decision Tree (DT), were utilized, yielding accuracy rates of 98.24% and 95.64%, respectively. The study also introduced a clustering method for feature extraction, improving CNN's accuracy. Sensitivity, Specificity, and Precision were employed alongside accu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#23558;&#32479;&#35745;&#21644;&#21442;&#25968;&#21270;&#23398;&#20064;&#33539;&#20363;&#30456;&#32467;&#21512;&#65292;&#25506;&#32034;&#20102;&#20174;&#35780;&#20272;oracle&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#26080;&#26465;&#20214;&#30340;&#19979;&#30028;&#21644;&#26597;&#35810;&#22797;&#26434;&#24230;&#21051;&#30011;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;QSQ&#35774;&#32622;&#21644;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.17716</link><description>&lt;p&gt;
&#32479;&#19968;&#65288;&#37327;&#23376;&#65289;&#32479;&#35745;&#21644;&#21442;&#25968;&#21270;&#65288;&#37327;&#23376;&#65289;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unifying (Quantum) Statistical and Parametrized (Quantum) Algorithms. (arXiv:2310.17716v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#23558;&#32479;&#35745;&#21644;&#21442;&#25968;&#21270;&#23398;&#20064;&#33539;&#20363;&#30456;&#32467;&#21512;&#65292;&#25506;&#32034;&#20102;&#20174;&#35780;&#20272;oracle&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#26080;&#26465;&#20214;&#30340;&#19979;&#30028;&#21644;&#26597;&#35810;&#22797;&#26434;&#24230;&#21051;&#30011;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;QSQ&#35774;&#32622;&#21644;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kearns&#30340;&#32479;&#35745;&#26597;&#35810;&#65288;SQ&#65289;oracle&#65288;STOC'93&#65289;&#20026;&#22823;&#22810;&#25968;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35270;&#35282;&#12290;&#28982;&#32780;&#22312;&#37327;&#23376;&#23398;&#20064;&#20013;&#36825;&#19968;&#28857;&#19981;&#20877;&#25104;&#31435;&#65292;&#22240;&#20026;&#35768;&#22810;&#35774;&#32622;&#26082;&#27809;&#26377;SQ&#30340;&#31867;&#27604;&#65292;&#20063;&#27809;&#26377;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#65288;QSQ&#65289;&#30340;&#31867;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;Kearns&#30340;SQ oracle&#21644;Valiant&#30340;&#24369;&#35780;&#20272;oracle&#65288;TOCT'14&#65289;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#24314;&#31435;&#20102;&#32479;&#35745;&#21644;&#21442;&#25968;&#21270;&#23398;&#20064;&#33539;&#20363;&#20043;&#38388;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20174;&#35780;&#20272;oracle&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#35813;oracle&#25552;&#20379;&#20102;&#20989;&#25968;&#20540;&#30340;&#20272;&#35745;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24191;&#27867;&#32780;&#30452;&#35266;&#30340;&#26694;&#26550;&#65292;&#20026;&#20174;&#35780;&#20272;&#26597;&#35810;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#26080;&#26465;&#20214;&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#23545;&#20110;&#23398;&#20064;&#32447;&#24615;&#20989;&#25968;&#31867;&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#36827;&#34892;&#20102;&#21051;&#30011;&#12290;&#35813;&#26694;&#26550;&#30452;&#25509;&#36866;&#29992;&#20110;QSQ&#35774;&#32622;&#20197;&#21450;&#25152;&#26377;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#24212;&#29992;&#26159;&#25193;&#23637;&#20808;&#21069;&#20851;&#20110;&#36755;&#20986;&#21487;&#23398;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kearns' statistical query (SQ) oracle (STOC'93) lends a unifying perspective for most classical machine learning algorithms. This ceases to be true in quantum learning, where many settings do not admit, neither an SQ analog nor a quantum statistical query (QSQ) analog. In this work, we take inspiration from Kearns' SQ oracle and Valiant's weak evaluation oracle (TOCT'14) and establish a unified perspective bridging the statistical and parametrized learning paradigms in a novel way. We explore the problem of learning from an evaluation oracle, which provides an estimate of function values, and introduce an extensive yet intuitive framework that yields unconditional lower bounds for learning from evaluation queries and characterizes the query complexity for learning linear function classes. The framework is directly applicable to the QSQ setting and virtually all algorithms based on loss function optimization.  Our first application is to extend prior results on the learnability of outpu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;k-means&#32858;&#31867;&#26041;&#27861;&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#24674;&#22797;&#26159;&#24369;&#19968;&#33268;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.17712</link><description>&lt;p&gt;
&#20351;&#29992;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Community Detection and Classification Guarantees Using Embeddings Learned by Node2Vec. (arXiv:2310.17712v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;k-means&#32858;&#31867;&#26041;&#27861;&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#24674;&#22797;&#26159;&#24369;&#19968;&#33268;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#32593;&#32476;&#30340;&#33410;&#28857;&#23884;&#20837;&#21040;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#30446;&#26631;&#65292;&#26377;&#21508;&#31181;&#24037;&#20855;&#21487;&#29992;&#12290;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#29992;&#20316;&#31038;&#21306;&#26816;&#27979;/&#33410;&#28857;&#32858;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#31561;&#20219;&#21153;&#30340;&#29305;&#24449;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#38500;&#20102;&#35889;&#32858;&#31867;&#26041;&#27861;&#20043;&#22806;&#65292;&#23545;&#20110;&#20854;&#20182;&#24120;&#29992;&#30340;&#23398;&#20064;&#23884;&#20837;&#26041;&#27861;&#65292;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#30001;node2vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;node2vec&#29983;&#25104;&#30340;&#23884;&#20837;&#21521;&#37327;&#24212;&#29992;k-means&#32858;&#31867;&#21487;&#20197;&#23545;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#24369;&#19968;&#33268;&#30340;&#31038;&#21306;&#24674;&#22797;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36825;&#20123;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#32467;&#26524;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#19982;&#32593;&#32476;&#25968;&#25454;&#30340;&#20854;&#20182;&#23884;&#20837;&#24037;&#20855;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding the nodes of a large network into an Euclidean space is a common objective in modern machine learning, with a variety of tools available. These embeddings can then be used as features for tasks such as community detection/node clustering or link prediction, where they achieve state of the art performance. With the exception of spectral clustering methods, there is little theoretical understanding for other commonly used approaches to learning embeddings. In this work we examine the theoretical properties of the embeddings learned by node2vec. Our main result shows that the use of k-means clustering on the embedding vectors produced by node2vec gives weakly consistent community recovery for the nodes in (degree corrected) stochastic block models. We also discuss the use of these embeddings for node and link prediction tasks. We demonstrate this result empirically, and examine how this relates to other embedding tools for network data.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#30001;&#35821;&#20041;&#36890;&#20449;&#22686;&#24378;&#30340;&#26080;&#32447;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20379;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#20449;&#24687;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#20108;&#36827;&#21046;&#20301;&#25552;&#21462;&#21644;&#20256;&#36755;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#25552;&#20379;&#26368;&#20248;AIGC&#26381;&#21153;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.17705</link><description>&lt;p&gt;
&#19968;&#31181;&#30001;&#35821;&#20041;&#36890;&#20449;&#22686;&#24378;&#30340;&#26080;&#32447;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20379;&#24212;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered by Semantic Communication. (arXiv:2310.17705v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17705
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#30001;&#35821;&#20041;&#36890;&#20449;&#22686;&#24378;&#30340;&#26080;&#32447;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20379;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#20449;&#24687;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#20108;&#36827;&#21046;&#20301;&#25552;&#21462;&#21644;&#20256;&#36755;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#25552;&#20379;&#26368;&#20248;AIGC&#26381;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#36890;&#36807;&#21019;&#24314;&#22810;&#26679;&#21270;&#19988;&#39640;&#36136;&#37327;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#26469;&#28385;&#36275;&#24191;&#22823;&#29992;&#25143;&#32676;&#20307;&#30340;&#38656;&#27714;&#12290;&#38543;&#30528;&#31227;&#21160;&#35774;&#22791;&#30340;&#26222;&#21450;&#21644;&#31227;&#21160;&#27969;&#37327;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#36890;&#36807;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#25552;&#20379;&#23545;&#39640;&#36136;&#37327;AIGC&#26381;&#21153;&#30340;&#26080;&#22788;&#19981;&#22312;&#30340;&#35775;&#38382;&#24050;&#25104;&#20026;AIGC&#20135;&#21697;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#31283;&#23450;&#30340;&#20449;&#36947;&#12289;&#26377;&#38480;&#30340;&#24102;&#23485;&#36164;&#28304;&#21644;&#20998;&#24067;&#19981;&#22343;&#21248;&#30340;&#35745;&#31639;&#36164;&#28304;&#30340;&#26080;&#32447;&#32593;&#32476;&#20013;&#25552;&#20379;&#26368;&#20248;&#30340;AIGC&#26381;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#35821;&#20041;&#36890;&#20449;&#65288;SemCom&#65289;&#22686;&#24378;&#30340;AIGC&#65288;SemAIGC&#65289;&#29983;&#25104;&#21644;&#20256;&#36755;&#26694;&#26550;&#65292;&#20854;&#20013;&#21482;&#38656;&#25552;&#21462;&#21644;&#20256;&#36755;&#20869;&#23481;&#30340;&#35821;&#20041;&#20449;&#24687;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#20108;&#36827;&#21046;&#20301;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SemAIGC&#22312;&#35821;&#20041;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#38598;&#25104;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#20869;&#23481;&#29983;&#25104;&#21644;&#28789;&#27963;&#35843;&#25972;&#35745;&#31639;&#24037;&#20316;&#36127;&#36733;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI applications are recently catering to a vast user base by creating diverse and high-quality AI-generated content (AIGC). With the proliferation of mobile devices and rapid growth of mobile traffic, providing ubiquitous access to high-quality AIGC services via wireless communication networks is becoming the future direction for AIGC products. However, it is challenging to provide optimal AIGC services in wireless networks with unstable channels, limited bandwidth resources, and unevenly distributed computational resources. To tackle these challenges, we propose a semantic communication (SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where only semantic information of the content rather than all the binary bits should be extracted and transmitted by using SemCom. Specifically, SemAIGC integrates diffusion-based models within the semantic encoder and decoder for efficient content generation and flexible adjustment of the computing workload of both tr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#23545;&#29031;&#22240;&#26524;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25935;&#24863;&#23646;&#24615;&#30340;&#21518;&#20195;&#30340;&#23545;&#29031;&#20998;&#24067;&#26469;&#30830;&#20445;&#20844;&#24179;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.17687</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#39044;&#27979;&#30340;&#23545;&#29031;&#22240;&#26524;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Fairness for Predictions using Generative Adversarial Networks. (arXiv:2310.17687v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17687
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#23545;&#29031;&#22240;&#26524;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25935;&#24863;&#23646;&#24615;&#30340;&#21518;&#20195;&#30340;&#23545;&#29031;&#20998;&#24067;&#26469;&#30830;&#20445;&#20844;&#24179;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27861;&#24459;&#12289;&#20262;&#29702;&#21644;&#31038;&#20250;&#21407;&#22240;&#65292;&#39044;&#27979;&#20013;&#30340;&#20844;&#24179;&#24615;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#36890;&#24120;&#36890;&#36807;&#23545;&#29031;&#22240;&#26524;&#20844;&#24179;&#24615;&#26469;&#23454;&#29616;&#65292;&#35813;&#20844;&#24179;&#24615;&#30830;&#20445;&#20010;&#20307;&#30340;&#39044;&#27979;&#19982;&#22312;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#19979;&#30340;&#23545;&#29031;&#19990;&#30028;&#20013;&#30340;&#39044;&#27979;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#35201;&#23454;&#29616;&#23545;&#29031;&#22240;&#26524;&#20844;&#24179;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23545;&#29031;&#26159;&#19981;&#21487;&#35266;&#23519;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;&#23545;&#29031;&#22240;&#26524;&#20844;&#24179;&#24615;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GCFN&#65289;&#65292;&#29992;&#20110;&#22312;&#23545;&#29031;&#22240;&#26524;&#20844;&#24179;&#24615;&#19979;&#36827;&#34892;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30452;&#25509;&#23398;&#20064;&#25935;&#24863;&#23646;&#24615;&#30340;&#21518;&#20195;&#30340;&#23545;&#29031;&#20998;&#24067;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#29031;&#23186;&#20171;&#27491;&#21017;&#21270;&#26469;&#23454;&#26045;&#20844;&#24179;&#39044;&#27979;&#12290;&#22914;&#26524;&#23545;&#29031;&#20998;&#24067;&#23398;&#20064;&#24471;&#36275;&#22815;&#22909;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#23398;&#19978;&#30830;&#20445;&#23545;&#29031;&#22240;&#26524;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;GCFN&#35299;&#20915;&#20102;&#23545;&#29031;&#22240;&#26524;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness in predictions is of direct importance in practice due to legal, ethical, and societal reasons. It is often achieved through counterfactual fairness, which ensures that the prediction for an individual is the same as that in a counterfactual world under a different sensitive attribute. However, achieving counterfactual fairness is challenging as counterfactuals are unobservable. In this paper, we develop a novel deep neural network called Generative Counterfactual Fairness Network (GCFN) for making predictions under counterfactual fairness. Specifically, we leverage a tailored generative adversarial network to directly learn the counterfactual distribution of the descendants of the sensitive attribute, which we then use to enforce fair predictions through a novel counterfactual mediator regularization. If the counterfactual distribution is learned sufficiently well, our method is mathematically guaranteed to ensure the notion of counterfactual fairness. Thereby, our GCFN addre
&lt;/p&gt;</description></item><item><title>Sliceformer &#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340; Transformer &#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#8220;&#20999;&#29255;-&#25490;&#24207;&#8221;&#25805;&#20316;&#26367;&#20195;&#20102;&#20256;&#32479;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#20854;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#25968;&#20540;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.17683</link><description>&lt;p&gt;
Sliceformer: &#23558;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#31616;&#21270;&#20026;&#25490;&#24207;&#22312;&#21028;&#21035;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sliceformer: Make Multi-head Attention as Simple as Sorting in Discriminative Tasks. (arXiv:2310.17683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17683
&lt;/p&gt;
&lt;p&gt;
Sliceformer &#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340; Transformer &#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#8220;&#20999;&#29255;-&#25490;&#24207;&#8221;&#25805;&#20316;&#26367;&#20195;&#20102;&#20256;&#32479;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#20854;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#25968;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26368;&#27969;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#20043;&#19968;&#65292;Transformer&#22312;&#35768;&#22810;&#22522;&#30784;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#65292;&#20363;&#22914;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;ViT&#21644;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;BERT&#21644;GPT&#12290;Transformer&#30340;&#26377;&#25928;&#24615;&#36890;&#24120;&#24402;&#22240;&#20110;&#20854;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;MHA&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#30001;&#20110;&#20854;&#8220;&#26597;&#35810;-&#38190;-&#20540;&#8221;&#26550;&#26500;&#32780;&#23548;&#33268;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#20197;&#21450;&#30001;&#20854;softmax&#25805;&#20316;&#24341;&#36215;&#30340;&#25968;&#20540;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#19978;&#36848;&#38382;&#39064;&#21644;&#27880;&#24847;&#21147;&#23618;&#30340;&#26368;&#36817;&#21457;&#23637;&#36235;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;Transformer&#26367;&#20195;&#26041;&#26696;&#65292;&#31216;&#20026;Sliceformer&#12290;&#25105;&#20204;&#30340;Sliceformer&#29992;&#19968;&#20010;&#26497;&#20854;&#31616;&#21333;&#30340;&#8220;&#20999;&#29255;-&#25490;&#24207;&#8221;&#25805;&#20316;&#26367;&#20195;&#20102;&#32463;&#20856;&#30340;MHA&#26426;&#21046;&#65292;&#21363;&#23558;&#36755;&#20837;&#32447;&#24615;&#25237;&#24433;&#21040;&#28508;&#31354;&#38388;&#65292;&#24182;&#27839;&#30528;&#19981;&#21516;&#30340;&#29305;&#24449;&#32500;&#24230;&#65288;&#25110;&#31561;&#20215;&#22320;&#31216;&#20026;&#36890;&#36947;&#65289;&#36827;&#34892;&#25490;&#24207;&#12290;&#23545;&#20110;&#27599;&#20010;&#29305;&#24449;&#32500;&#24230;&#65292;&#25490;&#24207;&#25805;&#20316;&#38544;&#21547;&#22320;&#29983;&#25104;&#20102;&#38544;&#34255;&#30340;&#21442;&#25968;&#21270;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
As one of the most popular neural network modules, Transformer plays a central role in many fundamental deep learning models, e.g., the ViT in computer vision and the BERT and GPT in natural language processing. The effectiveness of the Transformer is often attributed to its multi-head attention (MHA) mechanism. In this study, we discuss the limitations of MHA, including the high computational complexity due to its ``query-key-value'' architecture and the numerical issue caused by its softmax operation. Considering the above problems and the recent development tendency of the attention layer, we propose an effective and efficient surrogate of the Transformer, called Sliceformer. Our Sliceformer replaces the classic MHA mechanism with an extremely simple ``slicing-sorting'' operation, i.e., projecting inputs linearly to a latent space and sorting them along different feature dimensions (or equivalently, called channels). For each feature dimension, the sorting operation implicitly gener
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#35782;&#21035;&#30340;&#20363;&#23376;&#65292;&#19968;&#20010;&#26159;&#35782;&#21035;&#26408;&#21355;&#20108;&#28151;&#20081;&#21306;&#22495;&#30340;&#20912;&#22359;&#65292;&#21478;&#19968;&#20010;&#26159;&#35782;&#21035;&#27888;&#22374;&#19978;&#30340;&#20113;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21644;&#26032;&#25968;&#25454;&#27979;&#35797;&#65292;&#20998;&#21035;&#23454;&#29616;&#20102;68%&#21644;95%&#30340;&#31934;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2310.17681</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#34892;&#26143;&#31185;&#23398;&#25968;&#25454;&#38598;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Feature Extraction and Classification from Planetary Science Datasets enabled by Machine Learning. (arXiv:2310.17681v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#35782;&#21035;&#30340;&#20363;&#23376;&#65292;&#19968;&#20010;&#26159;&#35782;&#21035;&#26408;&#21355;&#20108;&#28151;&#20081;&#21306;&#22495;&#30340;&#20912;&#22359;&#65292;&#21478;&#19968;&#20010;&#26159;&#35782;&#21035;&#27888;&#22374;&#19978;&#30340;&#20113;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21644;&#26032;&#25968;&#25454;&#27979;&#35797;&#65292;&#20998;&#21035;&#23454;&#29616;&#20102;68%&#21644;95%&#30340;&#31934;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25105;&#20204;&#26368;&#36817;&#36827;&#34892;&#30340;&#30740;&#31350;&#30340;&#20363;&#23376;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#23545;&#22806;&#34892;&#26143;&#20219;&#21153;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#29305;&#24449;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#30740;&#31350;&#26159;&#22312;&#26408;&#21355;&#20108;&#30340;&#28151;&#20081;&#21306;&#22495;&#20013;&#35782;&#21035;&#20912;&#22359;&#65288;&#20063;&#34987;&#31216;&#20026;&#28418;&#28014;&#22359;&#12289;&#26495;&#22359;&#12289;&#22810;&#36793;&#24418;&#65289;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23545;&#34892;&#19994;&#26631;&#20934;&#30340;Mask R-CNN&#65288;&#22522;&#20110;&#21306;&#22495;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#28155;&#21152;&#21644;&#35757;&#32451;&#26032;&#30340;&#23618;&#27425;&#65292;&#20197;&#35782;&#21035;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26631;&#35760;&#30340;&#22359;&#12290;&#38543;&#21518;&#65292;&#26356;&#26032;&#21518;&#30340;&#27169;&#22411;&#34987;&#27979;&#35797;&#20110;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;68%&#30340;&#31934;&#24230;&#12290;&#22312;&#19981;&#21516;&#30340;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#23558;Mask R-CNN&#24212;&#29992;&#20110;&#27888;&#22374;&#19978;&#20113;&#30340;&#35782;&#21035;&#65292;&#21516;&#26679;&#36890;&#36807;&#26356;&#26032;&#30340;&#35757;&#32451;&#21644;&#23545;&#26032;&#25968;&#25454;&#30340;&#27979;&#35797;&#65292;&#33719;&#24471;&#20102;369&#24133;&#22270;&#20687;&#19978;95%&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30456;&#23545;&#25104;&#21151;&#30340;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#21644;&#35782;&#21035;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#34892;&#26143;&#31185;&#23398;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present two examples of recent investigations that we have undertaken, applying Machine Learning (ML) neural networks (NN) to image datasets from outer planet missions to achieve feature recognition. Our first investigation was to recognize ice blocks (also known as rafts, plates, polygons) in the chaos regions of fractured ice on Europa. We used a transfer learning approach, adding and training new layers to an industry-standard Mask R-CNN (Region-based Convolutional Neural Network) to recognize labeled blocks in a training dataset. Subsequently, the updated model was tested against a new dataset, achieving 68% precision. In a different application, we applied the Mask R-CNN to recognize clouds on Titan, again through updated training followed by testing against new data, with a precision of 95% over 369 images. We evaluate the relative successes of our techniques and suggest how training and recognition could be further improved. The new approaches we have used for p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#26368;&#20339;&#39034;&#24207;&#20998;&#25968;&#25628;&#32034;&#65288;BOSS&#65289;&#21644;&#29983;&#38271;-&#25910;&#32553;&#26641;&#65288;GSTs&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#25968;&#30334;&#20010;&#39640;&#24230;&#36830;&#25509;&#30340;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#20174;fMRI&#25968;&#25454;&#20013;&#24674;&#22797;&#33041;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2310.17679</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#20339;&#39034;&#24207;&#20998;&#25968;&#25628;&#32034;&#21644;&#29983;&#38271;-&#25910;&#32553;&#26641;&#24555;&#36895;&#25193;&#23637;&#30340;DAG&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow-Shrink Trees. (arXiv:2310.17679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#26368;&#20339;&#39034;&#24207;&#20998;&#25968;&#25628;&#32034;&#65288;BOSS&#65289;&#21644;&#29983;&#38271;-&#25910;&#32553;&#26641;&#65288;GSTs&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#25968;&#30334;&#20010;&#39640;&#24230;&#36830;&#25509;&#30340;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#20174;fMRI&#25968;&#25454;&#20013;&#24674;&#22797;&#33041;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22270;&#24418;&#26465;&#20214;&#29420;&#31435;&#32467;&#26500;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20063;&#26159;&#22240;&#26524;&#21457;&#29616;&#30340;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#26102;&#38388;&#36890;&#24120;&#38590;&#20197;&#36866;&#24212;&#20855;&#26377;&#25968;&#30334;&#20010;&#39640;&#24230;&#36830;&#25509;&#30340;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#20174;fMRI&#25968;&#25454;&#20013;&#24674;&#22797;&#33041;&#32593;&#32476;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26368;&#20339;&#39034;&#24207;&#20998;&#25968;&#25628;&#32034;&#65288;BOSS&#65289;&#21644;&#29983;&#38271;-&#25910;&#32553;&#26641;&#65288;GSTs&#65289;&#29992;&#20110;&#22312;&#36825;&#20010;&#33539;&#20363;&#20013;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#12290;BOSS&#36138;&#23146;&#22320;&#25628;&#32034;&#21464;&#37327;&#30340;&#25490;&#21015;&#65292;&#20351;&#29992;GSTs&#20174;&#25490;&#21015;&#26500;&#24314;&#21644;&#35780;&#20998;DAGs&#12290;GSTs&#26377;&#25928;&#22320;&#32531;&#23384;&#20998;&#25968;&#20197;&#28040;&#38500;&#20887;&#20313;&#35745;&#31639;&#12290;BOSS&#22312;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#24191;&#27867;&#30340;&#26465;&#20214;&#19979;&#19982;&#21508;&#31181;&#32452;&#21512;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#21033;&#30340;&#27604;&#36739;&#12290;&#20026;&#20102;&#35777;&#26126;&#23427;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;BOSS&#24212;&#29992;&#20110;&#20004;&#32452;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#65306;&#24102;&#26377;&#20266;&#32463;&#39564;&#22122;&#22768;&#20998;&#24067;&#30340;&#27169;&#25311;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Learning graphical conditional independence structures is an important machine learning problem and a cornerstone of causal discovery. However, the accuracy and execution time of learning algorithms generally struggle to scale to problems with hundreds of highly connected variables -- for instance, recovering brain networks from fMRI data. We introduce the best order score search (BOSS) and grow-shrink trees (GSTs) for learning directed acyclic graphs (DAGs) in this paradigm. BOSS greedily searches over permutations of variables, using GSTs to construct and score DAGs from permutations. GSTs efficiently cache scores to eliminate redundant calculations. BOSS achieves state-of-the-art performance in accuracy and execution time, comparing favorably to a variety of combinatorial and gradient-based learning algorithms under a broad range of conditions. To demonstrate its practicality, we apply BOSS to two sets of resting-state fMRI data: simulated data with pseudo-empirical noise distributi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#31354;&#23545;&#27604;&#23398;&#20064;&#65288;CL4ST&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#21644;&#20010;&#24615;&#21270;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17678</link><description>&lt;p&gt;
&#26102;&#31354;&#20803;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Meta Contrastive Learning. (arXiv:2310.17678v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17678
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#31354;&#23545;&#27604;&#23398;&#20064;&#65288;CL4ST&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#21644;&#20010;&#24615;&#21270;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#39044;&#27979;&#22312;&#21253;&#25324;&#20132;&#36890;&#39044;&#27979;&#21644;&#29359;&#32618;&#39044;&#27979;&#22312;&#20869;&#30340;&#22810;&#20010;&#29616;&#23454;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26088;&#22312;&#25552;&#39640;&#20844;&#20849;&#20132;&#36890;&#21644;&#23433;&#20840;&#31649;&#29702;&#12290;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNN&#65289;&#25429;&#25417;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#12290;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65292;&#22914;&#25968;&#25454;&#31232;&#32570;&#21644;&#31232;&#30095;&#24615;&#65292;&#23548;&#33268;&#25968;&#25454;&#22122;&#38899;&#21644;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#65292;&#26497;&#22823;&#38480;&#21046;&#20102;STGNN&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#36817;&#26399;&#24341;&#20837;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;STGNN&#27169;&#22411;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#22823;&#22810;&#25968;&#27169;&#22411;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36807;&#20110;&#20381;&#36182;&#20110;&#25163;&#21160;&#35774;&#35745;&#65292;&#19981;&#33021;&#38024;&#23545;&#19981;&#21516;&#30340;&#26102;&#31354;&#22270;&#65288;STG&#65289;&#22330;&#26223;&#36827;&#34892;&#23450;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#23545;&#27604;&#23398;&#20064;&#65288;CL4ST&#65289;&#26694;&#26550;&#26469;&#32534;&#30721;&#20581;&#22766;&#19988;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal prediction is crucial in numerous real-world applications, including traffic forecasting and crime prediction, which aim to improve public transportation and safety management. Many state-of-the-art models demonstrate the strong capability of spatio-temporal graph neural networks (STGNN) to capture complex spatio-temporal correlations. However, despite their effectiveness, existing approaches do not adequately address several key challenges. Data quality issues, such as data scarcity and sparsity, lead to data noise and a lack of supervised signals, which significantly limit the performance of STGNN. Although recent STGNN models with contrastive learning aim to address these challenges, most of them use pre-defined augmentation strategies that heavily depend on manual design and cannot be customized for different Spatio-Temporal Graph (STG) scenarios. To tackle these challenges, we propose a new spatio-temporal contrastive learning (CL4ST) framework to encode robust and
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#26234;&#33021;&#25163;&#26426;&#40614;&#20811;&#39118;&#20013;&#30340;&#21683;&#22013;&#22768;&#38899;&#26469;&#26816;&#27979;&#32467;&#26680;&#30149;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#26089;&#26399;&#35786;&#26029;&#21644;&#30417;&#27979;&#27835;&#30103;&#36827;&#23637;&#65292;&#20174;&#32780;&#20026;&#20840;&#29699;&#32467;&#26680;&#30149;&#30340;&#20998;&#31867;&#20351;&#29992;&#25552;&#20379;&#26356;&#26131;&#20110;&#33719;&#21462;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.17675</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21683;&#22013;&#22768;&#38899;&#20998;&#26512;&#26089;&#26399;&#26816;&#27979;&#32467;&#26680;&#30149;&#65306;&#26397;&#30528;&#26356;&#26131;&#20110;&#20840;&#29699;&#20998;&#31867;&#20351;&#29992;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Early Detection of Tuberculosis with Machine Learning Cough Audio Analysis: Towards More Accessible Global Triaging Usage. (arXiv:2310.17675v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17675
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#26234;&#33021;&#25163;&#26426;&#40614;&#20811;&#39118;&#20013;&#30340;&#21683;&#22013;&#22768;&#38899;&#26469;&#26816;&#27979;&#32467;&#26680;&#30149;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#26089;&#26399;&#35786;&#26029;&#21644;&#30417;&#27979;&#27835;&#30103;&#36827;&#23637;&#65292;&#20174;&#32780;&#20026;&#20840;&#29699;&#32467;&#26680;&#30149;&#30340;&#20998;&#31867;&#20351;&#29992;&#25552;&#20379;&#26356;&#26131;&#20110;&#33719;&#21462;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26680;&#30149;&#65288;TB&#65289;&#26159;&#19968;&#31181;&#20027;&#35201;&#24433;&#21709;&#32954;&#37096;&#30340;&#32454;&#33740;&#24615;&#30142;&#30149;&#65292;&#26159;&#20840;&#29699;&#27515;&#20129;&#29575;&#26368;&#39640;&#30340;&#20256;&#26579;&#30149;&#20043;&#19968;&#12290;&#21450;&#26102;&#26377;&#25928;&#30340;&#25239;&#32467;&#26680;&#27835;&#30103;&#23545;&#20110;&#38450;&#27490;&#32467;&#26680;&#30149;&#22312;&#20307;&#20869;&#20256;&#25773;&#65292;&#36991;&#20813;&#33268;&#21629;&#24182;&#21457;&#30151;&#33267;&#20851;&#37325;&#35201;&#12290;&#21683;&#22013;&#20316;&#20026;&#32467;&#26680;&#30149;&#30340;&#23458;&#35266;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#26159;&#19968;&#31181;&#30417;&#27979;&#27835;&#30103;&#21453;&#24212;&#24182;&#38543;&#30528;&#25104;&#21151;&#27835;&#30103;&#32780;&#20943;&#23569;&#30340;&#20998;&#31867;&#24037;&#20855;&#12290;&#30446;&#21069;&#29992;&#20110;&#32467;&#26680;&#30149;&#35786;&#26029;&#30340;&#40644;&#37329;&#26631;&#20934;&#26041;&#27861;&#32531;&#24930;&#19988;&#19981;&#26131;&#33719;&#21462;&#65292;&#23588;&#20854;&#22312;&#20065;&#26449;&#22320;&#21306;&#32467;&#26680;&#30149;&#30340;&#27969;&#34892;&#31243;&#24230;&#26368;&#39640;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#35786;&#26029;&#30740;&#31350;&#65292;&#22914;&#21033;&#29992;&#33016;&#37096;X&#20809;&#29255;&#65292;&#26080;&#27861;&#26377;&#25928;&#30417;&#27979;&#27835;&#30103;&#36827;&#23637;&#12290;&#20026;&#20102;&#23454;&#29616;&#26377;&#25928;&#30340;&#35786;&#26029;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#20174;&#26234;&#33021;&#25163;&#26426;&#30340;&#40614;&#20811;&#39118;&#20013;&#20998;&#26512;&#21683;&#22013;&#22768;&#38899;&#30340;&#27969;&#34892;&#30149;&#23398;&#29305;&#24449;&#26469;&#26816;&#27979;&#32467;&#26680;&#30149;&#12290;&#35813;&#26550;&#26500;&#21253;&#25324;&#19968;&#20010;2D-CNN&#21644;XGBoost&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;7&#20010;&#22269;&#23478;&#30340;724,964&#20010;&#21683;&#22013;&#22768;&#38899;&#26679;&#26412;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tuberculosis (TB), a bacterial disease mainly affecting the lungs, is one of the leading infectious causes of mortality worldwide. To prevent TB from spreading within the body, which causes life-threatening complications, timely and effective anti-TB treatment is crucial. Cough, an objective biomarker for TB, is a triage tool that monitors treatment response and regresses with successful therapy. Current gold standards for TB diagnosis are slow or inaccessible, especially in rural areas where TB is most prevalent. In addition, current machine learning (ML) diagnosis research, like utilizing chest radiographs, is ineffective and does not monitor treatment progression. To enable effective diagnosis, an ensemble model was developed that analyzes, using a novel ML architecture, coughs' acoustic epidemiologies from smartphones' microphones to detect TB. The architecture includes a 2D-CNN and XGBoost that was trained on 724,964 cough audio samples and demographics from 7 countries. After fea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#21644;&#29615;&#22659;&#20013;&#30340;&#27169;&#25311;&#26469;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#26377;&#25928;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.17671</link><description>&lt;p&gt;
&#23558;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#20174;&#27169;&#22411;&#20256;&#36882;&#21040;&#30828;&#20214;&#22312;&#29615;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transfer of Reinforcement Learning-Based Controllers from Model- to Hardware-in-the-Loop. (arXiv:2310.17671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#21644;&#29615;&#22659;&#20013;&#30340;&#27169;&#25311;&#26469;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#26377;&#25928;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#25511;&#21046;&#21151;&#33021;&#26159;&#36164;&#28304;&#12289;&#26102;&#38388;&#21644;&#25968;&#25454;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#65292;&#32463;&#24120;&#23548;&#33268;&#27425;&#20248;&#30340;&#25104;&#26412;&#21644;&#35299;&#20915;&#26041;&#27861;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20855;&#26377;&#22312;&#26368;&#23567;&#20154;&#20026;&#24178;&#39044;&#19979;&#33258;&#21160;&#35757;&#32451;&#20195;&#29702;&#25191;&#34892;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#29983;&#25104;&#30340;&#25104;&#26412;&#21644;&#23433;&#20840;&#32422;&#26463;&#65292;&#20854;&#24212;&#29992;&#22823;&#22810;&#38480;&#20110;&#32431;&#31929;&#30340;&#27169;&#25311;&#39046;&#22495;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#21151;&#33021;&#24320;&#21457;&#20013;&#20351;&#29992;RL&#65292;&#29983;&#25104;&#30340;&#20195;&#29702;&#24517;&#39035;&#33021;&#22815;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#21644;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#65288;XiL&#65289;&#27169;&#25311;&#26469;&#21152;&#36895;RL&#20195;&#29702;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#23545;&#20110;&#20869;&#29123;&#26426;&#30340;&#30636;&#24577;&#24223;&#27668;&#20877;&#24490;&#29615;&#25511;&#21046;&#26696;&#20363;&#65292;&#20351;&#29992;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#27169;&#22411;&#22312;&#29615;&#65288;MiL&#65289;&#27169;&#25311;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#31639;&#27861;&#65292;&#24494;&#35843;&#36229;&#21442;&#25968;&#65292;&#26368;&#21518;&#35757;&#32451;&#20505;&#36873;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of developing control functions for embedded systems is resource-, time-, and data-intensive, often resulting in sub-optimal cost and solutions approaches. Reinforcement Learning (RL) has great potential for autonomously training agents to perform complex control tasks with minimal human intervention. Due to costly data generation and safety constraints, however, its application is mostly limited to purely simulated domains. To use RL effectively in embedded system function development, the generated agents must be able to handle real-world applications. In this context, this work focuses on accelerating the training process of RL agents by combining Transfer Learning (TL) and X-in-the-Loop (XiL) simulation. For the use case of transient exhaust gas re-circulation control for an internal combustion engine, use of a computationally cheap Model-in-the-Loop (MiL) simulation is made to select a suitable algorithm, fine-tune hyperparameters, and finally train candidate agents fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#20307;&#20915;&#31574;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#26694;&#26550;&#65292;&#22312;&#39044;&#27979;&#32500;&#25252;&#24212;&#29992;&#20013;&#36890;&#36807;&#21516;&#26102;&#22788;&#29702;&#24050;&#30693;&#21644;&#26410;&#30693;&#20581;&#24247;&#29366;&#24577;&#65292;&#35299;&#20915;&#20102;&#24120;&#35268;CNN&#26041;&#26696;&#26080;&#27861;&#22788;&#29702;&#26032;&#24322;&#24120;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.17670</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#38598;&#20307;&#20915;&#31574;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#35782;&#21035;&#26410;&#30693;&#20581;&#24247;&#29366;&#24577;&#22312;&#39044;&#27979;&#32500;&#25252;&#24212;&#29992;&#20013; (arXiv:2310.17670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Unknown Health States Recognition With Collective Decision Based Deep Learning Networks In Predictive Maintenance Applications. (arXiv:2310.17670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#20307;&#20915;&#31574;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#26694;&#26550;&#65292;&#22312;&#39044;&#27979;&#32500;&#25252;&#24212;&#29992;&#20013;&#36890;&#36807;&#21516;&#26102;&#22788;&#29702;&#24050;&#30693;&#21644;&#26410;&#30693;&#20581;&#24247;&#29366;&#24577;&#65292;&#35299;&#20915;&#20102;&#24120;&#35268;CNN&#26041;&#26696;&#26080;&#27861;&#22788;&#29702;&#26032;&#24322;&#24120;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#33021;&#21147;&#30340;&#24555;&#36895;&#25552;&#21319;&#19979;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#30340;&#20915;&#31574;&#26041;&#26696;&#22312;&#39044;&#27979;&#32500;&#25252;&#24212;&#29992;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#20381;&#38752;&#20849;&#20139;&#26435;&#37325;&#21644;&#31354;&#38388;&#27744;&#21270;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#20174;&#24037;&#19994;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#26377;&#25928;&#30340;&#20581;&#24247;&#29366;&#24577;&#34920;&#31034;&#12290;&#35768;&#22810;&#22522;&#20110;CNN&#30340;&#26041;&#26696;&#65292;&#22914;&#24341;&#20837;&#27531;&#24046;&#23398;&#20064;&#21644;&#22810;&#23610;&#24230;&#23398;&#20064;&#30340;&#39640;&#32423;CNN&#65292;&#24050;&#32463;&#22312;&#20581;&#24247;&#29366;&#24577;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#26696;&#26080;&#27861;&#22788;&#29702;&#23646;&#20110;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#29366;&#24577;&#31867;&#21035;&#30340;&#26032;&#24322;&#24120;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21516;CNN&#30340;&#38598;&#20307;&#20915;&#31574;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#19968;&#31181;&#19968;&#23545;&#22810;&#32593;&#32476;(OVRN)&#65292;&#21516;&#26102;&#23454;&#29616;&#24050;&#30693;&#21644;&#26410;&#30693;&#20581;&#24247;&#29366;&#24577;&#30340;&#20998;&#31867;&#12290;OVRN&#23398;&#20064;&#29366;&#24577;&#29305;&#23450;&#30340;&#21028;&#21035;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
At present, decision making solutions developed based on deep learning (DL) models have received extensive attention in predictive maintenance (PM) applications along with the rapid improvement of computing power. Relying on the superior properties of shared weights and spatial pooling, Convolutional Neural Network (CNN) can learn effective representations of health states from industrial data. Many developed CNN-based schemes, such as advanced CNNs that introduce residual learning and multi-scale learning, have shown good performance in health state recognition tasks under the assumption that all the classes are known. However, these schemes have no ability to deal with new abnormal samples that belong to state classes not part of the training set. In this paper, a collective decision framework for different CNNs is proposed. It is based on a One-vs-Rest network (OVRN) to simultaneously achieve classification of known and unknown health states. OVRN learn state-specific discriminative
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#23450;&#20041;&#26041;&#27861;&#65292;&#26088;&#22312;&#20248;&#21270;&#25628;&#32034;&#26102;&#38388;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.17669</link><description>&lt;p&gt;
&#19968;&#31181;&#39640;&#25928;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#23450;&#20041;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Approach for Efficient Neural Architecture Search Space Definition. (arXiv:2310.17669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17669
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#23450;&#20041;&#26041;&#27861;&#65292;&#26088;&#22312;&#20248;&#21270;&#25628;&#32034;&#26102;&#38388;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24555;&#36895;&#21457;&#23637;&#30340;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#21508;&#31181;&#26032;&#30340;&#12289;&#26356;&#22797;&#26434;&#30340;&#31070;&#32463;&#26550;&#26500;&#27491;&#22312;&#20986;&#29616;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#39640;&#25928;&#20351;&#29992;&#38656;&#35201;&#20808;&#36827;&#30340;&#30693;&#35782;&#21644;&#19987;&#19994;&#25216;&#33021;&#65292;&#36825;&#22312;&#21171;&#21160;&#21147;&#24066;&#22330;&#19978;&#24448;&#24448;&#38590;&#20197;&#25214;&#21040;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20351;&#29992;&#35797;&#38169;&#26041;&#27861;&#25163;&#21160;&#25628;&#32034;&#20248;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#21644;&#24037;&#20855;&#26469;&#36741;&#21161;&#31070;&#32463;&#26550;&#26500;&#30340;&#29992;&#25143;&#65292;&#36825;&#23548;&#33268;&#20102;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#30340;&#28909;&#20999;&#20851;&#27880;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#26159;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21333;&#20803;&#30340;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#65292;&#26131;&#20110;&#29702;&#35299;&#21644;&#25805;&#20316;&#12290;&#35813;&#26041;&#27861;&#30340;&#30446;&#26631;&#26159;&#20248;&#21270;&#25628;&#32034;&#26102;&#38388;&#65292;&#24182;&#19988;&#36275;&#22815;&#36890;&#29992;&#65292;&#33021;&#22815;&#22788;&#29702;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
As we advance in the fast-growing era of Machine Learning, various new and more complex neural architectures are arising to tackle problem more efficiently. On the one hand their efficient usage requires advanced knowledge and expertise, which is most of the time difficult to find on the labor market. On the other hand, searching for an optimized neural architecture is a time-consuming task when it is performed manually using a trial and error approach. Hence, a method and a tool support is needed to assist users of neural architectures, leading to an eagerness in the field of Automatic Machine Learning (AutoML). When it comes to Deep Learning, an important part of AutoML is the Neural Architecture Search (NAS). In this paper, we propose a novel cell-based hierarchical search space, easy to comprehend and manipulate. The objectives of the proposed approach are to optimize the search-time and to be general enough to handle most of state of the art Convolutional Neural Networks (CNN) arc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24178;&#20928;&#25968;&#25454;&#38598;&#19978;&#65292;&#23545;&#20110;&#22122;&#22768;&#26631;&#31614;&#24773;&#26223;&#30340;&#25506;&#32034;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.17668</link><description>&lt;p&gt;
&#20026;&#20102;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine tuning Pre trained Models for Robustness Under Noisy Labels. (arXiv:2310.17668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17668
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24178;&#20928;&#25968;&#25454;&#38598;&#19978;&#65292;&#23545;&#20110;&#22122;&#22768;&#26631;&#31614;&#24773;&#26223;&#30340;&#25506;&#32034;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#20250;&#26174;&#33879;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#23398;&#20064;&#22122;&#22768;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#24178;&#20928;&#26679;&#26412;&#24182;&#20943;&#23569;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#38480;&#21046;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26576;&#19968;&#37096;&#20998;&#30340;&#24433;&#21709;&#21487;&#33021;&#20250;&#23548;&#33268;&#25972;&#20307;&#27867;&#21270;&#24615;&#33021;&#30340;&#38477;&#20302;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#32771;&#34385;&#20102;&#36890;&#36807;&#21033;&#29992;&#24040;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#35880;&#24910;&#21033;&#29992;&#22122;&#22768;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#19981;&#26029;&#22686;&#21152;&#30340;&#35757;&#32451;&#25104;&#26412;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#25928;&#29575;&#12290;&#22312;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#65292;&#20154;&#20204;&#27491;&#22312;&#19987;&#27880;&#20110;&#24320;&#21457;&#38024;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24494;&#35843;&#25216;&#26415;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#27867;&#21270;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#24178;&#20928;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#23545;&#20110;&#22122;&#22768;&#26631;&#31614;&#24773;&#26223;&#30340;&#25506;&#32034;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#31181;&#36866;&#29992;&#20110;&#22122;&#22768;&#26631;&#31614;&#22330;&#26223;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The presence of noisy labels in a training dataset can significantly impact the performance of machine learning models. To tackle this issue, researchers have explored methods for Learning with Noisy Labels to identify clean samples and reduce the influence of noisy labels. However, constraining the influence of a certain portion of the training dataset can result in a reduction in overall generalization performance. To alleviate this, recent studies have considered the careful utilization of noisy labels by leveraging huge computational resources. Therefore, the increasing training cost necessitates a reevaluation of efficiency. In other areas of research, there has been a focus on developing fine-tuning techniques for large pre-trained models that aim to achieve both high generalization performance and efficiency. However, these methods have mainly concentrated on clean datasets, and there has been limited exploration of the noisy label scenario. In this research, our aim is to find 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#31471;&#21040;&#31471;&#32423;&#32852;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#29305;&#23450;&#27169;&#22359;&#19978;&#24212;&#29992;&#20923;&#32467;&#12289;&#25554;&#20837;&#36866;&#37197;&#22120;&#21644;&#24494;&#35843;&#31561;&#33258;&#36866;&#24212;&#25805;&#20316;&#65292;&#24182;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#28155;&#21152;&#24809;&#32602;&#39033;&#65292;&#25104;&#21151;&#38480;&#21046;&#20102;&#25628;&#32034;&#21040;&#30340;&#26550;&#26500;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25628;&#32034;&#20986;&#19982;&#25163;&#24037;&#35774;&#35745;&#31867;&#20284;&#30340;&#35843;&#25972;&#26041;&#26696;&#65292;&#24182;&#23558;&#20248;&#21270;&#21442;&#25968;&#21387;&#32553;&#21040;&#20102;&#20840;&#24494;&#35843;&#30340;8.7%&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17664</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#32423;&#32852;&#22810;&#20219;&#21153;&#33258;&#36866;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cascaded Multi-task Adaptive Learning Based on Neural Architecture Search. (arXiv:2310.17664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17664
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#31471;&#21040;&#31471;&#32423;&#32852;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#29305;&#23450;&#27169;&#22359;&#19978;&#24212;&#29992;&#20923;&#32467;&#12289;&#25554;&#20837;&#36866;&#37197;&#22120;&#21644;&#24494;&#35843;&#31561;&#33258;&#36866;&#24212;&#25805;&#20316;&#65292;&#24182;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#28155;&#21152;&#24809;&#32602;&#39033;&#65292;&#25104;&#21151;&#38480;&#21046;&#20102;&#25628;&#32034;&#21040;&#30340;&#26550;&#26500;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25628;&#32034;&#20986;&#19982;&#25163;&#24037;&#35774;&#35745;&#31867;&#20284;&#30340;&#35843;&#25972;&#26041;&#26696;&#65292;&#24182;&#23558;&#20248;&#21270;&#21442;&#25968;&#21387;&#32553;&#21040;&#20102;&#20840;&#24494;&#35843;&#30340;8.7%&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32423;&#32852;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#26500;&#24314;&#31471;&#21040;&#31471;&#31995;&#32479;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#25972;&#20010;&#32423;&#32852;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#22312;&#21442;&#25968;&#21644;&#20869;&#23384;&#25928;&#29575;&#19978;&#24182;&#19981;&#39640;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#22312;&#32423;&#32852;&#27169;&#22411;&#19978;&#24212;&#29992;&#36866;&#37197;&#22120;&#27169;&#22359;&#26080;&#27861;&#36798;&#21040;&#19982;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#31471;&#21040;&#31471;&#32423;&#32852;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#27599;&#20010;&#29305;&#23450;&#27169;&#22411;&#19978;&#30340;&#20505;&#36873;&#33258;&#36866;&#24212;&#25805;&#20316;&#21253;&#25324;&#20923;&#32467;&#12289;&#25554;&#20837;&#36866;&#37197;&#22120;&#21644;&#24494;&#35843;&#12290;&#25105;&#20204;&#36824;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#28155;&#21152;&#20102;&#19968;&#20010;&#24809;&#32602;&#39033;&#65292;&#20197;&#38480;&#21046;&#23398;&#20064;&#21040;&#30340;&#32467;&#26500;&#65292;&#24182;&#32771;&#34385;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#24809;&#32602;&#39033;&#25104;&#21151;&#38480;&#21046;&#20102;&#25628;&#32034;&#21040;&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#25628;&#32034;&#20986;&#19982;&#25163;&#24037;&#35774;&#35745;&#31867;&#20284;&#30340;&#35843;&#25972;&#26041;&#26696;&#65292;&#23558;&#20248;&#21270;&#21442;&#25968;&#21387;&#32553;&#21040;&#20102;&#20840;&#24494;&#35843;&#30340;8.7%&#65292;&#19988;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cascading multiple pre-trained models is an effective way to compose an end-to-end system. However, fine-tuning the full cascaded model is parameter and memory inefficient and our observations reveal that only applying adapter modules on cascaded model can not achieve considerable performance as fine-tuning. We propose an automatic and effective adaptive learning method to optimize end-to-end cascaded multi-task models based on Neural Architecture Search (NAS) framework. The candidate adaptive operations on each specific module consist of frozen, inserting an adapter and fine-tuning. We further add a penalty item on the loss to limit the learned structure which takes the amount of trainable parameters into account. The penalty item successfully restrict the searched architecture and the proposed approach is able to search similar tuning scheme with hand-craft, compressing the optimizing parameters to 8.7% corresponding to full fine-tuning on SLURP with an even better performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.17658</link><description>&lt;p&gt;
&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21333;&#19968;&#32447;&#24615;&#23618;&#30340;&#36890;&#36947;&#30456;&#20851;(CD)&#25110;&#36890;&#36947;&#29420;&#31435;(CI)&#24314;&#27169;&#65292;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#35768;&#22810;&#22797;&#26434;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;CD&#21644;CI&#35270;&#20026;&#20004;&#31181;&#20114;&#34917;&#20294;&#20114;&#26021;&#30340;&#26041;&#27861;&#65292;&#26080;&#27861;&#21516;&#26102;&#21033;&#29992;&#36825;&#20004;&#20010;&#26497;&#31471;&#12290;&#32780;&#19988;&#65292;CD&#21644;CI&#37117;&#26159;&#38745;&#24577;&#31574;&#30053;&#65292;&#26080;&#27861;&#22312;&#27809;&#26377;&#22823;&#37327;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26159;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;CI&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;CSC&#65288;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#65289;&#65292;&#29992;&#20110;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#22686;&#24378;&#20102;CI&#31574;&#30053;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20943;&#23567;&#20102;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an emergence of various models for long-term time series forecasting. Recent studies have demonstrated that a single linear layer, using Channel Dependent (CD) or Channel Independent (CI) modeling, can even outperform a large number of sophisticated models. However, current research primarily considers CD and CI as two complementary yet mutually exclusive approaches, unable to harness these two extremes simultaneously. And it is also a challenging issue that both CD and CI are static strategies that cannot be determined to be optimal for a specific dataset without extensive experiments. In this paper, we reconsider whether the current CI strategy is the best solution for time series forecasting. First, we propose a simple yet effective strategy called CSC, which stands for $\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel Self-Clustering (CSC) enhances CI strategy's performance improvements while reducing parameter size, fo
&lt;/p&gt;</description></item><item><title>&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#32034;&#30789;&#30899;&#21270;&#29289;&#21151;&#29575;MOSFET&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#37325;&#24314;&#36864;&#21270;&#35774;&#22791;&#30340;&#29289;&#29702;&#21442;&#25968;&#25110;&#26816;&#32034;&#29289;&#29702;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2310.17657</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#30789;&#30899;&#21270;&#29289;&#21151;&#29575;MOSFET&#22120;&#20214;&#30340;&#39640;&#32423;&#32423;&#21035;-3&#21453;&#27169;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Algorithm for Advanced Level-3 Inverse-Modeling of Silicon-Carbide Power MOSFET Devices. (arXiv:2310.17657v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17657
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#32034;&#30789;&#30899;&#21270;&#29289;&#21151;&#29575;MOSFET&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#37325;&#24314;&#36864;&#21270;&#35774;&#22791;&#30340;&#29289;&#29702;&#21442;&#25968;&#25110;&#26816;&#32034;&#29289;&#29702;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#21453;&#27169;&#22411;&#24314;&#27169;&#28041;&#21450;&#35757;&#32451;&#28145;&#23618;&#32467;&#26500;&#20174;&#38745;&#24577;&#34892;&#20026;&#20013;&#39044;&#27979;&#35774;&#22791;&#21442;&#25968;&#12290;&#21453;&#35774;&#22791;&#24314;&#27169;&#36866;&#29992;&#20110;&#37325;&#24314;&#22312;&#26102;&#38388;&#19978;&#36864;&#21270;&#30340;&#35774;&#22791;&#30340;&#28418;&#31227;&#29289;&#29702;&#21442;&#25968;&#25110;&#26816;&#32034;&#29289;&#29702;&#37197;&#32622;&#12290;&#26377;&#24456;&#22810;&#21464;&#37327;&#21487;&#20197;&#24433;&#21709;&#21453;&#27169;&#22411;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#32034;&#30789;&#30899;&#21270;&#29289;&#21151;&#29575;MOSFET&#65288;SiC Power MOS&#65289;&#30340;&#32423;&#21035;-3&#27169;&#22411;&#30340;&#29289;&#29702;&#21442;&#25968;&#12290;SiC&#22120;&#20214;&#29992;&#20110;&#20256;&#32479;&#30789;&#22120;&#20214;&#22240;&#39640;&#28201;&#25110;&#39640;&#24320;&#20851;&#33021;&#21147;&#32780;&#22833;&#25928;&#30340;&#24212;&#29992;&#20013;&#12290;SiC&#21151;&#29575;&#22120;&#20214;&#30340;&#20027;&#35201;&#24212;&#29992;&#22312;&#27773;&#36710;&#39046;&#22495;&#65288;&#21363;&#22312;&#30005;&#21160;&#36710;&#39046;&#22495;&#65289;&#12290;&#30001;&#20110;&#29983;&#29702;&#36864;&#21270;&#25110;&#39640;&#24212;&#21147;&#29615;&#22659;&#65292;SiC&#21151;&#29575;MOS&#26174;&#31034;&#20986;&#29289;&#29702;&#21442;&#25968;&#30340;&#26174;&#33879;&#28418;&#31227;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21453;&#27169;&#22411;&#36827;&#34892;&#30417;&#27979;&#12290;&#26412;&#24037;&#20316;&#30340;&#30446;&#30340;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Inverse modelling with deep learning algorithms involves training deep architecture to predict device's parameters from its static behaviour. Inverse device modelling is suitable to reconstruct drifted physical parameters of devices temporally degraded or to retrieve physical configuration. There are many variables that can influence the performance of an inverse modelling method. In this work the authors propose a deep learning method trained for retrieving physical parameters of Level-3 model of Power Silicon-Carbide MOSFET (SiC Power MOS). The SiC devices are used in applications where classical silicon devices failed due to high-temperature or high switching capability. The key application of SiC power devices is in the automotive field (i.e. in the field of electrical vehicles). Due to physiological degradation or high-stressing environment, SiC Power MOS shows a significant drift of physical parameters which can be monitored by using inverse modelling. The aim of this work is to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#39044;&#27979;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#31639;&#27861;&#21644;&#22810;&#20010;&#24212;&#29992;&#12290;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#26465;&#20214;&#20107;&#20214;&#38598;&#21512;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#20915;&#31574;&#21046;&#23450;&#32773;&#21046;&#23450;&#39044;&#27979;&#65292;&#24182;&#23454;&#29616;&#26368;&#20248;&#20132;&#25442;&#21518;&#24724;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#19968;&#29702;&#35770;&#25512;&#24191;&#21040;&#22312;&#32447;&#32452;&#21512;&#20248;&#21270;&#21644;&#24191;&#20041;&#21338;&#24328;&#39046;&#22495;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#26080;&#21518;&#24724;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.17651</link><description>&lt;p&gt;
&#39640;&#32500;&#39044;&#27979;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional Prediction for Sequential Decision Making. (arXiv:2310.17651v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#39044;&#27979;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#31639;&#27861;&#21644;&#22810;&#20010;&#24212;&#29992;&#12290;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#26465;&#20214;&#20107;&#20214;&#38598;&#21512;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#20915;&#31574;&#21046;&#23450;&#32773;&#21046;&#23450;&#39044;&#27979;&#65292;&#24182;&#23454;&#29616;&#26368;&#20248;&#20132;&#25442;&#21518;&#24724;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#19968;&#29702;&#35770;&#25512;&#24191;&#21040;&#22312;&#32447;&#32452;&#21512;&#20248;&#21270;&#21644;&#24191;&#20041;&#21338;&#24328;&#39046;&#22495;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#26080;&#21518;&#24724;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#36873;&#25321;&#30340;&#39640;&#32500;&#29366;&#24577;&#19979;&#36827;&#34892;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#39044;&#27979;&#22312;&#20219;&#24847;&#26465;&#20214;&#20107;&#20214;&#19979;&#37117;&#26159;&#26080;&#20559;&#30340;&#65292;&#24182;&#26088;&#22312;&#26681;&#25454;&#21518;&#32493;&#20915;&#31574;&#21046;&#23450;&#21512;&#36866;&#30340;&#20107;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#31639;&#27861;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#19968;&#20123;&#24212;&#29992;&#65292;&#36825;&#20123;&#24212;&#29992;&#26469;&#33258;&#20110;&#36873;&#25321;&#19968;&#20010;&#21512;&#36866;&#30340;&#26465;&#20214;&#20107;&#20214;&#38598;&#21512;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#25928;&#22320;&#20026;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#20915;&#31574;&#21046;&#23450;&#32773;&#21046;&#23450;&#39044;&#27979;&#65292;&#22914;&#26524;&#20182;&#20204;&#23545;&#25105;&#20204;&#30340;&#39044;&#27979;&#20570;&#20986;&#20102;&#26368;&#20339;&#21453;&#24212;&#65292;&#27599;&#20010;&#20915;&#31574;&#21046;&#23450;&#32773;&#37117;&#20855;&#26377;&#26368;&#20248;&#30340;&#20132;&#25442;&#21518;&#24724;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#29702;&#35770;&#25512;&#24191;&#21040;&#22312;&#32447;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#65292;&#20915;&#31574;&#21046;&#23450;&#32773;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#34892;&#21160;&#31354;&#38388;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#31639;&#27861;&#65292;&#22312;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#23376;&#24207;&#21015;&#19978;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#20915;&#31574;&#21046;&#23450;&#32773;&#30340;&#26080;&#21518;&#24724;&#24615;&#65292;&#36825;&#20123;&#23376;&#24207;&#21015;&#21487;&#33021;&#20381;&#36182;&#20110;&#20182;&#20204;&#30340;&#34892;&#21160;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#21040;&#24191;&#20041;&#21338;&#24328;&#20013;&#33719;&#24471;&#20102;&#39640;&#25928;&#30340;&#26080;&#23376;&#24207;&#21015;&#21518;&#24724;&#31639;&#27861;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31867;&#26032;&#30340;&#21518;&#24724;&#20445;&#35777;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of making predictions of an adversarially chosen high-dimensional state that are unbiased subject to an arbitrary collection of conditioning events, with the goal of tailoring these events to downstream decision makers. We give efficient algorithms for solving this problem, as well as a number of applications that stem from choosing an appropriate set of conditioning events.  For example, we can efficiently make predictions targeted at polynomially many decision makers, giving each of them optimal swap regret if they best-respond to our predictions. We generalize this to online combinatorial optimization, where the decision makers have a very large action space, to give the first algorithms offering polynomially many decision makers no regret on polynomially many subsequences that may depend on their actions and the context. We apply these results to get efficient no-subsequence-regret algorithms in extensive-form games (EFGs), yielding a new family of regret guara
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#31163;&#25955;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#34920;&#31034;&#30340;&#22797;&#26434;&#24615;&#26469;&#25552;&#39640;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#25903;&#25345;&#26368;&#20339;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#20013;&#20063;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17550</link><description>&lt;p&gt;
&#20154;&#31867;&#24341;&#23548;&#30340;&#22797;&#26434;&#24230;&#25511;&#21046;&#25277;&#35937;&#21270;
&lt;/p&gt;
&lt;p&gt;
Human-Guided Complexity-Controlled Abstractions. (arXiv:2310.17550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#31163;&#25955;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#34920;&#31034;&#30340;&#22797;&#26434;&#24615;&#26469;&#25552;&#39640;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#25903;&#25345;&#26368;&#20339;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#20013;&#20063;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20294;&#36825;&#20123;&#34920;&#31034;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#29615;&#22659;&#25110;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#21508;&#31181;&#25277;&#35937;&#32423;&#21035;&#65288;&#20363;&#22914;&#65292;&#8220;&#40479;&#8221;&#19982;&#8220;&#40635;&#38592;&#8221;&#65289;&#19978;&#23398;&#20064;&#31163;&#25955;&#34920;&#31034;&#65288;&#21363;&#27010;&#24565;&#25110;&#21333;&#35789;&#65289;&#65292;&#24182;&#26681;&#25454;&#20219;&#21153;&#20351;&#29992;&#36866;&#24403;&#30340;&#25277;&#35937;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#31163;&#25955;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#34920;&#31034;&#20998;&#24067;&#30340;&#29109;&#26469;&#25511;&#21046;&#34920;&#31034;&#30340;&#22797;&#26434;&#24615;&#65288;&#22823;&#33268;&#19978;&#26159;&#20026;&#32534;&#30721;&#36755;&#20837;&#20998;&#37197;&#20102;&#22810;&#23569;&#20301;&#65289;&#12290;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#20165;&#20351;&#29992;&#23569;&#37327;&#24102;&#26631;&#31614;&#30340;&#31034;&#20363;&#29992;&#20110;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65288;1&#65289;&#35843;&#25972;&#34920;&#31034;&#20197;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#25903;&#25345;&#26368;&#39640;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22312;&#19968;&#20010;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#20013;&#65292;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#31163;&#25955;&#34920;&#31034;&#30340;&#21487;&#35270;&#21270;&#26469;&#30830;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24403;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often learn task-specific latent representations that fail to generalize to novel settings or tasks. Conversely, humans learn discrete representations (i.e., concepts or words) at a variety of abstraction levels (e.g., ``bird'' vs. ``sparrow'') and deploy the appropriate abstraction based on task. Inspired by this, we train neural models to generate a spectrum of discrete representations, and control the complexity of the representations (roughly, how many bits are allocated for encoding inputs) by tuning the entropy of the distribution over representations. In finetuning experiments, using only a small number of labeled examples for a new task, we show that (1) tuning the representation to a task-appropriate complexity level supports the highest finetuning performance, and (2) in a human-participant study, users were able to identify the appropriate complexity level for a downstream task using visualizations of discrete representations. Our results indicate a promising
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#22312;&#35843;&#25972;&#20102;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#21518;&#65292;&#20854;&#22312;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.17526</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#20154;&#31867;&#22312;&#31995;&#32479;&#35780;&#20215;&#36807;&#31243;&#20013;&#30340;&#35282;&#33394;&#65311;&#35780;&#20272;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#22312;&#35843;&#25972;&#20102;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#21518;&#65292;&#20854;&#22312;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#35780;&#20215;&#23545;&#20110;&#25351;&#23548;&#23454;&#36341;&#12289;&#30740;&#31350;&#21644;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24120;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#20154;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#33021;&#22815;&#21152;&#24555;&#21644;&#33258;&#21160;&#21270;&#31995;&#32479;&#35780;&#20215;&#30340;&#36807;&#31243;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#32463;&#36807;&#20840;&#38754;&#35780;&#20272;&#65292;&#32780;&#19988;&#36824;&#27809;&#26377;&#30740;&#31350;&#27979;&#35797;&#36807;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;LLM&#8212;&#8212;GPT-4&#12290;&#26412;&#39044;&#27880;&#20876;&#30740;&#31350;&#37319;&#29992;&#8220;&#26080;&#20154;&#21442;&#19982;&#8221;&#30340;&#26041;&#27861;&#35780;&#20272;&#20102;GPT-4&#22312;&#26631;&#39064;/&#25688;&#35201;&#31579;&#36873;&#12289;&#20840;&#25991;&#23457;&#26597;&#21644;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#22312;&#19981;&#21516;&#25991;&#29486;&#31867;&#22411;&#21644;&#35821;&#35328;&#19978;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#20294;&#32467;&#26524;&#21463;&#21040;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#35843;&#25972;&#20102;&#36825;&#20123;&#22240;&#32032;&#21518;&#65292;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;&#20351;&#29992;&#39640;&#21487;&#38752;&#24615;&#25552;&#31034;&#36827;&#34892;&#31579;&#36873;&#30340;&#30740;&#31350;&#20013;&#65292;&#31579;&#36873;&#20840;&#25991;&#25991;&#29486;&#30340;&#34920;&#29616;&#27700;&#24179;&#22312;&#19981;&#21516;&#38454;&#27573;&#21644;&#35821;&#35328;&#19978;&#22343;&#20026;&#26080;&#21040;&#20013;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.17513</link><description>&lt;p&gt;
&#12298;&#20302;&#31209;&#36866;&#24212;&#30340;&#34920;&#36798;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#30697;&#38453;&#30340;&#20302;&#31209;&#36866;&#24212;&#24615;&#65292;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#26159;LoRA&#30340;&#29702;&#35770;&#22522;&#30784;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#20174;&#29702;&#35770;&#35282;&#24230;&#20998;&#26512;LoRA&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#39318;&#27425;&#23581;&#35797;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#26524;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#65292;&#21017;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#24403;LoRA-rank&#20302;&#20110;&#38408;&#20540;&#26102;&#65292;&#25105;&#20204;&#36824;&#37327;&#21270;&#20102;&#36924;&#36817;&#35823;&#24046;&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20219;&#20309;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#20020;&#26102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;TCN&#65289;&#65292;&#20351;&#29992;&#26032;&#30340;&#21453;&#24212;Smiles-like&#34920;&#31034;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#21270;&#23398;&#21453;&#24212;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21457;&#29616;&#24494;&#35843;&#21327;&#35758;&#23545;&#27169;&#22411;&#29983;&#25104;&#33539;&#22260;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.17341</link><description>&lt;p&gt;
&#36890;&#36807;&#26242;&#26102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#20840;&#26032;&#21270;&#23398;&#21453;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
De-novo Chemical Reaction Generation by Means of Temporarily Convolutional Neural Networks. (arXiv:2310.17341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#20020;&#26102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;TCN&#65289;&#65292;&#20351;&#29992;&#26032;&#30340;&#21453;&#24212;Smiles-like&#34920;&#31034;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#21270;&#23398;&#21453;&#24212;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21457;&#29616;&#24494;&#35843;&#21327;&#35758;&#23545;&#27169;&#22411;&#29983;&#25104;&#33539;&#22260;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#20351;&#29992;&#26032;&#39062;&#30340;&#21453;&#24212;Smiles-like&#34920;&#31034;&#65288;CGRSmiles&#65289;&#26102;&#65292;&#23558;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#20020;&#26102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;TCN&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#20840;&#26032;&#30340;&#21453;&#24212;&#29983;&#25104;&#65292;&#24182;&#30452;&#25509;&#34701;&#21512;&#20102;&#21407;&#23376;&#26144;&#23556;&#12290;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20197;&#20854;&#33258;&#22238;&#24402;&#29305;&#24615;&#32780;&#38395;&#21517;&#65292;&#24182;&#32463;&#24120;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#20351;&#29992;&#65292;&#30452;&#25509;&#24212;&#29992;&#20110;SMILES&#29983;&#25104;&#12290;&#30456;&#23545;&#36739;&#26032;&#30340;TCN&#20855;&#26377;&#31867;&#20284;&#30340;&#24615;&#36136;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24863;&#21463;&#37326;&#65292;&#24182;&#36981;&#23432;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25152;&#38656;&#30340;&#22240;&#26524;&#24615;&#12290;&#36890;&#36807;TCN&#21644;RNN&#34920;&#36798;&#30340;&#20004;&#31181;&#28508;&#22312;&#34920;&#31034;&#30340;&#32452;&#21512;&#30456;&#27604;&#20165;&#20351;&#29992;RNN&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23558;&#19981;&#21516;&#30340;&#24494;&#35843;&#21327;&#35758;&#24212;&#29992;&#20110;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#38598;&#26102;&#65292;&#23545;&#27169;&#22411;&#30340;&#29983;&#25104;&#33539;&#22260;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present here a combination of two networks, Recurrent Neural Networks (RNN) and Temporarily Convolutional Neural Networks (TCN) in de novo reaction generation using the novel Reaction Smiles-like representation of reactions (CGRSmiles) with atom mapping directly incorporated. Recurrent Neural Networks are known for their autoregressive properties and are frequently used in language modelling with direct application to SMILES generation. The relatively novel TCNs possess similar properties with wide receptive field while obeying the causality required for natural language processing (NLP). The combination of both latent representations expressed through TCN and RNN results in an overall better performance compared to RNN alone. Additionally, it is shown that different fine-tuning protocols have a profound impact on generative scope of the model when applied on a dataset of interest via transfer learning.
&lt;/p&gt;</description></item><item><title>&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;(CoExBO)&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#24490;&#29615;&#65292;&#24179;&#34913;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#23558;&#29992;&#25143;&#35265;&#35299;&#34701;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#35299;&#37322;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#23545;&#20248;&#21270;&#36807;&#31243;&#30340;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17273</link><description>&lt;p&gt;
&#23558;&#24490;&#29615;&#24341;&#20837;&#20154;&#31867;&#65306;&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Looping in the Human: Collaborative and Explainable Bayesian Optimization. (arXiv:2310.17273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17273
&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;(CoExBO)&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#24490;&#29615;&#65292;&#24179;&#34913;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#23558;&#29992;&#25143;&#35265;&#35299;&#34701;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#35299;&#37322;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#23545;&#20248;&#21270;&#36807;&#31243;&#30340;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;&#35768;&#22810;&#20248;&#21270;&#22120;&#19968;&#26679;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#33719;&#24471;&#29992;&#25143;&#20449;&#20219;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;&#20854;&#19981;&#36879;&#26126;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#23581;&#35797;&#24320;&#21457;&#38754;&#21521;&#20154;&#31867;&#30340;&#20248;&#21270;&#22120;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20551;&#35774;&#29992;&#25143;&#30693;&#35782;&#26159;&#26126;&#30830;&#19988;&#26080;&#35823;&#30340;&#65292;&#24182;&#20027;&#35201;&#23558;&#29992;&#25143;&#20316;&#20026;&#20248;&#21270;&#36807;&#31243;&#30340;&#30417;&#30563;&#32773;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24179;&#34913;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#21512;&#20316;&#20249;&#20276;&#20851;&#31995;&#65292;&#21363;&#25105;&#20204;&#30340;&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CoExBO&#65289;&#26694;&#26550;&#12290;CoExBO&#20351;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#26080;&#32541;&#22320;&#23558;&#20154;&#31867;&#35265;&#35299;&#25972;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#19982;&#29992;&#25143;&#20351;&#29992;&#20559;&#22909;&#19968;&#33268;&#30340;&#31639;&#27861;&#24314;&#35758;&#12290;CoExBO&#35299;&#37322;&#20854;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20197;&#22521;&#20859;&#20449;&#20219;&#65292;&#20351;&#29992;&#25143;&#26356;&#28165;&#26970;&#22320;&#25484;&#25569;&#20248;&#21270;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;CoExBO&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#65292;&#20801;&#35768;&#29992;&#25143;&#29359;&#38169;&#35823;&#65307;&#21363;&#20351;&#22312;&#26497;&#31471;&#23545;&#25239;&#24615;&#24178;&#25200;&#19979;&#65292;&#31639;&#27861;&#20063;&#20250;&#28176;&#36827;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like many optimizers, Bayesian optimization often falls short of gaining user trust due to opacity. While attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. We relax these assumptions and propose a more balanced human-AI partnership with our Collaborative and Explainable Bayesian Optimization (CoExBO) framework. Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. Furthermore, CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23610;&#24230;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;&#30340;&#20934;&#30830;&#24230;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#25193;&#25955;&#27169;&#22411;&#19978;&#35843;&#25972;&#20197;&#23454;&#29616;&#24179;&#28369;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16779</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Diffusion Denoised Smoothing. (arXiv:2310.16779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23610;&#24230;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;&#30340;&#20934;&#30830;&#24230;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#25193;&#25955;&#27169;&#22411;&#19978;&#35843;&#25972;&#20197;&#23454;&#29616;&#24179;&#28369;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#38543;&#26426;&#24179;&#28369;&#24050;&#25104;&#20026;&#23569;&#25968;&#20960;&#20010;&#20999;&#23454;&#21487;&#34892;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#20026;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#8220;&#21435;&#22122;&#21644;&#20998;&#31867;&#8221;&#27969;&#31243;&#65292;&#21363;&#25152;&#35859;&#30340;&#21435;&#22122;&#24179;&#28369;&#65292;&#22312;&#20219;&#20309;&#20998;&#31867;&#22120;&#19978;&#25191;&#34892;&#38543;&#26426;&#24179;&#28369;&#65292;&#21069;&#25552;&#26159;&#26377;&#19968;&#20010;&#20934;&#30830;&#30340;&#21435;&#22122;&#22120;&#21487;&#29992;&#65292;&#27604;&#22914;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21435;&#22122;&#24179;&#28369;&#30340;&#20934;&#30830;&#24230;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#36136;&#30097;&#21738;&#31181;&#25193;&#25955;&#27169;&#22411;&#30340;&#34920;&#31034;&#24418;&#24335;&#33021;&#22815;&#26368;&#22823;&#21270;&#21435;&#22122;&#24179;&#28369;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#65292;&#26088;&#22312;&#23454;&#29616;&#20849;&#21516;&#22122;&#22768;&#27700;&#24179;&#19979;&#24179;&#28369;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#20849;&#20139;&#25193;&#25955;&#27169;&#22411;&#19978;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#21516;&#26102;&#20063;&#20026;&#20854;&#35748;&#35777;&#40065;&#26834;&#24615;&#34917;&#20607;&#20934;&#30830;&#24230;&#30340;&#25104;&#26412;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with recent diffusion models, randomized smoothing has become one of a few tangible approaches that offers adversarial robustness to models at scale, e.g., those of large pre-trained models. Specifically, one can perform randomized smoothing on any classifier via a simple "denoise-and-classify" pipeline, so-called denoised smoothing, given that an accurate denoiser is available - such as diffusion model. In this paper, we investigate the trade-off between accuracy and certified robustness of denoised smoothing: for example, we question on which representation of diffusion model would maximize the certified robustness of denoised smoothing. We consider a new objective that aims collective robustness of smoothed classifiers across multiple noise levels at a shared diffusion model, which also suggests a new way to compensate the cost of accuracy in randomized smoothing for its certified robustness. This objective motivates us to fine-tune diffusion model (a) to perform consistent de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ReBis&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#26080;&#22870;&#21169;&#25511;&#21046;&#20449;&#24687;&#21644;&#22870;&#21169;&#29305;&#23450;&#30693;&#35782;&#65292;&#26469;&#25429;&#25417;&#22270;&#20687;&#20013;&#30340;&#25511;&#21046;&#20013;&#24515;&#20449;&#24687;&#12290;ReBis&#21033;&#29992;&#21464;&#24418;&#22120;&#26550;&#26500;&#26469;&#24314;&#27169;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#20998;&#22359;&#25513;&#30721;&#28040;&#38500;&#26102;&#31354;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;ReBis&#36824;&#32467;&#21512;&#20102;&#31561;&#20223;&#20989;&#24335;&#25439;&#22833;&#21644;&#38750;&#23545;&#31216;&#37325;&#24314;&#25439;&#22833;&#65292;&#20197;&#38450;&#27490;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#29305;&#24449;&#23849;&#28291;&#12290;</title><link>http://arxiv.org/abs/2310.16655</link><description>&lt;p&gt;
&#20174;&#22270;&#20687;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#20013;&#24515;&#34920;&#31034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Control-Centric Representations in Reinforcement Learning from Images. (arXiv:2310.16655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ReBis&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#26080;&#22870;&#21169;&#25511;&#21046;&#20449;&#24687;&#21644;&#22870;&#21169;&#29305;&#23450;&#30693;&#35782;&#65292;&#26469;&#25429;&#25417;&#22270;&#20687;&#20013;&#30340;&#25511;&#21046;&#20013;&#24515;&#20449;&#24687;&#12290;ReBis&#21033;&#29992;&#21464;&#24418;&#22120;&#26550;&#26500;&#26469;&#24314;&#27169;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#20998;&#22359;&#25513;&#30721;&#28040;&#38500;&#26102;&#31354;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;ReBis&#36824;&#32467;&#21512;&#20102;&#31561;&#20223;&#20989;&#24335;&#25439;&#22833;&#21644;&#38750;&#23545;&#31216;&#37325;&#24314;&#25439;&#22833;&#65292;&#20197;&#38450;&#27490;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#29305;&#24449;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#39033;&#23454;&#38469;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#22312;&#20110;&#25552;&#21462;&#25511;&#21046;&#20013;&#24515;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#24573;&#30053;&#19981;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#34429;&#28982;&#36981;&#24490;&#31561;&#20223;&#20989;&#24335;&#21407;&#21017;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#30528;&#28508;&#22312;&#21160;&#21147;&#23398;&#30340;&#26377;&#38480;&#34920;&#36798;&#33021;&#21147;&#21644;&#36866;&#24212;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ReBis&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#20813;&#22870;&#21169;&#25511;&#21046;&#20449;&#24687;&#19982;&#22870;&#21169;&#29305;&#23450;&#30693;&#35782;&#38598;&#25104;&#26469;&#25429;&#25417;&#25511;&#21046;&#20013;&#24515;&#20449;&#24687;&#12290;ReBis&#21033;&#29992;&#21464;&#24418;&#22120;&#26550;&#26500;&#38544;&#24335;&#24314;&#27169;&#21160;&#21147;&#23398;&#65292;&#24182;&#32467;&#21512;&#20998;&#22359;&#25513;&#30721;&#28040;&#38500;&#26102;&#31354;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;ReBis&#23558;&#31561;&#20223;&#20989;&#24335;&#25439;&#22833;&#19982;&#38750;&#23545;&#31216;&#37325;&#24314;&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#20197;&#38450;&#27490;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#29305;&#24449;&#23849;&#28291;&#12290;&#22312;Atari&#28216;&#25103;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Image-based Reinforcement Learning is a practical yet challenging task. A major hurdle lies in extracting control-centric representations while disregarding irrelevant information. While approaches that follow the bisimulation principle exhibit the potential in learning state representations to address this issue, they still grapple with the limited expressive capacity of latent dynamics and the inadaptability to sparse reward environments. To address these limitations, we introduce ReBis, which aims to capture control-centric information by integrating reward-free control information alongside reward-specific knowledge. ReBis utilizes a transformer architecture to implicitly model the dynamics and incorporates block-wise masking to eliminate spatiotemporal redundancy. Moreover, ReBis combines bisimulation-based loss with asymmetric reconstruction loss to prevent feature collapse in environments with sparse rewards. Empirical studies on two large benchmarks, including Atari games and D
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#20559;&#21521;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16546</link><description>&lt;p&gt;
&#20048;&#35266;&#20027;&#20041;&#30340;&#38519;&#38449;&#65306;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion. (arXiv:2310.16546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#20559;&#21521;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35797;&#22270;&#21033;&#29992;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;&#65292;&#22914;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#20048;&#35266;&#20027;&#20041;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20272;&#35745;&#30340;&#26041;&#24046;&#36827;&#34892;&#20048;&#35266;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#25910;&#38598;&#30340;&#20559;&#24046;&#65292;&#38459;&#30861;&#25910;&#25947;&#25110;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#26469;&#36873;&#25321;&#21160;&#20316;&#65292;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#21333;&#21521;&#20542;&#21521;&#12290;&#25105;&#20204;&#36890;&#36807;&#25197;&#26354;&#39118;&#38505;&#24230;&#37327;&#25552;&#20379;&#20102;&#19968;&#20010;&#25200;&#21160;&#30340;&#20998;&#24067;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#31639;&#23376;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#20855;&#26377;&#36739;&#24369;&#30340;&#25910;&#32553;&#24615;&#36136;&#30340;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#65292;&#25152;&#25552;&#26041;&#27861;&#19981;&#20250;&#38519;&#20837;&#20559;&#21521;&#24615;&#30340;&#25506;&#32034;&#65292;&#24182;&#30830;&#20445;&#25910;&#25947;&#21040;&#26368;&#20248;&#22238;&#25253;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional reinforcement learning algorithms have attempted to utilize estimated uncertainty for exploration, such as optimism in the face of uncertainty. However, using the estimated variance for optimistic exploration may cause biased data collection and hinder convergence or performance. In this paper, we present a novel distributional reinforcement learning algorithm that selects actions by randomizing risk criterion to avoid one-sided tendency on risk. We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property. Our theoretical results support that the proposed method does not fall into biased exploration and is guaranteed to converge to an optimal return. Finally, we empirically show that our method outperforms other existing distribution-based algorithms in various environments including Atari 55 games.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#20026;&#20160;&#20040;&#19968;&#20010;&#20010;&#20307;&#34987;&#20998;&#31867;&#19982;&#30456;&#20284;&#20010;&#20307;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#26469;&#34920;&#31034;&#20010;&#20307;&#21644;&#19982;&#20854;&#30456;&#20284;&#20010;&#20307;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35821;&#20041;&#26469;&#30830;&#23450;&#23545;&#20010;&#20307;&#20998;&#31867;&#20135;&#29983;&#26368;&#22823;&#36129;&#29486;&#30340;&#23646;&#24615;-&#20540;&#23545;&#12290;</title><link>http://arxiv.org/abs/2310.16506</link><description>&lt;p&gt;
&#35782;&#21035;&#20559;&#35265;&#30340;&#21407;&#22240;&#65306;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Identifying Reasons for Bias: An Argumentation-Based Approach. (arXiv:2310.16506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#20026;&#20160;&#20040;&#19968;&#20010;&#20010;&#20307;&#34987;&#20998;&#31867;&#19982;&#30456;&#20284;&#20010;&#20307;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#26469;&#34920;&#31034;&#20010;&#20307;&#21644;&#19982;&#20854;&#30456;&#20284;&#20010;&#20307;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35821;&#20041;&#26469;&#30830;&#23450;&#23545;&#20010;&#20307;&#20998;&#31867;&#20135;&#29983;&#26368;&#22823;&#36129;&#29486;&#30340;&#23646;&#24615;-&#20540;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#22312;&#31038;&#20250;&#20013;&#30340;&#26222;&#21450;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#22312;&#26500;&#24314;&#20844;&#24179;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#21253;&#25324;&#20010;&#20154;&#29305;&#24449;&#65292;&#24182;&#19988;&#23545;&#20110;&#21738;&#20123;&#20010;&#20307;&#34987;&#19981;&#20844;&#24179;&#22320;&#20998;&#31867;&#27809;&#26377;&#36879;&#26126;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#20026;&#20160;&#20040;&#19968;&#20010;&#20010;&#20307;&#34987;&#20998;&#31867;&#19982;&#30456;&#20284;&#20010;&#20307;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#26469;&#34920;&#31034;&#20010;&#20307;&#21644;&#19982;&#20854;&#30456;&#20284;&#20010;&#20307;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35821;&#20041;&#26469;&#30830;&#23450;&#23545;&#20010;&#20307;&#20998;&#31867;&#20135;&#29983;&#26368;&#22823;&#36129;&#29486;&#30340;&#23646;&#24615;-&#20540;&#23545;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22312;&#20844;&#24179;&#39046;&#22495;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#35782;&#21035;&#24046;&#24322;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As algorithmic decision-making systems become more prevalent in society, ensuring the fairness of these systems is becoming increasingly important. Whilst there has been substantial research in building fair algorithmic decision-making systems, the majority of these methods require access to the training data, including personal characteristics, and are not transparent regarding which individuals are classified unfairly. In this paper, we propose a novel model-agnostic argumentation-based method to determine why an individual is classified differently in comparison to similar individuals. Our method uses a quantitative argumentation framework to represent attribute-value pairs of an individual and of those similar to them, and uses a well-known semantics to identify the attribute-value pairs in the individual contributing most to their different classification. We evaluate our method on two datasets commonly used in the fairness literature and illustrate its effectiveness in the identi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#20195;&#30721;&#25688;&#35201;&#30340;&#26377;&#25928;&#24615;&#21644;&#24341;&#20837;&#23545;&#25239;&#24615;&#26696;&#20363;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#20041;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#21644;&#32500;&#25252;&#25928;&#29575;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.16314</link><description>&lt;p&gt;
&#20102;&#35299;&#20195;&#30721;&#35821;&#20041;: &#23545;Transformer&#27169;&#22411;&#22312;&#25688;&#35201;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Understanding Code Semantics: An Evaluation of Transformer Models in Summarization. (arXiv:2310.16314v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#20195;&#30721;&#25688;&#35201;&#30340;&#26377;&#25928;&#24615;&#21644;&#24341;&#20837;&#23545;&#25239;&#24615;&#26696;&#20363;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#20041;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#21644;&#32500;&#25252;&#25928;&#29575;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20808;&#36827;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#28145;&#20837;&#30740;&#31350;&#20102;&#20195;&#30721;&#25688;&#35201;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#20989;&#25968;&#21644;&#21464;&#37327;&#21517;&#26469;&#35780;&#20272;&#20195;&#30721;&#25688;&#35201;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#25506;&#32034;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#20195;&#30721;&#35821;&#20041;&#65292;&#36824;&#26159;&#20165;&#20165;&#20381;&#36182;&#25991;&#26412;&#32447;&#32034;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#27515;&#20195;&#30721;&#21644;&#27880;&#37322;&#20195;&#30721;&#31561;&#23545;&#25239;&#24615;&#26696;&#20363;&#65292;&#28085;&#30422;&#20102;&#19977;&#31181;&#32534;&#31243;&#35821;&#35328;(Python&#12289;Javascript&#21644;Java)&#65292;&#20197;&#36827;&#19968;&#27493;&#23457;&#26597;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#21152;&#24378;Transformer&#27169;&#22411;&#29702;&#35299;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20026;&#26356;&#39640;&#25928;&#30340;&#36719;&#20214;&#24320;&#21457;&#23454;&#36341;&#21644;&#32500;&#25252;&#24037;&#20316;&#27969;&#31243;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into the intricacies of code summarization using advanced transformer-based language models. Through empirical studies, we evaluate the efficacy of code summarization by altering function and variable names to explore whether models truly understand code semantics or merely rely on textual cues. We have also introduced adversaries like dead code and commented code across three programming languages (Python, Javascript, and Java) to further scrutinize the model's understanding. Ultimately, our research aims to offer valuable insights into the inner workings of transformer-based LMs, enhancing their ability to understand code and contributing to more efficient software development practices and maintenance workflows.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24050;&#35265;&#21644;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#37117;&#33021;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.15970</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Accented Speech Recognition With Accent-specific Codebooks. (arXiv:2310.15970v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24050;&#35265;&#21644;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#37117;&#33021;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21475;&#38899;&#23545;&#20110;&#29616;&#26377;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#21475;&#38899;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#20005;&#37325;&#38459;&#30861;&#20102;ASR&#30340;&#26222;&#21450;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#36825;&#20123;&#21487;&#23398;&#20064;&#30340;&#20195;&#30721;&#26412;&#25429;&#25417;&#20102;&#21475;&#38899;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#34987;&#25972;&#21512;&#21040;ASR&#32534;&#30721;&#22120;&#23618;&#20013;&#12290;&#27169;&#22411;&#22312;&#24102;&#21475;&#38899;&#30340;&#33521;&#35821;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#27979;&#35797;&#25968;&#25454;&#20013;&#20063;&#21253;&#21547;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#21475;&#38899;&#12290;&#22312;Mozilla Common Voice&#22810;&#21475;&#38899;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#20165;&#22312;&#24050;&#35265;&#30340;&#33521;&#35821;&#21475;&#38899;&#20013;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;&#21333;&#35789;&#38169;&#35823;&#29575;&#30456;&#23545;&#25552;&#21319;&#39640;&#36798;37%&#65289;&#65292;&#32780;&#19988;&#22312;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#20063;&#33719;&#24471;&#20102;5%&#30340;&#30456;&#23545;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;L2Artic&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#35774;&#32622;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech accents pose a significant challenge to state-of-the-art automatic speech recognition (ASR) systems. Degradation in performance across underrepresented accents is a severe deterrent to the inclusive adoption of ASR. In this work, we propose a novel accent adaptation approach for end-to-end ASR systems using cross-attention with a trainable set of codebooks. These learnable codebooks capture accent-specific information and are integrated within the ASR encoder layers. The model is trained on accented English speech, while the test data also contained accents which were not seen during training. On the Mozilla Common Voice multi-accented dataset, we show that our proposed approach yields significant performance gains not only on the seen English accents (up to $37\%$ relative improvement in word error rate) but also on the unseen accents (up to $5\%$ relative improvement in WER). Further, we illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We also compare
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#20803;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#27979;&#22826;&#38451;&#33021;&#31890;&#23376;&#20107;&#20214;&#30340;&#21457;&#29983;&#21644;&#23646;&#24615;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#22810;&#20010;&#33322;&#22825;&#22120;&#25910;&#38598;&#30340;&#21407;&#20301;&#21644;&#36965;&#24863;&#35266;&#27979;&#25968;&#25454;&#65292;&#33021;&#22815;&#19982;&#20135;&#29983;&#22826;&#38451;&#33021;&#31890;&#23376;&#20107;&#20214;&#30340;&#29289;&#29702;&#36807;&#31243;&#30456;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2310.15390</link><description>&lt;p&gt;
MEMPSEP III. &#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#20803;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20351;&#29992;&#22810;&#20803;&#38598;&#25104;&#26041;&#27861;&#39044;&#27979;&#22826;&#38451;&#33021;&#31890;&#23376;&#20107;&#20214;&#30340;&#21457;&#29983;&#21644;&#23646;&#24615;&#65288;arXiv:2310.15390v1 [astro-ph.SR]&#65289;
&lt;/p&gt;
&lt;p&gt;
MEMPSEP III. A machine learning-oriented multivariate data set for forecasting the Occurrence and Properties of Solar Energetic Particle Events using a Multivariate Ensemble Approach. (arXiv:2310.15390v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15390
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#20803;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#27979;&#22826;&#38451;&#33021;&#31890;&#23376;&#20107;&#20214;&#30340;&#21457;&#29983;&#21644;&#23646;&#24615;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#22810;&#20010;&#33322;&#22825;&#22120;&#25910;&#38598;&#30340;&#21407;&#20301;&#21644;&#36965;&#24863;&#35266;&#27979;&#25968;&#25454;&#65292;&#33021;&#22815;&#19982;&#20135;&#29983;&#22826;&#38451;&#33021;&#31890;&#23376;&#20107;&#20214;&#30340;&#29289;&#29702;&#36807;&#31243;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#20803;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#22810;&#20010;&#33322;&#22825;&#22120;&#25910;&#38598;&#21407;&#20301;&#21644;&#36965;&#24863;&#35266;&#27979;&#30340;&#22826;&#38451;&#22280;&#23618;&#27979;&#37327;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#19982;&#20135;&#29983;&#22826;&#38451;&#33021;&#31890;&#23376;&#20107;&#20214;&#30340;&#29289;&#29702;&#36807;&#31243;&#30456;&#20851;&#12290;&#21033;&#29992;&#22320;&#29699;&#21516;&#27493;&#29615;&#22659;&#21355;&#26143;&#65288;GOES&#65289;&#22826;&#38451;&#31995;&#27963;&#21160;&#20107;&#20214;&#21015;&#34920;&#20174;&#22826;&#38451;&#27963;&#21160;&#21608;&#26399;&#65288;SC&#65289;23&#21644;SC 24&#30340;&#19968;&#37096;&#20998;&#65288;1998-2013&#65289;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;252&#20010;&#20135;&#29983;&#22826;&#38451;&#33021;&#31890;&#23376;&#65288;SEP&#65289;&#30340;&#22826;&#38451;&#20107;&#20214;&#65288;&#32768;&#26001;&#65289;&#21644;17,542&#20010;&#19981;&#20135;&#29983;SEP&#30340;&#20107;&#20214;&#12290;&#23545;&#20110;&#27599;&#20010;&#30830;&#23450;&#30340;&#20107;&#20214;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;1 au&#22788;&#30340;&#23616;&#37096;&#31561;&#31163;&#23376;&#20307;&#23646;&#24615;&#65292;&#22914;&#39640;&#33021;&#36136;&#23376;&#21644;&#30005;&#23376;&#25968;&#25454;&#65292;&#19978;&#28216;&#22826;&#38451;&#39118;&#26465;&#20214;&#20197;&#21450;&#22269;&#23478;&#22320;&#29702;&#23398;&#20250;&#21355;&#26143;&#65288;GOES&#65289;&#21644;&#39640;&#32423;&#32452;&#25104;&#25506;&#27979;&#22120;&#65288;ACE&#65289;&#33322;&#22825;&#22120;&#19978;&#30340;&#19981;&#21516;&#20202;&#22120;&#27979;&#24471;&#30340;&#26143;&#38469;&#30913;&#22330;&#30690;&#37327;&#37327;&#12290;&#25105;&#20204;&#36824;&#25910;&#38598;&#20102;&#26469;&#33258;&#22826;&#38451;&#21160;&#21147;&#23398;&#35266;&#27979;&#21355;&#26143;&#65288;SDO&#65289;&#12289;&#22826;&#38451;&#21644;&#26085;&#29699;&#21355;&#26143;&#65288;SoHO&#65289;&#20197;&#21450;Wind&#22826;&#38451;&#23556;&#30005;&#20202;&#30340;&#36965;&#24863;&#25968;&#25454;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#20801;&#35768;&#36827;&#34892;&#21508;&#31181;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
We introduce a new multivariate data set that utilizes multiple spacecraft collecting in-situ and remote sensing heliospheric measurements shown to be linked to physical processes responsible for generating solar energetic particles (SEPs). Using the Geostationary Operational Environmental Satellites (GOES) flare event list from Solar Cycle (SC) 23 and part of SC 24 (1998-2013), we identify 252 solar events (flares) that produce SEPs and 17,542 events that do not. For each identified event, we acquire the local plasma properties at 1 au, such as energetic proton and electron data, upstream solar wind conditions, and the interplanetary magnetic field vector quantities using various instruments onboard GOES and the Advanced Composition Explorer (ACE) spacecraft. We also collect remote sensing data from instruments onboard the Solar Dynamic Observatory (SDO), Solar and Heliospheric Observatory (SoHO), and the Wind solar radio instrument WAVES. The data set is designed to allow for variati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65288;MPI&#65289;&#26469;&#26377;&#25928;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#26469;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#36335;&#24452;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21457;&#29616;&#20505;&#36873;&#33647;&#29289;&#12290;</title><link>http://arxiv.org/abs/2310.15211</link><description>&lt;p&gt;
&#27169;&#25311;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#36335;&#24452;&#37325;&#35201;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Path Importance for Effective Alzheimer's Disease Drug Repurposing. (arXiv:2310.15211v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65288;MPI&#65289;&#26469;&#26377;&#25928;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#26469;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#36335;&#24452;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21457;&#29616;&#20505;&#36873;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33647;&#29289;&#37325;&#29992;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#19988;&#36164;&#28304;&#39640;&#25928;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#21457;&#29616;&#33539;&#24335;&#24050;&#32463;&#23853;&#38706;&#22836;&#35282;&#12290;&#22312;&#21508;&#31181;&#33647;&#29289;&#37325;&#29992;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#32593;&#32476;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#21033;&#29992;&#22797;&#26434;&#32593;&#32476;&#65292;&#25972;&#21512;&#22810;&#31181;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65288;&#22914;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65289;&#65292;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#28508;&#22312;&#33647;&#29289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#32593;&#32476;&#20013;&#30456;&#21516;&#38271;&#24230;&#30340;&#36335;&#24452;&#23545;&#20110;&#35782;&#21035;&#33647;&#29289;&#30340;&#27835;&#30103;&#25928;&#26524;&#20855;&#26377;&#30456;&#31561;&#30340;&#37325;&#35201;&#24615;&#12290;&#20854;&#20182;&#39046;&#22495;&#21457;&#29616;&#65292;&#30456;&#21516;&#38271;&#24230;&#30340;&#36335;&#24452;&#24182;&#19981;&#19968;&#23450;&#20855;&#26377;&#30456;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#20381;&#36182;&#20110;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#23545;&#33647;&#29289;&#37325;&#29992;&#23581;&#35797;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MPI&#65288;&#27169;&#25311;&#36335;&#24452;&#37325;&#35201;&#24615;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#26041;&#27861;&#12290;MPI&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#65292;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#26469;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#36335;&#24452;&#65292;&#36825;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#32593;&#32476;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#23398;&#20064;&#30340;&#33410;&#28857;&#23884;&#20837;&#21487;&#20197;&#25552;&#39640;&#33647;&#29289;&#37325;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, drug repurposing has emerged as an effective and resource-efficient paradigm for AD drug discovery. Among various methods for drug repurposing, network-based methods have shown promising results as they are capable of leveraging complex networks that integrate multiple interaction types, such as protein-protein interactions, to more effectively identify candidate drugs. However, existing approaches typically assume paths of the same length in the network have equal importance in identifying the therapeutic effect of drugs. Other domains have found that same length paths do not necessarily have the same importance. Thus, relying on this assumption may be deleterious to drug repurposing attempts. In this work, we propose MPI (Modeling Path Importance), a novel network-based method for AD drug repurposing. MPI is unique in that it prioritizes important paths via learned node embeddings, which can effectively capture a network's rich structural information. Thus, leveraging learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;-$\mathcal{T}$-&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14814</link><description>&lt;p&gt;
&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias. (arXiv:2310.14814v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;-$\mathcal{T}$-&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#23545;&#27169;&#22411;&#33258;&#20449;&#24230;&#39640;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20266;&#26631;&#31614;&#20998;&#37197;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#22788;&#29702;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#24120;&#20351;&#29992;softmax&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#33258;&#20449;&#24230;&#24230;&#37327;&#65292;&#23613;&#31649;&#24050;&#30693;&#23427;&#20204;&#23545;&#38169;&#35823;&#39044;&#27979;&#20063;&#36807;&#20110;&#33258;&#20449;&#12290;&#24403;&#25968;&#25454;&#26631;&#27880;&#21463;&#21040;&#26576;&#31181;&#32422;&#26463;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#23588;&#20026;&#26126;&#26174;&#65292;&#21363;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;$\mathcal{T}$-&#30456;&#20284;&#24230;&#65292;&#23427;&#22522;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#39044;&#27979;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#31283;&#23450;&#28857;&#24182;&#25551;&#36848;&#21333;&#20010;&#25104;&#21592;&#30340;&#22810;&#26679;&#24615;&#19982;&#20854;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#25552;&#20379;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#33258;&#20449;&#24230;&#24230;&#37327;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, despite the fact that they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#21360;&#24230;&#32929;&#24066;&#19978;&#30340;&#19977;&#31181;&#32452;&#21512;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#32047;&#35745;&#25910;&#30410;&#12289;&#27874;&#21160;&#29575;&#21644;&#22799;&#26222;&#27604;&#29575;&#31561;&#25351;&#26631;&#65292;&#30830;&#23450;&#20986;&#20102;&#22312;&#19981;&#21516;&#34892;&#19994;&#19979;&#34920;&#29616;&#26368;&#20339;&#30340;&#25237;&#36164;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.14748</link><description>&lt;p&gt;
&#21360;&#24230;&#32929;&#24066;&#32452;&#21512;&#20248;&#21270;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Portfolio Optimization Methods for the Indian Stock Market. (arXiv:2310.14748v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14748
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#21360;&#24230;&#32929;&#24066;&#19978;&#30340;&#19977;&#31181;&#32452;&#21512;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#32047;&#35745;&#25910;&#30410;&#12289;&#27874;&#21160;&#29575;&#21644;&#22799;&#26222;&#27604;&#29575;&#31561;&#25351;&#26631;&#65292;&#30830;&#23450;&#20986;&#20102;&#22312;&#19981;&#21516;&#34892;&#19994;&#19979;&#34920;&#29616;&#26368;&#20339;&#30340;&#25237;&#36164;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#33410;&#22312;&#21360;&#24230;&#32929;&#24066;&#19978;&#23545;MVP&#12289;HRP&#21644;HERC&#19977;&#31181;&#32452;&#21512;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#26469;&#33258;&#21360;&#24230;&#22269;&#23478;&#32929;&#31080;&#20132;&#26131;&#25152;&#19978;15&#20010;&#34892;&#19994;&#30340;&#32929;&#31080;&#12290;&#26681;&#25454;2022&#24180;7&#26376;1&#26085;&#21360;&#24230;&#22269;&#23478;&#32929;&#31080;&#20132;&#26131;&#25152;&#21457;&#24067;&#30340;&#25253;&#21578;&#65292;&#30830;&#23450;&#20102;&#27599;&#20010;&#32858;&#31867;&#30340;&#28909;&#38376;&#32929;&#31080;&#65292;&#22522;&#20110;&#20854;&#33258;&#30001;&#27969;&#36890;&#24066;&#20540;&#12290;&#23545;&#20110;&#27599;&#20010;&#34892;&#19994;&#65292;&#26681;&#25454;2019&#24180;7&#26376;1&#26085;&#33267;2022&#24180;6&#26376;30&#26085;&#26399;&#38388;&#30340;&#32929;&#31080;&#20215;&#26684;&#65292;&#37319;&#29992;&#19977;&#31181;&#32452;&#21512;&#20248;&#21270;&#26041;&#27861;&#35774;&#35745;&#20102;&#19977;&#20010;&#25237;&#36164;&#32452;&#21512;&#65292;&#24182;&#22312;2022&#24180;7&#26376;1&#26085;&#33267;2023&#24180;6&#26376;30&#26085;&#26399;&#38388;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#35780;&#20272;&#25237;&#36164;&#32452;&#21512;&#30340;&#34920;&#29616;&#20351;&#29992;&#20102;&#19977;&#20010;&#25351;&#26631;&#65292;&#21253;&#25324;&#32047;&#35745;&#25910;&#30410;&#12289;&#24180;&#27874;&#21160;&#29575;&#21644;&#22799;&#26222;&#27604;&#29575;&#12290;&#23545;&#20110;&#27599;&#20010;&#34892;&#19994;&#65292;&#35782;&#21035;&#20986;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26399;&#38388;&#32047;&#35745;&#25910;&#30410;&#26368;&#39640;&#12289;&#27874;&#21160;&#29575;&#26368;&#20302;&#21644;&#22799;&#26222;&#27604;&#29575;&#26368;&#22823;&#30340;&#25237;&#36164;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This chapter presents a comparative study of the three portfolio optimization methods, MVP, HRP, and HERC, on the Indian stock market, particularly focusing on the stocks chosen from 15 sectors listed on the National Stock Exchange of India. The top stocks of each cluster are identified based on their free-float market capitalization from the report of the NSE published on July 1, 2022 (NSE Website). For each sector, three portfolios are designed on stock prices from July 1, 2019, to June 30, 2022, following three portfolio optimization approaches. The portfolios are tested over the period from July 1, 2022, to June 30, 2023. For the evaluation of the performances of the portfolios, three metrics are used. These three metrics are cumulative returns, annual volatilities, and Sharpe ratios. For each sector, the portfolios that yield the highest cumulative return, the lowest volatility, and the maximum Sharpe Ratio over the training and the test periods are identified.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GNN&#27169;&#22411;&#35780;&#20272;&#30340;&#26032;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;DiscGraph&#38598;&#21512;&#26500;&#24314;&#21644;GNNEvaluator&#35757;&#32451;&#21644;&#25512;&#26029;&#20004;&#20010;&#38454;&#27573;&#30340;GNN&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#26410;&#30693;&#22270;&#19978;&#20934;&#30830;&#35780;&#20272;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.14586</link><description>&lt;p&gt;
GNNEvaluator: &#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#26410;&#30693;&#22270;&#19978;&#35780;&#20272;GNN&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
GNNEvaluator: Evaluating GNN Performance On Unseen Graphs Without Labels. (arXiv:2310.14586v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GNN&#27169;&#22411;&#35780;&#20272;&#30340;&#26032;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;DiscGraph&#38598;&#21512;&#26500;&#24314;&#21644;GNNEvaluator&#35757;&#32451;&#21644;&#25512;&#26029;&#20004;&#20010;&#38454;&#27573;&#30340;GNN&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#26410;&#30693;&#22270;&#19978;&#20934;&#30830;&#35780;&#20272;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24615;&#33021;&#26159;&#23454;&#38469;&#24212;&#29992;GNN&#27169;&#22411;&#21644;&#26381;&#21153;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#22240;&#20026;&#37096;&#32626;&#30340;GNN&#22312;&#25512;&#26029;&#26410;&#30693;&#21644;&#26080;&#26631;&#31614;&#27979;&#35797;&#22270;&#26102;&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#24615;&#33021;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;&#35757;&#32451;-&#27979;&#35797;&#22270;&#20998;&#24067;&#19981;&#21305;&#37197;&#25152;&#36896;&#25104;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;GNN&#27169;&#22411;&#35780;&#20272;&#65292;&#26088;&#22312;&#36890;&#36807;&#31934;&#30830;&#20272;&#35745;&#20854;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#26410;&#30693;&#22270;&#19978;&#30340;&#24615;&#33021;&#65288;&#22914;&#33410;&#28857;&#20998;&#31867;&#20934;&#30830;&#24615;&#65289;&#65292;&#26469;&#35780;&#20272;&#22312;&#24102;&#26631;&#31614;&#21644;&#35266;&#23519;&#21040;&#30340;&#22270;&#19978;&#35757;&#32451;&#30340;&#29305;&#23450;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;DiscGraph&#38598;&#21512;&#26500;&#24314;&#21644;GNNEvaluator&#35757;&#32451;&#21644;&#25512;&#26029;&#20004;&#20010;&#38454;&#27573;&#30340;GNN&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#12290;DiscGraph&#38598;&#21512;&#36890;&#36807;&#21033;&#29992;&#19982;&#28508;&#22312;&#33410;&#28857;&#23884;&#20837;&#21644;&#33410;&#28857;&#31867;&#21035;&#39044;&#27979;&#30456;&#20851;&#30340;GNN&#36755;&#20986;&#30340;&#24046;&#24322;&#24230;&#37327;&#20989;&#25968;&#65292;&#25429;&#25417;&#20102;&#24191;&#27867;&#21644;&#22810;&#26679;&#30340;&#22270;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#12290;&#22312;DiscGr&#30340;&#26377;&#25928;&#35757;&#32451;&#30417;&#30563;&#19979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;GNN&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#35780;&#20272;GNN&#27169;&#22411;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#26410;&#30693;&#22270;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the performance of graph neural networks (GNNs) is an essential task for practical GNN model deployment and serving, as deployed GNNs face significant performance uncertainty when inferring on unseen and unlabeled test graphs, due to mismatched training-test graph distributions. In this paper, we study a new problem, GNN model evaluation, that aims to assess the performance of a specific GNN model trained on labeled and observed graphs, by precisely estimating its performance (e.g., node classification accuracy) on unseen graphs without labels. Concretely, we propose a two-stage GNN model evaluation framework, including (1) DiscGraph set construction and (2) GNNEvaluator training and inference. The DiscGraph set captures wide-range and diverse graph data distribution discrepancies through a discrepancy measurement function, which exploits the outputs of GNNs related to latent node embeddings and node class predictions. Under the effective training supervision from the DiscGr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21253;&#25324;&#25512;&#23548;&#20102;&#25928;&#26524;&#21644;&#25104;&#21151;&#29575;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13786</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limits of Membership Inference Attacks on Machine Learning Models. (arXiv:2310.13786v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21253;&#25324;&#25512;&#23548;&#20102;&#25928;&#26524;&#21644;&#25104;&#21151;&#29575;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#21487;&#20197;&#25581;&#31034;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#26159;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#65292;&#21487;&#33021;&#26292;&#38706;&#20010;&#20154;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;MIA&#30340;&#22522;&#26412;&#32479;&#35745;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;&#32479;&#35745;&#37327;&#65292;&#35813;&#32479;&#35745;&#37327;&#20915;&#23450;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#21644;&#25104;&#21151;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#24773;&#20917;&#65292;&#24182;&#23545;&#36825;&#20010;&#24863;&#20852;&#36259;&#30340;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#30028;&#38480;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#38598;&#20013;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article explores the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. Then, we investigate several situations for which we provide bounds on this quantity of interest. This allows us to infer the accuracy of potential attacks as a function of the number of samples and other structural parameters of learning models, which in some cases can be directly estimated from the dataset.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#33647;&#29289;&#21644;&#32454;&#32990;&#31995;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#19982;&#33647;&#29289;&#20316;&#29992;&#26426;&#21046;&#21644;&#32454;&#32990;&#31995;&#30284;&#30151;&#31867;&#22411;&#30456;&#20851;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#24182;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#23545;&#33647;&#29289;&#21644;&#32454;&#32990;&#31995;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20010;&#24615;&#21270;&#30340;&#33647;&#29289;&#20248;&#20808;&#32423;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2310.13725</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#33647;&#29289;&#21644;&#32454;&#32990;&#31995;&#34920;&#24449;&#65292;&#20197;&#25913;&#21892;&#25239;&#30284;&#33647;&#29289;&#20248;&#20808;&#32423;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Enhancing drug and cell line representations via contrastive learning for improved anti-cancer drug prioritization. (arXiv:2310.13725v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13725
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#33647;&#29289;&#21644;&#32454;&#32990;&#31995;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#19982;&#33647;&#29289;&#20316;&#29992;&#26426;&#21046;&#21644;&#32454;&#32990;&#31995;&#30284;&#30151;&#31867;&#22411;&#30456;&#20851;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#24182;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#23545;&#33647;&#29289;&#21644;&#32454;&#32990;&#31995;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20010;&#24615;&#21270;&#30340;&#33647;&#29289;&#20248;&#20808;&#32423;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#30284;&#30151;&#30340;&#22797;&#26434;&#24615;&#21644;&#23545;&#27835;&#30103;&#30340;&#21487;&#21464;&#21453;&#24212;&#65292;&#36890;&#36807;&#22522;&#22240;&#32452;&#23398;&#24207;&#21015;&#20998;&#26512;&#36827;&#34892;&#30340;&#31934;&#30830;&#20010;&#20307;&#21270;&#30284;&#30151;&#27835;&#30103;&#24050;&#25104;&#20026;&#24403;&#21069;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#24739;&#32773;&#20135;&#29983;&#30340;&#25968;&#25454;&#37327;&#20351;&#24471;&#24555;&#36895;&#35782;&#21035;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#21463;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#22952;&#30861;&#20102;&#35745;&#31639;&#26041;&#27861;&#23398;&#20064;&#19982;&#26377;&#25928;&#33647;&#29289;-&#32454;&#32990;&#31995;&#37197;&#23545;&#30456;&#20851;&#30340;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#25913;&#36827;&#23398;&#20064;&#21040;&#30340;&#33647;&#29289;&#21644;&#32454;&#32990;&#31995;&#34920;&#24449;&#65292;&#20197;&#20445;&#30041;&#19982;&#33647;&#29289;&#20316;&#29992;&#26426;&#21046;&#21644;&#32454;&#32990;&#31995;&#30284;&#30151;&#31867;&#22411;&#30456;&#20851;&#30340;&#20851;&#31995;&#32467;&#26500;&#12290;&#38500;&#20102;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#25105;&#20204;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#30340;&#20998;&#31867;&#22120;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#26356;&#21152;&#24179;&#34913;&#22320;&#20381;&#36182;&#20110;&#33647;&#29289;&#21644;&#32454;&#32990;&#31995;&#29305;&#24449;&#12290;&#36825;&#26377;&#21161;&#20110;&#26356;&#20010;&#24615;&#21270;&#30340;&#33647;&#29289;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#20854;&#22522;&#20110;&#19982;&#33647;&#29289;&#32784;&#33647;&#24615;&#30456;&#20851;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to cancer's complex nature and variable response to therapy, precision oncology informed by omics sequence analysis has become the current standard of care. However, the amount of data produced for each patients makes it difficult to quickly identify the best treatment regimen. Moreover, limited data availability has hindered computational methods' abilities to learn patterns associated with effective drug-cell line pairs. In this work, we propose the use of contrastive learning to improve learned drug and cell line representations by preserving relationship structures associated with drug mechanism of action and cell line cancer types. In addition to achieving enhanced performance relative to a state-of-the-art method, we find that classifiers using our learned representations exhibit a more balances reliance on drug- and cell line-derived features when making predictions. This facilitates more personalized drug prioritizations that are informed by signals related to drug resistan
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.13548</link><description>&lt;p&gt;
&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12300;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12301;&#26159;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#19968;&#31181;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;RLHF&#21487;&#33021;&#20250;&#40723;&#21169;&#27169;&#22411;&#36890;&#36807;&#19982;&#29992;&#25143;&#20449;&#24565;&#30456;&#31526;&#30340;&#22238;&#31572;&#26469;&#20195;&#26367;&#30495;&#23454;&#22238;&#31572;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#35844;&#23194;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;RLHF&#35757;&#32451;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#20197;&#21450;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#26159;&#21542;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20116;&#20010;&#26368;&#20808;&#36827;&#30340;AI&#21161;&#25163;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#33258;&#30001;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#19968;&#36143;&#34920;&#29616;&#20986;&#35844;&#23194;&#34892;&#20026;&#12290;&#20026;&#20102;&#29702;&#35299;&#20154;&#31867;&#20559;&#22909;&#26159;&#21542;&#39537;&#21160;&#20102;RLHF&#27169;&#22411;&#30340;&#36825;&#31181;&#24191;&#27867;&#34892;&#20026;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#22238;&#31572;&#19982;&#29992;&#25143;&#30340;&#35266;&#28857;&#30456;&#31526;&#26102;&#65292;&#23427;&#26356;&#26377;&#21487;&#33021;&#34987;&#36873;&#20013;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#20559;&#22909;&#27169;&#22411;&#65288;PMs&#65289;&#23558;&#26377;&#35828;&#26381;&#21147;&#30340;&#35844;&#23194;&#22238;&#31572;&#19982;&#27491;&#30830;&#22238;&#31572;&#30456;&#27604;&#65292;&#26377;&#26102;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#22320;&#36873;&#25321;&#20102;&#35844;&#23194;&#22238;&#31572;&#12290;&#20248;&#21270;&#27169;&#22411;&#36755;&#20986;&#20197;&#28385;&#36275;PMs&#26377;&#26102;&#20063;&#20250;&#22312;&#30495;&#23454;&#24615;&#21644;&#35844;&#23194;&#34892;&#20026;&#20043;&#38388;&#20570;&#20986;&#21462;&#33293;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CARP&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#20998;&#21306;&#30340;&#35270;&#22270;&#36827;&#34892;&#19968;&#33268;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#38450;&#27490;&#20102;&#23849;&#28291;&#35299;&#20915;&#26041;&#26696;&#30340;&#20986;&#29616;&#12290;&#22312;&#24191;&#27867;&#30340;&#35780;&#20272;&#20013;&#65292;&#35777;&#26126;CARP&#30340;&#34920;&#31034;&#36866;&#29992;&#20110;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19982;11&#31181;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.12692</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#38543;&#26426;&#20998;&#21306;&#30340;&#35270;&#22270;&#36827;&#34892;&#19968;&#33268;&#20998;&#37197;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning via Consistent Assignment of Views over Random Partitions. (arXiv:2310.12692v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CARP&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#20998;&#21306;&#30340;&#35270;&#22270;&#36827;&#34892;&#19968;&#33268;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#38450;&#27490;&#20102;&#23849;&#28291;&#35299;&#20915;&#26041;&#26696;&#30340;&#20986;&#29616;&#12290;&#22312;&#24191;&#27867;&#30340;&#35780;&#20272;&#20013;&#65292;&#35777;&#26126;CARP&#30340;&#34920;&#31034;&#36866;&#29992;&#20110;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19982;11&#31181;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#21363;&#23545;&#38543;&#26426;&#20998;&#21306;&#30340;&#35270;&#22270;&#36827;&#34892;&#19968;&#33268;&#20998;&#37197;&#65288;CARP&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#35270;&#35273;&#29305;&#24449;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;CARP&#20197;&#31471;&#21040;&#31471;&#22312;&#32447;&#30340;&#26041;&#24335;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26469;&#23398;&#20064;&#21407;&#22411;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#38750;&#21487;&#24494;&#27169;&#22359;&#26469;&#35299;&#20915;&#32858;&#31867;&#20998;&#37197;&#38382;&#39064;&#12290;CARP&#36890;&#36807;&#22522;&#20110;&#21407;&#22411;&#30340;&#38543;&#26426;&#20998;&#21306;&#20248;&#21270;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#24182;&#24378;&#21046;&#35270;&#22270;&#20043;&#38388;&#30340;&#20998;&#37197;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#38450;&#27490;&#20102;&#32852;&#21512;&#23884;&#20837;&#35757;&#32451;&#20013;&#30340;&#23849;&#28291;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;CARP&#30340;&#34920;&#31034;&#36866;&#29992;&#20110;&#23398;&#20064;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;17&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;CARP&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21253;&#25324;&#32447;&#24615;&#35780;&#20272;&#12289;&#23569;&#26679;&#26412;&#20998;&#31867;&#12289;k-NN&#12289;k-means&#12289;&#22270;&#20687;&#26816;&#32034;&#21644;&#22797;&#21046;&#26816;&#27979;&#31561;&#35768;&#22810;&#26631;&#20934;&#21327;&#35758;&#12290;&#25105;&#20204;&#23558;CARP&#30340;&#24615;&#33021;&#19982;11&#31181;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Consistent Assignment of Views over Random Partitions (CARP), a self-supervised clustering method for representation learning of visual features. CARP learns prototypes in an end-to-end online fashion using gradient descent without additional non-differentiable modules to solve the cluster assignment problem. CARP optimizes a new pretext task based on random partitions of prototypes that regularizes the model and enforces consistency between views' assignments. Additionally, our method improves training stability and prevents collapsed solutions in joint-embedding training. Through an extensive evaluation, we demonstrate that CARP's representations are suitable for learning downstream tasks. We evaluate CARP's representations capabilities in 17 datasets across many standard protocols, including linear evaluation, few-shot classification, k-NN, k-means, image retrieval, and copy detection. We compare CARP performance to 11 existing self-supervised methods. We extensively abla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Jorge&#65292;&#19968;&#31181;GPU&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#39044;&#22788;&#29702;&#26041;&#27861;&#26367;&#20195;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#20860;&#20855;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;Jorge&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12298</link><description>&lt;p&gt;
Jorge: GPU&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#30340;&#36817;&#20284;&#39044;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization. (arXiv:2310.12298v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Jorge&#65292;&#19968;&#31181;GPU&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#39044;&#22788;&#29702;&#26041;&#27861;&#26367;&#20195;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#20860;&#20855;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;Jorge&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19982;&#19968;&#38454;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;&#20108;&#38454;&#20248;&#21270;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;&#25104;&#26412;&#36739;&#22823;&#65292;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#19968;&#30452;&#19981;&#22826;&#21463;&#27426;&#36814;&#12290;&#36825;&#31181;&#20248;&#21270;&#22120;&#20013;&#30340;&#20027;&#35201;&#25928;&#29575;&#29942;&#39048;&#26159;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#30340;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#65292;&#22312;GPU&#19978;&#35745;&#31639;&#26114;&#36149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Jorge&#65292;&#19968;&#31181;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#23427;&#20860;&#20855;&#20108;&#38454;&#26041;&#27861;&#30340;&#24555;&#36895;&#25910;&#25947;&#29305;&#24615;&#21644;&#19968;&#38454;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#23436;&#20840;&#28040;&#38500;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35745;&#31639;&#29942;&#39048;&#65292;&#29992;&#36817;&#20284;&#30340;&#39044;&#22788;&#29702;&#22120;&#35745;&#31639;&#26367;&#20195;&#12290;&#36825;&#20351;&#24471;Jorge&#22312;&#22681;&#38047;&#26102;&#38388;&#19978;&#22312;GPU&#19978;&#38750;&#24120;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#35843;&#25972;&#33391;&#22909;&#30340;SGD&#22522;&#20934;&#20013;&#30830;&#23450;Jorge&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#35843;&#21442;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;Jorge&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their better convergence properties compared to first-order optimizers, second-order optimizers for deep learning have been less popular due to their significant computational costs. The primary efficiency bottleneck in such optimizers is matrix inverse calculations in the preconditioning step, which are expensive to compute on GPUs. In this paper, we introduce Jorge, a second-order optimizer that promises the best of both worlds -- rapid convergence benefits of second-order methods, and high computational efficiency typical of first-order methods. We address the primary computational bottleneck of computing matrix inverses by completely eliminating them using an approximation of the preconditioner computation. This makes Jorge extremely efficient on GPUs in terms of wall-clock time. Further, we describe an approach to determine Jorge's hyperparameters directly from a well-tuned SGD baseline, thereby significantly minimizing tuning efforts. Our empirical evaluations demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#12289;&#26080;&#38656;&#32447;&#25628;&#32034;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#26368;&#20339;&#30340;&#25910;&#25947;&#36895;&#24230;&#35299;&#20915;&#21442;&#25968;&#26410;&#20808;&#32473;&#23450;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#26465;&#20214;&#24555;&#36895;&#26799;&#24230;&#27861;&#65288;AC-FGM&#65289;&#23454;&#29616;&#20102;O(1/k^2)&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#34987;&#25193;&#23637;&#21040;&#20855;&#26377;H\"{o}lder&#36830;&#32493;&#26799;&#24230;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#25152;&#26377;&#38382;&#39064;&#31867;&#20013;&#20197;&#32479;&#19968;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2310.10082</link><description>&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#30340;&#26080;&#38656;&#32447;&#25628;&#32034;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#32479;&#19968;&#26368;&#20248;&#35299;&#27861;
&lt;/p&gt;
&lt;p&gt;
A simple uniformly optimal method without line search for convex optimization. (arXiv:2310.10082v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#12289;&#26080;&#38656;&#32447;&#25628;&#32034;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#26368;&#20339;&#30340;&#25910;&#25947;&#36895;&#24230;&#35299;&#20915;&#21442;&#25968;&#26410;&#20808;&#32473;&#23450;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#26465;&#20214;&#24555;&#36895;&#26799;&#24230;&#27861;&#65288;AC-FGM&#65289;&#23454;&#29616;&#20102;O(1/k^2)&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#34987;&#25193;&#23637;&#21040;&#20855;&#26377;H\"{o}lder&#36830;&#32493;&#26799;&#24230;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#25152;&#26377;&#38382;&#39064;&#31867;&#20013;&#20197;&#32479;&#19968;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#25628;&#32034;&#65288;&#25110;&#22238;&#28335;&#65289;&#31243;&#24207;&#24191;&#27867;&#24212;&#29992;&#20110;&#29992;&#20110;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#38454;&#26041;&#27861;&#20013;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20855;&#26377;&#26410;&#30693;&#38382;&#39064;&#21442;&#25968;&#65288;&#20363;&#22914;&#65292;Lipschitz&#24120;&#25968;&#65289;&#30340;&#38382;&#39064;&#20013;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32447;&#25628;&#32034;&#20013;&#33719;&#24471;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#23545;&#20110;&#35299;&#20915;&#21442;&#25968;&#26410;&#20808;&#32473;&#23450;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#26159;&#22810;&#20313;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#31867;&#22411;&#31639;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#26465;&#20214;&#24555;&#36895;&#26799;&#24230;&#27861;&#65288;AC-FGM&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#20272;&#35745;&#20840;&#23616;Lipschitz&#24120;&#25968;&#25110;&#20351;&#29992;&#32447;&#25628;&#32034;&#31243;&#24207;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20339;&#30340;O(1/k^2)&#25910;&#25947;&#36895;&#24230;&#65292;&#29992;&#20110;&#24179;&#28369;&#20984;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;AC-FGM&#25193;&#23637;&#21040;&#27714;&#35299;&#20855;&#26377;H\"{o}lder&#36830;&#32493;&#26799;&#24230;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#23427;&#22312;&#25152;&#26377;&#20855;&#26377;&#25152;&#38656;&#31934;&#24230;&#35299;&#30340;&#38382;&#39064;&#31867;&#20013;&#33258;&#21160;&#22320;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Line search (or backtracking) procedures have been widely employed into first-order methods for solving convex optimization problems, especially those with unknown problem parameters (e.g., Lipschitz constant). In this paper, we show that line search is superfluous in attaining the optimal rate of convergence for solving a convex optimization problem whose parameters are not given a priori. In particular, we present a novel accelerated gradient descent type algorithm called auto-conditioned fast gradient method (AC-FGM) that can achieve an optimal $\mathcal{O}(1/k^2)$ rate of convergence for smooth convex optimization without requiring the estimate of a global Lipschitz constant or the employment of line search procedures. We then extend AC-FGM to solve convex optimization problems with H\"{o}lder continuous gradients and show that it automatically achieves the optimal rates of convergence uniformly for all problem classes with the desired accuracy of the solution as the only input. Fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#29420;&#31435;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#8220;&#27425;&#20248;&#38388;&#38553;&#8221;&#30340;&#26465;&#20214;&#21644;&#31934;&#30830;&#31574;&#30053;&#35780;&#20272;&#30340;&#39044;&#35328;&#26426;&#65292;&#22312;$\mathcal{O}(1/ \epsilon)$&#27425;&#36845;&#20195;&#20869;&#36798;&#21040;$\epsilon$-Nash&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.09727</link><description>&lt;p&gt;
Provably Fast Convergence of Independent Natural Policy Gradient for Markov Potential Games.&#65288;arXiv&#65306;2310.09727v2[cs.LG]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Provably Fast Convergence of Independent Natural Policy Gradient for Markov Potential Games. (arXiv:2310.09727v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#29420;&#31435;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#8220;&#27425;&#20248;&#38388;&#38553;&#8221;&#30340;&#26465;&#20214;&#21644;&#31934;&#30830;&#31574;&#30053;&#35780;&#20272;&#30340;&#39044;&#35328;&#26426;&#65292;&#22312;$\mathcal{O}(1/ \epsilon)$&#27425;&#36845;&#20195;&#20869;&#36798;&#21040;$\epsilon$-Nash&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#29420;&#31435;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;NPG&#65289;&#31639;&#27861;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19968;&#20123;&#25216;&#26415;&#20551;&#35774;&#21644;&#24341;&#20837;&#8220;&#27425;&#20248;&#38388;&#38553;&#8221;&#30340;&#26465;&#20214;&#19979;&#65292;&#20855;&#26377;&#25552;&#20379;&#31934;&#30830;&#31574;&#30053;&#35780;&#20272;&#30340;&#39044;&#35328;&#26426;&#30340;&#29420;&#31435;NPG&#26041;&#27861;&#28176;&#36817;&#22320;&#22312;$\mathcal{O}(1/ \epsilon)$&#27425;&#36845;&#20195;&#20869;&#36798;&#21040;$\epsilon$-Nash&#22343;&#34913;&#65288;NE&#65289;&#12290;&#36825;&#25913;&#36827;&#20102;&#20043;&#21069;&#26368;&#22909;&#32467;&#26524;$\mathcal{O}(1/ \epsilon^2)$&#27425;&#36845;&#20195;&#65292;&#24182;&#19988;&#19982;&#21333;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;&#21487;&#36798;&#21040;&#30340;$\mathcal{O}(1/ \epsilon)$&#27425;&#36845;&#20195;&#20855;&#26377;&#30456;&#21516;&#30340;&#25968;&#37327;&#32423;&#12290;&#36890;&#36807;&#21512;&#25104;&#28508;&#22312;&#21338;&#24328;&#21644;&#25317;&#22622;&#21338;&#24328;&#30340;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies an independent natural policy gradient (NPG) algorithm for the multi-agent reinforcement learning problem in Markov potential games. It is shown that, under mild technical assumptions and the introduction of the \textit{suboptimality gap}, the independent NPG method with an oracle providing exact policy evaluation asymptotically reaches an $\epsilon$-Nash Equilibrium (NE) within $\mathcal{O}(1/\epsilon)$ iterations. This improves upon the previous best result of $\mathcal{O}(1/\epsilon^2)$ iterations and is of the same order, $\mathcal{O}(1/\epsilon)$, that is achievable for the single-agent case. Empirical results for a synthetic potential game and a congestion game are presented to verify the theoretical bounds.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#36825;&#20123;&#31639;&#27861;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#37117;&#33021;&#25910;&#25947;&#21040;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#27169;&#22411;&#25552;&#21462;&#22122;&#22768;&#21644;&#26368;&#23567;&#35206;&#30422;&#25351;&#25968;&#65292;&#25552;&#20513;&#20102;&#26412;&#22320;&#27169;&#22411;&#36873;&#25321;&#21644;&#20840;&#23616;&#27169;&#22411;&#36873;&#25321;&#30340;&#32852;&#21512;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.08670</link><description>&lt;p&gt;
&#27599;&#20010;&#21442;&#25968;&#37117;&#24456;&#37325;&#35201;&#65306;&#30830;&#20445;&#20855;&#26377;&#21160;&#24577;&#24322;&#26500;&#27169;&#22411;&#32553;&#20943;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Every Parameter Matters: Ensuring the Convergence of Federated Learning with Dynamic Heterogeneous Models Reduction. (arXiv:2310.08670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#36825;&#20123;&#31639;&#27861;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#37117;&#33021;&#25910;&#25947;&#21040;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#27169;&#22411;&#25552;&#21462;&#22122;&#22768;&#21644;&#26368;&#23567;&#35206;&#30422;&#25351;&#25968;&#65292;&#25552;&#20513;&#20102;&#26412;&#22320;&#27169;&#22411;&#36873;&#25321;&#21644;&#20840;&#23616;&#27169;&#22411;&#36873;&#25321;&#30340;&#32852;&#21512;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36328;&#35774;&#22791;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#30001;&#20110;&#24213;&#31471;&#35774;&#22791;&#23384;&#22312;&#36164;&#28304;&#29942;&#39048;&#65292;&#37027;&#20123;&#21487;&#33021;&#25552;&#20379;&#29420;&#29305;&#36129;&#29486;&#30340;&#20302;&#31471;&#35774;&#22791;&#34987;&#25490;&#38500;&#22312;&#35757;&#32451;&#22823;&#27169;&#22411;&#20043;&#22806;&#65292;&#36825;&#32473;&#32852;&#37030;&#23398;&#20064;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#38598;&#20013;&#22312;&#24322;&#26500;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#19978;&#65292;&#36890;&#36807;&#20174;&#20840;&#23616;&#27169;&#22411;&#20013;&#25552;&#21462;&#32553;&#23567;&#23610;&#23544;&#30340;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26412;&#22320;&#35774;&#22791;&#12290;&#23613;&#31649;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#30340;&#19968;&#33324;&#29702;&#35770;&#20445;&#35777;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#20805;&#20998;&#26465;&#20214;&#19979;&#65292;&#23545;&#20110;IID&#21644;&#38750;IID&#25968;&#25454;&#65292;&#36825;&#20123;&#31639;&#27861;&#25910;&#25947;&#21040;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#20010;&#31283;&#23450;&#28857;&#65292;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#24179;&#28369;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24433;&#21709;&#20854;&#25910;&#25947;&#24615;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#27169;&#22411;&#25552;&#21462;&#22122;&#22768;&#21644;&#26368;&#23567;&#35206;&#30422;&#25351;&#25968;&#65292;&#24182;&#20027;&#24352;&#20102;&#26412;&#22320;&#27169;&#22411;&#36873;&#25321;&#21644;&#20840;&#23616;&#27169;&#22411;&#36873;&#25321;&#30340;&#32852;&#21512;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-device Federated Learning (FL) faces significant challenges where low-end clients that could potentially make unique contributions are excluded from training large models due to their resource bottlenecks. Recent research efforts have focused on model-heterogeneous FL, by extracting reduced-size models from the global model and applying them to local clients accordingly. Despite the empirical success, general theoretical guarantees of convergence on this method remain an open question. In this paper, we present a unifying framework for heterogeneous FL algorithms with online model extraction and provide a general convergence analysis. In particular, we prove that under certain sufficient conditions and for both IID and non-IID data, these algorithms converge to a stationary point of standard FL for general smooth cost functions. Moreover, we illuminate two key factors impacting its convergence: model-extraction noise and minimum coverage index, advocating a joint design of local 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#30456;&#20284;&#24615;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#33258;&#30456;&#20284;&#21160;&#21147;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#23545;&#20110;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#26694;&#26550;&#21487;&#20197;&#21028;&#26029;&#21160;&#21147;&#23398;&#26159;&#21542;&#33258;&#30456;&#20284;&#65307;&#23545;&#20110;&#19981;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#23427;&#21487;&#20197;&#30830;&#23450;&#21738;&#20010;&#21442;&#25968;&#38598;&#26356;&#25509;&#36817;&#33258;&#30456;&#20284;&#24615;&#12290;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#19982;&#23610;&#24230;&#26080;&#20851;&#30340;&#26680;&#36827;&#34892;&#20219;&#24847;&#23610;&#24230;&#30340;&#24314;&#27169;&#65292;&#24182;&#35782;&#21035;&#33258;&#30456;&#20284;&#31995;&#32479;&#20013;&#30340;&#24130;&#24459;&#25351;&#25968;&#12290;&#21021;&#27493;&#27979;&#35797;&#34920;&#26126;&#65292;&#26041;&#27861;&#23545;Ising&#27169;&#22411;&#30340;&#20020;&#30028;&#25351;&#25968;&#20855;&#26377;&#29702;&#35770;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08282</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#33258;&#30456;&#20284;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Data driven modeling of self-similar dynamics. (arXiv:2310.08282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#30456;&#20284;&#24615;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#33258;&#30456;&#20284;&#21160;&#21147;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#23545;&#20110;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#26694;&#26550;&#21487;&#20197;&#21028;&#26029;&#21160;&#21147;&#23398;&#26159;&#21542;&#33258;&#30456;&#20284;&#65307;&#23545;&#20110;&#19981;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#23427;&#21487;&#20197;&#30830;&#23450;&#21738;&#20010;&#21442;&#25968;&#38598;&#26356;&#25509;&#36817;&#33258;&#30456;&#20284;&#24615;&#12290;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#19982;&#23610;&#24230;&#26080;&#20851;&#30340;&#26680;&#36827;&#34892;&#20219;&#24847;&#23610;&#24230;&#30340;&#24314;&#27169;&#65292;&#24182;&#35782;&#21035;&#33258;&#30456;&#20284;&#31995;&#32479;&#20013;&#30340;&#24130;&#24459;&#25351;&#25968;&#12290;&#21021;&#27493;&#27979;&#35797;&#34920;&#26126;&#65292;&#26041;&#27861;&#23545;Ising&#27169;&#22411;&#30340;&#20020;&#30028;&#25351;&#25968;&#20855;&#26377;&#29702;&#35770;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#24314;&#27169;&#22797;&#26434;&#31995;&#32479;&#23545;&#20110;&#29702;&#35299;&#20854;&#22797;&#26434;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25968;&#25454;&#39537;&#21160;&#22810;&#23610;&#24230;&#24314;&#27169;&#24050;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#31995;&#32479;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33258;&#30456;&#20284;&#24615;&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#36825;&#34920;&#26126;&#22823;&#35268;&#27169;&#22797;&#26434;&#31995;&#32479;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#33258;&#30456;&#20284;&#24615;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#33258;&#30456;&#20284;&#21160;&#21147;&#31995;&#32479;&#30340;&#24314;&#27169;&#12290;&#23545;&#20110;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#21028;&#26029;&#21160;&#21147;&#23398;&#26159;&#21542;&#33258;&#30456;&#20284;&#12290;&#23545;&#20110;&#19981;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#23427;&#21487;&#20197;&#27604;&#36739;&#21644;&#30830;&#23450;&#21738;&#20010;&#21442;&#25968;&#38598;&#26356;&#25509;&#36817;&#33258;&#30456;&#20284;&#24615;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#25105;&#20204;&#20174;&#21160;&#21147;&#23398;&#20013;&#25552;&#21462;&#19982;&#23610;&#24230;&#26080;&#20851;&#30340;&#26680;&#36827;&#34892;&#20219;&#24847;&#23610;&#24230;&#30340;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#33258;&#30456;&#20284;&#31995;&#32479;&#20013;&#30340;&#24130;&#24459;&#25351;&#25968;&#12290;&#23545;Ising&#27169;&#22411;&#30340;&#21021;&#27493;&#27979;&#35797;&#20135;&#29983;&#20102;&#19982;&#29702;&#35770;&#19968;&#33268;&#30340;&#20020;&#30028;&#25351;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale modeling of complex systems is crucial for understanding their intricacies. Data-driven multiscale modeling has emerged as a promising approach to tackle challenges associated with complex systems. On the other hand, self-similarity is prevalent in complex systems, hinting that large-scale complex systems can be modeled at a reduced cost. In this paper, we introduce a multiscale neural network framework that incorporates self-similarity as prior knowledge, facilitating the modeling of self-similar dynamical systems. For deterministic dynamics, our framework can discern whether the dynamics are self-similar. For uncertain dynamics, it can compare and determine which parameter set is closer to self-similarity. The framework allows us to extract scale-invariant kernels from the dynamics for modeling at any scale. Moreover, our method can identify the power law exponents in self-similar systems. Preliminary tests on the Ising model yielded critical exponents consistent with theo
&lt;/p&gt;</description></item><item><title>MetaBox&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#24179;&#21488;&#65292;&#25552;&#20379;&#28789;&#27963;&#30340;&#31639;&#27861;&#27169;&#26495;&#12289;&#24191;&#27867;&#30340;&#38382;&#39064;&#23454;&#20363;&#21644;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26631;&#20934;&#21270;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20197;&#20419;&#36827;&#26041;&#27861;&#30340;&#20005;&#26684;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.08252</link><description>&lt;p&gt;
MetaBox&#65306;&#19968;&#31181;&#29992;&#20110;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning. (arXiv:2310.08252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08252
&lt;/p&gt;
&lt;p&gt;
MetaBox&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#24179;&#21488;&#65292;&#25552;&#20379;&#28789;&#27963;&#30340;&#31639;&#27861;&#27169;&#26495;&#12289;&#24191;&#27867;&#30340;&#38382;&#39064;&#23454;&#20363;&#21644;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26631;&#20934;&#21270;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20197;&#20419;&#36827;&#26041;&#27861;&#30340;&#20005;&#26684;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#65288;MetaBBO-RL&#65289;&#23637;&#31034;&#20102;&#22312;&#20803;&#32423;&#21035;&#19978;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20943;&#23569;&#23545;&#20302;&#32423;&#40657;&#31665;&#20248;&#21270;&#22120;&#30340;&#25163;&#21160;&#24494;&#35843;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#39046;&#22495;&#30001;&#20110;&#32570;&#20047;&#32479;&#19968;&#30340;&#22522;&#20934;&#32780;&#21463;&#21040;&#38459;&#30861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MetaBox&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;MetaBBO-RL&#26041;&#27861;&#32780;&#35774;&#35745;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#24179;&#21488;&#12290;MetaBox&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#31639;&#27861;&#27169;&#26495;&#65292;&#35753;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#22320;&#22312;&#24179;&#21488;&#20869;&#23454;&#29616;&#33258;&#24049;&#30340;&#29420;&#29305;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#20379;&#20102;&#36229;&#36807;300&#20010;&#38382;&#39064;&#23454;&#20363;&#65292;&#20174;&#21512;&#25104;&#21040;&#30495;&#23454;&#22330;&#26223;&#30340;&#24191;&#27867;&#33539;&#22260;&#65292;&#24182;&#19988;&#21253;&#21547;&#20102;19&#31181;&#22522;&#32447;&#26041;&#27861;&#30340;&#35814;&#23613;&#24211;&#65292;&#21253;&#25324;&#20256;&#32479;&#40657;&#31665;&#20248;&#21270;&#22120;&#21644;&#26368;&#36817;&#30340;MetaBBO-RL&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;MetaBox&#24341;&#20837;&#20102;&#19977;&#20010;&#26631;&#20934;&#21270;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20351;&#26041;&#27861;&#30340;&#35780;&#20272;&#26356;&#21152;&#20840;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to mitigate manual fine-tuning of low-level black-box optimizers. However, this field is hindered by the lack of a unified benchmark. To fill this gap, we introduce MetaBox, the first benchmark platform expressly tailored for developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible algorithmic template that allows users to effortlessly implement their unique designs within the platform. Moreover, it provides a broad spectrum of over 300 problem instances, collected from synthetic to realistic scenarios, and an extensive library of 19 baseline methods, including both traditional black-box optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three standardized performance metrics, enabling a more thorough assessment of the methods. In a bid to illustrate the utility of MetaBox for facilitating rigorous evaluation and in-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31163;&#32447;&#25511;&#21046;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#20316;&#20026;&#20915;&#31574;&#35821;&#26009;&#24211;&#65292;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#38382;&#36131;&#21046;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07747</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38382;&#36131;&#21046;&#65306;&#29992;&#35821;&#26009;&#24211;&#30340;&#20363;&#23376;&#35299;&#37322;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples. (arXiv:2310.07747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31163;&#32447;&#25511;&#21046;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#20316;&#20026;&#20915;&#31574;&#35821;&#26009;&#24211;&#65292;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#38382;&#36131;&#21046;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#23398;&#20064;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#30340;&#25511;&#21046;&#22120;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#38477;&#20302;&#22312;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#22312;&#36131;&#20219;&#25935;&#24863;&#30340;&#35774;&#32622;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#65289;&#20013;&#65292;&#20915;&#31574;&#38382;&#36131;&#21046;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#30340;&#25991;&#29486;&#23578;&#26410;&#20805;&#20998;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Accountable Offline Controller&#65288;AOC&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#31163;&#32447;&#25968;&#25454;&#38598;&#20316;&#20026;&#20915;&#31574;&#35821;&#26009;&#24211;&#65292;&#24182;&#26681;&#25454;&#19968;&#32452;&#23450;&#21046;&#30340;&#20363;&#23376;&#65288;&#31216;&#20026;&#35821;&#26009;&#24211;&#23376;&#38598;&#65289;&#36827;&#34892;&#38382;&#36131;&#21046;&#30340;&#25511;&#21046;&#12290;AOC&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#36816;&#34892;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#20005;&#26684;&#30340;&#31163;&#32447;&#27169;&#20223;&#35774;&#32622;&#65292;&#24182;&#34920;&#29616;&#20986;&#20445;&#25252;&#21644;&#36866;&#24212;&#24615;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#30340;&#21307;&#30103;&#20445;&#20581;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;AOC&#30340;&#24615;&#33021;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#20445;&#25345;&#38382;&#36131;&#21046;&#30340;&#21516;&#26102;&#33021;&#22815;&#31649;&#29702;&#39640;&#27700;&#24179;&#30340;&#31163;&#32447;&#25511;&#21046;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning transparent, interpretable controllers with offline data in decision-making systems is an essential area of research due to its potential to reduce the risk of applications in real-world systems. However, in responsibility-sensitive settings such as healthcare, decision accountability is of paramount importance, yet has not been adequately addressed by the literature. This paper introduces the Accountable Offline Controller (AOC) that employs the offline dataset as the Decision Corpus and performs accountable control based on a tailored selection of examples, referred to as the Corpus Subset. ABC operates effectively in low-data scenarios, can be extended to the strictly offline imitation setting, and displays qualities of both conservation and adaptability. We assess ABC's performance in both simulated and real-world healthcare scenarios, emphasizing its capability to manage offline control tasks with high levels of performance while maintaining accountability.  Keywords: Int
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#31070;&#32463;&#23849;&#28291;&#27010;&#24565;&#25193;&#23637;&#21040;&#31867;&#21035;&#25968;&#36828;&#22823;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#30340;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#30340;&#26368;&#23567;&#36793;&#30028;&#20540;&#34987;&#26368;&#22823;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.05351</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#31867;&#21035;&#19979;&#30340;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Generalized Neural Collapse for a Large Number of Classes. (arXiv:2310.05351v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#31070;&#32463;&#23849;&#28291;&#27010;&#24565;&#25193;&#23637;&#21040;&#31867;&#21035;&#25968;&#36828;&#22823;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#30340;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#30340;&#26368;&#23567;&#36793;&#30028;&#20540;&#34987;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#23849;&#28291;&#25552;&#20379;&#20102;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#26368;&#21518;&#19968;&#23618;&#34920;&#31034;&#65288;&#21363;&#29305;&#24449;&#65289;&#21644;&#20998;&#31867;&#22120;&#26435;&#37325;&#30340;&#20248;&#38597;&#25968;&#23398;&#25551;&#36848;&#12290;&#36825;&#31181;&#32467;&#26524;&#19981;&#20165;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#65292;&#36824;&#28608;&#21457;&#20102;&#25913;&#36827;&#23454;&#38469;&#28145;&#24230;&#27169;&#22411;&#30340;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#31070;&#32463;&#23849;&#28291;&#30340;&#29616;&#26377;&#32463;&#39564;&#21644;&#29702;&#35770;&#30740;&#31350;&#37117;&#38598;&#20013;&#20110;&#31867;&#21035;&#25968;&#30456;&#23545;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#36739;&#23567;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#23558;&#31070;&#32463;&#23849;&#28291;&#25193;&#23637;&#21040;&#31867;&#21035;&#25968;&#36828;&#22823;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#30340;&#24773;&#20917;&#65292;&#36825;&#22312;&#35821;&#35328;&#27169;&#22411;&#12289;&#26816;&#32034;&#31995;&#32479;&#21644;&#20154;&#33080;&#35782;&#21035;&#24212;&#29992;&#20013;&#24191;&#27867;&#20986;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29305;&#24449;&#21644;&#20998;&#31867;&#22120;&#23637;&#29616;&#20986;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#65292;&#20854;&#20013;&#26368;&#23567;&#30340;&#19968;&#23545;&#20854;&#20182;&#31867;&#21035;&#38388;&#36793;&#30028;&#20540;&#34987;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#39564;&#35777;&#23454;&#38469;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#30340;&#21457;&#29983;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#20197;&#34920;&#26126;&#8230;.
&lt;/p&gt;
&lt;p&gt;
Neural collapse provides an elegant mathematical characterization of learned last layer representations (a.k.a. features) and classifier weights in deep classification models. Such results not only provide insights but also motivate new techniques for improving practical deep models. However, most of the existing empirical and theoretical studies in neural collapse focus on the case that the number of classes is small relative to the dimension of the feature space. This paper extends neural collapse to cases where the number of classes are much larger than the dimension of feature space, which broadly occur for language models, retrieval systems, and face recognition applications. We show that the features and classifier exhibit a generalized neural collapse phenomenon, where the minimum one-vs-rest margins is maximized.We provide empirical study to verify the occurrence of generalized neural collapse in practical deep neural networks. Moreover, we provide theoretical study to show tha
&lt;/p&gt;</description></item><item><title>&#27969;&#34892;&#23398;&#20064;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#21270;&#30340;&#36890;&#20449;&#25299;&#25169;&#32467;&#26500;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01972</link><description>&lt;p&gt;
&#27969;&#34892;&#23398;&#20064;&#65306;&#36890;&#36807;&#38543;&#26426;&#36890;&#20449;&#22686;&#24378;&#20998;&#25955;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Epidemic Learning: Boosting Decentralized Learning with Randomized Communication. (arXiv:2310.01972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01972
&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#23398;&#20064;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#21270;&#30340;&#36890;&#20449;&#25299;&#25169;&#32467;&#26500;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27969;&#34892;&#23398;&#20064;&#65288;EL&#65289;&#30340;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#19981;&#26029;&#21464;&#21270;&#30340;&#36890;&#20449;&#25299;&#25169;&#32467;&#26500;&#65292;&#20197;&#27604;&#20256;&#32479;&#30340;DL&#26041;&#27861;&#26356;&#24555;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;EL&#30340;&#27599;&#19968;&#36718;&#20013;&#65292;&#27599;&#20010;&#33410;&#28857;&#23558;&#20854;&#27169;&#22411;&#26356;&#26032;&#21457;&#36865;&#32473;&#19968;&#20010;&#38543;&#26426;&#26679;&#26412;&#30340;$s$&#20010;&#20854;&#20182;&#33410;&#28857;&#65288;&#22312;$n$&#20010;&#33410;&#28857;&#30340;&#31995;&#32479;&#20013;&#65289;&#12290;&#25105;&#20204;&#23545;EL&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20854;&#21464;&#21270;&#30340;&#25299;&#25169;&#32467;&#26500;&#23548;&#33268;&#20102;&#27604;&#29616;&#26377;&#30340;&#65288;&#38745;&#24577;&#21644;&#21160;&#24577;&#65289;&#25299;&#25169;&#32467;&#26500;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#23545;&#20110;&#24179;&#28369;&#30340;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;EL&#30340;&#26242;&#24577;&#36845;&#20195;&#27425;&#25968;&#65292;&#21363;&#36798;&#21040;&#28176;&#36817;&#32447;&#24615;&#21152;&#36895;&#25152;&#38656;&#30340;&#36718;&#25968;&#65292;&#26159;$\mathcal{O}(\frac{n^3}{s^2})$&#65292;&#36229;&#36807;&#20102;&#24050;&#30693;&#30340;&#26368;&#20339;&#30028;&#38480;$\mathcal{O}({n^3})$&#65292;&#22686;&#21152;&#20102;$s^2$&#20493;&#65292;&#34920;&#26126;&#20102;&#38543;&#26426;&#36890;&#20449;&#22312;DL&#20013;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;96&#20010;&#33410;&#28857;&#30340;&#32593;&#32476;&#20013;&#23545;EL&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#29616;&#26377;&#30340;DL&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;EL&#36798;&#21040;&#20102;&#26356;&#24555;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Epidemic Learning (EL), a simple yet powerful decentralized learning (DL) algorithm that leverages changing communication topologies to achieve faster model convergence compared to conventional DL approaches. At each round of EL, each node sends its model updates to a random sample of $s$ other nodes (in a system of $n$ nodes). We provide an extensive theoretical analysis of EL, demonstrating that its changing topology culminates in superior convergence properties compared to the state-of-the-art (static and dynamic) topologies. Considering smooth non-convex loss functions, the number of transient iterations for EL, i.e., the rounds required to achieve asymptotic linear speedup, is in $\mathcal{O}(\frac{n^3}{s^2})$ which outperforms the best-known bound $\mathcal{O}({n^3})$ by a factor of $ s^2 $, indicating the benefit of randomized communication for DL. We empirically evaluate EL in a 96-node network and compare its performance with state-of-the-art DL approaches. Our resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01468</link><description>&lt;p&gt;
&#23454;&#20307;&#25512;&#26029;&#31454;&#25216;&#22330;&#65306;&#25506;&#31350;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#26126;&#30830;&#25552;&#38382;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#21547;&#31946;&#19981;&#28165;&#30340;&#26597;&#35810;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#34892;&#20026;&#38590;&#20197;&#39044;&#27979;&#24182;&#20135;&#29983;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#20197;&#26377;&#25928;&#35299;&#20915;&#27495;&#20041;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#33021;&#21147;&#38656;&#35201;&#23545;&#22810;&#20010;&#23545;&#35805;&#36718;&#27425;&#36827;&#34892;&#22797;&#26434;&#30340;&#29702;&#35299;&#12289;&#29366;&#24577;&#36319;&#36394;&#12289;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#27979;&#37327;&#36825;&#31181;&#33021;&#21147;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#65292;&#35780;&#20272;&#20102;LLMs&#25512;&#26029;&#33258;&#24049;&#19981;&#30693;&#36947;&#20294;&#34987;&#27861;&#23448;&#25581;&#31034;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#8220;&#23454;&#20307;&#25512;&#26029;&#28216;&#25103;&#8221;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;LLMs&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#23427;&#20204;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24378;&#22823;&#30340;LLMs...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
&lt;/p&gt;</description></item><item><title>DeepPCR&#26159;&#19968;&#31181;&#26032;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24182;&#34892;&#24490;&#29615;&#38477;&#35299;&#31639;&#27861;&#23558;&#24120;&#35268;&#30340;&#39034;&#24207;&#25805;&#20316;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#20013;&#24182;&#34892;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35745;&#31639;&#36895;&#24230;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.16318</link><description>&lt;p&gt;
DeepPCR&#65306;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24182;&#34892;&#21270;&#24207;&#21015;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
DeepPCR: Parallelizing Sequential Operations in Neural Networks. (arXiv:2309.16318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16318
&lt;/p&gt;
&lt;p&gt;
DeepPCR&#26159;&#19968;&#31181;&#26032;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24182;&#34892;&#24490;&#29615;&#38477;&#35299;&#31639;&#27861;&#23558;&#24120;&#35268;&#30340;&#39034;&#24207;&#25805;&#20316;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#20013;&#24182;&#34892;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35745;&#31639;&#36895;&#24230;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#21270;&#25216;&#26415;&#24050;&#32463;&#22312;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#20013;&#21464;&#24471;&#26222;&#36941;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20173;&#28982;&#26377;&#19968;&#20123;&#25805;&#20316;&#26159;&#25353;&#39034;&#24207;&#36827;&#34892;&#30340;&#12290;&#20363;&#22914;&#65292;&#21069;&#21521;&#20256;&#36882;&#21644;&#21453;&#21521;&#20256;&#36882;&#26159;&#36880;&#23618;&#25191;&#34892;&#30340;&#65292;&#24182;&#19988;&#25193;&#25955;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#36890;&#36807;&#24212;&#29992;&#19968;&#31995;&#21015;&#21435;&#22122;&#27493;&#39588;&#20135;&#29983;&#30340;&#12290;&#36825;&#31181;&#39034;&#24207;&#26041;&#27861;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#19982;&#25152;&#28041;&#21450;&#27493;&#39588;&#30340;&#25968;&#37327;&#25104;&#27491;&#27604;&#65292;&#38543;&#30528;&#27493;&#39588;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#21487;&#33021;&#20986;&#29616;&#28508;&#22312;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;DeepPCR&#65292;&#23427;&#24182;&#34892;&#21270;&#20102;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#21644;&#35757;&#32451;&#20013;&#36890;&#24120;&#26159;&#39034;&#24207;&#25805;&#20316;&#30340;&#27493;&#39588;&#12290;DeepPCR&#22522;&#20110;&#23558;$L$&#27493;&#39588;&#30340;&#24207;&#21015;&#35299;&#37322;&#20026;&#29305;&#23450;&#26041;&#31243;&#32452;&#30340;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#24182;&#34892;&#24490;&#29615;&#38477;&#35299;&#31639;&#27861;&#24674;&#22797;&#36825;&#20123;&#26041;&#31243;&#12290;&#36825;&#23558;&#39034;&#24207;&#25805;&#20316;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;$\mathcal{O}(L)$&#38477;&#20302;&#21040;$\mathcal{O}(\log_2L)$&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parallelization techniques have become ubiquitous for accelerating inference and training of deep neural networks. Despite this, several operations are still performed in a sequential manner. For instance, the forward and backward passes are executed layer-by-layer, and the output of diffusion models is produced by applying a sequence of denoising steps. This sequential approach results in a computational cost proportional to the number of steps involved, presenting a potential bottleneck as the number of steps increases. In this work, we introduce DeepPCR, a novel algorithm which parallelizes typically sequential operations used in inference and training of neural networks. DeepPCR is based on interpreting a sequence of $L$ steps as the solution of a specific system of equations, which we recover using the Parallel Cyclic Reduction algorithm. This reduces the complexity of computing the sequential operations from $\mathcal{O}(L)$ to $\mathcal{O}(\log_2L)$, thus yielding a speedup for 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22797;&#26434;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#25366;&#25496;&#24739;&#32773;&#36335;&#24452;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#36335;&#24452;&#27169;&#22411;&#12289;&#26032;&#30340;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#21644;&#22522;&#20110;&#20256;&#32479;&#20013;&#24515;&#24230;&#30340;&#25366;&#25496;&#26041;&#27861;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.14208</link><description>&lt;p&gt;
&#22522;&#20110;&#22797;&#26434;&#32593;&#32476;&#30340;&#26694;&#26550;&#29992;&#20110;&#24314;&#27169;&#21644;&#25366;&#25496;&#24739;&#32773;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Framework based on complex networks to model and mine patient pathways. (arXiv:2309.14208v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14208
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22797;&#26434;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#25366;&#25496;&#24739;&#32773;&#36335;&#24452;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#36335;&#24452;&#27169;&#22411;&#12289;&#26032;&#30340;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#21644;&#22522;&#20110;&#20256;&#32479;&#20013;&#24515;&#24230;&#30340;&#25366;&#25496;&#26041;&#27861;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21457;&#29616;&#29992;&#20110;&#34920;&#31034;&#19968;&#32452;&#24739;&#32773;&#19982;&#21307;&#30103;&#31995;&#32479;&#30340;&#25509;&#35302;&#21382;&#21490;&#30340;&#27169;&#22411;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#24739;&#32773;&#36335;&#24452;&#8221;&#65292;&#26159;&#19968;&#39033;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23427;&#25903;&#25345;&#20020;&#24202;&#21644;&#32452;&#32455;&#20915;&#31574;&#65292;&#20197;&#25552;&#39640;&#25552;&#20379;&#30340;&#27835;&#30103;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;&#24930;&#24615;&#30149;&#24739;&#32773;&#30340;&#36335;&#24452;&#24448;&#24448;&#22240;&#20154;&#32780;&#24322;&#65292;&#26377;&#37325;&#22797;&#30340;&#20219;&#21153;&#65292;&#24182;&#38656;&#35201;&#20998;&#26512;&#22810;&#20010;&#26041;&#38754;&#65288;&#24178;&#39044;&#12289;&#35786;&#26029;&#12289;&#21307;&#30103;&#19987;&#19994;&#31561;&#65289;&#65292;&#24433;&#21709;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#24314;&#27169;&#21644;&#25366;&#25496;&#36825;&#20123;&#36335;&#24452;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#22522;&#20110;&#22810;&#26041;&#38754;&#22270;&#30340;&#36335;&#24452;&#27169;&#22411;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#32791;&#26102;&#65292;&#29992;&#20110;&#27604;&#36739;&#36335;&#24452;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#22522;&#20110;&#20256;&#32479;&#20013;&#24515;&#24230;&#27979;&#37327;&#26041;&#27861;&#30340;&#25366;&#25496;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#36335;&#24452;&#20013;&#26368;&#30456;&#20851;&#30340;&#27493;&#39588;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#35780;&#20272;&#20102;&#36825;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automatic discovery of a model to represent the history of encounters of a group of patients with the healthcare system -- the so-called "pathway of patients" -- is a new field of research that supports clinical and organisational decisions to improve the quality and efficiency of the treatment provided. The pathways of patients with chronic conditions tend to vary significantly from one person to another, have repetitive tasks, and demand the analysis of multiple perspectives (interventions, diagnoses, medical specialities, among others) influencing the results. Therefore, modelling and mining those pathways is still a challenging task. In this work, we propose a framework comprising: (i) a pathway model based on a multi-aspect graph, (ii) a novel dissimilarity measurement to compare pathways taking the elapsed time into account, and (iii) a mining method based on traditional centrality measures to discover the most relevant steps of the pathways. We evaluated the framework using 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#65292;&#35813;&#26041;&#27861;&#27604;&#22522;&#20934;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#32422;11%&#21644;8%&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#22914;&#21457;&#20986;&#26089;&#26399;&#35686;&#21578;&#21644;&#23450;&#37327;&#29305;&#23450;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13135</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors. (arXiv:2309.13135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#65292;&#35813;&#26041;&#27861;&#27604;&#22522;&#20934;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#32422;11%&#21644;8%&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#22914;&#21457;&#20986;&#26089;&#26399;&#35686;&#21578;&#21644;&#23450;&#37327;&#29305;&#23450;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#23545;&#20110;&#26089;&#26399;&#26816;&#27979;&#19981;&#33391;&#32467;&#26524;&#21644;&#24739;&#32773;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#22024;&#26434;&#21644;&#38388;&#27463;&#24615;&#65292;&#23454;&#38469;&#20013;&#39044;&#27979;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#36825;&#20123;&#25361;&#25112;&#36890;&#24120;&#36890;&#36807;&#22806;&#37096;&#22240;&#32032;&#35825;&#23548;&#30340;&#21464;&#21270;&#28857;&#65288;&#22914;&#33647;&#29289;&#20351;&#29992;&#65289;&#32780;&#21152;&#21095;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#21521;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#25928;&#24212;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#21463;&#27835;&#30103;&#24433;&#21709;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#20351;&#29992;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#39044;&#27979;&#34880;&#31958;&#30340;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#33647;&#21160;&#23398;&#32534;&#30721;&#22120;&#20351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36229;&#36807;&#22522;&#20934;&#32422;11&#65285;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36229;&#36807;8&#65285;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#20363;&#22914;&#21457;&#20986;&#20851;&#20110;&#24847;&#22806;&#27835;&#30103;&#21453;&#24212;&#30340;&#26089;&#26399;&#35686;&#21578;&#65292;&#25110;&#24110;&#21161;&#34920;&#24449;&#29305;&#23450;&#20110;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting healthcare time series is crucial for early detection of adverse outcomes and for patient monitoring. Forecasting, however, can be difficult in practice due to noisy and intermittent data. The challenges are often exacerbated by change points induced via extrinsic factors, such as the administration of medication. We propose a novel encoder that informs deep learning models of the pharmacokinetic effects of drugs to allow for accurate forecasting of time series affected by treatment. We showcase the effectiveness of our approach in a task to forecast blood glucose using both realistically simulated and real-world data. Our pharmacokinetic encoder helps deep learning models surpass baselines by approximately 11% on simulated data and 8% on real-world data. The proposed approach can have multiple beneficial applications in clinical practice, such as issuing early warnings about unexpected treatment responses, or helping to characterize patient-specific treatment effects in te
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#32553;&#20943;&#20316;&#20026;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#20462;&#21098;&#27169;&#22411;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.12095</link><description>&lt;p&gt;
&#20855;&#26377;&#36125;&#21494;&#26031;&#27169;&#22411;&#32553;&#20943;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian sparsification for deep neural networks with Bayesian model reduction. (arXiv:2309.12095v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12095
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#32553;&#20943;&#20316;&#20026;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#20462;&#21098;&#27169;&#22411;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#24040;&#22823;&#33021;&#21147;&#24120;&#24120;&#21463;&#21040;&#20854;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#23545;&#20110;&#26377;&#25928;&#30340;&#31232;&#30095;&#25216;&#26415;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#36125;&#21494;&#26031;&#31232;&#30095;&#24615;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#32780;&#35328;&#26159;&#19968;&#31181;&#20851;&#38190;&#26041;&#27861;&#65292;&#21487;&#20197;&#20419;&#36827;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#35774;&#35745;&#26082;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21448;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#30446;&#21069;&#65292;&#36125;&#21494;&#26031;&#31232;&#30095;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#25216;&#26415;&#26159;&#23558;&#32467;&#26500;&#25910;&#32553;&#20808;&#39564;&#24212;&#29992;&#20110;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#40657;&#30418;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#30340;&#36817;&#20284;&#25512;&#26029;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#19982;&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#28857;&#20272;&#35745;&#30456;&#27604;&#65292;&#23436;&#25972;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#21453;&#28436;&#22312;&#35745;&#31639;&#26041;&#38754;&#38750;&#24120;&#32791;&#36153;&#26102;&#38388;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20513;&#20351;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#32553;&#20943;&#65288;BMR&#65289;&#20316;&#20026;&#27169;&#22411;&#26435;&#37325;&#20462;&#21098;&#30340;&#26356;&#39640;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#20316;&#20026;&#20915;&#31574;&#29575;&#30340;&#25512;&#24191;&#65292;BMR&#20801;&#35768;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#20107;&#21518;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Deep learning's immense capabilities are often constrained by the complexity of its models, leading to an increasing demand for effective sparsification techniques. Bayesian sparsification for deep learning emerges as a crucial approach, facilitating the design of models that are both computationally efficient and competitive in terms of performance across various deep learning applications. The state-of-the-art -- in Bayesian sparsification of deep neural networks -- combines structural shrinkage priors on model weights with an approximate inference scheme based on black-box stochastic variational inference. However, model inversion of the full generative model is exceptionally computationally demanding, especially when compared to standard deep learning of point estimates. In this context, we advocate for the use of Bayesian model reduction (BMR) as a more efficient alternative for pruning of model weights. As a generalization of the Savage-Dickey ratio, BMR allows a post-hoc elimina
&lt;/p&gt;</description></item><item><title>beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.07867</link><description>&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07867
&lt;/p&gt;
&lt;p&gt;
beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;beta&#25193;&#25955;&#65292;&#19968;&#31181;&#23558;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#38598;&#25104;&#21040;&#19968;&#36215;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#20351;&#29992;&#20102;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#65292;beta&#25193;&#25955;&#21033;&#29992;&#20102;&#38543;&#26102;&#38388;&#30340;&#20056;&#27861;&#36716;&#25442;&#26469;&#21019;&#24314;&#27491;&#21521;&#21644;&#21453;&#21521;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#21516;&#26102;&#32500;&#25345;&#30528;&#27491;&#21521;&#36793;&#32536;&#20998;&#24067;&#21644;&#21453;&#21521;&#26465;&#20214;&#20998;&#24067;&#65292;&#32473;&#23450;&#20219;&#24847;&#26102;&#38388;&#28857;&#30340;&#25968;&#25454;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;&#20256;&#32479;&#27169;&#22411;&#20381;&#36182;&#20110;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#21644;&#37325;&#26032;&#21152;&#26435;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#65292;beta&#25193;&#25955;&#26159;&#20056;&#27861;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#20174;KL&#25955;&#24230;&#30340;&#20984;&#24615;&#25512;&#23548;&#20986;&#26469;&#30340;KL&#25955;&#24230;&#19978;&#30028;&#65288;KLUB&#65289;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;KLUB&#30456;&#23545;&#20110;&#36127;ELBO&#26469;&#35828;&#23545;&#20110;&#20248;&#21270;beta&#25193;&#25955;&#26356;&#21152;&#26377;&#25928;&#65292;&#36127;ELBO&#20063;&#21487;&#20197;&#20316;&#20026;&#30456;&#21516;KL&#25955;&#24230;&#30340;KLUB&#65292;&#21482;&#26159;&#20854;&#20004;&#20010;&#21442;&#25968;&#20132;&#25442;&#20102;&#20301;&#32622;&#12290;beta&#25193;&#25955;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;Bregman&#25955;&#24230;&#20026;&#25351;&#26631;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, furt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$d$-DRFWL(2) GNNs&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#38480;&#21046;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23454;&#29616;&#24490;&#29615;&#35745;&#25968;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#23376;&#22270;GNNs&#30340;&#39044;&#22788;&#29702;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.04941</link><description>&lt;p&gt;
&#38480;&#21046;&#36317;&#31163;&#30340;&#27665;&#38388;&#20256;&#35828;Weisfeiler-Leman&#22270;&#31070;&#32463;&#32593;&#32476;&#21450;&#21487;&#35777;&#26126;&#30340;&#24490;&#29615;&#35745;&#25968;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power. (arXiv:2309.04941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$d$-DRFWL(2) GNNs&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#38480;&#21046;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23454;&#29616;&#24490;&#29615;&#35745;&#25968;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#23376;&#22270;GNNs&#30340;&#39044;&#22788;&#29702;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#33021;&#21147;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#25104;&#21151;&#35745;&#25968;&#29305;&#23450;&#30340;&#22270;&#23376;&#32467;&#26500;&#65292;&#23588;&#20854;&#26159;&#24490;&#29615;&#65292;&#23545;&#20110;GNNs&#30340;&#25104;&#21151;&#38750;&#24120;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#23427;&#24050;&#34987;&#29992;&#20316;&#35780;&#20272;GNNs&#34920;&#36798;&#33021;&#21147;&#30340;&#19968;&#31181;&#24120;&#29992;&#25351;&#26631;&#12290;&#35768;&#22810;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#24490;&#29615;&#35745;&#25968;&#33021;&#21147;&#30340;GNN&#27169;&#22411;&#37117;&#22522;&#20110;&#23376;&#22270;GNNs&#65292;&#21363;&#20174;&#36755;&#20837;&#22270;&#20013;&#25552;&#21462;&#19968;&#32452;&#23376;&#22270;&#65292;&#20026;&#27599;&#20010;&#23376;&#22270;&#29983;&#25104;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#22686;&#24378;&#36755;&#20837;&#22270;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#32321;&#37325;&#30340;&#39044;&#22788;&#29702;&#65292;&#24182;&#19988;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#36739;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;GNN&#31867;&#21035;-- $d$-Distance-Restricted FWL(2) GNNs&#65292;&#25110;&#32773; $d$-DRFWL(2) GNNs&#65292;&#20811;&#26381;&#20102;&#23376;&#22270;GNNs&#30340;&#19978;&#36848;&#38480;&#21046;&#12290;$d$-DRFWL(2) GNNs&#23558;&#20114;&#30456;&#20043;&#38388;&#36317;&#31163;&#19981;&#36229;&#36807;$d$&#30340;&#33410;&#28857;&#23545;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#30340;&#21333;&#20301;&#65292;&#20197;&#24179;&#34913;&#34920;&#36798;&#33021;&#21147;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of graph neural networks (GNNs) to count certain graph substructures, especially cycles, is important for the success of GNNs on a wide range of tasks. It has been recently used as a popular metric for evaluating the expressive power of GNNs. Many of the proposed GNN models with provable cycle counting power are based on subgraph GNNs, i.e., extracting a bag of subgraphs from the input graph, generating representations for each subgraph, and using them to augment the representation of the input graph. However, those methods require heavy preprocessing, and suffer from high time and memory costs. In this paper, we overcome the aforementioned limitations of subgraph GNNs by proposing a novel class of GNNs -- $d$-Distance-Restricted FWL(2) GNNs, or $d$-DRFWL(2) GNNs. $d$-DRFWL(2) GNNs use node pairs whose mutual distances are at most $d$ as the units for message passing to balance the expressive power and complexity. By performing message passing among distance-restricted node
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28508;&#22312;&#20960;&#20309;&#25628;&#32034;(NLGS)&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#36317;&#31163;&#26469;&#33258;&#21160;&#35782;&#21035;&#19979;&#28216;&#20219;&#21153;&#30340;&#26368;&#20339;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04810</link><description>&lt;p&gt;
&#31070;&#32463;&#28508;&#22312;&#20960;&#20309;&#25628;&#32034;&#65306;&#36890;&#36807;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#20449;&#24687;&#39537;&#21160;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#36827;&#34892;&#20056;&#31215;&#27969;&#24418;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization. (arXiv:2309.04810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28508;&#22312;&#20960;&#20309;&#25628;&#32034;(NLGS)&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#36317;&#31163;&#26469;&#33258;&#21160;&#35782;&#21035;&#19979;&#28216;&#20219;&#21153;&#30340;&#26368;&#20339;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#19982;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#23545;&#40784;&#65292;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#24658;&#23450;&#26354;&#29575;&#30340;&#21452;&#26354;&#21644;&#29699;&#24418;&#31354;&#38388;&#65292;&#25110;&#32773;&#23427;&#20204;&#30340;&#32452;&#21512;&#65292;&#26469;&#26356;&#22909;&#22320;&#24314;&#27169;&#28508;&#22312;&#31354;&#38388;&#24182;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#33258;&#21160;&#35782;&#21035;&#19979;&#28216;&#20219;&#21153;&#30340;&#26368;&#20339;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#38382;&#39064;&#36824;&#27809;&#26377;&#32473;&#20104;&#36275;&#22815;&#20851;&#27880;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#19978;&#23450;&#20041;&#20102;&#36825;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#31216;&#20026;&#31070;&#32463;&#28508;&#22312;&#20960;&#20309;&#25628;&#32034;(NLGS)&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#36317;&#31163;&#30340;&#20505;&#36873;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#20043;&#38388;&#30340;&#26032;&#27010;&#24565;&#36317;&#31163;&#65292;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20026;&#20102;&#35745;&#31639;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#36317;&#31163;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#26597;&#35810;&#35780;&#20272;&#25628;&#32034;&#30001;&#24658;&#23450;&#26354;&#29575;&#27169;&#22411;&#31354;&#38388;&#20056;&#31215;&#32452;&#25104;&#30340;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#30340;&#21407;&#21017;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research indicates that the performance of machine learning models can be improved by aligning the geometry of the latent space with the underlying data structure. Rather than relying solely on Euclidean space, researchers have proposed using hyperbolic and spherical spaces with constant curvature, or combinations thereof, to better model the latent space and enhance model performance. However, little attention has been given to the problem of automatically identifying the optimal latent geometry for the downstream task. We mathematically define this novel formulation and coin it as neural latent geometry search (NLGS). More specifically, we introduce a principled method that searches for a latent geometry composed of a product of constant curvature model spaces with minimal query evaluations. To accomplish this, we propose a novel notion of distance between candidate latent geometries based on the Gromov-Hausdorff distance from metric geometry. In order to compute the Gromov-Ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#21457;&#29616;&#21508;&#31181;&#31867;&#22411;&#30340;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#21644;&#24352;&#37327;&#20540;&#20989;&#25968;&#26500;&#25104;&#30340;&#26032;&#39062;&#26550;&#26500;&#65292;&#22312;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#21644;&#26799;&#24230;&#19979;&#38477;&#30340;&#24110;&#21161;&#19979;&#65292;&#39640;&#25928;&#22320;&#20248;&#21270;&#24182;&#23398;&#20064;&#21040;&#23545;&#31216;&#24615;&#12290;&#22312;&#22270;&#20687;&#25968;&#23383;&#27714;&#21644;&#21644;&#22810;&#39033;&#24335;&#22238;&#24402;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02898</link><description>&lt;p&gt;
&#21457;&#29616;&#31163;&#25955;&#23545;&#31216;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Discovering Discrete Symmetries. (arXiv:2309.02898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#21457;&#29616;&#21508;&#31181;&#31867;&#22411;&#30340;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#21644;&#24352;&#37327;&#20540;&#20989;&#25968;&#26500;&#25104;&#30340;&#26032;&#39062;&#26550;&#26500;&#65292;&#22312;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#21644;&#26799;&#24230;&#19979;&#38477;&#30340;&#24110;&#21161;&#19979;&#65292;&#39640;&#25928;&#22320;&#20248;&#21270;&#24182;&#23398;&#20064;&#21040;&#23545;&#31216;&#24615;&#12290;&#22312;&#22270;&#20687;&#25968;&#23383;&#27714;&#21644;&#21644;&#22810;&#39033;&#24335;&#22238;&#24402;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#19968;&#31867;&#23545;&#31216;&#24615;&#20013;&#23398;&#20064;&#19968;&#20010;&#31526;&#21512;&#23545;&#31216;&#24615;&#30340;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#21457;&#29616;&#21253;&#25324;&#23616;&#37096;&#23545;&#31216;&#12289;&#20108;&#38754;&#35282;&#21644;&#24490;&#29615;&#23376;&#32676;&#22312;&#20869;&#30340;&#24191;&#27867;&#23376;&#32676;&#30340;&#23545;&#31216;&#24615;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#30001;&#32447;&#24615;&#21644;&#24352;&#37327;&#20540;&#20989;&#25968;&#32452;&#25104;&#30340;&#26032;&#39062;&#26550;&#26500;&#65292;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#34920;&#36798;&#36825;&#20123;&#23376;&#32676;&#19981;&#21464;&#30340;&#20989;&#25968;&#12290;&#26550;&#26500;&#30340;&#32467;&#26500;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#21644;&#26799;&#24230;&#19979;&#38477;&#20998;&#21035;&#39640;&#25928;&#20248;&#21270;&#32447;&#24615;&#21644;&#24352;&#37327;&#20540;&#20989;&#25968;&#65292;&#24182;&#25512;&#26029;&#20986;&#26368;&#32456;&#23398;&#20064;&#21040;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26550;&#26500;&#20013;&#24352;&#37327;&#20540;&#20989;&#25968;&#30340;&#24517;&#35201;&#24615;&#12290;&#23545;&#22270;&#20687;&#25968;&#23383;&#27714;&#21644;&#21644;&#22810;&#39033;&#24335;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning a function respecting a symmetry from among a class of symmetries. We develop a unified framework that enables symmetry discovery across a broad range of subgroups including locally symmetric, dihedral and cyclic subgroups. At the core of the framework is a novel architecture composed of linear and tensor-valued functions that expresses functions invariant to these subgroups in a principled manner. The structure of the architecture enables us to leverage multi-armed bandit algorithms and gradient descent to efficiently optimize over the linear and the tensor-valued functions, respectively, and to infer the symmetry that is ultimately learnt. We also discuss the necessity of the tensor-valued functions in the architecture. Experiments on image-digit sum and polynomial regression tasks demonstrate the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#30340;&#20301;&#23485;&#21387;&#32553;&#33267;4&#20301;&#65292;&#23454;&#29616;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#30340;&#35814;&#32454;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#22359;&#29366;&#37327;&#21270;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#36817;&#20284;&#22797;&#26434;&#30340;&#24322;&#24120;&#20540;&#27169;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#36739;&#23567;&#30340;&#22359;&#22823;&#23567;&#24182;&#21516;&#26102;&#21033;&#29992;&#34892;&#19978;&#21644;&#21015;&#19978;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#22909;&#30340;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#38500;&#38646;&#28857;&#30340;&#32447;&#24615;&#37327;&#21270;&#22120;&#35299;&#20915;&#20102;&#37327;&#21270;&#31532;&#20108;&#38454;&#30697;&#26102;&#30340;&#38646;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;4&#20301;&#20248;&#21270;&#22120;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01507</link><description>&lt;p&gt;
&#20869;&#23384;&#39640;&#25928;&#30340;&#20855;&#26377;4&#20301;&#29366;&#24577;&#30340;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Memory Efficient Optimizers with 4-bit States. (arXiv:2309.01507v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#30340;&#20301;&#23485;&#21387;&#32553;&#33267;4&#20301;&#65292;&#23454;&#29616;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#30340;&#35814;&#32454;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#22359;&#29366;&#37327;&#21270;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#36817;&#20284;&#22797;&#26434;&#30340;&#24322;&#24120;&#20540;&#27169;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#36739;&#23567;&#30340;&#22359;&#22823;&#23567;&#24182;&#21516;&#26102;&#21033;&#29992;&#34892;&#19978;&#21644;&#21015;&#19978;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#22909;&#30340;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#38500;&#38646;&#28857;&#30340;&#32447;&#24615;&#37327;&#21270;&#22120;&#35299;&#20915;&#20102;&#37327;&#21270;&#31532;&#20108;&#38454;&#30697;&#26102;&#30340;&#38646;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;4&#20301;&#20248;&#21270;&#22120;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#22120;&#29366;&#24577;&#26159;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#20027;&#35201;&#20869;&#23384;&#28040;&#32791;&#26469;&#28304;&#65292;&#38480;&#21046;&#20102;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#20869;&#21487;&#35757;&#32451;&#30340;&#26368;&#22823;&#27169;&#22411;&#12290;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#20174;32&#20301;&#28014;&#28857;&#25968;&#21387;&#32553;&#21040;&#26356;&#20302;&#30340;&#20301;&#23485;&#26377;&#26395;&#20943;&#23567;&#35757;&#32451;&#20869;&#23384;&#21344;&#29992;&#65292;&#32780;&#24403;&#21069;&#26368;&#20302;&#21487;&#36798;&#21040;&#30340;&#20301;&#23485;&#20026;8&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#30340;&#32463;&#39564;&#20998;&#26512;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#20301;&#23485;&#38477;&#33267;4&#20301;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#30697;&#20855;&#26377;&#22797;&#26434;&#30340;&#24322;&#24120;&#20540;&#27169;&#24335;&#65292;&#26080;&#27861;&#36890;&#36807;&#24403;&#21069;&#30340;&#22359;&#29366;&#37327;&#21270;&#26041;&#27861;&#20934;&#30830;&#36817;&#20284;&#12290;&#25105;&#20204;&#20351;&#29992;&#36739;&#23567;&#30340;&#22359;&#22823;&#23567;&#65292;&#24182;&#25552;&#20986;&#21516;&#26102;&#21033;&#29992;&#34892;&#19978;&#21644;&#21015;&#19978;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#22909;&#30340;&#37327;&#21270;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#37327;&#21270;&#31532;&#20108;&#38454;&#30697;&#26102;&#30340;&#38646;&#28857;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25490;&#38500;&#38646;&#28857;&#30340;&#32447;&#24615;&#37327;&#21270;&#22120;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;4&#20301;&#20248;&#21270;&#22120;&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#22312;&#20869;&#30340;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizer is evaluated on a wide variety of benchmarks including natural language understanding, machine translat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; ROBAI &#30340;&#31639;&#27861;&#65292;&#26088;&#22312;&#24555;&#36895;&#35782;&#21035;&#24182;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#36830;&#32493;&#22238;&#21512;&#20013;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#35813;&#31639;&#27861;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#21644;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#35201;&#27714;&#19979;&#22343;&#23454;&#29616;&#20102;&#28176;&#36827;&#26368;&#20248;&#36951;&#25022;&#65292;&#24182;&#19988;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#19979;&#20165;&#38656; $\mathcal{O}(\log T)$ &#22238;&#21512;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#22312;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#19979;&#20165;&#38656; $\mathcal{O}(\log^2 T)$ &#22238;&#21512;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#33218;&#12290;</title><link>http://arxiv.org/abs/2309.00591</link><description>&lt;p&gt;
&#24555;&#36895;&#21644;&#36951;&#25022;&#26368;&#23567;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65306;&#22522;&#26412;&#38480;&#21046;&#21644;&#20302;&#22797;&#26434;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms. (arXiv:2309.00591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; ROBAI &#30340;&#31639;&#27861;&#65292;&#26088;&#22312;&#24555;&#36895;&#35782;&#21035;&#24182;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#36830;&#32493;&#22238;&#21512;&#20013;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#35813;&#31639;&#27861;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#21644;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#35201;&#27714;&#19979;&#22343;&#23454;&#29616;&#20102;&#28176;&#36827;&#26368;&#20248;&#36951;&#25022;&#65292;&#24182;&#19988;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#19979;&#20165;&#38656; $\mathcal{O}(\log T)$ &#22238;&#21512;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#22312;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#19979;&#20165;&#38656; $\mathcal{O}(\log^2 T)$ &#22238;&#21512;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#33218;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#21452;&#37325;&#30446;&#26631;&#30340;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;(MAB)&#38382;&#39064;&#65306;(i) &#24555;&#36895;&#35782;&#21035;&#24182;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#20197;&#21450;(ii) &#22312;&#19968;&#31995;&#21015;T&#20010;&#36830;&#32493;&#22238;&#21512;&#20013;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#23613;&#31649;&#27599;&#20010;&#30446;&#26631;&#37117;&#24050;&#32463;&#24471;&#21040;&#20102;&#29420;&#31435;&#30340;&#28145;&#20837;&#30740;&#31350;&#65292;&#21363;(i)&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#21644;(ii)&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#20294;&#26159;&#21516;&#26102;&#23454;&#29616;&#36825;&#20004;&#20010;&#30446;&#26631;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#23613;&#31649;&#23427;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#8221;(ROBAI)&#65292;&#26088;&#22312;&#23454;&#29616;&#36825;&#20004;&#20010;&#21452;&#37325;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#20855;&#26377;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#21644;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#35201;&#27714;&#30340;ROBAI&#65292;&#25105;&#20204;&#20998;&#21035;&#25552;&#20986;&#20102;$\mathsf{EOCP}$&#31639;&#27861;&#21450;&#20854;&#21464;&#20307;&#65292;&#19981;&#20165;&#22312;&#39640;&#26031;&#32769;&#34382;&#26426;&#21644;&#19968;&#33324;&#32769;&#34382;&#26426;&#20013;&#36798;&#21040;&#20102;&#28176;&#36827;&#26368;&#20248;&#36951;&#25022;&#65292;&#32780;&#19988;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#19979;&#65292;&#22312;$\mathcal{O}(\log T)$&#22238;&#21512;&#20869;&#36873;&#25321;&#20102;&#26368;&#20339;&#33218;&#65292;&#22312;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#19979;&#65292;&#36873;&#25321;&#20102;&#26368;&#20339;&#33218;&#22312;$\mathcal{O}(\log^2 T)$&#22238;&#21512;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a stochastic multi-armed bandit (MAB) problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) reward maximization throughout a sequence of $T$ consecutive rounds. Though each objective has been individually well-studied, i.e., best arm identification for (i) and regret minimization for (ii), the simultaneous realization of both objectives remains an open problem, despite its practical importance. This paper introduces \emph{Regret Optimal Best Arm Identification} (ROBAI) which aims to achieve these dual objectives. To solve ROBAI with both pre-determined stopping time and adaptive stopping time requirements, we present the $\mathsf{EOCP}$ algorithm and its variants respectively, which not only achieve asymptotic optimal regret in both Gaussian and general bandits, but also commit to the optimal arm in $\mathcal{O}(\log T)$ rounds with pre-determined stopping time and $\mathcal{O}(\log^2 T)$ rounds with adaptive stopping ti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#30340;&#32467;&#26500;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#37325;&#39640;&#26031;&#29275;&#39039;&#26041;&#21521;&#39044;&#35328;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20598;&#24418;&#24335;&#35745;&#31639;&#36825;&#20123;&#39044;&#35328;&#65292;&#24471;&#21040;&#20102;&#26082;&#20855;&#26377;&#35745;&#31639;&#22909;&#22788;&#21448;&#20855;&#26377;&#26032;&#35265;&#35299;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#39044;&#35328;&#20316;&#20026;&#38543;&#26426;&#26799;&#24230;&#30340;&#19979;&#38477;&#26041;&#21521;&#30340;&#20248;&#21183;&#65292;&#24182;&#30740;&#31350;&#20102;&#35745;&#31639;&#19978;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.08886</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21452;&#39640;&#26031;&#29275;&#39039;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Dual Gauss-Newton Directions for Deep Learning. (arXiv:2308.08886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#30340;&#32467;&#26500;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#37325;&#39640;&#26031;&#29275;&#39039;&#26041;&#21521;&#39044;&#35328;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20598;&#24418;&#24335;&#35745;&#31639;&#36825;&#20123;&#39044;&#35328;&#65292;&#24471;&#21040;&#20102;&#26082;&#20855;&#26377;&#35745;&#31639;&#22909;&#22788;&#21448;&#20855;&#26377;&#26032;&#35265;&#35299;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#39044;&#35328;&#20316;&#20026;&#38543;&#26426;&#26799;&#24230;&#30340;&#19979;&#38477;&#26041;&#21521;&#30340;&#20248;&#21183;&#65292;&#24182;&#30740;&#31350;&#20102;&#35745;&#31639;&#19978;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#39640;&#26031;&#29275;&#39039;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#30340;&#32467;&#26500;&#65292;&#21363;&#30001;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#32447;&#24615;&#32593;&#32476;&#32452;&#25104;&#65292;&#20197;&#23548;&#20986;&#27604;&#38543;&#26426;&#26799;&#24230;&#26356;&#22909;&#30340;&#26041;&#21521;&#39044;&#35328;&#65292;&#22522;&#20110;&#37096;&#20998;&#32447;&#24615;&#21270;&#30340;&#24605;&#24819;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23427;&#20204;&#30340;&#23545;&#20598;&#24418;&#24335;&#26469;&#35745;&#31639;&#36825;&#26679;&#30340;&#26041;&#21521;&#39044;&#35328;&#65292;&#20174;&#32780;&#33719;&#24471;&#35745;&#31639;&#19978;&#30340;&#22909;&#22788;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#39044;&#35328;&#23450;&#20041;&#20102;&#21487;&#20197;&#29992;&#20316;&#29616;&#26377;&#20248;&#21270;&#31639;&#27861;&#20013;&#38543;&#26426;&#26799;&#24230;&#30340;&#26367;&#20195;&#21697;&#30340;&#19979;&#38477;&#26041;&#21521;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;&#23545;&#20598;&#24418;&#24335;&#30340;&#20248;&#21183;&#20197;&#21450;&#28041;&#21450;&#35745;&#31639;&#36825;&#20123;&#39044;&#35328;&#30340;&#35745;&#31639;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by Gauss-Newton-like methods, we study the benefit of leveraging the structure of deep learning objectives, namely, the composition of a convex loss function and of a nonlinear network, in order to derive better direction oracles than stochastic gradients, based on the idea of partial linearization. In a departure from previous works, we propose to compute such direction oracles via their dual formulation, leading to both computational benefits and new insights. We demonstrate that the resulting oracles define descent directions that can be used as a drop-in replacement for stochastic gradients, in existing optimization algorithms. We empirically study the advantage of using the dual formulation as well as the computational trade-offs involved in the computation of such oracles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;pFedHR&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#23454;&#29616;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;pFedHR&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#20351;&#29992;&#19981;&#20860;&#23481;&#25968;&#25454;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.08643</link><description>&lt;p&gt;
&#36890;&#36807;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#23454;&#29616;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Federated Learning via Heterogeneous Model Reassembly. (arXiv:2308.08643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;pFedHR&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#23454;&#29616;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;pFedHR&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#20351;&#29992;&#19981;&#20860;&#23481;&#25968;&#25454;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#27169;&#22411;&#24322;&#26500;&#30340;&#23454;&#38469;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20855;&#26377;&#19981;&#21516;&#32593;&#32476;&#32467;&#26500;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;pFedHR&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#23454;&#29616;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24322;&#26500;&#27169;&#22411;&#20010;&#24615;&#21270;&#38382;&#39064;&#35270;&#20026;&#26381;&#21153;&#22120;&#31471;&#30340;&#27169;&#22411;&#21305;&#37197;&#20248;&#21270;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;pFedHR&#33021;&#22815;&#33258;&#21160;&#19988;&#21160;&#24577;&#22320;&#29983;&#25104;&#20855;&#26377;&#26368;&#23567;&#20154;&#24037;&#24178;&#39044;&#30340;&#20449;&#24687;&#20016;&#23500;&#19988;&#22810;&#26679;&#21270;&#30340;&#20010;&#24615;&#21270;&#20505;&#36873;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#25216;&#26415;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#36731;&#20102;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#20844;&#20849;&#25968;&#25454;&#19982;&#23458;&#25143;&#31471;&#25968;&#25454;&#36896;&#25104;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;IID&#21644;&#38750;IID&#35774;&#32622;&#19979;&#65292;pFedHR&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;pFedHR&#26377;&#25928;&#38477;&#20302;&#20102;&#20351;&#29992;&#19981;&#20860;&#23481;&#25968;&#25454;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures. To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning. In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side. Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention. Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent. Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings. Additionally, pFedHR effectively reduces the adverse impact of usi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#23398;&#20064;&#38431;&#21015;&#20013;&#30340;&#25104;&#26412; (CLQ)&#65292;&#29992;&#20110;&#37327;&#21270;&#30001;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#26102;&#38388;&#24179;&#22343;&#38431;&#21015;&#38271;&#24230;&#26368;&#22823;&#22686;&#21152;&#37327;&#12290;&#35813;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#23398;&#20064;&#38431;&#21015;&#31995;&#32479;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#19981;&#23616;&#38480;&#20110;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07817</link><description>&lt;p&gt;
&#37327;&#21270;&#38431;&#21015;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Cost of Learning in Queueing Systems. (arXiv:2308.07817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#23398;&#20064;&#38431;&#21015;&#20013;&#30340;&#25104;&#26412; (CLQ)&#65292;&#29992;&#20110;&#37327;&#21270;&#30001;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#26102;&#38388;&#24179;&#22343;&#38431;&#21015;&#38271;&#24230;&#26368;&#22823;&#22686;&#21152;&#37327;&#12290;&#35813;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#23398;&#20064;&#38431;&#21015;&#31995;&#32479;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#19981;&#23616;&#38480;&#20110;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38431;&#21015;&#31995;&#32479;&#26159;&#24191;&#27867;&#24212;&#29992;&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#36890;&#20449;&#32593;&#32476;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#26381;&#21153;&#31995;&#32479;&#31561;&#31561;&#12290;&#34429;&#28982;&#23427;&#20204;&#30340;&#26368;&#20248;&#25511;&#21046;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#20551;&#35774;&#31995;&#32479;&#21442;&#25968;&#30340;&#23436;&#32654;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#24456;&#24120;&#35265;&#65292;&#22240;&#27492;&#26368;&#36817;&#19968;&#31995;&#21015;&#20851;&#20110;&#38431;&#21015;&#31995;&#32479;&#30340;&#23398;&#20064;&#30340;&#30740;&#31350;&#20135;&#29983;&#20102;&#12290;&#36825;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#20027;&#35201;&#20851;&#27880;&#25152;&#25552;&#31639;&#27861;&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#28176;&#36817;&#24230;&#37327;&#65292;&#21363;&#30528;&#30524;&#20110;&#21518;&#26399;&#24615;&#33021;&#30340;&#24230;&#37327;&#65292;&#26080;&#27861;&#25429;&#25417;&#23398;&#20064;&#38431;&#21015;&#31995;&#32479;&#20013;&#22266;&#26377;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#36890;&#24120;&#20986;&#29616;&#22312;&#26089;&#26399;&#38454;&#27573;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#38431;&#21015;&#20013;&#30340;&#25104;&#26412; (CLQ)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#34913;&#37327;&#30001;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#26102;&#38388;&#24179;&#22343;&#38431;&#21015;&#38271;&#24230;&#30340;&#26368;&#22823;&#22686;&#21152;&#37327;&#12290;&#25105;&#20204;&#23545;&#21333;&#38431;&#21015;&#22810;&#26381;&#21153;&#22120;&#31995;&#32479;&#30340;CLQ&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Queueing systems are widely applicable stochastic models with use cases in communication networks, healthcare, service systems, etc. Although their optimal control has been extensively studied, most existing approaches assume perfect knowledge of system parameters. Of course, this assumption rarely holds in practice where there is parameter uncertainty, thus motivating a recent line of work on bandit learning for queueing systems. This nascent stream of research focuses on the asymptotic performance of the proposed algorithms.  In this paper, we argue that an asymptotic metric, which focuses on late-stage performance, is insufficient to capture the intrinsic statistical complexity of learning in queueing systems which typically occurs in the early stage. Instead, we propose the Cost of Learning in Queueing (CLQ), a new metric that quantifies the maximum increase in time-averaged queue length caused by parameter uncertainty. We characterize the CLQ of a single-queue multi-server system,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#32467;&#26524;&#26159;&#30830;&#23450;&#24615;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#20195;&#29702;&#21464;&#37327;&#30340;&#20869;&#26680;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#22238;&#24402;&#21644;&#26368;&#22823;&#30697;&#32422;&#26463;&#30340;&#26041;&#27861;&#21487;&#20197;&#19968;&#33268;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#25104;&#21151;&#24674;&#22797;&#20102;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2308.04585</link><description>&lt;p&gt;
&#20915;&#23450;&#24615;&#28151;&#28102;&#19979;&#30340;&#20869;&#26680;&#21333;&#19968;&#20195;&#29702;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Kernel Single Proxy Control for Deterministic Confounding. (arXiv:2308.04585v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#32467;&#26524;&#26159;&#30830;&#23450;&#24615;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#20195;&#29702;&#21464;&#37327;&#30340;&#20869;&#26680;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#22238;&#24402;&#21644;&#26368;&#22823;&#30697;&#32422;&#26463;&#30340;&#26041;&#27861;&#21487;&#20197;&#19968;&#33268;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#25104;&#21151;&#24674;&#22797;&#20102;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#35266;&#27979;&#21040;&#19982;&#28151;&#28102;&#22240;&#32032;&#30456;&#20851;&#30340;&#20195;&#29702;&#21464;&#37327;&#12290;&#23613;&#31649;&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#65288;PCL&#65289;&#20351;&#29992;&#20004;&#20010;&#20195;&#29702;&#21464;&#37327;&#26469;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#32467;&#26524;&#26159;&#30830;&#23450;&#24615;&#29983;&#25104;&#30340;&#65292;&#21017;&#20351;&#29992;&#21333;&#20010;&#20195;&#29702;&#21464;&#37327;&#23601;&#36275;&#20197;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#65292;&#24182;&#27010;&#25324;&#20102;&#25511;&#21046;&#32467;&#26524;&#26657;&#20934;&#27861;&#65288;COCA&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20869;&#26680;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#20004;&#38454;&#27573;&#22238;&#24402;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#30697;&#32422;&#26463;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#19968;&#33268;&#22320;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#23454;&#39564;&#25104;&#21151;&#22320;&#24674;&#22797;&#20102;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of causal effect estimation with an unobserved confounder, where we observe a proxy variable that is associated with the confounder. Although Proxy Causal Learning (PCL) uses two proxy variables to recover the true causal effect, we show that a single proxy variable is sufficient for causal estimation if the outcome is generated deterministically, generalizing Control Outcome Calibration Approach (COCA). We propose two kernel-based methods for this setting: the first based on the two-stage regression approach, and the second based on a maximum moment restriction approach. We prove that both approaches can consistently estimate the causal effect, and we empirically demonstrate that we can successfully recover the causal effect on a synthetic dataset.
&lt;/p&gt;</description></item><item><title>Thinker&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#19990;&#30028;&#27169;&#22411;&#21644;&#27169;&#22411;&#20132;&#20114;&#21160;&#20316;&#20351;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23454;&#29616;&#33258;&#20027;&#35268;&#21010;&#65292;&#28040;&#38500;&#20102;&#25163;&#24037;&#35774;&#35745;&#35268;&#21010;&#31639;&#27861;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;Sokoban&#28216;&#25103;&#21644;Atari 2600&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14993</link><description>&lt;p&gt;
Thinker: &#23398;&#20064;&#35268;&#21010;&#21644;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Thinker: Learning to Plan and Act. (arXiv:2307.14993v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14993
&lt;/p&gt;
&lt;p&gt;
Thinker&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#19990;&#30028;&#27169;&#22411;&#21644;&#27169;&#22411;&#20132;&#20114;&#21160;&#20316;&#20351;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23454;&#29616;&#33258;&#20027;&#35268;&#21010;&#65292;&#28040;&#38500;&#20102;&#25163;&#24037;&#35774;&#35745;&#35268;&#21010;&#31639;&#27861;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;Sokoban&#28216;&#25103;&#21644;Atari 2600&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Thinker&#31639;&#27861;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#22320;&#19982;&#23398;&#20064;&#30340;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24182;&#21033;&#29992;&#20854;&#12290; Thinker&#31639;&#27861;&#36890;&#36807;&#32473;&#29615;&#22659;&#28155;&#21152;&#19990;&#30028;&#27169;&#22411;&#26469;&#25913;&#21464;&#29615;&#22659;&#65292;&#24182;&#24341;&#20837;&#20102;&#29992;&#20110;&#19982;&#19990;&#30028;&#27169;&#22411;&#20132;&#20114;&#30340;&#26032;&#21160;&#20316;&#12290;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#21160;&#20316;&#20351;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#22312;&#36873;&#25321;&#26368;&#32456;&#30340;&#29615;&#22659;&#21160;&#20316;&#20043;&#21069;&#21521;&#19990;&#30028;&#27169;&#22411;&#25552;&#20986;&#22791;&#36873;&#35745;&#21010;&#26469;&#36827;&#34892;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#20195;&#29702;&#33258;&#20027;&#23398;&#20064;&#22914;&#20309;&#36827;&#34892;&#35268;&#21010;&#26469;&#28040;&#38500;&#20102;&#25163;&#24037;&#35774;&#35745;&#30340;&#35268;&#21010;&#31639;&#27861;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#20801;&#35768;&#23545;&#20195;&#29702;&#30340;&#35745;&#21010;&#36827;&#34892;&#26131;&#20110;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;Sokoban&#28216;&#25103;&#21644;Atari 2600&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;Thinker&#31639;&#27861;&#20998;&#21035;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;Thinker&#31639;&#27861;&#35757;&#32451;&#30340;&#20195;&#29702;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#24050;&#32463;&#23398;&#21040;&#20102;&#20248;&#31168;&#30340;&#35268;&#21010;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have lear
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21407;&#20301;&#36827;&#34892;&#36731;&#37327;&#32423;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#21644;&#20302;&#38656;&#27714;&#30340;&#35745;&#31639;&#36164;&#28304;&#20419;&#36827;&#20102;&#20809;&#23398;&#35745;&#31639;&#25216;&#26415;&#20174;&#23454;&#39564;&#23460;&#30740;&#31350;&#21521;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2307.11957</link><description>&lt;p&gt;
&#36890;&#36807;&#21407;&#20301;&#26080;&#27169;&#22411;&#20248;&#21270;&#23454;&#29616;&#39640;&#24615;&#33021;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
High-performance real-world optical computing trained by in situ model-free optimization. (arXiv:2307.11957v1 [physics.optics] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21407;&#20301;&#36827;&#34892;&#36731;&#37327;&#32423;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#21644;&#20302;&#38656;&#27714;&#30340;&#35745;&#31639;&#36164;&#28304;&#20419;&#36827;&#20102;&#20809;&#23398;&#35745;&#31639;&#25216;&#26415;&#20174;&#23454;&#39564;&#23460;&#30740;&#31350;&#21521;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#39640;&#36895;&#21644;&#20302;&#33021;&#32791;&#30340;&#25968;&#25454;&#22788;&#29702;&#65292;&#20294;&#22312;&#35745;&#31639;&#23494;&#38598;&#30340;&#35757;&#32451;&#21644;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#36716;&#25442;&#20013;&#23384;&#22312;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#26799;&#24230;&#20272;&#35745;&#31639;&#27861;&#30340;&#36731;&#37327;&#32423;&#21407;&#20301;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26080;&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#31995;&#32479;&#35270;&#20026;&#40657;&#30418;&#23376;&#65292;&#30452;&#25509;&#23558;&#25439;&#22833;&#21453;&#21521;&#20256;&#25773;&#21040;&#20809;&#23398;&#26435;&#37325;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23545;&#35745;&#31639;&#23494;&#38598;&#21644;&#26377;&#20559;&#35265;&#30340;&#31995;&#32479;&#27169;&#25311;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#22312;&#21333;&#23618;&#34893;&#23556;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#22312;MNIST&#21644;FMNIST&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#26080;&#22270;&#29255;&#21644;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#65292;&#32467;&#21512;&#20854;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#20302;&#38656;&#27714;&#65292;&#21152;&#36895;&#20102;&#20809;&#23398;&#35745;&#31639;&#20174;&#23454;&#39564;&#23460;&#28436;&#31034;&#21040;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Android in the Wild (AITW)&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35774;&#22791;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20154;&#31867;&#31034;&#33539;&#30340;&#35774;&#22791;&#20132;&#20114;&#12289;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#31181;Android&#29256;&#26412;&#21644;&#35774;&#22791;&#31867;&#22411;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#20174;&#35270;&#35273;&#22806;&#35266;&#20013;&#25512;&#26029;&#29992;&#25143;&#30028;&#38754;&#20013;&#21487;&#29992;&#30340;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.10088</link><description>&lt;p&gt;
&#22312;&#37326;&#22806;&#30340;Android&#65306;&#29992;&#20110;Android&#35774;&#22791;&#25511;&#21046;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Android in the Wild: A Large-Scale Dataset for Android Device Control. (arXiv:2307.10088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10088
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Android in the Wild (AITW)&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35774;&#22791;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20154;&#31867;&#31034;&#33539;&#30340;&#35774;&#22791;&#20132;&#20114;&#12289;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#31181;Android&#29256;&#26412;&#21644;&#35774;&#22791;&#31867;&#22411;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#20174;&#35270;&#35273;&#22806;&#35266;&#20013;&#25512;&#26029;&#29992;&#25143;&#30028;&#38754;&#20013;&#21487;&#29992;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33021;&#22815;&#35299;&#37322;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#24182;&#30452;&#25509;&#25511;&#21046;&#25968;&#23383;&#35774;&#22791;&#29992;&#25143;&#30028;&#38754;&#25191;&#34892;&#30340;&#35774;&#22791;&#25511;&#21046;&#31995;&#32479;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35774;&#22791;&#25511;&#21046;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#65292;Android in the Wild (AITW)&#65292;&#35813;&#25968;&#25454;&#38598;&#27604;&#24403;&#21069;&#25968;&#25454;&#38598;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#35774;&#22791;&#20132;&#20114;&#30340;&#20154;&#31867;&#31034;&#33539;&#65292;&#21253;&#25324;&#23631;&#24149;&#21644;&#25805;&#20316;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12290;&#23427;&#21253;&#25324;715k&#20010;&#21095;&#38598;&#65292;&#28085;&#30422;30k&#20010;&#19981;&#21516;&#30340;&#25351;&#20196;&#65292;&#22235;&#20010;Android&#29256;&#26412;&#65288;v10-13&#65289;&#65292;&#20197;&#21450;&#20843;&#31181;&#19981;&#21516;&#30340;&#35774;&#22791;&#31867;&#22411;&#65288;&#20174;Pixel 2 XL&#21040;Pixel 6&#65289;&#21644;&#19981;&#21516;&#30340;&#23631;&#24149;&#20998;&#36776;&#29575;&#12290;&#23427;&#21253;&#21547;&#38656;&#35201;&#35821;&#35328;&#21644;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#35821;&#20041;&#29702;&#35299;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65306;&#24517;&#39035;&#20174;&#23427;&#20204;&#30340;&#35270;&#35273;&#22806;&#35266;&#20013;&#25512;&#26029;&#20986;&#29992;&#25143;&#30028;&#38754;&#20013;&#21487;&#29992;&#30340;&#25805;&#20316;&#12290;&#32780;&#19988;&#65292;&#34892;&#21160;&#31354;&#38388;&#19981;&#20877;&#26159;&#31616;&#21333;&#30340;&#22522;&#20110;&#29992;&#25143;&#30028;&#38754;&#20803;&#32032;&#30340;&#34892;&#21160;&#65292;&#32780;&#26159;&#21253;&#21547;&#31934;&#30830;&#30340;&#25163;&#21183;&#65288;&#20363;&#22914;&#65292;&#27700;&#24179;&#28378;&#21160;&#65289;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly controlling its user interface. We present a dataset for device-control research, Android in the Wild (AITW), which is orders of magnitude larger than current datasets. The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10-13),and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It contains multi-step tasks that require semantic understanding of language and visual context. This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance. And, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., horizontal scrolls 
&lt;/p&gt;</description></item><item><title>HYTREL&#26159;&#19968;&#31181;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#22270;&#26469;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#21644;&#20854;&#20182;&#19977;&#20010;&#32467;&#26500;&#23646;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;HYTREL&#22312;&#22235;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#26368;&#23569;&#30340;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.08623</link><description>&lt;p&gt;
HYTREL: &#22522;&#20110;&#36229;&#22270;&#30340;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HYTREL: Hypergraph-enhanced Tabular Data Representation Learning. (arXiv:2307.08623v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08623
&lt;/p&gt;
&lt;p&gt;
HYTREL&#26159;&#19968;&#31181;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#22270;&#26469;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#21644;&#20854;&#20182;&#19977;&#20010;&#32467;&#26500;&#23646;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;HYTREL&#22312;&#22235;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#26368;&#23569;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#39044;&#35757;&#32451;&#22312;&#22823;&#37327;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#35821;&#35328;&#27169;&#22411;&#37117;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27169;&#22411;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#34920;&#26684;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#34892;/&#21015;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20998;&#23618;&#32467;&#26500;&#31561;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HYTREL&#30340;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#22270;&#26469;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#21644;&#20854;&#20182;&#19977;&#20010;&#32467;&#26500;&#23646;&#24615;&#8212;&#8212;&#20854;&#20013;&#65292;&#34920;&#26684;&#21333;&#20803;&#26684;&#26500;&#25104;&#33410;&#28857;&#65292;&#24182;&#19988;&#22312;&#27599;&#34892;&#12289;&#27599;&#21015;&#21644;&#25972;&#20010;&#34920;&#26684;&#20013;&#20849;&#21516;&#20986;&#29616;&#30340;&#21333;&#20803;&#26684;&#34987;&#29992;&#26469;&#24418;&#25104;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#36229;&#36793;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HYTREL&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#26159;&#26368;&#22823;&#19981;&#21464;&#30340;&#65292;&#21363;&#65292;&#20004;&#20010;&#34920;&#26684;&#36890;&#36807;HYTREL&#33719;&#24471;&#30340;&#34920;&#31034;&#30456;&#21516;&#65292;&#24403;&#19988;&#20165;&#24403;&#36825;&#20004;&#20010;&#34920;&#26684;&#22312;&#25490;&#21015;&#19978;&#26159;&#30456;&#21516;&#30340;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;HYTREL&#22312;&#22235;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#26368;&#23569;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models pretrained on large collections of tabular data have demonstrated their effectiveness in several downstream tasks. However, many of these models do not take into account the row/column permutation invariances, hierarchical structure, etc. that exist in tabular data. To alleviate these limitations, we propose HYTREL, a tabular language model, that captures the permutation invariances and three more structural properties of tabular data by using hypergraphs - where the table cells make up the nodes and the cells occurring jointly together in each row, column, and the entire table are used to form three different types of hyperedges. We show that HYTREL is maximally invariant under certain conditions for tabular data, i.e., two tables obtain the same representations via HYTREL iff the two tables are identical up to permutations. Our empirical results demonstrate that HYTREL consistently outperforms other competitive baselines on four downstream tasks with minimal pretraini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20056;&#27861;&#24179;&#28369;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#35777;&#26126;&#27169;&#22411;&#30340;Lipschitz&#24615;&#36136;&#65292;&#20445;&#35777;&#20102;&#20854;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#38750;&#24179;&#20961;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.05902</link><description>&lt;p&gt;
&#24102;&#26377;&#20056;&#27861;&#24179;&#28369;&#30340;&#29305;&#24449;&#24402;&#22240;&#31283;&#23450;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Stability Guarantees for Feature Attributions with Multiplicative Smoothing. (arXiv:2307.05902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20056;&#27861;&#24179;&#28369;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#35777;&#26126;&#27169;&#22411;&#30340;Lipschitz&#24615;&#36136;&#65292;&#20445;&#35777;&#20102;&#20854;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#38750;&#24179;&#20961;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#24448;&#24448;&#19981;&#33021;&#25552;&#20379;&#20219;&#20309;&#24418;&#24335;&#30340;&#20445;&#35777;&#65292;&#20063;&#21487;&#33021;&#19981;&#21453;&#26144;&#24213;&#23618;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#31283;&#23450;&#24615;&#20316;&#20026;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#19968;&#20010;&#23646;&#24615;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#26524;&#27169;&#22411;&#22312;&#29305;&#24449;&#23631;&#34109;&#26041;&#38754;&#20855;&#26377;&#36275;&#22815;&#30340;Lipschitz&#24615;&#36136;&#65292;&#21017;&#21487;&#20197;&#20445;&#35777;&#25918;&#26494;&#21464;&#20307;&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#20056;&#27861;&#24179;&#28369;&#65288;MuS&#65289;&#30340;&#24179;&#28369;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MuS&#20811;&#26381;&#20102;&#26631;&#20934;&#24179;&#28369;&#25216;&#26415;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#20998;&#31867;&#22120;&#21644;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;&#22914;LIME&#21644;SHAP&#65289;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;MuS&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;MuS&#36171;&#20104;&#20102;&#29305;&#24449;&#24402;&#22240;&#20197;&#38750;&#24179;&#20961;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanation methods for machine learning models tend to not provide any formal guarantees and may not reflect the underlying decision-making process. In this work, we analyze stability as a property for reliable feature attribution methods. We prove that relaxed variants of stability are guaranteed if the model is sufficiently Lipschitz with respect to the masking of features. To achieve such a model, we develop a smoothing method called Multiplicative Smoothing (MuS). We show that MuS overcomes theoretical limitations of standard smoothing techniques and can be integrated with any classifier and feature attribution method. We evaluate MuS on vision and language models with a variety of feature attribution methods, such as LIME and SHAP, and demonstrate that MuS endows feature attributions with non-trivial stability guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#65292;&#26377;&#25928;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#20013;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#24050;&#26377;&#25968;&#25454;&#38598;DebateSum&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20363;&#23376;&#21644;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#65292;&#21019;&#24314;&#21644;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#36866;&#21512;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.04090</link><description>&lt;p&gt;
DebateKG: &#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs. (arXiv:2307.04090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#65292;&#26377;&#25928;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#20013;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#24050;&#26377;&#25968;&#25454;&#38598;DebateSum&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20363;&#23376;&#21644;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#65292;&#21019;&#24314;&#21644;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#36866;&#21512;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30456;&#20851;&#24037;&#20316;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#22312;&#35299;&#20915;&#31454;&#36187;&#36777;&#35770;&#20013;&#30340;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#24615;&#12290;&#31454;&#36187;&#36777;&#35770;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#36777;&#25163;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#26500;&#24314;&#26377;&#25928;&#30340;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;DebateSum&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#36825;&#31181;&#28508;&#21147;&#65292;&#35813;&#25968;&#25454;&#38598;&#38024;&#23545;&#30340;&#26159;&#19968;&#31181;&#21517;&#20026;&#25919;&#31574;&#36777;&#35770;&#30340;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#31867;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#21521;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;53180&#20010;&#26032;&#30340;&#20363;&#23376;&#65292;&#24182;&#20026;&#27599;&#20010;&#20363;&#23376;&#25552;&#20379;&#36827;&#19968;&#27493;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;DebateSum&#12290;&#25105;&#20204;&#21033;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#20135;&#29983;&#24182;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#22312;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work within the Argument Mining community has shown the applicability of Natural Language Processing systems for solving problems found within competitive debate. One of the most important tasks within competitive debate is for debaters to create high quality debate cases. We show that effective debate cases can be constructed using constrained shortest path traversals on Argumentative Semantic Knowledge Graphs. We study this potential in the context of a type of American Competitive Debate, called Policy Debate, which already has a large scale dataset targeting it called DebateSum. We significantly improve upon DebateSum by introducing 53180 new examples, as well as further useful metadata for every example, to the dataset. We leverage the txtai semantic search and knowledge graph toolchain to produce and contribute 9 semantic knowledge graphs built on this dataset. We create a unique method for evaluating which knowledge graphs are better in the context of producing policy deb
&lt;/p&gt;</description></item><item><title>[SF]$^2$M&#26159;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#38543;&#26426;&#21160;&#21147;&#23398;&#12290;&#23427;&#23558;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#29983;&#25104;&#24314;&#27169;&#35299;&#37322;&#20026;Schr\"odinger&#26725;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38745;&#24577;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#20256;&#36755;&#26469;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23398;&#20064;&#32454;&#32990;&#21160;&#21147;&#23398;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.03672</link><description>&lt;p&gt;
&#36890;&#36807;&#24471;&#20998;&#21644;&#27969;&#21305;&#37197;&#23454;&#29616;&#26080;&#38656;&#27169;&#25311;&#30340;Schr\"odinger&#26725;
&lt;/p&gt;
&lt;p&gt;
Simulation-free Schr\"odinger bridges via score and flow matching. (arXiv:2307.03672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03672
&lt;/p&gt;
&lt;p&gt;
[SF]$^2$M&#26159;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#38543;&#26426;&#21160;&#21147;&#23398;&#12290;&#23427;&#23558;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#29983;&#25104;&#24314;&#27169;&#35299;&#37322;&#20026;Schr\"odinger&#26725;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38745;&#24577;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#20256;&#36755;&#26469;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23398;&#20064;&#32454;&#32990;&#21160;&#21147;&#23398;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#38656;&#27169;&#25311;&#30340;&#24471;&#20998;&#21644;&#27969;&#21305;&#37197;&#65288;[SF]$^2$M&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25512;&#26029;&#32473;&#23450;&#26469;&#33258;&#20219;&#24847;&#20998;&#24067;&#30340;&#26410;&#37197;&#23545;&#28304;&#21644;&#30446;&#26631;&#26679;&#26412;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#24191;&#20102;&#29992;&#20110;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#35757;&#32451;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#30340;&#27969;&#21305;&#37197;&#25439;&#22833;&#12290;[SF]$^2$M&#23558;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#29983;&#25104;&#24314;&#27169;&#35299;&#37322;&#20026;Schr\"odinger&#26725;&#38382;&#39064;&#12290;&#23427;&#20381;&#36182;&#20110;&#38745;&#24577;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#20256;&#36755;&#25110;&#23567;&#25209;&#37327;&#36817;&#20284;&#65292;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;Schr\"odinger&#26725;&#65292;&#32780;&#26080;&#38656;&#27169;&#25311;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#20808;&#21069;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;[SF]$^2$M&#26356;&#39640;&#25928;&#24182;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;Schr\"odinger&#26725;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;[SF]$^2$M&#24212;&#29992;&#20110;&#20174;&#24555;&#29031;&#25968;&#25454;&#20013;&#23398;&#20064;&#32454;&#32990;&#21160;&#21147;&#23398;&#30340;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;[SF]$^2$M&#26159;&#39318;&#20010;&#33021;&#22815;&#20934;&#30830;&#24314;&#27169;&#39640;&#32500;&#32454;&#32990;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present simulation-free score and flow matching ([SF]$^2$M), a simulation-free objective for inferring stochastic dynamics given unpaired source and target samples drawn from arbitrary distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic generative modeling as a Schr\"odinger bridge (SB) problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]$^2$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]$^2$M is the first method to accurately model cell dynamics in high dimensions and can 
&lt;/p&gt;</description></item><item><title>&#31561;&#21464;&#29699;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#36890;&#36807;&#38477;&#20302;&#23545;&#29305;&#23450;&#35757;&#32451;&#38598;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#22312;&#21435;&#22122;&#21644;&#37325;&#24314;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.03298</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#21644;&#39640;&#24615;&#33021;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#30340;&#31561;&#21464;&#29699;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Equivariant Spherical CNN for Data Efficient and High-Performance Medical Image Processing. (arXiv:2307.03298v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03298
&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#29699;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#36890;&#36807;&#38477;&#20302;&#23545;&#29305;&#23450;&#35757;&#32451;&#38598;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#22312;&#21435;&#22122;&#21644;&#37325;&#24314;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31361;&#20986;&#20102;&#31561;&#21464;&#32593;&#32476;&#20316;&#20026;&#39640;&#25928;&#21644;&#39640;&#24615;&#33021;&#36884;&#24452;&#22312;&#26029;&#23618;&#25195;&#25551;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24314;&#31435;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#23616;&#38480;&#24615;&#20043;&#19978;&#65292;CNN&#24050;&#32463;&#22312;&#21508;&#31181;&#21307;&#23398;&#24433;&#20687;&#31995;&#32479;&#30340;&#21518;&#22788;&#29702;&#20013;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;CNN&#30340;&#25928;&#29575;&#20005;&#37325;&#20381;&#36182;&#20110;&#19968;&#20010;&#19981;&#21464;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31561;&#21464;&#32593;&#32476;&#65292;&#26088;&#22312;&#20943;&#23569;CNN&#23545;&#29305;&#23450;&#35757;&#32451;&#38598;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#31561;&#21464;CNN&#22312;&#29699;&#20449;&#21495;&#19978;&#22312;&#26029;&#23618;&#25195;&#25551;&#21307;&#23398;&#25104;&#20687;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29699;&#24418;CNN&#65288;SCNN&#65289;&#22312;&#21435;&#22122;&#21644;&#37325;&#24314;&#22522;&#20934;&#38382;&#39064;&#19978;&#20855;&#26377;&#20248;&#36234;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;SCNN&#20316;&#20026;&#20256;&#32479;&#22270;&#20687;&#37325;&#24314;&#24037;&#20855;&#30340;&#34917;&#20805;&#65292;&#22686;&#24378;&#32467;&#26524;&#21516;&#26102;&#20943;&#23569;&#23545;&#35757;&#32451;&#38598;&#30340;&#20381;&#36182;&#24615;&#12290;&#22312;&#25152;&#26377;&#26696;&#20363;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;...
&lt;/p&gt;
&lt;p&gt;
This work highlights the significance of equivariant networks as efficient and high-performance approaches for tomography applications. Our study builds upon the limitations of Convolutional Neural Networks (CNNs), which have shown promise in post-processing various medical imaging systems. However, the efficiency of conventional CNNs heavily relies on an undiminished and proper training set. To tackle this issue, in this study, we introduce an equivariant network, aiming to reduce CNN's dependency on specific training sets. We evaluate the efficacy of equivariant CNNs on spherical signals for tomographic medical imaging problems. Our results demonstrate superior quality and computational efficiency of spherical CNNs (SCNNs) in denoising and reconstructing benchmark problems. Furthermore, we propose a novel approach to employ SCNNs as a complement to conventional image reconstruction tools, enhancing the outcomes while reducing reliance on the training set. Across all cases, we observe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19981;&#36830;&#32493;ReLU&#32593;&#32476;&#23454;&#29616;&#20102;&#28145;&#23618;&#21512;&#21516;&#35774;&#35745;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#20154;&#21644;&#22996;&#25176;&#20154;&#30340;&#32422;&#26463;&#21644;&#30446;&#26631;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#65292;&#25903;&#25345;&#24182;&#34892;&#25512;&#26029;&#20197;&#27714;&#35299;&#26368;&#20248;&#21512;&#21516;&#12290;</title><link>http://arxiv.org/abs/2307.02318</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#36830;&#32493;&#32593;&#32476;&#23454;&#29616;&#28145;&#23618;&#21512;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep Contract Design via Discontinuous Networks. (arXiv:2307.02318v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19981;&#36830;&#32493;ReLU&#32593;&#32476;&#23454;&#29616;&#20102;&#28145;&#23618;&#21512;&#21516;&#35774;&#35745;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#20154;&#21644;&#22996;&#25176;&#20154;&#30340;&#32422;&#26463;&#21644;&#30446;&#26631;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#65292;&#25903;&#25345;&#24182;&#34892;&#25512;&#26029;&#20197;&#27714;&#35299;&#26368;&#20248;&#21512;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#21516;&#35774;&#35745;&#28041;&#21450;&#19968;&#20010;&#22996;&#25176;&#20154;&#23545;&#30001;&#20195;&#29702;&#20154;&#30340;&#34892;&#21160;&#20135;&#29983;&#30340;&#32467;&#26524;&#25903;&#20184;&#30340;&#21512;&#21516;&#32422;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#23545;&#26368;&#20248;&#21512;&#21516;&#33258;&#21160;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#26041;&#27861;&#65306;&#19981;&#36830;&#32493;ReLU&#65288;DeLU&#65289;&#32593;&#32476;&#65292;&#23427;&#23558;&#22996;&#25176;&#20154;&#30340;&#25928;&#29992;&#24314;&#27169;&#20026;&#21512;&#21516;&#35774;&#35745;&#30340;&#19981;&#36830;&#32493;&#20998;&#27573;&#20223;&#23556;&#20989;&#25968;&#65292;&#20854;&#20013;&#27599;&#20010;&#20998;&#27573;&#23545;&#24212;&#20110;&#20195;&#29702;&#20154;&#37319;&#21462;&#29305;&#23450;&#30340;&#34892;&#21160;&#12290;DeLU&#32593;&#32476;&#36890;&#36807;&#32447;&#24615;&#35268;&#21010;&#25110;&#20869;&#28857;&#26041;&#27861;&#38544;&#24335;&#23398;&#20064;&#20195;&#29702;&#20154;&#30340;&#28608;&#21169;&#30456;&#23481;&#32422;&#26463;&#21644;&#22996;&#25176;&#20154;&#30340;&#25928;&#29992;&#26368;&#22823;&#21270;&#30446;&#26631;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#65292;&#24182;&#25903;&#25345;&#27599;&#20010;&#20998;&#27573;&#30340;&#24182;&#34892;&#25512;&#26029;&#20197;&#27714;&#35299;&#26368;&#20248;&#21512;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32463;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#36817;&#20284;&#22996;&#25176;&#20154;&#25928;&#29992;&#20989;&#25968;&#24182;&#25193;&#23637;&#20197;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#21512;&#21516;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contract design involves a principal who establishes contractual agreements about payments for outcomes that arise from the actions of an agent. In this paper, we initiate the study of deep learning for the automated design of optimal contracts. We introduce a novel representation: the Discontinuous ReLU (DeLU) network, which models the principal's utility as a discontinuous piecewise affine function of the design of a contract where each piece corresponds to the agent taking a particular action. DeLU networks implicitly learn closed-form expressions for the incentive compatibility constraints of the agent and the utility maximization objective of the principal, and support parallel inference on each piece through linear programming or interior-point methods that solve for optimal contracts. We provide empirical results that demonstrate success in approximating the principal's utility function with a small number of training samples and scaling to find approximately optimal contracts o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#20943;&#23569;&#24207;&#21015;&#38388;&#20887;&#20313;&#20449;&#24687;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01616</link><description>&lt;p&gt;
SageFormer&#65306;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting. (arXiv:2307.01616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#20943;&#23569;&#24207;&#21015;&#38388;&#20887;&#20313;&#20449;&#24687;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;Transformer&#65292;&#23637;&#31034;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#35299;&#20915;&#36328;&#24207;&#21015;&#20381;&#36182;&#24615;&#30340;&#37325;&#35201;&#24615;&#38382;&#39064;&#19978;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#26088;&#22312;&#20351;&#29992;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;SageFormer&#35299;&#20915;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#26377;&#25928;&#22320;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#20197;&#21450;&#20943;&#23569;&#24207;&#21015;&#20043;&#38388;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25152;&#25552;&#35758;&#30340;&#31995;&#21015;&#24863;&#30693;&#26694;&#26550;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20013;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#36328;&#24207;&#21015;&#20381;&#36182;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SageFormer&#30456;&#27604;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#23637;&#31034;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series forecasting plays a critical role in diverse domains. While recent advancements in deep learning methods, especially Transformers, have shown promise, there remains a gap in addressing the significance of inter-series dependencies. This paper introduces SageFormer, a Series-aware Graph-enhanced Transformer model designed to effectively capture and model dependencies between series using graph structures. SageFormer tackles two key challenges: effectively representing diverse temporal patterns across series and mitigating redundant information among series. Importantly, the proposed series-aware framework seamlessly integrates with existing Transformer-based models, augmenting their ability to model inter-series dependencies. Through extensive experiments on real-world and synthetic datasets, we showcase the superior performance of SageFormer compared to previous state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WireMask-BBO&#30340;&#40657;&#30418;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#25513;&#27169;&#24341;&#23548;&#30340;&#36138;&#24515;&#36807;&#31243;&#36827;&#34892;&#23439;&#21333;&#20803;&#24067;&#23616;&#65292;&#22312;&#26377;&#25928;&#38477;&#20302;HPWL&#30340;&#21516;&#26102;&#33410;&#30465;&#22823;&#37327;&#26102;&#38388;&#12290;&#27492;&#26041;&#27861;&#36824;&#21487;&#20197;&#23545;&#29616;&#26377;&#24067;&#23616;&#36827;&#34892;&#24494;&#35843;&#65292;&#25913;&#21892;50%&#30340;HPWL&#12290;</title><link>http://arxiv.org/abs/2306.16844</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#25513;&#27169;&#24341;&#23548;&#30340;&#40657;&#30418;&#20248;&#21270;&#23454;&#29616;&#23439;&#21333;&#20803;&#24067;&#23616;
&lt;/p&gt;
&lt;p&gt;
Macro Placement by Wire-Mask-Guided Black-Box Optimization. (arXiv:2306.16844v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WireMask-BBO&#30340;&#40657;&#30418;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#25513;&#27169;&#24341;&#23548;&#30340;&#36138;&#24515;&#36807;&#31243;&#36827;&#34892;&#23439;&#21333;&#20803;&#24067;&#23616;&#65292;&#22312;&#26377;&#25928;&#38477;&#20302;HPWL&#30340;&#21516;&#26102;&#33410;&#30465;&#22823;&#37327;&#26102;&#38388;&#12290;&#27492;&#26041;&#27861;&#36824;&#21487;&#20197;&#23545;&#29616;&#26377;&#24067;&#23616;&#36827;&#34892;&#24494;&#35843;&#65292;&#25913;&#21892;50%&#30340;HPWL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#22823;&#35268;&#27169;&#38598;&#25104;&#65288;VLSI&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#33455;&#29255;&#24067;&#23616;&#20013;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#25216;&#26415;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#12290;&#23439;&#21333;&#20803;&#24067;&#23616;&#20316;&#20026;&#35813;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#23376;&#38382;&#39064;&#65292;&#35797;&#22270;&#30830;&#23450;&#25152;&#26377;&#23439;&#21333;&#20803;&#30340;&#20301;&#32622;&#65292;&#20197;&#26368;&#23567;&#21270;&#21322;&#21608;&#38271;&#32447;&#38271;&#65288;HPWL&#65289;&#24182;&#36991;&#20813;&#37325;&#21472;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#21253;&#25324;&#22522;&#20110;&#25171;&#21253;&#12289;&#20998;&#26512;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40657;&#30418;&#20248;&#21270;&#65288;BBO&#65289;&#26694;&#26550;&#65288;&#31216;&#20026;WireMask-BBO&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#25513;&#27169;&#24341;&#23548;&#30340;&#36138;&#24515;&#36807;&#31243;&#36827;&#34892;&#30446;&#26631;&#35780;&#20272;&#26469;&#36827;&#34892;&#23439;&#21333;&#20803;&#24067;&#23616;&#12290;&#37197;&#22791;&#19981;&#21516;&#30340;BBO&#31639;&#27861;&#65292;WireMask-BBO&#22312;&#23454;&#36341;&#20013;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#26102;&#38388;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#30701;&#30340;HPWL&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#23558;&#20854;&#35270;&#20026;&#21021;&#22987;&#35299;&#26469;&#24494;&#35843;&#29616;&#26377;&#30340;&#24067;&#23616;&#65292;&#20174;&#32780;&#20351;HPWL&#25913;&#21892;&#22810;&#36798;50%&#12290;WireMask-BBO&#20855;&#26377;&#24341;&#39046;&#33455;&#29255;&#24067;&#23616;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of very large-scale integration (VLSI) technology has posed new challenges for electronic design automation (EDA) techniques in chip floorplanning. During this process, macro placement is an important subproblem, which tries to determine the positions of all macros with the aim of minimizing half-perimeter wirelength (HPWL) and avoiding overlapping. Previous methods include packing-based, analytical and reinforcement learning methods. In this paper, we propose a new black-box optimization (BBO) framework (called WireMask-BBO) for macro placement, by using a wire-mask-guided greedy procedure for objective evaluation. Equipped with different BBO algorithms, WireMask-BBO empirically achieves significant improvements over previous methods, i.e., achieves significantly shorter HPWL by using much less time. Furthermore, it can fine-tune existing placements by treating them as initial solutions, which can bring up to 50% improvement in HPWL. WireMask-BBO has the potential to s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;LeanDojo&#65292;&#35813;&#24037;&#20855;&#36890;&#36807;&#25552;&#21462;Lean&#30340;&#25968;&#25454;&#65292;&#20026;&#23450;&#29702;&#35777;&#26126;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#24179;&#21488;&#12290;&#21033;&#29992;LeanDojo&#30340;&#25968;&#25454;&#65292;&#24320;&#21457;&#20102;ReProver&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#26126;&#22120;&#65292;&#21487;&#20197;&#20174;&#24222;&#22823;&#30340;&#25968;&#23398;&#24211;&#20013;&#36873;&#25321;&#21629;&#39064;&#65292;&#35757;&#32451;&#25104;&#26412;&#20302;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#19968;&#21608;&#30340;GPU&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.15626</link><description>&lt;p&gt;
LeanDojo: &#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#29702;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
LeanDojo: Theorem Proving with Retrieval-Augmented Language Models. (arXiv:2306.15626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;LeanDojo&#65292;&#35813;&#24037;&#20855;&#36890;&#36807;&#25552;&#21462;Lean&#30340;&#25968;&#25454;&#65292;&#20026;&#23450;&#29702;&#35777;&#26126;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#24179;&#21488;&#12290;&#21033;&#29992;LeanDojo&#30340;&#25968;&#25454;&#65292;&#24320;&#21457;&#20102;ReProver&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#26126;&#22120;&#65292;&#21487;&#20197;&#20174;&#24222;&#22823;&#30340;&#25968;&#23398;&#24211;&#20013;&#36873;&#25321;&#21629;&#39064;&#65292;&#35757;&#32451;&#25104;&#26412;&#20302;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#19968;&#21608;&#30340;GPU&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20351;&#29992;Lean&#31561;&#35777;&#26126;&#21161;&#25163;&#35777;&#26126;&#24418;&#24335;&#23450;&#29702;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31169;&#26377;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#22823;&#37327;&#35745;&#31639;&#35201;&#27714;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24456;&#38590;&#22797;&#21046;&#25110;&#24314;&#31435;&#22312;&#20854;&#22522;&#30784;&#19978;&#65292;&#36825;&#32473;&#23450;&#29702;&#35777;&#26126;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#30740;&#31350;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LeanDojo&#26469;&#28040;&#38500;&#36825;&#20123;&#38556;&#30861;&#65306;&#19968;&#20010;&#21253;&#21547;&#24037;&#20855;&#21253;&#12289;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;Lean&#28216;&#20048;&#22330;&#12290;LeanDojo&#20174;Lean&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#24182;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#32534;&#31243;&#19982;&#35777;&#26126;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#23427;&#21253;&#21547;&#35777;&#26126;&#20013;&#21629;&#39064;&#30340;&#32454;&#31890;&#24230;&#27880;&#37322;&#65292;&#20026;&#21629;&#39064;&#36873;&#25321;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65306;&#36825;&#26159;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;ReProver&#65288;&#26816;&#32034;&#22686;&#24378;&#30340;&#35777;&#26126;&#22120;&#65289;&#65306;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;LLM&#30340;&#35777;&#26126;&#22120;&#65292;&#36890;&#36807;&#26816;&#32034;&#20174;&#24222;&#22823;&#30340;&#25968;&#23398;&#24211;&#20013;&#36873;&#25321;&#21629;&#39064;&#12290;&#23427;&#25104;&#26412;&#20302;&#24265;&#65292;&#21482;&#38656;&#35201;&#19968;&#21608;&#30340;GPU&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26816;&#32034;&#22120;&#21033;&#29992;&#20102;LeanDojo&#30340;pro&#30456;&#20851;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection: a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): the first LLM-based prover that is augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's prog
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32852;&#21512;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14884</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Modulate pre-trained Models in RL. (arXiv:2306.14884v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32852;&#21512;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#12289;&#28216;&#25103;&#21644;&#20223;&#30495;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#19981;&#36275;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#24448;&#24448;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24403;&#22312;&#26032;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#23545;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#20026;&#20102;&#30740;&#31350;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;Meta-World&#21644;DMControl&#20004;&#20010;&#22522;&#20934;&#22871;&#20214;&#30340;&#25968;&#25454;&#38598;&#19978;&#32852;&#21512;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24120;&#35265;&#30340;&#22810;&#31181;&#24494;&#35843;&#26041;&#27861;&#22312;&#26032;&#20219;&#21153;&#24615;&#33021;&#21644;&#23545;&#39044;&#35757;&#32451;&#20219;&#21153;&#24615;&#33021;&#20445;&#30041;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has been successful in various domains like robotics, game playing, and simulation. While RL agents have shown impressive capabilities in their specific tasks, they insufficiently adapt to new tasks. In supervised learning, this adaptation problem is addressed by large-scale pre-training followed by fine-tuning to new down-stream tasks. Recently, pre-training on multiple tasks has been gaining traction in RL. However, fine-tuning a pre-trained model often suffers from catastrophic forgetting. That is, the performance on the pre-training tasks deteriorates when fine-tuning on new tasks. To investigate the catastrophic forgetting phenomenon, we first jointly pre-train a model on datasets from two benchmark suites, namely Meta-World and DMControl. Then, we evaluate and compare a variety of fine-tuning methods prevalent in natural language processing, both in terms of performance on new tasks, and how well performance on pre-training tasks is retained. Our study
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TACO&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#21644;&#21160;&#20316;&#34920;&#31034;&#65292;&#25552;&#39640;&#20195;&#29702;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.13229</link><description>&lt;p&gt;
TACO&#65306;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning. (arXiv:2306.13229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TACO&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#21644;&#21160;&#20316;&#34920;&#31034;&#65292;&#25552;&#39640;&#20195;&#29702;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#21407;&#22987;&#20687;&#32032;&#25968;&#25454;&#20013;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35797;&#22270;&#36890;&#36807;&#21019;&#24314;&#33258;&#30417;&#30563;&#36741;&#21161;&#20219;&#21153;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#29366;&#24577;&#39044;&#27979;&#20016;&#23500;&#20195;&#29702;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#25511;&#21046;&#30456;&#20851;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30446;&#26631;&#36890;&#24120;&#19981;&#36275;&#20197;&#23398;&#20064;&#33021;&#22815;&#34920;&#31034;&#26368;&#20248;&#31574;&#30053;&#25110;&#20540;&#20989;&#25968;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#23427;&#20204;&#36890;&#24120;&#32771;&#34385;&#20855;&#26377;&#23567;&#30340;&#25277;&#35937;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#24573;&#35270;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#20013;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TACO&#65306;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23427;&#65292;&#20195;&#29702;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#28508;&#22312;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#12290;TACO&#36890;&#36807;&#20248;&#21270;&#37325;&#26032;&#33719;&#24471;&#35266;&#23519;&#19982;&#26368;&#36817;&#30340;&#22810;&#20010;&#20808;&#21069;&#35266;&#23519;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#19982;&#21160;&#20316;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in reinforcement learning (RL) from raw pixel data, sample inefficiency continues to present a substantial obstacle. Prior works have attempted to address this challenge by creating self-supervised auxiliary tasks, aiming to enrich the agent's learned representations with control-relevant information for future state prediction. However, these objectives are often insufficient to learn representations that can represent the optimal policy or value function, and they often consider tasks with small, abstract discrete action spaces and thus overlook the importance of action representation learning in continuous control. In this paper, we introduce TACO: Temporal Action-driven Contrastive Learning, a simple yet powerful temporal contrastive learning approach that facilitates the concurrent acquisition of latent state and action representations for agents. TACO simultaneously learns a state and an action representation by optimizing the mutual information between re
&lt;/p&gt;</description></item><item><title>&#25299;&#25169;&#35270;&#24046;&#26159;&#19968;&#31181;&#27604;&#36739;&#35757;&#32451;&#27169;&#22411;&#21644;&#21442;&#32771;&#25968;&#25454;&#38598;&#22810;&#23610;&#24230;&#20960;&#20309;&#32467;&#26500;&#30456;&#20284;&#24615;&#30340;&#29702;&#35770;&#21644;&#35745;&#31639;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#20272;&#35745;&#27169;&#22411;&#20013;&#30340;&#25299;&#25169;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11835</link><description>&lt;p&gt;
&#25299;&#25169;&#35270;&#24046;&#65306;&#28145;&#24230;&#24863;&#30693;&#27169;&#22411;&#30340;&#20960;&#20309;&#35268;&#33539;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
Topological Parallax: A Geometric Specification for Deep Perception Models. (arXiv:2306.11835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11835
&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#35270;&#24046;&#26159;&#19968;&#31181;&#27604;&#36739;&#35757;&#32451;&#27169;&#22411;&#21644;&#21442;&#32771;&#25968;&#25454;&#38598;&#22810;&#23610;&#24230;&#20960;&#20309;&#32467;&#26500;&#30456;&#20284;&#24615;&#30340;&#29702;&#35770;&#21644;&#35745;&#31639;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#20272;&#35745;&#27169;&#22411;&#20013;&#30340;&#25299;&#25169;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#35777;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#25299;&#25169;&#35270;&#24046;&#20316;&#20026;&#27604;&#36739;&#24050;&#35757;&#32451;&#27169;&#22411;&#21644;&#21442;&#32771;&#25968;&#25454;&#38598;&#30340;&#22810;&#23610;&#24230;&#20960;&#20309;&#32467;&#26500;&#30456;&#20284;&#24615;&#30340;&#29702;&#35770;&#21644;&#35745;&#31639;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#21644;&#20363;&#23376;&#34920;&#26126;&#65292;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#36825;&#31181;&#20960;&#20309;&#30456;&#20284;&#24615;&#23545;&#20110;&#21487;&#20449;&#30340;&#25554;&#20540;&#21644;&#25200;&#21160;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#25105;&#20204;&#29468;&#27979;&#65292;&#36825;&#20010;&#26032;&#27010;&#24565;&#23558;&#20026;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#36807;&#25311;&#21512;&#21644;&#27867;&#21270;&#20043;&#38388;&#19981;&#26126;&#30830;&#30340;&#20851;&#31995;&#30340;&#24403;&#21069;&#35752;&#35770;&#22686;&#28155;&#20215;&#20540;&#12290;&#22312;&#20856;&#22411;&#30340;DNN&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#30340;&#26174;&#24335;&#20960;&#20309;&#25551;&#36848;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#35270;&#24046;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#20351;&#29992;&#21442;&#32771;&#25968;&#25454;&#38598;&#30340;&#27979;&#22320;&#30072;&#21464;&#23545;Rips&#22797;&#21512;&#20307;&#30340;&#24433;&#21709;&#26469;&#20272;&#35745;&#27169;&#22411;&#20013;&#30340;&#25299;&#25169;&#29305;&#24449;&#65288;&#32452;&#20214;&#12289;&#21608;&#26399;&#12289;&#31354;&#27934;&#31561;&#65289;&#12290;&#22240;&#27492;&#65292;&#35270;&#24046;&#25351;&#31034;&#27169;&#22411;&#19982;&#25968;&#25454;&#38598;&#26159;&#21542;&#20849;&#20139;&#31867;&#20284;&#30340;&#22810;&#23610;&#24230;&#20960;&#20309;&#29305;&#24449;&#12290;&#35270;&#24046;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#20174;&#19981;&#21516;&#35282;&#24230;&#35266;&#23519;&#25968;&#25454;&#30340;&#30452;&#35266;&#27010;&#24565;&#65292;&#24182;&#20026;&#29702;&#35299;&#28145;&#24230;&#24863;&#30693;&#27169;&#22411;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
For safety and robustness of AI systems, we introduce topological parallax as a theoretical and computational tool that compares a trained model to a reference dataset to determine whether they have similar multiscale geometric structure. Our proofs and examples show that this geometric similarity between dataset and model is essential to trustworthy interpolation and perturbation, and we conjecture that this new concept will add value to the current debate regarding the unclear relationship between overfitting and generalization in applications of deep-learning. In typical DNN applications, an explicit geometric description of the model is impossible, but parallax can estimate topological features (components, cycles, voids, etc.) in the model by examining the effect on the Rips complex of geodesic distortions using the reference dataset. Thus, parallax indicates whether the model shares similar multiscale geometric features with the dataset. Parallax presents theoretically via topolo
&lt;/p&gt;</description></item><item><title>IMP-MARL&#26159;&#19968;&#22871;&#29992;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#35774;&#26045;&#31649;&#29702;&#35268;&#21010;&#30340;&#24320;&#28304;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#22871;&#20214;&#65292;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#22522;&#20934;&#24179;&#21488;&#26469;&#35780;&#20272;&#21512;&#20316;MARL&#26041;&#27861;&#22312;&#29616;&#23454;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11551</link><description>&lt;p&gt;
IMP-MARL: &#19968;&#22871;&#29992;&#20110;&#22522;&#20110;MARL&#30340;&#22823;&#35268;&#27169;&#22522;&#30784;&#35774;&#26045;&#31649;&#29702;&#35268;&#21010;&#30340;&#29615;&#22659;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
IMP-MARL: a Suite of Environments for Large-scale Infrastructure Management Planning via MARL. (arXiv:2306.11551v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11551
&lt;/p&gt;
&lt;p&gt;
IMP-MARL&#26159;&#19968;&#22871;&#29992;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#35774;&#26045;&#31649;&#29702;&#35268;&#21010;&#30340;&#24320;&#28304;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#22871;&#20214;&#65292;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#22522;&#20934;&#24179;&#21488;&#26469;&#35780;&#20272;&#21512;&#20316;MARL&#26041;&#27861;&#22312;&#29616;&#23454;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;IMP-MARL&#65292;&#36825;&#26159;&#19968;&#22871;&#29992;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#35774;&#26045;&#31649;&#29702;&#35268;&#21010;&#65288;IMP&#65289;&#30340;&#24320;&#28304;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#29615;&#22659;&#22871;&#20214;&#65292;&#20026;&#21512;&#20316;MARL&#26041;&#27861;&#22312;&#29616;&#23454;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#24179;&#21488;&#12290;&#22312;IMP&#20013;&#65292;&#22810;&#32452;&#20998;&#24037;&#31243;&#31995;&#32479;&#30001;&#20110;&#20854;&#32452;&#20214;&#30340;&#25439;&#22351;&#24773;&#20917;&#23384;&#22312;&#25925;&#38556;&#39118;&#38505;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#20026;&#19968;&#20010;&#29305;&#23450;&#30340;&#31995;&#32479;&#32452;&#20214;&#36827;&#34892;&#26816;&#26597;&#21644;&#32500;&#20462;&#35268;&#21010;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#32500;&#25252;&#25104;&#26412;&#30340;&#21516;&#26102;&#21512;&#20316;&#20197;&#26368;&#23567;&#21270;&#31995;&#32479;&#25925;&#38556;&#39118;&#38505;&#12290;&#36890;&#36807;IMP-MARL&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20960;&#20010;&#29615;&#22659;&#65292;&#21253;&#25324;&#19968;&#20010;&#19982;&#28023;&#19978;&#39118;&#30005;&#32467;&#26500;&#31995;&#32479;&#30456;&#20851;&#30340;&#29615;&#22659;&#65292;&#20197;&#28385;&#36275;&#25913;&#21892;&#31649;&#29702;&#31574;&#30053;&#20197;&#25903;&#25345;&#21487;&#25345;&#32493;&#21644;&#21487;&#38752;&#33021;&#28304;&#31995;&#32479;&#30340;&#29616;&#20170;&#38656;&#27714;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#22810;100&#20010;&#26234;&#33021;&#20307;&#30340;IMP&#23454;&#38469;&#24037;&#31243;&#29615;&#22659;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#30446;&#21069;&#21512;&#20316;MARL&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce IMP-MARL, an open-source suite of multi-agent reinforcement learning (MARL) environments for large-scale Infrastructure Management Planning (IMP), offering a platform for benchmarking the scalability of cooperative MARL methods in real-world engineering applications. In IMP, a multi-component engineering system is subject to a risk of failure due to its components' damage condition. Specifically, each agent plans inspections and repairs for a specific system component, aiming to minimise maintenance costs while cooperating to minimise system failure risk. With IMP-MARL, we release several environments including one related to offshore wind structural systems, in an effort to meet today's needs to improve management strategies to support sustainable and reliable energy systems. Supported by IMP practical engineering environments featuring up to 100 agents, we conduct a benchmark campaign, where the scalability and performance of state-of-the-art cooperative MARL methods are
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#22240;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;GRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#26377;&#26426;&#20307;&#30340;&#36827;&#21270;&#21644;&#21033;&#29992;&#23398;&#20064;&#22522;&#22240;&#26469;&#23398;&#20064;&#21644;&#28436;&#21270;&#26234;&#33021;&#20307;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;GRL&#22312;&#26234;&#33021;&#20307;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10225</link><description>&lt;p&gt;
&#26234;&#33021;&#20307;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
Genes in Intelligent Agents. (arXiv:2306.10225v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10225
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#22240;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;GRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#26377;&#26426;&#20307;&#30340;&#36827;&#21270;&#21644;&#21033;&#29992;&#23398;&#20064;&#22522;&#22240;&#26469;&#23398;&#20064;&#21644;&#28436;&#21270;&#26234;&#33021;&#20307;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;GRL&#22312;&#26234;&#33021;&#20307;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#30028;&#20013;&#65292;&#22522;&#22240;&#36890;&#36807;&#25968;&#21313;&#20159;&#24180;&#30340;&#20256;&#36882;&#21644;&#31215;&#32047;&#65292;&#36171;&#20104;&#22320;&#29699;&#19978;&#29983;&#29289;&#30340;&#24403;&#21069;&#29983;&#29289;&#26234;&#33021;&#12290;&#21463;&#21040;&#29983;&#29289;&#26234;&#33021;&#30340;&#21551;&#21457;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#33268;&#21147;&#20110;&#26500;&#24314;&#26426;&#22120;&#26234;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#32321;&#33635;&#30340;&#25104;&#21151;&#65292;&#20294;&#26426;&#22120;&#26234;&#33021;&#20173;&#36828;&#36828;&#33853;&#21518;&#20110;&#29983;&#29289;&#26234;&#33021;&#12290;&#21407;&#22240;&#21487;&#33021;&#22312;&#20110;&#21160;&#29289;&#22825;&#29983;&#20855;&#26377;&#26576;&#31181;&#22522;&#22240;&#32534;&#30721;&#30340;&#26234;&#33021;&#65292;&#32780;&#26426;&#22120;&#32570;&#20047;&#27492;&#31867;&#26234;&#33021;&#65292;&#38656;&#35201;&#20174;&#22836;&#23398;&#20064;&#12290;&#21463;&#21040;&#21160;&#29289;&#22522;&#22240;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#26426;&#22120;&#30340;&#8220;&#22522;&#22240;&#8221;&#65292;&#31216;&#20026;&#8220;&#23398;&#20064;&#22522;&#22240;&#8221;&#65292;&#24182;&#25552;&#20986;&#20102;&#36951;&#20256;&#22686;&#24378;&#23398;&#20064;&#65288;GRL&#65289;&#12290;GRL&#26159;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#27169;&#25311;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#26377;&#26426;&#20307;&#30340;&#36827;&#21270;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#22522;&#22240;&#26469;&#23398;&#20064;&#21644;&#28436;&#21270;&#26234;&#33021;&#20307;&#12290;&#21033;&#29992;GRL&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#23398;&#20064;&#22522;&#22240;&#37319;&#29992;&#20102;&#26234;&#33021;&#20307;&#31070;&#32463;&#32593;&#32476;&#30340;&#29255;&#27573;&#24418;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#32487;&#25215;&#12290;&#36825;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;GRL&#22312;&#26234;&#33021;&#20307;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The genes in nature give the lives on earth the current biological intelligence through transmission and accumulation over billions of years. Inspired by the biological intelligence, artificial intelligence (AI) has devoted to building the machine intelligence. Although it has achieved thriving successes, the machine intelligence still lags far behind the biological intelligence. The reason may lie in that animals are born with some intelligence encoded in their genes, but machines lack such intelligence and learn from scratch. Inspired by the genes of animals, we define the ``genes'' of machines named as the ``learngenes'' and propose the Genetic Reinforcement Learning (GRL). GRL is a computational framework that simulates the evolution of organisms in reinforcement learning (RL) and leverages the learngenes to learn and evolve the intelligence agents. Leveraging GRL, we first show that the learngenes take the form of the fragments of the agents' neural networks and can be inherited a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23454;&#29992;&#30340;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#31639;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#19981;&#33021;&#22815;&#20840;&#31243;&#21521;&#26368;&#20248;&#28857;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2306.09850</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#31639;&#27861;&#19981;&#33021;&#20840;&#31243;&#21521;&#26368;&#20248;&#28857;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima. (arXiv:2306.09850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09850
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23454;&#29992;&#30340;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#31639;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#19981;&#33021;&#22815;&#20840;&#31243;&#21521;&#26368;&#20248;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;(SAM)&#26159;&#19968;&#31181;&#20248;&#21270;&#22120;&#65292;&#23427;&#22522;&#20110;&#24403;&#21069;&#28857;$x_t$&#30340;&#26799;&#24230;&#65292;&#22312;&#25200;&#21160;$y_t=x_t+\rho\frac{\nabla f(x_t)}{\lVert\nabla f(x_t)\rVert}$&#22788;&#36827;&#34892;&#19979;&#38477;&#12290;&#29616;&#26377;&#30740;&#31350;&#35777;&#26126;&#20102;SAM&#23545;&#20110;&#24179;&#28369;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#20551;&#35774;&#25200;&#21160;&#30340;&#22823;&#23567;$\rho$&#36880;&#28176;&#34928;&#20943;&#21644;/&#25110;&#22312;$y_t$&#20013;&#27809;&#26377;&#26799;&#24230;&#24402;&#19968;&#21270;&#65292;&#36825;&#19982;&#23454;&#36341;&#19981;&#31526;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#23454;&#29992;&#37197;&#32622;&#65288;&#21363;&#24120;&#25968;$\rho$&#21644;$y_t$&#20013;&#30340;&#26799;&#24230;&#24402;&#19968;&#21270;&#65289;&#30340;&#30830;&#23450;&#24615;/&#38543;&#26426;&#29256;&#26412;&#30340;SAM&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#65288;&#38750;&#65289;&#20984;&#24615;&#20551;&#35774;&#30340;&#24179;&#28369;&#20989;&#25968;&#19978;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;SAM&#22312;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#25110;&#31283;&#23450;&#28857;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#24179;&#28369;&#24378;&#20984;&#20989;&#25968;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30830;&#23450;&#24615;SAM&#20855;&#26377;&#20005;&#26684;&#30340;&#20840;&#23616;&#25910;&#25947;&#29575;&#20026;$\tilde\Theta(\frac{1}{T^2})$&#65292;&#32780;&#38543;&#26426;SAM&#30340;&#25910;&#25947;&#30028;&#21017;&#21463;&#21040;&#22122;&#22768;&#27700;&#24179;&#38477;&#20302;&#30340;&#24433;&#21709;&#65292;&#36825;&#34920;&#26126;&#20102;&#24179;&#38754;&#30446;&#26631;&#34920;&#38754;&#30340;&#23574;&#38160;&#24230;&#21644;&#24179;&#32531;&#24615;&#20043;&#38388;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step based on the gradient at a perturbation $y_t = x_t + \rho \frac{\nabla f(x_t)}{\lVert \nabla f(x_t) \rVert}$ of the current point $x_t$. Existing studies prove convergence of SAM for smooth functions, but they do so by assuming decaying perturbation size $\rho$ and/or no gradient normalization in $y_t$, which is detached from practice. To address this gap, we study deterministic/stochastic versions of SAM with practical configurations (i.e., constant $\rho$ and gradient normalization in $y_t$) and explore their convergence properties on smooth functions with (non)convexity assumptions. Perhaps surprisingly, in many scenarios, we find out that SAM has limited capability to converge to global minima or stationary points. For smooth strongly convex functions, we show that while deterministic SAM enjoys tight global convergence rates of $\tilde \Theta(\frac{1}{T^2})$, the convergence bound of stochastic SAM suffer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09539</link><description>&lt;p&gt;
&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22312;&#38656;&#35201;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#24615;&#24182;&#19988;&#38656;&#35201;&#39640;&#25928;&#25193;&#23637;&#21040;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24778;&#20154;&#30340;&#25928;&#26524;&#12290;&#23613;&#31649;&#26368;&#21021;&#26159;&#20026;&#36830;&#32493;&#20449;&#21495;&#35774;&#35745;&#30340;&#65292;&#20294;SSM&#22312;&#35270;&#35273;&#21644;&#38899;&#39057;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65307;&#28982;&#32780;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;SSM&#20173;&#28982;&#33853;&#21518;&#20110;Transformers&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#23618;&#65292;&#23427;&#22312;&#20869;&#37096;&#32452;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#21270;&#30340;SSM&#23376;&#23618;&#21644;&#19968;&#20010;&#29992;&#20110;&#30701;&#26399;&#24207;&#21015;&#34920;&#31034;&#30340;&#22359;&#21464;&#25442;&#22120;&#23376;&#23618;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#12289;&#23436;&#20840;&#21487;&#24182;&#34892;&#30340;&#38598;&#25104;SSM&#21644;&#22359;&#27880;&#24847;&#21147;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#30340;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;NetHack&#30340;&#24037;&#20855;&#21644;&#22522;&#20934;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#36164;&#28304;&#12289;&#23454;&#29616;&#21644;&#22522;&#20934;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;&#35813;&#24211;&#25552;&#20379;&#20102;ORL&#31038;&#21306;&#25152;&#29087;&#24713;&#30340;&#24037;&#20316;&#27969;&#22522;&#30784;&#65292;&#24182;&#38468;&#24102;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.08772</link><description>&lt;p&gt;
Katakomba&#65306;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;NetHack&#30340;&#24037;&#20855;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Katakomba: Tools and Benchmarks for Data-Driven NetHack. (arXiv:2306.08772v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;NetHack&#30340;&#24037;&#20855;&#21644;&#22522;&#20934;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#36164;&#28304;&#12289;&#23454;&#29616;&#21644;&#22522;&#20934;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;&#35813;&#24211;&#25552;&#20379;&#20102;ORL&#31038;&#21306;&#25152;&#29087;&#24713;&#30340;&#24037;&#20316;&#27969;&#22522;&#30784;&#65292;&#24182;&#38468;&#24102;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NetHack&#34987;&#35748;&#20026;&#26159;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#21069;&#27839;&#65292;&#20854;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#20173;&#38656;&#36214;&#19978;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20351;&#29992;&#31867;&#20284;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#39046;&#22495;&#26368;&#26032;&#21457;&#23637;&#20013;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;ORL&#65289;&#25104;&#20026;&#31361;&#30772;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#26368;&#36817;&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;NetHack&#25968;&#25454;&#38598;&#34987;&#37322;&#25918;&#20986;&#65292;&#34429;&#28982;&#36825;&#26159;&#21521;&#21069;&#36808;&#20986;&#30340;&#24517;&#35201;&#19968;&#27493;&#65292;&#20294;&#23427;&#23578;&#26410;&#22312;ORL&#31038;&#21306;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#26377;&#19977;&#20010;&#20027;&#35201;&#38556;&#30861;&#38656;&#35201;&#20811;&#26381;&#65306;&#36164;&#28304;&#12289;&#23454;&#29616;&#21644;&#22522;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;ORL&#31038;&#21306;&#29087;&#24713;&#30340;&#24037;&#20316;&#27969;&#22522;&#30784;&#65306;&#39044;&#23450;&#20041;&#30340;D4RL&#39118;&#26684;&#20219;&#21153;&#65292;&#31616;&#27905;&#30340;&#22522;&#20934;&#23454;&#29616;&#65292;&#20197;&#21450;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#24182;&#38468;&#26377;&#19982;&#20113;&#31471;&#21516;&#27493;&#30340;&#37197;&#32622;&#21644;&#26085;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;
NetHack is known as the frontier of reinforcement learning research where learning-based methods still need to catch up to rule-based solutions. One of the promising directions for a breakthrough is using pre-collected datasets similar to recent developments in robotics, recommender systems, and more under the umbrella of offline reinforcement learning (ORL). Recently, a large-scale NetHack dataset was released; while it was a necessary step forward, it has yet to gain wide adoption in the ORL community. In this work, we argue that there are three major obstacles for adoption: resource-wise, implementation-wise, and benchmark-wise. To address them, we develop an open-source library that provides workflow fundamentals familiar to the ORL community: pre-defined D4RL-style tasks, uncluttered baseline implementations, and reliable evaluation tools with accompanying configs and logs synced to the cloud.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#22270;&#32467;&#26500;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#29366;&#24577;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#21464;&#20998;&#25512;&#26029;&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#21040;&#28789;&#27963;&#30340;&#26102;&#31354;&#20808;&#39564;&#65292;&#24182;&#33719;&#24471;&#38381;&#24335;&#21518;&#39564;&#35745;&#31639;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#21345;&#23572;&#26364;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08445</link><description>&lt;p&gt;
&#28145;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#29992;&#20110;&#22270;&#32467;&#26500;&#21160;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Deep Gaussian Markov Random Fields for Graph-Structured Dynamical Systems. (arXiv:2306.08445v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#22270;&#32467;&#26500;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#29366;&#24577;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#21464;&#20998;&#25512;&#26029;&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#21040;&#28789;&#27963;&#30340;&#26102;&#31354;&#20808;&#39564;&#65292;&#24182;&#33719;&#24471;&#38381;&#24335;&#21518;&#39564;&#35745;&#31639;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#21345;&#23572;&#26364;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#26029;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#26102;&#31354;&#31995;&#32479;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#26377;&#20851;&#29366;&#24577;&#21464;&#37327;&#20381;&#36182;&#32467;&#26500;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#65288;&#37096;&#20998;&#65289;&#26410;&#30693;&#21160;&#21147;&#23398;&#21644;&#26377;&#38480;&#21382;&#21490;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20013;&#36827;&#34892;&#29366;&#24577;&#20272;&#35745;&#21644;&#23398;&#20064;&#30340;&#35745;&#31639;&#26377;&#25928;&#26041;&#27861;&#12290;&#20511;&#37492;&#28145;&#24230;&#23398;&#20064;&#19982;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;GMRF&#65289;&#20013;&#30340;&#26377;&#21407;&#21017;&#30340;&#25512;&#26029;&#30340;&#26368;&#36817;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#22270;&#32467;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#37325;&#26032;&#23450;&#20041;&#20026;&#30001;&#31616;&#21333;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22270;&#23618;&#23450;&#20041;&#30340;&#28145;&#24230;GMRF&#12290;&#36825;&#20135;&#29983;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26102;&#31354;&#20808;&#39564;&#65292;&#21487;&#20197;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#20174;&#21333;&#20010;&#26102;&#38388;&#24207;&#21015;&#19978;&#39640;&#25928;&#22320;&#23398;&#20064;&#12290;&#22312;&#32447;&#24615;&#39640;&#26031;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#20445;&#30041;&#20102;&#19968;&#20010;&#23553;&#38381;&#24418;&#24335;&#30340;&#21518;&#39564;&#65292;&#21487;&#20197;&#20351;&#29992;&#20849;&#36717;&#26799;&#24230;&#27861;&#39640;&#25928;&#22320;&#37319;&#26679;&#65292;&#19982;&#32463;&#20856;&#30340;&#21345;&#23572;&#26364;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic inference in high-dimensional state-space models is computationally challenging. For many spatiotemporal systems, however, prior knowledge about the dependency structure of state variables is available. We leverage this structure to develop a computationally efficient approach to state estimation and learning in graph-structured state-space models with (partially) unknown dynamics and limited historical data. Building on recent methods that combine ideas from deep learning with principled inference in Gaussian Markov random fields (GMRF), we reformulate graph-structured state-space models as Deep GMRFs defined by simple spatial and temporal graph layers. This results in a flexible spatiotemporal prior that can be learned efficiently from a single time sequence via variational inference. Under linear Gaussian assumptions, we retain a closed-form posterior, which can be sampled efficiently using the conjugate gradient method, scaling favourably compared to classical Kalman 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27491;&#20132;&#24494;&#35843;&#65288;OFT&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24341;&#23548;&#21644;&#25511;&#21046;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32422;&#26463;&#27491;&#20132;&#24494;&#35843;&#65288;COFT&#65289;&#65292;&#26469;&#25552;&#39640;&#24494;&#35843;&#30340;&#31283;&#23450;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#35821;&#20041;&#29983;&#25104;&#33021;&#21147;&#24182;&#29983;&#25104;&#29305;&#23450;&#20027;&#39064;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.07280</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#20132;&#24494;&#35843;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Controlling Text-to-Image Diffusion by Orthogonal Finetuning. (arXiv:2306.07280v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27491;&#20132;&#24494;&#35843;&#65288;OFT&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24341;&#23548;&#21644;&#25511;&#21046;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32422;&#26463;&#27491;&#20132;&#24494;&#35843;&#65288;COFT&#65289;&#65292;&#26469;&#25552;&#39640;&#24494;&#35843;&#30340;&#31283;&#23450;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#35821;&#20041;&#29983;&#25104;&#33021;&#21147;&#24182;&#29983;&#25104;&#29305;&#23450;&#20027;&#39064;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#30495;&#23454;&#24863;&#22270;&#20687;&#26041;&#38754;&#26377;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#24341;&#23548;&#25110;&#25511;&#21046;&#36825;&#20123;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#25191;&#34892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#24494;&#35843;&#26041;&#27861;&#8212;&#8212;&#27491;&#20132;&#24494;&#35843;&#65288;OFT&#65289;&#65292;&#29992;&#20110;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#35843;&#25972;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;OFT&#21487;&#20197;&#35777;&#26126;&#22320;&#20445;&#25345;&#29305;&#24449;&#23545;&#31070;&#32463;&#20803;&#22312;&#21333;&#20301;&#36229;&#29699;&#38754;&#19978;&#30340;&#20851;&#31995;&#25152;&#34920;&#24449;&#30340;&#36229;&#29699;&#24418;&#33021;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#23646;&#24615;&#23545;&#20110;&#20445;&#25345;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#29983;&#25104;&#33021;&#21147;&#38750;&#24120;&#20851;&#38190;&#12290;&#20026;&#20102;&#25552;&#39640;&#24494;&#35843;&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#32422;&#26463;&#27491;&#20132;&#24494;&#35843;&#65288;COFT&#65289;&#65292;&#23427;&#23545;&#36229;&#29699;&#38754;&#26045;&#21152;&#20102;&#39069;&#22806;&#30340;&#21322;&#24452;&#32422;&#26463;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#20219;&#21153;&#65306;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#29305;&#23450;&#20027;&#39064;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Large text-to-image diffusion models have impressive capabilities in generating photorealistic images from text prompts. How to effectively guide or control these powerful models to perform different downstream tasks becomes an important open problem. To tackle this challenge, we introduce a principled finetuning method -- Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to downstream tasks. Unlike existing methods, OFT can provably preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. We find that this property is crucial for preserving the semantic generation ability of text-to-image diffusion models. To improve finetuning stability, we further propose Constrained Orthogonal Finetuning (COFT) which imposes an additional radius constraint to the hypersphere. Specifically, we consider two important finetuning text-to-image tasks: subject-driven generation where the goal is to generate subject-specific images
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24403;&#22870;&#21169;&#21576;&#8220;&#37325;&#23614;&#8221;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#23454;&#20363;&#30456;&#20851;&#31639;&#27861;&#65292;&#24182;&#24471;&#21040;&#20102;&#26497;&#23567;&#26368;&#22823;&#21270;&#30340;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.06836</link><description>&lt;p&gt;
&#29992;&#20989;&#25968;&#36924;&#36817;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#37325;&#23614;&#22870;&#21169;&#38382;&#39064;&#30340;&#26497;&#23567;&#26368;&#22823;&#21270;&#31639;&#27861;&#21644;&#23454;&#20363;&#30456;&#20851;&#36951;&#25022;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds. (arXiv:2306.06836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24403;&#22870;&#21169;&#21576;&#8220;&#37325;&#23614;&#8221;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#23454;&#20363;&#30456;&#20851;&#31639;&#27861;&#65292;&#24182;&#24471;&#21040;&#20102;&#26497;&#23567;&#26368;&#22823;&#21270;&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26377;&#35768;&#22810;&#24037;&#20316;&#37117;&#19987;&#27880;&#20110;&#20026;&#26377;&#30028;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#26377;&#25928;&#31639;&#27861;&#65292;&#20294;&#24403;&#22870;&#21169;&#21576;&#29616;&#8220;&#37325;&#23614;&#8221;&#20998;&#24067;&#26102;&#8212;&#8212;&#21363;&#23384;&#22312;&#26576;&#20010; $\epsilon\in(0,1]$ &#20351;&#24471;&#20165;&#26377;&#26377;&#38480;&#30340;$(1+\epsilon)$-&#38454;&#30697;&#8212;&#8212;&#26159;&#21542;&#23384;&#22312;&#23545;&#22823;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#25110;&#26102;&#25928;&#24615;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340; RL &#20013;&#30340;&#36825;&#31181;&#22870;&#21169;&#26426;&#21046;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#37325;&#23614;&#32447;&#24615;&#36172;&#33218;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#8212;&#8212;\textsc{Heavy-OFUL}&#65292;&#20854;&#23454;&#29616;&#20102;&#19968;&#31181;&#23454;&#20363;&#30456;&#20851;&#30340; $T$-round &#36951;&#25022;&#24230;&#37327;&#65292;&#20026; $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$&#65292;&#36825;&#26159;&#36825;&#31181;&#31867;&#22411;&#30340;\emph{&#31532;&#19968;&#31687;}&#25991;&#31456;&#12290;$\nu_t^{1+\epsilon}$&#26159;&#31532; $t$ &#36718;&#22870;&#21169;&#30340; $(1+\epsilon)$-&#38454;&#20013;&#24515;&#30697;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#22312;&#24212;&#29992;&#20110; st &#30340;&#26368;&#22351;&#24773;&#20917;&#26102;&#65292;&#19978;&#36848;&#30028;&#26159;&#26497;&#23567;&#20540;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
While numerous works have focused on devising efficient algorithms for reinforcement learning (RL) with uniformly bounded rewards, it remains an open question whether sample or time-efficient algorithms for RL with large state-action space exist when the rewards are \emph{heavy-tailed}, i.e., with only finite $(1+\epsilon)$-th moments for some $\epsilon\in(0,1]$. In this work, we address the challenge of such rewards in RL with linear function approximation. We first design an algorithm, \textsc{Heavy-OFUL}, for heavy-tailed linear bandits, achieving an \emph{instance-dependent} $T$-round regret of $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$, the \emph{first} of this kind. Here, $d$ is the feature dimension, and $\nu_t^{1+\epsilon}$ is the $(1+\epsilon)$-th central moment of the reward at the $t$-th round. We further show the above bound is minimax optimal when applied to the worst-case instances in st
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#25913;&#36827;&#30340;RVQGAN&#36827;&#34892;&#39640;&#20445;&#30495;&#38899;&#39057;&#21387;&#32553;&#30340;&#31639;&#27861;&#65292;&#22312;&#20445;&#25345;&#38899;&#39057;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;90&#20493;&#30340;&#21387;&#32553;&#12290;&#36825;&#31181;&#31639;&#27861;&#32467;&#21512;&#20102;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#29983;&#25104;&#21644;&#22270;&#20687;&#39046;&#22495;&#30340;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#23545;&#25239;&#24615;&#21644;&#37325;&#24314;&#25439;&#22833;&#12290;&#36890;&#36807;&#20351;&#29992;&#21333;&#19968;&#36890;&#29992;&#27169;&#22411;&#23545;&#25152;&#26377;&#39046;&#22495;&#36827;&#34892;&#21387;&#32553;&#65292;&#35813;&#31639;&#27861;&#22312;&#38899;&#39057;&#29983;&#25104;&#24314;&#27169;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;&#19982;&#31454;&#20105;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06546</link><description>&lt;p&gt;
&#20351;&#29992;&#25913;&#36827;&#30340;RVQGAN&#36827;&#34892;&#39640;&#20445;&#30495;&#38899;&#39057;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
High-Fidelity Audio Compression with Improved RVQGAN. (arXiv:2306.06546v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#25913;&#36827;&#30340;RVQGAN&#36827;&#34892;&#39640;&#20445;&#30495;&#38899;&#39057;&#21387;&#32553;&#30340;&#31639;&#27861;&#65292;&#22312;&#20445;&#25345;&#38899;&#39057;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;90&#20493;&#30340;&#21387;&#32553;&#12290;&#36825;&#31181;&#31639;&#27861;&#32467;&#21512;&#20102;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#29983;&#25104;&#21644;&#22270;&#20687;&#39046;&#22495;&#30340;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#23545;&#25239;&#24615;&#21644;&#37325;&#24314;&#25439;&#22833;&#12290;&#36890;&#36807;&#20351;&#29992;&#21333;&#19968;&#36890;&#29992;&#27169;&#22411;&#23545;&#25152;&#26377;&#39046;&#22495;&#36827;&#34892;&#21387;&#32553;&#65292;&#35813;&#31639;&#27861;&#22312;&#38899;&#39057;&#29983;&#25104;&#24314;&#27169;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;&#19982;&#31454;&#20105;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#21151;&#22320;&#29992;&#20110;&#24314;&#27169;&#33258;&#28982;&#20449;&#21495;&#65292;&#22914;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#38899;&#20048;&#12290;&#20854;&#20013;&#20851;&#38190;&#32452;&#20214;&#26159;&#19968;&#20010;&#33021;&#23558;&#39640;&#32500;&#33258;&#28982;&#20449;&#21495;&#21387;&#32553;&#20026;&#36739;&#20302;&#32500;&#24230;&#31163;&#25955;&#20196;&#29260;&#30340;&#39640;&#36136;&#37327;&#31070;&#32463;&#21387;&#32553;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#20840;&#33021;&#31070;&#32463;&#38899;&#39057;&#21387;&#32553;&#31639;&#27861;&#65292;&#23558;44.1 KHz&#38899;&#39057;&#20197;8kbps&#30340;&#24102;&#23485;&#21387;&#32553;&#32422;90&#20493;&#12290;&#25105;&#20204;&#23558;&#39640;&#20445;&#30495;&#38899;&#39057;&#29983;&#25104;&#30340;&#36827;&#23637;&#19982;&#22270;&#20687;&#39046;&#22495;&#30340;&#26356;&#22909;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#25913;&#36827;&#23545;&#25239;&#24615;&#21644;&#37325;&#24314;&#25439;&#22833;&#12290;&#25105;&#20204;&#20351;&#29992;&#21333;&#19968;&#36890;&#29992;&#27169;&#22411;&#23545;&#25152;&#26377;&#39046;&#22495;&#65288;&#35821;&#38899;&#12289;&#29615;&#22659;&#12289;&#38899;&#20048;&#31561;&#65289;&#36827;&#34892;&#21387;&#32553;&#65292;&#20351;&#20854;&#24191;&#27867;&#36866;&#29992;&#20110;&#38899;&#39057;&#30340;&#29983;&#25104;&#24314;&#27169;&#12290;&#25105;&#20204;&#19982;&#31454;&#20105;&#30340;&#38899;&#39057;&#21387;&#32553;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#27599;&#20010;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#30340;&#23454;&#39564;&#65292;&#20197;&#21450;&#24320;&#28304;&#20195;&#30721;&#21644;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#26032;&#39062;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#30028;&#38480;&#26174;&#33879;&#25193;&#23637;&#20102;PAC-Bayesian&#30028;&#38480;&#30340;&#33539;&#22260;&#65292;&#24182;&#22312;&#32463;&#20856;&#30340;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.04375</link><description>&lt;p&gt;
&#22522;&#20110;Wasserstein&#30340;&#39640;&#27010;&#29575;&#27867;&#21270;&#30028;&#38480;&#19979;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning via Wasserstein-Based High Probability Generalisation Bounds. (arXiv:2306.04375v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#26032;&#39062;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#30028;&#38480;&#26174;&#33879;&#25193;&#23637;&#20102;PAC-Bayesian&#30028;&#38480;&#30340;&#33539;&#22260;&#65292;&#24182;&#22312;&#32463;&#20856;&#30340;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;SRM&#65289;&#20013;&#65292;&#26368;&#23567;&#21270;&#24635;&#20307;&#39118;&#38505;&#25110;&#27867;&#21270;&#24046;&#36317;&#19978;&#38480;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#36825;&#23588;&#20854;&#26159;PAC-Bayesian&#23398;&#20064;&#30340;&#26680;&#24515;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#20854;&#21462;&#24471;&#20102;&#25104;&#21151;&#24182;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;PAC-Bayesian&#26694;&#26550;&#30340;&#23616;&#38480;&#26159;&#22823;&#22810;&#25968;&#30028;&#38480;&#28041;&#21450;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#39033;&#65288;&#25110;&#20854;&#21464;&#21270;&#65289;&#65292;&#36825;&#21487;&#33021;&#34920;&#29616;&#20986;&#19981;&#35268;&#21017;&#34892;&#20026;&#24182;&#26080;&#27861;&#25429;&#25417;&#23398;&#20064;&#38382;&#39064;&#30340;&#24213;&#23618;&#20960;&#20309;&#32467;&#26500;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#20225;&#22270;&#29992;Wasserstein&#36317;&#31163;&#26367;&#25442;PAC-Bayesian&#30028;&#38480;&#20013;&#30340;KL&#25955;&#24230;&#12290;&#21363;&#20351;&#36825;&#20123;&#30028;&#38480;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#20445;&#25345;&#26399;&#26395;&#65292;&#35201;&#20040;&#23545;&#26377;&#30028;&#25439;&#22833;&#26377;&#25928;&#65292;&#35201;&#20040;&#38590;&#20197;&#22312;SRM&#26694;&#26550;&#20013;&#26368;&#23567;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#26032;&#39062;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#30028;&#38480;&#20197;&#26174;&#33879;&#24615;&#22320;&#25193;&#23637;&#20102;PAC-Bayesian&#30028;&#38480;&#30340;&#33539;&#22260;&#65292;&#24182;&#22312;&#20960;&#31181;&#32463;&#20856;&#30340;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#26032;&#30340;&#30028;&#38480;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26377;&#21069;&#36884;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimising upper bounds on the population risk or the generalisation gap has been widely used in structural risk minimisation (SRM) - this is in particular at the core of PAC-Bayesian learning. Despite its successes and unfailing surge of interest in recent years, a limitation of the PAC-Bayesian framework is that most bounds involve a Kullback-Leibler (KL) divergence term (or its variations), which might exhibit erratic behavior and fail to capture the underlying geometric structure of the learning problem - hence restricting its use in practical applications. As a remedy, recent studies have attempted to replace the KL divergence in the PAC-Bayesian bounds with the Wasserstein distance. Even though these bounds alleviated the aforementioned issues to a certain extent, they either hold in expectation, are for bounded losses, or are nontrivial to minimize in an SRM framework. In this work, we contribute to this line of research and prove novel Wasserstein distance-based PAC-Bayesian ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04220</link><description>&lt;p&gt;
&#22312;&#34920;&#38754;&#20043;&#19979;&#23547;&#25214;&#65306;&#21033;&#29992;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#21644;&#38590;&#20197;&#25511;&#21046;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#23567;&#19988;&#35206;&#30422;&#33539;&#22260;&#29421;&#31364;&#65292;&#20174;&#32780;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#37096;&#32626;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#19979;&#26174;&#33879;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;(T-symmetry)&#24378;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDM)&#65292;&#24314;&#31435;&#20102;&#19968;&#23545;&#27491;&#21521;&#21644;&#21453;&#21521;&#28508;&#22312;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;TDM&#20026;&#23567;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;T-symmetry&#30340;&#31526;&#21512;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;OOD&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
&lt;/p&gt;</description></item><item><title>FAMO&#26159;&#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21152;&#26435;&#26041;&#24335;&#23454;&#29616;&#24179;&#34913;&#30340;&#20219;&#21153;&#25439;&#22833;&#20943;&#23569;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26799;&#24230;&#25805;&#20316;&#25216;&#26415;&#20855;&#26377;&#30456;&#20284;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03792</link><description>&lt;p&gt;
FAMO&#65306;&#24555;&#36895;&#33258;&#36866;&#24212;&#22810;&#20219;&#21153;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
FAMO: Fast Adaptive Multitask Optimization. (arXiv:2306.03792v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03792
&lt;/p&gt;
&lt;p&gt;
FAMO&#26159;&#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21152;&#26435;&#26041;&#24335;&#23454;&#29616;&#24179;&#34913;&#30340;&#20219;&#21153;&#25439;&#22833;&#20943;&#23569;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26799;&#24230;&#25805;&#20316;&#25216;&#26415;&#20855;&#26377;&#30456;&#20284;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#38271;&#20037;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#33021;&#22815;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20174;&#22810;&#26679;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#22810;&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#25152;&#26377;&#20219;&#21153;&#30340;&#24179;&#22343;&#25439;&#22833;&#24212;&#29992;&#26799;&#24230;&#19979;&#38477;&#21487;&#33021;&#20250;&#30001;&#20110;&#26576;&#20123;&#20219;&#21153;&#30340;&#20005;&#37325;&#27424;&#20248;&#21270;&#32780;&#23548;&#33268;&#36739;&#24046;&#30340;&#22810;&#20219;&#21153;&#24615;&#33021;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#25805;&#32437;&#20219;&#21153;&#26799;&#24230;&#20197;&#33719;&#24471;&#26356;&#24179;&#34913;&#30340;&#25439;&#22833;&#20943;&#23569;&#65292;&#20294;&#38656;&#35201;&#23384;&#20648;&#21644;&#35745;&#31639;&#25152;&#26377;&#20219;&#21153;&#30340;&#26799;&#24230;&#65288;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#20026;O(k)&#65292;&#20854;&#20013;k&#26159;&#20219;&#21153;&#25968;&#37327;&#65289;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#20013;&#30340;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24555;&#36895;&#33258;&#36866;&#24212;&#22810;&#20219;&#21153;&#20248;&#21270;&#65288;FAMO&#65289;&#65292;&#19968;&#31181;&#21160;&#24577;&#21152;&#26435;&#26041;&#27861;&#65292;&#20197;O(1)&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#20197;&#24179;&#34913;&#30340;&#26041;&#24335;&#20943;&#23569;&#20219;&#21153;&#25439;&#22833;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#22810;&#20219;&#21153;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;FAMO&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#26799;&#24230;&#25805;&#20316;&#25216;&#26415;&#30456;&#23218;&#32654;&#25110;&#26356;&#20248;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the grand enduring goals of AI is to create generalist agents that can learn multiple different tasks from diverse data via multitask learning (MTL). However, in practice, applying gradient descent (GD) on the average loss across all tasks may yield poor multitask performance due to severe under-optimization of certain tasks. Previous approaches that manipulate task gradients for a more balanced loss decrease require storing and computing all task gradients ($\mathcal{O}(k)$ space and time where $k$ is the number of tasks), limiting their use in large-scale scenarios. In this work, we introduce Fast Adaptive Multitask Optimization FAMO, a dynamic weighting method that decreases task losses in a balanced way using $\mathcal{O}(1)$ space and time. We conduct an extensive set of experiments covering multi-task supervised and reinforcement learning problems. Our results indicate that FAMO achieves comparable or superior performance to state-of-the-art gradient manipulation technique
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#21644;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#30340;&#39118;&#38505;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#27169;&#22411;&#32500;&#24230;&#22686;&#38271;&#36895;&#24230;&#22823;&#20110;&#20219;&#20309;&#24120;&#25968;&#20493;&#30340;&#26679;&#26412;&#25968;&#26102;&#23427;&#20204;&#20043;&#38388;&#30340;&#28176;&#36817;&#19968;&#33268;&#24615;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#36825;&#20004;&#20010;&#25968;&#37327;&#22312;&#38480;&#23450;&#32500;&#24230;&#19978;&#20855;&#26377;&#39640;&#26031;&#27874;&#21160;&#65292;&#24182;&#34920;&#29616;&#20986;&#30456;&#20284;&#30340;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03783</link><description>&lt;p&gt;
&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#20013;&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28176;&#36817;&#24615;
&lt;/p&gt;
&lt;p&gt;
Asymptotics of Bayesian Uncertainty Estimation in Random Features Regression. (arXiv:2306.03783v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03783
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#21644;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#30340;&#39118;&#38505;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#27169;&#22411;&#32500;&#24230;&#22686;&#38271;&#36895;&#24230;&#22823;&#20110;&#20219;&#20309;&#24120;&#25968;&#20493;&#30340;&#26679;&#26412;&#25968;&#26102;&#23427;&#20204;&#20043;&#38388;&#30340;&#28176;&#36817;&#19968;&#33268;&#24615;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#36825;&#20004;&#20010;&#25968;&#37327;&#22312;&#38480;&#23450;&#32500;&#24230;&#19978;&#20855;&#26377;&#39640;&#26031;&#27874;&#21160;&#65292;&#24182;&#34920;&#29616;&#20986;&#30456;&#20284;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#36125;&#21494;&#26031;&#22238;&#24402;&#27169;&#22411;&#20013;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#21644;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#65288;MAP&#65289;&#39118;&#38505;&#22312;&#36229;&#21442;&#25968;&#21270;&#21306;&#22495;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#20851;&#27880;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65288;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#65289;&#30340;&#26041;&#24046;&#65292;&#24182;&#23558;&#20854;&#28176;&#36817;&#24615;&#19982;MAP&#20272;&#35745;&#22120;&#30340;&#39118;&#38505;&#36827;&#34892;&#27604;&#36739;&#12290;&#24403;&#27169;&#22411;&#32500;&#24230;&#22686;&#38271;&#36895;&#24230;&#22823;&#20110;&#20219;&#20309;&#24120;&#25968;&#20493;&#30340;&#26679;&#26412;&#25968;&#26102;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#28176;&#36817;&#19968;&#33268;&#24615;&#21463;&#21040;&#20449;&#22122;&#27604;&#30340;&#30456;&#21464;&#30340;&#25511;&#21046;&#12290;&#24403;&#26679;&#26412;&#25968;&#22686;&#38271;&#36895;&#24230;&#22823;&#20110;&#20219;&#20309;&#24120;&#25968;&#20493;&#30340;&#27169;&#22411;&#32500;&#24230;&#26102;&#65292;&#23427;&#20204;&#20063;&#20250;&#28176;&#36817;&#19968;&#33268;&#12290;&#25968;&#20540;&#27169;&#25311;&#35828;&#26126;&#20102;&#20004;&#20010;&#25968;&#37327;&#30340;&#26377;&#38480;&#32500;&#20998;&#24067;&#24615;&#36136;&#12290;&#25105;&#20204;&#25512;&#27979;&#23427;&#20204;&#20855;&#26377;&#39640;&#26031;&#27874;&#21160;&#65292;&#24182;&#34920;&#29616;&#20986;&#19982;&#20043;&#21069;&#22312;&#39640;&#26031;&#24207;&#21015;&#27169;&#22411;&#20013;&#21457;&#29616;&#30340;&#31867;&#20284;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we compare and contrast the behavior of the posterior predictive distribution to the risk of the maximum a posteriori estimator for the random features regression model in the overparameterized regime. We will focus on the variance of the posterior predictive distribution (Bayesian model average) and compare its asymptotics to that of the risk of the MAP estimator. In the regime where the model dimensions grow faster than any constant multiple of the number of samples, asymptotic agreement between these two quantities is governed by the phase transition in the signal-to-noise ratio. They also asymptotically agree with each other when the number of samples grow faster than any constant multiple of model dimensions. Numerical simulations illustrate finer distributional properties of the two quantities for finite dimensions. We conjecture they have Gaussian fluctuations and exhibit similar properties as found by previous authors in a Gaussian sequence model, which is of inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$(k, t)$-FWL&#21644;$k$-FWL+&#20004;&#31181;&#26041;&#27861;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#21487;&#20197;&#22312;$O(n^2)$&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#19979;&#65292;&#35299;&#20915;&#22270;&#21516;&#26500;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03266</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#27665;&#38388;&#23041;&#26031;&#36153;&#21202;-&#33713;&#26364;&#31639;&#27861;&#65292;&#23454;&#29616;$O(n^2)$&#31354;&#38388;&#20869;&#20219;&#24847;&#34920;&#36798;&#33021;&#21147;&#30340;GNNs
&lt;/p&gt;
&lt;p&gt;
Towards Arbitrarily Expressive GNNs in $O(n^2)$ Space by Rethinking Folklore Weisfeiler-Lehman. (arXiv:2306.03266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$(k, t)$-FWL&#21644;$k$-FWL+&#20004;&#31181;&#26041;&#27861;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#21487;&#20197;&#22312;$O(n^2)$&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#19979;&#65292;&#35299;&#20915;&#22270;&#21516;&#26500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#24050;&#25104;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20854;&#34920;&#36798;&#33021;&#21147;&#21463;&#21040;&#19968;&#32500;&#23041;&#26031;&#36153;&#21202;-&#33713;&#26364;&#65288;1-WL&#65289;&#27979;&#35797;&#30340;&#38480;&#21046;&#12290;&#19968;&#20123;&#30740;&#31350;&#21463;&#21040;$k$-WL/FWL&#65288;&#27665;&#38388;WL&#65289;&#30340;&#21551;&#21457;&#24182;&#35774;&#35745;&#20854;&#30456;&#24212;&#30340;&#31070;&#32463;&#29256;&#26412;&#12290;&#23613;&#31649;&#20855;&#26377;&#24456;&#39640;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#23384;&#22312;&#20005;&#37325;&#23616;&#38480;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;$(k, t)$-FWL&#21644;$k$-FWL+&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing neural networks (MPNNs) have emerged as the most popular framework of graph neural networks (GNNs) in recent years. However, their expressive power is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Some works are inspired by $k$-WL/FWL (Folklore WL) and design the corresponding neural versions. Despite the high expressive power, there are serious limitations in this line of research. In particular, (1) $k$-WL/FWL requires at least $O(n^k)$ space complexity, which is impractical for large graphs even when $k=3$; (2) The design space of $k$-WL/FWL is rigid, with the only adjustable hyper-parameter being $k$. To tackle the first limitation, we propose an extension, $(k, t)$-FWL. We theoretically prove that even if we fix the space complexity to $O(n^2)$ in $(k, t)$-FWL, we can construct an expressiveness hierarchy up to solving the graph isomorphism problem. To tackle the second problem, we propose $k$-FWL+, which considers any equivariant set as neighbors ins
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#23384;&#22312;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#21512;&#24182;&#21518;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01708</link><description>&lt;p&gt;
&#21512;&#24182;&#27169;&#22411;&#26102;&#22914;&#20309;&#35299;&#20915;&#24178;&#25200;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Resolving Interference When Merging Models. (arXiv:2306.01708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#23384;&#22312;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#21512;&#24182;&#21518;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#25913;&#36827;&#19979;&#28216;&#24615;&#33021;&#65292;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#21644;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24050;&#26377;&#30340;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#23548;&#33268;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#26102;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#20808;&#21069;&#30340;&#21512;&#24182;&#25216;&#26415;&#30001;&#20110;&#20004;&#20010;&#20027;&#35201;&#24178;&#25200;&#26469;&#28304;&#32780;&#19981;&#24910;&#20002;&#22833;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65306;(a)&#20887;&#20313;&#21442;&#25968;&#20540;&#24341;&#36215;&#30340;&#24178;&#25200;&#21644;(b)&#34920;&#31034;&#21516;&#19968;&#21442;&#25968;&#20540;&#30340;&#31526;&#21495;&#22312;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#24615;&#32467;&#26524;&#30340;&#37096;&#20998;&#21453;&#20107;&#23454;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25935;&#24863;&#24615;&#27169;&#22411;&#8212;&#8212;&#26354;&#29575;&#25935;&#24863;&#27169;&#22411;&#65292;&#36890;&#36807;&#38480;&#21046;&#20989;&#25968;&#32423;&#38598;&#30340;&#26354;&#29575;&#26469;&#33719;&#24471;&#20449;&#24687;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.01424</link><description>&lt;p&gt;
&#24102;&#26354;&#29575;&#25935;&#24863;&#27169;&#22411;&#30340;&#36830;&#32493;&#24615;&#32467;&#26524;&#30340;&#37096;&#20998;&#21453;&#20107;&#23454;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model. (arXiv:2306.01424v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#24615;&#32467;&#26524;&#30340;&#37096;&#20998;&#21453;&#20107;&#23454;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25935;&#24863;&#24615;&#27169;&#22411;&#8212;&#8212;&#26354;&#29575;&#25935;&#24863;&#27169;&#22411;&#65292;&#36890;&#36807;&#38480;&#21046;&#20989;&#25968;&#32423;&#38598;&#30340;&#26354;&#29575;&#26469;&#33719;&#24471;&#20449;&#24687;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#26029;&#26088;&#22312;&#22238;&#31572;&#8220;&#22914;&#26524;&#8221;&#38382;&#39064;&#65292;&#22240;&#27492;&#23646;&#20110;Pearl&#22240;&#26524;&#20851;&#31995;&#38454;&#26799;&#20013;&#26368;&#31934;&#32454;&#30340;&#25512;&#29702;&#31867;&#22411;&#12290;&#29616;&#26377;&#30340;&#38024;&#23545;&#20855;&#26377;&#36830;&#32493;&#32467;&#26524;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#26041;&#27861;&#26088;&#22312;&#36827;&#34892;&#28857;&#35782;&#21035;&#65292;&#22240;&#27492;&#23545;&#22522;&#30784;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#36827;&#34892;&#20102;&#24378;&#26377;&#21147;&#19988;&#19981;&#33258;&#28982;&#30340;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#26088;&#22312;&#36827;&#34892;&#36830;&#32493;&#32467;&#26524;&#30340;&#37096;&#20998;&#21453;&#20107;&#23454;&#35782;&#21035;&#65292;&#21363;&#24403;&#21453;&#20107;&#23454;&#26597;&#35810;&#23384;&#22312;&#20855;&#26377;&#20449;&#24687;&#36793;&#30028;&#30340;&#26080;&#30693;&#21306;&#38388;&#20013;&#26102;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#26159;&#36830;&#32493;&#21487;&#24494;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20989;&#25968;&#30340;&#32423;&#38598;&#30340;&#26354;&#29575;&#20063;&#26159;&#38750;&#20449;&#24687;&#30340;&#65292;&#21453;&#20107;&#23454;&#26597;&#35810;&#30340;&#26080;&#30693;&#21306;&#38388;&#20063;&#26159;&#38750;&#20449;&#24687;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25935;&#24863;&#24615;&#27169;&#22411;&#31216;&#20026;&#26354;&#29575;&#25935;&#24863;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#38480;&#21046;&#20989;&#25968;&#32423;&#38598;&#30340;&#26354;&#29575;&#26469;&#33719;&#24471;&#20449;&#24687;&#36793;&#30028;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#28857;&#21453;&#20107;&#23454;&#35782;&#21035;&#26041;&#27861;&#21487;&#20197;&#35270;&#20026;&#25105;&#20204;&#25552;&#20986;&#26694;&#26550;&#30340;&#29305;&#23450;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual inference aims to answer retrospective ''what if'' questions and thus belongs to the most fine-grained type of inference in Pearl's causality ladder. Existing methods for counterfactual inference with continuous outcomes aim at point identification and thus make strong and unnatural assumptions about the underlying structural causal model. In this paper, we relax these assumptions and aim at partial counterfactual identification of continuous outcomes, i.e., when the counterfactual query resides in an ignorance interval with informative bounds. We prove that, in general, the ignorance interval of the counterfactual queries has non-informative bounds, already when functions of structural causal models are continuously differentiable. As a remedy, we propose a novel sensitivity model called Curvature Sensitivity Model. This allows us to obtain informative bounds by bounding the curvature of level sets of the functions. We further show that existing point counterfactual ide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#24310;&#36831;&#21644;&#32570;&#22833;&#29366;&#24577;&#35266;&#23519;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#24182;&#24314;&#31435;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#36793;&#30028;&#65292;&#23613;&#31649;&#26377;&#25439;&#21487;&#35266;&#23519;&#24615;&#23545;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#31867;&#21035;&#36896;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20294;&#23398;&#20064;&#20173;&#28982;&#26159;&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.01243</link><description>&lt;p&gt;
&#26377;&#25439;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#65306;&#23398;&#20064;&#22312;&#24310;&#36831;&#21644;&#32570;&#22833;&#29615;&#22659;&#20013;&#20570;&#20986;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations. (arXiv:2306.01243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#24310;&#36831;&#21644;&#32570;&#22833;&#29366;&#24577;&#35266;&#23519;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#24182;&#24314;&#31435;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#36793;&#30028;&#65292;&#23613;&#31649;&#26377;&#25439;&#21487;&#35266;&#23519;&#24615;&#23545;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#31867;&#21035;&#36896;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20294;&#23398;&#20064;&#20173;&#28982;&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#21508;&#31181;&#24418;&#24335;&#30340;&#26377;&#25439;&#21487;&#35266;&#23519;&#24615;&#21487;&#33021;&#20250;&#20351;&#38382;&#39064;&#21464;&#24471;&#22797;&#26434;&#12290;&#24403;&#20195;&#29702;&#30001;&#20110;&#24310;&#36831;&#25110;&#20002;&#22833;&#30340;&#36890;&#36947;&#32780;&#26080;&#27861;&#35266;&#23519;&#21040;&#31995;&#32479;&#30340;&#26368;&#26032;&#29366;&#24577;&#26102;&#65292;&#36825;&#20123;&#24773;&#20917;&#20250;&#20986;&#29616;&#65292;&#20294;&#20195;&#29702;&#20173;&#24517;&#39035;&#20570;&#20986;&#23454;&#26102;&#20915;&#31574;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#22312;&#25511;&#21046;&#31995;&#32479;&#20013;&#36827;&#34892;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#65292;&#20854;&#20013;&#20195;&#29702;&#24517;&#39035;&#22312;&#24310;&#36831;&#21644;&#32570;&#22833;&#29366;&#24577;&#35266;&#23519;&#29615;&#22659;&#20013;&#25805;&#20316;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#36793;&#30028;&#65292;&#24418;&#24335;&#20026;$ \tilde{\mathcal{O}}(\sqrt{{\rm poly}(H) SAK}) $&#65292;&#20197;&#22312;&#24310;&#36831;&#21644;&#32570;&#22833;&#35266;&#23519;&#35774;&#32622;&#19979;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#12290;&#23613;&#31649;&#26377;&#25439;&#21487;&#35266;&#23519;&#24615;&#23545;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#31867;&#21035;&#36896;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#23398;&#20064;&#20173;&#28982;&#26159;&#39640;&#25928;&#30340;&#65292;&#36951;&#25022;&#36793;&#30028;&#26368;&#20248;&#22320;&#20381;&#36182;&#20110;&#21407;&#22987;&#31995;&#32479;&#30340;&#29366;&#24577;-&#21160;&#20316;&#22823;&#23567;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21463;&#24433;&#21709;&#30340;&#35266;&#23519;&#19979;&#26368;&#20339;&#31574;&#30053;&#30340;&#24615;&#33021;&#19982;&#26368;&#20248;&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world reinforcement learning (RL) systems, various forms of impaired observability can complicate matters. These situations arise when an agent is unable to observe the most recent state of the system due to latency or lossy channels, yet the agent must still make real-time decisions. This paper introduces a theoretical investigation into efficient RL in control systems where agents must act with delayed and missing state observations. We establish near-optimal regret bounds, of the form $\tilde{\mathcal{O}}(\sqrt{{\rm poly}(H) SAK})$, for RL in both the delayed and missing observation settings. Despite impaired observability posing significant challenges to the policy class and planning, our results demonstrate that learning remains efficient, with the regret bound optimally depending on the state-action size of the original system. Additionally, we provide a characterization of the performance of the optimal policy under impaired observability, comparing it to the optimal val
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#30340;&#24605;&#32500;&#26469;&#35757;&#32451;AI&#20195;&#29702;&#65292;&#20197;&#22312;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#31561;&#33021;&#21147;&#26041;&#38754;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.00323</link><description>&lt;p&gt;
&#8220;&#24605;&#32500;&#20811;&#38534;&#65306;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#23398;&#20064;&#24605;&#32771;&#24182;&#34892;&#21160;&#8221;&#12290;&#65288;arXiv:2306.00323v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Thought Cloning: Learning to Think while Acting by Imitating Human Thinking. (arXiv:2306.00323v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#30340;&#24605;&#32500;&#26469;&#35757;&#32451;AI&#20195;&#29702;&#65292;&#20197;&#22312;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#31561;&#33021;&#21147;&#26041;&#38754;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#38750;&#20961;&#30340;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#12289;&#37325;&#26032;&#35268;&#21010;&#21644;&#36866;&#24212;&#26032;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#22312;&#36825;&#20123;&#33021;&#21147;&#20013;&#36828;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20551;&#35774;&#20854;&#20013;&#19968;&#20010;&#35748;&#30693;&#32570;&#38519;&#30340;&#21407;&#22240;&#26159;&#20182;&#20204;&#32570;&#20047;&#20351;&#29992;&#35821;&#35328;&#24605;&#32771;&#25152;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#35757;&#32451;AI&#20195;&#29702;&#20154;&#20687;&#20154;&#31867;&#19968;&#26679;&#24605;&#32771;&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#20854;&#24819;&#27861;&#19981;&#20165;&#26159;&#20811;&#38534;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#34892;&#20026;&#65292;&#32780;&#19988;&#36824;&#21253;&#25324;&#20154;&#31867;&#22312;&#25191;&#34892;&#36825;&#20123;&#34892;&#20026;&#26102;&#25152;&#20135;&#29983;&#30340;&#24819;&#27861;&#12290;&#34429;&#28982;&#25105;&#20204;&#24076;&#26395;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#22312;&#22788;&#29702;&#32593;&#32476;&#35268;&#27169;&#30340;&#20154;&#31867;&#24605;&#32500;&#21644;&#34892;&#20026;&#25968;&#25454;&#26102;&#33021;&#22815;&#21457;&#25381;&#20986;&#33394;&#65288;&#20363;&#22914;&#65292;&#24102;&#26377;&#21095;&#26412;&#30340;&#22312;&#32447;&#35270;&#39057;&#65289;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22312;&#24605;&#32771;&#21644;&#34892;&#21160;&#25968;&#25454;&#20026;&#21512;&#25104;&#29983;&#25104;&#30340;&#39046;&#22495;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#23398;&#20064;&#36895;&#24230;&#27604;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;</title><link>http://arxiv.org/abs/2306.00006</link><description>&lt;p&gt;
&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65306;&#29992;&#20110;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#30340;&#21333;&#31867;&#21516;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#65288;GAD&#65289;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#21457;&#29616;&#19968;&#31181;&#26222;&#36941;&#30340;&#23646;&#24615;......&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327; - &#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;......&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270; (TAM)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19982;_neighbors&#30340;&#26412;&#22320;&#20146;&#21644;&#21147;&#26469;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#26412;&#25991;&#25152;&#25552;&#26041;&#27861;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#21487;&#20197;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;
One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#26032;&#39062;&#21487;&#38752;&#30340;&#33073;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#23454;&#29616;&#65306;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#20010;&#24615;&#21270;&#30340;&#21058;&#37327;-&#21453;&#24212;&#65292;&#20272;&#35745;&#20542;&#21521;&#24471;&#20998;&#26816;&#27979;&#20849;&#20139;&#21327;&#21464;&#37327;-&#27835;&#30103;&#31354;&#38388;&#20013;&#30340;&#37325;&#21472;&#26377;&#38480;&#21306;&#22495;&#65292;&#28982;&#21518;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#25214;&#21040;&#26368;&#20339;&#30340;&#20010;&#24615;&#21270;&#21058;&#37327;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.19742</link><description>&lt;p&gt;
&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#21487;&#38752;&#33073;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reliable Off-Policy Learning for Dosage Combinations. (arXiv:2305.19742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#26032;&#39062;&#21487;&#38752;&#30340;&#33073;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#23454;&#29616;&#65306;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#20010;&#24615;&#21270;&#30340;&#21058;&#37327;-&#21453;&#24212;&#65292;&#20272;&#35745;&#20542;&#21521;&#24471;&#20998;&#26816;&#27979;&#20849;&#20139;&#21327;&#21464;&#37327;-&#27835;&#30103;&#31354;&#38388;&#20013;&#30340;&#37325;&#21472;&#26377;&#38480;&#21306;&#22495;&#65292;&#28982;&#21518;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#25214;&#21040;&#26368;&#20339;&#30340;&#20010;&#24615;&#21270;&#21058;&#37327;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#21307;&#23398;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#30284;&#30151;&#27835;&#30103;&#25110;&#21361;&#37325;&#25252;&#29702;&#65292;&#36890;&#24120;&#24517;&#39035;&#23545;&#21058;&#37327;&#32452;&#21512;&#36827;&#34892;&#36873;&#25321;&#65292;&#21363;&#22810;&#31181;&#36830;&#32493;&#27835;&#30103;&#12290;&#29616;&#26377;&#30340;&#36825;&#39033;&#20219;&#21153;&#30340;&#24037;&#20316;&#24050;&#32463;&#29420;&#31435;&#22320;&#24314;&#27169;&#20102;&#22810;&#31181;&#27835;&#30103;&#30340;&#25928;&#26524;&#65292;&#32780;&#20272;&#35745;&#32852;&#21512;&#25928;&#26524;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#38754;&#20020;&#30528;&#38750;&#24179;&#20961;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#21487;&#38752;&#33073;&#26426;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20272;&#35745;&#20010;&#24615;&#21270;&#30340;&#21058;&#37327;-&#21453;&#24212;&#20989;&#25968;&#65292;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#30456;&#20851;&#21058;&#37327;&#30340;&#32852;&#21512;&#25928;&#24212;&#12290;&#65288;2&#65289;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#37327;&#20272;&#35745;&#24191;&#20041;&#20542;&#21521;&#24471;&#20998;&#65292;&#20197;&#26816;&#27979;&#20849;&#20139;&#21327;&#21464;&#37327;-&#27835;&#30103;&#31354;&#38388;&#20013;&#37325;&#21472;&#26377;&#38480;&#30340;&#21306;&#22495;&#12290;&#65288;3&#65289;&#25105;&#20204;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#20010;&#24615;&#21270;&#21058;&#37327;&#32452;&#21512;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#30830;&#20445;&#21487;&#38752;&#22320;&#20272;&#35745;&#31574;&#30053;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-making in personalized medicine such as cancer therapy or critical care must often make choices for dosage combinations, i.e., multiple continuous treatments. Existing work for this task has modeled the effect of multiple treatments independently, while estimating the joint effect has received little attention but comes with non-trivial challenges. In this paper, we propose a novel method for reliable off-policy learning for dosage combinations. Our method proceeds along three steps: (1) We develop a tailored neural network that estimates the individualized dose-response function while accounting for the joint effect of multiple dependent dosages. (2) We estimate the generalized propensity score using conditional normalizing flows in order to detect regions with limited overlap in the shared covariate-treatment space. (3) We present a gradient-based learning algorithm to find the optimal, individualized dosage combinations. Here, we ensure reliable estimation of the policy val
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#19968;&#31181;&#21487;&#33021;&#8220;&#35268;&#33539;&#19981;&#27491;&#30830;&#8221;&#27169;&#22411;&#30340;&#36890;&#29992;&#21327;&#35758;&#65292;&#8220;&#25554;&#20214;&#24335;&#34920;&#29616;&#20248;&#21270;&#8221;&#12290;</title><link>http://arxiv.org/abs/2305.18728</link><description>&lt;p&gt;
&#25554;&#20214;&#21270;&#34920;&#29616;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Plug-in Performative Optimization. (arXiv:2305.18728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18728
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#19968;&#31181;&#21487;&#33021;&#8220;&#35268;&#33539;&#19981;&#27491;&#30830;&#8221;&#27169;&#22411;&#30340;&#36890;&#29992;&#21327;&#35758;&#65292;&#8220;&#25554;&#20214;&#24335;&#34920;&#29616;&#20248;&#21270;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#39044;&#27979;&#20855;&#26377;&#34920;&#29616;&#24615;&#26102;&#65292;&#36873;&#25321;&#21738;&#20010;&#39044;&#27979;&#22120;&#37096;&#32626;&#23558;&#24433;&#21709;&#26410;&#26469;&#35266;&#27979;&#30340;&#20998;&#24067;&#12290;&#22312;&#34920;&#29616;&#24615;&#23398;&#20064;&#20013;&#65292;&#24635;&#20307;&#30446;&#26631;&#26159;&#25214;&#21040;&#20855;&#26377;&#20302;&#8220;&#34920;&#29616;&#24615;&#39118;&#38505;&#8221;&#30340;&#39044;&#27979;&#22120;&#65292;&#21363;&#22312;&#20854;&#24341;&#23548;&#30340;&#20998;&#24067;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#26368;&#20248;&#21270;&#34920;&#29616;&#24615;&#39118;&#38505;&#30340;&#19968;&#31995;&#21015;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#36172;&#24466;&#31639;&#27861;&#21644;&#20854;&#20182;&#26080;&#23548;&#25968;&#26041;&#27861;&#65292;&#22312;&#34920;&#29616;&#24615;&#21453;&#39304;&#20013;&#19981;&#30693;&#36947;&#20219;&#20309;&#32467;&#26500;&#65292;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#26497;&#24930;&#12290;&#34917;&#20805;&#30340;&#19968;&#31995;&#21015;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#21453;&#39304;&#20013;&#30340;&#26174;&#24335;&#8220;&#27169;&#22411;&#8221;&#65292;&#20363;&#22914;&#25112;&#30053;&#20998;&#31867;&#20013;&#30340;&#26368;&#20339;&#21709;&#24212;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#36895;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36895;&#29575;&#20851;&#38190;&#20381;&#36182;&#20110;&#21453;&#39304;&#27169;&#22411;&#30340;&#35268;&#33539;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21551;&#21160;&#20102;&#23545;&#22312;&#34920;&#29616;&#24615;&#39044;&#27979;&#20013;&#20351;&#29992;&#21487;&#33021;&#30340;&#8220;&#35268;&#33539;&#19981;&#27491;&#30830;&#8221;&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#22411;&#30340;&#36890;&#29992;&#21327;&#35758;&#65292;&#31216;&#20026;&#8220;&#25554;&#20214;&#24335;&#34920;&#29616;&#20248;&#21270;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
When predictions are performative, the choice of which predictor to deploy influences the distribution of future observations. The overarching goal in learning under performativity is to find a predictor that has low \emph{performative risk}, that is, good performance on its induced distribution. One family of solutions for optimizing the performative risk, including bandits and other derivative-free methods, is agnostic to any structure in the performative feedback, leading to exceedingly slow convergence rates. A complementary family of solutions makes use of explicit \emph{models} for the feedback, such as best-response models in strategic classification, enabling significantly faster rates. However, these rates critically rely on the feedback model being well-specified. In this work we initiate a study of the use of possibly \emph{misspecified} models in performative prediction. We study a general protocol for making use of models, called \emph{plug-in performative optimization}, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#37326;&#22806;&#35270;&#39057;&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#21270;&#19990;&#30028;&#27169;&#22411;&#65288;ContextWM&#65289;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;ContextWM&#37319;&#29992;&#19978;&#19979;&#25991;&#21270;&#25193;&#23637;&#30340;&#28508;&#22312;&#21160;&#24577;&#27169;&#22411;&#24314;&#27169;&#65292;&#20174;&#32780;&#21487;&#20197;&#26356;&#22909;&#22320;&#27867;&#21270;&#19981;&#21516;&#22330;&#26223;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#25552;&#39640;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25511;&#21046;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18499</link><description>&lt;p&gt;
&#22522;&#20110;&#37326;&#22806;&#35270;&#39057;&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#21270;&#19990;&#30028;&#27169;&#22411;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning. (arXiv:2305.18499v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#37326;&#22806;&#35270;&#39057;&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#21270;&#19990;&#30028;&#27169;&#22411;&#65288;ContextWM&#65289;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;ContextWM&#37319;&#29992;&#19978;&#19979;&#25991;&#21270;&#25193;&#23637;&#30340;&#28508;&#22312;&#21160;&#24577;&#27169;&#22411;&#24314;&#27169;&#65292;&#20174;&#32780;&#21487;&#20197;&#26356;&#22909;&#22320;&#27867;&#21270;&#19981;&#21516;&#22330;&#26223;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#25552;&#39640;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25511;&#21046;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#24050;&#22312;&#21508;&#31181;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#35843;&#26597;&#20102;&#36825;&#31181;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#20165;&#38480;&#20110;&#29305;&#23450;&#39046;&#22495;&#25110;&#27169;&#25311;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#37327;&#37326;&#22806;&#35270;&#39057;&#36827;&#34892;&#39044;&#35757;&#32451;&#19990;&#30028;&#27169;&#22411;&#65292;&#20197;&#39640;&#25928;&#23398;&#20064;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#37326;&#22806;&#35270;&#39057;&#23384;&#22312;&#21508;&#31181;&#19978;&#19979;&#25991;&#22240;&#32032;&#65292;&#22914;&#38169;&#32508;&#22797;&#26434;&#30340;&#32972;&#26223;&#21644;&#32441;&#29702;&#22806;&#35266;&#65292;&#36825;&#20351;&#24471;&#19990;&#30028;&#27169;&#22411;&#26080;&#27861;&#25552;&#21462;&#20849;&#20139;&#30340;&#19990;&#30028;&#30693;&#35782;&#20197;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#21270;&#19990;&#30028;&#27169;&#22411;&#65288;ContextWM&#65289;&#65292;&#26174;&#24335;&#22320;&#23545;&#19978;&#19979;&#25991;&#21644;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#20811;&#26381;&#37326;&#22806;&#35270;&#39057;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#20419;&#36827;&#19981;&#21516;&#22330;&#26223;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#21270;&#25193;&#23637;&#30340;&#28508;&#22312;&#21160;&#24577;&#27169;&#22411;&#26469;&#25429;&#25417;&#39640;&#32423;&#29366;&#24577;&#21644;&#20302;&#32423;&#35266;&#23519;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#37326;&#22806;&#35270;&#39057;&#23545;ContextWM&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#27604;&#65292;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25511;&#21046;&#24615;&#33021;&#22343;&#24471;&#21040;&#20102;&#26174;&#30528;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training methods utilizing large and diverse datasets have achieved tremendous success across a range of domains. Recent work has investigated such unsupervised pre-training methods for model-based reinforcement learning (MBRL) but is limited to domain-specific or simulated data. In this paper, we study the problem of pre-training world models with abundant in-the-wild videos for efficient learning of downstream visual control tasks. However, in-the-wild videos are complicated with various contextual factors, such as intricate backgrounds and textured appearance, which precludes a world model from extracting shared world knowledge to generalize better. To tackle this issue, we introduce Contextualized World Models (ContextWM) that explicitly model both the context and dynamics to overcome the complexity and diversity of in-the-wild videos and facilitate knowledge transfer between distinct scenes. Specifically, a contextualized extension of the latent dynamics model is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33041;&#40836;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#30382;&#36136;&#21402;&#24230;&#29305;&#24449;&#25429;&#25417;&#21152;&#36895;&#32769;&#21270;&#65292;&#24182;&#21453;&#26144;&#20986;&#22686;&#21152;&#30340;&#31070;&#32463;&#30142;&#30149;&#25110;&#35748;&#30693;&#38556;&#30861;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.18370</link><description>&lt;p&gt;
&#20351;&#29992;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33041;&#40836;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Brain Age Prediction using coVariance Neural Networks. (arXiv:2305.18370v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33041;&#40836;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#30382;&#36136;&#21402;&#24230;&#29305;&#24449;&#25429;&#25417;&#21152;&#36895;&#32769;&#21270;&#65292;&#24182;&#21453;&#26144;&#20986;&#22686;&#21152;&#30340;&#31070;&#32463;&#30142;&#30149;&#25110;&#35748;&#30693;&#38556;&#30861;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34987;&#29992;&#20110;&#21033;&#29992;&#33041;&#25104;&#20687;&#25968;&#25454;&#20026;&#20010;&#20307;&#25552;&#20379;&#8220;&#33041;&#40836;&#8221;&#20272;&#35745;&#12290;&#30001;&#20110;&#33041;&#40836;&#19982;&#23454;&#38469;&#24180;&#40836;&#23384;&#22312;&#24046;&#24322;&#65288;&#31216;&#20026;&#8220;&#33041;&#40836;&#24046;&#8221;&#65289;&#65292;&#22240;&#27492;&#21487;&#20197;&#25429;&#25417;&#30001;&#20110;&#19981;&#33391;&#20581;&#24247;&#29366;&#20917;&#23548;&#33268;&#30340;&#21152;&#36895;&#32769;&#21270;&#65292;&#24182;&#22240;&#27492;&#21453;&#26144;&#20986;&#22686;&#21152;&#30340;&#31070;&#32463;&#30142;&#30149;&#25110;&#35748;&#30693;&#38556;&#30861;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#33041;&#40836;&#39044;&#27979;&#31639;&#27861;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#26041;&#27861;&#35770;&#20381;&#25454;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476; (VNN)&#26469;&#25552;&#20986;&#19968;&#31181;&#35299;&#21078;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#30382;&#36136;&#21402;&#24230;&#29305;&#24449;&#36827;&#34892;&#33041;&#40836;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#33041;&#40836;&#39044;&#27979;&#26694;&#26550;&#19981;&#20165;&#25193;&#23637;&#21040;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149; (AD) &#20013;&#33041;&#40836;&#24046;&#30340;&#31895;&#30053;&#25351;&#26631;&#65292;&#32780;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computational neuroscience, there has been an increased interest in developing machine learning algorithms that leverage brain imaging data to provide estimates of "brain age" for an individual. Importantly, the discordance between brain age and chronological age (referred to as "brain age gap") can capture accelerated aging due to adverse health conditions and therefore, can reflect increased vulnerability towards neurological disease or cognitive impairments. However, widespread adoption of brain age for clinical decision support has been hindered due to lack of transparency and methodological justifications in most existing brain age prediction algorithms. In this paper, we leverage coVariance neural networks (VNN) to propose an anatomically interpretable framework for brain age prediction using cortical thickness features. Specifically, our brain age prediction framework extends beyond the coarse metric of brain age gap in Alzheimer's disease (AD) and we make two important obser
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#21305;&#37197;&#25216;&#26415;&#30340;&#21518;&#39564;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#25512;&#29702;&#65292;&#36890;&#36807;&#25552;&#20379;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#24341;&#21147;&#27874;&#25512;&#26029;&#19978;&#21462;&#24471;&#20102;&#21487;&#27604;&#31163;&#25955;&#27969;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17161</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#27169;&#25311;&#25512;&#29702;&#30340;&#27969;&#21305;&#37197;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flow Matching for Scalable Simulation-Based Inference. (arXiv:2305.17161v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17161
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#21305;&#37197;&#25216;&#26415;&#30340;&#21518;&#39564;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#25512;&#29702;&#65292;&#36890;&#36807;&#25552;&#20379;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#24341;&#21147;&#27874;&#25512;&#26029;&#19978;&#21462;&#24471;&#20102;&#21487;&#27604;&#31163;&#25955;&#27969;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31163;&#25955;&#35268;&#33539;&#21270;&#27969;&#30340;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#26041;&#27861;&#24050;&#25104;&#20026;&#27169;&#25311;&#25512;&#29702;&#65288;SBI&#65289;&#30340;&#25104;&#29087;&#24037;&#20855;&#65292;&#20294;&#23558;&#20854;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24314;&#31435;&#22312;&#26368;&#36817;&#29983;&#25104;&#24314;&#27169;&#26041;&#38754;&#30340;&#36827;&#23637;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#27969;&#21305;&#37197;&#30340;&#21518;&#39564;&#20272;&#35745;&#65288;FMPE&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#35268;&#33539;&#21270;&#27969;&#36827;&#34892;SBI&#30340;&#25216;&#26415;&#12290;&#19982;&#31163;&#25955;&#27969;&#19981;&#21516;&#65292;&#20687;&#25193;&#25955;&#27169;&#22411;&#19968;&#26679;&#65292;&#27969;&#21305;&#37197;&#20801;&#35768;&#26080;&#32422;&#26463;&#30340;&#26550;&#26500;&#65292;&#25552;&#20379;&#20102;&#29992;&#20110;&#22797;&#26434;&#25968;&#25454;&#27169;&#24577;&#30340;&#22686;&#24378;&#28789;&#27963;&#24615;&#12290;&#22240;&#27492;&#65292;&#27969;&#21305;&#37197;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#23494;&#24230;&#35780;&#20272;&#12289;&#24555;&#36895;&#30340;&#35757;&#32451;&#21644;&#26080;&#32541;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;SBI&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FMPE&#22312;SBI&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#28982;&#21518;&#22312;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31185;&#23398;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#25913;&#36827;&#30340;&#21487;&#25193;&#23637;&#24615;&#65306;&#23545;&#20110;&#24341;&#21147;&#27874;&#25512;&#26029;&#65292;FMPE&#32988;&#36807;&#22522;&#20110;&#30456;&#20284;&#31163;&#25955;&#27969;&#30340;&#26041;&#27861;&#65292;&#22312;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;30%&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#26377;&#35828;&#26381;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures--making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30% with substanti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#25968;&#25454;&#22686;&#24378;&#22312;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#36731;&#24494;&#30340;&#22256;&#38590;&#24230;&#19981;&#21487;&#25110;&#32570;&#12290;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DA&#25805;&#20316;&#8212;&#8212;Rand PR&#65292;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#26368;&#23567;&#30340;&#22256;&#38590;&#24230;&#65292;&#24050;&#32463;&#22312;&#22810;&#31181;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.16379</link><description>&lt;p&gt;
&#26377;&#25928;&#22686;&#24378;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#21033;&#29992;&#29575;&#65306;&#20197;&#23569;&#23398;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning. (arXiv:2305.16379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#25968;&#25454;&#22686;&#24378;&#22312;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#36731;&#24494;&#30340;&#22256;&#38590;&#24230;&#19981;&#21487;&#25110;&#32570;&#12290;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DA&#25805;&#20316;&#8212;&#8212;Rand PR&#65292;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#26368;&#23567;&#30340;&#22256;&#38590;&#24230;&#65292;&#24050;&#32463;&#22312;&#22810;&#31181;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#22686;&#24378;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20165;&#20351;&#29992;&#31616;&#21333;&#30340;&#35266;&#23519;&#21464;&#25442;&#23601;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#39069;&#22806;&#36741;&#21161;&#34920;&#31034;&#20219;&#21153;&#25110;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;DA&#30340;&#21738;&#20123;&#23646;&#24615;&#26159;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#35270;&#35273;RL&#30340;&#26377;&#25928;&#24615;&#30340;&#21407;&#22240;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;DA&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;DA&#23646;&#24615;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20197;&#19979;&#35265;&#35299;&#21644;&#25913;&#36827;&#65306;&#65288;1&#65289;&#23545;&#20110;&#21333;&#20010;DA&#25805;&#20316;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20805;&#36275;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#36731;&#24494;&#30340;&#22256;&#38590;&#24230;&#37117;&#26159;&#19981;&#21487;&#32570;&#23569;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;DA&#25805;&#20316;&#8212;&#8212;&#38543;&#26426;PadResize&#65288;Rand PR&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#26368;&#23567;&#30340;&#22256;&#38590;&#24230;&#12290;&#65288;2&#65289;&#23545;&#20110;&#22810;&#31867;&#22411;&#30340;DA&#34701;&#21512;&#26041;&#26696;&#65292;&#22686;&#21152;&#30340;DA&#22256;&#38590;&#24230;&#21644;&#19981;&#31283;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Data augmentation (DA) is a crucial technique for enhancing the sample efficiency of visual reinforcement learning (RL) algorithms. Notably, employing simple observation transformations alone can yield outstanding performance without extra auxiliary representation tasks or pre-trained encoders. However, it remains unclear which attributes of DA account for its effectiveness in achieving sample-efficient visual RL. To investigate this issue and further explore the potential of DA, this work conducts comprehensive experiments to assess the impact of DA's attributes on its efficacy and provides the following insights and improvements: (1) For individual DA operations, we reveal that both ample spatial diversity and slight hardness are indispensable. Building on this finding, we introduce Random PadResize (Rand PR), a new DA operation that offers abundant spatial diversity with minimal hardness. (2) For multi-type DA fusion schemes, the increased DA hardness and unstable data distribution 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;/&#21487;&#29702;&#35299;&#30340;&#22270;&#20687;&#32534;&#31243;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;VPGen&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#36880;&#27493;T2I&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;T2I&#29983;&#25104;&#20998;&#35299;&#20026;&#19977;&#20010;&#27493;&#39588;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#21069;&#20004;&#20010;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#27604;&#31471;&#21040;&#31471;&#27169;&#22411;&#26356;&#24378;&#30340;&#31354;&#38388;&#25511;&#21046;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.15328</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#19982;&#35780;&#20272;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Visual Programming for Text-to-Image Generation and Evaluation. (arXiv:2305.15328v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;/&#21487;&#29702;&#35299;&#30340;&#22270;&#20687;&#32534;&#31243;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;VPGen&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#36880;&#27493;T2I&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;T2I&#29983;&#25104;&#20998;&#35299;&#20026;&#19977;&#20010;&#27493;&#39588;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#21069;&#20004;&#20010;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#27604;&#31471;&#21040;&#31471;&#27169;&#22411;&#26356;&#24378;&#30340;&#31354;&#38388;&#25511;&#21046;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#35270;&#35273;&#27169;&#22359;&#30340;&#25511;&#21046;&#22120;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;/&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#32534;&#31243;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VPGen&#65292;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#36880;&#27493;T2I&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;T2I&#29983;&#25104;&#20998;&#35299;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;&#23545;&#35937;/&#35745;&#25968;&#29983;&#25104;&#12289;&#24067;&#23616;&#29983;&#25104;&#21644;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#21069;&#20004;&#20010;&#27493;&#39588;&#65288;&#23545;&#35937;/&#35745;&#25968;&#29983;&#25104;&#21644;&#24067;&#23616;&#29983;&#25104;&#65289;&#65292;&#36890;&#36807;&#22312;&#25991;&#26412;&#24067;&#23616;&#23545;&#19978;&#24494;&#35843;&#23427;&#12290;&#25105;&#20204;&#30340;&#36880;&#27493;T2I&#29983;&#25104;&#26694;&#26550;&#25552;&#20379;&#20102;&#27604;&#31471;&#21040;&#31471;&#27169;&#22411;&#26356;&#24378;&#30340;&#31354;&#38388;&#25511;&#21046;&#33021;&#21147;&#65292;&#32780;&#31471;&#21040;&#31471;&#27169;&#22411;&#26159;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#30340;&#24067;&#23616;&#24341;&#23548;T2I&#20316;&#21697;&#30340;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models have demonstrated impressive performance in many domains, recent works have adopted language models (LMs) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping LMs with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce VPGen, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an LM to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can on
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#24182;&#25552;&#20379;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#35265;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20284;&#23454;&#20307;&#30340;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.14695</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65306;&#19968;&#31181;&#22240;&#26524;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Causal View of Entity Bias in (Large) Language Models. (arXiv:2305.14695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14695
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#24182;&#25552;&#20379;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#35265;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20284;&#23454;&#20307;&#30340;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#20559;&#35265;&#24191;&#27867;&#24433;&#21709;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23548;&#33268;&#23427;&#20204;&#36807;&#24230;&#20381;&#36182;&#65288;&#26377;&#20559;&#35265;&#30340;&#65289;&#21442;&#25968;&#21270;&#30693;&#35782;&#26469;&#36827;&#34892;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#23613;&#31649;&#22240;&#26524;&#30456;&#20851;&#30340;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#32531;&#35299;&#23454;&#20307;&#20559;&#35265;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#31934;&#30830;&#20272;&#35745;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#21442;&#25968;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#40657;&#30418;&#23376;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#26080;&#27861;&#35843;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#65292;&#20854;&#21442;&#25968;&#27604;&#36739;&#23481;&#26131;&#20272;&#35745;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#30333;&#30418;&#21644;&#40657;&#30418;&#35774;&#32622;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#12290;&#36825;&#31181;&#22240;&#26524;&#24178;&#39044;&#23558;&#21407;&#22987;&#23454;&#20307;&#19982;&#30456;&#37051;&#23454;&#20307;&#19968;&#36215;&#36827;&#34892;&#25200;&#21160;&#12290;&#36825;&#31181;&#24178;&#39044;&#20943;&#23569;&#20102;&#19982;&#21407;&#22987;&#23454;&#20307;&#30456;&#20851;&#30340;&#29305;&#23450;&#20559;&#21521;&#20449;&#24687;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;&#20102;&#26469;&#33258;&#31867;&#20284;&#23454;&#20307;&#30340;&#36275;&#22815;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity bias widely affects pretrained (large) language models, causing them to excessively rely on (biased) parametric knowledge to make unfaithful predictions. Although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. To address these problems, we propose a specific structured causal model (SCM) whose parameters are comparatively easier to estimate. Building upon this SCM, we propose causal intervention techniques to mitigate entity bias for both white-box and black-box settings. The proposed causal intervention perturbs the original entity with neighboring entities. This intervention reduces specific biasing information pertaining to the original entity while still preserving sufficient common predictive information from similar entities. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22359;&#22352;&#26631;&#25554;&#20837;&#21644;&#25773;&#25918;&#65288;BC-PnP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#30340;&#21435;&#22122;&#22120;&#20316;&#20026;&#20808;&#39564;&#65292;&#39640;&#25928;&#22320;&#35299;&#20915;&#30450;&#21453;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#38750;&#20984;&#25968;&#25454;&#36866;&#24212;&#24230;&#39033;&#21644;&#25193;&#24352;&#21435;&#22122;&#22120;&#30340;&#26465;&#20214;&#19979;&#25910;&#25947;&#21040;&#19968;&#20010;&#38544;&#24335;&#20989;&#25968;&#30340;&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.12672</link><description>&lt;p&gt;
&#30450;&#21453;&#38382;&#39064;&#30340;&#22359;&#22352;&#26631;&#25554;&#20837;&#21644;&#25773;&#25918;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Block Coordinate Plug-and-Play Methods for Blind Inverse Problems. (arXiv:2305.12672v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22359;&#22352;&#26631;&#25554;&#20837;&#21644;&#25773;&#25918;&#65288;BC-PnP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#30340;&#21435;&#22122;&#22120;&#20316;&#20026;&#20808;&#39564;&#65292;&#39640;&#25928;&#22320;&#35299;&#20915;&#30450;&#21453;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#38750;&#20984;&#25968;&#25454;&#36866;&#24212;&#24230;&#39033;&#21644;&#25193;&#24352;&#21435;&#22122;&#22120;&#30340;&#26465;&#20214;&#19979;&#25910;&#25947;&#21040;&#19968;&#20010;&#38544;&#24335;&#20989;&#25968;&#30340;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25554;&#20837;&#21644;&#25773;&#25918;&#65288;PnP&#65289;&#20808;&#39564;&#26159;&#19968;&#31867;&#35299;&#20915;&#25104;&#20687;&#21453;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#29289;&#29702;&#27979;&#37327;&#27169;&#22411;&#21644;&#23398;&#20064;&#22270;&#20687;&#21435;&#22122;&#22120;&#30340;&#32452;&#21512;&#36816;&#31639;&#31526;&#30340;&#19981;&#21160;&#28857;&#26469;&#23454;&#29616;&#12290;&#34429;&#28982;PnP&#26041;&#27861;&#24050;&#24191;&#27867;&#29992;&#20110;&#24050;&#30693;&#27979;&#37327;&#31639;&#23376;&#30340;&#22270;&#20687;&#24674;&#22797;&#65292;&#20294;&#22312;&#35299;&#20915;&#30450;&#21453;&#38382;&#39064;&#26041;&#38754;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22359;&#22352;&#26631;PnP&#65288;BC-PnP&#65289;&#26041;&#27861;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#30340;&#21435;&#22122;&#22120;&#20316;&#20026;&#26410;&#30693;&#22270;&#20687;&#21644;&#26410;&#30693;&#27979;&#37327;&#31639;&#23376;&#30340;&#20808;&#39564;&#26469;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#32852;&#21512;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#30450;&#21453;&#38382;&#39064;&#20860;&#23481;&#30340;BC-PnP&#25910;&#25947;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#32771;&#34385;&#20102;&#38750;&#20984;&#25968;&#25454;&#36866;&#24212;&#24230;&#39033;&#21644;&#25193;&#24352;&#21435;&#22122;&#22120;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#20102;BC-PnP&#25910;&#25947;&#21040;&#19982;&#36817;&#20284;&#30340;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#65288;MMSE&#65289;&#21435;&#22122;&#22120;&#30456;&#20851;&#32852;&#30340;&#38544;&#24335;&#20989;&#25968;&#30340;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30450;&#21453;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plug-and-play (PnP) prior is a well-known class of methods for solving imaging inverse problems by computing fixed-points of operators combining physical measurement models and learned image denoisers. While PnP methods have been extensively used for image recovery with known measurement operators, there is little work on PnP for solving blind inverse problems. We address this gap by presenting a new block-coordinate PnP (BC-PnP) method that efficiently solves this joint estimation problem by introducing learned denoisers as priors on both the unknown image and the unknown measurement operator. We present a new convergence theory for BC-PnP compatible with blind inverse problems by considering nonconvex data-fidelity terms and expansive denoisers. Our theory analyzes the convergence of BC-PnP to a stationary point of an implicit function associated with an approximate minimum mean-squared error (MMSE) denoiser. We numerically validate our method on two blind inverse problems: automatic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiTurnCleanup&#20219;&#21153;&#65292;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#21487;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12029</link><description>&lt;p&gt;
MultiTurnCleanup&#65306;&#29992;&#20110;&#22810;&#36718;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#28165;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup. (arXiv:2305.12029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiTurnCleanup&#20219;&#21153;&#65292;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#21487;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35843;&#19981;&#36830;&#32493;&#26816;&#27979;&#27169;&#22411;&#20391;&#37325;&#20110;&#21333;&#20010;&#35828;&#35805;&#32773;&#30340;&#27599;&#20010;&#35805;&#35821;&#12290;&#28982;&#32780;&#65292;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#35768;&#22810;&#19981;&#36830;&#32493;&#29616;&#35937;&#37117;&#21457;&#29983;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#65292;&#36825;&#24433;&#21709;&#20102;&#20154;&#31867;&#30340;&#21487;&#35835;&#24615;&#21644;&#19979;&#28216; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#26032;&#30340;&#8220;MultiTurnCleanup&#8221;&#20219;&#21153;&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#65292;&#24182;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25968;&#25454;&#26631;&#27880;&#27169;&#24335;&#20197;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current disfluency detection models focus on individual utterances each from a single speaker. However, numerous discontinuity phenomena in spoken conversational transcripts occur across multiple turns, hampering human readability and the performance of downstream NLP tasks. This study addresses these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken conversational transcripts and collecting a new dataset, MultiTurnCleanup1. We design a data labeling schema to collect the high-quality dataset and provide extensive data analysis. Furthermore, we leverage two modeling approaches for experimental evaluation as benchmarks for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20849;&#26354;&#25233;&#21046;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#24212;&#23545;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#26131;&#21463;&#20849;&#38169;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24809;&#32602;&#38750;&#32447;&#24615;&#36716;&#25442;&#30340;&#29305;&#24449;&#21464;&#37327;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11475</link><description>&lt;p&gt;
&#26354;&#32447;&#19978;&#25196;&#65306;&#22312;&#21487;&#24494;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#20013;&#30340;&#20849;&#26354;&#25233;&#21046;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models. (arXiv:2305.11475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20849;&#26354;&#25233;&#21046;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#24212;&#23545;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#26131;&#21463;&#20849;&#38169;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24809;&#32602;&#38750;&#32447;&#24615;&#36716;&#25442;&#30340;&#29305;&#24449;&#21464;&#37327;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#65288;GAM&#65289;&#21487;&#34920;&#36798;&#30446;&#26631;&#21464;&#37327;&#20026;&#29305;&#24449;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#21644;&#35299;&#37322;&#24615;&#65292;&#20854;&#20877;&#27425;&#21463;&#21040;&#27426;&#36814;&#12290;&#23613;&#31649;GAM&#30446;&#21069;&#22791;&#21463;&#28909;&#25447;&#65292;&#20294;&#20854;&#26131;&#21463;&#20849;&#38169;&#24615;&#65292;&#21363;&#29305;&#24449;&#20043;&#38388;&#30340;&#65288;&#21487;&#33021;&#26159;&#38750;&#32447;&#24615;&#30340;&#65289;&#20381;&#36182;&#24615;&#36804;&#20170;&#20026;&#27490;&#22823;&#22810;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20849;&#38169;&#24615;&#22914;&#20309;&#20005;&#37325;&#30772;&#22351;GAM&#30340;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#27861;&#65306;&#19968;&#20010;&#22312;&#38750;&#32447;&#24615;&#36716;&#25442;&#30340;&#29305;&#24449;&#21464;&#37327;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#19978;&#36827;&#34892;&#24809;&#32602;&#30340;&#27010;&#24565;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#35813;&#36807;&#31243;&#36866;&#29992;&#20110;&#20219;&#20309;&#21487;&#24494;&#30340;&#21152;&#24615;&#27169;&#22411;&#65292;&#22914;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#25110;&#31070;&#32463;&#39044;&#35328;&#12290;&#24182;&#19988;&#36890;&#36807;&#28040;&#38500;&#33258;&#25105;&#25269;&#28040;&#30340;&#29305;&#24449;&#36129;&#29486;&#30340;&#27495;&#20041;&#65292;&#22686;&#24378;&#20102;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#26102;&#38388;&#24207;&#21015;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Additive Models (GAMs) have recently experienced a resurgence in popularity due to their interpretability, which arises from expressing the target value as a sum of non-linear transformations of the features. Despite the current enthusiasm for GAMs, their susceptibility to concurvity - i.e., (possibly non-linear) dependencies between the features - has hitherto been largely overlooked. Here, we demonstrate how concurvity can severly impair the interpretability of GAMs and propose a remedy: a conceptually simple, yet effective regularizer which penalizes pairwise correlations of the non-linearly transformed feature variables. This procedure is applicable to any differentiable additive model, such as Neural Additive Models or NeuralProphet, and enhances interpretability by eliminating ambiguities due to self-canceling feature contributions. We validate the effectiveness of our regularizer in experiments on synthetic as well as real-world datasets for time-series and tabular d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20381;&#36182;&#25968;&#25454;&#30340;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#22122;&#22768;&#27700;&#24179;&#65292;&#25552;&#20986;&#20102;&#19978;&#38480;&#30028;&#38480;&#65292;&#24182;&#22312;&#35823;&#24046;&#35268;&#33539;&#19979;&#34920;&#29616;&#20986;&#20248;&#38597;&#30340;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.11165</link><description>&lt;p&gt;
&#32447;&#24615;&#22238;&#24402;&#20013;&#20381;&#36182;&#25968;&#25454;&#30340;&#22122;&#22768;&#27700;&#24179;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The noise level in linear regression with dependent data. (arXiv:2305.11165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20381;&#36182;&#25968;&#25454;&#30340;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#22122;&#22768;&#27700;&#24179;&#65292;&#25552;&#20986;&#20102;&#19978;&#38480;&#30028;&#38480;&#65292;&#24182;&#22312;&#35823;&#24046;&#35268;&#33539;&#19979;&#34920;&#29616;&#20986;&#20248;&#38597;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#26410;&#20570;&#20219;&#20309;&#23454;&#29616;&#20551;&#35774;&#20986;&#21457;&#65292;&#23545;&#20110;&#20855;&#26377;&#20381;&#36182;($\beta$-mixing)&#25968;&#25454;&#30340;&#38543;&#26426;&#35774;&#35745;&#32447;&#24615;&#22238;&#24402;&#65292;&#25512;&#23548;&#20102;&#20854;&#19978;&#38480;&#30028;&#38480;&#12290;&#19982;&#20165;&#22312;&#21487;&#23454;&#29616;&#30340;&#38789;&#22122;&#22768;&#33539;&#22260;&#20869;&#19981;&#21487;&#29992;&#23574;&#38160;&#30340;&#23454;&#20363;&#26368;&#20248;&#38750;&#28176;&#36817;&#24615;&#30456;&#27604;&#65292;&#25991;&#29486;&#20013;&#27809;&#26377;&#21487;&#29992;&#30340;&#19978;&#38480;&#30028;&#38480;&#12290;&#22312;&#24688;&#24403;&#30340;&#24120;&#25968;&#22240;&#32032;&#19979;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#27491;&#30830;&#22320;&#22238;&#24402;&#20102;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#39044;&#27979;&#30340;&#26041;&#24046;&#39033; - &#38382;&#39064;&#30340;&#22122;&#22768;&#27700;&#24179; - &#24182;&#22240;&#27492;&#22312;&#24341;&#20837;&#38169;&#35823;&#35268;&#33539;&#26102;&#34920;&#29616;&#20986;&#36880;&#28176;&#38477;&#20302;&#30340;&#20248;&#38597;&#24615;&#12290;&#22312;&#39044;&#29123;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#20013;&#24230;&#20559;&#24046;&#33539;&#22260;&#20869;&#26159;&#23574;&#38160;&#30340;&#65292;&#29305;&#21035;&#26159;&#19981;&#20250;&#33192;&#32960;&#24341;&#39046;&#39033;&#26399;&#38480;&#19982;&#28151;&#21512;&#26102;&#38388;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive upper bounds for random design linear regression with dependent ($\beta$-mixing) data absent any realizability assumptions. In contrast to the strictly realizable martingale noise regime, no sharp instance-optimal non-asymptotics are available in the literature. Up to constant factors, our analysis correctly recovers the variance term predicted by the Central Limit Theorem -- the noise level of the problem -- and thus exhibits graceful degradation as we introduce misspecification. Past a burn-in, our result is sharp in the moderate deviations regime, and in particular does not inflate the leading order term by mixing time factors.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#65292;&#20294;&#37325;&#26032;&#35757;&#32451;ML&#27169;&#22411;&#24448;&#24448;&#19981;&#21487;&#34892;&#12290;&#38024;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#32531;&#35299;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07512</link><description>&lt;p&gt;
&#23398;&#20064;&#21435;&#23398;&#20064;&#65306;&#26426;&#22120;&#21435;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learn to Unlearn: A Survey on Machine Unlearning. (arXiv:2305.07512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#65292;&#20294;&#37325;&#26032;&#35757;&#32451;ML&#27169;&#22411;&#24448;&#24448;&#19981;&#21487;&#34892;&#12290;&#38024;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#32531;&#35299;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#21547;&#31169;&#23494;&#20449;&#24687;&#65292;&#23454;&#29616;&#34987;&#36951;&#24536;&#26435;&#26159;&#35768;&#22810;&#25968;&#25454;&#24212;&#29992;&#30340;&#38590;&#39064;&#12290;&#26426;&#22120;&#21435;&#23398;&#20064;&#24050;&#25104;&#20026;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#30340;&#31616;&#35201;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#31934;&#30830;&#21644;&#36817;&#20284;&#26041;&#27861;&#12289;&#21487;&#33021;&#30340;&#25915;&#20987;&#20197;&#21450;&#39564;&#35777;&#26041;&#27861;&#12290;&#26412;&#32508;&#36848;&#27604;&#36739;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20351;&#29992;Deltagrad&#31934;&#30830;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#26412;&#32508;&#36848;&#36824;&#24378;&#35843;&#20102;&#25361;&#25112;&#65292;&#22914;&#38750;IID&#21024;&#38500;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#21644;&#24212;&#29992;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25104;&#20026;&#23547;&#27714;&#26426;&#22120;&#21435;&#23398;&#20064;&#36164;&#26009;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#30340;&#26377;&#20215;&#20540;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) models contain private information, and implementing the right to be forgotten is a challenging privacy issue in many data applications. Machine unlearning has emerged as an alternative to remove sensitive data from a trained model, but completely retraining ML models is often not feasible. This survey provides a concise appraisal of Machine Unlearning techniques, encompassing both exact and approximate methods, probable attacks, and verification approaches. The survey compares the merits and limitations each method and evaluates their performance using the Deltagrad exact machine unlearning method. The survey also highlights challenges like the pressing need for a robust model for non-IID deletion to mitigate fairness issues. Overall, the survey provides a thorough synopsis of machine unlearning techniques and applications, noting future research directions in this evolving field. The survey aims to be a valuable resource for researchers and practitioners seeking
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.05480</link><description>&lt;p&gt;
&#25506;&#31350;&#23376;&#35789;&#20998;&#21106;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24819;&#30740;&#31350;&#35789;&#27573;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#65292;&#22312;&#33452;&#20848;&#35821;&#21644;&#20420;&#35821;&#20013;&#35757;&#32451;&#20102;GPT-2&#21644;BERT&#27169;&#22411;&#12290;&#20316;&#20026;&#27604;&#36739;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#20351;&#29992;BPE&#21644;Morfessor&#20998;&#21106;&#31639;&#27861;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;StateMorph&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20844;&#21496;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#20844;&#21496;&#20998;&#31867;&#36807;&#31243;&#65292;&#20174;&#32780;&#20943;&#23569;&#20256;&#32479;&#26041;&#27861;&#22914;&#20840;&#29699;&#20135;&#19994;&#20998;&#31867;&#26631;&#20934;&#65288;GICS&#65289;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2305.01028</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#22312;&#20844;&#21496;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Company classification using zero-shot learning. (arXiv:2305.01028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20844;&#21496;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#20844;&#21496;&#20998;&#31867;&#36807;&#31243;&#65292;&#20174;&#32780;&#20943;&#23569;&#20256;&#32479;&#26041;&#27861;&#22914;&#20840;&#29699;&#20135;&#19994;&#20998;&#31867;&#26631;&#20934;&#65288;GICS&#65289;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#35768;&#22810;&#21830;&#19994;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20844;&#21496;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20174;&#20844;&#21496;&#25551;&#36848;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#24212;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#23558;&#20844;&#21496;&#20998;&#31867;&#21040;&#30456;&#20851;&#31867;&#21035;&#65292;&#26080;&#38656;&#20026;&#27599;&#20010;&#31867;&#21035;&#25552;&#20379;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#20844;&#21496;&#25991;&#26412;&#25551;&#36848;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#31616;&#21270;&#20844;&#21496;&#20998;&#31867;&#36807;&#31243;&#65292;&#20174;&#32780;&#20943;&#23569;&#20256;&#32479;&#26041;&#27861;&#22914;&#20840;&#29699;&#20135;&#19994;&#20998;&#31867;&#26631;&#20934;&#65288;GICS&#65289;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33258;&#21160;&#21270;&#20844;&#21496;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#26159;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, natural language processing (NLP) has become increasingly important in a variety of business applications, including sentiment analysis, text classification, and named entity recognition. In this paper, we propose an approach for company classification using NLP and zero-shot learning. Our method utilizes pre-trained transformer models to extract features from company descriptions, and then applies zero-shot learning to classify companies into relevant categories without the need for specific training data for each category. We evaluate our approach on publicly available datasets of textual descriptions of companies, and demonstrate that it can streamline the process of company classification, thereby reducing the time and resources required in traditional approaches such as the Global Industry Classification Standard (GICS). The results show that this method has potential for automation of company classification, making it a promising avenue for future research in thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Zip-NeRF &#25216;&#26415;&#65292;&#23558; mip-NeRF 360 &#21644;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#25239;&#38191;&#40831;&#12289;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#24182;&#38477;&#20302;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06706</link><description>&lt;p&gt;
Zip-NeRF&#65306;&#25239;&#38191;&#40831;&#32593;&#26684;&#21270;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields. (arXiv:2304.06706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Zip-NeRF &#25216;&#26415;&#65292;&#23558; mip-NeRF 360 &#21644;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#25239;&#38191;&#40831;&#12289;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#24182;&#38477;&#20302;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#30340;&#32593;&#26684;&#21270;&#34920;&#31034;&#21487;&#20197;&#21152;&#36895;&#35757;&#32451;&#65292;&#20294;&#32570;&#20047;&#23545;&#27604;&#20363;&#30340;&#26126;&#30830;&#29702;&#35299;&#65292;&#23481;&#26131;&#24341;&#20837;&#38191;&#40831;&#25110;&#20002;&#22833;&#22330;&#26223;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28210;&#26579;&#21644;&#20449;&#21495;&#22788;&#29702;&#24605;&#24819;&#29992;&#20110;&#23558; mip-NeRF 360 &#21644;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#35823;&#24046;&#29575;&#27604;&#20808;&#21069;&#30340;&#25216;&#26415;&#20302;8%&#21040;76%&#65292;&#24182;&#27604; mip-NeRF 360 &#24555;22&#20493;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8% - 76% lower than either prior technique, and that trains 22x faster than mip-NeRF 360.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26723;&#23618;&#27425;&#32467;&#26500;&#35825;&#23548;&#26469;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#36807;&#25311;&#21512;&#21644;&#26377;&#38480;&#30340;&#26222;&#36866;&#24615;&#65292;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02247</link><description>&lt;p&gt;
&#12298;&#35299;&#24320;&#32467;&#26500;&#19982;&#39118;&#26684;&#30340;&#32445;&#24102;&#65306;&#36890;&#36807;&#35825;&#23548;&#25991;&#26723;&#23618;&#27425;&#32467;&#26500;&#26469;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#12299;
&lt;/p&gt;
&lt;p&gt;
Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy. (arXiv:2304.02247v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26723;&#23618;&#27425;&#32467;&#26500;&#35825;&#23548;&#26469;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#36807;&#25311;&#21512;&#21644;&#26377;&#38480;&#30340;&#26222;&#36866;&#24615;&#65292;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26032;&#38395;&#25991;&#31456;&#20013;&#25919;&#27835;&#20559;&#35265;&#26816;&#27979;&#26041;&#38754;&#30340;&#37325;&#35201;&#24046;&#36317;&#36827;&#34892;&#30740;&#31350;&#12290;&#20808;&#21069;&#36827;&#34892;&#30417;&#30563;&#24335;&#25991;&#26723;&#20998;&#31867;&#30340;&#24037;&#20316;&#21487;&#33021;&#20250;&#20559;&#21521;&#21508;&#32593;&#31449;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#26377;&#38480;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#21477;&#23376;&#32423;&#35821;&#20041;&#21644;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31181;&#26356;&#24378;&#22823;&#21644;&#19981;&#21463;&#39118;&#26684;&#24433;&#21709;&#30340;&#26816;&#27979;&#25919;&#27835;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22836;&#20998;&#23618;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#36890;&#36807;&#21508;&#31181;&#27880;&#24847;&#21147;&#22836;&#30340;&#19981;&#21516;&#38598;&#21512;&#26377;&#25928;&#22320;&#32534;&#30721;&#38271;&#25991;&#26723;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#36825;&#31181;&#22495;&#20381;&#36182;&#24615;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#26032;&#38395;&#20013;&#24120;&#29992;&#30340;&#35805;&#35821;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address an important gap in detection of political bias in news articles. Previous works that perform supervised document classification can be biased towards the writing style of each news outlet, leading to overfitting and limited generalizability. Our approach overcomes this limitation by considering both the sentence-level semantics and the document-level rhetorical structure, resulting in a more robust and style-agnostic approach to detecting political bias in news articles. We introduce a novel multi-head hierarchical attention model that effectively encodes the structure of long documents through a diverse ensemble of attention heads. While journalism follows a formalized rhetorical structure, the writing style may vary by news outlet. We demonstrate that our method overcomes this domain dependency and outperforms previous approaches for robustness and accuracy. Further analysis demonstrates the ability of our model to capture the discourse structures commonly used in the jou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11403</link><description>&lt;p&gt;
eP-ALM:&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#24863;&#30693;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36804;&#20170;&#20026;&#27490;&#32473;&#19990;&#30028;&#30041;&#19979;&#20102;&#28145;&#21051;&#21360;&#35937;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#27169;&#22411;&#25152;&#20855;&#26377;&#30340;&#38750;&#21516;&#23547;&#24120;&#30340;&#33021;&#21147;&#12290;&#22312;&#35270;&#35273;&#26041;&#38754;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;&#21363;ViT&#65289;&#20063;&#22312;&#36861;&#38543;&#21516;&#19968;&#36235;&#21183;&#65292;&#21462;&#24471;&#20102;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#38543;&#30528;&#36825;&#31181;&#21333;&#27169;&#22411;&#30340;&#20016;&#23500;&#22810;&#26679;&#65292;&#33258;&#28982;&#20250;&#24341;&#21457;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#38656;&#35201;&#36319;&#38543;&#36825;&#20010;&#36235;&#21183;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#21162;&#21147;&#38598;&#20013;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#39640;&#25928;&#36866;&#24212;&#65292;&#24182;&#25552;&#20986;&#29992;&#24863;&#30693;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#20182;&#20204;&#20173;&#28982;&#35757;&#32451;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#20381;&#36182;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#22312;&#24040;&#22823;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;CLIP&#65289;&#65292;&#24182;&#28155;&#21152;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#22823;&#22810;&#25968;&#20851;&#27880;Zero-Shot&#21644;In Context Learning&#65292;&#35266;&#23519;&#21040;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;eP-ALM&#65292;&#19968;&#31181;&#23558;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#20855;&#26377;&#26497;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#26032;&#30340;&#39044;&#35757;&#32451;&#65292;&#20173;&#28982;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. On the vision side, transformer models (i.e., ViT) are following the same trend, achieving the best performance on challenging benchmarks. With the abundance of such unimodal models, a natural question arises; do we need also to follow this trend to tackle multimodal tasks? In this work, we propose to rather direct effort to efficient adaptations of existing models, and propose to augment Language Models with perception. Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#32791;&#25955;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#27714;&#35299;McKean-Vlasov&#31867;&#22411;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(MVEs)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#24191;&#20041;&#33258;&#27965;&#21183;&#26469;&#25511;&#21046;&#20551;&#35774;&#35299;&#19982;&#30495;&#23454;&#35299;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20989;&#25968;&#36924;&#36817;&#65292;&#24182;&#22312;&#22810;&#20010;&#31034;&#20363;&#38382;&#39064;&#19978;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11205</link><description>&lt;p&gt;
&#22522;&#20110;&#29109;&#32791;&#25955;&#30340;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;McKean-Vlasov&#31867;&#22411;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Entropy-dissipation Informed Neural Network for McKean-Vlasov Type PDEs. (arXiv:2303.11205v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#32791;&#25955;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#27714;&#35299;McKean-Vlasov&#31867;&#22411;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(MVEs)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#24191;&#20041;&#33258;&#27965;&#21183;&#26469;&#25511;&#21046;&#20551;&#35774;&#35299;&#19982;&#30495;&#23454;&#35299;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20989;&#25968;&#36924;&#36817;&#65292;&#24182;&#22312;&#22810;&#20010;&#31034;&#20363;&#38382;&#39064;&#19978;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#33258;&#27965;&#24615;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;McKean-Vlasov&#26041;&#31243;(MVE)&#20013;&#12290;&#34429;&#28982;Fokker-Planck&#26041;&#31243;(FPE)&#25551;&#36848;&#20102;&#28418;&#31227;&#21644;&#25193;&#25955;&#19979;&#31890;&#23376;&#30340;&#23439;&#35266;&#34892;&#20026;&#65292;&#20294;MVE&#32771;&#34385;&#20102;&#39069;&#22806;&#30340;&#31890;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#22312;&#29289;&#29702;&#31995;&#32479;&#20013;&#36890;&#24120;&#20855;&#26377;&#22855;&#24322;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#31034;&#20363;&#65292;&#21363;&#20855;&#26377;&#24211;&#20177;&#30456;&#20114;&#20316;&#29992;&#30340;MVE&#21644;&#20108;&#32500;Navier-Stokes&#26041;&#31243;&#30340;&#28065;&#37327;&#34920;&#36848;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24191;&#20041;&#33258;&#27965;&#21183;&#36890;&#36807;&#29109;&#32791;&#25955;&#25511;&#21046;&#20102;&#20551;&#35774;&#35299;&#19982;&#30495;&#23454;&#35299;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#22522;&#20110;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#26368;&#23567;&#21270;&#35813;&#21183;&#20989;&#25968;&#26469;&#27714;&#35299;MVE&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20989;&#25968;&#36924;&#36817;&#12290;&#36890;&#36807;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;PDE&#27714;&#35299;&#22120;&#22312;&#20960;&#20010;&#31034;&#20363;&#38382;&#39064;&#19978;&#30340;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend the concept of self-consistency for the Fokker-Planck equation (FPE) to the more general McKean-Vlasov equation (MVE). While FPE describes the macroscopic behavior of particles under drift and diffusion, MVE accounts for the additional inter-particle interactions, which are often highly singular in physical systems. Two important examples considered in this paper are the MVE with Coulomb interactions and the vorticity formulation of the 2D Navier-Stokes equation. We show that a generalized self-consistency potential controls the KL-divergence between a hypothesis solution to the ground truth, through entropy dissipation. Built on this result, we propose to solve the MVEs by minimizing this potential function, while utilizing the neural networks for function approximation. We validate the empirical performance of our approach by comparing with state-of-the-art NN-based PDE solvers on several example problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#65292;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#26469;&#25193;&#20805;&#25968;&#25454;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2303.06614</link><description>&lt;p&gt;
&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#65306;&#26088;&#22312;&#29992;&#25193;&#20805;&#25968;&#25454;&#26469;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Synthetic Experience Replay. (arXiv:2303.06614v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#65292;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#26469;&#25193;&#20805;&#25968;&#25454;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#30340;&#19968;&#20010;&#20851;&#38190;&#20027;&#39064;&#26159;&#65292;&#24403;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#20135;&#29983;&#20196;&#20154;&#24778;&#24322;&#30340;&#32467;&#26524;&#12290;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#31181;&#33539;&#24335;&#36890;&#24120;&#36890;&#36807;&#32463;&#39564;&#22238;&#25918;&#23454;&#29616;&#65292;&#20854;&#20013;&#36807;&#21435;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#31574;&#30053;&#25110;&#20540;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#25110;&#33258;&#30417;&#30563;&#23398;&#20064;&#19981;&#21516;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24517;&#39035;&#25910;&#38598;&#33258;&#24049;&#30340;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#26159;&#26377;&#38480;&#30340;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#22909;&#22788;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#21363;&#20351;&#26159;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#24320;&#22987;&#26102;&#20063;&#21487;&#33021;&#20986;&#29616;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#29983;&#25104;&#24314;&#27169;&#30340;&#24040;&#22823;&#36827;&#27493;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#65288;SynthER&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#26469;&#28789;&#27963;&#22320;&#19978;&#37319;&#26679;&#20195;&#29702;&#25910;&#38598;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SynthER&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#26080;&#35770;&#26159;&#22312;&#24863;&#30693;&#29615;&#22659;&#36824;&#26159;&#22312;&#20687;&#32032;&#29615;&#22659;&#20013;&#12290;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key theme in the past decade has been that when large neural networks and large datasets combine they can produce remarkable results. In deep reinforcement learning (RL), this paradigm is commonly made possible through experience replay, whereby a dataset of past experiences is used to train a policy or value function. However, unlike in supervised or self-supervised learning, an RL agent has to collect its own data, which is often limited. Thus, it is challenging to reap the benefits of deep learning, and even small neural networks can overfit at the start of training. In this work, we leverage the tremendous recent progress in generative modeling and propose Synthetic Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an agent's collected experience. We show that SynthER is an effective method for training RL agents across offline and online settings, in both proprioceptive and pixel-based environments. In offline settings, we observe drastic improvements 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#20195;&#25968;&#35270;&#35282;&#65292;&#34701;&#21512;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;1-Lipschitz&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#20984;&#28508;&#22312;&#23618;&#21644;&#36817;&#27491;&#20132;&#23618;&#65292;&#24182;&#21033;&#29992;&#35299;&#26512;&#35299;&#25512;&#23548;&#21644;&#25512;&#24191;&#20102;&#35768;&#22810;&#29616;&#26377;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.03169</link><description>&lt;p&gt;
&#20851;&#20110;Lipschitz&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#20195;&#25968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Unified Algebraic Perspective on Lipschitz Neural Networks. (arXiv:2303.03169v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#20195;&#25968;&#35270;&#35282;&#65292;&#34701;&#21512;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;1-Lipschitz&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#20984;&#28508;&#22312;&#23618;&#21644;&#36817;&#27491;&#20132;&#23618;&#65292;&#24182;&#21033;&#29992;&#35299;&#26512;&#35299;&#25512;&#23548;&#21644;&#25512;&#24191;&#20102;&#35768;&#22810;&#29616;&#26377;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#33268;&#21147;&#20110;&#35774;&#35745;&#21644;&#35757;&#32451;&#20855;&#26377;&#25511;&#21046;&#30340;Lipschitz&#24120;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#30446;&#26631;&#26159;&#25552;&#39640;&#24182;&#26377;&#26102;&#20445;&#35777;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#20174;&#19981;&#21516;&#30340;&#32972;&#26223;&#20013;&#27762;&#21462;&#28789;&#24863;&#26469;&#35774;&#35745;1-Lipschitz&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;&#20984;&#28508;&#22312;&#23618;&#20174;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#30340;&#31163;&#25955;&#21270;&#20013;&#24471;&#20986;&#65292;&#36817;&#27491;&#20132;&#23618;&#21017;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#30697;&#38453;&#37325;&#26032;&#32553;&#25918;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#22312;&#37325;&#35201;&#30340;&#26159;&#22312;&#36890;&#29992;&#30340;&#29702;&#35770;&#35270;&#35282;&#19979;&#32771;&#34385;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21644;&#26377;&#24076;&#26395;&#30340;&#36129;&#29486;&#65292;&#20197;&#26356;&#22909;&#22320;&#35774;&#35745;&#26032;&#30340;&#21644;&#25913;&#36827;&#30340;&#23618;&#27425;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20195;&#25968;&#35270;&#35282;&#65292;&#32479;&#19968;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;1-Lipschitz&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#20043;&#21069;&#25552;&#21040;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#27491;&#20132;&#24615;&#21644;&#35889;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#29616;&#26377;&#30340;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#25214;&#21040;&#35299;&#26512;&#35299;&#26469;&#25512;&#23548;&#21644;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
Important research efforts have focused on the design and training of neural networks with a controlled Lipschitz constant. The goal is to increase and sometimes guarantee the robustness against adversarial attacks. Recent promising techniques draw inspirations from different backgrounds to design 1-Lipschitz neural networks, just to name a few: convex potential layers derive from the discretization of continuous dynamical systems, Almost-Orthogonal-Layer proposes a tailored method for matrix rescaling. However, it is today important to consider the recent and promising contributions in the field under a common theoretical lens to better design new and improved layers. This paper introduces a novel algebraic perspective unifying various types of 1-Lipschitz neural networks, including the ones previously mentioned, along with methods based on orthogonality and spectral methods. Interestingly, we show that many existing techniques can be derived and generalized via finding analytical sol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#36136;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#19979;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#31867;&#21035;$F$&#30456;&#27604;&#31867;&#21035;$G$&#26356;&#8220;&#31616;&#21333;&#8221;&#26102;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#22120;&#23545;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#22312;$\textbf{y}$&#30340;&#20559;&#31227;&#36828;&#23567;&#20110;$\textbf{x}$&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;ERM&#30340;&#34892;&#20026;&#19982;&#27491;&#20132;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13934</link><description>&lt;p&gt;
&#24322;&#36136;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#32479;&#35745;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Statistical Learning under Heterogenous Distribution Shift. (arXiv:2302.13934v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#36136;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#19979;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#31867;&#21035;$F$&#30456;&#27604;&#31867;&#21035;$G$&#26356;&#8220;&#31616;&#21333;&#8221;&#26102;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#22120;&#23545;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#22312;$\textbf{y}$&#30340;&#20559;&#31227;&#36828;&#23567;&#20110;$\textbf{x}$&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;ERM&#30340;&#34892;&#20026;&#19982;&#27491;&#20132;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#38543;&#26426;&#21464;&#37327;&#23545;$(\mathbf{x},\mathbf{y})$&#20013;&#39044;&#27979;&#30446;&#26631;$\mathbf{z}$, &#20854;&#20013;&#30495;&#23454;&#30340;&#39044;&#27979;&#22120;&#26159;&#21152;&#27861;&#30340;$\mathbb{E}[\mathbf{z} \mid \mathbf{x},\mathbf{y}] = f_\star(\mathbf{x}) +g_{\star}(\mathbf{y})$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32473;&#23450;&#35757;&#32451;&#20998;&#24067;&#19978;&#25311;&#21512;&#30340;&#20989;&#25968;$f+g$, $f \in F$&#21644;$g \in G$&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#22312;&#34920;&#29616;&#19978;&#30340;&#24046;&#24322;&#65292;&#20294;&#22312;&#27979;&#35797;&#20998;&#24067;&#19978;&#24471;&#21040;&#35780;&#20272;&#26102;&#20250;&#26174;&#31034;&#20986;&#21327;&#21464;&#37327;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#31867;&#21035;$F$&#27604;$G$&#26356;&#8220;&#31616;&#21333;&#8221;&#65288;&#20363;&#22914;&#65292;&#20197;&#24230;&#37327;&#29109;&#20026;&#34913;&#37327;&#26631;&#20934;&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#22120;&#23545;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#30340;&#25239;&#24178;&#25200;&#33021;&#21147;&#26356;&#24378;&#65292;&#20854;&#20013;$\textbf{y}$&#30340;&#20559;&#31227;&#35201;&#36828;&#23567;&#20110;$\textbf{x}$&#30340;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ERM&#30340;&#34892;&#20026;&#19982;&#27491;&#20132;&#26426;&#22120;&#23398;&#20064;$\textbf{ qualitatively similarly}$&#65306;ERM&#24674;&#22797;&#39044;&#27979;&#22120;&#20013;&#30340;$f$&#25104;&#20998;&#30340;&#36895;&#29575;&#20165;&#23545;&#20110;&#31867;&#21035;$G$&#30340;&#22797;&#26434;&#24615;&#20855;&#26377;&#36739;&#20302;&#38454;&#30340;&#20381;&#36182;&#24615;&#65292;&#35843;&#25972;&#21518;...
&lt;/p&gt;
&lt;p&gt;
This paper studies the prediction of a target $\mathbf{z}$ from a pair of random variables $(\mathbf{x},\mathbf{y})$, where the ground-truth predictor is additive $\mathbb{E}[\mathbf{z} \mid \mathbf{x},\mathbf{y}] = f_\star(\mathbf{x}) +g_{\star}(\mathbf{y})$. We study the performance of empirical risk minimization (ERM) over functions $f+g$, $f \in F$ and $g \in G$, fit on a given training distribution, but evaluated on a test distribution which exhibits covariate shift. We show that, when the class $F$ is "simpler" than $G$ (measured, e.g., in terms of its metric entropy), our predictor is more resilient to $\textbf{heterogenous covariate shifts}$ in which the shift in $\mathbf{x}$ is much greater than that in $\mathbf{y}$. Our analysis proceeds by demonstrating that ERM behaves $\textbf{qualitatively similarly to orthogonal machine learning}$: the rate at which ERM recovers the $f$-component of the predictor has only a lower-order dependence on the complexity of the class $G$, adjus
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;VAE&#27169;&#22411;&#23481;&#37327;&#65292;&#37319;&#29992;&#26080;&#38480;&#28151;&#21512;&#30340;&#38750;&#23545;&#31216;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#20316;&#20026;&#35299;&#30721;&#22120;&#65292;&#20855;&#26377;&#20998;&#24067;&#25311;&#21512;&#33021;&#21147;&#21644;&#35843;&#25972;&#25968;&#25454;&#38544;&#31169;&#32423;&#21035;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.11294</link><description>&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#65306;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation. (arXiv:2302.11294v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11294
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;VAE&#27169;&#22411;&#23481;&#37327;&#65292;&#37319;&#29992;&#26080;&#38480;&#28151;&#21512;&#30340;&#38750;&#23545;&#31216;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#20316;&#20026;&#35299;&#30721;&#22120;&#65292;&#20855;&#26377;&#20998;&#24067;&#25311;&#21512;&#33021;&#21147;&#21644;&#35843;&#25972;&#25968;&#25454;&#38544;&#31169;&#32423;&#21035;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#22312;&#35745;&#31639;&#24314;&#27169;&#26041;&#38754;&#24456;&#39640;&#25928;&#65292;&#20294;&#39640;&#26031;&#20551;&#35774;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#23427;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#27169;&#22411;&#23481;&#37327;&#65288;&#21363;&#20998;&#24067;&#26063;&#30340;&#34920;&#36798;&#33021;&#21147;&#65289;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;VAE&#26694;&#26550;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;VAE&#27169;&#22411;&#30340;&#35299;&#30721;&#22120;&#30001;&#26080;&#38480;&#32452;&#21512;&#30340;&#38750;&#23545;&#31216;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#26500;&#25104;&#65292;&#20855;&#26377;&#36830;&#32493;&#21464;&#37327;&#30340;&#20998;&#24067;&#25311;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#20272;&#35745;&#19968;&#33324;&#20998;&#20301;&#20989;&#25968;&#30340;&#38750;&#21442;&#25968;M-estimator&#30340;&#29305;&#27530;&#24418;&#24335;&#34920;&#31034;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#19982;&#20998;&#20301;&#25968;&#20272;&#35745;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#29305;&#21035;&#26159;&#22312;&#36731;&#26494;&#35843;&#25972;&#25968;&#25454;&#38544;&#31169;&#32423;&#21035;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#29616;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gaussianity assumption has been consistently criticized as a main limitation of the Variational Autoencoder (VAE), despite its efficiency in computational modeling. In this paper, we propose a new approach that expands the model capacity (i.e., expressive power of distributional family) without sacrificing the computational advantages of the VAE framework. Our VAE model's decoder is composed of an infinite mixture of asymmetric Laplacian distribution, which possesses general distribution fitting capabilities for continuous variables. Our model is represented by a special form of a nonparametric M-estimator for estimating general quantile functions, and we theoretically establish the relevance between the proposed model and quantile estimation. We apply the proposed model to synthetic data generation, and particularly, our model demonstrates superiority in easily adjusting the level of data privacy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#38024;&#23545;&#32479;&#35745;&#32858;&#31867;&#30340;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#22797;&#21046;&#32858;&#31867;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#21253;&#25324;&#21033;&#29992;&#36817;&#20284;&#31639;&#27861;&#32452;&#21512;&#38382;&#39064;&#30340;&#40657;&#30418;&#26041;&#24335;&#35299;&#20915;&#32479;&#35745;$k$-medians&#12289;&#32479;&#35745;$k$-means&#21644;&#32479;&#35745;$k$-centers&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#34920;&#31034;&#31639;&#27861;&#22797;&#26434;&#24230;&#30340;&#20989;&#25968;&#21644;&#35823;&#24046;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.10359</link><description>&lt;p&gt;
&#21487;&#22797;&#21046;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Replicable Clustering. (arXiv:2302.10359v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#38024;&#23545;&#32479;&#35745;&#32858;&#31867;&#30340;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#22797;&#21046;&#32858;&#31867;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#21253;&#25324;&#21033;&#29992;&#36817;&#20284;&#31639;&#27861;&#32452;&#21512;&#38382;&#39064;&#30340;&#40657;&#30418;&#26041;&#24335;&#35299;&#20915;&#32479;&#35745;$k$-medians&#12289;&#32479;&#35745;$k$-means&#21644;&#32479;&#35745;$k$-centers&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#34920;&#31034;&#31639;&#27861;&#22797;&#26434;&#24230;&#30340;&#20989;&#25968;&#21644;&#35823;&#24046;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26368;&#36817;&#30001;Impagliazzo&#31561;&#20154;[2022]&#24341;&#20837;&#30340;&#21487;&#22797;&#21046;&#24615;&#27010;&#24565;&#19979;&#35774;&#35745;&#20102;&#22312;&#32479;&#35745;&#32858;&#31867;&#20013;&#21487;&#22797;&#21046;&#30340;&#31639;&#27861;&#12290;&#26681;&#25454;&#36825;&#20010;&#23450;&#20041;&#65292;&#22914;&#26524;&#19968;&#20010;&#32858;&#31867;&#31639;&#27861;&#26159;&#21487;&#22797;&#21046;&#30340;&#65292;&#37027;&#20040;&#22312;&#21516;&#19968;&#20998;&#24067;&#30340;&#20004;&#20010;&#19981;&#21516;&#36755;&#20837;&#19978;&#25191;&#34892;&#26102;&#65292;&#21482;&#35201;&#20854;&#20869;&#37096;&#38543;&#26426;&#24615;&#22312;&#25191;&#34892;&#20013;&#24471;&#21040;&#20849;&#20139;&#65292;&#23601;&#33021;&#39640;&#27010;&#29575;&#22320;&#20135;&#29983;&#23436;&#20840;&#30456;&#21516;&#30340;&#26679;&#26412;&#31354;&#38388;&#20998;&#21306;&#12290;&#25105;&#20204;&#36890;&#36807;&#40657;&#30418;&#30340;&#26041;&#24335;&#21033;&#29992;&#32452;&#21512;&#23545;&#24212;&#38382;&#39064;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#20026;&#32479;&#35745;$k$-medians&#12289;&#32479;&#35745;$k$-means&#21644;&#32479;&#35745;$k$-centers&#38382;&#39064;&#25552;&#20986;&#20102;&#36825;&#26679;&#30340;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#21487;&#22797;&#21046;&#30340;$O(1)$-&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#36866;&#29992;&#20110;&#32479;&#35745;&#27431;&#20960;&#37324;&#24471;$k$-medians ($k$-means)&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\operatorname{poly}(d)$&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;$O(1)$-&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#22312;&#32479;&#35745;&#27431;&#20960;&#37324;&#24471;$k$-centers$&#26102;&#20855;&#26377;&#39069;&#22806;&#30340;$O(1)$-&#21152;&#24615;&#35823;&#24046;&#65292;&#23613;&#31649;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\exp(d)$&#12290;
&lt;/p&gt;
&lt;p&gt;
We design replicable algorithms in the context of statistical clustering under the recently introduced notion of replicability from Impagliazzo et al. [2022]. According to this definition, a clustering algorithm is replicable if, with high probability, its output induces the exact same partition of the sample space after two executions on different inputs drawn from the same distribution, when its internal randomness is shared across the executions. We propose such algorithms for the statistical $k$-medians, statistical $k$-means, and statistical $k$-centers problems by utilizing approximation routines for their combinatorial counterparts in a black-box manner. In particular, we demonstrate a replicable $O(1)$-approximation algorithm for statistical Euclidean $k$-medians ($k$-means) with $\operatorname{poly}(d)$ sample complexity. We also describe an $O(1)$-approximation algorithm with an additional $O(1)$-additive error for statistical Euclidean $k$-centers, albeit with $\exp(d)$ samp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21453;&#39304;&#22270;&#30340;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#32531;&#35299;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#21487;&#34892;&#24615;&#21644;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.08631</link><description>&lt;p&gt;
&#21453;&#39304;&#22270;&#19978;&#30340;&#23454;&#29992;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Practical Contextual Bandits with Feedback Graphs. (arXiv:2302.08631v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21453;&#39304;&#22270;&#30340;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#32531;&#35299;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#21487;&#34892;&#24615;&#21644;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24773;&#22659;&#36172;&#21338;&#24050;&#32463;&#26377;&#25104;&#29087;&#30340;&#29702;&#35770;&#65292;&#20294;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#19981;&#21516;&#30340;&#21453;&#39304;&#27169;&#24335;&#26469;&#21152;&#36895;&#23398;&#20064;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#21453;&#39304;&#22270;&#19978;&#30340;&#36172;&#21338;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#26469;&#32531;&#35299;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#22522;&#20110;&#22238;&#24402;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#24050;&#30693;&#30340;&#26497;&#23567;&#26497;&#20540;&#65292;&#22240;&#27492;&#20943;&#23569;&#20102;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While contextual bandit has a mature theory, effectively leveraging different feedback patterns to enhance the pace of learning remains unclear. Bandits with feedback graphs, which interpolates between the full information and bandit regimes, provides a promising framework to mitigate the statistical complexity of learning. In this paper, we propose and analyze an approach to contextual bandits with feedback graphs based upon reduction to regression. The resulting algorithms are computationally practical and achieve established minimax rates, thereby reducing the statistical complexity in real-world applications.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#26410;&#37197;&#23545;&#30340;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#32447;&#24615;&#27169;&#22411;&#19979;&#32852;&#21512;&#20998;&#24067;&#21644;&#20849;&#20139;&#22240;&#26524;&#22270;&#30340;&#21487;&#35782;&#21035;&#26465;&#20214;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#20197;&#24674;&#22797;&#20849;&#20139;&#30340;&#28508;&#22312;&#22240;&#26524;&#22270;&#12290;</title><link>http://arxiv.org/abs/2302.00993</link><description>&lt;p&gt;
&#26410;&#37197;&#23545;&#30340;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unpaired Multi-Domain Causal Representation Learning. (arXiv:2302.00993v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00993
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#26410;&#37197;&#23545;&#30340;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#32447;&#24615;&#27169;&#22411;&#19979;&#32852;&#21512;&#20998;&#24067;&#21644;&#20849;&#20139;&#22240;&#26524;&#22270;&#30340;&#21487;&#35782;&#21035;&#26465;&#20214;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#20197;&#24674;&#22797;&#20849;&#20139;&#30340;&#28508;&#22312;&#22240;&#26524;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#30001;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#22312;&#21464;&#37327;&#32452;&#25104;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#24773;&#26223;&#65292;&#20854;&#20013;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#26469;&#33258;&#21487;&#33021;&#20849;&#20139;&#22240;&#26524;&#34920;&#31034;&#30340;&#22810;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#20551;&#35774;&#19981;&#21516;&#39046;&#22495;&#30340;&#35266;&#27979;&#32467;&#26524;&#26159;&#26410;&#37197;&#23545;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#21482;&#35266;&#23519;&#27599;&#20010;&#39046;&#22495;&#30340;&#36793;&#32536;&#20998;&#24067;&#65292;&#32780;&#19981;&#26159;&#23427;&#20204;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#32447;&#24615;&#27169;&#22411;&#19979;&#32852;&#21512;&#20998;&#24067;&#21644;&#20849;&#20139;&#22240;&#26524;&#22270;&#21487;&#35782;&#21035;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#21482;&#35201;&#25105;&#20204;&#33021;&#22815;&#20174;&#27599;&#20010;&#39046;&#22495;&#30340;&#36793;&#32536;&#20998;&#24067;&#20013;&#21807;&#19968;&#24674;&#22797;&#32852;&#21512;&#20998;&#24067;&#21644;&#20849;&#20139;&#22240;&#26524;&#34920;&#31034;&#65292;&#21487;&#35782;&#21035;&#24615;&#23601;&#33021;&#22815;&#25104;&#31435;&#12290;&#25105;&#20204;&#23558;&#35782;&#21035;&#24615;&#32467;&#26524;&#36716;&#21270;&#20026;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#24674;&#22797;&#20849;&#20139;&#30340;&#28508;&#22312;&#22240;&#26524;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of causal representation learning is to find a representation of data that consists of causally related latent variables. We consider a setup where one has access to data from multiple domains that potentially share a causal representation. Crucially, observations in different domains are assumed to be unpaired, that is, we only observe the marginal distribution in each domain but not their joint distribution. In this paper, we give sufficient conditions for identifiability of the joint distribution and the shared causal graph in a linear setup. Identifiability holds if we can uniquely recover the joint distribution and the shared causal representation from the marginal distributions in each domain. We transform our identifiability results into a practical method to recover the shared latent causal graph.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#22522;&#20110;&#20559;&#22909;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#21644;&#35774;&#35745;&#26032;&#30340;&#31574;&#30053;&#35780;&#20998;&#25351;&#26631;&#65292;&#33021;&#22815;&#20174;&#32473;&#23450;&#30340;&#20559;&#22909;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12842</link><description>&lt;p&gt;
&#19981;&#20381;&#36182;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#22522;&#20110;&#20559;&#22909;&#30340;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Preference-based Policy Optimization without Reward Modeling. (arXiv:2301.12842v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#22522;&#20110;&#20559;&#22909;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#21644;&#35774;&#35745;&#26032;&#30340;&#31574;&#30053;&#35780;&#20998;&#25351;&#26631;&#65292;&#33021;&#22815;&#20174;&#32473;&#23450;&#30340;&#20559;&#22909;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;(PbRL)&#26159;&#19968;&#31181;&#20351;RL&#20195;&#29702;&#33021;&#22815;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22312;&#21046;&#23450;&#22870;&#21169;&#20989;&#25968;&#26102;&#23384;&#22312;&#25361;&#25112;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;PbRL&#26041;&#27861;&#19968;&#33324;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#26681;&#25454;&#32473;&#23450;&#30340;&#20559;&#22909;&#25968;&#25454;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#27169;&#22411;&#37319;&#29992;&#29616;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20165;&#36890;&#36807;&#20559;&#22909;&#20449;&#24687;&#33719;&#21462;&#20934;&#30830;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22312;&#20559;&#22909;&#26469;&#33258;&#20154;&#31867;&#25945;&#24072;&#26102;&#65292;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;PbRL&#31639;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#35780;&#20998;&#25351;&#26631;&#65292;&#20026;&#19982;&#32473;&#23450;&#20559;&#22909;&#19968;&#33268;&#30340;&#31574;&#30053;&#20998;&#37197;&#39640;&#20998;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#24102;&#26377;&#23454;&#38469;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#30340;&#31163;&#32447;RL&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#25110;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference-based reinforcement learning (PbRL) is an approach that enables RL agents to learn from preference, which is particularly useful when formulating a reward function is challenging. Existing PbRL methods generally involve a two-step procedure: they first learn a reward model based on given preference data and then employ off-the-shelf reinforcement learning algorithms using the learned reward model. However, obtaining an accurate reward model solely from preference information, especially when the preference is from human teachers, can be difficult. Instead, we propose a PbRL algorithm that directly learns from preference without requiring any reward modeling. To achieve this, we adopt a contrastive learning framework to design a novel policy scoring metric that assigns a high score to policies that align with the given preferences. We apply our algorithm to offline RL tasks with actual human preference labels and show that our algorithm outperforms or is on par with the exist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20154;&#24037;&#21644;&#26426;&#22120;&#23457;&#26680;&#21592;&#30340;&#20849;&#24773;&#20882;&#29359;&#21644;&#22122;&#22768;&#23457;&#35745;&#30740;&#31350;&#20102;&#20882;&#29359;&#24615;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#24046;&#24322;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23457;&#26680;&#21592;&#20043;&#38388;&#23384;&#22312;&#24191;&#27867;&#30340;&#20998;&#27495;&#65292;&#24182;&#19988;&#20154;&#24037;&#23457;&#26680;&#21592;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#31867;&#22120;&#26080;&#27861;&#39044;&#27979;&#20854;&#20182;&#23457;&#26680;&#21592;&#30340;&#22238;&#24212;&#12290;&#36825;&#23545;&#20110;&#20869;&#23481;&#23457;&#26680;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2301.12534</link><description>&lt;p&gt;
&#20154;&#24037;&#21644;&#26426;&#22120;&#20851;&#20110;&#20160;&#20040;&#26159;&#20882;&#29359;&#23384;&#22312;&#36739;&#22823;&#20998;&#27495;&#30340;&#20849;&#24773;&#20882;&#29359;&#21644;&#22122;&#22768;&#23457;&#35745;&#65306;&#32479;&#19968;&#20027;&#35266;&#20882;&#29359;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive. (arXiv:2301.12534v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20154;&#24037;&#21644;&#26426;&#22120;&#23457;&#26680;&#21592;&#30340;&#20849;&#24773;&#20882;&#29359;&#21644;&#22122;&#22768;&#23457;&#35745;&#30740;&#31350;&#20102;&#20882;&#29359;&#24615;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#24046;&#24322;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23457;&#26680;&#21592;&#20043;&#38388;&#23384;&#22312;&#24191;&#27867;&#30340;&#20998;&#27495;&#65292;&#24182;&#19988;&#20154;&#24037;&#23457;&#26680;&#21592;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#31867;&#22120;&#26080;&#27861;&#39044;&#27979;&#20854;&#20182;&#23457;&#26680;&#21592;&#30340;&#22238;&#24212;&#12290;&#36825;&#23545;&#20110;&#20869;&#23481;&#23457;&#26680;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20882;&#29359;&#24615;&#35328;&#35770;&#26816;&#27979;&#26159;&#20869;&#23481;&#23457;&#26680;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20160;&#20040;&#26159;&#20882;&#29359;&#24615;&#30340;&#21487;&#20197;&#26159;&#39640;&#24230;&#20027;&#35266;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#28041;&#21450;&#21040;&#29616;&#23454;&#19990;&#30028;&#31038;&#20132;&#32593;&#31449;&#25919;&#27835;&#35328;&#35770;&#26102;&#65292;&#20154;&#24037;&#21644;&#26426;&#22120;&#23457;&#26680;&#21592;&#23545;&#20110;&#20160;&#20040;&#26159;&#20882;&#29359;&#24615;&#30340;&#23384;&#22312;&#20998;&#27495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65288;1&#65289;&#23457;&#26680;&#21592;&#20043;&#38388;&#65288;&#21253;&#25324;&#20154;&#24037;&#21644;&#26426;&#22120;&#65289;&#23384;&#22312;&#24191;&#27867;&#20998;&#27495;&#65307;&#21644;&#65288;2&#65289;&#20154;&#24037;&#23457;&#26680;&#21592;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#31867;&#22120;&#26080;&#27861;&#39044;&#27979;&#20854;&#20182;&#23457;&#26680;&#21592;&#22522;&#20110;&#20182;&#20204;&#30340;&#25919;&#27835;&#20542;&#21521;&#22914;&#20309;&#22238;&#24212;&#12290;&#23545;&#20110;&#65288;1&#65289;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#35268;&#27169;&#30340;&#22122;&#22768;&#23457;&#35745;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#21644;&#20154;&#24037;&#22238;&#31572;&#12290;&#23545;&#20110;&#65288;2&#65289;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#39318;&#21019;&#30340;&#20849;&#24773;&#20882;&#29359;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#22122;&#22768;&#23457;&#35745;&#25581;&#31034;&#20102;&#19981;&#21516;&#26426;&#22120;&#23457;&#26680;&#21592;&#20043;&#38388;&#30340;&#23457;&#26680;&#32467;&#26524;&#24046;&#24322;&#24456;&#22823;&#12290;&#25105;&#20204;&#19982;&#20154;&#24037;&#23457;&#26680;&#21592;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25919;&#27835;&#20542;&#21521;&#32467;&#21512;&#25935;&#24863;&#38382;&#39064;&#20250;&#24433;&#21709;&#21040;&#19968;&#23545;&#19968;&#30340;&#20882;&#29359;&#65292;&#20197;&#21450;&#20849;&#24773;&#20882;&#29359;&#12290;&#25968;&#25454;&#38598;&#21487;&#36890;&#36807;https://github.com/Homan-Lab/voic&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offensive speech detection is a key component of content moderation. However, what is offensive can be highly subjective. This paper investigates how machine and human moderators disagree on what is offensive when it comes to real-world social web political discourse. We show that (1) there is extensive disagreement among the moderators (humans and machines); and (2) human and large-language-model classifiers are unable to predict how other human raters will respond, based on their political leanings. For (1), we conduct a noise audit at an unprecedented scale that combines both machine and human responses. For (2), we introduce a first-of-its-kind dataset of vicarious offense. Our noise audit reveals that moderation outcomes vary wildly across different machine moderators. Our experiments with human moderators suggest that political leanings combined with sensitive issues affect both first-person and vicarious offense. The dataset is available through https://github.com/Homan-Lab/voic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ReSQue&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#22788;&#29702;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;ReSQue&#19982;&#26368;&#26032;&#30340;&#29699;&#39044;&#35328;&#21152;&#36895;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22312;&#24182;&#34892;&#21644;&#31169;&#23494;&#29615;&#22659;&#19979;&#36798;&#21040;&#26368;&#20808;&#36827;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#22312;&#21333;&#20301;&#29699;&#20013;&#30340;&#20984;&#20248;&#21270;&#30446;&#26631;&#65292;&#20316;&#32773;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#31526;&#21512;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20248;&#21270;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#20339;&#24635;&#20307;&#24037;&#20316;&#37327;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.00457</link><description>&lt;p&gt;
ReSQue&#30340;&#24182;&#34892;&#21644;&#31169;&#23494;&#38543;&#26426;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
ReSQueing Parallel and Private Stochastic Convex Optimization. (arXiv:2301.00457v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ReSQue&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#22788;&#29702;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;ReSQue&#19982;&#26368;&#26032;&#30340;&#29699;&#39044;&#35328;&#21152;&#36895;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22312;&#24182;&#34892;&#21644;&#31169;&#23494;&#29615;&#22659;&#19979;&#36798;&#21040;&#26368;&#20808;&#36827;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#22312;&#21333;&#20301;&#29699;&#20013;&#30340;&#20984;&#20248;&#21270;&#30446;&#26631;&#65292;&#20316;&#32773;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#31526;&#21512;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20248;&#21270;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#20339;&#24635;&#20307;&#24037;&#20316;&#37327;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;SCO&#65289;&#24037;&#20855;&#65306;&#19968;&#31181;&#22522;&#20110;&#37325;&#21152;&#26435;&#38543;&#26426;&#26597;&#35810;&#65288;ReSQue&#65289;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#19982;&#65288;&#39640;&#26031;&#65289;&#27010;&#29575;&#23494;&#24230;&#21367;&#31215;&#30340;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#23558;ReSQue&#19982;&#26368;&#26032;&#30340;&#29699;&#39044;&#35328;&#21152;&#36895;&#25216;&#26415;&#30456;&#32467;&#21512;[CJJJLST20, ACJJS21]&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22312;&#24182;&#34892;&#21644;&#31169;&#23494;&#29615;&#22659;&#19979;&#23454;&#29616;SCO&#30340;&#31639;&#27861;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#22797;&#26434;&#24230;&#12290;&#23545;&#20110;$\mathbb{R}^d$&#20013;&#34987;&#21333;&#20301;&#29699;&#32422;&#26463;&#30340;SCO&#30446;&#26631;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20197;&#19979;&#32467;&#26524;&#65288;&#22810;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24182;&#34892;&#31639;&#27861;&#65292;&#36890;&#36807;$d^{1/3}\epsilon_{\text{opt}}^{-2/3}$&#26799;&#24230;&#39044;&#35328;&#26597;&#35810;&#28145;&#24230;&#21644;$d^{1/3}\epsilon_{\text{opt}}^{-2/3} + \epsilon_{\text{opt}}^{-2}$&#24635;&#26799;&#24230;&#26597;&#35810;&#25968;&#65292;&#20197; $\epsilon_{\text{opt}}$ &#33719;&#24471;&#20248;&#21270;&#35823;&#24046;&#65292;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;&#26377;&#30028;&#26041;&#24046;&#30340;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#22120;&#12290;&#23545;&#20110;$\epsilon_{\text{opt}} \in [d^{-1}, d^{-1/4}]$&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;[BJLLS19]&#30340;&#26368;&#20808;&#36827;&#39044;&#35328;&#28145;&#24230;&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#20339;&#24635;&#20307;&#24037;&#20316;&#37327;
&lt;/p&gt;
&lt;p&gt;
We introduce a new tool for stochastic convex optimization (SCO): a Reweighted Stochastic Query (ReSQue) estimator for the gradient of a function convolved with a (Gaussian) probability density. Combining ReSQue with recent advances in ball oracle acceleration [CJJJLST20, ACJJS21], we develop algorithms achieving state-of-the-art complexities for SCO in parallel and private settings. For a SCO objective constrained to the unit ball in $\mathbb{R}^d$, we obtain the following results (up to polylogarithmic factors). We give a parallel algorithm obtaining optimization error $\epsilon_{\text{opt}}$ with $d^{1/3}\epsilon_{\text{opt}}^{-2/3}$ gradient oracle query depth and $d^{1/3}\epsilon_{\text{opt}}^{-2/3} + \epsilon_{\text{opt}}^{-2}$ gradient queries in total, assuming access to a bounded-variance stochastic gradient estimator. For $\epsilon_{\text{opt}} \in [d^{-1}, d^{-1/4}]$, our algorithm matches the state-of-the-art oracle depth of [BJLLS19] while maintaining the optimal total wor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19979;&#30028;&#20248;&#21270;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#35299;&#37322;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#65292;&#24182;&#36890;&#36807;&#25968;&#23398;&#37327;&#21270;&#36825;&#20123;&#35201;&#27714;&#12290;&#36890;&#36807;&#20248;&#21270;&#19979;&#30028;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2212.07056</link><description>&lt;p&gt;
&#20851;&#20110;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#30340;&#27010;&#29575;&#65306;&#19968;&#31181;&#19979;&#30028;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Probability of Necessity and Sufficiency of Explaining Graph Neural Networks: A Lower Bound Optimization Approach. (arXiv:2212.07056v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07056
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19979;&#30028;&#20248;&#21270;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#35299;&#37322;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#65292;&#24182;&#36890;&#36807;&#25968;&#23398;&#37327;&#21270;&#36825;&#20123;&#35201;&#27714;&#12290;&#36890;&#36807;&#20248;&#21270;&#19979;&#30028;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#21508;&#31181;GNN&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#23427;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#19968;&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#35299;&#37322;&#24212;&#35813;&#21516;&#26102;&#20855;&#26377;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#35299;&#37322;&#26041;&#27861;&#20165;&#20851;&#27880;&#20854;&#20013;&#20043;&#19968;&#65292;&#24517;&#35201;&#24615;&#25110;&#20805;&#20998;&#24615;&#65292;&#25110;&#32773;&#26159;&#20004;&#32773;&#20043;&#38388;&#30340;&#21551;&#21457;&#24335;&#26435;&#34913;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26377;&#28508;&#21147;&#30830;&#23450;&#26368;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25968;&#23398;&#19978;&#37327;&#21270;&#35299;&#37322;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#21333;&#35843;&#24615;&#21644;&#21453;&#20107;&#23454;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#33719;&#24471;PNS&#30340;&#22256;&#38590;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;PNS&#30340;&#19981;&#21487;&#35782;&#21035;&#24615;&#65292;&#25105;&#20204;&#27714;&#21161;&#20110;PNS&#30340;&#19968;&#20010;&#19979;&#30028;&#65292;&#21487;&#20197;&#36890;&#36807;&#21453;&#20107;&#23454;&#20272;&#35745;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;GNN&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#26694;&#26550;&#65288;NSEG&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#19979;&#30028;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The explainability of Graph Neural Networks (GNNs) is critical to various GNN applications, yet it remains a significant challenge. A convincing explanation should be both necessary and sufficient simultaneously. However, existing GNN explaining approaches focus on only one of the two aspects, necessity or sufficiency, or a heuristic trade-off between the two. Theoretically, the Probability of Necessity and Sufficiency (PNS) holds the potential to identify the most necessary and sufficient explanation since it can mathematically quantify the necessity and sufficiency of an explanation. Nevertheless, the difficulty of obtaining PNS due to non-monotonicity and the challenge of counterfactual estimation limit its wide use. To address the non-identifiability of PNS, we resort to a lower bound of PNS that can be optimized via counterfactual estimation, and propose a framework of Necessary and Sufficient Explanation for GNN (NSEG) via optimizing that lower bound. Specifically, we depict the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;&#21487;&#23450;&#21521;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31616;&#21333;&#28789;&#27963;&#30340;&#26500;&#24314;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25512;&#24191;&#21040;&#20219;&#20309;&#20855;&#26377;&#31561;&#21464;MLP&#30340;&#32676;G&#12290;</title><link>http://arxiv.org/abs/2212.06096</link><description>&lt;p&gt;
&#38544;&#24335;&#21367;&#31215;&#26680;&#29992;&#20110;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Implicit Convolutional Kernels for Steerable CNNs. (arXiv:2212.06096v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;&#21487;&#23450;&#21521;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31616;&#21333;&#28789;&#27963;&#30340;&#26500;&#24314;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25512;&#24191;&#21040;&#20219;&#20309;&#20855;&#26377;&#31561;&#21464;MLP&#30340;&#32676;G&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#26500;&#24314;&#19982;&#24179;&#31227;&#21644;&#20854;&#20182;&#21464;&#25442;&#31561;&#21516;&#21464;&#25442;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36825;&#20123;&#21464;&#25442;&#23646;&#20110;&#22522;&#20110;&#21407;&#28857;&#20445;&#25345;&#30340;&#32676;G&#65292;&#20363;&#22914;&#21453;&#23556;&#21644;&#26059;&#36716;&#12290;&#23427;&#20204;&#20381;&#36182;&#20110;&#36890;&#36807;&#22312;&#26680;&#31354;&#38388;&#19978;&#24378;&#21152;&#29305;&#23450;&#20110;&#32676;G&#30340;&#31561;&#21464;&#24615;&#32422;&#26463;&#26469;&#35299;&#26512;&#27714;&#35299;&#24471;&#21040;&#30340;G-&#23450;&#21521;&#21367;&#31215;&#26680;&#30340;&#26631;&#20934;&#21367;&#31215;&#12290;&#30001;&#20110;&#35299;&#20915;&#26041;&#26696;&#23545;&#29305;&#23450;&#30340;&#32676;G&#23450;&#21046;&#65292;&#26680;&#22522;&#30784;&#30340;&#23454;&#29616;&#19981;&#33021;&#25512;&#24191;&#21040;&#20854;&#20182;&#23545;&#31216;&#21464;&#25442;&#65292;&#36825;&#23548;&#33268;&#20102;&#36890;&#29992;&#32676;&#31561;&#21464;&#27169;&#22411;&#30340;&#24320;&#21457;&#22797;&#26434;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36890;&#36807;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#21442;&#25968;&#21270;G-&#23450;&#21521;&#21367;&#31215;&#26680;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12290;&#25152;&#24471;&#21040;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#28789;&#27963;&#30340;&#23454;&#29616;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#20110;&#20219;&#20309;&#21487;&#20197;&#26500;&#24314;G-&#31561;&#21464;MLP&#30340;&#32676;G&#37117;&#21487;&#20197;&#25512;&#24191;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;N&#20307;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steerable convolutional neural networks (CNNs) provide a general framework for building neural networks equivariant to translations and other transformations belonging to an origin-preserving group $G$, such as reflections and rotations. They rely on standard convolutions with $G$-steerable kernels obtained by analytically solving the group-specific equivariance constraint imposed onto the kernel space. As the solution is tailored to a particular group $G$, the implementation of a kernel basis does not generalize to other symmetry transformations, which complicates the development of general group equivariant models. We propose using implicit neural representation via multi-layer perceptrons (MLPs) to parameterize $G$-steerable kernels. The resulting framework offers a simple and flexible way to implement Steerable CNNs and generalizes to any group $G$ for which a $G$-equivariant MLP can be built. We prove the effectiveness of our method on multiple tasks, including N-body simulations,
&lt;/p&gt;</description></item><item><title>&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#39044;&#27979;&#32593;&#32476;&#36890;&#36807;&#38544;&#24335;&#26041;&#24046;&#27491;&#21017;&#21270;&#36991;&#20813;&#34920;&#31034;&#23849;&#28291;&#65292;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#20855;&#26377;&#19981;&#21516;&#30340;&#21160;&#24577;&#26426;&#21046;&#65292;&#24182;&#19988;&#29305;&#24449;&#20540;&#20316;&#20026;&#23398;&#20064;&#29575;&#20056;&#25968;&#12290;&#24341;&#20837;&#19968;&#26063;&#31561;&#21521;&#24615;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#24179;&#34913;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.04858</link><description>&lt;p&gt;
&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#38544;&#24335;&#26041;&#24046;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Implicit variance regularization in non-contrastive SSL. (arXiv:2212.04858v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04858
&lt;/p&gt;
&lt;p&gt;
&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#39044;&#27979;&#32593;&#32476;&#36890;&#36807;&#38544;&#24335;&#26041;&#24046;&#27491;&#21017;&#21270;&#36991;&#20813;&#34920;&#31034;&#23849;&#28291;&#65292;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#20855;&#26377;&#19981;&#21516;&#30340;&#21160;&#24577;&#26426;&#21046;&#65292;&#24182;&#19988;&#29305;&#24449;&#20540;&#20316;&#20026;&#23398;&#20064;&#29575;&#20056;&#25968;&#12290;&#24341;&#20837;&#19968;&#26063;&#31561;&#21521;&#24615;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#24179;&#34913;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;BYOL&#21644;SimSiam&#65289;&#20381;&#36182;&#20110;&#38750;&#23545;&#31216;&#39044;&#27979;&#32593;&#32476;&#26469;&#36991;&#20813;&#34920;&#31034;&#23849;&#28291;&#32780;&#26080;&#38656;&#36127;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#32593;&#32476;&#22914;&#20309;&#20419;&#36827;&#31283;&#23450;&#23398;&#20064;&#36824;&#19981;&#23436;&#20840;&#28165;&#26970;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#29702;&#35770;&#20998;&#26512;&#20551;&#35774;&#27431;&#20960;&#37324;&#24471;&#25439;&#22833;&#65292;&#20294;&#22823;&#22810;&#25968;&#23454;&#38469;&#23454;&#29616;&#20381;&#36182;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#38381;&#24335;&#32447;&#24615;&#39044;&#27979;&#32593;&#32476;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#20998;&#26512;&#30740;&#31350;&#23398;&#20064;&#21160;&#21147;&#23398;&#19982;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20004;&#32773;&#22343;&#36890;&#36807;&#38544;&#24335;&#26041;&#24046;&#27491;&#21017;&#21270;&#26469;&#36991;&#20813;&#23849;&#28291;&#65292;&#23613;&#31649;&#20855;&#26377;&#19981;&#21516;&#30340;&#21160;&#24577;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#29305;&#24449;&#20540;&#20316;&#20026;&#26377;&#25928;&#30340;&#23398;&#20064;&#29575;&#20056;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#26063;&#31561;&#21521;&#24615;&#25439;&#22833;&#20989;&#25968;&#65288;IsoLoss&#65289;&#65292;&#20197;&#22312;&#29305;&#24449;&#27169;&#24335;&#20043;&#38388;&#24179;&#34913;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;IsoLoss&#21152;&#36895;&#20102;&#21021;&#22987;&#23398;&#20064;&#21160;&#21147;&#23398;&#24182;&#22686;&#21152;&#20102;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#25670;&#33073;...
&lt;/p&gt;
&lt;p&gt;
Non-contrastive SSL methods like BYOL and SimSiam rely on asymmetric predictor networks to avoid representational collapse without negative samples. Yet, how predictor networks facilitate stable learning is not fully understood. While previous theoretical analyses assumed Euclidean losses, most practical implementations rely on cosine similarity. To gain further theoretical insight into non-contrastive SSL, we analytically study learning dynamics in conjunction with Euclidean and cosine similarity in the eigenspace of closed-form linear predictor networks. We show that both avoid collapse through implicit variance regularization albeit through different dynamical mechanisms. Moreover, we find that the eigenvalues act as effective learning rate multipliers and propose a family of isotropic loss functions (IsoLoss) that equalize convergence rates across eigenmodes. Empirically, IsoLoss speeds up the initial learning dynamics and increases robustness, thereby allowing us to dispense with 
&lt;/p&gt;</description></item><item><title>CORL&#26159;&#19968;&#20010;&#38754;&#21521;&#30740;&#31350;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#24211;&#65292;&#25552;&#20379;&#20102;&#32463;&#36807;&#20805;&#20998;&#22522;&#20934;&#27979;&#35797;&#30340;&#21333;&#25991;&#20214;&#23454;&#29616;&#31163;&#32447;&#21644;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#24320;&#21457;&#20307;&#39564;&#21644;&#23454;&#39564;&#36319;&#36394;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.07105</link><description>&lt;p&gt;
CORL: &#38754;&#21521;&#30740;&#31350;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#24211;
&lt;/p&gt;
&lt;p&gt;
CORL: Research-oriented Deep Offline Reinforcement Learning Library. (arXiv:2210.07105v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07105
&lt;/p&gt;
&lt;p&gt;
CORL&#26159;&#19968;&#20010;&#38754;&#21521;&#30740;&#31350;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#24211;&#65292;&#25552;&#20379;&#20102;&#32463;&#36807;&#20805;&#20998;&#22522;&#20934;&#27979;&#35797;&#30340;&#21333;&#25991;&#20214;&#23454;&#29616;&#31163;&#32447;&#21644;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#24320;&#21457;&#20307;&#39564;&#21644;&#23454;&#39564;&#36319;&#36394;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CORL&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#32463;&#36807;&#20805;&#20998;&#22522;&#20934;&#27979;&#35797;&#30340;&#21333;&#25991;&#20214;&#23454;&#29616;&#28145;&#24230;&#31163;&#32447;&#21644;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#23427;&#24378;&#35843;&#31616;&#21333;&#30340;&#24320;&#21457;&#20307;&#39564;&#65292;&#20855;&#26377;&#30452;&#35266;&#30340;&#20195;&#30721;&#24211;&#21644;&#29616;&#20195;&#20998;&#26512;&#36319;&#36394;&#24037;&#20855;&#12290;&#22312;CORL&#20013;&#65292;&#25105;&#20204;&#23558;&#26041;&#27861;&#23454;&#29616;&#38548;&#31163;&#21040;&#21333;&#29420;&#30340;&#21333;&#20010;&#25991;&#20214;&#20013;&#65292;&#20351;&#24615;&#33021;&#30456;&#20851;&#30340;&#32454;&#33410;&#26356;&#23481;&#26131;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#36319;&#36394;&#21151;&#33021;&#21487;&#29992;&#20110;&#24110;&#21161;&#35760;&#24405;&#25351;&#26631;&#12289;&#36229;&#21442;&#25968;&#12289;&#20381;&#36182;&#39033;&#31561;&#21040;&#20113;&#31471;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#24120;&#29992;&#30340;D4RL&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#20102;&#23454;&#29616;&#30340;&#21487;&#38752;&#24615;&#65292;&#25552;&#20379;&#20102;&#36879;&#26126;&#30340;&#32467;&#26524;&#28304;&#65292;&#21487;&#29992;&#20110;&#24378;&#22823;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#20363;&#22914;&#24615;&#33021;&#27010;&#35201;&#12289;&#25913;&#36827;&#27010;&#29575;&#25110;&#39044;&#26399;&#22312;&#32447;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
CORL is an open-source library that provides thoroughly benchmarked single-file implementations of both deep offline and offline-to-online reinforcement learning algorithms. It emphasizes a simple developing experience with a straightforward codebase and a modern analysis tracking tool. In CORL, we isolate methods implementation into separate single files, making performance-relevant details easier to recognize. Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, dependencies, and more to the cloud. Finally, we have ensured the reliability of the implementations by benchmarking commonly employed D4RL datasets providing a transparent source of results that can be reused for robust evaluation tools such as performance profiles, probability of improvement, or expected online performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35782;&#21035;&#20986;&#20855;&#26377;&#26368;&#39640;&#21512;&#29702;&#24615;&#19988;&#33021;&#22815;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#30340;&#23376;&#22270;&#12290;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;NP&#23436;&#20840;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.04627</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#22240;&#26524;&#32593;&#32476;&#20013;&#30340;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Causal Effect Identification in Uncertain Causal Networks. (arXiv:2208.04627v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35782;&#21035;&#20986;&#20855;&#26377;&#26368;&#39640;&#21512;&#29702;&#24615;&#19988;&#33021;&#22815;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#30340;&#23376;&#22270;&#12290;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;NP&#23436;&#20840;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#35782;&#21035;&#26159;&#22240;&#26524;&#25512;&#26029;&#25991;&#29486;&#30340;&#26680;&#24515;&#65292;&#22312;&#37027;&#37324;&#25552;&#20986;&#20102;&#23436;&#25972;&#30340;&#31639;&#27861;&#26469;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#22240;&#26524;&#26597;&#35810;&#12290;&#36825;&#20123;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#20381;&#36182;&#20110;&#23545;&#27491;&#30830;&#25351;&#23450;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#38480;&#23450;&#24615;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#21487;&#29992;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#27010;&#29575;&#27169;&#22411;&#30340;&#35774;&#32622;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22240;&#26524;&#22270;&#20013;&#30340;&#36793;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#34920;&#31034;&#39046;&#22495;&#19987;&#23478;&#30340;&#20449;&#20219;&#31243;&#24230;&#65292;&#20063;&#21487;&#20197;&#21453;&#26144;&#29305;&#23450;&#32479;&#35745;&#26816;&#39564;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#20135;&#29983;&#30340;&#38382;&#39064;&#26159;&#65306;&#22312;&#32473;&#23450;&#36825;&#26679;&#30340;&#27010;&#29575;&#22270;&#21644;&#24863;&#20852;&#36259;&#30340;&#29305;&#23450;&#22240;&#26524;&#25928;&#24212;&#30340;&#24773;&#20917;&#19979;&#65292;&#21738;&#20010;&#23376;&#22270;&#20855;&#26377;&#26368;&#39640;&#30340;&#21512;&#29702;&#24615;&#65292;&#24182;&#19988;&#22240;&#26524;&#25928;&#24212;&#33021;&#22815;&#35782;&#21035;&#65311;&#25105;&#20204;&#35777;&#26126;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#24402;&#32467;&#20026;&#35299;&#20915;&#19968;&#20010;NP&#23436;&#20840;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36793;&#35782;&#21035;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal identification is at the core of the causal inference literature, where complete algorithms have been proposed to identify causal queries of interest. The validity of these algorithms hinges on the restrictive assumption of having access to a correctly specified causal structure. In this work, we study the setting where a probabilistic model of the causal structure is available. Specifically, the edges in a causal graph exist with uncertainties which may, for example, represent degree of belief from domain experts. Alternatively, the uncertainty about an edge may reflect the confidence of a particular statistical test. The question that naturally arises in this setting is: Given such a probabilistic graph and a specific causal effect of interest, what is the subgraph which has the highest plausibility and for which the causal effect is identifiable? We show that answering this question reduces to solving an NP-complete combinatorial optimization problem which we call the edge ID
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411; (dlglm) &#21450;&#20854;&#22312;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#30340;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#22320;&#22788;&#29702;&#36755;&#20837;&#29305;&#24449;&#21644;&#21709;&#24212;&#30340;&#21487;&#24573;&#30053;&#21644;&#19981;&#21487;&#24573;&#30053;&#30340;&#32570;&#22833;&#27169;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#32479;&#35745;&#27169;&#25311;&#35777;&#26126;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#19981;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#31561;&#39046;&#22495;&#20013;&#12290;</title><link>http://arxiv.org/abs/2207.08911</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21450;&#20854;&#22312;&#32570;&#22833;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deeply-Learned Generalized Linear Models with Missing Data. (arXiv:2207.08911v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411; (dlglm) &#21450;&#20854;&#22312;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#30340;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#22320;&#22788;&#29702;&#36755;&#20837;&#29305;&#24449;&#21644;&#21709;&#24212;&#30340;&#21487;&#24573;&#30053;&#21644;&#19981;&#21487;&#24573;&#30053;&#30340;&#32570;&#22833;&#27169;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#32479;&#35745;&#27169;&#25311;&#35777;&#26126;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#19981;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#31561;&#39046;&#22495;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#22312;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#26174;&#33879;&#24212;&#29992;&#65292;&#20294;&#29616;&#20195;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#32570;&#22833;&#25968;&#25454;&#30340;&#26356;&#21152;&#26222;&#36941;&#21644;&#22797;&#26434;&#24615;&#32473;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;(dlglm)&#30340;&#27491;&#24335;&#22788;&#29702;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26102;&#33021;&#22815;&#28789;&#27963;&#22320;&#22788;&#29702;&#36755;&#20837;&#29305;&#24449;&#21644;&#21709;&#24212;&#30340;&#21487;&#24573;&#30053;&#21644;&#19981;&#21487;&#24573;&#30053;&#30340;&#32570;&#22833;&#27169;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#32479;&#35745;&#27169;&#25311;&#35777;&#26126;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#19981;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20197;UCI&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38134;&#34892;&#33829;&#38144;&#25968;&#25454;&#38598;&#20026;&#20363;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) methods have dramatically increased in popularity in recent years, with significant growth in their application to supervised learning problems in the biomedical sciences. However, the greater prevalence and complexity of missing data in modern biomedical datasets present significant challenges for DL methods. Here, we provide a formal treatment of missing data in the context of deeply learned generalized linear models, a supervised DL architecture for regression and classification problems. We propose a new architecture, \textit{dlglm}, that is one of the first to be able to flexibly account for both ignorable and non-ignorable patterns of missingness in input features and response at training time. We demonstrate through statistical simulation that our method outperforms existing approaches for supervised learning tasks in the presence of missing not at random (MNAR) missingness. We conclude with a case study of a Bank Marketing dataset from the UCI Machine Learnin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19982;&#20256;&#32479;&#30340;&#36864;&#28779;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#32463;&#20856;&#36864;&#28779;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#19981;&#30456;&#20851;&#30340;&#35299;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#37319;&#26679;&#26041;&#26696;&#30340;&#32570;&#28857;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#30340;&#20248;&#21270;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#24182;&#19982;&#27169;&#25311;&#36864;&#28779;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2207.08189</link><description>&lt;p&gt;
&#20351;&#29992;&#36864;&#28779;&#31639;&#27861;&#22686;&#24378;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Supplementing Recurrent Neural Networks with Annealing to Solve Combinatorial Optimization Problems. (arXiv:2207.08189v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19982;&#20256;&#32479;&#30340;&#36864;&#28779;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#32463;&#20856;&#36864;&#28779;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#19981;&#30456;&#20851;&#30340;&#35299;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#37319;&#26679;&#26041;&#26696;&#30340;&#32570;&#28857;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#30340;&#20248;&#21270;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#24182;&#19982;&#27169;&#25311;&#36864;&#28779;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#36890;&#36807;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;&#22914;&#27169;&#25311;&#36864;&#28779;&#65289;&#26469;&#27714;&#35299;&#65292;&#20197;&#36890;&#36807;&#28909;&#21147;&#27874;&#21160;&#22312;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#25216;&#26415;&#29983;&#25104;&#26032;&#30340;&#35299;&#12290;&#36825;&#31181;&#37319;&#26679;&#26041;&#26696;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#23616;&#38480;&#24615;&#65292;&#27604;&#22914;&#25910;&#25947;&#36895;&#24230;&#24930;&#19988;&#22312;&#20302;&#28201;&#19979;&#20542;&#21521;&#20110;&#20572;&#30041;&#22312;&#30456;&#21516;&#30340;&#23616;&#37096;&#25628;&#32034;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21464;&#20998;&#32463;&#20856;&#36864;&#28779;&#65288;VCA&#65289;&#26694;&#26550;&#65292;&#23558;&#33258;&#22238;&#24402;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#19982;&#20256;&#32479;&#36864;&#28779;&#30456;&#32467;&#21512;&#65292;&#20197;&#37319;&#26679;&#19981;&#30456;&#20851;&#30340;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;VCA&#20316;&#20026;&#35299;&#20915;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25506;&#32034;&#27604;&#36739;&#20102;VCA&#19982;&#27169;&#25311;&#36864;&#28779;&#22312;&#35299;&#20915;&#19977;&#20010;&#27969;&#34892;&#30340;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65306;&#26368;&#22823;&#21106;&#38382;&#39064;&#65288;Max-Cut&#65289;&#12289;&#25252;&#22763;&#25490;&#29677;&#38382;&#39064;&#65288;NSP&#65289;&#21644;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combinatorial optimization problems can be solved by heuristic algorithms such as simulated annealing (SA) which aims to find the optimal solution within a large search space through thermal fluctuations. The algorithm generates new solutions through Markov-chain Monte Carlo techniques. This sampling scheme can result in severe limitations, such as slow convergence and a tendency to stay within the same local search space at small temperatures. To overcome these shortcomings, we use the variational classical annealing (VCA) framework that combines autoregressive recurrent neural networks (RNNs) with traditional annealing to sample solutions that are uncorrelated. In this paper, we demonstrate the potential of using VCA as an approach to solving real-world optimization problems. We explore VCA's performance in comparison with SA at solving three popular optimization problems: the maximum cut problem (Max-Cut), the nurse scheduling problem (NSP), and the traveling salesman problem (TSP).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2207.07051</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#23545;&#25512;&#29702;&#20219;&#21153;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#25512;&#29702;&#26159;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#39640;&#20110;&#38543;&#26426;&#30340;&#24615;&#33021;&#65292;&#20294;&#23384;&#22312;&#35768;&#22810;&#19981;&#23436;&#21892;&#20043;&#22788;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#25277;&#35937;&#25512;&#29702;&#20063;&#26159;&#19981;&#23436;&#32654;&#30340;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25512;&#29702;&#21463;&#21040;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#21644;&#20449;&#24565;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#8220;&#20869;&#23481;&#25928;&#24212;&#8221;&#65307;&#24403;&#38382;&#39064;&#30340;&#35821;&#20041;&#20869;&#23481;&#25903;&#25345;&#27491;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#26102;&#65292;&#20154;&#31867;&#26356;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#20869;&#23481;&#32416;&#32544;&#30340;&#25512;&#29702;&#27169;&#24335;&#22312;&#20851;&#20110;&#20154;&#31867;&#26234;&#33021;&#22522;&#26412;&#24615;&#36136;&#30340;&#20105;&#35770;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#28151;&#20837;&#20869;&#23481;&#26469;&#22238;&#31572;&#36923;&#36753;&#38382;&#39064;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#25429;&#25417;&#20102;&#19968;&#20123;&#20154;&#31867;&#30693;&#35782;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#25506;&#32034;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#21028;&#26029;&#19977;&#27573;&#35770;&#30340;&#36923;&#36753;&#26377;&#25928;&#24615;&#21644;Wason&#36873;&#25321;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#32463;&#39564;&#24615;X&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;EXM&#65289;"&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#19981;&#21487;&#20998;&#35299;&#30446;&#26631;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20379;&#20102;&#31639;&#27861;&#22522;&#30784;&#30340;&#35814;&#32454;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2206.00439</link><description>&lt;p&gt;
&#31639;&#27861;&#22522;&#30784;&#30340;&#32463;&#39564;&#24615;X&#39118;&#38505;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Foundations of Empirical X-risk Minimization. (arXiv:2206.00439v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#32463;&#39564;&#24615;X&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;EXM&#65289;"&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#19981;&#21487;&#20998;&#35299;&#30446;&#26631;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20379;&#20102;&#31639;&#27861;&#22522;&#30784;&#30340;&#35814;&#32454;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#32463;&#39564;&#24615;X&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;EXM&#65289;"&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20248;&#21270;&#26694;&#26550;&#12290;X&#39118;&#38505;&#26159;&#19968;&#20010;&#29992;&#20110;&#34920;&#31034;&#19968;&#31867;&#32452;&#21512;&#24230;&#37327;&#25110;&#30446;&#26631;&#30340;&#26415;&#35821;&#65292;&#22312;&#20854;&#20013;&#65292;&#23558;&#27599;&#20010;&#25968;&#25454;&#28857;&#19982;&#22823;&#37327;&#30340;&#26126;&#30830;&#25110;&#38544;&#21547;&#30340;&#39033;&#30446;&#36827;&#34892;&#27604;&#36739;&#26469;&#23450;&#20041;&#39118;&#38505;&#20989;&#25968;&#12290;&#23427;&#21253;&#25324;&#35768;&#22810;&#24191;&#27867;&#20351;&#29992;&#30340;&#20195;&#29702;&#30446;&#26631;&#21644;&#19981;&#21487;&#20998;&#35299;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20363;&#22914;AUROC&#12289;AUPRC&#12289;&#37096;&#20998;AUROC&#12289;NDCG&#12289;MAP&#12289;&#22312;&#21069;K&#20010;&#20301;&#32622;&#30340;&#31934;&#30830;&#24230;/&#21484;&#22238;&#29575;&#12289;&#22312;&#29305;&#23450;&#21484;&#22238;&#29575;&#27700;&#24179;&#19978;&#30340;&#31934;&#30830;&#24230;&#12289;&#21015;&#34920;&#25439;&#22833;&#12289;p&#33539;&#25968;&#25512;&#23548;&#12289;&#39030;&#37096;&#25512;&#23548;&#12289;&#20840;&#23616;&#23545;&#27604;&#25439;&#22833;&#31561;&#12290;&#34429;&#28982;&#36825;&#20123;&#19981;&#21487;&#20998;&#35299;&#30340;&#30446;&#26631;&#21450;&#20854;&#20248;&#21270;&#31639;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20449;&#24687;&#26816;&#32034;&#31561;&#39046;&#22495;&#30340;&#25991;&#29486;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#30740;&#31350;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#38754;&#20020;&#30528;&#19968;&#20123;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;EXM&#30340;&#31639;&#27861;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#36817;&#30340;&#20005;&#26684;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
This manuscript introduces a new optimization framework for machine learning and AI, named {\bf empirical X-risk minimization (EXM)}. X-risk is a term introduced to represent a family of compositional measures or objectives, in which each data point is compared with a large number of items explicitly or implicitly for defining a risk function. It includes surrogate objectives of many widely used measures and non-decomposable losses, e.g., AUROC, AUPRC, partial AUROC, NDCG, MAP, precision/recall at top $K$ positions, precision at a certain recall level, listwise losses, p-norm push, top push, global contrastive losses, etc. While these non-decomposable objectives and their optimization algorithms have been studied in the literature of machine learning, computer vision, information retrieval, and etc, optimizing these objectives has encountered some unique challenges for deep learning. In this paper, we present recent rigorous efforts for EXM with a focus on its algorithmic foundations a
&lt;/p&gt;</description></item><item><title>AdaTask&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#20219;&#21153;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#24403;&#20219;&#21153;&#34987;&#38543;&#26426;&#28608;&#27963;&#26102;&#65292;&#23427;&#30340;&#36951;&#25022;&#27604;&#36816;&#34892;&#29420;&#31435;&#31639;&#27861;&#26356;&#23567;&#65292;&#21487;&#20197;&#25552;&#39640;&#19968;&#20010;&#22240;&#23376;&#36798;&#21040;$\sqrt{N}$&#12290;AdaTask&#36890;&#36807;&#39532;&#27663;&#36317;&#31163;&#21183;&#20989;&#25968;&#21644;&#21464;&#20998;&#34920;&#31034;&#21516;&#26102;&#23398;&#20064;&#20219;&#21153;&#21644;&#20219;&#21153;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2205.15802</link><description>&lt;p&gt;
AdaTask&#65306;&#33258;&#36866;&#24212;&#22810;&#20219;&#21153;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaTask: Adaptive Multitask Online Learning. (arXiv:2205.15802v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15802
&lt;/p&gt;
&lt;p&gt;
AdaTask&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#20219;&#21153;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#24403;&#20219;&#21153;&#34987;&#38543;&#26426;&#28608;&#27963;&#26102;&#65292;&#23427;&#30340;&#36951;&#25022;&#27604;&#36816;&#34892;&#29420;&#31435;&#31639;&#27861;&#26356;&#23567;&#65292;&#21487;&#20197;&#25552;&#39640;&#19968;&#20010;&#22240;&#23376;&#36798;&#21040;$\sqrt{N}$&#12290;AdaTask&#36890;&#36807;&#39532;&#27663;&#36317;&#31163;&#21183;&#20989;&#25968;&#21644;&#21464;&#20998;&#34920;&#31034;&#21516;&#26102;&#23398;&#20064;&#20219;&#21153;&#21644;&#20219;&#21153;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#24182;&#20998;&#26512;&#20102;AdaTask&#65292;&#19968;&#31181;&#36866;&#24212;&#20219;&#21153;&#26410;&#30693;&#32467;&#26500;&#30340;&#22810;&#20219;&#21153;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#24403;$N$&#20010;&#20219;&#21153;&#34987;&#38543;&#26426;&#28608;&#27963;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;AdaTask&#30340;&#36951;&#25022;&#27604;&#36816;&#34892;$N$&#20010;&#29420;&#31435;&#31639;&#27861;&#65288;&#27599;&#20010;&#20219;&#21153;&#19968;&#20010;&#31639;&#27861;&#65289;&#33719;&#24471;&#30340;&#36951;&#25022;&#35201;&#22909;&#65292;&#21487;&#20197;&#25552;&#39640;&#19968;&#20010;&#22240;&#23376;&#65292;&#36798;&#21040;$\sqrt{N}$&#12290;AdaTask&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;Follow-the-Regularized-Leader&#30340;&#27604;&#36739;&#22120;&#33258;&#36866;&#24212;&#29256;&#26412;&#65292;&#20351;&#29992;&#39532;&#27663;&#36317;&#31163;&#21183;&#20989;&#25968;&#12290;&#36890;&#36807;&#23545;&#36825;&#20010;&#21183;&#20989;&#25968;&#30340;&#21464;&#20998;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;AdaTask&#22914;&#20309;&#21516;&#26102;&#23398;&#20064;&#20219;&#21153;&#21644;&#23427;&#20204;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25903;&#25345;&#25105;&#20204;&#21457;&#29616;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and analyze AdaTask, a multitask online learning algorithm that adapts to the unknown structure of the tasks. When the $N$ tasks are stochastically activated, we show that the regret of AdaTask is better, by a factor that can be as large as $\sqrt{N}$, than the regret achieved by running $N$ independent algorithms, one for each task. AdaTask can be seen as a comparator-adaptive version of Follow-the-Regularized-Leader with a Mahalanobis norm potential. Through a variational formulation of this potential, our analysis reveals how AdaTask jointly learns the tasks and their structure. Experiments supporting our findings are presented.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#24102;&#32422;&#26463;&#23398;&#20064;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#30340;&#26041;&#27861;&#35770;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#21046;&#23450;&#27969;&#31243;&#65292;&#24182;&#21033;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#21487;&#34920;&#31034;&#24615;&#25429;&#25417;&#20915;&#31574;&#12289;&#19978;&#19979;&#25991;&#21464;&#37327;&#21644;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#22788;&#29702;&#23398;&#20064;&#25968;&#25454;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2111.04469</link><description>&lt;p&gt;
&#24102;&#32422;&#26463;&#23398;&#20064;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Mixed-Integer Optimization with Constraint Learning. (arXiv:2111.04469v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.04469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#24102;&#32422;&#26463;&#23398;&#20064;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#30340;&#26041;&#27861;&#35770;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#21046;&#23450;&#27969;&#31243;&#65292;&#24182;&#21033;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#21487;&#34920;&#31034;&#24615;&#25429;&#25417;&#20915;&#31574;&#12289;&#19978;&#19979;&#25991;&#21464;&#37327;&#21644;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#22788;&#29702;&#23398;&#20064;&#25968;&#25454;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#26041;&#27861;&#35770;&#22522;&#30784;&#65292;&#29992;&#20110;&#24102;&#32422;&#26463;&#23398;&#20064;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#21046;&#23450;&#27969;&#31243;&#65292;&#20854;&#20013;&#32422;&#26463;&#21644;&#30446;&#26631;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#23884;&#20837;&#21040;&#20248;&#21270;&#20844;&#24335;&#20013;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#21253;&#25324;&#32447;&#24615;&#27169;&#22411;&#12289;&#20915;&#31574;&#26641;&#12289;&#38598;&#25104;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65289;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#21487;&#34920;&#31034;&#24615;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#20915;&#31574;&#12289;&#19978;&#19979;&#25991;&#21464;&#37327;&#21644;&#32467;&#26524;&#20043;&#38388;&#30340;&#21508;&#31181;&#28508;&#22312;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#31181;&#22788;&#29702;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#35266;&#27979;&#20540;&#30340;&#20984;&#21253;&#26469;&#34920;&#24449;&#20915;&#31574;&#30340;&#20449;&#20219;&#21306;&#22495;&#65292;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#25512;&#33616;&#24182;&#36991;&#20813;&#22806;&#25512;&#12290;&#25105;&#20204;&#20351;&#29992;&#21015;&#29983;&#25104;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#36825;&#31181;&#34920;&#31034;&#21152;&#20837;&#21040;&#20248;&#21270;&#20844;&#24335;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#30340;&#20844;&#24335;&#26469;&#22788;&#29702;&#20302;&#23494;&#24230;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish a broad methodological foundation for mixed-integer optimization with learned constraints. We propose an end-to-end pipeline for data-driven decision making in which constraints and objectives are directly learned from data using machine learning, and the trained models are embedded in an optimization formulation. We exploit the mixed-integer optimization-representability of many machine learning methods, including linear models, decision trees, ensembles, and multi-layer perceptrons, which allows us to capture various underlying relationships between decisions, contextual variables, and outcomes. We also introduce two approaches for handling the inherent uncertainty of learning from data. First, we characterize a decision trust region using the convex hull of the observations, to ensure credible recommendations and avoid extrapolation. We efficiently incorporate this representation using column generation and propose a more flexible formulation to deal with low-density re
&lt;/p&gt;</description></item><item><title>NeuroBack&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65292;&#20351;&#24471;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2110.14053</link><description>&lt;p&gt;
NeuroBack: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks. (arXiv:2110.14053v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.14053
&lt;/p&gt;
&lt;p&gt;
NeuroBack&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65292;&#20351;&#24471;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#26159;&#19968;&#20010;&#24433;&#21709;&#21040;&#35268;&#21010;&#12289;&#39564;&#35777;&#21644;&#23433;&#20840;&#31561;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#30340;NP&#23436;&#20840;&#38382;&#39064;&#12290;&#20027;&#27969;&#30340;&#29616;&#20195;SAT&#27714;&#35299;&#22120;&#22522;&#20110;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#65288;CDCL&#65289;&#31639;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22686;&#24378;CDCL SAT&#27714;&#35299;&#22120;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#20040;&#27809;&#26377;&#20351;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#36164;&#28304;&#36827;&#34892;&#39057;&#32321;&#30340;&#22312;&#32447;&#27169;&#22411;&#25512;&#26029;&#12290;&#20026;&#20102;&#20351;GNN&#30340;&#25913;&#36827;&#21464;&#24471;&#23454;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroBack&#30340;&#26041;&#27861;&#65292;&#23427;&#24314;&#31435;&#22312;&#20004;&#20010;&#27934;&#23519;&#19978;&#65306;&#65288;1&#65289;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#65288;&#29978;&#33267;&#20840;&#37096;&#65289;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65288;&#21363;&#20540;&#65289;&#23545;&#20110;CDCL SAT&#27714;&#35299;&#33267;&#20851;&#37325;&#35201;&#65292;&#65288;2&#65289;&#22312;SAT&#27714;&#35299;&#24320;&#22987;&#20043;&#21069;&#65292;&#21482;&#38656;&#26597;&#35810;&#19968;&#27425;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21363;&#21487;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#31163;&#32447;&#27169;&#22411;&#25512;&#26029;&#20351;NeuroBack&#33021;&#22815;&#20165;&#22312;CPU&#19978;&#25191;&#34892;&#65292;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propositional satisfiability (SAT) is an NP-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural Networks (GNNs). However, so far this approach either has not made solving more effective, or required substantial GPU resources for frequent online model inferences. Aiming to make GNN improvements practical, this paper proposes an approach called NeuroBack, which builds on two insights: (1) predicting phases (i.e., values) of variables appearing in the majority (or even all) of the satisfying assignments are essential for CDCL SAT solving, and (2) it is sufficient to query the neural model only once for the predictions before the SAT solving starts. Once trained, the offline model inference allows NeuroBack to execute exclusively on the CPU, removing its reliance on GPU resources. To t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Eigenlearning&#26694;&#26550;&#65292;&#36890;&#36807;&#38480;&#21046;&#26680;&#22238;&#24402;&#22312;&#23398;&#20064;&#27491;&#20132;&#22522;&#20989;&#25968;&#26041;&#38754;&#30340;&#33021;&#21147;&#24182;&#21033;&#29992;&#23432;&#24658;&#23450;&#24459;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#20026;Nakkiran&#31561;&#20154;&#30340;&#8220;&#28145;&#24230;&#24341;&#23548;&#8221;&#29616;&#35937;&#65292;&#32463;&#20856;&#22855;&#20598;&#38382;&#39064;&#38590;&#24230;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#19982;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#31995;&#32479;&#36827;&#34892;&#20102;&#31867;&#27604;&#12290;</title><link>http://arxiv.org/abs/2110.03922</link><description>&lt;p&gt;
Eigenlearning&#26694;&#26550;&#65306;&#26680;&#22238;&#24402;&#21644;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#23432;&#24658;&#23450;&#24459;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks. (arXiv:2110.03922v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03922
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Eigenlearning&#26694;&#26550;&#65292;&#36890;&#36807;&#38480;&#21046;&#26680;&#22238;&#24402;&#22312;&#23398;&#20064;&#27491;&#20132;&#22522;&#20989;&#25968;&#26041;&#38754;&#30340;&#33021;&#21147;&#24182;&#21033;&#29992;&#23432;&#24658;&#23450;&#24459;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#20026;Nakkiran&#31561;&#20154;&#30340;&#8220;&#28145;&#24230;&#24341;&#23548;&#8221;&#29616;&#35937;&#65292;&#32463;&#20856;&#22855;&#20598;&#38382;&#39064;&#38590;&#24230;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#19982;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#31995;&#32479;&#36827;&#34892;&#20102;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#30340;&#27979;&#35797;&#39118;&#38505;&#21644;&#20854;&#20182;&#27867;&#21270;&#25351;&#26631;&#23548;&#20986;&#20102;&#31616;&#21333;&#30340;&#38381;&#24335;&#20272;&#35745;&#12290;&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#25512;&#23548;&#22823;&#22823;&#31616;&#21270;&#65292;&#26368;&#32456;&#34920;&#36798;&#24335;&#26356;&#26131;&#20110;&#35299;&#37322;&#12290;&#36825;&#20123;&#25913;&#36827;&#24471;&#30410;&#20110;&#25105;&#20204;&#35782;&#21035;&#20986;&#30340;&#19968;&#20010;&#23574;&#38160;&#30340;&#23432;&#24658;&#23450;&#24459;&#65292;&#23427;&#38480;&#21046;&#20102;KRR&#23398;&#20064;&#20219;&#20309;&#27491;&#20132;&#22522;&#20989;&#25968;&#30340;&#33021;&#21147;&#12290;&#27979;&#35797;&#39118;&#38505;&#21644;&#20854;&#20182;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#21487;&#20197;&#36879;&#26126;&#22320;&#29992;&#20110;&#25105;&#20204;&#22312;&#26680;&#29305;&#24449;&#22522;&#20013;&#35780;&#20272;&#30340;&#23432;&#24658;&#37327;&#26469;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#25913;&#36827;&#30340;&#26694;&#26550;&#26469;&#65306;i&#65289;&#20026;Nakkiran&#31561;&#20154;&#65288;2020&#65289;&#30340;&#8220;&#28145;&#24230;&#24341;&#23548;&#8221;&#25552;&#20379;&#29702;&#35770;&#35299;&#37322;&#65292;ii&#65289;&#25512;&#24191;&#20808;&#21069;&#20851;&#20110;&#32463;&#20856;&#22855;&#20598;&#38382;&#39064;&#38590;&#24230;&#30340;&#32467;&#26524;&#65292;iii&#65289;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#25552;&#20379;&#29702;&#35770;&#24037;&#20855;&#65292;&#24182;iv&#65289;&#22312;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#21644;&#29087;&#30693;&#31995;&#32479;&#20043;&#38388;&#30340;&#20005;&#23494;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive simple closed-form estimates for the test risk and other generalization metrics of kernel ridge regression (KRR). Relative to prior work, our derivations are greatly simplified and our final expressions are more readily interpreted. These improvements are enabled by our identification of a sharp conservation law which limits the ability of KRR to learn any orthonormal basis of functions. Test risk and other objects of interest are expressed transparently in terms of our conserved quantity evaluated in the kernel eigenbasis. We use our improved framework to: i) provide a theoretical explanation for the "deep bootstrap" of Nakkiran et al (2020), ii) generalize a previous result regarding the hardness of the classic parity problem, iii) fashion a theoretical tool for the study of adversarial robustness, and iv) draw a tight analogy between KRR and a well-studied system in statistical physics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#23545;&#19981;&#21516;&#20154;&#32676;&#30340;&#24433;&#21709;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#34429;&#28982;&#23427;&#20250;&#22312;&#25152;&#26377;&#20154;&#21475;&#32676;&#20307;&#20013;&#20135;&#29983;&#35823;&#24046;&#65292;&#20294;&#35823;&#24046;&#30340;&#31867;&#22411;&#20250;&#26377;&#31995;&#32479;&#24615;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24179;&#26435;&#20449;&#24687;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24046;&#24322;&#24182;&#25193;&#22823;&#26426;&#20250;&#30340;&#33719;&#21462;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#24179;&#26435;&#34892;&#21160;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2102.10019</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65306;&#24179;&#26435;&#34892;&#21160;&#19982;&#24179;&#26435;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information. (arXiv:2102.10019v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.10019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#23545;&#19981;&#21516;&#20154;&#32676;&#30340;&#24433;&#21709;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#34429;&#28982;&#23427;&#20250;&#22312;&#25152;&#26377;&#20154;&#21475;&#32676;&#20307;&#20013;&#20135;&#29983;&#35823;&#24046;&#65292;&#20294;&#35823;&#24046;&#30340;&#31867;&#22411;&#20250;&#26377;&#31995;&#32479;&#24615;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24179;&#26435;&#20449;&#24687;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24046;&#24322;&#24182;&#25193;&#22823;&#26426;&#20250;&#30340;&#33719;&#21462;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#24179;&#26435;&#34892;&#21160;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proves that uncertainty has a disparate impact on different demographic groups, with varying types of errors. The proposed strategy, called Affirmative Information, can eliminate this disparity and broaden access to opportunity, serving as an alternative to Affirmative Action.
&lt;/p&gt;
&lt;p&gt;
&#20687;&#36151;&#27454;&#25209;&#20934;&#12289;&#21307;&#30103;&#24178;&#39044;&#21644;&#22823;&#23398;&#24405;&#21462;&#36825;&#26679;&#30340;&#20851;&#38190;&#20915;&#31574;&#26159;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#27979;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#19981;&#24179;&#31561;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#23427;&#20250;&#22312;&#25152;&#26377;&#20154;&#21475;&#32676;&#20307;&#20013;&#20135;&#29983;&#35823;&#24046;&#65292;&#20294;&#35823;&#24046;&#30340;&#31867;&#22411;&#20250;&#26377;&#31995;&#32479;&#24615;&#30340;&#21464;&#21270;&#65306;&#24179;&#22343;&#32467;&#26524;&#36739;&#39640;&#30340;&#32676;&#20307;&#36890;&#24120;&#34987;&#20998;&#37197;&#26356;&#39640;&#30340;&#20551;&#38451;&#24615;&#29575;&#65292;&#32780;&#24179;&#22343;&#32467;&#26524;&#36739;&#20302;&#30340;&#32676;&#20307;&#21017;&#34987;&#20998;&#37197;&#26356;&#39640;&#30340;&#20551;&#38452;&#24615;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39069;&#22806;&#30340;&#25968;&#25454;&#33719;&#21462;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24046;&#24322;&#24182;&#25193;&#22823;&#26426;&#20250;&#30340;&#33719;&#21462;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#24179;&#26435;&#20449;&#24687;&#30340;&#31574;&#30053;&#21487;&#20197;&#20316;&#20026;&#24179;&#26435;&#34892;&#21160;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critical decisions like loan approvals, medical interventions, and college admissions are guided by predictions made in the presence of uncertainty. In this paper, we prove that uncertainty has a disparate impact. While it imparts errors across all demographic groups, the types of errors vary systematically: Groups with higher average outcomes are typically assigned higher false positive rates, while those with lower average outcomes are assigned higher false negative rates. We show that additional data acquisition can eliminate the disparity and broaden access to opportunity. The strategy, which we call Affirmative Information, could stand as an alternative to Affirmative Action.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;TLS&#21327;&#35758;&#30340;&#29616;&#20195;&#32593;&#39029;&#25351;&#32441;&#35782;&#21035;&#23545;&#25163;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#37327;&#32593;&#39029;&#12289;&#20934;&#30830;&#20998;&#31867;&#12289;&#20302;&#25104;&#26412;&#30340;TLS-specific&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#24212;&#30340;&#23545;&#31574;&#21644;&#23545;&#29616;&#26377;&#23545;&#31574;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2010.10294</link><description>&lt;p&gt;
&#20174;TLS&#36319;&#36394;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#32593;&#39029;&#25351;&#32441;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Adaptive Webpage Fingerprinting from TLS Traces. (arXiv:2010.10294v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.10294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;TLS&#21327;&#35758;&#30340;&#29616;&#20195;&#32593;&#39029;&#25351;&#32441;&#35782;&#21035;&#23545;&#25163;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#37327;&#32593;&#39029;&#12289;&#20934;&#30830;&#20998;&#31867;&#12289;&#20302;&#25104;&#26412;&#30340;TLS-specific&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#24212;&#30340;&#23545;&#31574;&#21644;&#23545;&#29616;&#26377;&#23545;&#31574;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#39029;&#25351;&#32441;&#35782;&#21035;&#20013;&#65292;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#27983;&#35272;&#22120;&#19982;&#32593;&#31449;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#21152;&#23494;TLS&#27969;&#37327;&#20013;&#30340;&#27169;&#24335;&#65292;&#36335;&#24452;&#19978;&#30340;&#23545;&#25163;&#33021;&#22815;&#25512;&#26029;&#20986;&#21463;&#23475;&#29992;&#25143;&#21152;&#36733;&#30340;&#20855;&#20307;&#32593;&#39029;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;TLS&#21327;&#35758;&#30740;&#31350;&#29616;&#20195;&#32593;&#39029;&#25351;&#32441;&#35782;&#21035;&#23545;&#25163;&#65292;&#26088;&#22312;&#25581;&#31034;&#20854;&#33021;&#21147;&#24182;&#25552;&#20379;&#28508;&#22312;&#30340;&#38450;&#24481;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#65288;&#20840;&#29699;&#22823;&#22810;&#25968;&#20114;&#32852;&#32593;&#29992;&#25143;&#20381;&#36182;&#20110;&#24102;&#26377;TLS&#30340;&#26631;&#20934;&#32593;&#39029;&#27983;&#35272;&#65289;&#21644;&#28508;&#22312;&#30340;&#23454;&#38469;&#24433;&#21709;&#65292;&#20294;&#22823;&#22810;&#25968;&#36807;&#21435;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#38024;&#23545;&#21311;&#21517;&#32593;&#32476;&#65288;&#22914;Tor&#65289;&#30340;&#29305;&#23450;&#25915;&#20987;&#19978;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#23450;&#20110;TLS&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#65306;1&#65289;&#25193;&#23637;&#21040;&#21069;&#25152;&#26410;&#26377;&#30340;&#30446;&#26631;&#32593;&#39029;&#25968;&#37327;&#65292;2&#65289;&#21487;&#20197;&#20934;&#30830;&#22320;&#23545;&#25968;&#21315;&#20010;&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#26410;&#36935;&#21040;&#30340;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#65292;3&#65289;&#21363;&#20351;&#22312;&#39057;&#32321;&#39029;&#38754;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#20063;&#20855;&#26377;&#36739;&#20302;&#30340;&#36816;&#33829;&#25104;&#26412;&#12290;&#22522;&#20110;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;TLS-specific&#30340;&#23545;&#31574;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#23545;&#31574;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In webpage fingerprinting, an on-path adversary infers the specific webpage loaded by a victim user by analysing the patterns in the encrypted TLS traffic exchanged between the user's browser and the website's servers. This work studies modern webpage fingerprinting adversaries against the TLS protocol; aiming to shed light on their capabilities and inform potential defences. Despite the importance of this research area (the majority of global Internet users rely on standard web browsing with TLS) and the potential real-life impact, most past works have focused on attacks specific to anonymity networks (e.g., Tor). We introduce a TLS-specific model that: 1) scales to an unprecedented number of target webpages, 2) can accurately classify thousands of classes it never encountered during training, and 3) has low operational costs even in scenarios of frequent page updates. Based on these findings, we then discuss TLS-specific countermeasures and evaluate the effectiveness of the existing 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#35745;&#31639;&#26694;&#26550;&#19979;&#30340;&#24847;&#35782;&#29702;&#35770;&#65292;&#31216;&#20026;&#8220;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#8221;&#65288;GARIM&#65289;&#12290;&#35813;&#29702;&#35770;&#35748;&#20026;&#24847;&#35782;&#25903;&#25345;&#23545;&#30446;&#26631;&#30456;&#20851;&#30340;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#20027;&#21160;&#25805;&#20316;&#65292;&#20351;&#20854;&#19982;&#36861;&#27714;&#30340;&#30446;&#26631;&#26356;&#21152;&#23545;&#40784;&#65292;&#20174;&#32780;&#22686;&#21152;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/1912.13490</link><description>&lt;p&gt;
&#24847;&#35782;&#30340;&#31070;&#32463;&#35745;&#31639;&#27169;&#22411;&#65306;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#29702;&#35770;&#65288;GARIM&#65289;
&lt;/p&gt;
&lt;p&gt;
A Neurocomputational Account of Consciousness: The Goal-Aligning Representation Internal Manipulation Theory (GARIM). (arXiv:1912.13490v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.13490
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#35745;&#31639;&#26694;&#26550;&#19979;&#30340;&#24847;&#35782;&#29702;&#35770;&#65292;&#31216;&#20026;&#8220;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#8221;&#65288;GARIM&#65289;&#12290;&#35813;&#29702;&#35770;&#35748;&#20026;&#24847;&#35782;&#25903;&#25345;&#23545;&#30446;&#26631;&#30456;&#20851;&#30340;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#20027;&#21160;&#25805;&#20316;&#65292;&#20351;&#20854;&#19982;&#36861;&#27714;&#30340;&#30446;&#26631;&#26356;&#21152;&#23545;&#40784;&#65292;&#20174;&#32780;&#22686;&#21152;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#35782;&#20316;&#20026;&#20154;&#31867;&#35748;&#30693;&#30340;&#26680;&#24515;&#35201;&#32032;&#65292;&#24050;&#32463;&#36890;&#36807;&#31070;&#32463;&#31185;&#23398;&#12289;&#24515;&#29702;&#23398;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#31561;&#22810;&#31181;&#31185;&#23398;&#26041;&#27861;&#36827;&#34892;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39046;&#22495;&#20043;&#38388;&#30340;&#19981;&#33391;&#25972;&#21512;&#38480;&#21046;&#20102;&#23545;&#24847;&#35782;&#30340;&#23436;&#25972;&#21644;&#28165;&#26224;&#29702;&#35299;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#31070;&#32463;&#35745;&#31639;&#26694;&#26550;&#19979;&#30340;&#8220;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#8221;&#65288;GARIM&#65289;&#24847;&#35782;&#29702;&#35770;&#65292;&#20026;&#25913;&#21892;&#36825;&#31181;&#25972;&#21512;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;GARIM&#29702;&#35770;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#65292;&#24847;&#35782;&#25903;&#25345;&#23545;&#30446;&#26631;&#30456;&#20851;&#30340;&#20869;&#37096;&#34920;&#31034;&#65288;&#22914;&#19990;&#30028;&#29366;&#24577;&#12289;&#23545;&#35937;&#21644;&#34892;&#20026;&#24207;&#21015;&#65289;&#36827;&#34892;&#20027;&#21160;&#25805;&#20316;&#65292;&#20351;&#23427;&#20204;&#19982;&#36861;&#27714;&#30340;&#30446;&#26631;&#26356;&#21152;&#23545;&#40784;&#12290;&#36825;&#20123;&#25805;&#20316;&#20351;&#24471;&#24847;&#35782;&#20195;&#29702;&#33021;&#22815;&#22312;&#20869;&#37096;&#20135;&#29983;&#20854;&#25152;&#32570;&#20047;&#30340;&#30693;&#35782;&#65292;&#20197;&#24212;&#23545;&#26032;&#26465;&#20214;&#21644;&#30446;&#26631;&#65292;&#20174;&#32780;&#22686;&#21152;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#28789;&#27963;&#24615;&#12290;&#34920;&#31034;&#30340;&#25805;&#20316;&#30001;&#22235;&#20010;&#31070;&#32463;&#21151;&#33021;&#23439;&#31995;&#32479;&#65288;Hierarc...
&lt;/p&gt;
&lt;p&gt;
Consciousness, a central element of human cognition, has been studied with multiple scientific approaches spanning neuroscience, psychology, artificial intelligence and robotics. Unfortunately, poor integration between these fields limits a full and clear understanding of consciousness. Here we contribute to improving this integration by proposing, within a neurocomputational framework, the `Goal-Aligning Representations Internal Manipulation' (GARIM) theory of consciousness. The central idea of the GARIM theory is that consciousness supports the active manipulation of goal-relevant internal representations (e.g., world states, objects, and action sequences), making them more aligned with the goals pursued. These manipulations allow the conscious agent to internally produce the knowledge it lacks to cope with novel conditions and goals, increasing the flexibility of goal-directed behaviour. The manipulation of representations is supported by four neuro-functional macro-systems (hierarc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#36807;&#31243;&#65292;&#21487;&#20197;&#32771;&#34385;&#20132;&#26131;&#25104;&#26412;&#21644;&#31354;&#22836;&#38480;&#21046;&#65292;&#24182;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/1911.11880</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Framework on Enhancing Portfolio Management with Reinforcement Learning. (arXiv:1911.11880v2 [q-fin.PM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.11880
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#36807;&#31243;&#65292;&#21487;&#20197;&#32771;&#34385;&#20132;&#26131;&#25104;&#26412;&#21644;&#31354;&#22836;&#38480;&#21046;&#65292;&#24182;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#26159;&#37329;&#34701;&#20013;&#28041;&#21450;&#23545;&#36164;&#37329;&#21644;&#36164;&#20135;&#36827;&#34892;&#25345;&#32493;&#37325;&#26032;&#37197;&#32622;&#20197;&#28385;&#36275;&#26399;&#26395;&#25910;&#30410;&#21644;&#39118;&#38505;&#37197;&#32622;&#30340;&#33402;&#26415;&#19982;&#31185;&#23398;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#22522;&#20110;&#37329;&#34701;&#25968;&#25454;&#35757;&#32451;RL&#20195;&#29702;&#20197;&#20248;&#21270;&#36164;&#20135;&#37325;&#26032;&#37197;&#32622;&#36807;&#31243;&#12290;&#23613;&#31649;&#20043;&#21069;&#26377;&#23581;&#35797;&#23558;RL&#19982;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#30456;&#32467;&#21512;&#65292;&#20294;&#20043;&#21069;&#30340;&#24037;&#20316;&#26410;&#32771;&#34385;&#20132;&#26131;&#25104;&#26412;&#25110;&#31354;&#22836;&#38480;&#21046;&#31561;&#23454;&#38469;&#26041;&#38754;&#65292;&#38480;&#21046;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;RL&#26694;&#26550;&#65292;&#29992;&#20110;&#36164;&#20135;&#31649;&#29702;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#36830;&#32493;&#30340;&#36164;&#20135;&#26435;&#37325;&#12289;&#31354;&#22836;&#20132;&#26131;&#20197;&#21450;&#21033;&#29992;&#30456;&#20851;&#29305;&#24449;&#36827;&#34892;&#20915;&#31574;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;RL&#31639;&#27861;&#30340;&#24615;&#33021;&#65306;&#31574;&#30053;&#26799;&#24230;&#19982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;PGAC&#65289;&#65292;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#21644;&#36827;&#21270;&#31574;&#30053;&#65288;ES&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Portfolio management is the art and science in fiance that concerns continuous reallocation of funds and assets across financial instruments to meet the desired returns to risk profile. Deep reinforcement learning (RL) has gained increasing interest in portfolio management, where RL agents are trained base on financial data to optimize the asset reallocation process. Though there are prior efforts in trying to combine RL and portfolio management, previous works did not consider practical aspects such as transaction costs or short selling restrictions, limiting their applicability. To address these limitations, we propose a general RL framework for asset management that enables continuous asset weights, short selling and making decisions with relevant features. We compare the performance of three different RL algorithms: Policy Gradient with Actor-Critic (PGAC), Proximal Policy Optimization (PPO), and Evolution Strategies (ES) and demonstrate their advantages in a simulated environment 
&lt;/p&gt;</description></item></channel></rss>