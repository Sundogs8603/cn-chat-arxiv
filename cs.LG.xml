<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#24182;&#25552;&#20379;&#20102;&#26465;&#20214;&#12290;&#36825;&#26159;&#23545;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#30340;&#36845;&#20195;&#26041;&#27861;&#30340;&#19968;&#31181;&#37325;&#35201;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2401.05394</link><description>&lt;p&gt;
&#36845;&#20195;&#27491;&#21017;&#21270;&#19982;k&#25903;&#25745;&#33539;&#25968;&#65306;&#31232;&#30095;&#24674;&#22797;&#30340;&#37325;&#35201;&#34917;&#20805;
&lt;/p&gt;
&lt;p&gt;
Iterative Regularization with k-Support Norm: an Important Complement to Sparse Recovery. (arXiv:2401.05394v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05394
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#24182;&#25552;&#20379;&#20102;&#26465;&#20214;&#12290;&#36825;&#26159;&#23545;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#30340;&#36845;&#20195;&#26041;&#27861;&#30340;&#19968;&#31181;&#37325;&#35201;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24674;&#22797;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#30001;&#20110;&#31232;&#30095;&#24674;&#22797;&#30340;NP&#22256;&#38590;&#24615;&#36136;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#21463;&#38480;&#20110;&#36866;&#29992;&#26465;&#20214;&#65288;&#29978;&#33267;&#26410;&#30693;&#65289;&#65292;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#26368;&#36817;&#65292;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#20986;&#29616;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#19968;&#27425;&#36890;&#36807;&#26469;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#26041;&#27861;&#20013;&#32321;&#29712;&#30340;&#32593;&#26684;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#36845;&#20195;&#26041;&#27861;&#37117;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#65292;&#38656;&#35201;&#21463;&#38480;&#30340;&#36866;&#29992;&#26465;&#20214;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#26356;&#24191;&#27867;&#30340;&#26465;&#20214;&#19979;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#22522;&#20110;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#32780;&#19981;&#26159;$\ell_1$&#33539;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;IRKSN&#36827;&#34892;&#31232;&#30095;&#24674;&#22797;&#30340;&#26465;&#20214;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse recovery is ubiquitous in machine learning and signal processing. Due to the NP-hard nature of sparse recovery, existing methods are known to suffer either from restrictive (or even unknown) applicability conditions, or high computational cost. Recently, iterative regularization methods have emerged as a promising fast approach because they can achieve sparse recovery in one pass through early stopping, rather than the tedious grid-search used in the traditional methods. However, most of those iterative methods are based on the $\ell_1$ norm which requires restrictive applicability conditions and could fail in many cases. Therefore, achieving sparse recovery with iterative regularization methods under a wider range of conditions has yet to be further explored. To address this issue, we propose a novel iterative regularization algorithm, IRKSN, based on the $k$-support norm regularizer rather than the $\ell_1$ norm. We provide conditions for sparse recovery with IRKSN, and compar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;PhilEO Bench&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;EO&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;EO&#39046;&#22495;&#20013;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#26694;&#26550;&#21253;&#25324;&#27979;&#35797;&#24179;&#21488;&#21644;&#19968;&#20010;400 GB Sentinel-2&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#24314;&#31569;&#23494;&#24230;&#20272;&#35745;&#12289;&#36947;&#36335;&#20998;&#21106;&#21644;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#30340;&#26631;&#31614;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.04464</link><description>&lt;p&gt;
PhilEO Bench: &#35780;&#20272;&#22320;&#29702;&#31354;&#38388;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PhilEO Bench: Evaluating Geo-Spatial Foundation Models. (arXiv:2401.04464v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;PhilEO Bench&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;EO&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;EO&#39046;&#22495;&#20013;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#26694;&#26550;&#21253;&#25324;&#27979;&#35797;&#24179;&#21488;&#21644;&#19968;&#20010;400 GB Sentinel-2&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#24314;&#31569;&#23494;&#24230;&#20272;&#35745;&#12289;&#36947;&#36335;&#20998;&#21106;&#21644;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#30340;&#26631;&#31614;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#21355;&#26143;&#25429;&#25417;&#21040;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#20854;&#20013;Sentinel-2&#26143;&#24231;&#27599;&#22825;&#20135;&#29983;1.6 TB&#30340;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#36965;&#24863;&#25104;&#20026;&#19968;&#20010;&#25968;&#25454;&#20016;&#23500;&#30340;&#39046;&#22495;&#65292;&#38750;&#24120;&#36866;&#21512;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;ML&#27169;&#22411;&#21040;EO&#39046;&#22495;&#30340;&#29942;&#39048;&#22312;&#20110;&#32570;&#20047;&#32463;&#36807;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#22240;&#20026;&#27880;&#37322;&#26159;&#19968;&#39033;&#36153;&#26102;&#36153;&#21147;&#30340;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;PhilEO Bench&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;EO&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#19981;&#21516;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#19968;&#33268;&#24615;&#22522;&#20934;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#21644;&#19968;&#20010;&#26032;&#39062;&#30340;400 GB Sentinel-2&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#26631;&#31614;&#65292;&#21363;&#24314;&#31569;&#23494;&#24230;&#20272;&#35745;&#12289;&#36947;&#36335;&#20998;&#21106;&#21644;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324;Prithvi&#21644;SatMAE&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive amounts of unlabelled data are captured by Earth Observation (EO) satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily. This makes Remote Sensing a data-rich domain well suited to Machine Learning (ML) solutions. However, a bottleneck in applying ML models to EO is the lack of annotated data as annotation is a labour-intensive and costly process. As a result, research in this domain has focused on Self-Supervised Learning and Foundation Model approaches. This paper addresses the need to evaluate different Foundation Models on a fair and uniform benchmark by introducing the PhilEO Bench, a novel evaluation framework for EO Foundation Models. The framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset containing labels for three downstream tasks, building density estimation, road segmentation, and land cover classification. We present experiments using our framework evaluating different Foundation Models, including Prithvi and SatMAE, at mu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#25968;&#25454;&#20026;&#20013;&#24515;&#8221;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#25968;&#25454;&#25910;&#38598;&#12289;&#22788;&#29702;&#21644;&#20998;&#26512;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#23558;&#29616;&#26377;&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65306;&#28145;&#24230;&#27169;&#22411;&#30340;&#35299;&#37322;&#12289;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#25968;&#25454;&#25366;&#25496;&#25805;&#20316;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;XAI&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04374</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#65306;&#19968;&#20010;&#25968;&#25454;&#25366;&#25496;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective. (arXiv:2401.04374v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#25968;&#25454;&#20026;&#20013;&#24515;&#8221;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#25968;&#25454;&#25910;&#38598;&#12289;&#22788;&#29702;&#21644;&#20998;&#26512;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#23558;&#29616;&#26377;&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65306;&#28145;&#24230;&#27169;&#22411;&#30340;&#35299;&#37322;&#12289;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#25968;&#25454;&#25366;&#25496;&#25805;&#20316;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;XAI&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#22797;&#26434;&#24615;&#21644;&#36879;&#26126;&#24230;&#19981;&#36275;&#65292;&#20154;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20197;&#20351;&#36825;&#20123;&#31995;&#32479;&#26356;&#20855;&#35299;&#37322;&#24615;&#25110;&#22312;&#21487;&#35775;&#38382;&#30340;&#26415;&#35821;&#20013;&#35299;&#37322;&#20854;&#34892;&#20026;&#12290;&#19982;&#22823;&#22810;&#25968;&#35780;&#35770;&#19981;&#21516;&#65292;&#35813;&#24037;&#20316;&#37319;&#29992;&#8220;&#25968;&#25454;&#20026;&#20013;&#24515;&#8221;&#30340;&#35266;&#28857;&#65292;&#30740;&#31350;&#25968;&#25454;&#25910;&#38598;&#65292;&#22788;&#29702;&#21644;&#20998;&#26512;&#22914;&#20309;&#20419;&#25104;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#24037;&#20316;&#20998;&#20026;&#19977;&#31867;&#65292;&#26681;&#25454;&#20854;&#30446;&#30340;&#36827;&#34892;&#20998;&#31867;&#65306;&#28145;&#24230;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#28041;&#21450;&#23558;&#25968;&#25454;&#28857;&#19982;&#27169;&#22411;&#36755;&#20986;&#30456;&#20851;&#32852;&#30340;&#29305;&#24449;&#24402;&#22240;&#21644;&#25512;&#29702;&#36807;&#31243;&#65307;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#35757;&#32451;&#25968;&#25454;&#32454;&#24494;&#24046;&#24322;&#65288;&#22914;&#25968;&#25454;&#35780;&#20272;&#21644;&#26679;&#26412;&#24322;&#24120;&#65289;&#23545;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65307;&#20197;&#21450;&#39046;&#22495;&#30693;&#35782;&#30340;&#35265;&#35299;&#65292;&#20174;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#21457;&#29616;&#28508;&#22312;&#27169;&#24335;&#65292;&#24182;&#20419;&#36827;&#31038;&#20250;&#20215;&#20540;&#21644;&#31185;&#23398;&#21457;&#29616;&#30340;&#26032;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;XAI&#26041;&#27861;&#35770;&#25552;&#28860;&#20026;&#25968;&#25454;&#25366;&#25496;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the complexity and lack of transparency in deep neural networks (DNNs), extensive efforts have been made to make these systems more interpretable or explain their behaviors in accessible terms. Unlike most reviews, which focus on algorithmic and model-centric perspectives, this work takes a "data-centric" view, examining how data collection, processing, and analysis contribute to explainable AI (XAI). We categorize existing work into three categories subject to their purposes: interpretations of deep models, referring to feature attributions and reasoning processes that correlate data points with model outputs; influences of training data, examining the impact of training data nuances, such as data valuation and sample anomalies, on decision-making processes; and insights of domain knowledge, discovering latent patterns and fostering new knowledge from data and models to advance social values and scientific discovery. Specifically, we distill XAI methodologies into data mining op
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#12290;&#32463;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;PINN&#26469;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#21040;&#39640;&#39057;&#29575;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#32593;&#32476;&#21442;&#25968;&#65292;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#28857;&#21644;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.02810</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#39640;&#39057;&#29575;&#21644;&#22810;&#23610;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning. (arXiv:2401.02810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#12290;&#32463;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;PINN&#26469;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#21040;&#39640;&#39057;&#29575;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#32593;&#32476;&#21442;&#25968;&#65292;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#28857;&#21644;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs/PDEs&#65289;&#30340;&#25968;&#25454;&#39537;&#21160;&#27714;&#35299;&#22120;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#21069;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#24120;&#24120;&#23548;&#33268;&#35757;&#32451;&#22833;&#36133;&#12290;&#24403;&#35299;&#20915;&#39640;&#39057;&#29575;&#21644;&#22810;&#23610;&#24230;&#38382;&#39064;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#35757;&#32451;PINN&#30340;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#24320;&#22987;&#35757;&#32451;&#65292;&#24182;&#36880;&#28176;&#25509;&#36817;&#39640;&#39057;&#29575;&#38382;&#39064;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;PINN&#26469;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#21040;&#39640;&#39057;&#29575;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#32593;&#32476;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#23427;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#28857;&#21644;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural network (PINN) is a data-driven solver for partial and ordinary differential equations(ODEs/PDEs). It provides a unified framework to address both forward and inverse problems. However, the complexity of the objective function often leads to training failures. This issue is particularly prominent when solving high-frequency and multi-scale problems. We proposed using transfer learning to boost the robustness and convergence of training PINN, starting training from low-frequency problems and gradually approaching high-frequency problems. Through two case studies, we discovered that transfer learning can effectively train PINN to approximate solutions from low-frequency problems to high-frequency problems without increasing network parameters. Furthermore, it requires fewer data points and less training time. We elaborately described our training strategy, including optimizer selection, and suggested guidelines for using transfer learning to train neural networks 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.02333</link><description>&lt;p&gt;
&#36229;&#36234;&#25552;&#21462;&#65306;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19978;&#19979;&#25991;&#21270;&#30340;&#34920;&#26684;&#25968;&#25454;&#20197;&#23454;&#29616;&#39640;&#25928;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models. (arXiv:2401.02333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104; (RAG) &#26550;&#26500;&#22312;&#20174;&#21508;&#31181;&#25991;&#20214;&#20013;&#26816;&#32034;&#20449;&#24687;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#21253;&#21547;&#22797;&#26434;&#34920;&#26684;&#32467;&#26500;&#30340; PDF &#25991;&#26723;&#20013;&#30340;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#26102;&#20250;&#36935;&#21040;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558; PDF &#23384;&#20648;&#22312;&#26816;&#32034;&#25968;&#25454;&#24211;&#20013;&#65292;&#24182;&#21333;&#29420;&#25552;&#21462;&#34920;&#26684;&#20869;&#23481;&#12290;&#25552;&#21462;&#30340;&#34920;&#26684;&#32463;&#36807;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#22788;&#29702;&#65292;&#23558;&#26631;&#39064;&#19982;&#30456;&#24212;&#30340;&#20540;&#36830;&#25509;&#36215;&#26469;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#20016;&#23500;&#25968;&#25454;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340; Llama-2-chat &#35821;&#35328;&#27169;&#22411;&#22312; RAG &#26550;&#26500;&#20013;&#36827;&#34892;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#27425;&#24615;&#25552;&#31034;&#20351;&#29992; ChatGPT 3.5 API &#22686;&#24378;&#34920;&#26684;&#25968;&#25454;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#20016;&#23500;&#30340;&#25968;&#25454;&#19982;&#20854;&#20182; PDF &#25991;&#20214;&#19968;&#36215;&#36755;&#20837;&#26816;&#32034;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents. However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures.This research introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately. The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values. To ensure a comprehensive understanding of the enriched data, we employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG architecture. Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt. This enriched data is then fed into the retrieval database alongside other PDFs. Our appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24212;&#23545;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#29615;&#22659;&#21160;&#24577;&#20551;&#35774;&#30340;&#38480;&#21046;&#21644;&#35268;&#21010;&#36807;&#31243;&#30340;&#24754;&#35266;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01841</link><description>&lt;p&gt;
&#25353;&#29031;&#20320;&#30340;&#23398;&#20064;&#34892;&#21160;&#65306;&#38750;&#31283;&#24577;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#33258;&#36866;&#24212;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes. (arXiv:2401.01841v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24212;&#23545;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#29615;&#22659;&#21160;&#24577;&#20551;&#35774;&#30340;&#38480;&#21046;&#21644;&#35268;&#21010;&#36807;&#31243;&#30340;&#24754;&#35266;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#65292;&#22788;&#29702;&#38750;&#31283;&#24577;&#29615;&#22659;&#26159;&#19968;&#20010;&#22522;&#26412;&#65288;&#19988;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#26410;&#35299;&#20915;&#30340;&#65289;&#25361;&#25112;&#65292;&#20854;&#20013;&#22806;&#37096;&#29615;&#22659;&#26465;&#20214;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#36825;&#31867;&#38382;&#39064;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#38750;&#31283;&#24577;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;NSMDP&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NSMDP&#20915;&#31574;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#20551;&#35774;&#24403;&#21069;&#26102;&#21051;&#26356;&#26032;&#30340;&#29615;&#22659;&#21160;&#24577;&#26159;&#24050;&#30693;&#30340;&#65288;&#23613;&#31649;&#26410;&#26469;&#21160;&#24577;&#21487;&#33021;&#20250;&#25913;&#21464;&#65289;&#65307;&#20854;&#27425;&#65292;&#35268;&#21010;&#36807;&#31243;&#20027;&#35201;&#26159;&#24754;&#35266;&#30340;&#65292;&#21363;&#20195;&#29702;&#20154;&#20250;&#8220;&#23433;&#20840;&#34892;&#21160;&#8221;&#20197;&#32771;&#34385;&#29615;&#22659;&#30340;&#38750;&#31283;&#24577;&#28436;&#21464;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20004;&#20010;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#26159;&#26080;&#25928;&#30340;-&#26356;&#26032;&#30340;&#29615;&#22659;&#26465;&#20214;&#24456;&#23569;&#26159;&#24050;&#30693;&#30340;&#65292;&#24182;&#19988;&#24403;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#26102;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#26356;&#26032;&#30340;&#21160;&#24577;&#24182;&#36991;&#20813;&#24754;&#35266;&#65292;&#33267;&#23569;&#22312;&#20854;&#23545;&#21160;&#24577;&#26377;&#20449;&#24515;&#30340;&#29366;&#24577;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental (and largely open) challenge in sequential decision-making is dealing with non-stationary environments, where exogenous environmental conditions change over time. Such problems are traditionally modeled as non-stationary Markov decision processes (NSMDP). However, existing approaches for decision-making in NSMDPs have two major shortcomings: first, they assume that the updated environmental dynamics at the current time are known (although future dynamics can change); and second, planning is largely pessimistic, i.e., the agent acts ``safely'' to account for the non-stationary evolution of the environment. We argue that both these assumptions are invalid in practice -updated environmental conditions are rarely known, and as the agent interacts with the environment, it can learn about the updated dynamics and avoid being pessimistic, at least in states whose dynamics it is confident about. We present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree Se
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#22270;&#29983;&#25104;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#23545;&#40784;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01326</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#22270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction. (arXiv:2401.01326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01326
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#22270;&#29983;&#25104;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#23545;&#40784;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#30340;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#19982;&#20256;&#32479;&#30340;&#29983;&#25104;&#24335;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#36328;&#24230;&#30340;&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#32447;&#24615;&#21270;&#30340;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#25991;&#26412;&#36328;&#24230;&#65292;&#36793;&#34920;&#31034;&#20851;&#31995;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#25351;&#21521;&#26426;&#21046;&#30340;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#21160;&#24577;&#35789;&#27719;&#34920;&#26469;&#34920;&#31034;&#36328;&#24230;&#21644;&#20851;&#31995;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36328;&#24230;&#34920;&#31034;&#25429;&#25417;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#32467;&#26500;&#29305;&#24449;&#21644;&#36793;&#30028;&#65292;&#21516;&#26102;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/urchade/ATG&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method for joint entity and relation extraction from unstructured text by framing it as a conditional sequence generation problem. In contrast to conventional generative information extraction models that are left-to-right token-level generators, our approach is \textit{span-based}. It generates a linearized graph where nodes represent text spans and edges represent relation triplets. Our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types. Our model can capture the structural characteristics and boundaries of entities and relations through span representations while simultaneously grounding the generated output in the original text thanks to the pointing mechanism. Evaluation on benchmark datasets validates the effectiveness of our approach, demonstrating competitive results. Code is available at https://github.com/urchade/ATG.
&lt;/p&gt;</description></item><item><title>&#36816;&#21160;&#20998;&#21106;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#22788;&#29702;&#25668;&#20687;&#26426;&#33258;&#25105;&#36816;&#21160;&#12289;&#40060;&#30524;&#38236;&#22836;&#24452;&#21521;&#30072;&#21464;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#25928;&#26524;&#36739;&#24046;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;WoodScape&#40060;&#30524;&#36816;&#21160;&#20998;&#21106;&#25361;&#25112;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#40060;&#30524;&#36816;&#21160;&#20998;&#21106;&#30340;&#27604;&#36187;&#20043;&#19968;&#65292;&#26088;&#22312;&#25506;&#32034;&#21644;&#35780;&#20272;&#20854;&#28508;&#21147;&#21644;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.00910</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;WoodScape&#36816;&#21160;&#20998;&#21106;--CVPR 2023 OmniCV&#30740;&#35752;&#20250;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV Workshop Challenge. (arXiv:2401.00910v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00910
&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#20998;&#21106;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#22788;&#29702;&#25668;&#20687;&#26426;&#33258;&#25105;&#36816;&#21160;&#12289;&#40060;&#30524;&#38236;&#22836;&#24452;&#21521;&#30072;&#21464;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#25928;&#26524;&#36739;&#24046;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;WoodScape&#40060;&#30524;&#36816;&#21160;&#20998;&#21106;&#25361;&#25112;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#40060;&#30524;&#36816;&#21160;&#20998;&#21106;&#30340;&#27604;&#36187;&#20043;&#19968;&#65292;&#26088;&#22312;&#25506;&#32034;&#21644;&#35780;&#20272;&#20854;&#28508;&#21147;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#20998;&#21106;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#22797;&#26434;&#20294;&#19981;&#21487;&#25110;&#32570;&#30340;&#20219;&#21153;&#12290;&#25668;&#20687;&#26426;&#30340;&#33258;&#25105;&#36816;&#21160;&#12289;&#40060;&#30524;&#38236;&#22836;&#30340;&#24452;&#21521;&#30072;&#21464;&#20197;&#21450;&#23545;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#38656;&#27714;&#24341;&#20837;&#20102;&#25361;&#25112;&#65292;&#20351;&#24471;&#20256;&#32479;&#21644;&#26631;&#20934;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26041;&#27861;&#25928;&#26524;&#36739;&#24046;&#12290;&#30456;&#24212;&#30340;&#32321;&#29712;&#30340;&#25968;&#25454;&#26631;&#27880;&#12289;&#22810;&#26679;&#21270;&#21644;&#32597;&#35265;&#24773;&#20917;&#30340;&#34920;&#24449;&#20197;&#21450;&#24191;&#27867;&#30340;&#25968;&#25454;&#37319;&#38598;&#38656;&#27714;&#24378;&#35843;&#20102;&#29992;&#20110;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;Parallel Domain&#24320;&#21457;&#30340;PD-WoodScape&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;WoodScape&#40060;&#30524;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WoodScape&#40060;&#30524;&#36816;&#21160;&#20998;&#21106;&#25361;&#25112;&#65292;&#20316;&#20026;CVPR 2023&#20840;&#26223;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;OmniCV&#65289;&#30740;&#35752;&#20250;&#30340;&#19968;&#37096;&#20998;&#20030;&#34892;&#12290;&#20316;&#20026;&#39318;&#20010;&#19987;&#27880;&#20110;&#40060;&#30524;&#36816;&#21160;&#20998;&#21106;&#30340;&#27604;&#36187;&#20043;&#19968;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#21644;&#35780;&#20272;&#20854;&#28508;&#21147;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion segmentation is a complex yet indispensable task in autonomous driving. The challenges introduced by the ego-motion of the cameras, radial distortion in fisheye lenses, and the need for temporal consistency make the task more complicated, rendering traditional and standard Convolutional Neural Network (CNN) approaches less effective. The consequent laborious data labeling, representation of diverse and uncommon scenarios, and extensive data capture requirements underscore the imperative of synthetic data for improving machine learning model performance. To this end, we employ the PD-WoodScape synthetic dataset developed by Parallel Domain, alongside the WoodScape fisheye dataset. Thus, we present the WoodScape fisheye motion segmentation challenge for autonomous driving, held as part of the CVPR 2023 Workshop on Omnidirectional Computer Vision (OmniCV). As one of the first competitions focused on fisheye motion segmentation, we aim to explore and evaluate the potential and impac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#26102;&#38388;&#30456;&#20851;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#22330;&#35770;&#20013;&#20174;&#24213;&#23618;&#33258;&#30001;&#29702;&#35770;&#21040;&#30446;&#26631;&#29702;&#35770;&#30340;&#31163;&#25955;-&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#12290;</title><link>http://arxiv.org/abs/2401.00828</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#27969;&#30340;&#37327;&#23376;&#22330;&#35770;&#22810;&#26684;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Lattice Sampling of Quantum Field Theories via Neural Operator-based Flows. (arXiv:2401.00828v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#26102;&#38388;&#30456;&#20851;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#22330;&#35770;&#20013;&#20174;&#24213;&#23618;&#33258;&#30001;&#29702;&#35770;&#21040;&#30446;&#26631;&#29702;&#35770;&#30340;&#31163;&#25955;-&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20174;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#20013;&#37319;&#26679;&#31163;&#25955;&#22330;&#37197;&#32622;$\phi$&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;$S$&#26159;&#26576;&#20010;&#37327;&#23376;&#22330;&#35770;&#36830;&#32493;&#27431;&#20960;&#37324;&#24471;&#20316;&#29992;$\mathcal S$&#30340;&#26684;&#28857;&#31163;&#25955;&#21270;&#12290;&#25105;&#20204;&#23558;&#35813;&#23494;&#24230;&#36817;&#20284;&#35270;&#20026;&#24213;&#23618;&#20989;&#25968;&#23494;&#24230;$[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$&#30340;&#23398;&#20064;&#31639;&#23376;&#23454;&#20363;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#20284;&#26102;&#38388;&#30456;&#20851;&#31639;&#23376;$\mathcal V_t$&#30340;&#26041;&#27861;&#65292;&#20854;&#26102;&#38388;&#31215;&#20998;&#25552;&#20379;&#20102;&#33258;&#30001;&#29702;&#35770;$[\mathcal D\phi(x)]\mathcal Z_0^{-1}e^{-\mathcal S_{0}[\phi(x)]}$&#30340;&#20989;&#25968;&#20998;&#24067;&#19982;&#30446;&#26631;&#29702;&#35770;$[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#24403;&#36873;&#25321;&#29305;&#23450;&#30340;&#26684;&#28857;&#26102;&#65292;&#31639;&#23376;$\mathcal V_t$&#21487;&#20197;&#31163;&#25955;&#21270;&#20026;&#26377;&#38480;&#32500;&#30340;&#26102;&#38388;&#30456;&#20851;&#30690;&#37327;&#22330;$V_t$&#65292;&#20174;&#32780;&#22312;&#31163;&#25955;&#26684;&#28857;&#19978;&#23454;&#29616;&#20102;&#36830;&#32493;&#30340;&#24402;&#19968;&#21270;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sampling discrete field configurations $\phi$ from the Boltzmann distribution $[d\phi] Z^{-1} e^{-S[\phi]}$, where $S$ is the lattice-discretization of the continuous Euclidean action $\mathcal S$ of some quantum field theory. Since such densities arise as the approximation of the underlying functional density $[\mathcal D\phi(x)] \mathcal Z^{-1} e^{-\mathcal S[\phi(x)]}$, we frame the task as an instance of operator learning. In particular, we propose to approximate a time-dependent operator $\mathcal V_t$ whose time integral provides a mapping between the functional distributions of the free theory $[\mathcal D\phi(x)] \mathcal Z_0^{-1} e^{-\mathcal S_{0}[\phi(x)]}$ and of the target theory $[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$. Whenever a particular lattice is chosen, the operator $\mathcal V_t$ can be discretized to a finite dimensional, time-dependent vector field $V_t$ which in turn induces a continuous normalizing flow between fi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#30340;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.00773</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#30340;&#23376;&#25277;&#26679;&#38598;&#21512;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Outlier Detection using Random Subspace and Subsampling Ensembles of Dirichlet Process Mixtures. (arXiv:2401.00773v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00773
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#30340;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#28151;&#21512;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#32479;&#35745;&#21407;&#29702;&#19978;&#26377;&#30452;&#35266;&#22522;&#30784;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#20316;&#20026;&#20256;&#32479;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#22312;&#32858;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#26126;&#26174;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#24191;&#27867;&#37319;&#29992;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#21463;&#21040;&#19982;&#26500;&#24314;&#26816;&#27979;&#22120;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#24322;&#24120;&#20540;&#30340;&#25935;&#24863;&#24615;&#26377;&#20851;&#30340;&#25361;&#25112;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#38598;&#21512;&#30340;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#65292;&#19981;&#20165;&#30830;&#20445;&#20102;&#39640;&#25928;&#35745;&#31639;&#65292;&#36824;&#22686;&#24378;&#20102;&#32467;&#26524;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic mixture models are acknowledged as a valuable tool for unsupervised outlier detection owing to their interpretability and intuitive grounding in statistical principles. Within this framework, Dirichlet process mixture models emerge as a compelling alternative to conventional finite mixture models for both clustering and outlier detection tasks. However, despite their evident advantages, the widespread adoption of Dirichlet process mixture models in unsupervised outlier detection has been hampered by challenges related to computational inefficiency and sensitivity to outliers during the construction of detectors. To tackle these challenges, we propose a novel outlier detection method based on ensembles of Dirichlet process Gaussian mixtures. The proposed method is a fully unsupervised algorithm that capitalizes on random subspace and subsampling ensembles, not only ensuring efficient computation but also enhancing the robustness of the resulting outlier detector. Moreover,
&lt;/p&gt;</description></item><item><title>&#22312;&#28145;&#24230;&#21704;&#23494;&#39039;&#22238;&#24402;&#20013;&#65292;&#23454;&#29616;&#21327;&#26041;&#24046;&#21644;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#32423;&#32852;&#22238;&#24402;&#26694;&#26550;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#21327;&#21464;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#24182;&#20135;&#29983;&#21327;&#21464;&#29305;&#24449;&#21644;&#22522;&#32447;&#39044;&#27979;&#65292;&#36741;&#21161;&#31532;&#20108;&#38454;&#27573;&#23398;&#20064;&#21327;&#26041;&#24046;&#12290;&#21516;&#26102;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#38750;&#32447;&#24615;&#22270;&#24418;Transformer&#32593;&#32476;&#36827;&#34892;&#32467;&#26500;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21704;&#23494;&#39039;&#39044;&#27979;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.00744</link><description>&lt;p&gt;
&#22312;&#26230;&#20307;&#26448;&#26009;&#30740;&#31350;&#20013;&#65292;&#23558;&#21327;&#26041;&#24046;&#21644;&#34920;&#36798;&#33021;&#21147;&#34701;&#21512;&#20026;&#28145;&#24230;&#21704;&#23494;&#39039;&#22238;&#24402;&#65306;&#19968;&#31181;&#28151;&#21512;&#32423;&#32852;&#22238;&#24402;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework. (arXiv:2401.00744v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00744
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#21704;&#23494;&#39039;&#22238;&#24402;&#20013;&#65292;&#23454;&#29616;&#21327;&#26041;&#24046;&#21644;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#32423;&#32852;&#22238;&#24402;&#26694;&#26550;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#21327;&#21464;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#24182;&#20135;&#29983;&#21327;&#21464;&#29305;&#24449;&#21644;&#22522;&#32447;&#39044;&#27979;&#65292;&#36741;&#21161;&#31532;&#20108;&#38454;&#27573;&#23398;&#20064;&#21327;&#26041;&#24046;&#12290;&#21516;&#26102;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#38750;&#32447;&#24615;&#22270;&#24418;Transformer&#32593;&#32476;&#36827;&#34892;&#32467;&#26500;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21704;&#23494;&#39039;&#39044;&#27979;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#30740;&#31350;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#21704;&#23494;&#39039;&#22238;&#24402;&#37327;&#23376;&#31995;&#32479;&#38656;&#35201;&#28385;&#36275;&#21327;&#26041;&#24046;&#23450;&#24459;&#65292;&#20854;&#20013;&#23454;&#29616;SO(3)&#31561;&#21464;&#24615;&#32780;&#19981;&#25439;&#22833;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#26159;&#19968;&#20010;&#38590;&#20197;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#38750;&#32447;&#24615;&#26144;&#23556;&#30340;&#29702;&#35770;&#31561;&#21464;&#24615;&#20445;&#35777;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#21327;&#26041;&#24046;-&#34920;&#36798;&#33021;&#21147;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#20998;&#20026;&#20004;&#20010;&#32423;&#32852;&#22238;&#24402;&#38454;&#27573;&#12290;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#19968;&#20010;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#21327;&#21464;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#19977;&#32500;&#21407;&#23376;&#31995;&#32479;&#30340;&#23545;&#31216;&#24615;&#65292;&#20135;&#29983;&#29702;&#35770;&#19978;&#30340;&#21327;&#21464;&#29305;&#24449;&#21644;&#22522;&#32447;&#21704;&#23494;&#39039;&#39044;&#27979;&#65292;&#24110;&#21161;&#31532;&#20108;&#38454;&#27573;&#23398;&#20064;&#21327;&#21464;&#24615;&#12290;&#21516;&#26102;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#38750;&#32447;&#24615;&#19977;&#32500;&#22270;&#24418;Transformer&#32593;&#32476;&#26469;&#36827;&#34892;&#19977;&#32500;&#21407;&#23376;&#31995;&#32479;&#30340;&#32467;&#26500;&#24314;&#27169;&#65292;&#23558;&#31532;&#19968;&#38454;&#27573;&#30340;&#36755;&#20986;&#31934;&#32454;&#21270;&#20026;&#20855;&#26377;&#26356;&#22909;&#34920;&#36798;&#33021;&#21147;&#30340;&#21704;&#23494;&#39039;&#39044;&#27979;&#12290;&#36890;&#36807;&#29702;&#35770;&#19978;&#30340;&#21327;&#21464;&#24615;&#21644;&#26356;&#22909;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23545;&#21704;&#23494;&#39039;&#22238;&#24402;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning for Hamiltonian regression of quantum systems in material research necessitates satisfying the covariance laws, among which achieving SO(3)-equivariance without sacrificing the expressiveness of networks remains an elusive challenge due to the restriction to non-linear mappings on guaranteeing theoretical equivariance. To alleviate the covariance-expressiveness dilemma, we propose a hybrid framework with two cascaded regression stages. The first stage, with a theoretically-guaranteed covariant neural network modeling symmetry properties of 3D atom systems, yields theoretically covariant features and baseline Hamiltonian predictions, assisting the second stage in learning covariance. Meanwhile, the second stage, powered by a non-linear 3D graph Transformer network we propose for structural modeling of 3D atomic systems, refines the first stage's output as a fine-grained prediction of Hamiltonians with better expressiveness capability. The combination of a theoretically cov
&lt;/p&gt;</description></item><item><title>MosaicBERT&#26159;&#19968;&#31181;&#20248;&#21270;&#30340;BERT&#39118;&#26684;&#21452;&#21521;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#39033;&#21019;&#26032;&#25216;&#26415;&#21644;&#20248;&#21270;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2312.17482</link><description>&lt;p&gt;
MosaicBERT&#65306;&#19968;&#31181;&#38024;&#23545;&#24555;&#36895;&#39044;&#35757;&#32451;&#36827;&#34892;&#20248;&#21270;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining. (arXiv:2312.17482v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17482
&lt;/p&gt;
&lt;p&gt;
MosaicBERT&#26159;&#19968;&#31181;&#20248;&#21270;&#30340;BERT&#39118;&#26684;&#21452;&#21521;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#39033;&#21019;&#26032;&#25216;&#26415;&#21644;&#20248;&#21270;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;BERT&#39118;&#26684;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#22312;NLP&#30740;&#31350;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#19981;&#20250;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#33258;&#24049;&#30340;BERT&#12290;&#22312;BERT&#39318;&#27425;&#23853;&#38706;&#22836;&#35282;&#30340;&#36807;&#21435;&#21322;-decade&#65292;&#24050;&#32463;&#23545;&#20854;&#20182;&#21464;&#21387;&#22120;&#26550;&#26500;&#21644;&#35757;&#32451;&#37197;&#32622;&#36827;&#34892;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#23578;&#26410;&#31995;&#32479;&#22320;&#32435;&#20837;BERT&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MosaicBERT&#65292;&#19968;&#31181;&#32463;&#39564;&#20248;&#21270;&#29992;&#20110;&#24555;&#36895;&#39044;&#35757;&#32451;&#30340;BERT&#39118;&#26684;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#31181;&#39640;&#25928;&#30340;&#26550;&#26500;&#23558;FlashAttention&#12289;&#24102;&#26377;&#32447;&#24615;&#20559;&#24046;&#30340;Attention (ALiBi)&#12289;&#38376;&#25511;&#32447;&#24615;&#21333;&#20803; (GLU)&#12289;&#21160;&#24577;&#31227;&#38500;&#22635;&#20805;&#20196;&#29260;&#30340;&#27169;&#22359;&#21644;&#20302;&#31934;&#24230;LayerNorm&#31561;&#24341;&#20837;&#20102;&#32463;&#20856;&#30340;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#22359;&#12290;&#35757;&#32451;&#26041;&#27861;&#36824;&#21253;&#25324;30%&#30340;&#25513;&#30721;&#27604;&#29575;&#29992;&#20110;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169; (MLM) &#30446;&#26631;&#65292;bfloat16&#31934;&#24230;&#65292;&#20197;&#21450;&#38024;&#23545;GPU&#21534;&#21520;&#37327;&#36827;&#34892;&#20248;&#21270;&#30340;&#35789;&#27719;&#22823;&#23567;&#65292;&#27492;&#22806;&#36824;&#37319;&#29992;&#20102;RoBERTa&#21644;&#20854;&#20182;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although BERT-style encoder models are heavily used in NLP research, many researchers do not pretrain their own BERTs from scratch due to the high cost of training. In the past half-decade since BERT first rose to prominence, many advances have been made with other transformer architectures and training configurations that have yet to be systematically incorporated into BERT. Here, we introduce MosaicBERT, a BERT-style encoder architecture and training recipe that is empirically optimized for fast pretraining. This efficient architecture incorporates FlashAttention, Attention with Linear Biases (ALiBi), Gated Linear Units (GLU), a module to dynamically remove padded tokens, and low precision LayerNorm into the classic transformer encoder block. The training recipe includes a 30% masking ratio for the Masked Language Modeling (MLM) objective, bfloat16 precision, and vocabulary size optimized for GPU throughput, in addition to best-practices from RoBERTa and other encoder models. When pr
&lt;/p&gt;</description></item><item><title>RLPlanner&#26159;&#19968;&#31181;&#38024;&#23545;&#33455;&#29255;&#32452;&#22522;&#20110;&#27169;&#22359;&#30340;&#31995;&#32479;&#30340;&#26089;&#26399;&#24067;&#23616;&#35774;&#35745;&#24037;&#20855;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#24555;&#36895;&#28909;&#35780;&#20272;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#24635;&#32447;&#38271;&#24230;&#21644;&#28201;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#36895;&#21270;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2312.16895</link><description>&lt;p&gt;
RLPlanner: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33455;&#29255;&#32452;&#22522;&#20110;&#27169;&#22359;&#30340;&#24555;&#36895;&#28909;&#20998;&#26512;&#30340;&#24067;&#23616;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
RLPlanner: Reinforcement Learning based Floorplanning for Chiplets with Fast Thermal Analysis. (arXiv:2312.16895v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16895
&lt;/p&gt;
&lt;p&gt;
RLPlanner&#26159;&#19968;&#31181;&#38024;&#23545;&#33455;&#29255;&#32452;&#22522;&#20110;&#27169;&#22359;&#30340;&#31995;&#32479;&#30340;&#26089;&#26399;&#24067;&#23616;&#35774;&#35745;&#24037;&#20855;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#24555;&#36895;&#28909;&#35780;&#20272;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#24635;&#32447;&#38271;&#24230;&#21644;&#28201;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#36895;&#21270;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#20302;&#25104;&#26412;&#21644;&#31454;&#20105;&#24615;&#33021;&#65292;&#33455;&#29255;&#32452;&#22522;&#20110;&#27169;&#22359;&#30340;&#31995;&#32479;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#38543;&#30528;&#33455;&#29255;&#32452;&#22522;&#20110;&#27169;&#22359;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#32039;&#20945;&#24615;&#30340;&#22686;&#21152;&#65292;&#22312;&#24067;&#23616;&#35774;&#35745;&#38454;&#27573;&#24517;&#39035;&#20180;&#32454;&#32771;&#34385;&#24494;&#20984;&#28857;&#20998;&#37197;&#12289;&#20114;&#36830;&#24310;&#36831;&#21644;&#28909;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RLPlanner&#65292;&#19968;&#31181;&#29992;&#20110;&#33455;&#29255;&#32452;&#22522;&#20110;&#27169;&#22359;&#31995;&#32479;&#30340;&#39640;&#25928;&#30340;&#26089;&#26399;&#24067;&#23616;&#35774;&#35745;&#24037;&#20855;&#65292;&#20855;&#26377;&#19968;&#31181;&#26032;&#39062;&#30340;&#24555;&#36895;&#28909;&#35780;&#20272;&#26041;&#27861;&#12290;RLPlanner&#21033;&#29992;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#21516;&#26102;&#26368;&#23567;&#21270;&#24635;&#32447;&#38271;&#24230;&#21644;&#28201;&#24230;&#12290;&#20026;&#20102;&#32531;&#35299;&#32791;&#26102;&#30340;&#28909;&#35745;&#31639;&#65292;RLPlanner&#37319;&#29992;&#20102;&#24320;&#21457;&#30340;&#24555;&#36895;&#28909;&#35780;&#20272;&#26041;&#27861;&#26469;&#21152;&#24555;&#36845;&#20195;&#21644;&#20248;&#21270;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#24555;&#36895;&#28909;&#35780;&#20272;&#26041;&#27861;&#30456;&#23545;&#20110;&#24320;&#28304;&#28909;&#27714;&#35299;&#22120;HotSpot&#65292;&#23454;&#29616;&#20102;0.25 K&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#21644;&#36229;&#36807;120&#20493;&#30340;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chiplet-based systems have gained significant attention in recent years due to their low cost and competitive performance. As the complexity and compactness of a chiplet-based system increase, careful consideration must be given to microbump assignments, interconnect delays, and thermal limitations during the floorplanning stage. This paper introduces RLPlanner, an efficient early-stage floorplanning tool for chiplet-based systems with a novel fast thermal evaluation method. RLPlanner employs advanced reinforcement learning to jointly minimize total wirelength and temperature. To alleviate the time-consuming thermal calculations, RLPlanner incorporates the developed fast thermal evaluation method to expedite the iterations and optimizations. Comprehensive experiments demonstrate that our proposed fast thermal evaluation method achieves a mean absolute error (MAE) of 0.25 K and delivers over 120x speed-up compared to the open-source thermal solver HotSpot. When integrated with our fast 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#31163;&#25506;&#32034;&#21644;&#21033;&#29992;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#20048;&#35266;&#19982;&#24754;&#35266;&#28436;&#21592;&#30340;&#21452;&#28436;&#21592;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#21644;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2312.15965</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65306;&#25506;&#32034;&#19982;&#21033;&#29992;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcemen Learning with Decoupling Exploration and Utilization. (arXiv:2312.15965v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#31163;&#25506;&#32034;&#21644;&#21033;&#29992;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#20048;&#35266;&#19982;&#24754;&#35266;&#28436;&#21592;&#30340;&#21452;&#28436;&#21592;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#21644;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36807;&#20110;&#20445;&#23432;&#65292;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#22788;&#29702;&#38480;&#21046;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#23548;&#33268;&#31639;&#27861;&#21482;&#36866;&#24212;&#26576;&#20010;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#27425;&#20248;&#35299;&#12290;&#21516;&#26679;&#65292;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20043;&#21069;&#30340;&#24809;&#32602;&#24615;&#24754;&#35266;&#20027;&#20041;&#20063;&#21093;&#22842;&#20102;&#27169;&#22411;&#30340;&#25506;&#32034;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20048;&#35266;&#19982;&#24754;&#35266;&#28436;&#21592;&#24378;&#21270;&#23398;&#20064;&#65288;OPARL&#65289;&#12290;OPARL&#37319;&#29992;&#29420;&#29305;&#30340;&#21452;&#28436;&#21592;&#26041;&#27861;&#65306;&#20048;&#35266;&#28436;&#21592;&#19987;&#27880;&#20110;&#25506;&#32034;&#65292;&#24754;&#35266;&#28436;&#21592;&#19987;&#27880;&#20110;&#21033;&#29992;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;&#25506;&#32034;&#21644;&#21033;&#29992;&#31574;&#30053;&#12290;&#36825;&#31181;&#32452;&#21512;&#22312;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#20419;&#36827;&#20102;&#26356;&#24179;&#34913;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24754;&#35266;&#30340;&#21033;&#29992;&#31574;&#30053;&#20248;&#21270;&#30528;&#37325;&#20110;&#20135;&#29983;&#39640;&#22870;&#21169;&#21160;&#20316;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network(DNN) generalization is limited by the over-reliance of current offline reinforcement learning techniques on conservative processing of existing datasets. This method frequently results in algorithms that settle for suboptimal solutions that only adjust to a certain dataset. Similarly, in online reinforcement learning, the previously imposed punitive pessimism also deprives the model of its exploratory potential. Our research proposes a novel framework, Optimistic and Pessimistic Actor Reinforcement Learning (OPARL). OPARL employs a unique dual-actor approach: an optimistic actor dedicated to exploration and a pessimistic actor focused on utilization, thereby effectively differentiating between exploration and utilization strategies. This unique combination in reinforcement learning methods fosters a more balanced and efficient approach. It enables the optimization of policies that focus on actions yielding high rewards through pessimistic utilization strategies, whi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#26469;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23398;&#20064;&#20844;&#20849;&#25968;&#25454;&#20013;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#23454;&#29616;&#26368;&#20248;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#65292;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#21487;&#20197;&#28040;&#38500;&#31169;&#26377;&#21327;&#35843;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#32431;&#23616;&#37096;&#23398;&#20064;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.15551</link><description>&lt;p&gt;
&#21033;&#29992;&#20844;&#20849;&#34920;&#31034;&#26469;&#36827;&#34892;&#31169;&#26377;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Public Representations for Private Transfer Learning. (arXiv:2312.15551v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#26469;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23398;&#20064;&#20844;&#20849;&#25968;&#25454;&#20013;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#23454;&#29616;&#26368;&#20248;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#65292;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#21487;&#20197;&#28040;&#38500;&#31169;&#26377;&#21327;&#35843;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#32431;&#23616;&#37096;&#23398;&#20064;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;&#20844;&#20849;&#25968;&#25454;&#32435;&#20837;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#30340;&#26368;&#26032;&#23454;&#35777;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#20174;&#20844;&#20849;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#20849;&#20139;&#34920;&#31034;&#22914;&#20309;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#32447;&#24615;&#22238;&#24402;&#30340;&#20004;&#31181;&#24120;&#35265;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#65292;&#20004;&#32773;&#37117;&#20551;&#35774;&#20844;&#20849;&#20219;&#21153;&#21644;&#31169;&#26377;&#20219;&#21153;&#65288;&#22238;&#24402;&#21521;&#37327;&#65289;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20849;&#20139;&#19968;&#20010;&#20302;&#31209;&#23376;&#31354;&#38388;&#12290;&#22312;&#31532;&#19968;&#31181;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#22312;&#25152;&#26377;&#29992;&#25143;&#20043;&#38388;&#20849;&#20139;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#27599;&#20010;&#29992;&#25143;&#23545;&#24212;&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#34892;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#20272;&#35745;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#30340;&#31639;&#27861;&#31867;&#20013;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#27169;&#22411;&#20010;&#24615;&#21270;&#30340;&#31532;&#20108;&#31181;&#24773;&#26223;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#26377;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#21487;&#20197;&#36991;&#20813;&#31169;&#26377;&#21327;&#35843;&#65292;&#22240;&#20026;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#20869;&#32431;&#31929;&#30340;&#23616;&#37096;&#23398;&#20064;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the recent empirical success of incorporating public data into differentially private learning, we theoretically investigate how a shared representation learned from public data can improve private learning. We explore two common scenarios of transfer learning for linear regression, both of which assume the public and private tasks (regression vectors) share a low-rank subspace in a high-dimensional space. In the first single-task transfer scenario, the goal is to learn a single model shared across all users, each corresponding to a row in a dataset. We provide matching upper and lower bounds showing that our algorithm achieves the optimal excess risk within a natural class of algorithms that search for the linear model within the given subspace estimate. In the second scenario of multitask model personalization, we show that with sufficient public data, users can avoid private coordination, as purely local learning within the given subspace achieves the same utility. Take
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#27604;&#36739;&#19987;&#26377;LLMs&#21644;&#24320;&#28304;SLMs&#30340;&#26435;&#34913;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#20998;&#26512;&#24037;&#20855;SLaM&#26469;&#27979;&#35797;&#20135;&#21697;&#21151;&#33021;&#12290;&#22312;&#23454;&#38469;&#20135;&#21697;&#21151;&#33021;&#26367;&#25442;&#26102;&#65292;&#23545;&#20110;&#29616;&#26377;&#33021;&#21147;&#26159;&#21542;&#33021;&#22815;&#34987;&#24320;&#28304;SLMs&#20195;&#26367;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2312.14972</link><description>&lt;p&gt;
&#22312;&#29983;&#20135;&#20013;&#29992;&#24320;&#28304;SLMs&#26367;&#20195;&#19987;&#26377;LLMs&#30340;&#26435;&#34913;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs in Production. (arXiv:2312.14972v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#27604;&#36739;&#19987;&#26377;LLMs&#21644;&#24320;&#28304;SLMs&#30340;&#26435;&#34913;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#20998;&#26512;&#24037;&#20855;SLaM&#26469;&#27979;&#35797;&#20135;&#21697;&#21151;&#33021;&#12290;&#22312;&#23454;&#38469;&#20135;&#21697;&#21151;&#33021;&#26367;&#25442;&#26102;&#65292;&#23545;&#20110;&#29616;&#26377;&#33021;&#21147;&#26159;&#21542;&#33021;&#22815;&#34987;&#24320;&#28304;SLMs&#20195;&#26367;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20844;&#21496;&#20381;&#36182;&#20110;&#31649;&#29702;&#30340;AI&#27169;&#22411;&#30340;API&#65292;&#22914;OpenAI&#30340;GPT-4&#65292;&#20197;&#22312;&#20854;&#20135;&#21697;&#20013;&#21019;&#24314;AI&#22686;&#24378;&#20307;&#39564;&#12290;&#38500;&#20102;&#20351;&#29992;&#20415;&#21033;&#21644;&#32553;&#30701;&#29983;&#20135;&#26102;&#38388;&#30340;&#22909;&#22788;&#22806;&#65292;&#20381;&#36182;&#19987;&#26377;API&#36824;&#20855;&#26377;&#27169;&#22411;&#25511;&#21046;&#12289;&#24615;&#33021;&#21487;&#38752;&#24615;&#12289;&#19978;&#32447;&#21487;&#39044;&#27979;&#24615;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#32570;&#28857;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#32463;&#28044;&#29616;&#20102;&#35768;&#22810;&#20379;&#21830;&#19994;&#20351;&#29992;&#30340;&#24320;&#28304;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26367;&#20195;&#29616;&#26377;&#33021;&#21147;&#30340;&#20934;&#22791;&#24773;&#20917;&#23578;&#19981;&#28165;&#26970;&#65292;&#24182;&#19988;&#27809;&#26377;&#29616;&#25104;&#30340;&#31995;&#32479;&#26041;&#27861;&#26469;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#29616;&#20195;&#24320;&#28304;SLMs&#21450;&#20854;&#22312;&#26367;&#20195;&#30495;&#23454;&#20135;&#21697;&#21151;&#33021;&#30340;&#19987;&#26377;LLM APIs&#26102;&#25152;&#20570;&#30340;&#26435;&#34913;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;SLaM&#30340;&#33258;&#21160;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#20351;&#24471;&#21487;&#20197;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#27979;&#35797;&#20351;&#29992;&#20219;&#24847;SLMs&#30340;&#20135;&#21697;&#21151;&#33021;&#12290;&#20351;&#29992;SLaM&#65292;&#25105;&#20204;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many companies rely on APIs of managed AI models such as OpenAI's GPT-4 to create AI-enabled experiences in their products. Along with the benefits of ease of use and shortened time to production, this reliance on proprietary APIs has downsides in terms of model control, performance reliability, up-time predictability, and cost. At the same time, there has been a flurry of open source small language models (SLMs) that have been made available for commercial use. However, their readiness to replace existing capabilities remains unclear, and a systematic approach to test these models is not readily available. In this paper, we present a systematic evaluation methodology for, and characterization of, modern open source SLMs and their trade-offs when replacing a proprietary LLM APIs for a real-world product feature. We have designed SLaM, an automated analysis tool that enables the quantitative and qualitative testing of product features utilizing arbitrary SLMs. Using SLaM, we examine bot
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#27491;&#21017;&#21270;&#27969;&#30340;&#21464;&#31181;&#65292;&#21363;&#26102;&#38388;&#21464;&#25442;&#27491;&#21017;&#21270;&#27969;(TCNF)&#65292;&#36890;&#36807;&#26102;&#38388;&#21464;&#24418;&#30340;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#24314;&#27169;&#19968;&#20123;&#26080;&#27861;&#29992;&#20854;&#20182;&#26041;&#27861;&#24314;&#27169;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDEs)&#65292;&#21253;&#25324;&#33879;&#21517;&#30340;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#65292;&#24182;&#27867;&#21270;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32467;&#26524;&#30340;&#20934;&#30830;&#24230;&#21644;&#25512;&#26029;&#21644;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2312.14698</link><description>&lt;p&gt;
&#20934;&#30830;&#24314;&#27169;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#38388;&#21464;&#25442;&#27491;&#21017;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Time-changed normalizing flows for accurate SDE modeling. (arXiv:2312.14698v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#27491;&#21017;&#21270;&#27969;&#30340;&#21464;&#31181;&#65292;&#21363;&#26102;&#38388;&#21464;&#25442;&#27491;&#21017;&#21270;&#27969;(TCNF)&#65292;&#36890;&#36807;&#26102;&#38388;&#21464;&#24418;&#30340;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#24314;&#27169;&#19968;&#20123;&#26080;&#27861;&#29992;&#20854;&#20182;&#26041;&#27861;&#24314;&#27169;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDEs)&#65292;&#21253;&#25324;&#33879;&#21517;&#30340;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#65292;&#24182;&#27867;&#21270;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32467;&#26524;&#30340;&#20934;&#30830;&#24230;&#21644;&#25512;&#26029;&#21644;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#33539;&#24335;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20854;&#20013;&#27969;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#26159;&#27491;&#21017;&#21270;&#27969;&#65292;&#36890;&#36807;&#24494;&#20998;&#21516;&#32986;&#21464;&#25442;&#23558;&#22522;&#26412;&#20998;&#24067;&#36716;&#21464;&#20026;&#20934;&#30830;&#20272;&#35745;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;&#23558;&#27491;&#21017;&#21270;&#27969;&#26694;&#26550;&#25193;&#23637;&#21040;&#22788;&#29702;&#26102;&#38388;&#32034;&#24341;&#27969;&#20135;&#29983;&#21160;&#24577;&#27491;&#21017;&#21270;&#27969;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#27169;&#25311;&#26102;&#38388;&#24207;&#21015;&#12289;&#38543;&#26426;&#36807;&#31243;&#21644;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDEs)&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#27491;&#21017;&#21270;&#27969;&#21464;&#20307;&#65292;&#21363;&#26102;&#38388;&#21464;&#25442;&#27491;&#21017;&#21270;&#27969;(TCNF)&#65292;&#23427;&#22522;&#20110;&#24067;&#26391;&#36816;&#21160;&#30340;&#26102;&#38388;&#21464;&#24418;&#65292;&#26500;&#25104;&#20102;&#19968;&#26063;&#22810;&#25165;&#22810;&#33402;&#30340;&#39640;&#26031;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#27169;&#25311;&#19968;&#20123;&#26080;&#27861;&#20351;&#29992;&#20854;&#20182;&#26041;&#27861;&#24314;&#27169;&#30340;SDEs&#65292;&#21253;&#25324;&#33879;&#21517;&#30340;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#20197;&#21450;&#27867;&#21270;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#21644;&#26356;&#22909;&#30340;&#25512;&#26029;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generative paradigm has become increasingly important in machine learning and deep learning models. Among popular generative models are normalizing flows, which enable exact likelihood estimation by transforming a base distribution through diffeomorphic transformations. Extending the normalizing flow framework to handle time-indexed flows gave dynamic normalizing flows, a powerful tool to model time series, stochastic processes, and neural stochastic differential equations (SDEs). In this work, we propose a novel variant of dynamic normalizing flows, a Time Changed Normalizing Flow (TCNF), based on time deformation of a Brownian motion which constitutes a versatile and extensive family of Gaussian processes. This approach enables us to effectively model some SDEs, that cannot be modeled otherwise, including standard ones such as the well-known Ornstein-Uhlenbeck process, and generalizes prior methodologies, leading to improved results and better inference and prediction capability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35856;&#27874;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#24494;&#20998;&#30340;DSP&#21644;&#39057;&#35889;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#26469;&#36827;&#34892;&#35856;&#27874;&#20449;&#21495;&#30340;&#33258;&#32534;&#30721;&#21644;&#37325;&#26500;&#65292;&#20026;&#31070;&#32463;&#38899;&#39057;&#24212;&#29992;&#20013;&#30340;&#26080;&#30417;&#30563;&#21442;&#25968;&#20272;&#35745;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2312.14507</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#35856;&#27874;&#21442;&#25968;&#20272;&#35745;&#65306;&#22522;&#20110;&#21487;&#24494;&#20998;DSP&#21644;&#39057;&#35889;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Harmonic Parameter Estimation Using Differentiable DSP and Spectral Optimal Transport. (arXiv:2312.14507v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35856;&#27874;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#24494;&#20998;&#30340;DSP&#21644;&#39057;&#35889;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#26469;&#36827;&#34892;&#35856;&#27874;&#20449;&#21495;&#30340;&#33258;&#32534;&#30721;&#21644;&#37325;&#26500;&#65292;&#20026;&#31070;&#32463;&#38899;&#39057;&#24212;&#29992;&#20013;&#30340;&#26080;&#30417;&#30563;&#21442;&#25968;&#20272;&#35745;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#38899;&#39640;&#26465;&#20214;&#23545;&#21512;&#25104;&#22120;&#30340;&#24615;&#33021;&#36827;&#34892;&#22686;&#24378;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#26631;&#20934;&#38899;&#39057;&#21040;&#38899;&#39057;&#37325;&#26500;&#25439;&#22833;&#26102;&#65292;&#32852;&#21512;&#35757;&#32451;&#38899;&#39640;&#20272;&#35745;&#22120;&#21644;&#21512;&#25104;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23548;&#33268;&#20381;&#36182;&#22806;&#37096;&#38899;&#39640;&#36319;&#36394;&#22120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21551;&#21457;&#30340;&#39057;&#35889;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#26368;&#23567;&#21270;&#20102;&#39057;&#35889;&#33021;&#37327;&#30340;&#20301;&#31227;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#33258;&#32534;&#30721;&#20219;&#21153;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#65292;&#35813;&#20219;&#21153;&#23558;&#35856;&#27874;&#20449;&#21495;&#25311;&#21512;&#21040;&#35856;&#27874;&#27169;&#26495;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#32534;&#30721;&#22120;&#20849;&#21516;&#20272;&#35745;&#22522;&#39057;&#21644;&#35856;&#27874;&#30340;&#25391;&#24133;&#65292;&#24182;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#35856;&#27874;&#21512;&#25104;&#22120;&#37325;&#26500;&#20449;&#21495;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#20110;&#25913;&#36827;&#31070;&#32463;&#38899;&#39057;&#24212;&#29992;&#20013;&#30340;&#26080;&#30417;&#30563;&#21442;&#25968;&#20272;&#35745;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In neural audio signal processing, pitch conditioning has been used to enhance the performance of synthesizers. However, jointly training pitch estimators and synthesizers is a challenge when using standard audio-to-audio reconstruction loss, leading to reliance on external pitch trackers. To address this issue, we propose using a spectral loss function inspired by optimal transportation theory that minimizes the displacement of spectral energy. We validate this approach through an unsupervised autoencoding task that fits a harmonic template to harmonic signals. We jointly estimate the fundamental frequency and amplitudes of harmonics using a lightweight encoder and reconstruct the signals using a differentiable harmonic synthesizer. The proposed approach offers a promising direction for improving unsupervised parameter estimation in neural audio applications.
&lt;/p&gt;</description></item><item><title>WellFactor&#26159;&#19968;&#31181;&#20351;&#29992;&#32508;&#21512;&#23884;&#20837;&#21307;&#30103;&#25968;&#25454;&#30340;&#24739;&#32773;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21463;&#32422;&#26463;&#30340;&#20302;&#31209;&#36924;&#36817;&#12289;&#32467;&#21512;&#26631;&#31614;&#20449;&#24687;&#26469;&#20248;&#21270;&#23884;&#20837;&#32467;&#26524;&#65292;&#21516;&#26102;&#20855;&#26377;&#21363;&#26102;&#35745;&#31639;&#26032;&#25968;&#25454;&#23884;&#20837;&#30340;&#29305;&#28857;&#12290;&#22312;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2312.14129</link><description>&lt;p&gt;
WellFactor:&#20351;&#29992;&#32508;&#21512;&#23884;&#20837;&#21307;&#30103;&#25968;&#25454;&#30340;&#24739;&#32773;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WellFactor: Patient Profiling using Integrative Embedding of Healthcare Data. (arXiv:2312.14129v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14129
&lt;/p&gt;
&lt;p&gt;
WellFactor&#26159;&#19968;&#31181;&#20351;&#29992;&#32508;&#21512;&#23884;&#20837;&#21307;&#30103;&#25968;&#25454;&#30340;&#24739;&#32773;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21463;&#32422;&#26463;&#30340;&#20302;&#31209;&#36924;&#36817;&#12289;&#32467;&#21512;&#26631;&#31614;&#20449;&#24687;&#26469;&#20248;&#21270;&#23884;&#20837;&#32467;&#26524;&#65292;&#21516;&#26102;&#20855;&#26377;&#21363;&#26102;&#35745;&#31639;&#26032;&#25968;&#25454;&#23884;&#20837;&#30340;&#29305;&#28857;&#12290;&#22312;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#21307;&#30103;&#34892;&#19994;&#20013;&#65292;&#24179;&#21488;&#29616;&#22312;&#19981;&#20165;&#21487;&#20197;&#35775;&#38382;&#20256;&#32479;&#30340;&#21307;&#30103;&#35760;&#24405;&#65292;&#36824;&#21487;&#20197;&#33719;&#21462;&#28085;&#30422;&#21508;&#31181;&#24739;&#32773;&#20114;&#21160;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#26469;&#33258;&#21307;&#30103;&#32593;&#31449;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#20016;&#23500;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WellFactor&#65306;&#19968;&#31181;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#26469;&#28304;&#20449;&#24687;&#26469;&#24471;&#20986;&#24739;&#32773;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#21033;&#29992;&#21463;&#32422;&#26463;&#30340;&#20302;&#31209;&#36924;&#36817;&#12290;WellFactor&#34987;&#20248;&#21270;&#20026;&#22788;&#29702;&#21307;&#30103;&#25968;&#25454;&#20013;&#32463;&#24120;&#23384;&#22312;&#30340;&#31232;&#30095;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#23884;&#20837;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#26126;&#26234;&#30340;&#24739;&#32773;&#35270;&#35282;&#12290;WellFactor&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#28857;&#26159;&#33021;&#22815;&#21363;&#26102;&#35745;&#31639;&#26032;&#30340;&#12289;&#20197;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#24739;&#32773;&#25968;&#25454;&#30340;&#23884;&#20837;&#65292;&#28040;&#38500;&#20102;&#37325;&#26032;&#35775;&#38382;&#25972;&#20010;&#25968;&#25454;&#38598;&#25110;&#37325;&#26032;&#35745;&#31639;&#23884;&#20837;&#30340;&#38656;&#35201;&#12290;&#23545;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#35777;&#26126;&#20102;WellFactor&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving healthcare industry, platforms now have access to not only traditional medical records, but also diverse data sets encompassing various patient interactions, such as those from healthcare web portals. To address this rich diversity of data, we introduce WellFactor: a method that derives patient profiles by integrating information from these sources. Central to our approach is the utilization of constrained low-rank approximation. WellFactor is optimized to handle the sparsity that is often inherent in healthcare data. Moreover, by incorporating task-specific label information, our method refines the embedding results, offering a more informed perspective on patients. One important feature of WellFactor is its ability to compute embeddings for new, previously unobserved patient data instantaneously, eliminating the need to revisit the entire data set or recomputing the embedding. Comprehensive evaluations on real-world healthcare data demonstrate WellFactor's eff
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#21548;&#38899;&#20048;&#24182;&#29983;&#25104;&#38899;&#20048;&#30340;&#27169;&#22411;&#12290;&#37319;&#29992;&#20102;&#38750;&#33258;&#22238;&#24402;&#30340;Transformer&#27169;&#22411;&#26550;&#26500;&#20197;&#21450;&#19968;&#20123;&#26032;&#39062;&#30340;&#26550;&#26500;&#21644;&#37319;&#26679;&#25913;&#36827;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#26465;&#20214;&#27169;&#22411;&#30340;&#38899;&#39057;&#36136;&#37327;&#65292;&#24182;&#22312;&#38899;&#20048;&#36830;&#36143;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2312.08723</link><description>&lt;p&gt;
StemGen: &#19968;&#20010;&#33021;&#22815;&#21548;&#38899;&#20048;&#24182;&#29983;&#25104;&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
StemGen: A music generation model that listens. (arXiv:2312.08723v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#21548;&#38899;&#20048;&#24182;&#29983;&#25104;&#38899;&#20048;&#30340;&#27169;&#22411;&#12290;&#37319;&#29992;&#20102;&#38750;&#33258;&#22238;&#24402;&#30340;Transformer&#27169;&#22411;&#26550;&#26500;&#20197;&#21450;&#19968;&#20123;&#26032;&#39062;&#30340;&#26550;&#26500;&#21644;&#37319;&#26679;&#25913;&#36827;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#26465;&#20214;&#27169;&#22411;&#30340;&#38899;&#39057;&#36136;&#37327;&#65292;&#24182;&#22312;&#38899;&#20048;&#36830;&#36143;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#38899;&#20048;&#38899;&#39057;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#27963;&#21160;&#38750;&#24120;&#27963;&#36291;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#38598;&#20013;&#22312;&#26681;&#25454;&#25277;&#35937;&#30340;&#26465;&#20214;&#20449;&#24687;&#29983;&#25104;&#23436;&#20840;&#28151;&#21512;&#30340;&#38899;&#20048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#33539;&#24335;&#65292;&#29992;&#20110;&#20135;&#29983;&#33021;&#22815;&#21548;&#38899;&#20048;&#24182;&#22238;&#24212;&#38899;&#20048;&#29615;&#22659;&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#38750;&#33258;&#22238;&#24402;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26550;&#26500;&#26500;&#24314;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#39062;&#30340;&#26550;&#26500;&#21644;&#37319;&#26679;&#25913;&#36827;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#19987;&#26377;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#25152;&#25551;&#36848;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;&#36136;&#37327;&#24230;&#37327;&#21644;&#22522;&#20110;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#25551;&#36848;&#31526;&#30340;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#26465;&#20214;&#27169;&#22411;&#30340;&#38899;&#39057;&#36136;&#37327;&#65292;&#24182;&#19988;&#22312;&#38899;&#20048;&#36830;&#36143;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end generation of musical audio using deep learning techniques has seen an explosion of activity recently. However, most models concentrate on generating fully mixed music in response to abstract conditioning information. In this work, we present an alternative paradigm for producing music generation models that can listen and respond to musical context. We describe how such a model can be constructed using a non-autoregressive, transformer-based model architecture and present a number of novel architectural and sampling improvements. We train the described architecture on both an open-source and a proprietary dataset. We evaluate the produced models using standard quality metrics and a new approach based on music information retrieval descriptors. The resulting model reaches the audio quality of state-of-the-art text-conditioned models, as well as exhibiting strong musical coherence with its context.
&lt;/p&gt;</description></item><item><title>&#24418;&#24577;&#23398;&#29305;&#24449;&#20998;&#26512;&#22312;&#34920;&#22411;&#33647;&#29289;&#21457;&#29616;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20998;&#26512;&#22823;&#35268;&#27169;&#39640;&#20869;&#23481;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20419;&#36827;&#20102;&#33647;&#29289;&#20316;&#29992;&#26426;&#21046;&#30340;&#29702;&#35299;&#21644;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2312.07899</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#30340;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Morphological Profiling for Drug Discovery in the Era of Deep Learning. (arXiv:2312.07899v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07899
&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#23398;&#29305;&#24449;&#20998;&#26512;&#22312;&#34920;&#22411;&#33647;&#29289;&#21457;&#29616;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20998;&#26512;&#22823;&#35268;&#27169;&#39640;&#20869;&#23481;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20419;&#36827;&#20102;&#33647;&#29289;&#20316;&#29992;&#26426;&#21046;&#30340;&#29702;&#35299;&#21644;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#23398;&#29305;&#24449;&#20998;&#26512;&#26159;&#34920;&#22411;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#39640;&#36890;&#37327;&#33258;&#21160;&#25104;&#20687;&#30340;&#20986;&#29616;&#20351;&#24471;&#33021;&#22815;&#20197;&#21333;&#32454;&#32990;&#20998;&#36776;&#29575;&#25429;&#25417;&#32454;&#32990;&#25110;&#29983;&#29289;&#20307;&#23545;&#24178;&#25200;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#30340;&#24191;&#27867;&#33539;&#22260;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#20351;&#24471;&#22312;&#39640;&#36890;&#37327;&#19979;&#20998;&#26512;&#22823;&#35268;&#27169;&#39640;&#20869;&#23481;&#22270;&#20687;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#36825;&#20123;&#21162;&#21147;&#20419;&#36827;&#20102;&#23545;&#21270;&#21512;&#29289;&#20316;&#29992;&#26426;&#21046;&#12289;&#33647;&#29289;&#20877;&#21033;&#29992;&#12289;&#24178;&#25200;&#19979;&#32454;&#32990;&#24418;&#24577;&#21160;&#21147;&#23398;&#29305;&#24449;&#30340;&#20102;&#35299;&#65292;&#24182;&#26368;&#32456;&#26377;&#21161;&#20110;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;&#24418;&#24577;&#23398;&#29305;&#24449;&#20998;&#26512;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#22270;&#20687;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#65292;&#35843;&#26597;&#20102;&#21253;&#25324;&#29305;&#24449;&#24037;&#31243;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#26512;&#31574;&#30053;&#22312;&#20869;&#30340;&#24191;&#27867;&#33539;&#22260;&#30340;&#20998;&#26512;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Morphological profiling is a valuable tool in phenotypic drug discovery. The advent of high-throughput automated imaging has enabled the capturing of a wide range of morphological features of cells or organisms in response to perturbations at the single-cell resolution. Concurrently, significant advances in machine learning and deep learning, especially in computer vision, have led to substantial improvements in analyzing large-scale high-content images at high-throughput. These efforts have facilitated understanding of compound mechanism-of-action (MOA), drug repurposing, characterization of cell morphodynamics under perturbation, and ultimately contributing to the development of novel therapeutics. In this review, we provide a comprehensive overview of the recent advances in the field of morphological profiling. We summarize the image profiling analysis workflow, survey a broad spectrum of analysis strategies encompassing feature engineering- and deep learning-based approaches, and i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#23610;&#24230;&#30340;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#26657;&#27491;&#65292;&#20197;&#22686;&#24378;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20943;&#23569;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.07586</link><description>&lt;p&gt;
&#29305;&#24449;&#24341;&#23548;&#65306;&#22823;&#23610;&#24230;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#23610;&#24230;&#30340;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#26657;&#27491;&#65292;&#20197;&#22686;&#24378;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20943;&#23569;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#30340;&#23548;&#24341;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DDPM)&#32447;&#24615;&#22320;&#23558;&#19981;&#21516;&#30340;&#26465;&#20214;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#25552;&#20379;&#23545;&#26679;&#26412;&#30340;&#22686;&#24378;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#24403;&#23548;&#21521;&#23610;&#24230;&#21464;&#22823;&#26102;&#20135;&#29983;&#30340;&#38750;&#32447;&#24615;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#65292;&#19968;&#31181;&#37319;&#26679;&#26041;&#27861;&#65292;&#20026;&#26080;&#20998;&#31867;&#22120;&#23548;&#21521;&#30340;DDPM&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#29702;&#30340;&#38750;&#32447;&#24615;&#26657;&#27491;&#12290;&#36825;&#31181;&#26657;&#27491;&#36843;&#20351;&#23548;&#21521;&#30340;DDPM&#36981;&#23432;&#20854;&#24213;&#23618;&#25193;&#25955;&#36807;&#31243;&#30340;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#65292;&#26080;&#38656;&#23548;&#25968;&#65292;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#20860;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#29305;&#24449;&#24341;&#23548;&#22686;&#24378;&#20102;&#23545;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#24182;&#20943;&#23569;&#20102;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#23545;&#20174;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21040;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#22914;&#30913;&#30456;&#21464;&#30340;&#21508;&#31181;&#24212;&#29992;&#37117;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popular guidance for denoising diffusion probabilistic model (DDPM) linearly combines distinct conditional models together to provide enhanced control over samples. However, this approach overlooks nonlinear effects that become significant when guidance scale is large. To address this issue, we propose characteristic guidance, a sampling method that provides first-principle non-linear correction for classifier-free guided DDPMs. Such correction forces the guided DDPMs to respect the Fokker-Planck equation of their underlying diffusion process, in a way that is training-free, derivative-free, and compatible with existing sampling methods. Experiments show that characteristic guidance enhances control and reduces color and exposure issues in image generation, proving effective in diverse applications ranging from latent space sampling to solving physics problems like magnet phase transitions.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#24341;&#20837;&#21464;&#20998;&#25512;&#29702;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#28508;&#22312;&#29366;&#24577;&#21644;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20943;&#23569;&#20102;&#21464;&#20998;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2312.05910</link><description>&lt;p&gt;
&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#19982;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#38750;&#22343;&#22330;&#21644;&#22312;&#32447;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Ensemble Kalman Filtering Meets Gaussian Process SSM for Non-Mean-Field and Online Inference. (arXiv:2312.05910v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05910
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#24341;&#20837;&#21464;&#20998;&#25512;&#29702;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#28508;&#22312;&#29366;&#24577;&#21644;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20943;&#23569;&#20102;&#21464;&#20998;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;GPSSMs&#65289;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21644;&#21407;&#21017;&#24615;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GPSSMs&#21464;&#20998;&#23398;&#20064;&#21644;&#25512;&#29702;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20248;&#21270;&#22823;&#37327;&#21464;&#20998;&#21442;&#25968;&#65292;&#23548;&#33268;&#24615;&#33021;&#21644;&#25928;&#29575;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#65288;EnKF&#65289;&#65292;&#19968;&#31181;&#25104;&#29087;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#28388;&#27874;&#25216;&#26415;&#65292;&#32435;&#20837;&#21464;&#20998;&#25512;&#29702;&#26694;&#26550;&#20013;&#65292;&#20197;&#36817;&#20284;&#28508;&#22312;&#29366;&#24577;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#36825;&#31181;&#21033;&#29992;EnKF&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#28508;&#22312;&#29366;&#24577;&#21644;GP&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#21464;&#20998;&#20998;&#24067;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#21464;&#20998;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#23545;&#22810;&#20010;&#39033;&#36827;&#34892;&#27714;&#21644;&#26469;&#30452;&#25509;&#35780;&#20272;&#21464;&#20998;&#25512;&#29702;&#20013;&#30340;&#36817;&#20284;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian process state-space models (GPSSMs) are a versatile and principled family of nonlinear dynamical system models. However, existing variational learning and inference methods for GPSSMs often necessitate optimizing a substantial number of variational parameters, leading to inadequate performance and efficiency. To overcome this issue, we propose incorporating the ensemble Kalman filter (EnKF), a well-established model-based filtering technique, into the variational inference framework to approximate the posterior distribution of latent states. This utilization of EnKF can effectively exploit the dependencies between latent states and GP dynamics, while eliminating the need for parameterizing the variational distribution, thereby significantly reducing the number of variational parameters. Moreover, we show that our proposed algorithm allows straightforward evaluation of an approximated evidence lower bound (ELBO) in variational inference via simply summating multiple terms with 
&lt;/p&gt;</description></item><item><title>FreqFed&#26159;&#19968;&#31181;&#22522;&#20110;&#39057;&#35889;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38024;&#23545;&#24615;&#21644;&#38750;&#38024;&#23545;&#24615;&#30340;&#27602;&#21270;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2312.04432</link><description>&lt;p&gt;
FreqFed:&#19968;&#31181;&#22522;&#20110;&#39057;&#35889;&#20998;&#26512;&#30340;&#26041;&#27861;&#29992;&#20110;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27602;&#21270;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning. (arXiv:2312.04432v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04432
&lt;/p&gt;
&lt;p&gt;
FreqFed&#26159;&#19968;&#31181;&#22522;&#20110;&#39057;&#35889;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38024;&#23545;&#24615;&#21644;&#38750;&#38024;&#23545;&#24615;&#30340;&#27602;&#21270;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26159;&#19968;&#31181;&#21327;&#20316;&#23398;&#20064;&#33539; Paradigm&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;FL &#23481;&#26131;&#21463;&#21040;&#27602;&#21270;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#23558;&#31713;&#25913;&#30340;&#27169;&#22411;&#26356;&#26032;&#25554;&#20837;&#32852;&#37030;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#26469;&#30772;&#22351;&#25110;&#30772;&#22351;&#39044;&#27979;&#65288;&#26080;&#30446;&#26631;&#27602;&#21270;&#65289;&#65292;&#25110;&#32773; implant (targeted poisoning or backdoors) &#38544;&#21547;&#30340;&#21151;&#33021;&#12290;&#38024;&#23545; FL &#20013;&#30340;&#27602;&#21270;&#25915;&#20987;&#65292;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#23384;&#22312;&#22810;&#31181;&#38480;&#21046;&#65292;&#27604;&#22914;&#20381;&#36182;&#20110;&#23545;&#25915;&#20987;&#31867;&#22411;&#12289;&#31574;&#30053;&#25110;&#25968;&#25454;&#20998;&#24067;&#30340;&#29305;&#23450;&#20551;&#35774;&#65292;&#25110;&#32773;&#22312;&#39640;&#32423;&#27880;&#20837;&#25216;&#26415;&#21644;&#31574;&#30053;&#26041;&#38754;&#19981;&#22815;&#24378;&#22823;, &#19988;&#21516;&#26102;&#20445;&#25345;&#32858;&#21512;&#27169;&#22411;&#30340;&#25928;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#38450;&#24481;&#25514;&#26045;&#30340;&#19981;&#36275;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#38024;&#23545;&#24615;&#21644;&#38750;&#38024;&#23545;&#24615;&#30340;&#27602;&#21270;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; FreqFed&#65292;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#26426;&#21046;&#65292;&#29992;&#20110;&#23558;&#27169;&#22411;&#26356;&#26032;&#65288;&#21363;&#26435;&#37325;&#65289;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a collaborative learning paradigm allowing multiple clients to jointly train a model without sharing their training data. However, FL is susceptible to poisoning attacks, in which the adversary injects manipulated model updates into the federated model aggregation process to corrupt or destroy predictions (untargeted poisoning) or implant hidden functionalities (targeted poisoning or backdoors). Existing defenses against poisoning attacks in FL have several limitations, such as relying on specific assumptions about attack types and strategies or data distributions or not sufficiently robust against advanced injection techniques and strategies and simultaneously maintaining the utility of the aggregated model. To address the deficiencies of existing defenses, we take a generic and completely different approach to detect poisoning (targeted and untargeted) attacks. We present FreqFed, a novel aggregation mechanism that transforms the model updates (i.e., weight
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;CLadder&#65292;&#24182;&#21033;&#29992;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#23558;&#31526;&#21495;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22810;&#20010;LLMs&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2312.04350</link><description>&lt;p&gt;
CLadder: &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models. (arXiv:2312.04350v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;CLadder&#65292;&#24182;&#21033;&#29992;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#23558;&#31526;&#21495;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22810;&#20010;LLMs&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#30340;&#33021;&#21147;&#34987;&#24191;&#27867;&#35270;&#20026;&#26234;&#33021;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#21542;&#36830;&#36143;&#22320;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;LLMs&#20013;&#30340;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;&#65292;&#26410;&#33021;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25353;&#29031;&#19968;&#32452;&#26126;&#30830;&#23450;&#20041;&#30340;&#24418;&#24335;&#35268;&#21017;&#25191;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#21463;&#21040;Judea Pearl&#31561;&#20154;&#25552;&#20986;&#30340;&#8220;&#22240;&#26524;&#25512;&#26029;&#24341;&#25806;&#8221;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;10K&#20010;&#26679;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLadder&#65292;&#36890;&#36807;&#19968;&#31181;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#65292;&#22522;&#20110;&#19968;&#32452;&#22240;&#26524;&#22270;&#21644;&#26597;&#35810;(&#32852;&#21512;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;)&#65292;&#24471;&#21040;&#31526;&#21495;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#24182;&#23558;&#20854;&#32763;&#35793;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#20010;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24341;&#20837;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the "causal inference engine" postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#26469;&#25913;&#36827;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2312.04234</link><description>&lt;p&gt;
&#22270;&#21367;&#31215;&#22312;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#36215;&#21040;&#20102;&#25913;&#36827;&#30340;&#20316;&#29992;&#65281;&#65288;arXiv&#65306;2312.04234v2 [cs.LG]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutions Enrich the Self-Attention in Transformers!. (arXiv:2312.04234v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#26469;&#25913;&#36827;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22240;&#20854;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#32780;&#38395;&#21517;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;Transformer&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#21363;&#34920;&#31034;&#22312;&#21508;&#20010;&#23618;&#20043;&#38388;&#36235;&#20110;&#26080;&#27861;&#21306;&#20998;&#30340;&#20540;&#65292;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#25105;&#20204;&#23558;&#21407;&#22987;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#37322;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#22270;&#28388;&#27874;&#22120;&#65292;&#24182;&#20174;&#22270;&#20449;&#21495;&#22788;&#29702;&#65288;GSP&#65289;&#30340;&#35282;&#24230;&#37325;&#26032;&#35774;&#35745;&#23427;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#28388;&#27874;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GFSA&#65289;&#65292;&#20197;&#23398;&#20064;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#26377;&#25928;&#30340;&#26426;&#21046;&#65292;&#20854;&#22797;&#26434;&#24230;&#30053;&#39640;&#20110;&#21407;&#22987;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GFSA&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#27169;&#24335;&#20998;&#31867;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#20195;&#30721;&#20998;&#31867;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#25913;&#36827;&#20102;Transformer&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers, renowned for their self-attention mechanism, have achieved state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc. However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation. We interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing (GSP) perspective. We propose graph-filter-based self-attention (GFSA) to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism. We demonstrate that GFSA improves the performance of Transformers in various fields, including computer vision, natural language processing, graph pattern classification, speech recognition, and code classification.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#32479;&#35745;&#23398;&#30340;&#38181;&#24418;&#20998;&#24067;&#20989;&#25968;&#36716;&#21270;&#20026;&#22810;&#20934;&#21017;&#20915;&#31574;&#21046;&#23450;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#25490;&#21517;&#20989;&#25968;&#21040;&#38598;&#21512;&#65292;&#24314;&#31435;&#20102;&#38598;&#21512;&#20248;&#21270;&#26041;&#27861;&#19982;&#22522;&#20110;&#38598;&#21512;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.03006</link><description>&lt;p&gt;
&#22810;&#20934;&#21017;&#20915;&#31574;&#21046;&#23450;&#30340;&#22810;&#26435;&#37325;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Multi-Weight Ranking for Multi-Criteria Decision Making. (arXiv:2312.03006v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03006
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#32479;&#35745;&#23398;&#30340;&#38181;&#24418;&#20998;&#24067;&#20989;&#25968;&#36716;&#21270;&#20026;&#22810;&#20934;&#21017;&#20915;&#31574;&#21046;&#23450;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#25490;&#21517;&#20989;&#25968;&#21040;&#38598;&#21512;&#65292;&#24314;&#31435;&#20102;&#38598;&#21512;&#20248;&#21270;&#26041;&#27861;&#19982;&#22522;&#20110;&#38598;&#21512;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20013;&#30340;&#38181;&#24418;&#20998;&#24067;&#20989;&#25968;&#34987;&#36716;&#21270;&#20026;&#22810;&#20934;&#21017;&#20915;&#31574;&#21046;&#23450;&#24037;&#20855;&#12290;&#36890;&#36807;&#20363;&#23376;&#23637;&#31034;&#65292;&#19982;&#32431;&#21152;&#26435;&#24635;&#21644;&#26631;&#37327;&#21270;&#30456;&#27604;&#65292;&#36825;&#31181;&#26631;&#37327;&#21270;&#26041;&#27861;&#33021;&#22815;&#25214;&#20986; Pareto &#36793;&#30028;&#30340;&#8220;&#38750;&#20984;&#8221;&#37096;&#20998;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#25490;&#21517;&#20989;&#25968;&#25193;&#23637;&#21040;&#38598;&#21512;&#65292;&#20026;&#38598;&#21512;&#20559;&#22909;&#25552;&#20379;&#19968;&#20803;&#25351;&#26631;&#65292;&#39318;&#27425;&#24314;&#31435;&#20102;&#38598;&#21512;&#20248;&#21270;&#26041;&#27861;&#19982;&#22522;&#20110;&#38598;&#21512;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25991;&#20013;&#36824;&#31616;&#36848;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cone distribution functions from statistics are turned into Multi-Criteria Decision Making tools. It is demonstrated that this procedure can be considered as an upgrade of the weighted sum scalarization insofar as it absorbs a whole collection of weighted sum scalarizations at once instead of fixing a particular one in advance. As examples show, this type of scalarization--in contrast to a pure weighted sum scalarization-is also able to detect ``non-convex" parts of the Pareto frontier. Situations are characterized in which different types of rank reversal occur, and it is explained why this might even be useful for analyzing the ranking procedure. The ranking functions are then extended to sets providing unary indicators for set preferences which establishes, for the first time, the link between set optimization methods and set-based multi-objective optimization. A potential application in machine learning is outlined.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20219;&#21153;&#23545;&#31216;&#24615;&#30340;&#21487;&#35777;&#26126;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#27169;&#22411;&#21644;&#35748;&#35777;&#26041;&#27861;&#26469;&#23454;&#29616;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24320;&#21457;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#38543;&#26426;&#24179;&#28369;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#23545;&#20110;&#20855;&#26377;&#36830;&#32493;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#30340;&#35748;&#35777;&#26041;&#27861;&#19981;&#21487;&#29992;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.02708</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#30340;&#23545;&#31216;&#20219;&#21153;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65306;&#22270;&#24418;&#12289;&#28857;&#20113;&#12289;&#20998;&#23376;&#31561;
&lt;/p&gt;
&lt;p&gt;
Provable Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More. (arXiv:2312.02708v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20219;&#21153;&#23545;&#31216;&#24615;&#30340;&#21487;&#35777;&#26126;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#27169;&#22411;&#21644;&#35748;&#35777;&#26041;&#27861;&#26469;&#23454;&#29616;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24320;&#21457;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#38543;&#26426;&#24179;&#28369;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#23545;&#20110;&#20855;&#26377;&#36830;&#32493;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#30340;&#35748;&#35777;&#26041;&#27861;&#19981;&#21487;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#35748;&#20026;&#22312;&#36755;&#20837;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#65288;&#20960;&#20046;&#65289;&#24658;&#23450;&#30340;&#39044;&#27979;&#26159;&#20581;&#22766;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20219;&#21153;&#65292;&#22914;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#25110;&#28857;&#20113;&#20998;&#21106;&#65292;&#20855;&#26377;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#65292;&#22914;&#26059;&#36716;&#25110;&#32622;&#25442;&#23545;&#31216;&#24615;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#22823;&#33539;&#25968;&#30340;&#25200;&#21160;&#20063;&#19981;&#19968;&#23450;&#20250;&#25913;&#21464;&#36755;&#20837;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#26377;&#20123;&#25200;&#21160;&#38656;&#35201;&#26126;&#30830;&#25913;&#21464;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20219;&#21153;&#23545;&#31216;&#24615;&#30340;&#21487;&#38752;&#23545;&#25239;&#40065;&#26834;&#24615;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#19982;&#20219;&#21153;&#23545;&#31216;&#24615;&#30456;&#21305;&#37197;&#30340;&#27169;&#22411;&#21644;&#35748;&#35777;&#20256;&#32479;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26469;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#20855;&#26377;&#36830;&#32493;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#65292;&#35748;&#35777;&#26041;&#27861;&#19981;&#21487;&#29992;&#12290;&#36890;&#36807;&#24320;&#21457;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#38543;&#26426;&#24179;&#28369;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20010;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
A machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. However, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. In such tasks, even perturbations with large norm do not necessarily change an input's semantic content. Furthermore, there are perturbations for which a model's prediction explicitly needs to change. For the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. We then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task's equivariances (2) certifying traditional adversarial robustness. Certification methods are, however, unavailable for many models, such as those with continuous equivariances. We close this gap by developing the framework of equivariance-preserving randomized smoothing, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#24211;&#27169;&#25311;&#19981;&#21305;&#37197;&#30340;&#26465;&#20214;&#65292;&#31995;&#32479;&#35780;&#20272;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23581;&#35797;&#20102;&#26032;&#30340;&#35774;&#35745;&#26041;&#38754;&#65292;&#22914;&#22122;&#22768;&#35843;&#24230;&#21644;&#37319;&#26679;&#22120;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.02683</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Heun&#37319;&#26679;&#22120;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#21305;&#37197;&#21644;&#19981;&#21305;&#37197;&#26465;&#20214;&#19979;&#30340;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Based Speech Enhancement in Matched and Mismatched Conditions Using a Heun-Based Sampler. (arXiv:2312.02683v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02683
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#24211;&#27169;&#25311;&#19981;&#21305;&#37197;&#30340;&#26465;&#20214;&#65292;&#31995;&#32479;&#35780;&#20272;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23581;&#35797;&#20102;&#26032;&#30340;&#35774;&#35745;&#26041;&#38754;&#65292;&#22914;&#22122;&#22768;&#35843;&#24230;&#21644;&#37319;&#26679;&#22120;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#36817;&#25104;&#21151;&#24212;&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#19981;&#21305;&#37197;&#26465;&#20214;&#19979;&#19982;&#26368;&#20808;&#36827;&#30340;&#21028;&#21035;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#21482;&#20351;&#29992;&#20102;&#19968;&#20010;&#25968;&#25454;&#24211;&#36827;&#34892;&#35757;&#32451;&#65292;&#21478;&#19968;&#20010;&#25968;&#25454;&#24211;&#36827;&#34892;&#27979;&#35797;&#65292;&#36825;&#20351;&#24471;&#32467;&#26524;&#39640;&#24230;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#25968;&#25454;&#24211;&#12290;&#27492;&#22806;&#65292;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20123;&#36827;&#23637;&#21253;&#25324;&#25193;&#25955;&#27169;&#22411;&#30340;&#20960;&#20010;&#35774;&#35745;&#26041;&#38754;&#65292;&#22914;&#22122;&#22768;&#35843;&#24230;&#25110;&#21453;&#21521;&#37319;&#26679;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#35821;&#38899;&#12289;&#22122;&#22768;&#21644;&#21452;&#32819;&#23460;&#20869;&#33033;&#20914;&#21709;&#24212;&#65288;BRIR&#65289;&#25968;&#25454;&#24211;&#26469;&#27169;&#25311;&#19981;&#21305;&#37197;&#30340;&#22768;&#23398;&#26465;&#20214;&#65292;&#31995;&#32479;&#35780;&#20272;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#19968;&#31181;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#23578;&#26410;&#24212;&#29992;&#30340;&#22122;&#22768;&#35843;&#24230;&#21644;&#37319;&#26679;&#22120;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a new class of generative models that have recently been applied to speech enhancement successfully. Previous works have demonstrated their superior performance in mismatched conditions compared to state-of-the art discriminative models. However, this was investigated with a single database for training and another one for testing, which makes the results highly dependent on the particular databases. Moreover, recent developments from the image generation literature remain largely unexplored for speech enhancement. These include several design aspects of diffusion models, such as the noise schedule or the reverse sampler. In this work, we systematically assess the generalization performance of a diffusion-based speech enhancement model by using multiple speech, noise and binaural room impulse response (BRIR) databases to simulate mismatched acoustic conditions. We also experiment with a noise schedule and a sampler that have not been applied to speech enhancement b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#26816;&#27979;&#21644;&#32531;&#35299;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24573;&#30053;&#34987;&#24576;&#30097;&#20026;&#25915;&#20987;&#32773;&#30340;&#20195;&#29702;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.02102</link><description>&lt;p&gt;
&#32531;&#35299;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mitigating Data Injection Attacks on Federated Learning. (arXiv:2312.02102v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#26816;&#27979;&#21644;&#32531;&#35299;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24573;&#30053;&#34987;&#24576;&#30097;&#20026;&#25915;&#20987;&#32773;&#30340;&#20195;&#29702;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20801;&#35768;&#22810;&#20010;&#23454;&#20307;&#22312;&#19981;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#20854;&#20248;&#21183;&#65292;&#32852;&#37030;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#23545;&#32593;&#32476;&#20013;&#29305;&#23450;&#20195;&#29702;&#30340;&#25511;&#21046;&#26435;&#30340;&#24694;&#24847;&#23454;&#20307;&#21487;&#20197;&#25805;&#32437;&#23398;&#20064;&#36807;&#31243;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#35299;&#20915;&#36825;&#20123;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#26816;&#27979;&#21644;&#32531;&#35299;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#32531;&#35299;&#26041;&#27861;&#26159;&#19968;&#31181;&#23616;&#37096;&#26041;&#26696;&#65292;&#22312;&#21327;&#35843;&#33410;&#28857;&#30340;&#21333;&#20010;&#35757;&#32451;&#23454;&#20363;&#20013;&#25191;&#34892;&#65292;&#20801;&#35768;&#22312;&#31639;&#27861;&#25910;&#25947;&#26399;&#38388;&#36827;&#34892;&#32531;&#35299;&#12290;&#27599;&#24403;&#24576;&#30097;&#26576;&#20010;&#20195;&#29702;&#26159;&#25915;&#20987;&#32773;&#26102;&#65292;&#20250;&#22312;&#19968;&#23450;&#26102;&#38388;&#20869;&#24573;&#30053;&#20854;&#25968;&#25454;&#65292;&#27492;&#20915;&#31574;&#32463;&#24120;&#34987;&#37325;&#26032;&#35780;&#20272;&#12290;&#25105;&#20204;&#35777;&#26126;&#27010;&#29575;&#19978;&#33021;&#22815;&#23454;&#29616;&#32531;&#35299;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a technique that allows multiple entities to collaboratively train models using their data without compromising data privacy. However, despite its advantages, federated learning can be susceptible to false data injection attacks. In these scenarios, a malicious entity with control over specific agents in the network can manipulate the learning process, leading to a suboptimal model. Consequently, addressing these data injection attacks presents a significant research challenge in federated learning systems. In this paper, we propose a novel technique to detect and mitigate data injection attacks on federated learning systems. Our mitigation method is a local scheme, performed during a single instance of training by the coordinating node, allowing the mitigation during the convergence of the algorithm. Whenever an agent is suspected to be an attacker, its data will be ignored for a certain period, this decision will often be re-evaluated. We prove that with probabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;Shannon&#29109;&#21644;Kullback-Leibler&#25955;&#24230;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#20540;&#31034;&#20363;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#39640;&#26031;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;KL&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#38477;&#20302;&#21040;&#20108;&#27425;&#12290;</title><link>http://arxiv.org/abs/2312.01520</link><description>&lt;p&gt;
Bayesian&#32593;&#32476;&#30340;&#29109;&#21644;Kullback-Leibler&#25955;&#24230;&#65306;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#39640;&#25928;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Entropy and the Kullback-Leibler Divergence for Bayesian Networks: Computational Complexity and Efficient Implementation. (arXiv:2312.01520v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;Shannon&#29109;&#21644;Kullback-Leibler&#25955;&#24230;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#20540;&#31034;&#20363;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#39640;&#26031;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;KL&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#38477;&#20302;&#21040;&#20108;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BNs&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#20204;&#30340;&#22270;&#32467;&#26500;&#21487;&#20197;&#22788;&#29702;&#39640;&#32500;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#31232;&#30095;&#30340;&#19968;&#31995;&#21015;&#36739;&#23567;&#38382;&#39064;&#65292;&#36825;&#26159;Judea Pearl&#30340;&#22240;&#26524;&#24615;&#30340;&#22522;&#30784;&#65292;&#20063;&#20915;&#23450;&#20102;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#20960;&#20046;&#27809;&#26377;&#20851;&#20110;&#22914;&#20309;&#22312;&#26368;&#24120;&#35265;&#30340;&#20998;&#24067;&#20551;&#35774;&#19979;&#35745;&#31639;BNs&#30340;Shannon&#29109;&#21644;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#30340;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;BNs&#30340;&#22270;&#32467;&#26500;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#24182;&#29992;&#19968;&#25972;&#22871;&#25968;&#20540;&#31034;&#20363;&#35828;&#26126;&#20102;&#23427;&#20204;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#23558;&#39640;&#26031;BNs&#30340;KL&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#38477;&#20302;&#21040;&#20108;&#27425;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian networks (BNs) are a foundational model in machine learning and causal inference. Their graphical structure can handle high-dimensional problems, divide them into a sparse collection of smaller ones, underlies Judea Pearl's causality, and determines their explainability and interpretability. Despite their popularity, there are almost no resources in the literature on how to compute Shannon's entropy and the Kullback-Leibler (KL) divergence for BNs under their most common distributional assumptions. In this paper, we provide computationally efficient algorithms for both by leveraging BNs' graphical structure, and we illustrate them with a complete set of numerical examples. In the process, we show it is possible to reduce the computational complexity of KL from cubic to quadratic for Gaussian BNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#31561;&#21464;&#37327;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;EQNN&#65289;&#21644;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#19982;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#12298;$\mathbb{Z}_2\times \mathbb{Z}_2$&#12299;EQNN&#21644;QNN&#22312;&#36739;&#23567;&#30340;&#21442;&#25968;&#38598;&#21644;&#36866;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2311.18744</link><description>&lt;p&gt;
&#12298;$\mathbb{Z}_2\times \mathbb{Z}_2$&#12299;&#31561;&#21464;&#37327;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65306;&#19982;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20934;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
$\mathbb{Z}_2\times \mathbb{Z}_2$ Equivariant Quantum Neural Networks: Benchmarking against Classical Neural Networks. (arXiv:2311.18744v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#31561;&#21464;&#37327;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;EQNN&#65289;&#21644;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#19982;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#12298;$\mathbb{Z}_2\times \mathbb{Z}_2$&#12299;EQNN&#21644;QNN&#22312;&#36739;&#23567;&#30340;&#21442;&#25968;&#38598;&#21644;&#36866;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#31561;&#21464;&#37327;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;EQNN&#65289;&#21644;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#19982;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#24212;&#29289;&#65306;&#31561;&#21464;&#37327;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#29609;&#20855;&#31034;&#20363;&#35780;&#20272;&#27599;&#20010;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#20851;&#27880;&#27169;&#22411;&#22797;&#26434;&#24230;&#65288;&#30001;&#21442;&#25968;&#25968;&#37327;&#27979;&#37327;&#65289;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#12298;$\mathbb{Z}_2\times \mathbb{Z}_2$&#12299;EQNN&#21644;QNN&#22312;&#36739;&#23567;&#30340;&#21442;&#25968;&#38598;&#21644;&#36866;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#19978;&#25552;&#20379;&#20102;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive comparative analysis of the performance of Equivariant Quantum Neural Networks (EQNN) and Quantum Neural Networks (QNN), juxtaposed against their classical counterparts: Equivariant Neural Networks (ENN) and Deep Neural Networks (DNN). We evaluate the performance of each network with two toy examples for a binary classification task, focusing on model complexity (measured by the number of parameters) and the size of the training data set. Our results show that the $\mathbb{Z}_2\times \mathbb{Z}_2$ EQNN and the QNN provide superior performance for smaller parameter sets and modest training data samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26230;&#26684;&#35268;&#33539;&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;L-CNN&#65289;&#25551;&#36848;&#20102;&#22522;&#20110;&#37325;&#25972;&#21270;&#32676;&#21464;&#25442;&#30340;&#22266;&#23450;&#28857;&#20316;&#29992;&#65288;FP&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#21442;&#25968;&#21270;FP&#20316;&#29992;&#65292;&#21487;&#20197;&#35268;&#36991;&#20020;&#30028;&#20943;&#36895;&#21644;&#25299;&#25169;&#20923;&#32467;&#38382;&#39064;&#65292;&#24182;&#22312;&#31895;&#26230;&#26684;&#19978;&#20135;&#29983;&#20855;&#26377;&#38750;&#24120;&#23567;&#26230;&#26684;&#25928;&#24212;&#30340;&#29289;&#29702;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2311.17816</link><description>&lt;p&gt;
&#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#33719;&#24471;&#30340;&#22266;&#23450;&#28857;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fixed point actions from convolutional neural networks. (arXiv:2311.17816v1 [hep-lat] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26230;&#26684;&#35268;&#33539;&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;L-CNN&#65289;&#25551;&#36848;&#20102;&#22522;&#20110;&#37325;&#25972;&#21270;&#32676;&#21464;&#25442;&#30340;&#22266;&#23450;&#28857;&#20316;&#29992;&#65288;FP&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#21442;&#25968;&#21270;FP&#20316;&#29992;&#65292;&#21487;&#20197;&#35268;&#36991;&#20020;&#30028;&#20943;&#36895;&#21644;&#25299;&#25169;&#20923;&#32467;&#38382;&#39064;&#65292;&#24182;&#22312;&#31895;&#26230;&#26684;&#19978;&#20135;&#29983;&#20855;&#26377;&#38750;&#24120;&#23567;&#26230;&#26684;&#25928;&#24212;&#30340;&#29289;&#29702;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26230;&#26684;&#35268;&#33539;&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;L-CNN&#65289;&#21487;&#29992;&#20110;&#24418;&#25104;&#20219;&#24847;&#24418;&#29366;&#30340;Wilson&#29615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36817;&#20284;&#26230;&#26684;&#19978;&#30340;&#20219;&#20309;&#35268;&#33539;&#20381;&#21464;&#25110;&#35268;&#33539;&#19981;&#21464;&#20989;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;L-CNN&#26469;&#25551;&#36848;&#22522;&#20110;&#37325;&#25972;&#21270;&#32676;&#21464;&#25442;&#30340;&#22266;&#23450;&#28857;&#65288;FP&#65289;&#20316;&#29992;&#12290;FP&#20316;&#29992;&#22312;&#32463;&#20856;&#35268;&#33539;&#22330;&#28385;&#36275;&#36816;&#21160;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#26159;&#32463;&#20856;&#23436;&#32654;&#30340;&#65292;&#21363;&#23427;&#20204;&#27809;&#26377;&#26230;&#26684;&#25928;&#24212;&#65292;&#24182;&#19988;&#20855;&#26377;&#23610;&#24230;&#19981;&#21464;&#30340;&#30636;&#23376;&#35299;&#12290;FP&#20316;&#29992;&#22312;&#26230;&#26684;&#38388;&#36317;&#30340;&#25152;&#26377;&#38454;&#23618;&#20013;&#37117;&#26159;&#26641;&#32423;Symanzik&#25913;&#36827;&#30340;&#65292;&#21363;&#20351;&#22312;&#31895;&#26230;&#26684;&#19978;&#20063;&#33021;&#20135;&#29983;&#20855;&#26377;&#38750;&#24120;&#23567;&#26230;&#26684;&#25928;&#24212;&#30340;&#29289;&#29702;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#19982;&#36739;&#26087;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;L-CNN&#22312;&#21442;&#25968;&#21270;FP&#20316;&#29992;&#26041;&#38754;&#26356;&#20934;&#30830;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#33021;&#25552;&#20379;&#19968;&#31181;&#35268;&#36991;&#20020;&#30028;&#20943;&#36895;&#21644;&#25299;&#25169;&#20923;&#32467;&#20197;&#25509;&#36817;&#36830;&#32493;&#26497;&#38480;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lattice gauge-equivariant convolutional neural networks (L-CNNs) can be used to form arbitrarily shaped Wilson loops and can approximate any gauge-covariant or gauge-invariant function on the lattice. Here we use L-CNNs to describe fixed point (FP) actions which are based on renormalization group transformations. FP actions are classically perfect, i.e., they have no lattice artifacts on classical gauge-field configurations satisfying the equations of motion, and therefore possess scale invariant instanton solutions. FP actions are tree-level Symanzik-improved to all orders in the lattice spacing and can produce physical predictions with very small lattice artifacts even on coarse lattices. We find that L-CNNs are much more accurate at parametrizing the FP action compared to older approaches. They may therefore provide a way to circumvent critical slowing down and topological freezing towards the continuum limit.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20934;&#30830;&#21487;&#38752;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25925;&#38556;&#33410;&#28857;&#20256;&#25773;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2311.16522</link><description>&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#20013;&#21160;&#24577;&#25925;&#38556;&#29305;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20934;&#30830;&#21487;&#38752;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25925;&#38556;&#33410;&#28857;&#20256;&#25773;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#24378;&#36816;&#32500;&#30340;&#26234;&#33021;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#30693;&#35782;&#22270;&#35889;&#26469;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#12290;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#20026;&#20102;&#39564;&#35777;&#33410;&#28857;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#36827;&#34892;&#20102;&#27599;&#20010;&#33410;&#28857;&#36755;&#20986;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20223;&#30495;&#22330;&#26223;&#20013;&#20934;&#30830;&#22320;&#23450;&#20301;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#24314;&#27169;&#21487;&#20197;&#23450;&#24615;&#22320;&#32771;&#23519;&#25925;&#38556;&#22914;&#20309;&#22312;&#33410;&#28857;&#38388;&#20256;&#25773;&#65292;&#20026;&#20998;&#26512;&#25925;&#38556;&#33410;&#28857;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance the intelligence degree in operation and maintenance, a novel method for fault detection in power grids is proposed. The proposed GNN-based approach first identifies fault nodes through a specialized feature extraction method coupled with a knowledge graph. By incorporating temporal data, the method leverages the status of nodes from preceding and subsequent time periods to help current fault detection. To validate the effectiveness of the node features, a correlation analysis of the output features from each node was conducted. The results from experiments show that this method can accurately locate fault nodes in simulation scenarios with a remarkable accuracy. Additionally, the graph neural network based feature modeling allows for a qualitative examination of how faults spread across nodes, which provides valuable insights for analyzing fault nodes.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(MMPDE-Net)&#65292;&#36890;&#36807;&#35299;&#20915;&#31227;&#21160;&#32593;&#26684;PDE&#26469;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#65292;&#24182;&#19988;&#32467;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#30340;&#26694;&#26550;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2311.16167</link><description>&lt;p&gt;
&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE. (arXiv:2311.16167v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(MMPDE-Net)&#65292;&#36890;&#36807;&#35299;&#20915;&#31227;&#21160;&#32593;&#26684;PDE&#26469;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#65292;&#24182;&#19988;&#32467;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#30340;&#26694;&#26550;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;&#26041;&#27861;&#30340;&#31471;&#21040;&#31471;&#33258;&#36866;&#24212;&#37319;&#26679;&#31070;&#32463;&#32593;&#32476;&#65288;MMPDE-Net&#65289;&#65292;&#36890;&#36807;&#27714;&#35299;&#31227;&#21160;&#32593;&#26684;PDE&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25913;&#21892;&#37319;&#26679;&#28857;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;MMPDE-Net&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#65292;&#20351;&#24471;&#37319;&#26679;&#28857;&#26356;&#21152;&#31934;&#30830;&#21644;&#21487;&#25511;&#12290;&#30001;&#20110;MMPDE-Net&#26159;&#29420;&#31435;&#20110;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#65292;&#24182;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#36890;&#36807;&#35823;&#24046;&#20998;&#26512;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#20856;&#22411;&#23454;&#20363;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#65292;&#20174;&#32780;&#25968;&#20540;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose an end-to-end adaptive sampling neural network (MMPDE-Net) based on the moving mesh method, which can adaptively generate new sampling points by solving the moving mesh PDE. This model focuses on improving the quality of sampling points generation. Moreover, we develop an iterative algorithm based on MMPDE-Net, which makes the sampling points more precise and controllable. Since MMPDE-Net is a framework independent of the deep learning solver, we combine it with physics-informed neural networks (PINN) to propose moving sampling PINN (MS-PINN) and demonstrate its effectiveness by error analysis under some assumptions. Finally, we demonstrate the performance improvement of MS-PINN compared to PINN through numerical experiments of four typical examples, which numerically verify the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#20889;&#20316;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24212;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#26816;&#27979;&#21040;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#20889;&#20316;&#20043;&#38388;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2311.15565</link><description>&lt;p&gt;
&#35780;&#20272;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text. (arXiv:2311.15565v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#20889;&#20316;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24212;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#26816;&#27979;&#21040;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#20889;&#20316;&#20043;&#38388;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#30340;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26469;&#20934;&#30830;&#21306;&#20998;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#20889;&#20316;&#12290;&#25105;&#24212;&#29992;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#26041;&#27861;&#35770;&#65292;&#21033;&#29992;&#20102;&#19968;&#20010;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;AI&#21644;&#20154;&#31867;&#25991;&#26412;&#65292;&#27599;&#20010;&#25991;&#26412;&#37117;&#26631;&#26377;&#25351;&#31034;&#12290;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20415;&#20110;&#23545;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#32467;&#21512;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20010;&#23450;&#21046;&#27169;&#22411;&#20351;&#24471;&#23427;&#33021;&#22815;&#26816;&#27979;&#20986;AI&#21644;&#20154;&#31867;&#20869;&#23481;&#20043;&#38388;&#24494;&#22937;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
My research investigates the use of cutting-edge hybrid deep learning models to accurately differentiate between AI-generated text and human writing. I applied a robust methodology, utilising a carefully selected dataset comprising AI and human texts from various sources, each tagged with instructions. Advanced natural language processing techniques facilitated the analysis of textual features. Combining sophisticated neural networks, the custom model enabled it to detect nuanced differences between AI and human content.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#25506;&#32034;&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#35838;&#31243;&#23398;&#20064;&#26159;&#25913;&#21892;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#36884;&#24452;&#65292;&#32780;&#27169;&#20223;&#23398;&#20064;&#20063;&#24212;&#35813;&#34987;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.13326</link><description>&lt;p&gt;
&#12298;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26080;&#20851;&#25511;&#21046;&#12299;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#25506;&#32034;&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#35838;&#31243;&#23398;&#20064;&#26159;&#25913;&#21892;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#36884;&#24452;&#65292;&#32780;&#27169;&#20223;&#23398;&#20064;&#20063;&#24212;&#35813;&#34987;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#24050;&#34987;&#24191;&#27867;&#36816;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#24230;&#38543;&#26426;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#21033;&#29992;&#36825;&#20123;&#24819;&#27861;&#36827;&#34892;&#25511;&#21046;&#20219;&#21153;&#30340;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20004;&#20010;&#26041;&#38754;&#25506;&#35752;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#20195;&#34920;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#20102;&#35838;&#31243;&#23398;&#20064;&#30340;&#22522;&#26412;&#24605;&#24819;&#65292;&#32780;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#20174;&#19987;&#23478;&#20013;&#33976;&#39311;&#20986;&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35838;&#31243;&#23398;&#20064;&#22312;&#25913;&#36827;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#30340;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#24212;&#34987;&#35270;&#20026;&#19968;&#31181;&#26032;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#38543;&#26426;&#31181;&#23376;&#22806;&#26679;&#26412;&#23454;&#35777;&#21644;&#28040;&#34701;&#30740;&#31350;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#30340;&#35838;&#31243;&#23398;&#20064;&#38750;&#24120;&#40723;&#33310;&#20154;&#24515;&#12290;&#36825;&#20123;&#21457;&#29616;&#23588;&#20854;&#40723;&#33310;&#20154;&#24515;&#65292;&#22240;&#20026;&#25105;&#20204;&#22312;&#22522;&#32447;&#19978;&#35843;&#25972;&#20102;&#25152;&#26377;&#37325;&#21472;&#30340;&#36229;&#21442;&#25968;&#65292;&#32473;&#20986;&#20102;&#22522;&#32447;&#30340;&#20248;&#21183;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#20223;&#23398;&#20064;&#24212;&#35813;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curriculum learning and imitation learning have been leveraged extensively in the robotics domain. However, minimal research has been done on leveraging these ideas on control tasks over highly stochastic time-series data. Here, we theoretically and empirically explore these approaches in a representative control task over complex time-series data. We implement the fundamental ideas of curriculum learning via data augmentation, while imitation learning is implemented via policy distillation from an oracle. Our findings reveal that curriculum learning should be considered a novel direction in improving control-task performance over complex time-series. Our ample random-seed out-sample empirics and ablation studies are highly encouraging for curriculum learning for time-series control. These findings are especially encouraging as we tune all overlapping hyperparameters on the baseline -- giving an advantage to the baseline. On the other hand, we find that imitation learning should be use
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#36731;&#37327;&#32423;&#36229;&#22768;&#21333;&#35282;&#24230;&#24179;&#38754;&#27874;&#25104;&#20687;&#27874;&#26463;&#24418;&#25104;&#22120;&#65288;Tiny-VBF&#65289;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#26356;&#24555;&#30340;&#21253;&#32476;&#26816;&#27979;&#36895;&#24230;&#65292;&#26356;&#39640;&#30340;&#23545;&#27604;&#24230;&#21644;&#20998;&#36776;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.12082</link><description>&lt;p&gt;
Tiny-VBF: &#22522;&#20110;Vision Transformer&#30340;&#36731;&#37327;&#32423;&#36229;&#22768;&#21333;&#35282;&#24230;&#24179;&#38754;&#27874;&#25104;&#20687;&#27874;&#26463;&#24418;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Tiny-VBF: Resource-Efficient Vision Transformer based Lightweight Beamformer for Ultrasound Single-Angle Plane Wave Imaging. (arXiv:2311.12082v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#36731;&#37327;&#32423;&#36229;&#22768;&#21333;&#35282;&#24230;&#24179;&#38754;&#27874;&#25104;&#20687;&#27874;&#26463;&#24418;&#25104;&#22120;&#65288;Tiny-VBF&#65289;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#26356;&#24555;&#30340;&#21253;&#32476;&#26816;&#27979;&#36895;&#24230;&#65292;&#26356;&#39640;&#30340;&#23545;&#27604;&#24230;&#21644;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21152;&#36895;&#35745;&#31639;&#23494;&#38598;&#22411;&#38750;&#23454;&#26102;&#36229;&#22768;&#25104;&#20687;&#27874;&#26463;&#24418;&#25104;&#31639;&#27861;&#19968;&#30452;&#22312;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#22797;&#26434;&#24615;&#32473;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#26032;&#22411;&#24494;&#22411;&#27874;&#26463;&#24418;&#25104;&#22120;&#65288;Tiny-VBF&#65289;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#36890;&#36807;&#21333;&#35282;&#24230;&#24179;&#38754;&#27874;&#36229;&#22768;&#28608;&#21457;&#33719;&#24471;&#30340;&#21407;&#22987;&#23556;&#39057;&#36890;&#36947;&#25968;&#25454;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;Tiny-VBF&#30340;&#36755;&#20986;&#25552;&#20379;&#24555;&#36895;&#21253;&#32476;&#26816;&#27979;&#65292;&#20165;&#38656;&#35201;&#26497;&#20302;&#30340;&#24103;&#29575;&#65292;&#21363;&#23545;&#20110;368 x 128&#30340;&#24103;&#22823;&#23567;&#65292;&#27599;&#24103;&#20165;&#20026;0.34 GOPs&#12290;&#22312;&#20307;&#22806;&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;Tiny-CNN&#30456;&#27604;&#65292;&#20854;&#23545;&#27604;&#24230;&#25552;&#39640;&#20102;8%&#65292;&#36724;&#21521;&#21644;&#27178;&#21521;&#20998;&#36776;&#29575;&#20998;&#21035;&#25552;&#39640;&#20102;5%&#21644;33%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20307;&#22806;&#25968;&#25454;&#38598;&#19978;&#30340;&#23545;&#27604;&#24230;&#25552;&#39640;&#20102;4.2%&#65292;&#36724;&#21521;&#21644;&#27178;&#21521;&#20998;&#36776;&#29575;&#20998;&#21035;&#25552;&#39640;&#20102;4%&#21644;20%&#12290;
&lt;/p&gt;
&lt;p&gt;
Accelerating compute intensive non-real-time beam-forming algorithms in ultrasound imaging using deep learning architectures has been gaining momentum in the recent past. Nonetheless, the complexity of the state-of-the-art deep learning techniques poses challenges for deployment on resource-constrained edge devices. In this work, we propose a novel vision transformer based tiny beamformer (Tiny-VBF), which works on the raw radio-frequency channel data acquired through single-angle plane wave insonification. The output of our Tiny-VBF provides fast envelope detection requiring very low frame rate, i.e. 0.34 GOPs/Frame for a frame size of 368 x 128 in comparison to the state-of-the-art deep learning models. It also exhibited an 8% increase in contrast and gains of 5% and 33% in axial and lateral resolution respectively when compared to Tiny-CNN on in-vitro dataset. Additionally, our model showed a 4.2% increase in contrast and gains of 4% and 20% in axial and lateral resolution respectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#21644;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#36830;&#32493;&#26102;&#31354;&#27169;&#22411;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#30701;&#26399;&#26102;&#38388;&#24207;&#21015;&#21644;&#38271;&#26399;&#32479;&#35745;&#25968;&#25454;&#19978;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2311.11798</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#31354;&#27169;&#22411;&#30340;&#22522;&#20110;&#26799;&#24230;&#21644;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Operator Learning for Continuous Spatial-Temporal Model with Gradient-Based and Derivative-Free Optimization Methods. (arXiv:2311.11798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#21644;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#36830;&#32493;&#26102;&#31354;&#27169;&#22411;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#30701;&#26399;&#26102;&#38388;&#24207;&#21015;&#21644;&#38271;&#26399;&#32479;&#35745;&#25968;&#25454;&#19978;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24037;&#31243;&#24212;&#29992;&#20013;&#65292;&#24120;&#24120;&#20351;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#26102;&#31354;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#23545;&#20110;&#31354;&#38388;&#21644;&#26102;&#38388;&#31163;&#25955;&#21270;&#30340;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#19981;&#21516;&#26102;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#25913;&#21892;&#26657;&#20934;&#27169;&#22411;&#30340;&#38271;&#26399;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20248;&#21270;&#26041;&#26696;&#65292;&#21033;&#29992;&#20102;&#22522;&#20110;&#26799;&#24230;&#21644;&#26080;&#23548;&#25968;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#30701;&#26399;&#26102;&#38388;&#24207;&#21015;&#21644;&#38271;&#26399;&#32479;&#35745;&#25968;&#25454;&#19978;&#39640;&#25928;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#25968;&#20540;&#23454;&#20363;&#65292;&#21253;&#25324;&#31896;&#24615;Burgers&#26041;&#31243;&#12289;Navier-Stokes&#26041;&#31243;&#21644;Kuramoto-Sivashinsky&#26041;&#31243;&#65292;&#26469;&#30740;&#31350;&#31354;&#38388;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations are often used in the spatial-temporal modeling of complex dynamical systems in many engineering applications. In this work, we build on the recent progress of operator learning and present a data-driven modeling framework that is continuous in both space and time. A key feature of the proposed model is the resolution-invariance with respect to both spatial and temporal discretizations, without demanding abundant training data in different temporal resolutions. To improve the long-term performance of the calibrated model, we further propose a hybrid optimization scheme that leverages both gradient-based and derivative-free optimization methods and efficiently trains on both short-term time series and long-term statistics. We investigate the performance of the spatial-temporal continuous learning framework with three numerical examples, including the viscous Burgers' equation, the Navier-Stokes equations, and the Kuramoto-Sivashinsky equation. The results 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26816;&#27979;&#36234;&#30028;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#24402;&#22240;&#24322;&#24120;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#20869;&#20998;&#24067;&#21644;&#36234;&#30028;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#24341;&#20837;&#38646;&#33192;&#32960;&#24322;&#24120;&#21644;&#36890;&#36947;&#24179;&#22343;&#24322;&#24120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;GAIA&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#26816;&#27979;&#36234;&#30028;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2311.09620</link><description>&lt;p&gt;
GAIA: &#25506;&#32034;&#22522;&#20110;&#26799;&#24230;&#30340;&#24402;&#22240;&#24322;&#24120;&#24615;&#20197;&#36827;&#34892;&#36234;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection. (arXiv:2311.09620v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26816;&#27979;&#36234;&#30028;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#24402;&#22240;&#24322;&#24120;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#20869;&#20998;&#24067;&#21644;&#36234;&#30028;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#24341;&#20837;&#38646;&#33192;&#32960;&#24322;&#24120;&#21644;&#36890;&#36947;&#24179;&#22343;&#24322;&#24120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;GAIA&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#26816;&#27979;&#36234;&#30028;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#26816;&#27979;&#36234;&#30028;&#65288;OOD&#65289;&#26679;&#26412;&#23545;&#20110;&#20445;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#38024;&#23545;&#22312;&#20998;&#26512;&#27169;&#22411;&#35797;&#22270;&#35299;&#37322;&#20854;&#39044;&#27979;&#20915;&#31574;&#26102;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35270;&#35282;&#26469;&#37327;&#21270;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#21644;&#36234;&#30028;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#24402;&#22240;&#26041;&#27861;&#22312;&#20026;&#36234;&#30028;&#25968;&#25454;&#20998;&#37197;&#29305;&#24449;&#37325;&#35201;&#24615;&#26041;&#38754;&#36935;&#21040;&#20102;&#25361;&#25112;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#35299;&#37322;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24402;&#22240;&#26799;&#24230;&#22914;&#20309;&#23548;&#33268;&#19981;&#30830;&#23450;&#30340;&#35299;&#37322;&#32467;&#26524;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#24418;&#24335;&#30340;&#36234;&#30028;&#26816;&#27979;&#24322;&#24120;&#65306;&#38646;&#33192;&#32960;&#24322;&#24120;&#21644;&#36890;&#36947;&#24179;&#22343;&#24322;&#24120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GAIA&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#26799;&#24230;&#24322;&#24120;&#26816;&#26597;&#21644;&#32858;&#21512;&#12290;&#36890;&#36807;&#22312;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#65288;CIFAR&#65289;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;GAIA&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting out-of-distribution (OOD) examples is crucial to guarantee the reliability and safety of deep neural networks in real-world settings. In this paper, we offer an innovative perspective on quantifying the disparities between in-distribution (ID) and OOD data -- analyzing the uncertainty that arises when models attempt to explain their predictive decisions. This perspective is motivated by our observation that gradient-based attribution methods encounter challenges in assigning feature importance to OOD data, thereby yielding divergent explanation patterns. Consequently, we investigate how attribution gradients lead to uncertain explanation outcomes and introduce two forms of abnormalities for OOD detection: the zero-deflation abnormality and the channel-wise average abnormality. We then propose GAIA, a simple and effective approach that incorporates Gradient Abnormality Inspection and Aggregation. The effectiveness of GAIA is validated on both commonly utilized (CIFAR) and larg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#20307;&#29305;&#24449;&#65292;&#22312;&#20998;&#37197;&#32593;&#32476;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20998;&#37197;&#25991;&#26412;&#20013;&#36827;&#34892;&#21305;&#37197;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#30005;&#21147;&#20998;&#37197;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#20013;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.08724</link><description>&lt;p&gt;
&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Construction in Power Distribution Networks. (arXiv:2311.08724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#20307;&#29305;&#24449;&#65292;&#22312;&#20998;&#37197;&#32593;&#32476;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20998;&#37197;&#25991;&#26412;&#20013;&#36827;&#34892;&#21305;&#37197;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#30005;&#21147;&#20998;&#37197;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#20013;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#20307;&#29305;&#24449;&#65292;&#21253;&#25324;&#20854;&#35821;&#20041;&#12289;&#38899;&#38901;&#21644;&#21477;&#27861;&#29305;&#24449;&#65292;&#22312;&#20998;&#37197;&#32593;&#32476;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20998;&#37197;&#25991;&#26412;&#20013;&#36827;&#34892;&#21305;&#37197;&#12290;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22686;&#24378;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#23558;&#20998;&#37197;&#25991;&#26412;&#23454;&#20307;&#19982;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#21305;&#37197;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#30005;&#21147;&#20998;&#37197;&#22330;&#26223;&#20013;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38142;&#25509;&#21508;&#31181;&#23454;&#20307;&#31867;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#30005;&#21147;&#20998;&#37197;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a method for knowledge graph construction in power distribution networks. This method leverages entity features, which involve their semantic, phonetic, and syntactic characteristics, in both the knowledge graph of distribution network and the dispatching texts. An enhanced model based on Convolutional Neural Network, is utilized for effectively matching dispatch text entities with those in the knowledge graph. The effectiveness of this model is evaluated through experiments in real-world power distribution dispatch scenarios. The results indicate that, compared with the baselines, the proposed model excels in linking a variety of entity types, demonstrating high overall accuracy in power distribution knowledge graph construction task.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#26368;&#23567;&#33539;&#25968;&#27973;&#23618;&#21435;&#22122;&#22120;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#65292;&#25512;&#23548;&#20986;&#19968;&#20803;&#25968;&#25454;&#21644;&#22810;&#20803;&#25968;&#25454;&#19978;&#30340;&#38381;&#21512;&#24418;&#24335;&#65292;&#24182;&#21457;&#29616;&#20854;&#20855;&#26377;&#25910;&#32553;&#24615;&#21644;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.06748</link><description>&lt;p&gt;
&#26368;&#23567;&#33539;&#25968;&#27973;&#23618;&#21435;&#22122;&#22120;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Minimum-Norm Shallow Denoisers Look in Function Space?. (arXiv:2311.06748v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.06748
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#26368;&#23567;&#33539;&#25968;&#27973;&#23618;&#21435;&#22122;&#22120;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#65292;&#25512;&#23548;&#20986;&#19968;&#20803;&#25968;&#25454;&#21644;&#22810;&#20803;&#25968;&#25454;&#19978;&#30340;&#38381;&#21512;&#24418;&#24335;&#65292;&#24182;&#21457;&#29616;&#20854;&#20855;&#26377;&#25910;&#32553;&#24615;&#21644;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21435;&#22122;&#22120;&#65288;NN&#21435;&#22122;&#22120;&#65289;&#26159;&#35768;&#22810;&#24120;&#35265;&#20219;&#21153;&#20013;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#65292;&#20174;&#22270;&#20687;&#37325;&#24314;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#20174;&#29702;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#25104;&#21151;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#26088;&#22312;&#25551;&#36848;&#27973;&#23618;ReLU NN&#21435;&#22122;&#22120;&#23454;&#29616;&#30340;&#20989;&#25968;&#29305;&#24615;--&#22312;&#25554;&#20540;&#30340;&#24120;&#35265;&#29702;&#35770;&#35774;&#32622;&#19979;&#65288;&#21363;&#38646;&#35757;&#32451;&#25439;&#22833;&#65289;&#20197;&#21450;&#26368;&#23567;&#34920;&#31034;&#25104;&#26412;&#65288;&#21363;&#26368;&#23567;&#30340;l^2&#33539;&#25968;&#26435;&#37325;&#65289;&#12290;&#39318;&#20808;&#65292;&#23545;&#20110;&#19968;&#20803;&#25968;&#25454;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;NN&#21435;&#22122;&#22120;&#20989;&#25968;&#30340;&#38381;&#21512;&#24418;&#24335;&#65292;&#21457;&#29616;&#23427;&#23545;&#24178;&#20928;&#25968;&#25454;&#28857;&#20855;&#26377;&#25910;&#32553;&#24615;&#65292;&#24182;&#35777;&#26126;&#22312;&#20302;&#22122;&#22768;&#27700;&#24179;&#19979;&#23427;&#27604;&#32463;&#39564;MMSE&#20272;&#35745;&#22120;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#25509;&#19979;&#26469;&#65292;&#23545;&#20110;&#22810;&#20803;&#25968;&#25454;&#65292;&#25105;&#20204;&#22312;&#22810;&#31181;&#20960;&#20309;&#20551;&#35774;&#19979;&#25214;&#21040;&#20102;&#38381;&#21512;&#24418;&#24335;&#30340;NN&#21435;&#22122;&#22120;&#20989;&#25968;&#65306;&#25968;&#25454;&#21253;&#21547;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#25968;&#25454;&#21253;&#21547;&#22312;&#21333;&#21521;&#23556;&#32447;&#30340;&#24182;&#38598;&#20013;&#65292;&#25110;&#32773;&#22810;&#31181;&#31867;&#22411;&#30340;&#31616;&#21333;&#24418;&#29366;&#12290;&#36825;&#20123;&#20989;&#25968;&#20998;&#35299;&#20026;&#19968;&#20010;&#21644;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network (NN) denoisers are an essential building block in many common tasks, ranging from image reconstruction to image generation. However, the success of these models is not well understood from a theoretical perspective. In this paper, we aim to characterize the functions realized by shallow ReLU NN denoisers -- in the common theoretical setting of interpolation (i.e., zero training loss) with a minimal representation cost (i.e., minimal $\ell^2$ norm weights). First, for univariate data, we derive a closed form for the NN denoiser function, find it is contractive toward the clean data points, and prove it generalizes better than the empirical MMSE estimator at a low noise level. Next, for multivariate data, we find the NN denoiser functions in a closed form under various geometric assumptions on the training data: data contained in a low-dimensional subspace, data contained in a union of one-sided rays, or several types of simplexes. These functions decompose into a sum of s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#24863;&#30693;&#20998;&#31163;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#36793;&#32536;&#20113;&#36164;&#28304;&#38598;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#32593;&#32476;&#39640;&#25928;&#24615;&#21644;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#37096;&#32626;&#22312;&#36739;&#24369;&#35774;&#22791;&#19978;&#12290;</title><link>http://arxiv.org/abs/2311.05739</link><description>&lt;p&gt;
&#36793;&#32536;&#32593;&#32476;&#25928;&#29575;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Architecture for Network-Efficiency at the Edge. (arXiv:2311.05739v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.05739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#24863;&#30693;&#20998;&#31163;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#36793;&#32536;&#20113;&#36164;&#28304;&#38598;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#32593;&#32476;&#39640;&#25928;&#24615;&#21644;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#37096;&#32626;&#22312;&#36739;&#24369;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#35774;&#22791;&#19978;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#23548;&#33268;&#20102;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#29616;&#26377;&#36793;&#32536;&#20113;&#36164;&#28304;&#38598;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65307;&#30001;&#20110;&#20854;&#22312;&#35774;&#22791;&#33021;&#32791;&#12289;&#24310;&#36831;&#12289;&#32593;&#32476;&#21033;&#29992;&#21644;&#38544;&#31169;&#25913;&#36827;&#31561;&#26041;&#38754;&#30340;&#22810;&#37325;&#22909;&#22788;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#20998;&#31163;&#24182;&#35745;&#31639;&#30340;&#20998;&#31163;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#32467;&#21512;&#23545;&#36890;&#20449;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;&#30340;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30340;&#22909;&#22788;&#36827;&#19968;&#27493;&#25552;&#39640;&#65292;&#21487;&#20197;&#20316;&#20026;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65289;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#24863;&#30693;&#20998;&#31163;&#23398;&#20064;&#26041;&#27861;&#65288;'deprune'&#65289;&#65292;&#20197;&#25913;&#21892;&#21644;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#20854;&#26356;&#21152;&#32593;&#32476;&#39640;&#25928;&#65288;&#20351;&#29992;&#26356;&#23569;&#30340;&#32593;&#32476;&#36164;&#28304;&#21644;&#26356;&#24555;&#65289;&#65292;&#36825;&#23558;&#20351;&#23427;&#20204;&#25104;&#20026;&#22312;&#36739;&#24369;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing number of AI-driven applications in the mobile devices has led to solutions that integrate deep learning models with the available edge-cloud resources; due to multiple benefits such as reduction in on-device energy consumption, improved latency, improved network usage, and certain privacy improvements, split learning, where deep learning models are split away from the mobile device and computed in a distributed manner, has become an extensively explored topic. Combined with compression-aware methods where learning adapts to compression of communicated data, the benefits of this approach have further improved and could serve as an alternative to established approaches like federated learning methods. In this work, we develop an adaptive compression-aware split learning method ('deprune') to improve and train deep learning models so that they are much more network-efficient (use less network resources and are faster), which would make them ideal to deploy in weaker devices w
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26550;&#26500;&#65292;&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#23558;&#36755;&#20837;&#25968;&#25454;&#21387;&#32553;&#21518;&#20256;&#36882;&#32473;&#37327;&#23376;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#22312;&#28151;&#21512;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20998;&#31163;&#37327;&#23376;&#21644;&#32463;&#20856;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2311.05559</link><description>&lt;p&gt;
&#22312;&#28151;&#21512;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#20998;&#31163;&#37327;&#23376;&#21644;&#32463;&#20856;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
Disentangling Quantum and Classical Contributions in Hybrid Quantum Machine Learning Architectures. (arXiv:2311.05559v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.05559
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26550;&#26500;&#65292;&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#23558;&#36755;&#20837;&#25968;&#25454;&#21387;&#32553;&#21518;&#20256;&#36882;&#32473;&#37327;&#23376;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#22312;&#28151;&#21512;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20998;&#31163;&#37327;&#23376;&#21644;&#32463;&#20856;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#20026;&#25968;&#25454;&#23494;&#38598;&#22411;&#20219;&#21153;&#25552;&#20379;&#20102;&#26356;&#24378;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#37327;&#23376;&#30828;&#20214;&#23545;&#36755;&#20837;&#35268;&#27169;&#26377;&#24456;&#22823;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26550;&#26500;&#65306;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#24471;&#21040;&#36755;&#20837;&#25968;&#25454;&#30340;&#21387;&#32553;&#29256;&#26412;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;&#37327;&#23376;&#37096;&#20998;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#23558;&#35813;&#27169;&#22411;&#30340;&#20998;&#31867;&#33021;&#21147;&#19982;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#26550;&#26500;&#12289;&#20004;&#31181;&#32431;&#32463;&#20856;&#26550;&#26500;&#21644;&#19968;&#31181;&#37327;&#23376;&#26550;&#26500;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computing offers the potential for superior computational capabilities, particularly for data-intensive tasks. However, the current state of quantum hardware puts heavy restrictions on input size. To address this, hybrid transfer learning solutions have been developed, merging pre-trained classical models, capable of handling extensive inputs, with variational quantum circuits. Yet, it remains unclear how much each component -- classical and quantum -- contributes to the model's results. We propose a novel hybrid architecture: instead of utilizing a pre-trained network for compression, we employ an autoencoder to derive a compressed version of the input data. This compressed data is then channeled through the encoder part of the autoencoder to the quantum component. We assess our model's classification capabilities against two state-of-the-art hybrid transfer learning architectures, two purely classical architectures and one quantum architecture. Their accuracy is compared acro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.04131</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#23450;&#20301;&#36328;&#20219;&#21153;&#24207;&#21015;&#32487;&#32493;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04131
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#22797;&#26434;&#30340;&#26550;&#26500;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#23558;Transformer&#27169;&#22411;&#36824;&#21407;&#20026;&#21487;&#35835;&#30340;&#30005;&#36335;&#34920;&#31034;&#65292;&#29992;&#20110;&#23454;&#29616;&#31639;&#27861;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#26469;&#25193;&#23637;&#36825;&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#23383;&#12289;&#25968;&#23383;&#35789;&#21644;&#26376;&#20221;&#30340;&#36882;&#22686;&#24207;&#21015;&#12290;&#36890;&#36807;&#24212;&#29992;&#30005;&#36335;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36127;&#36131;&#26816;&#27979;&#24207;&#21015;&#25104;&#21592;&#21644;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#25104;&#21592;&#30340;&#20851;&#38190;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35821;&#20041;&#30456;&#20851;&#24207;&#21015;&#20381;&#36182;&#20110;&#20855;&#26377;&#31867;&#20284;&#20316;&#29992;&#30340;&#20849;&#20139;&#30005;&#36335;&#23376;&#22270;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35760;&#24405;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#65292;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#26356;&#23433;&#20840;&#30340;&#32534;&#36753;&#36807;&#31243;&#12290;&#36825;&#31181;&#23545;Transformer&#30340;&#26426;&#26800;&#29702;&#35299;&#26159;&#26500;&#24314;&#26356;&#20581;&#22766;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#26356;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
&lt;/p&gt;</description></item><item><title>Uni-O4&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#30446;&#26631;&#23454;&#29616;&#20102;&#26080;&#32541;&#20256;&#36882;&#65292;&#22686;&#24378;&#20102;&#23398;&#20064;&#33539;&#24335;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;Uni-O4&#21033;&#29992;&#22810;&#26679;&#30340;&#38598;&#21512;&#31574;&#30053;&#35299;&#20915;&#20102;&#20272;&#35745;&#34892;&#20026;&#31574;&#30053;&#21644;&#31163;&#32447;&#25968;&#25454;&#38598;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.03351</link><description>&lt;p&gt;
Uni-O4: &#23558;&#22312;&#32447;&#19982;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32479;&#19968;&#36215;&#26469;&#65292;&#37319;&#29992;&#22810;&#27493;&#22312;&#32447;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization. (arXiv:2311.03351v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03351
&lt;/p&gt;
&lt;p&gt;
Uni-O4&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#30446;&#26631;&#23454;&#29616;&#20102;&#26080;&#32541;&#20256;&#36882;&#65292;&#22686;&#24378;&#20102;&#23398;&#20064;&#33539;&#24335;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;Uni-O4&#21033;&#29992;&#22810;&#26679;&#30340;&#38598;&#21512;&#31574;&#30053;&#35299;&#20915;&#20102;&#20272;&#35745;&#34892;&#20026;&#31574;&#30053;&#21644;&#31163;&#32447;&#25968;&#25454;&#38598;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#23545;&#20110;&#39640;&#25928;&#21644;&#23433;&#20840;&#30340;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#23558;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#35270;&#20026;&#29420;&#31435;&#30340;&#36807;&#31243;&#65292;&#23548;&#33268;&#37325;&#22797;&#30340;&#35774;&#35745;&#21644;&#26377;&#38480;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Uni-o4&#65292;&#23427;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#20013;&#37117;&#20351;&#29992;&#20102;&#19968;&#20010;&#22312;&#32447;&#31574;&#30053;&#30446;&#26631;&#12290;&#30001;&#20110;&#20004;&#20010;&#38454;&#27573;&#30340;&#30446;&#26631;&#23545;&#40784;&#65292;RL&#20195;&#29702;&#21487;&#20197;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#20043;&#38388;&#26080;&#32541;&#20256;&#36882;&#12290;&#36825;&#31181;&#24615;&#36136;&#22686;&#24378;&#20102;&#23398;&#20064;&#33539;&#24335;&#30340;&#28789;&#27963;&#24615;&#65292;&#20801;&#35768;&#20219;&#24847;&#32452;&#21512;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#12289;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;Uni-o4&#21033;&#29992;&#22810;&#26679;&#30340;&#38598;&#21512;&#31574;&#30053;&#26469;&#35299;&#20915;&#20272;&#35745;&#34892;&#20026;&#31574;&#30053;&#21644;&#31163;&#32447;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;
&lt;/p&gt;
&lt;p&gt;
Combining offline and online reinforcement learning (RL) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures, resulting in redundant designs and limited performance. We ask: Can we achieve straightforward yet effective offline and online learning without introducing extra conservatism or regularization? In this study, we propose Uni-o4, which utilizes an on-policy objective for both offline and online learning. Owning to the alignment of objectives in two phases, the RL agent can transfer between offline and online learning seamlessly. This property enhances the flexibility of the learning paradigm, allowing for arbitrary combinations of pretraining, fine-tuning, offline, and online learning. In the offline phase, specifically, Uni-o4 leverages diverse ensemble policies to address the mismatch issues between the estimated behavior policy and the offline dataset. Through a simple offline policy evaluation (OPE
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#28155;&#21152;&#26426;&#21046;&#31227;&#20301;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SAMS-VAE&#65289;&#65292;&#29992;&#20110;&#24314;&#27169;&#32454;&#32990;&#30340;&#25200;&#21160;&#24773;&#20917;&#65292;&#24182;&#32467;&#21512;&#22797;&#21512;&#24615;&#12289;&#35299;&#32544;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#31232;&#30095;&#21270;&#22788;&#29702;&#20840;&#23616;&#28508;&#21464;&#37327;&#65292;SAMS-VAE&#33021;&#22815;&#35782;&#21035;&#20986;&#29305;&#23450;&#20110;&#24178;&#25200;&#30340;&#28508;&#22312;&#23376;&#31354;&#38388;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2311.02794</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#28155;&#21152;&#26426;&#21046;&#31227;&#20301;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#23545;&#32454;&#32990;&#25200;&#21160;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder. (arXiv:2311.02794v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#28155;&#21152;&#26426;&#21046;&#31227;&#20301;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SAMS-VAE&#65289;&#65292;&#29992;&#20110;&#24314;&#27169;&#32454;&#32990;&#30340;&#25200;&#21160;&#24773;&#20917;&#65292;&#24182;&#32467;&#21512;&#22797;&#21512;&#24615;&#12289;&#35299;&#32544;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#31232;&#30095;&#21270;&#22788;&#29702;&#20840;&#23616;&#28508;&#21464;&#37327;&#65292;SAMS-VAE&#33021;&#22815;&#35782;&#21035;&#20986;&#29305;&#23450;&#20110;&#24178;&#25200;&#30340;&#28508;&#22312;&#23376;&#31354;&#38388;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#24178;&#39044;&#19979;&#35266;&#27979;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#31185;&#23398;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#38656;&#35201;&#23545;&#32454;&#32990;&#30340;&#22810;&#31181;&#24178;&#39044;&#25928;&#24212;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#25581;&#31034;&#26410;&#30693;&#30340;&#29983;&#29289;&#20316;&#29992;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31232;&#30095;&#28155;&#21152;&#26426;&#21046;&#31227;&#20301;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SAMS-VAE&#65289;&#65292;&#20197;&#32452;&#21512;&#22797;&#21512;&#24615;&#12289;&#35299;&#32544;&#21644;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#25200;&#21160;&#27169;&#22411;&#12290;SAMS-VAE&#23558;&#25200;&#21160;&#26679;&#26412;&#30340;&#28508;&#22312;&#29366;&#24577;&#24314;&#27169;&#20026;&#19968;&#20010;&#23616;&#37096;&#28508;&#22312;&#21464;&#37327;&#21644;&#31232;&#30095;&#20840;&#23616;&#21464;&#37327;&#20043;&#21644;&#65292;&#29992;&#20110;&#25429;&#25417;&#26679;&#26412;&#29305;&#23450;&#30340;&#21464;&#21270;&#21644;&#28508;&#22312;&#24178;&#39044;&#25928;&#24212;&#12290;&#20851;&#38190;&#26159;&#65292;SAMS-VAE&#36890;&#36807;&#23545;&#21508;&#20010;&#24178;&#39044;&#30340;&#20840;&#23616;&#28508;&#21464;&#37327;&#36827;&#34892;&#31232;&#30095;&#21270;&#22788;&#29702;&#65292;&#20174;&#32780;&#35782;&#21035;&#20986;&#35299;&#32544;&#30340;&#12289;&#24178;&#25200;&#29305;&#23450;&#30340;&#28508;&#22312;&#23376;&#31354;&#38388;&#65292;&#36825;&#20123;&#23376;&#31354;&#38388;&#20855;&#26377;&#28789;&#27963;&#30340;&#32452;&#21512;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#21333;&#32454;&#32990;&#27979;&#24207;&#25968;&#25454;&#38598;&#19978;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#20102;SAMS-VAE&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models of observations under interventions have been a vibrant topic of interest across machine learning and the sciences in recent years. For example, in drug discovery, there is a need to model the effects of diverse interventions on cells in order to characterize unknown biological mechanisms of action. We propose the Sparse Additive Mechanism Shift Variational Autoencoder, SAMS-VAE, to combine compositionality, disentanglement, and interpretability for perturbation models. SAMS-VAE models the latent state of a perturbed sample as the sum of a local latent variable capturing sample-specific variation and sparse global variables of latent intervention effects. Crucially, SAMS-VAE sparsifies these global latent variables for individual perturbations to identify disentangled, perturbation-specific latent subspaces that are flexibly composable. We evaluate SAMS-VAE both quantitatively and qualitatively on a range of tasks using two popular single cell sequencing datasets. In 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#22312;&#22810;&#27169;&#24577;&#34920;&#31034;&#12289;&#34701;&#21512;&#12289;&#32763;&#35793;&#12289;&#23545;&#40784;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#38754;&#30340;&#25361;&#25112;&#21644;&#21019;&#26032;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#25968;&#25454;&#20559;&#24046;&#21644;&#8220;&#22823;&#25968;&#25454;&#8221;&#31232;&#32570;&#24615;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.02332</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#20020;&#24202;&#29983;&#29289;&#21307;&#23398;&#20013;&#30340;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#65306;&#35843;&#26597;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects. (arXiv:2311.02332v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02332
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#22312;&#22810;&#27169;&#24577;&#34920;&#31034;&#12289;&#34701;&#21512;&#12289;&#32763;&#35793;&#12289;&#23545;&#40784;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#38754;&#30340;&#25361;&#25112;&#21644;&#21019;&#26032;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#25968;&#25454;&#20559;&#24046;&#21644;&#8220;&#22823;&#25968;&#25454;&#8221;&#31232;&#32570;&#24615;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#24050;&#32463;&#20174;&#20256;&#32479;&#21644;&#32479;&#35745;&#26041;&#27861;&#36716;&#21521;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#24403;&#21069;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#28145;&#36828;&#24433;&#21709;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#22312;&#22810;&#27169;&#24577;&#34920;&#31034;&#12289;&#34701;&#21512;&#12289;&#32763;&#35793;&#12289;&#23545;&#40784;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#21019;&#26032;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#23545;&#20020;&#24202;&#39044;&#27979;&#30340;&#36716;&#21464;&#28508;&#21147;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#25552;&#20986;&#20102;&#30097;&#38382;&#65292;&#20851;&#27880;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#19982;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#35768;&#22810;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#25968;&#25454;&#20559;&#24046;&#21644;&#8220;&#22823;&#25968;&#25454;&#8221;&#30340;&#31232;&#32570;&#24615;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#26377;&#25928;&#21019;&#26032;&#21644;&#21512;&#20316;&#21162;&#21147;&#20197;&#36827;&#19968;&#27493;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) applications in medical artificial intelligence (AI) systems have shifted from traditional and statistical methods to increasing application of deep learning models. This survey navigates the current landscape of multimodal ML, focusing on its profound impact on medical image analysis and clinical decision support systems. Emphasizing challenges and innovations in addressing multimodal representation, fusion, translation, alignment, and co-learning, the paper explores the transformative potential of multimodal models for clinical predictions. It also questions practical implementation of such models, bringing attention to the dynamics between decision support systems and healthcare providers. Despite advancements, challenges such as data biases and the scarcity of "big data" in many biomedical domains persist. We conclude with a discussion on effective innovation and collaborative efforts to further the miss
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65292;&#23398;&#29983;&#32593;&#32476;&#26159;&#21542;&#24212;&#35813;&#22797;&#21046;&#25945;&#24072;&#31070;&#32463;&#20803;&#25110;&#24179;&#22343;&#19968;&#32452;&#25945;&#24072;&#31070;&#32463;&#20803;&#30340;&#26435;&#37325;&#12290;&#30740;&#31350;&#21457;&#29616;&#23545;&#20110;&#29305;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#21644;&#36755;&#20837;&#20998;&#24067;&#65292;&#24403;&#25945;&#24072;&#32593;&#32476;&#30340;&#36755;&#20837;&#21521;&#37327;&#27491;&#20132;&#19988;&#36755;&#20986;&#26435;&#37325;&#20026;&#37193;&#26102;&#65292;&#22797;&#21046;-&#24179;&#22343;&#37197;&#32622;&#23558;&#36798;&#21040;&#20248;&#21270;&#32467;&#26524;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#23398;&#29983;&#31070;&#32463;&#20803;&#22797;&#21046;&#19968;&#20010;&#25945;&#24072;&#31070;&#32463;&#20803;&#65292;&#26368;&#21518;&#19968;&#20010;&#23398;&#29983;&#31070;&#32463;&#20803;&#23545;&#25152;&#26377;&#25945;&#24072;&#31070;&#32463;&#20803;&#21462;&#24179;&#22343;&#20540;&#12290;</title><link>http://arxiv.org/abs/2311.01644</link><description>&lt;p&gt;
&#23398;&#29983;&#32593;&#32476;&#26159;&#21542;&#24212;&#35813;&#22797;&#21046;&#25110;&#24179;&#22343;&#25945;&#24072;&#26435;&#37325;&#65311;
&lt;/p&gt;
&lt;p&gt;
Should Under-parameterized Student Networks Copy or Average Teacher Weights?. (arXiv:2311.01644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65292;&#23398;&#29983;&#32593;&#32476;&#26159;&#21542;&#24212;&#35813;&#22797;&#21046;&#25945;&#24072;&#31070;&#32463;&#20803;&#25110;&#24179;&#22343;&#19968;&#32452;&#25945;&#24072;&#31070;&#32463;&#20803;&#30340;&#26435;&#37325;&#12290;&#30740;&#31350;&#21457;&#29616;&#23545;&#20110;&#29305;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#21644;&#36755;&#20837;&#20998;&#24067;&#65292;&#24403;&#25945;&#24072;&#32593;&#32476;&#30340;&#36755;&#20837;&#21521;&#37327;&#27491;&#20132;&#19988;&#36755;&#20986;&#26435;&#37325;&#20026;&#37193;&#26102;&#65292;&#22797;&#21046;-&#24179;&#22343;&#37197;&#32622;&#23558;&#36798;&#21040;&#20248;&#21270;&#32467;&#26524;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#23398;&#29983;&#31070;&#32463;&#20803;&#22797;&#21046;&#19968;&#20010;&#25945;&#24072;&#31070;&#32463;&#20803;&#65292;&#26368;&#21518;&#19968;&#20010;&#23398;&#29983;&#31070;&#32463;&#20803;&#23545;&#25152;&#26377;&#25945;&#24072;&#31070;&#32463;&#20803;&#21462;&#24179;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#36830;&#32493;&#20989;&#25968; $f^*$ &#37117;&#21487;&#20197;&#29992;&#36275;&#22815;&#22810;&#30340;&#31070;&#32463;&#20803; $k$&#26469;&#36817;&#20284;&#12290;&#25105;&#20204;&#32771;&#34385; $f^*$ &#26412;&#36523;&#26159;&#19968;&#20010;&#20855;&#26377;&#19968;&#20010;&#38544;&#34255;&#23618;&#21644; $k$ &#20010;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#12290;&#29992;&#20855;&#26377; $n&lt;k$ &#20010;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817; $f^*$ &#21487;&#20197;&#30475;&#20316;&#26159;&#23558;&#19968;&#20010;&#27424;&#21442;&#25968;&#21270;&#30340;&#8220;&#23398;&#29983;&#8221;&#32593;&#32476;&#19982; $k$ &#20010;&#31070;&#32463;&#20803;&#30340;&#8220;&#25945;&#24072;&#8221;&#32593;&#32476;&#36827;&#34892;&#25311;&#21512;&#12290;&#30001;&#20110;&#23398;&#29983;&#20855;&#26377;&#36739;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#25152;&#20197;&#19981;&#28165;&#26970;&#27599;&#20010; $n$ &#20010;&#23398;&#29983;&#31070;&#32463;&#20803;&#24212;&#35813;&#22797;&#21046;&#19968;&#20010;&#25945;&#24072;&#31070;&#32463;&#20803;&#36824;&#26159;&#24179;&#22343;&#19968;&#32452;&#25945;&#24072;&#31070;&#32463;&#20803;&#12290;&#23545;&#20110;&#20855;&#26377; erf &#28608;&#27963;&#20989;&#25968;&#21644;&#26631;&#20934;&#39640;&#26031;&#36755;&#20837;&#20998;&#24067;&#30340;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#25945;&#24072;&#30340;&#36755;&#20837;&#21521;&#37327;&#26159;&#27491;&#20132;&#30340;&#24182;&#19988;&#36755;&#20986;&#26435;&#37325;&#26159;&#37193;&#30340;&#26102;&#20505;&#65292;&#8220;&#22797;&#21046;-&#24179;&#22343;&#8221;&#37197;&#32622;&#26159;&#20020;&#30028;&#28857;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#26679;&#30340;&#37197;&#32622;&#20013;&#65292;&#20248;&#21270;&#32467;&#26524;&#26159;&#24403; $n-1$ &#20010;&#23398;&#29983;&#31070;&#32463;&#20803;&#20998;&#21035;&#22797;&#21046;&#19968;&#20010;&#25945;&#24072;&#31070;&#32463;&#20803;&#65292;&#24182;&#19988;&#31532; $n$ &#20010;&#23398;&#29983;&#31070;&#32463;&#20803;&#26159;&#25152;&#26377;&#25945;&#24072;&#31070;&#32463;&#20803;&#30340;&#24179;&#22343;&#12290;
&lt;/p&gt;
&lt;p&gt;
Any continuous function $f^*$ can be approximated arbitrarily well by a neural network with sufficiently many neurons $k$. We consider the case when $f^*$ itself is a neural network with one hidden layer and $k$ neurons. Approximating $f^*$ with a neural network with $n&lt; k$ neurons can thus be seen as fitting an under-parameterized "student" network with $n$ neurons to a "teacher" network with $k$ neurons. As the student has fewer neurons than the teacher, it is unclear, whether each of the $n$ student neurons should copy one of the teacher neurons or rather average a group of teacher neurons. For shallow neural networks with erf activation function and for the standard Gaussian input distribution, we prove that "copy-average" configurations are critical points if the teacher's incoming vectors are orthonormal and its outgoing weights are unitary. Moreover, the optimum among such configurations is reached when $n-1$ student neurons each copy one teacher neuron and the $n$-th student ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#35777;&#20998;&#26512;&#20102;&#32463;&#20856;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#20197;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#20026;&#20363;&#23637;&#31034;&#20102;&#22270;&#20687;&#21387;&#32553;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#20943;&#23567;&#25968;&#25454;&#23610;&#23544;&#24182;&#20445;&#30041;&#24517;&#35201;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#36731;&#25968;&#25454;&#31649;&#29702;&#22522;&#30784;&#35774;&#26045;&#30340;&#36127;&#25285;&#24182;&#26041;&#20415;&#25968;&#25454;&#20849;&#20139;&#25110;&#20113;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2311.01352</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26174;&#24494;&#22270;&#20687;&#21387;&#32553;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep learning based Image Compression for Microscopy Images: An Empirical Study. (arXiv:2311.01352v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#35777;&#20998;&#26512;&#20102;&#32463;&#20856;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#20197;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#20026;&#20363;&#23637;&#31034;&#20102;&#22270;&#20687;&#21387;&#32553;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#20943;&#23567;&#25968;&#25454;&#23610;&#23544;&#24182;&#20445;&#30041;&#24517;&#35201;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#36731;&#25968;&#25454;&#31649;&#29702;&#22522;&#30784;&#35774;&#26045;&#30340;&#36127;&#25285;&#24182;&#26041;&#20415;&#25968;&#25454;&#20849;&#20139;&#25110;&#20113;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#26174;&#24494;&#38236;&#21644;&#29983;&#29289;&#25104;&#20687;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#26174;&#24494;&#22270;&#20687;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#36807;&#32593;&#32476;&#36827;&#34892;&#23384;&#20648;&#12289;&#20998;&#26512;&#29978;&#33267;&#20849;&#20139;&#12290;&#25968;&#25454;&#30340;&#35268;&#27169;&#23545;&#24403;&#21069;&#30340;&#25968;&#25454;&#22522;&#30784;&#35774;&#26045;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20943;&#23567;&#25968;&#25454;&#23610;&#23544;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#22270;&#20687;&#21387;&#32553;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#32463;&#20856;&#30340;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#22788;&#29702;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#65288;&#21363;&#20174;&#20142;&#22330;&#22270;&#20687;&#39044;&#27979;&#33639;&#20809;&#22270;&#20687;&#65289;&#34987;&#29992;&#20316;&#27604;&#36739;&#21644;&#20998;&#26512;&#30340;&#31034;&#20363;&#24212;&#29992;&#12290;&#26377;&#25928;&#30340;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#20002;&#22833;&#24517;&#35201;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23567;&#25968;&#25454;&#23610;&#23544;&#65292;&#20174;&#32780;&#20943;&#36731;&#25968;&#25454;&#31649;&#29702;&#22522;&#30784;&#35774;&#26045;&#30340;&#36127;&#25285;&#65292;&#24182;&#23454;&#29616;&#25968;&#25454;&#20849;&#20139;&#25110;&#20113;&#35745;&#31639;&#19979;&#30340;&#24555;&#36895;&#20256;&#36755;&#12290;&#20026;&#20102;&#20197;&#36825;&#26679;&#30340;&#26041;&#24335;&#21387;&#32553;&#22270;&#20687;&#65292;&#22810;&#31181;&#32463;&#20856;&#30340;&#20002;&#22833;&#21387;&#32553;&#26041;&#27861;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#25439;&#21387;&#32553;&#26041;&#27861;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the fast development of modern microscopes and bioimaging techniques, an unprecedentedly large amount of imaging data are being generated, stored, analyzed, and even shared through networks. The size of the data poses great challenges for current data infrastructure. One common way to reduce the data size is by image compression. This present study analyzes classic and deep learning based image compression methods, and their impact on deep learning based image processing models. Deep learning based label-free prediction models (i.e., predicting fluorescent images from bright field images) are used as an example application for comparison and analysis. Effective image compression methods could help reduce the data size significantly without losing necessary information, and therefore reduce the burden on data management infrastructure and permit fast transmission through the network for data sharing or cloud computing. To compress images in such a wanted way, multiple classical los
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2311.01017</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#25945;&#20250;&#26234;&#33021;&#20307;&#19990;&#30028;&#30340;&#36816;&#20316;&#26041;&#24335;&#12290;&#23613;&#31649;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20294;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#19982;&#20351;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#25193;&#23637;&#19990;&#30028;&#27169;&#22411;&#30340;&#36827;&#23637;&#30456;&#23545;&#36739;&#24930;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#65306;&#22788;&#29702;&#22797;&#26434;&#21644;&#26080;&#32467;&#26500;&#30340;&#35266;&#23519;&#31354;&#38388;&#20197;&#21450;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;&#39318;&#20808;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#28982;&#21518;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#24182;&#34892;&#35299;&#30721;&#21644;&#21435;&#22122;&#26631;&#35760;&#65292;&#25105;&#20204;&#23558;&#36974;&#34109;&#29983;&#25104;&#22270;&#20687;&#36716;&#25442;&#22120;&#36716;&#25442;&#20026;&#31163;&#25955;&#25193;&#25955;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#31616;&#21333;&#30340;&#25913;&#36827;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;&#24403;&#24212;&#29992;&#20110;&#28857;&#20113;&#35266;&#23519;&#30340;&#19990;&#30028;&#27169;&#22411;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#22312;&#32447;&#36716;&#25442;&#21450;&#20854;&#24102;&#26377;&#20999;&#25442;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#38408;&#20540;&#31639;&#27861;&#20197;&#21450;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#26368;&#23567;&#21270;&#21644;&#26368;&#22823;&#21270;&#21464;&#20307;&#20013;&#37117;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20598</link><description>&lt;p&gt;
&#22312;&#32447;&#36716;&#25442;&#21450;&#20854;&#24102;&#26377;&#20999;&#25442;&#25104;&#26412;&#65306;&#31283;&#20581;&#21644;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Conversion with Switching Costs: Robust and Learning-Augmented Algorithms. (arXiv:2310.20598v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#22312;&#32447;&#36716;&#25442;&#21450;&#20854;&#24102;&#26377;&#20999;&#25442;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#38408;&#20540;&#31639;&#27861;&#20197;&#21450;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#26368;&#23567;&#21270;&#21644;&#26368;&#22823;&#21270;&#21464;&#20307;&#20013;&#37117;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#24182;&#30740;&#31350;&#22312;&#32447;&#36716;&#25442;&#21450;&#20854;&#24102;&#26377;&#20999;&#25442;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#28085;&#30422;&#20102;&#33021;&#28304;&#21644;&#21487;&#25345;&#32493;&#24615;&#20132;&#21449;&#39046;&#22495;&#20013;&#20986;&#29616;&#30340;&#19968;&#31995;&#21015;&#26032;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#22312;&#32447;&#29609;&#23478;&#35797;&#22270;&#22312;&#22266;&#23450;&#30340;&#26102;&#38388;&#27573;&#20869;&#36141;&#20080;&#65288;&#25110;&#38144;&#21806;&#65289;&#36164;&#20135;&#30340;&#20998;&#25968;&#20221;&#39069;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#37117;&#20250;&#20844;&#24067;&#25104;&#26412;&#65288;&#25110;&#20215;&#26684;&#65289;&#20989;&#25968;&#65292;&#24182;&#19988;&#29609;&#23478;&#24517;&#39035;&#20570;&#20986;&#19981;&#21487;&#25764;&#28040;&#30340;&#20915;&#31574;&#65292;&#20915;&#23450;&#36716;&#25442;&#30340;&#36164;&#20135;&#25968;&#37327;&#12290;&#24403;&#29609;&#23478;&#36830;&#32493;&#26102;&#38388;&#27493;&#39588;&#20013;&#25913;&#21464;&#20915;&#31574;&#26102;&#65292;&#20063;&#20250;&#20135;&#29983;&#20999;&#25442;&#25104;&#26412;&#65292;&#21363;&#22312;&#36141;&#20080;&#37327;&#22686;&#21152;&#25110;&#20943;&#23569;&#26102;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#23567;&#21270;&#21644;&#26368;&#22823;&#21270;&#21464;&#20307;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#65288;&#31283;&#20581;&#65289;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#23427;&#20204;&#26159;&#30830;&#23450;&#24615;&#22312;&#32447;&#31639;&#27861;&#20013;&#30340;&#26368;&#20248;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#21033;&#29992;&#19981;&#21487;&#20449;&#30340;&#40657;&#30418;&#24314;&#35758;&#65288;&#20363;&#22914;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#65289;&#26469;&#26174;&#33879;&#25913;&#21892;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and study online conversion with switching costs, a family of online problems that capture emerging problems at the intersection of energy and sustainability. In this problem, an online player attempts to purchase (alternatively, sell) fractional shares of an asset during a fixed time horizon with length $T$. At each time step, a cost function (alternatively, price function) is revealed, and the player must irrevocably decide an amount of asset to convert. The player also incurs a switching cost whenever their decision changes in consecutive time steps, i.e., when they increase or decrease their purchasing amount. We introduce competitive (robust) threshold-based algorithms for both the minimization and maximization variants of this problem, and show they are optimal among deterministic online algorithms. We then propose learning-augmented algorithms that take advantage of untrusted black-box advice (such as predictions from a machine learning model) to achieve significant
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;PiNMT&#27169;&#22411;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#37096;&#20998;&#21644;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;IW&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19680</link><description>&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;
&lt;/p&gt;
&lt;p&gt;
Integrating Pre-trained Language Model into Neural Machine Translation. (arXiv:2310.19680v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19680
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;PiNMT&#27169;&#22411;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#37096;&#20998;&#21644;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;IW&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#39640;&#36136;&#37327;&#30340;&#21452;&#35821;&#35821;&#35328;&#23545;&#25968;&#25454;&#30340;&#19981;&#36275;&#20173;&#28982;&#26159;&#25552;&#39640;NMT&#24615;&#33021;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19968;&#30452;&#22312;&#25506;&#32034;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;PLM&#21644;NMT&#27169;&#22411;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PLM&#25972;&#21512;&#30340;NMT&#65288;PiNMT&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;PiNMT&#27169;&#22411;&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65292;&#20998;&#21035;&#26159;PLM&#22810;&#23618;&#36716;&#25442;&#22120;&#65292;&#23884;&#20837;&#34701;&#21512;&#21644;&#20313;&#24358;&#23545;&#40784;&#65292;&#27599;&#20010;&#37096;&#20998;&#22312;&#21521;NMT&#25552;&#20379;&#26377;&#25928;&#30340;PLM&#20449;&#24687;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;&#20998;&#31163;&#23398;&#20064;&#29575;&#21644;&#21452;&#27493;&#35757;&#32451;&#12290;&#36890;&#36807;&#23454;&#26045;&#25152;&#25552;&#20986;&#30340;PiNMT&#27169;&#22411;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;IW&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation (NMT) has become a significant technology in natural language processing through extensive research and development. However, the deficiency of high-quality bilingual language pair data still poses a major challenge to improving NMT performance. Recent studies have been exploring the use of contextual information from pre-trained language model (PLM) to address this problem. Yet, the issue of incompatibility between PLM and NMT model remains unresolved. This study proposes PLM-integrated NMT (PiNMT) model to overcome the identified problems. PiNMT model consists of three critical components, PLM Multi Layer Converter, Embedding Fusion, and Cosine Alignment, each playing a vital role in providing effective PLM information to NMT. Furthermore, two training strategies, Separate Learning Rates and Dual Step Training, are also introduced in this paper. By implementing the proposed PiNMT model and training strategy, we achieve state-of-the-art performance on the IW
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#35760;&#24518;&#25200;&#21160;&#26041;&#31243;&#65288;MPE&#65289;&#65292;&#35813;&#26041;&#31243;&#36890;&#36807;&#24212;&#29992;&#36125;&#21494;&#26031;&#21407;&#29702;&#23558;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#25200;&#21160;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#27169;&#22411;&#22312;&#26410;&#35265;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.19273</link><description>&lt;p&gt;
The Memory Perturbation Equation: Understanding Model's Sensitivity to Data&#65288;&#29702;&#35299;&#27169;&#22411;&#23545;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#30340;&#35760;&#24518;&#25200;&#21160;&#26041;&#31243;&#65289;
&lt;/p&gt;
&lt;p&gt;
The Memory Perturbation Equation: Understanding Model's Sensitivity to Data. (arXiv:2310.19273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19273
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#35760;&#24518;&#25200;&#21160;&#26041;&#31243;&#65288;MPE&#65289;&#65292;&#35813;&#26041;&#31243;&#36890;&#36807;&#24212;&#29992;&#36125;&#21494;&#26031;&#21407;&#29702;&#23558;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#25200;&#21160;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#27169;&#22411;&#22312;&#26410;&#35265;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#27169;&#22411;&#23545;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#23545;&#20110;&#35757;&#32451;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20063;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35760;&#24518;&#25200;&#21160;&#26041;&#31243;&#65288;MPE&#65289;&#65292;&#23427;&#23558;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#19982;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#25200;&#21160;&#32852;&#31995;&#36215;&#26469;&#12290;&#20351;&#29992;&#36125;&#21494;&#26031;&#21407;&#29702;&#23548;&#20986;&#30340;MPE&#23558;&#29616;&#26377;&#30340;&#25935;&#24863;&#24615;&#24230;&#37327;&#32479;&#19968;&#36215;&#26469;&#65292;&#27867;&#21270;&#21040;&#21508;&#31181;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#26377;&#20851;&#25935;&#24863;&#24615;&#30340;&#26377;&#29992;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#25935;&#24863;&#24615;&#20272;&#35745;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#22312;&#26410;&#35265;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#25552;&#20986;&#30340;&#26041;&#31243;&#39044;&#35745;&#23558;&#23545;&#26410;&#26469;&#30340;&#40065;&#26834;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#30740;&#31350;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding model's sensitivity to its training data is crucial but can also be challenging and costly, especially during training. To simplify such issues, we present the Memory-Perturbation Equation (MPE) which relates model's sensitivity to perturbation in its training data. Derived using Bayesian principles, the MPE unifies existing sensitivity measures, generalizes them to a wide-variety of models and algorithms, and unravels useful properties regarding sensitivities. Our empirical results show that sensitivity estimates obtained during training can be used to faithfully predict generalization on unseen test data. The proposed equation is expected to be useful for future research on robust and adaptive learning.
&lt;/p&gt;</description></item><item><title>Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.</title><link>http://arxiv.org/abs/2310.17072</link><description>&lt;p&gt;
&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17072
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;MMP&#65289;&#20026;&#32473;&#23450;&#20219;&#21153;&#29983;&#25104;&#19968;&#31995;&#21015;&#36830;&#32493;&#36712;&#36857;&#27969;&#24418;&#65292;&#27599;&#19968;&#20010;&#36712;&#36857;&#27969;&#24418;&#37117;&#33021;&#25104;&#21151;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#30001;&#23545;&#27969;&#24418;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#35299;&#30721;&#20989;&#25968;&#20197;&#21450;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#23494;&#24230;&#32452;&#25104;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#30001;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20960;&#20309;&#25197;&#26354;&#65292;MMP&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;--&#36890;&#36807;&#21464;&#24418;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#30456;&#20284;&#30340;&#36816;&#21160;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#27861;&#30456;&#37051;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;IMMP&#65289;&#65292;&#20854;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20445;&#25345;&#20102;&#27969;&#24418;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#21644;&#20351;&#29992;&#20102;&#19968;&#20010;Riemannian&#24230;&#37327;&#65292;&#29992;&#20110;&#36816;&#21160;&#31354;&#38388;&#65288;&#21363;&#65292;&#21442;&#25968;&#21270;&#26354;&#32447;&#31354;&#38388;&#65289;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;CurveGeom Riemannian&#24230;&#37327;&#12290;&#23545;&#20110;&#24179;&#38754;&#38556;&#30861;&#36991;&#35753;&#36816;&#21160;&#21644;&#25512;&#21160;&#25805;&#32437;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IMMP&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;MMP&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Gabe-YHLee/IMMP&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Motion Manifold Primitive (MMP) produces, for a given task, a continuous manifold of trajectories each of which can successfully complete the task. It consists of the decoder function that parametrizes the manifold and the probability density in the latent coordinate space. In this paper, we first show that the MMP performance can significantly degrade due to the geometric distortion in the latent space -- by distortion, we mean that similar motions are not located nearby in the latent space. We then propose {\it Isometric Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the geometry of the manifold. For this purpose, we formulate and use a Riemannian metric for the motion space (i.e., parametric curve space), which we call a {\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding motions and pushing manipulation tasks show that IMMP significantly outperforms existing MMP methods. Code is available at https://github.com/Gabe-YHLee/IMMP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#23545;&#19978;&#34892;&#21644;&#19979;&#34892;&#36890;&#20449;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#40065;&#26834;&#24615;&#21462;&#20915;&#20110;&#23458;&#25143;&#31471;&#25968;&#37327;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#20540;&#33539;&#22260;&#12290;&#23454;&#39564;&#35777;&#26126;&#19978;&#34892;&#36890;&#20449;&#21487;&#20197;&#23481;&#24525;&#26356;&#39640;&#30340;&#35823;&#30721;&#29575;&#27604;&#19979;&#34892;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2310.16652</link><description>&lt;p&gt;
Federated Learning&#22312;&#36890;&#20449;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#26377;&#22810;&#24378;&#65311;&#26469;&#33258;&#19978;&#34892;&#21644;&#19979;&#34892;&#36890;&#36947;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Robust is Federated Learning to Communication Error? A Comparison Study Between Uplink and Downlink Channels. (arXiv:2310.16652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#23545;&#19978;&#34892;&#21644;&#19979;&#34892;&#36890;&#20449;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#40065;&#26834;&#24615;&#21462;&#20915;&#20110;&#23458;&#25143;&#31471;&#25968;&#37327;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#20540;&#33539;&#22260;&#12290;&#23454;&#39564;&#35777;&#26126;&#19978;&#34892;&#36890;&#20449;&#21487;&#20197;&#23481;&#24525;&#26356;&#39640;&#30340;&#35823;&#30721;&#29575;&#27604;&#19979;&#34892;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#33021;&#21147;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#23454;&#29616;&#26102;&#65292;FL&#33021;&#22815;&#23481;&#24525;&#22810;&#23569;&#36890;&#20449;&#38169;&#35823;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;FL&#23545;&#19978;&#34892;&#21644;&#19979;&#34892;&#36890;&#20449;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#40065;&#26834;&#24615;&#21462;&#20915;&#20110;&#20004;&#20010;&#20851;&#38190;&#21442;&#25968;&#65292;&#21363;&#23458;&#25143;&#31471;&#25968;&#37327;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#20540;&#33539;&#22260;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FL&#30340;&#19978;&#34892;&#36890;&#20449;&#21487;&#20197;&#23481;&#24525;&#26356;&#39640;&#30340;&#35823;&#30721;&#29575;&#65288;BER&#65289;&#27604;&#19979;&#34892;&#36890;&#20449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#37327;&#30340;&#24046;&#24322;&#20844;&#24335;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#20123;&#21457;&#29616;&#21644;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because of its privacy-preserving capability, federated learning (FL) has attracted significant attention from both academia and industry. However, when being implemented over wireless networks, it is not clear how much communication error can be tolerated by FL. This paper investigates the robustness of FL to the uplink and downlink communication error. Our theoretical analysis reveals that the robustness depends on two critical parameters, namely the number of clients and the numerical range of model parameters. It is also shown that the uplink communication in FL can tolerate a higher bit error rate (BER) than downlink communication, and this difference is quantified by a proposed formula. The findings and theoretical analyses are further validated by extensive experiments.
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16221</link><description>&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16221
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26159;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#30001;&#21487;&#20998;&#35299;&#20026;&#22810;&#20010;&#23454;&#20307;&#30340;&#23545;&#35937;&#32452;&#25104;&#65288;&#20363;&#22914;&#65292;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#20687;&#32032;&#65292;&#23558;&#22270;&#24418;&#20998;&#35299;&#20026;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#65289;&#12290;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#20854;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#19978;&#20855;&#26377;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;-&#36890;&#36807;&#22312;&#20998;&#31867;&#20043;&#21069;&#38543;&#26426;&#28155;&#21152;&#22122;&#22768;&#26469;&#20445;&#35777;&#22810;&#25968;&#25237;&#31080;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23545;&#25163;&#19981;&#26159;&#20219;&#24847;&#24178;&#25200;&#25972;&#20010;&#23545;&#35937;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#32780;&#26159;&#23545;&#35937;&#30340;&#26576;&#20010;&#23454;&#20307;&#30340;&#23376;&#38598;&#65288;&#20363;&#22914;&#20687;&#32032;&#65289;&#26102;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#23545;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#65306;&#25105;&#20204;&#36890;&#36807;&#20165;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20307;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#26469;&#37096;&#20998;&#24179;&#28369;&#23545;&#35937;&#12290;&#36890;&#36807;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#28155;&#21152;&#22122;&#22768;&#65292;&#25105;&#20204;&#33719;&#24471;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#22122;&#22768;&#20998;&#24067;&#21021;&#22987;&#21270;&#20998;&#23618;&#24179;&#28369;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#24605;&#32771;&#20998;&#23376;&#25513;&#30721;&#22270;&#27169;&#22411;&#20013;&#30340;&#20998;&#35789;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#23545;&#20998;&#23376;&#20998;&#35789;&#22120;&#30340;&#24635;&#32467;&#21644;&#35299;&#30721;&#22120;&#30340;&#25506;&#32034;&#65292;&#22635;&#34917;&#20102;&#20998;&#35789;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.14753</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20998;&#23376;&#25513;&#30721;&#22270;&#27169;&#22411;&#20013;&#30340;&#20998;&#35789;&#22120;&#21644;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Rethinking Tokenizer and Decoder in Masked Graph Modeling for Molecules. (arXiv:2310.14753v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14753
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20998;&#23376;&#25513;&#30721;&#22270;&#27169;&#22411;&#20013;&#30340;&#20998;&#35789;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#23545;&#20998;&#23376;&#20998;&#35789;&#22120;&#30340;&#24635;&#32467;&#21644;&#35299;&#30721;&#22120;&#30340;&#25506;&#32034;&#65292;&#22635;&#34917;&#20102;&#20998;&#35789;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#22270;&#27169;&#22411;&#22312;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#20247;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20998;&#23376;&#22270;&#12290;&#36890;&#36807;&#23545;&#20043;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#20180;&#32454;&#23457;&#26597;&#65292;&#25105;&#20204;&#21487;&#20197;&#25581;&#31034;&#19968;&#20010;&#24120;&#35265;&#30340;&#26041;&#26696;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#22270;&#20998;&#35789;&#22120;&#65292;&#23427;&#23558;&#20998;&#23376;&#22270;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#29255;&#27573;&#65288;&#21363;&#23376;&#22270;&#65289;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#26631;&#35760;&#65307;&#65288;2&#65289;&#22270;&#25513;&#30721;&#65292;&#29992;&#25513;&#30721;&#30772;&#22351;&#22270;&#65307;&#65288;3&#65289;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#39318;&#20808;&#22312;&#25513;&#30721;&#22270;&#19978;&#24212;&#29992;&#32534;&#30721;&#22120;&#29983;&#25104;&#34920;&#31034;&#65292;&#28982;&#21518;&#22312;&#34920;&#31034;&#19978;&#20351;&#29992;&#35299;&#30721;&#22120;&#24674;&#22797;&#21407;&#22987;&#22270;&#30340;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;MGM&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22270;&#25513;&#30721;&#21644;&#32534;&#30721;&#22120;&#65292;&#23545;&#20110;&#20998;&#35789;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#29702;&#35299;&#36739;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#39318;&#20808;&#24635;&#32467;&#20102;&#33410;&#28857;&#12289;&#36793;&#12289;&#20027;&#39064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24120;&#29992;&#20998;&#23376;&#20998;&#35789;&#22120;&#65292;&#28982;&#21518;&#32771;&#23519;&#23427;&#20204;&#20316;&#20026;MGM&#37325;&#26500;&#30446;&#26631;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22312;MGM&#20013;&#37319;&#29992;&#39640;&#34920;&#36798;&#35299;&#30721;&#22120;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked graph modeling excels in the self-supervised representation learning of molecular graphs. Scrutinizing previous studies, we can reveal a common scheme consisting of three key components: (1) graph tokenizer, which breaks a molecular graph into smaller fragments (i.e., subgraphs) and converts them into tokens; (2) graph masking, which corrupts the graph with masks; (3) graph autoencoder, which first applies an encoder on the masked graph to generate the representations, and then employs a decoder on the representations to recover the tokens of the original graph. However, the previous MGM studies focus extensively on graph masking and encoder, while there is limited understanding of tokenizer and decoder. To bridge the gap, we first summarize popular molecule tokenizers at the granularity of node, edge, motif, and Graph Neural Networks (GNNs), and then examine their roles as the MGM's reconstruction targets. Further, we explore the potential of adopting an expressive decoder in M
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#33258;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#30446;&#21069;&#30340;&#27169;&#22411;&#22312;&#33258;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.14053</link><description>&lt;p&gt;
&#36229;&#36234;&#20934;&#30830;&#24615;&#65306;&#29992;IdentityChain&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain. (arXiv:2310.14053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#33258;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#30446;&#21069;&#30340;&#27169;&#22411;&#22312;&#33258;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;(Code LLMs)&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#22240;&#27492;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#20934;&#30830;&#24615;&#35780;&#20272;&#26041;&#27861;&#35780;&#20272;Code LLMs&#22312;&#19968;&#31995;&#21015;&#29420;&#31435;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20294;&#24573;&#35270;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#33258;&#19968;&#33268;&#24615;&#12290;&#30452;&#35266;&#26469;&#35762;&#65292;&#19968;&#20010;&#21487;&#20449;&#36182;&#30340;&#27169;&#22411;&#22312;&#20026;&#20854;&#33258;&#36523;&#30340;&#20195;&#30721;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20197;&#21450;&#20026;&#20854;&#33258;&#36523;&#30340;&#35268;&#33539;&#29983;&#25104;&#20195;&#30721;&#26102;&#24212;&#35813;&#26159;&#33258;&#19968;&#33268;&#30340;&#12290;&#26410;&#33021;&#20445;&#25345;&#33258;&#19968;&#33268;&#24615;&#25581;&#31034;&#20102;&#23545;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20849;&#20139;&#35821;&#20041;&#30340;&#29702;&#35299;&#30340;&#19981;&#36275;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#39318;&#20808;&#27491;&#24335;&#23450;&#20041;&#20102;Code LLMs&#30340;&#33258;&#19968;&#33268;&#24615;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;IdentityChain&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#33258;&#19968;&#33268;&#24615;&#21644;&#20256;&#32479;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;11&#20010;Code LLMs&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#26410;&#33021;&#20445;&#25345;&#33258;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indee
&lt;/p&gt;</description></item><item><title>DeepFDR&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#35299;&#20915;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#30340;&#22810;&#37325;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13349</link><description>&lt;p&gt;
DeepFDR&#65306;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepFDR: A Deep Learning-based False Discovery Rate Control Method for Neuroimaging Data. (arXiv:2310.13349v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13349
&lt;/p&gt;
&lt;p&gt;
DeepFDR&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#35299;&#20915;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#30340;&#22810;&#37325;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20307;&#32032;&#30340;&#22810;&#37325;&#26816;&#39564;&#22312;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20998;&#26512;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#20256;&#32479;&#30340;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#22522;&#20110;&#20307;&#32032;&#30340;&#26816;&#39564;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#27979;&#35797;&#33021;&#21147;&#30340;&#22823;&#24133;&#25439;&#22833;&#12290;&#34429;&#28982;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#20123;&#31354;&#38388;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#65292;&#20294;&#26159;&#24403;&#22788;&#29702;&#22797;&#26434;&#30340;&#33041;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#26102;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#26368;&#20248;&#24615;&#20173;&#23384;&#22312;&#30097;&#38382;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#22312;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#32780;&#22270;&#20687;&#20998;&#21106;&#19982;&#22522;&#20110;&#20307;&#32032;&#30340;&#22810;&#37325;&#26816;&#39564;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepFDR&#30340;&#26032;&#22411;&#31354;&#38388;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#21106;&#26469;&#35299;&#20915;&#22522;&#20110;&#20307;&#32032;&#30340;&#22810;&#37325;&#26816;&#39564;&#38382;&#39064;&#12290;&#21253;&#25324;&#20840;&#38754;&#30340;&#27169;&#25311;&#21644;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;FDG-PET&#24433;&#20687;&#20998;&#26512;&#22312;&#20869;&#30340;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;DeepFDR&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;DeepFDR&#19981;&#20165;&#22312;&#34394;&#35686;&#25511;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36824;&#26377;&#25928;&#38477;&#20302;&#20102;&#34394;&#20551;&#30340;&#38750;&#21457;&#29616;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voxel-based multiple testing is widely used in neuroimaging data analysis. Traditional false discovery rate (FDR) control methods often ignore the spatial dependence among the voxel-based tests and thus suffer from substantial loss of testing power. While recent spatial FDR control methods have emerged, their validity and optimality remain questionable when handling the complex spatial dependencies of the brain. Concurrently, deep learning methods have revolutionized image segmentation, a task closely related to voxel-based multiple testing. In this paper, we propose DeepFDR, a novel spatial FDR control method that leverages unsupervised deep learning-based image segmentation to address the voxel-based multiple testing problem. Numerical studies, including comprehensive simulations and Alzheimer's disease FDG-PET image analysis, demonstrate DeepFDR's superiority over existing methods. DeepFDR not only excels in FDR control and effectively diminishes the false nondiscovery rate, but als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#21442;&#25968;&#21270;&#30340;&#20998;&#24067;&#32422;&#26463;&#26435;&#37325;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#21644;&#26368;&#20248;&#20256;&#36755;&#24037;&#20855;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#35266;&#27979;&#25968;&#25454;&#30340;&#26368;&#20248;&#26435;&#37325;&#35843;&#25972;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12447</link><description>&lt;p&gt;
&#32422;&#26463;&#37325;&#21152;&#26435;&#20998;&#24067;&#65306;&#19968;&#31181;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Constrained Reweighting of Distributions: an Optimal Transport Approach. (arXiv:2310.12447v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#21442;&#25968;&#21270;&#30340;&#20998;&#24067;&#32422;&#26463;&#26435;&#37325;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#21644;&#26368;&#20248;&#20256;&#36755;&#24037;&#20855;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#35266;&#27979;&#25968;&#25454;&#30340;&#26368;&#20248;&#26435;&#37325;&#35843;&#25972;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32463;&#24120;&#36935;&#21040;&#30340;&#38382;&#39064;&#26159;&#35201;&#35782;&#21035;&#20986;&#31526;&#21512;&#39044;&#23450;&#20041;&#30340;&#26435;&#37325;&#32422;&#26463;&#26465;&#20214;&#30340;&#35266;&#27979;&#25968;&#25454;&#30340;&#32463;&#39564;&#20998;&#24067;&#30340;&#26368;&#20248;&#35843;&#25972;&#29256;&#26412;&#12290;&#36825;&#20123;&#32422;&#26463;&#36890;&#24120;&#34920;&#29616;&#20026;&#23545;&#26435;&#37325;&#30340;&#30697;&#12289;&#23614;&#37096;&#34892;&#20026;&#12289;&#24418;&#29366;&#12289;&#27169;&#24335;&#25968;&#37327;&#31561;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#30340;&#20998;&#24067;&#32422;&#26463;&#26435;&#37325;&#24182;&#21033;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#21644;&#26368;&#20248;&#20256;&#36755;&#24037;&#20855;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#30830;&#20445;&#35266;&#27979;&#25968;&#25454;&#30340;&#26368;&#22823;&#29109;&#26435;&#37325;&#35843;&#25972;&#32463;&#39564;&#20998;&#24067;&#19982;&#39044;&#23450;&#30340;&#27010;&#29575;&#20998;&#24067;&#22312;&#26368;&#20248;&#20256;&#36755;&#24230;&#37327;&#19979;&#25509;&#36817;&#65292;&#24182;&#20801;&#35768;&#32454;&#24494;&#30340;&#20559;&#24046;&#12290;&#35813;&#26694;&#26550;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#20854;&#20013;&#25968;&#25454;&#37325;&#21152;&#26435;&#26159;&#21512;&#29702;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We commonly encounter the problem of identifying an optimally weight adjusted version of the empirical distribution of observed data, adhering to predefined constraints on the weights. Such constraints often manifest as restrictions on the moments, tail behaviour, shapes, number of modes, etc., of the resulting weight adjusted empirical distribution. In this article, we substantially enhance the flexibility of such methodology by introducing a nonparametrically imbued distributional constraints on the weights, and developing a general framework leveraging the maximum entropy principle and tools from optimal transport. The key idea is to ensure that the maximum entropy weight adjusted empirical distribution of the observed data is close to a pre-specified probability distribution in terms of the optimal transport metric while allowing for subtle departures. The versatility of the framework is demonstrated in the context of three disparate applications where data re-weighting is warrante
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;MDP&#20013;&#23398;&#20064;&#969;-regular&#30446;&#26631;&#65292;&#19981;&#38656;&#35201;&#31995;&#32479;&#25299;&#25169;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.12248</link><description>&lt;p&gt;
MDP&#20013;LTL&#21644;&#969;-regular&#30446;&#26631;&#30340;PAC&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs. (arXiv:2310.12248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;MDP&#20013;&#23398;&#20064;&#969;-regular&#30446;&#26631;&#65292;&#19981;&#38656;&#35201;&#31995;&#32479;&#25299;&#25169;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#24207;&#36923;&#36753;&#65288;LTL&#65289;&#21644;&#969;-regular&#30446;&#26631;&#26159;&#36817;&#26399;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#36798;&#38750;&#39532;&#23572;&#21487;&#22827;&#30446;&#26631;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;MDP&#20013;&#30340;&#969;-regular&#30446;&#26631;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20174;&#31995;&#32479;&#30340;&#37319;&#26679;&#36712;&#36857;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#31995;&#32479;&#25299;&#25169;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear temporal logic (LTL) and omega-regular objectives -- a superset of LTL -- have seen recent use as a way to express non-Markovian objectives in reinforcement learning. We introduce a model-based probably approximately correct (PAC) learning algorithm for omega-regular objectives in Markov decision processes. Unlike prior approaches, our algorithm learns from sampled trajectories of the system and does not require prior knowledge of the system's topology.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#36830;&#32493;&#23398;&#20064;&#37325;&#22609;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37319;&#29992;&#20803;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#38656;&#35201;&#23545;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#36830;&#32493;&#23398;&#20064;episode&#19978;&#30340;&#20803;&#32423;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#36890;&#29992;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.11952</link><description>&lt;p&gt;
&#23558;&#36830;&#32493;&#23398;&#20064;&#37325;&#22609;&#20026;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Recasting Continual Learning as Sequence Modeling. (arXiv:2310.11952v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#36830;&#32493;&#23398;&#20064;&#37325;&#22609;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37319;&#29992;&#20803;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#38656;&#35201;&#23545;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#36830;&#32493;&#23398;&#20064;episode&#19978;&#30340;&#20803;&#32423;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#36890;&#29992;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#20004;&#20010;&#37325;&#35201;&#39046;&#22495;&#20043;&#38388;&#30340;&#24378;&#36830;&#25509;&#65306;&#36830;&#32493;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36830;&#32493;&#23398;&#20064;&#20316;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#36827;&#34892;&#34920;&#36848;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#20808;&#36827;&#30340;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#25104;&#20026;&#24207;&#21015;&#27169;&#22411;&#30340;&#21069;&#21521;&#20256;&#25773;&#12290;&#36890;&#36807;&#37319;&#29992;&#20803;&#36830;&#32493;&#23398;&#20064;(MCL)&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#22810;&#20010;&#36830;&#32493;&#23398;&#20064;episode&#19978;&#23545;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#20803;&#32423;&#35757;&#32451;&#12290;&#20316;&#20026;&#25105;&#20204;&#26032;&#26694;&#26550;&#30340;&#19968;&#20010;&#20855;&#20307;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;Transformer&#21450;&#20854;&#39640;&#25928;&#21464;&#20307;&#24212;&#29992;&#20110;MCL&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#19971;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#36890;&#29992;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning. Under this formulation, the continual learning process becomes the forward pass of a sequence model. By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes. As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods. Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#19982;&#25191;&#34892;&#22120;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#25512;&#29702;&#12290;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#24212;&#29992;&#20869;&#23884;&#30340;&#19990;&#30028;&#30693;&#35782;&#35299;&#37322;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#36827;&#34892;&#29289;&#29702;&#39046;&#22495;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.09605</link><description>&lt;p&gt;
&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#20351;LLMs&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Penetrative AI: Making LLMs Comprehend the Physical World. (arXiv:2310.09605v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#19982;&#25191;&#34892;&#22120;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#25512;&#29702;&#12290;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#24212;&#29992;&#20869;&#23884;&#30340;&#19990;&#30028;&#30693;&#35782;&#35299;&#37322;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#36827;&#34892;&#29289;&#29702;&#39046;&#22495;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#30340;&#24615;&#36136;&#20197;&#21450;&#23427;&#20204;&#22312;&#28041;&#21450;&#30495;&#23454;&#29289;&#29702;&#19990;&#30028;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#25972;&#21512;&#24120;&#35782;&#20154;&#31867;&#30693;&#35782;&#30340;&#28508;&#21147;&#20173;&#23384;&#22312;&#30097;&#38382;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;LLMs&#22914;&#20309;&#36890;&#36807;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#25512;&#29702;&#26469;&#25506;&#35752;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#19968;&#27010;&#24565;&#31216;&#20026;&#8220;&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#8221;&#12290;&#35770;&#25991;&#22312;LLMs&#33021;&#22815;&#36879;&#36807;&#22788;&#29702;&#24863;&#30693;&#20449;&#21495;&#30340;&#20004;&#20010;&#23618;&#38754;&#19978;&#25506;&#32034;&#20102;&#36825;&#31181;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#65288;ChatGPT&#26159;&#25105;&#20204;&#30740;&#31350;&#20013;&#30340;&#20195;&#34920;&#24615;&#20363;&#23376;&#65289;&#22312;&#24212;&#29992;&#20869;&#23884;&#30340;&#19990;&#30028;&#30693;&#35782;&#35299;&#37322;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#23545;&#29289;&#29702;&#39046;&#22495;&#30340;&#20219;&#21153;&#36827;&#34892;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#30456;&#24403;&#29420;&#29305;&#30340;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#20026;LLMs&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks. Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world. This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term "Penetrative AI". The paper explores such an extension at two levels of LLMs' ability to penetrate into the physical world via the processing of sensory signals. Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the embedded world knowledge for interpreting IoT sensor data and reasoning over them about tasks in the physical realm. Not only this opens up new applications for LLMs 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#35745;&#25968;&#20219;&#21153;&#20013;&#30340;&#31639;&#27861;&#25512;&#24191;&#65292;&#24182;&#35777;&#26126;&#20102;&#26631;&#20934;&#30340;Transformer&#30340;&#26550;&#26500;&#20915;&#31574;&#20250;&#38459;&#30861;&#20854;&#22312;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#28040;&#38500;&#38382;&#39064;&#25805;&#20316;&#65292;&#20462;&#25913;&#21518;&#30340;Transformer&#22312;&#35745;&#25968;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#31639;&#27861;&#25512;&#24191;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08661</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#36827;&#34892;&#35745;&#25968;&#21644;&#31639;&#27861;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
Counting and Algorithmic Generalization with Transformers. (arXiv:2310.08661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08661
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#35745;&#25968;&#20219;&#21153;&#20013;&#30340;&#31639;&#27861;&#25512;&#24191;&#65292;&#24182;&#35777;&#26126;&#20102;&#26631;&#20934;&#30340;Transformer&#30340;&#26550;&#26500;&#20915;&#31574;&#20250;&#38459;&#30861;&#20854;&#22312;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#28040;&#38500;&#38382;&#39064;&#25805;&#20316;&#65292;&#20462;&#25913;&#21518;&#30340;Transformer&#22312;&#35745;&#25968;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#31639;&#27861;&#25512;&#24191;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31639;&#27861;&#25512;&#24191;&#26159;&#25351;&#23398;&#20064;&#29983;&#25104;&#25968;&#25454;&#30340;&#24213;&#23618;&#31639;&#27861;&#65292;&#20197;&#19968;&#31181;&#23545;&#36229;&#20986;&#20998;&#24067;&#30340;&#26041;&#24335;&#36827;&#34892;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#36825;&#23545;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35828;&#36890;&#24120;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#35745;&#25968;&#26102;&#38656;&#35201;&#30340;&#31639;&#27861;&#25512;&#24191;&#65292;&#26080;&#35770;&#26159;&#38544;&#24335;&#36824;&#26159;&#26174;&#24335;&#12290;&#25105;&#20204;&#34920;&#26126;&#26631;&#20934;&#30340;Transformer&#26159;&#22522;&#20110;&#38459;&#30861;&#36825;&#31867;&#20219;&#21153;&#30340;&#26550;&#26500;&#20915;&#31574;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;&#23618;&#24402;&#19968;&#21270;&#21644;&#36890;&#36807;softmax&#24402;&#19968;&#21270;&#27880;&#24847;&#21147;&#26435;&#37325;&#30340;&#21518;&#26524;&#12290;&#36890;&#36807;&#28040;&#38500;&#36825;&#20123;&#38382;&#39064;&#25805;&#20316;&#65292;&#25105;&#20204;&#35777;&#26126;&#20462;&#25913;&#21518;&#30340;Transformer&#21487;&#20197;&#22312;&#35745;&#25968;&#26041;&#38754;&#23637;&#31034;&#20986;&#33391;&#22909;&#30340;&#31639;&#27861;&#25512;&#24191;&#24615;&#33021;&#65292;&#21516;&#26102;&#37319;&#29992;&#38750;&#24120;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic generalization in machine learning refers to the ability to learn the underlying algorithm that generates data in a way that generalizes out-of-distribution. This is generally considered a difficult task for most machine learning algorithms. Here, we analyze algorithmic generalization when counting is required, either implicitly or explicitly. We show that standard Transformers are based on architectural decisions that hinder out-of-distribution performance for such tasks. In particular, we discuss the consequences of using layer normalization and of normalizing the attention weights via softmax. With ablation of the problematic operations, we demonstrate that a modified transformer can exhibit a good algorithmic generalization performance on counting while using a very lightweight architecture.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24378;&#22823;&#36890;&#29992;&#24615;&#30340;&#36731;&#32534;&#30721;&#22120;&#21644;&#37325;&#35299;&#30721;&#22120;&#65288;LEHD&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#33410;&#28857;&#38388;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#22312;&#22823;&#35268;&#27169;&#23454;&#20363;&#19978;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#26053;&#34892;&#21830;&#38382;&#39064;&#21644;&#23481;&#37327;&#38480;&#21046;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07985</link><description>&lt;p&gt;
&#20855;&#26377;&#37325;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#65306;&#26397;&#30528;&#22823;&#35268;&#27169;&#36890;&#29992;&#21270;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization. (arXiv:2310.07985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07985
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24378;&#22823;&#36890;&#29992;&#24615;&#30340;&#36731;&#32534;&#30721;&#22120;&#21644;&#37325;&#35299;&#30721;&#22120;&#65288;LEHD&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#33410;&#28857;&#38388;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#22312;&#22823;&#35268;&#27169;&#23454;&#20363;&#19978;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#26053;&#34892;&#21830;&#38382;&#39064;&#21644;&#23481;&#37327;&#38480;&#21046;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#65288;NCO&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20256;&#32479;&#30340;&#19987;&#23478;&#31639;&#27861;&#35774;&#35745;&#26469;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26500;&#36896;&#24615;&#30340;NCO&#26041;&#27861;&#19981;&#33021;&#35299;&#20915;&#22823;&#35268;&#27169;&#23454;&#20363;&#22823;&#23567;&#30340;&#38382;&#39064;&#65292;&#36825;&#26174;&#33879;&#38477;&#20302;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#32534;&#30721;&#22120;&#21644;&#37325;&#35299;&#30721;&#22120;&#65288;LEHD&#65289;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#36890;&#29992;&#24615;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;LEHD&#27169;&#22411;&#21487;&#20197;&#21160;&#24577;&#22320;&#23398;&#20064;&#21040;&#19981;&#21516;&#23610;&#23544;&#30340;&#25152;&#26377;&#21487;&#29992;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36825;&#26377;&#21033;&#20110;&#27169;&#22411;&#23545;&#21508;&#31181;&#35268;&#27169;&#30340;&#38382;&#39064;&#36827;&#34892;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;LEHD&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#26696;&#21644;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#26426;&#21046;&#12290;&#36890;&#36807;&#22312;&#23567;&#35268;&#27169;&#38382;&#39064;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;LEHD&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36817;&#20046;&#26368;&#20248;&#30340;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#21644;&#23481;&#37327;&#38480;&#21046;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Neural combinatorial optimization (NCO) is a promising learning-based approach for solving challenging combinatorial optimization problems without specialized algorithm design by experts. However, most constructive NCO methods cannot solve problems with large-scale instance sizes, which significantly diminishes their usefulness for real-world applications. In this work, we propose a novel Light Encoder and Heavy Decoder (LEHD) model with a strong generalization ability to address this critical issue. The LEHD model can learn to dynamically capture the relationships between all available nodes of varying sizes, which is beneficial for model generalization to problems of various scales. Moreover, we develop a data-efficient training scheme and a flexible solution construction mechanism for the proposed LEHD model. By training on small-scale problem instances, the LEHD model can generate nearly optimal solutions for the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CausalVul&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24615;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#25200;&#21160;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#27169;&#22411;&#19981;&#31283;&#23450;&#21644;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07958</link><description>&lt;p&gt;
&#36808;&#21521;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Causal Deep Learning for Vulnerability Detection. (arXiv:2310.07958v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CausalVul&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24615;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#25200;&#21160;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#27169;&#22411;&#19981;&#31283;&#23450;&#21644;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#28431;&#27934;&#26816;&#27979;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38459;&#30861;&#20854;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#26377;&#29992;&#30340;&#37325;&#35201;&#25361;&#25112;&#26159;&#27169;&#22411;&#22312;&#25200;&#21160;&#19979;&#19981;&#31283;&#23450;&#65292;&#24182;&#19988;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#24212;&#29992;&#21040;&#26410;&#35265;&#36807;&#30340;&#39033;&#30446;&#19978;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#22240;&#20026;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#38750;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#21464;&#37327;&#21517;&#65292;&#19982;&#26631;&#31614;&#20855;&#26377;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#24403;&#25200;&#21160;&#21644;OOD&#25968;&#25454;&#38598;&#19981;&#20877;&#20855;&#26377;&#30456;&#21516;&#30340;&#34394;&#20551;&#29305;&#24449;&#26102;&#65292;&#27169;&#22411;&#39044;&#27979;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#24615;&#24341;&#20837;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;CausalVul&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#25200;&#21160;&#26469;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#29992;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;&#34394;&#20551;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#19978;&#24212;&#29992;&#20102;&#22240;&#26524;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;do-&#35745;&#31639;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to sys
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#32467;&#21512;&#27491;&#21017;&#21270;&#21644;&#29289;&#29702;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33258;&#21160;&#27169;&#22411;&#21457;&#29616;&#65292;&#25506;&#32034;&#20102;&#38750;&#32447;&#24615;&#22238;&#24402;&#20013;&#31232;&#30095;&#22238;&#24402;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#28508;&#21147;&#65292;&#24182;&#38024;&#23545;&#26448;&#26009;&#24314;&#27169;&#25552;&#20986;&#20102;&#24120;&#35265;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.06872</link><description>&lt;p&gt;
&#20851;&#20110;&#31232;&#30095;&#22238;&#24402;&#12289;Lp&#27491;&#21017;&#21270;&#21644;&#33258;&#21160;&#27169;&#22411;&#21457;&#29616;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On sparse regression, Lp-regularization, and automated model discovery. (arXiv:2310.06872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06872
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#32467;&#21512;&#27491;&#21017;&#21270;&#21644;&#29289;&#29702;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33258;&#21160;&#27169;&#22411;&#21457;&#29616;&#65292;&#25506;&#32034;&#20102;&#38750;&#32447;&#24615;&#22238;&#24402;&#20013;&#31232;&#30095;&#22238;&#24402;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#28508;&#21147;&#65292;&#24182;&#38024;&#23545;&#26448;&#26009;&#24314;&#27169;&#25552;&#20986;&#20102;&#24120;&#35265;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#22238;&#24402;&#21644;&#29305;&#24449;&#25552;&#21462;&#26159;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#36827;&#34892;&#30693;&#35782;&#21457;&#29616;&#30340;&#22522;&#30784;&#12290;&#23427;&#20204;&#30340;&#30446;&#26631;&#26159;&#21457;&#29616;&#33021;&#22815;&#25552;&#20379;&#31185;&#23398;&#21464;&#37327;&#20043;&#38388;&#31616;&#21333;&#20851;&#31995;&#30340;&#21487;&#35299;&#37322;&#21644;&#39044;&#27979;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#32447;&#24615;&#22238;&#24402;&#30340;&#27169;&#22411;&#21457;&#29616;&#26041;&#38754;&#30340;&#32479;&#35745;&#24037;&#20855;&#24050;&#32463;&#24471;&#21040;&#24456;&#22909;&#22320;&#24314;&#31435;&#65292;&#20294;&#22312;&#26448;&#26009;&#24314;&#27169;&#20013;&#23558;&#20854;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#22238;&#24402;&#26159;&#39640;&#24230;&#38382;&#39064;&#29305;&#23450;&#19988;&#29702;&#35299;&#19981;&#22815;&#20805;&#20998;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#21160;&#27169;&#22411;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#24341;&#20837;&#31232;&#30095;&#24615;&#65306;&#27491;&#21017;&#21270;&#21644;&#29289;&#29702;&#32422;&#26463;&#12290;&#25105;&#20204;&#23558;Lp&#27491;&#21017;&#21270;&#30340;&#27010;&#24565;&#19982;&#22522;&#20110;&#26412;&#39046;&#22495;&#22312;&#36816;&#21160;&#23398;&#21644;&#28909;&#21147;&#23398;&#26041;&#38754;&#30340;&#30693;&#35782;&#30340;&#26500;&#25104;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#23545;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36827;&#34892;&#20102;&#20960;&#21315;&#27425;&#21457;&#29616;&#36816;&#34892;&#26469;&#25512;&#26029;&#20986;&#20849;&#21516;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#36235;&#21183;&#65306;L2&#27491;&#21017;&#21270;&#25110;&#23725;&#22238;&#24402;&#26159;&#26368;&#20339;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse regression and feature extraction are the cornerstones of knowledge discovery from massive data. Their goal is to discover interpretable and predictive models that provide simple relationships among scientific variables. While the statistical tools for model discovery are well established in the context of linear regression, their generalization to nonlinear regression in material modeling is highly problem-specific and insufficiently understood. Here we explore the potential of neural networks for automatic model discovery and induce sparsity by a hybrid approach that combines two strategies: regularization and physical constraints. We integrate the concept of Lp regularization for subset selection with constitutive neural networks that leverage our domain knowledge in kinematics and thermodynamics. We train our networks with both, synthetic and real data, and perform several thousand discovery runs to infer common guidelines and trends: L2 regularization or ridge regression is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20462;&#25913;&#32479;&#35745;&#24615;&#27495;&#35270;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#30001;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#30340;&#21487;&#21512;&#21516;&#21270;&#20449;&#24565;&#65292;&#32473;&#30417;&#31649;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#36229;&#36807;&#32943;&#23450;&#34892;&#21160;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35201;&#27714;&#20844;&#21496;&#36873;&#21462;&#19968;&#20010;&#24179;&#34913;&#19981;&#21516;&#32676;&#20307;&#30495;&#27491;&#38451;&#24615;&#29575;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#23454;&#29616;&#26426;&#20250;&#24179;&#31561;&#26469;&#28040;&#38500;&#32479;&#35745;&#24615;&#27495;&#35270;&#12290;</title><link>http://arxiv.org/abs/2310.04585</link><description>&lt;p&gt;
&#26426;&#20250;&#24179;&#31561;&#23545;&#32479;&#35745;&#24615;&#27495;&#35270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Equal Opportunity on Statistical Discrimination. (arXiv:2310.04585v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20462;&#25913;&#32479;&#35745;&#24615;&#27495;&#35270;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#30001;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#30340;&#21487;&#21512;&#21516;&#21270;&#20449;&#24565;&#65292;&#32473;&#30417;&#31649;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#36229;&#36807;&#32943;&#23450;&#34892;&#21160;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35201;&#27714;&#20844;&#21496;&#36873;&#21462;&#19968;&#20010;&#24179;&#34913;&#19981;&#21516;&#32676;&#20307;&#30495;&#27491;&#38451;&#24615;&#29575;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#23454;&#29616;&#26426;&#20250;&#24179;&#31561;&#26469;&#28040;&#38500;&#32479;&#35745;&#24615;&#27495;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20462;&#25913;&#20102;Coate&#21644;Loury&#65288;1993&#65289;&#30340;&#32463;&#20856;&#32479;&#35745;&#24615;&#27495;&#35270;&#27169;&#22411;&#65292;&#20551;&#35774;&#20844;&#21496;&#23545;&#20010;&#20307;&#26410;&#35266;&#23519;&#21040;&#30340;&#31867;&#21035;&#30340;&#20449;&#24565;&#26159;&#30001;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#30340;&#65292;&#22240;&#27492;&#26159;&#21487;&#21512;&#21516;&#21270;&#30340;&#12290;&#36825;&#25193;&#23637;&#20102;&#30417;&#31649;&#32773;&#30340;&#24037;&#20855;&#31665;&#65292;&#36229;&#20986;&#20102;&#20687;&#32943;&#23450;&#34892;&#21160;&#36825;&#26679;&#30340;&#26080;&#20449;&#24565;&#35268;&#23450;&#12290;&#21487;&#21512;&#21516;&#21270;&#30340;&#20449;&#24565;&#20351;&#24471;&#35201;&#27714;&#20844;&#21496;&#36873;&#25321;&#19968;&#20010;&#20915;&#31574;&#31574;&#30053;&#65292;&#20351;&#24471;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#30495;&#27491;&#38451;&#24615;&#29575;&#30456;&#31561;&#65288;&#31639;&#27861;&#20844;&#24179;&#25991;&#29486;&#20013;&#25152;&#31216;&#30340;&#26426;&#20250;&#24179;&#31561;&#65289;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#32943;&#23450;&#34892;&#21160;&#19981;&#19968;&#23450;&#33021;&#28040;&#38500;&#32479;&#35745;&#24615;&#27495;&#35270;&#65292;&#20294;&#26412;&#25991;&#34920;&#26126;&#23454;&#26045;&#26426;&#20250;&#24179;&#31561;&#21487;&#20197;&#20570;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
I modify the canonical statistical discrimination model of Coate and Loury (1993) by assuming the firm's belief about an individual's unobserved class is machine learning-generated and, therefore, contractible. This expands the toolkit of a regulator beyond belief-free regulations like affirmative action. Contractible beliefs make it feasible to require the firm to select a decision policy that equalizes true positive rates across groups -- what the algorithmic fairness literature calls equal opportunity. While affirmative action does not necessarily end statistical discrimination, I show that imposing equal opportunity does.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#35299;&#20915;&#37325;&#26500;&#29305;&#24449;&#31354;&#38388;&#19981;&#21487;&#29702;&#35299;&#21644;&#32570;&#20047;&#31995;&#32479;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17011</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#24863;&#30693;&#33258;&#21160;&#36716;&#25442;&#22686;&#24378;&#29305;&#24449;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
Feature Cognition Enhancement via Interaction-Aware Automated Transformation. (arXiv:2309.17011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17011
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#35299;&#20915;&#37325;&#26500;&#29305;&#24449;&#31354;&#38388;&#19981;&#21487;&#29702;&#35299;&#21644;&#32570;&#20047;&#31995;&#32479;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#34920;&#31034;&#31354;&#38388;&#23545;&#20110;&#38477;&#20302;&#32500;&#25968;&#28798;&#38590;&#12289;&#22686;&#24378;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12289;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#24615;&#20197;&#21450;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#32463;&#20856;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#65288;AutoFE&#65289;&#30340;&#36827;&#23637;&#22312;&#35299;&#20915;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#21508;&#31181;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20363;&#22914;&#23545;&#22823;&#37327;&#21171;&#21160;&#21644;&#32463;&#39564;&#30340;&#20381;&#36182;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23884;&#20837;&#21018;&#24615;&#29305;&#24449;&#31354;&#38388;&#37325;&#24314;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#20197;&#19979;&#38480;&#21046;&#65306;1&#65289;&#20135;&#29983;&#28508;&#22312;&#30340;&#38590;&#20197;&#29702;&#35299;&#21644;&#19981;&#21512;&#24120;&#29702;&#30340;&#37325;&#26500;&#29305;&#24449;&#31354;&#38388;&#65292;&#28304;&#20110;&#24573;&#35270;&#19987;&#23478;&#32423;&#30340;&#35748;&#30693;&#36807;&#31243;&#65307;2&#65289;&#32570;&#20047;&#31995;&#32479;&#30340;&#25506;&#32034;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#25910;&#25947;&#32531;&#24930;&#20197;&#25214;&#21040;&#26368;&#20339;&#29305;&#24449;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#20102;&#29305;&#24449;&#31354;&#38388;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#35748;&#30693;&#36807;&#31243;&#21644;&#31995;&#32479;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating an effective representation space is crucial for mitigating the curse of dimensionality, enhancing model generalization, addressing data sparsity, and leveraging classical models more effectively. Recent advancements in automated feature engineering (AutoFE) have made significant progress in addressing various challenges associated with representation learning, issues such as heavy reliance on intensive labor and empirical experiences, lack of explainable explicitness, and inflexible feature space reconstruction embedded into downstream tasks. However, these approaches are constrained by: 1) generation of potentially unintelligible and illogical reconstructed feature spaces, stemming from the neglect of expert-level cognitive processes; 2) lack of systematic exploration, which subsequently results in slower model convergence for identification of optimal feature space. To address these, we introduce an interaction-aware reinforced generation perspective. We redefine feature sp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36716;&#25442;&#20197;&#23454;&#29616;&#36890;&#29992;&#30340;&#23454;&#20363;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#26469;&#39044;&#27979;&#22270;&#20687;&#30340;&#21464;&#25442;&#20998;&#24067;&#65292;&#24182;&#23545;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#25512;&#24191;&#19981;&#21464;&#24615;&#30340;&#31867;&#21035;&#38388;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#36866;&#24212;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23039;&#21183;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#21464;&#25442;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2309.16672</link><description>&lt;p&gt;
&#23398;&#20064;&#36716;&#25442;&#20197;&#23454;&#29616;&#36890;&#29992;&#30340;&#23454;&#20363;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning to Transform for Generalizable Instance-wise Invariance. (arXiv:2309.16672v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36716;&#25442;&#20197;&#23454;&#29616;&#36890;&#29992;&#30340;&#23454;&#20363;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#26469;&#39044;&#27979;&#22270;&#20687;&#30340;&#21464;&#25442;&#20998;&#24067;&#65292;&#24182;&#23545;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#25512;&#24191;&#19981;&#21464;&#24615;&#30340;&#31867;&#21035;&#38388;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#36866;&#24212;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23039;&#21183;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#21464;&#25442;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#26500;&#24314;&#23545;&#33258;&#28982;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#21464;&#25442;&#20855;&#26377;&#24378;&#40065;&#26834;&#24615;&#30340;&#31995;&#32479;&#12290;&#20256;&#32479;&#19978;&#65292;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25110;&#23558;&#19981;&#21464;&#24615;&#30828;&#32534;&#30721;&#21040;&#26550;&#26500;&#20013;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#36807;&#22810;&#25110;&#36807;&#23569;&#30340;&#19981;&#21464;&#24615;&#37117;&#21487;&#33021;&#20250;&#24433;&#21709;&#32467;&#26524;&#65292;&#27491;&#30830;&#30340;&#19981;&#21464;&#24615;&#31243;&#24230;&#22312;&#20808;&#39564;&#20013;&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#23454;&#20363;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#24212;&#35813;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36866;&#24403;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#25512;&#26029;&#12290;&#25105;&#20204;&#23558;&#19981;&#21464;&#24615;&#35270;&#20026;&#19968;&#20010;&#39044;&#27979;&#38382;&#39064;&#12290;&#32473;&#23450;&#20219;&#20309;&#22270;&#20687;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24402;&#19968;&#21270;&#27969;&#26469;&#39044;&#27979;&#21464;&#25442;&#30340;&#20998;&#24067;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#39044;&#27979;&#36827;&#34892;&#24179;&#22343;&#12290;&#30001;&#20110;&#36825;&#20010;&#20998;&#24067;&#20165;&#21462;&#20915;&#20110;&#23454;&#20363;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#20998;&#31867;&#20043;&#21069;&#23545;&#23454;&#20363;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#22312;&#31867;&#21035;&#20043;&#38388;&#25512;&#24191;&#19981;&#21464;&#24615;&#12290;&#21516;&#26679;&#30340;&#20998;&#24067;&#20063;&#21487;&#20197;&#29992;&#20110;&#36866;&#24212;&#36229;&#20986;&#20998;&#24067;&#30340;&#23039;&#21183;&#12290;&#36825;&#20010;&#24402;&#19968;&#21270;&#27969;&#26159;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#27604;Augerino&#21644;InstaAug&#26356;&#22810;&#33539;&#22260;&#30340;&#21464;&#25442;&#12290;&#24403;&#29992;&#20316;&#25968;&#25454;&#22686;&#24378;&#26102;&#65292;&#25105;&#20204;&#30340;m
&lt;/p&gt;
&lt;p&gt;
Computer vision research has long aimed to build systems that are robust to spatial transformations found in natural data. Traditionally, this is done using data augmentation or hard-coding invariances into the architecture. However, too much or too little invariance can hurt, and the correct amount is unknown a priori and dependent on the instance. Ideally, the appropriate invariance would be learned from data and inferred at test-time.  We treat invariance as a prediction problem. Given any image, we use a normalizing flow to predict a distribution over transformations and average the predictions over them. Since this distribution only depends on the instance, we can align instances before classifying them and generalize invariance across classes. The same distribution can also be used to adapt to out-of-distribution poses. This normalizing flow is trained end-to-end and can learn a much larger range of transformations than Augerino and InstaAug. When used as data augmentation, our m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SFAVEL&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#12290;&#36825;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.16540</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Fact Verification by Language Model Distillation. (arXiv:2309.16540v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SFAVEL&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#12290;&#36825;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#26088;&#22312;&#36890;&#36807;&#21487;&#38752;&#30693;&#35782;&#24211;&#20013;&#30340;&#35777;&#25454;&#26469;&#39564;&#35777;&#20027;&#24352;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#31639;&#27861;&#24517;&#39035;&#20026;&#27599;&#20010;&#20027;&#24352;&#29983;&#25104;&#26082;&#35821;&#20041;&#26126;&#30830;&#21448;&#32039;&#20945;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#19982;&#28304;&#20449;&#24687;&#36827;&#34892;&#35821;&#20041;&#23545;&#40784;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#21069;&#32773;&#36890;&#36807;&#23398;&#20064;&#21253;&#21547;&#20027;&#24352;&#21450;&#20854;&#30456;&#24212;&#26631;&#31614;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#26469;&#35299;&#20915;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SFAVEL&#65288;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#33976;&#39311;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#32780;&#26080;&#38656;&#27880;&#37322;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#30340;&#65292;&#35813;&#20989;&#25968;&#40723;&#21169;&#29305;&#24449;&#22312;&#20445;&#25345;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;&#21644;&#35777;&#25454;&#23545;&#40784;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36798;&#21040;&#26032;&#39062;&#30340;&#29366;&#24577;&#19968;.
&lt;/p&gt;
&lt;p&gt;
Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#38899;&#39057;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#20013;&#25439;&#22833;&#20989;&#25968;&#26368;&#23567;&#20540;&#30340;&#38160;&#24230;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#30740;&#31350;&#21457;&#29616;&#38160;&#24230;&#26356;&#39640;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20174;&#20043;&#21069;&#26410;&#35265;&#35774;&#22791;&#24405;&#21046;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#26469;&#35828;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#26159;&#26368;&#23567;&#20540;&#38160;&#24230;&#30340;&#20027;&#35201;&#24433;&#21709;&#22240;&#32032;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#21487;&#27604;&#24615;&#26041;&#38754;&#30340;&#30456;&#20851;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.16369</link><description>&lt;p&gt;
&#25226;&#26368;&#23567;&#20540;&#30340;&#38160;&#24230;&#30340;&#35752;&#35770;&#24102;&#20837;&#21040;&#38899;&#39057;&#39046;&#22495;:&#19968;&#20010;&#29992;&#20110;&#22768;&#23398;&#22330;&#26223;&#20998;&#31867;&#30340;&#28388;&#27874;&#22120;&#24402;&#19968;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bringing the Discussion of Minima Sharpness to the Audio Domain: a Filter-Normalised Evaluation for Acoustic Scene Classification. (arXiv:2309.16369v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#38899;&#39057;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#20013;&#25439;&#22833;&#20989;&#25968;&#26368;&#23567;&#20540;&#30340;&#38160;&#24230;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#30740;&#31350;&#21457;&#29616;&#38160;&#24230;&#26356;&#39640;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20174;&#20043;&#21069;&#26410;&#35265;&#35774;&#22791;&#24405;&#21046;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#26469;&#35828;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#26159;&#26368;&#23567;&#20540;&#38160;&#24230;&#30340;&#20027;&#35201;&#24433;&#21709;&#22240;&#32032;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#21487;&#27604;&#24615;&#26041;&#38754;&#30340;&#30456;&#20851;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32972;&#26223;&#19979;&#65292;&#25439;&#22833;&#20989;&#25968;&#26368;&#23567;&#20540;&#30340;&#38160;&#24230;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#19968;&#30452;&#23384;&#22312;&#20105;&#35758;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#36873;&#23450;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#20294;&#25105;&#20204;&#22312;DCASE2020&#25361;&#25112;&#25968;&#25454;&#30340;&#38899;&#39057;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#20013;&#25506;&#32034;&#20102;&#36825;&#19968;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#20108;&#32500;&#28388;&#27874;&#22120;&#24402;&#19968;&#21270;&#21487;&#35270;&#21270;&#21644;&#27966;&#29983;&#30340;&#38160;&#24230;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#38160;&#24230;&#26356;&#39640;&#30340;&#26368;&#23567;&#20540;&#36890;&#24120;&#27604;&#24179;&#22374;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20174;&#20043;&#21069;&#26410;&#35265;&#35774;&#22791;&#24405;&#21046;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#26469;&#35828;&#12290;&#36825;&#36827;&#19968;&#27493;&#21152;&#28145;&#20102;&#20851;&#20110;&#24179;&#22374;&#26368;&#23567;&#20540;&#27867;&#21270;&#33021;&#21147;&#30340;&#20105;&#35758;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#29305;&#21035;&#26159;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#26159;&#26368;&#23567;&#20540;&#38160;&#24230;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#21487;&#27604;&#24615;&#26041;&#38754;&#30340;&#30456;&#20851;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#35757;&#32451;&#27169;&#22411;&#29366;&#24577;&#21644;&#25439;&#22833;&#22270;&#26223;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The correlation between the sharpness of loss minima and generalisation in the context of deep neural networks has been subject to discussion for a long time. Whilst mostly investigated in the context of selected benchmark data sets in the area of computer vision, we explore this aspect for the audio scene classification task of the DCASE2020 challenge data. Our analysis is based on twodimensional filter-normalised visualisations and a derived sharpness measure. Our exploratory analysis shows that sharper minima tend to show better generalisation than flat minima -even more so for out-of-domain data, recorded from previously unseen devices-, thus adding to the dispute about better generalisation capabilities of flat minima. We further find that, in particular, the choice of optimisers is a main driver of the sharpness of minima and we discuss resulting limitations with respect to comparability. Our code, trained model states and loss landscape visualisations are publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#36827;&#34892;&#35821;&#38899;&#36716;&#25442;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20462;&#25913;&#35821;&#38899;&#30340;&#38901;&#24459;&#21644;&#24773;&#24863;&#20449;&#24687;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#25351;&#20196;&#24182;&#20135;&#29983;&#21512;&#29702;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.14324</link><description>&lt;p&gt;
&#36890;&#29992;&#25991;&#26412;&#25351;&#23548;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards General-Purpose Text-Instruction-Guided Voice Conversion. (arXiv:2309.14324v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#36827;&#34892;&#35821;&#38899;&#36716;&#25442;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20462;&#25913;&#35821;&#38899;&#30340;&#38901;&#24459;&#21644;&#24773;&#24863;&#20449;&#24687;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#25351;&#20196;&#24182;&#20135;&#29983;&#21512;&#29702;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#22914;"&#24930;&#24930;&#28165;&#26224;&#22320;&#35828;&#35805;&#65292;&#22768;&#38899;&#20302;&#27785;"&#25110;"&#20197;&#24555;&#20048;&#30340;&#23569;&#24180;&#22768;&#38899;&#35828;&#35805;"&#26469;&#25351;&#23548;&#36716;&#25442;&#36807;&#31243;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#22686;&#21152;&#20102;&#35821;&#38899;&#36716;&#25442;&#30340;&#36890;&#29992;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;&#35813;&#25552;&#20986;&#30340;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#26159;&#19968;&#20010;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#19968;&#31995;&#21015;&#31163;&#25955;&#32534;&#30721;&#65292;&#24471;&#21040;&#36716;&#25442;&#21518;&#30340;&#35821;&#38899;&#32534;&#30721;&#24207;&#21015;&#12290;&#23427;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20316;&#20026;&#26679;&#24335;&#25552;&#31034;&#65292;&#20462;&#25913;&#32473;&#23450;&#35821;&#38899;&#30340;&#38901;&#24459;&#21644;&#24773;&#24863;&#20449;&#24687;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#22788;&#29702;&#35821;&#38899;&#30340;&#21508;&#31181;&#20449;&#24687;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#29420;&#31435;&#30340;&#32534;&#30721;&#22120;&#22788;&#29702;&#28304;&#35821;&#38899;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#38901;&#24459;&#21644;&#20869;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#29702;&#35299;&#25351;&#20196;&#24182;&#20135;&#29983;&#21512;&#29702;&#32467;&#26524;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel voice conversion (VC) model, guided by text instructions such as "articulate slowly with a deep tone" or "speak in a cheerful boyish voice". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#31574;&#30053;&#38236;&#20687;&#26799;&#24230;&#31639;&#27861;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24456;&#22909;&#22320;&#36924;&#36817;&#20215;&#20540;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#19988;&#36924;&#36817;&#35823;&#24046;&#21463;&#32593;&#32476;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#21487;&#20197;&#32487;&#25215;&#20043;&#21069;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13915</link><description>&lt;p&gt;
&#31070;&#32463;&#31574;&#30053;&#38236;&#20687;&#26799;&#24230;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#31574;&#30053;&#20248;&#21270;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on Low-Dimensional Manifolds. (arXiv:2309.13915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#31574;&#30053;&#38236;&#20687;&#26799;&#24230;&#31639;&#27861;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24456;&#22909;&#22320;&#36924;&#36817;&#20215;&#20540;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#19988;&#36924;&#36817;&#35823;&#24046;&#21463;&#32593;&#32476;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#21487;&#20197;&#32487;&#25215;&#20043;&#21069;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#37197;&#22791;&#26377;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#22312;&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#20998;&#26512;&#26080;&#27861;&#35299;&#37322;&#23427;&#20204;&#20026;&#20309;&#33021;&#25269;&#25239;&#32500;&#24230;&#35781;&#21650;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#31070;&#32463;&#31574;&#30053;&#38236;&#20687;&#26799;&#24230;&#65288;NPMD&#65289;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#21463;&#21040;&#35768;&#22810;&#39640;&#32500;&#29615;&#22659;&#20855;&#26377;&#20302;&#32500;&#32467;&#26500;&#30340;&#32463;&#39564;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#20363;&#22914;&#23558;&#22270;&#20687;&#20316;&#20026;&#29366;&#24577;&#65292;&#25105;&#20204;&#23558;&#29366;&#24577;&#31354;&#38388;&#35270;&#20026;&#23884;&#20837;&#22312;$D$&#32500;&#27431;&#27663;&#31354;&#38388;&#20013;&#30340;$d$&#32500;&#27969;&#24418;&#65292;&#20854;&#20013;$d\ll D$&#26159;&#20869;&#22312;&#32500;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;NPMD&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#20215;&#20540;&#20989;&#25968;&#21644;&#31574;&#30053;&#37117;&#21487;&#20197;&#24456;&#22909;&#22320;&#30001;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36924;&#36817;&#12290;&#36924;&#36817;&#35823;&#24046;&#30001;&#32593;&#32476;&#30340;&#22823;&#23567;&#25511;&#21046;&#65292;&#24182;&#19988;&#21069;&#19968;&#20010;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#21487;&#20197;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy-based algorithms equipped with deep neural networks have achieved great success in solving high-dimensional policy optimization problems in reinforcement learning. However, current analyses cannot explain why they are resistant to the curse of dimensionality. In this work, we study the sample complexity of the neural policy mirror descent (NPMD) algorithm with convolutional neural networks (CNN) as function approximators. Motivated by the empirical observation that many high-dimensional environments have state spaces possessing low-dimensional structures, such as those taking images as states, we consider the state space to be a $d$-dimensional manifold embedded in the $D$-dimensional Euclidean space with intrinsic dimension $d\ll D$. We show that in each iteration of NPMD, both the value function and the policy can be well approximated by CNNs. The approximation errors are controlled by the size of the networks, and the smoothness of the previous networks can be inherited. As a
&lt;/p&gt;</description></item><item><title>AMPLIFY&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#30340;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#20110;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12689</link><description>&lt;p&gt;
AMPLIFY: &#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26631;&#31614;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer. (arXiv:2309.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12689
&lt;/p&gt;
&lt;p&gt;
AMPLIFY&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#30340;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#20110;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#21407;&#22987;&#26679;&#26412;&#30340;&#32447;&#24615;&#32452;&#21512;&#29983;&#25104;&#26032;&#30340;&#22686;&#24378;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#21407;&#22987;&#26679;&#26412;&#20013;&#23384;&#22312;&#22122;&#38899;&#25110;&#24322;&#24120;&#29305;&#24449;&#65292;Mixup&#21487;&#33021;&#23558;&#20854;&#20256;&#25773;&#21040;&#22686;&#24378;&#26679;&#26412;&#20013;&#65292;&#23548;&#33268;&#27169;&#22411;&#23545;&#36825;&#20123;&#24322;&#24120;&#20540;&#36807;&#20110;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Mixup&#26041;&#27861;&#31216;&#20026;AMPLIFY&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Transformer&#33258;&#36523;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#26080;&#38656;&#22686;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#20302;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#24120;&#35265;Mixup&#26041;&#27861;&#65288;&#20363;&#22914;&#35821;&#21477;Mixup&#65289;&#20013;&#36164;&#28304;&#28040;&#32791;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26356;&#23567;&#30340;&#35745;&#31639;&#36164;&#28304;&#25104;&#26412;&#19979;&#65292;AMPLIFY&#22312;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;Mixup&#26041;&#27861;&#65292;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is an effective data augmentation method that generates new augmented samples by aggregating linear combinations of different original samples. However, if there are noises or aberrant features in the original samples, Mixup may propagate them to the augmented samples, leading to over-sensitivity of the model to these outliers . To solve this problem, this paper proposes a new Mixup method called AMPLIFY. This method uses the Attention mechanism of Transformer itself to reduce the influence of noises and aberrant values in the original samples on the prediction results, without increasing additional trainable parameters, and the computational cost is very low, thereby avoiding the problem of high resource consumption in common Mixup methods such as Sentence Mixup . The experimental results show that, under a smaller computational resource cost, AMPLIFY outperforms other Mixup methods in text classification tasks on 7 benchmark datasets, providing new ideas and new ways to further
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#31639;&#27861;&#65292;&#33021;&#22815;&#21152;&#36895;GPU&#23545;&#20110;&#39034;&#24207;&#27169;&#22411;&#30340;&#35780;&#20272;&#36895;&#24230;&#65292;&#25552;&#39640;&#20102;3&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#38477;&#20302;&#36755;&#20986;&#20934;&#30830;&#24615;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#65292;&#24182;&#22312;&#38271;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#21457;&#29616;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12252</link><description>&lt;p&gt;
&#22312;&#24207;&#21015;&#38271;&#24230;&#19978;&#24182;&#34892;&#21270;&#38750;&#32447;&#24615;&#39034;&#24207;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Parallelizing non-linear sequential models over the sequence length. (arXiv:2309.12252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#31639;&#27861;&#65292;&#33021;&#22815;&#21152;&#36895;GPU&#23545;&#20110;&#39034;&#24207;&#27169;&#22411;&#30340;&#35780;&#20272;&#36895;&#24230;&#65292;&#25552;&#39640;&#20102;3&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#38477;&#20302;&#36755;&#20986;&#20934;&#30830;&#24615;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#65292;&#24182;&#22312;&#38271;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#21457;&#29616;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#27169;&#22411;&#65292;&#20363;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19968;&#30452;&#30001;&#20110;&#20854;&#26412;&#36136;&#19978;&#30340;&#39034;&#24207;&#29305;&#24615;&#32780;&#23384;&#22312;&#35757;&#32451;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#22810;&#24180;&#26469;&#36825;&#20010;&#29942;&#39048;&#19968;&#30452;&#23384;&#22312;&#65292;&#22240;&#20026;&#24456;&#22810;&#20154;&#35748;&#20026;&#39034;&#24207;&#27169;&#22411;&#26080;&#27861;&#24182;&#34892;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#24182;&#34892;&#31639;&#27861;&#25361;&#25112;&#20102;&#36825;&#20010;&#38271;&#26399;&#20197;&#26469;&#30340;&#20449;&#24565;&#65292;&#21152;&#36895;&#20102;GPU&#23545;&#20110;&#39034;&#24207;&#27169;&#22411;&#30340;&#35780;&#20272;&#36895;&#24230;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;3&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#29306;&#29298;&#36755;&#20986;&#20934;&#30830;&#24615;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#39034;&#24207;&#27169;&#22411;&#26550;&#26500;&#20013;&#30340;&#20219;&#20309;&#29305;&#27530;&#32467;&#26500;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#39034;&#24207;&#27169;&#22411;&#21487;&#20197;&#27604;&#24120;&#35268;&#30340;&#39034;&#24207;&#26041;&#27861;&#24555;10&#20493;&#20197;&#19978;&#65292;&#32780;&#35757;&#32451;&#32467;&#26524;&#27809;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;&#20511;&#21161;&#36825;&#31181;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;17k&#20010;&#26102;&#38388;&#26679;&#26412;&#30340;&#38271;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#21457;&#29616;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#20811;&#26381;&#35757;&#32451;&#29942;&#39048;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20351;&#24471;&#39034;&#24207;&#27169;&#22411;&#30340;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential models, such as Recurrent Neural Networks and Neural Ordinary Differential Equations, have long suffered from slow training due to their inherent sequential nature. For many years this bottleneck has persisted, as many thought sequential models could not be parallelized. We challenge this long-held belief with our parallel algorithm that accelerates GPU evaluation of sequential models by up to 3 orders of magnitude faster without compromising output accuracy. The algorithm does not need any special structure in the sequential models' architecture, making it applicable to a wide range of architectures. Using our method, training sequential models can be more than 10 times faster than the common sequential method without any meaningful difference in the training results. Leveraging this accelerated training, we discovered the efficacy of the Gated Recurrent Unit in a long time series classification problem with 17k time samples. By overcoming the training bottleneck, our work 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25913;&#36827;&#30340;&#26041;&#24046;&#26368;&#23567;&#21270;&#30340;&#20998;&#22359;&#37327;&#21270;&#31574;&#30053;&#65292;&#29992;&#20110;&#21387;&#32553;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28608;&#27963;&#65292;&#23454;&#29616;&#20869;&#23384;&#28040;&#32791;&#30340;&#38477;&#20302;&#21644;&#36816;&#34892;&#26102;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2309.11856</link><description>&lt;p&gt;
&#20351;&#29992;&#25913;&#36827;&#30340;&#26041;&#24046;&#26368;&#23567;&#21270;&#30340;&#20998;&#22359;&#37327;&#21270;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#28608;&#27963;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization. (arXiv:2309.11856v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25913;&#36827;&#30340;&#26041;&#24046;&#26368;&#23567;&#21270;&#30340;&#20998;&#22359;&#37327;&#21270;&#31574;&#30053;&#65292;&#29992;&#20110;&#21387;&#32553;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28608;&#27963;&#65292;&#23454;&#29616;&#20869;&#23384;&#28040;&#32791;&#30340;&#38477;&#20302;&#21644;&#36816;&#34892;&#26102;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#39640;&#25928;&#35757;&#32451;&#65292;&#37325;&#28857;&#26159;&#20943;&#23569;&#20854;&#20869;&#23384;&#28040;&#32791;&#12290;Liu&#31561;&#20154;&#65288;2022&#24180;&#65289;&#25552;&#20986;&#20102;&#26497;&#38480;&#28608;&#27963;&#21387;&#32553;&#65288;EXACT&#65289;&#65292;&#36890;&#36807;&#23558;&#20013;&#38388;&#28608;&#27963;&#22270;&#30340;&#37327;&#21270;&#38477;&#33267;INT2&#31934;&#24230;&#65292;&#23454;&#29616;&#20102;&#20869;&#23384;&#28040;&#32791;&#30340;&#21095;&#28872;&#20943;&#23569;&#12290;&#20182;&#20204;&#22312;&#23454;&#29616;&#22823;&#24133;&#20943;&#23569;GPU&#20869;&#23384;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#34920;&#29616;&#20960;&#20046;&#27809;&#26377;&#38477;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#28608;&#27963;&#22270;&#30340;&#20998;&#22359;&#37327;&#21270;&#65292;&#23545;EXACT&#31574;&#30053;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#23454;&#39564;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#22359;&#22823;&#23567;&#65292;&#24182;&#23637;&#31034;&#20102;&#36827;&#19968;&#27493;&#30340;&#20869;&#23384;&#28040;&#32791;&#38477;&#20302;&#65288;&gt;15%&#65289;&#21644;&#27599;&#20010;epoch&#30340;&#36816;&#34892;&#26102;&#21152;&#36895;&#65288;&#32422;5%&#65289;&#65292;&#21363;&#20351;&#36827;&#34892;&#20102;&#26497;&#20854;&#22823;&#30340;&#37327;&#21270;&#31243;&#24230;&#65292;&#20063;&#33021;&#33719;&#24471;&#19982;&#21407;&#22987;EXACT&#30456;&#20284;&#30340;&#24615;&#33021;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;EXACT&#20013;&#20851;&#20110;&#20013;&#38388;&#28608;&#27963;&#22270;&#20998;&#24067;&#30340;&#20551;&#35774;&#36827;&#34892;&#20102;&#32416;&#27491;&#65288;&#20551;&#35774;&#20026;u
&lt;/p&gt;
&lt;p&gt;
Efficient training of large-scale graph neural networks (GNNs) has been studied with a specific focus on reducing their memory consumption. Work by Liu et al. (2022) proposed extreme activation compression (EXACT) which demonstrated drastic reduction in memory consumption by performing quantization of the intermediate activation maps down to using INT2 precision. They showed little to no reduction in performance while achieving large reductions in GPU memory consumption. In this work, we present an improvement to the EXACT strategy by using block-wise quantization of the intermediate activation maps. We experimentally analyze different block sizes and show further reduction in memory consumption (&gt;15%), and runtime speedup per epoch (about 5%) even when performing extreme extents of quantization with similar performance trade-offs as with the original EXACT. Further, we present a correction to the assumptions on the distribution of intermediate activation maps in EXACT (assumed to be u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#38480;&#21046;&#12290;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#29983;&#25104;&#21435;&#22122;&#35780;&#20998;&#21305;&#37197;&#25439;&#22833;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#31532;&#20108;&#38454;&#27573;&#36890;&#36807;&#35299;&#20915;&#21453;&#21521;&#36807;&#31243;&#26469;&#35745;&#31639;&#22686;&#24378;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#25439;&#22833;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;5&#20010;&#20989;&#25968;&#35780;&#20272;&#23601;&#33021;&#36798;&#21040;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;60&#20010;&#20989;&#25968;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.09677</link><description>&lt;p&gt;
&#21333;&#27493;&#21644;&#23569;&#27493;&#25193;&#25955;&#29992;&#20110;&#29983;&#25104;&#24335;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Single and Few-step Diffusion for Generative Speech Enhancement. (arXiv:2309.09677v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#38480;&#21046;&#12290;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#29983;&#25104;&#21435;&#22122;&#35780;&#20998;&#21305;&#37197;&#25439;&#22833;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#31532;&#20108;&#38454;&#27573;&#36890;&#36807;&#35299;&#20915;&#21453;&#21521;&#36807;&#31243;&#26469;&#35745;&#31639;&#22686;&#24378;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#25439;&#22833;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;5&#20010;&#20989;&#25968;&#35780;&#20272;&#23601;&#33021;&#36798;&#21040;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;60&#20010;&#20989;&#25968;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#21033;&#29992;&#20219;&#21153;&#36866;&#24212;&#24615;&#25193;&#25955;&#36807;&#31243;&#26469;&#29983;&#25104;&#32473;&#23450;&#22024;&#26434;&#28151;&#21512;&#22768;&#38899;&#30340;&#32431;&#20928;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#22312;&#27979;&#35797;&#26102;&#65292;&#29992;&#20110;&#35780;&#20998;&#20272;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#34987;&#22810;&#27425;&#35843;&#29992;&#20197;&#35299;&#20915;&#36845;&#20195;&#30340;&#21453;&#21521;&#36807;&#31243;&#12290;&#36825;&#23548;&#33268;&#25512;&#29702;&#36807;&#31243;&#32531;&#24930;&#65292;&#24182;&#23548;&#33268;&#22312;&#37319;&#26679;&#36712;&#36857;&#20013;&#31215;&#32047;&#31163;&#25955;&#21270;&#35823;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#29992;&#29983;&#25104;&#21435;&#22122;&#35780;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#24120;&#35268;&#26041;&#24335;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#21453;&#21521;&#36807;&#31243;&#26469;&#35745;&#31639;&#22686;&#24378;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#25439;&#22833;&#23558;&#32467;&#26524;&#20272;&#35745;&#19982;&#32431;&#20928;&#35821;&#38899;&#30446;&#26631;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#36825;&#20010;&#31532;&#20108;&#35757;&#32451;&#38454;&#27573;&#21482;&#38656;&#35201;5&#20010;&#20989;&#25968;&#35780;&#20272;&#65292;&#23601;&#33021;&#36798;&#21040;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;60&#20010;&#20989;&#25968;&#35780;&#20272;&#12290;&#34429;&#28982;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown promising results in speech enhancement, using a task-adapted diffusion process for the conditional generation of clean speech given a noisy mixture. However, at test time, the neural network used for score estimation is called multiple times to solve the iterative reverse process. This results in a slow inference process and causes discretization errors that accumulate over the sampling trajectory. In this paper, we address these limitations through a two-stage training approach. In the first stage, we train the diffusion model the usual way using the generative denoising score matching loss. In the second stage, we compute the enhanced signal by solving the reverse process and compare the resulting estimate to the clean speech target using a predictive loss. We show that using this second training stage enables achieving the same performance as the baseline model using only 5 function evaluations instead of 60 function evaluations. While the performance of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;SE-Resnet-Bi-LSTM&#26550;&#26500;&#36827;&#34892;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#65292;&#24182;&#36827;&#34892;&#20102;&#27169;&#22411;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#21644;&#23439;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.07156</link><description>&lt;p&gt;
&#28145;&#20837;&#30740;&#31350;&#30561;&#30496;&#65306;&#22522;&#20110;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#19982;&#27169;&#22411;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Deep Dive into Sleep: Single-Channel EEG-Based Sleep Stage Classification with Model Interpretability. (arXiv:2309.07156v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;SE-Resnet-Bi-LSTM&#26550;&#26500;&#36827;&#34892;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#65292;&#24182;&#36827;&#34892;&#20102;&#27169;&#22411;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#21644;&#23439;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#29983;&#29702;&#36807;&#31243;&#65292;&#22312;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#21344;&#25454;&#30528;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#20934;&#30830;&#20998;&#31867;&#30561;&#30496;&#38454;&#27573;&#26159;&#35780;&#20272;&#30561;&#30496;&#36136;&#37327;&#21644;&#35782;&#21035;&#21487;&#33021;&#30340;&#30561;&#30496;&#38556;&#30861;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;SE-Resnet-Bi-LSTM&#26550;&#26500;&#23558;&#30561;&#30496;&#20998;&#31867;&#20026;&#20116;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#12290;&#20998;&#31867;&#36807;&#31243;&#22522;&#20110;&#23545;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#20998;&#26512;&#12290;&#24314;&#35758;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#22522;&#26412;&#20803;&#32032;&#32452;&#25104;&#65306;&#21033;&#29992;SE-ResNet&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#21033;&#29992;Bi-LSTM&#21333;&#20803;&#22534;&#26632;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#35780;&#20272;&#24471;&#21040;&#35777;&#23454;&#65292;&#20998;&#21035;&#26159;SLeepEDF-20&#12289;SleepEDF-78&#21644;SHHS&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#65292;&#20998;&#21035;&#20026;87.5&#65285;&#12289;83.9&#65285;&#21644;87.8&#65285;&#65292;&#24182;&#19988;&#22312;&#23439;F1&#24471;&#20998;&#26041;&#38754;&#20998;&#21035;&#20026;82.5&#12289;78.9&#21644;81.9&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep, a fundamental physiological process, occupies a significant portion of our lives. Accurate classification of sleep stages serves as a crucial tool for evaluating sleep quality and identifying probable sleep disorders. This work introduces a novel methodology that utilises a SE-Resnet-Bi-LSTM architecture to classify sleep into five separate stages. The classification process is based on the analysis of single-channel electroencephalograms (EEGs). The framework that has been suggested consists of two fundamental elements: a feature extractor that utilises SE-ResNet, and a temporal context encoder that use stacks of Bi-LSTM units.The effectiveness of our approach is substantiated by thorough assessments conducted on three different datasets, namely SLeepEDF-20, SleepEDF-78, and SHHS. Significantly, our methodology attains notable levels of accuracy, specifically 87.5\%, 83.9\%, and 87.8\%, along with macro-F1 scores of 82.5, 78.9, and 81.9 for the corresponding datasets. Notably, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#26694;&#26550;&#26469;&#36880;&#27493;&#23454;&#29616;&#23545;&#22810;&#20010;&#25935;&#24863;&#29305;&#24449;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#36793;&#38469;Wasserstein&#37325;&#24515;&#25193;&#23637;&#20102;&#26631;&#20934;&#30340;&#24378;&#20154;&#21475;&#24179;&#31561;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#38381;&#24335;&#35299;&#26469;&#35299;&#37322;&#25935;&#24863;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06627</link><description>&lt;p&gt;
&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#30340;&#39034;&#24207;&#20844;&#24179;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Sequentially Fair Mechanism for Multiple Sensitive Attributes. (arXiv:2309.06627v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#26694;&#26550;&#26469;&#36880;&#27493;&#23454;&#29616;&#23545;&#22810;&#20010;&#25935;&#24863;&#29305;&#24449;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#36793;&#38469;Wasserstein&#37325;&#24515;&#25193;&#23637;&#20102;&#26631;&#20934;&#30340;&#24378;&#20154;&#21475;&#24179;&#31561;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#38381;&#24335;&#35299;&#26469;&#35299;&#37322;&#25935;&#24863;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#26631;&#20934;&#29992;&#20363;&#20013;&#65292;&#30446;&#26631;&#26159;&#28040;&#38500;&#25935;&#24863;&#21464;&#37327;&#21644;&#30456;&#24212;&#20998;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#24037;&#20855;&#21644;&#23450;&#20041;&#30340;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#26694;&#26550;&#65292;&#21487;&#20197;&#36880;&#27493;&#23454;&#29616;&#23545;&#19968;&#32452;&#25935;&#24863;&#29305;&#24449;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22810;&#36793;&#38469;Wasserstein&#37325;&#24515;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#23558;&#26631;&#20934;&#30340;&#24378;&#20154;&#21475;&#24179;&#31561;&#27010;&#24565;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#25935;&#24863;&#29305;&#24449;&#30340;&#24773;&#20917;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#20026;&#26368;&#20248;&#30340;&#39034;&#24207;&#20844;&#24179;&#39044;&#27979;&#22120;&#25552;&#20379;&#20102;&#38381;&#24335;&#35299;&#65292;&#21487;&#20197;&#28165;&#26970;&#22320;&#35299;&#37322;&#25935;&#24863;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#26080;&#32541;&#25193;&#23637;&#21040;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the standard use case of Algorithmic Fairness, the goal is to eliminate the relationship between a sensitive variable and a corresponding score. Throughout recent years, the scientific community has developed a host of definitions and tools to solve this task, which work well in many practical applications. However, the applicability and effectivity of these tools and definitions becomes less straightfoward in the case of multiple sensitive attributes. To tackle this issue, we propose a sequential framework, which allows to progressively achieve fairness across a set of sensitive features. We accomplish this by leveraging multi-marginal Wasserstein barycenters, which extends the standard notion of Strong Demographic Parity to the case with multiple sensitive characteristics. This method also provides a closed-form solution for the optimal, sequentially fair predictor, permitting a clear interpretation of inter-sensitive feature correlations. Our approach seamlessly extends to approx
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#26080;&#32447;&#32593;&#32476;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22411;&#21098;&#26525;&#21644;&#20010;&#24615;&#21270;&#65292;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#20943;&#23569;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#35774;&#22791;&#30340;&#23398;&#20064;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.01816</link><description>&lt;p&gt;
&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#26080;&#32447;&#32593;&#32476;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Computation and Communication Efficient Federated Learning over Wireless Networks. (arXiv:2309.01816v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01816
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#26080;&#32447;&#32593;&#32476;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22411;&#21098;&#26525;&#21644;&#20010;&#24615;&#21270;&#65292;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#20943;&#23569;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#35774;&#22791;&#30340;&#23398;&#20064;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#33021;&#22815;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#23398;&#20064;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#22791;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#23398;&#20064;&#20934;&#30830;&#24230;&#19979;&#38477;&#65292;&#22312;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#21644;&#26080;&#32447;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#26356;&#26032;&#22823;&#35268;&#27169;&#23398;&#20064;&#27169;&#22411;&#20250;&#22686;&#21152;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FL&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#22411;&#21098;&#26525;&#21644;&#20010;&#24615;&#21270;&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#23558;&#23398;&#20064;&#27169;&#22411;&#20998;&#20026;&#20840;&#23616;&#37096;&#20998;&#21644;&#20010;&#24615;&#21270;&#37096;&#20998;&#65292;&#20840;&#23616;&#37096;&#20998;&#36890;&#36807;&#27169;&#22411;&#21098;&#26525;&#19982;&#25152;&#26377;&#35774;&#22791;&#20849;&#20139;&#20197;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#65292;&#20010;&#24615;&#21270;&#37096;&#20998;&#38024;&#23545;&#29305;&#23450;&#35774;&#22791;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;FL&#36807;&#31243;&#20013;&#35843;&#25972;&#27169;&#22411;&#22823;&#23567;&#20197;&#20943;&#23569;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25968;&#25454;&#35774;&#22791;&#30340;&#23398;&#20064;&#20934;&#30830;&#24230;&#12290;&#28982;&#21518;&#65292;&#23545;&#25152;&#25552;&#20986;&#30340;FL&#26694;&#26550;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#20197;&#21450;&#25910;&#25947;&#20998;&#26512;&#36827;&#34892;&#20102;&#25968;&#23398;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables distributed learning across edge devices while protecting data privacy. However, the learning accuracy decreases due to the heterogeneity of devices' data, and the computation and communication latency increase when updating large-scale learning models on devices with limited computational capability and wireless resources. We consider a novel FL framework with partial model pruning and personalization to overcome these challenges. This framework splits the learning model into a global part with model pruning shared with all devices to learn data representations and a personalized part to be fine-tuned for a specific device, which adapts the model size during FL to reduce both computation and communication latency and increases the learning accuracy for the device with non-independent and identically distributed (non-IID) data. Then, the computation and communication latency and the convergence analysis of the proposed FL framework are mathematically ana
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35270;&#35273;&#24863;&#30693;&#12289;&#35821;&#35328;&#21644;&#21475;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#20302;&#32500;&#27010;&#24565;&#23884;&#20837;&#31639;&#27861;&#65292;&#23558;&#20154;&#31867;&#32463;&#39564;&#19982;&#33258;&#21160;&#26426;&#22120;&#30456;&#20284;&#24230;&#26680;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#21475;&#21619;&#20998;&#31867;&#65292;&#24182;&#19982;&#20154;&#31867;&#21475;&#21619;&#30693;&#35273;&#30456;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2308.16900</link><description>&lt;p&gt;
&#23398;&#20064;&#21697;&#21619;&#65306;&#19968;&#20010;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning to Taste: A Multimodal Wine Dataset. (arXiv:2308.16900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35270;&#35273;&#24863;&#30693;&#12289;&#35821;&#35328;&#21644;&#21475;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#20302;&#32500;&#27010;&#24565;&#23884;&#20837;&#31639;&#27861;&#65292;&#23558;&#20154;&#31867;&#32463;&#39564;&#19982;&#33258;&#21160;&#26426;&#22120;&#30456;&#20284;&#24230;&#26680;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#21475;&#21619;&#20998;&#31867;&#65292;&#24182;&#19982;&#20154;&#31867;&#21475;&#21619;&#30693;&#35273;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;WineSensed&#65292;&#29992;&#20110;&#30740;&#31350;&#35270;&#35273;&#24863;&#30693;&#12289;&#35821;&#35328;&#21644;&#21475;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;89.7&#19975;&#24352;&#33889;&#33796;&#37202;&#26631;&#31614;&#22270;&#29255;&#21644;82.4&#19975;&#26465;&#26469;&#33258;Vivino&#24179;&#21488;&#30340;&#33889;&#33796;&#37202;&#35780;&#35770;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#36229;&#36807;35&#19975;&#20010;&#29420;&#29305;&#30340;&#24180;&#20221;&#65292;&#38468;&#24102;&#20102;&#24180;&#20221;&#12289;&#20135;&#22320;&#12289;&#35780;&#20998;&#12289;&#37202;&#31934;&#21547;&#37327;&#12289;&#20215;&#26684;&#21644;&#33889;&#33796;&#32452;&#25104;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#21697;&#37202;&#23454;&#39564;&#23545;&#37096;&#20998;&#25968;&#25454;&#36827;&#34892;&#20102;&#32454;&#31890;&#24230;&#30340;&#21475;&#21619;&#27880;&#37322;&#65292;&#20849;&#26377;256&#21517;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#26681;&#25454;&#21475;&#21619;&#30340;&#30456;&#20284;&#24615;&#23545;&#33889;&#33796;&#37202;&#36827;&#34892;&#25490;&#24207;&#65292;&#24471;&#21040;&#20102;&#36229;&#36807;5&#21315;&#20010;&#37197;&#23545;&#30340;&#21475;&#21619;&#36317;&#31163;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#32500;&#27010;&#24565;&#23884;&#20837;&#31639;&#27861;&#65292;&#23558;&#20154;&#31867;&#32463;&#39564;&#19982;&#33258;&#21160;&#26426;&#22120;&#30456;&#20284;&#24230;&#26680;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20010;&#20849;&#20139;&#30340;&#27010;&#24565;&#23884;&#20837;&#31354;&#38388;&#22312;&#31895;&#31890;&#24230;&#21475;&#21619;&#20998;&#31867;&#65288;&#37202;&#31934;&#21547;&#37327;&#65292;&#22269;&#23478;&#65292;&#33889;&#33796;&#65292;&#20215;&#26684;&#65292;&#35780;&#20998;&#65289;&#19978;&#25913;&#36827;&#65292;&#24182;&#19988;&#19982;&#22797;&#26434;&#30340;&#20154;&#31867;&#21475;&#21619;&#30693;&#35273;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present WineSensed, a large multimodal wine dataset for studying the relations between visual perception, language, and flavor. The dataset encompasses 897k images of wine labels and 824k reviews of wines curated from the Vivino platform. It has over 350k unique vintages, annotated with year, region, rating, alcohol percentage, price, and grape composition. We obtained fine-grained flavor annotations on a subset by conducting a wine-tasting experiment with 256 participants who were asked to rank wines based on their similarity in flavor, resulting in more than 5k pairwise flavor distances. We propose a low-dimensional concept embedding algorithm that combines human experience with automatic machine similarity kernels. We demonstrate that this shared concept embedding space improves upon separate embedding spaces for coarse flavor classification (alcohol percentage, country, grape, price, rating) and aligns with the intricate human perception of flavor.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26469;&#25554;&#20540;&#23665;&#21306;&#22825;&#27668;&#39044;&#25253;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#35266;&#27979;&#25968;&#25454;&#21644;&#21608;&#22260;&#24179;&#21407;&#30340;&#39044;&#25253;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#22320;&#24418;&#20013;&#25968;&#20540;&#27169;&#25311;&#31934;&#24230;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.13983</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#23665;&#21306;&#22825;&#27668;&#39044;&#25253;&#30340;&#25554;&#20540;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Interpolation of mountain weather forecasts by machine learning. (arXiv:2308.13983v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26469;&#25554;&#20540;&#23665;&#21306;&#22825;&#27668;&#39044;&#25253;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#35266;&#27979;&#25968;&#25454;&#21644;&#21608;&#22260;&#24179;&#21407;&#30340;&#39044;&#25253;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#22320;&#24418;&#20013;&#25968;&#20540;&#27169;&#25311;&#31934;&#24230;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#25968;&#20540;&#27169;&#25311;&#26041;&#27861;&#30340;&#36827;&#23637;&#25552;&#39640;&#20102;&#22825;&#27668;&#39044;&#25253;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#22320;&#24418;&#22914;&#23665;&#22320;&#22320;&#21306;&#65292;&#30001;&#20110;&#25968;&#20540;&#27169;&#25311;&#20013;&#20351;&#29992;&#20102;&#20960;&#20844;&#37324;&#24179;&#26041;&#30340;&#32593;&#26684;&#65292;&#31934;&#24230;&#20250;&#38477;&#20302;&#12290;&#34429;&#28982;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30452;&#25509;&#24212;&#29992;&#38590;&#20197;&#21033;&#29992;&#29289;&#29702;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#24403;&#21069;&#35266;&#27979;&#25968;&#25454;&#21644;&#21608;&#22260;&#24179;&#21407;&#30340;&#39044;&#25253;&#25968;&#25454;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#8220;&#25554;&#20540;&#8221;&#26410;&#26469;&#23665;&#21306;&#30340;&#22825;&#27668;&#12290;&#36890;&#24120;&#65292;&#22825;&#27668;&#39044;&#27979;&#20381;&#36182;&#20110;&#25968;&#20540;&#27169;&#25311;&#65292;&#22240;&#27492;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#34987;&#35270;&#20026;&#38388;&#25509;&#34701;&#21512;&#25968;&#20540;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#36824;&#30740;&#31350;&#20102;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in numerical simulation methods based on physical models have enhanced the accuracy of weather forecasts. However, the precision diminishes in complex terrains like mountainous regions due to the several kilometers square grid used in numerical simulations. While statistical machine learning has also significantly advanced, its direct application is difficult to utilize physics knowledge. This paper proposes a method that employs machine learning to ``interpolate'' future weather in mountainous regions using current observed data and forecast data from surrounding plains. Generally, weather prediction relies on numerical simulations, so this approach can be considered a hybrid method that indirectly merges numerical simulation and machine learning. The use of binary cross-entropy in precipitation prediction is also examined.
&lt;/p&gt;</description></item><item><title>FoX&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#65292;&#24182;&#24341;&#23548;&#26234;&#33021;&#20307;&#22312;&#19981;&#21516;&#24418;&#25104;&#20013;&#35775;&#38382;&#26377;&#24847;&#20041;&#30340;&#29366;&#24577;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11272</link><description>&lt;p&gt;
FoX:&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24418;&#25104;&#24863;&#30693;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
FoX: Formation-aware exploration in multi-agent reinforcement learning. (arXiv:2308.11272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11272
&lt;/p&gt;
&lt;p&gt;
FoX&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#65292;&#24182;&#24341;&#23548;&#26234;&#33021;&#20307;&#22312;&#19981;&#21516;&#24418;&#25104;&#20013;&#35775;&#38382;&#26377;&#24847;&#20041;&#30340;&#29366;&#24577;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064; (MARL) &#22312;&#21508;&#31181;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26234;&#33021;&#20307;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#32780;&#25351;&#25968;&#22686;&#38271;&#30340;&#25506;&#32034;&#31354;&#38388;&#65292;&#25506;&#32034;&#20173;&#28982;&#26159;MARL&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#35299;&#20915;&#25506;&#32034;&#31354;&#38388;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#25506;&#32034;&#31354;&#38388;&#19978;&#23450;&#20041;&#20102;&#19968;&#20010;&#22522;&#20110;&#24418;&#25104;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#24182;&#26088;&#22312;&#36890;&#36807;&#20165;&#25506;&#32034;&#19981;&#21516;&#24418;&#24335;&#30340;&#26377;&#24847;&#20041;&#29366;&#24577;&#26469;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24418;&#25104;&#24863;&#30693;&#25506;&#32034; (FoX) &#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#23548;&#37096;&#20998;&#21487;&#35266;&#27979;&#26234;&#33021;&#20307;&#20973;&#20511;&#33258;&#36523;&#35266;&#27979;&#20449;&#24687;&#20805;&#20998;&#20102;&#35299;&#20854;&#24403;&#21069;&#24418;&#25104;&#65292;&#40723;&#21169;&#20182;&#20204;&#35775;&#38382;&#19981;&#21516;&#24418;&#25104;&#20013;&#30340;&#29366;&#24577;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;FoX&#26694;&#26550;&#22312;Google Research Football&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;MARL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep multi-agent reinforcement learning (MARL) has gained significant popularity due to its success in various cooperative multi-agent tasks. However, exploration still remains a challenging problem in MARL due to the partial observability of the agents and the exploration space that can grow exponentially as the number of agents increases. Firstly, in order to address the scalability issue of the exploration space, we define a formation-based equivalence relation on the exploration space and aim to reduce the search space by exploring only meaningful states in different formations. Then, we propose a novel formation-aware exploration (FoX) framework that encourages partially observable agents to visit the states in diverse formations by guiding them to be well aware of their current formation solely based on their own observations. Numerical results show that the proposed FoX framework significantly outperforms the state-of-the-art MARL algorithms on Google Research Football
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#31574;&#30053;&#26799;&#24230;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#22810;&#36718;&#23545;&#35805;&#23545;&#40784;&#31574;&#30053;&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#31579;&#36873;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#25552;&#31034;&#38598;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.07272</link><description>&lt;p&gt;
&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#31163;&#25955;&#25552;&#31034;&#29983;&#25104;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#23545;&#35805;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Generation for Few-shot Learning. (arXiv:2308.07272v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#31574;&#30053;&#26799;&#24230;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#22810;&#36718;&#23545;&#35805;&#23545;&#40784;&#31574;&#30053;&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#31579;&#36873;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#25552;&#31034;&#38598;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23569;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#33539;&#24335;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#26469;&#35774;&#35745;&#22522;&#26412;&#25552;&#31034;&#38598;&#24182;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#65292;&#36825;&#26082;&#36153;&#26102;&#21448;&#20302;&#25928;&#65292;&#32780;&#19988;&#20027;&#35266;&#24615;&#36739;&#24378;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#36830;&#32493;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;PLMs&#30340;&#26799;&#24230;&#20449;&#24687;&#26469;&#25913;&#36827;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#21487;&#35835;&#24615;&#21644;&#36890;&#29992;&#24615;&#20302;&#24120;&#24120;&#26159;&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#31574;&#30053;&#26799;&#24230;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;($DP_2O$)&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-4&#30340;&#22810;&#36718;&#23545;&#35805;&#23545;&#40784;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#35835;&#24615;&#25552;&#31034;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25552;&#31034;&#31579;&#36873;&#24230;&#37327;&#65292;&#20197;&#32447;&#24615;&#22797;&#26434;&#24230;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;(RL)&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based pre-trained language models (PLMs) paradigm have succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization ($DP_2O$) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework ba
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20892;&#23398;&#30740;&#31350;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#36890;&#36807;&#25972;&#21512;&#38543;&#26426;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#23398;&#20064;&#33021;&#21147;&#65292;&#23454;&#29616;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.06399</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#23398;&#20064;&#20855;&#26377;&#24322;&#26500;&#20892;&#19994;&#25968;&#25454;&#38598;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering. (arXiv:2308.06399v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20892;&#23398;&#30740;&#31350;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#36890;&#36807;&#25972;&#21512;&#38543;&#26426;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#23398;&#20064;&#33021;&#21147;&#65292;&#23454;&#29616;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#22810;&#26679;&#20294;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#20013;&#65292;&#20854;&#20013;&#21327;&#21464;&#37327;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#32852;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#22312;&#21253;&#25324;&#20892;&#23398;&#30740;&#31350;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#37117;&#24456;&#26222;&#36941;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#20351;&#29992;&#23618;&#27425;&#27169;&#22411;&#65292;&#20063;&#34987;&#31216;&#20026;&#22810;&#23618;&#27169;&#22411;&#65292;&#26469;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#65292;&#24182;&#36866;&#24212;&#23427;&#20204;&#30340;&#19981;&#21516;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32467;&#26500;&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#24322;&#36136;&#24615;&#65292;&#22240;&#20026;&#21464;&#37327;&#36890;&#24120;&#24418;&#25104;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#12290;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BNs&#65289;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#26469;&#27169;&#25311;&#36825;&#31181;&#20851;&#31995;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#38543;&#26426;&#25928;&#24212;&#25972;&#21512;&#21040;BN&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#25968;&#25454;&#12290;&#26469;&#33258;&#30495;&#23454;&#20892;&#23398;&#35797;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.  Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#25805;&#20316;&#22120;&#32593;&#32476;&#26469;&#27169;&#25311;&#22312;&#30495;&#23454;&#30340;3D&#22330;&#26223;&#20013;&#24102;&#26377;&#31227;&#21160;&#28304;&#30340;&#22768;&#38899;&#20256;&#25773;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#32039;&#20945;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#24555;&#36895;&#39044;&#27979;&#22768;&#38899;&#20256;&#25773;&#65292;&#36991;&#20813;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#33033;&#20914;&#21709;&#24212;&#30340;&#31163;&#32447;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2308.05141</link><description>&lt;p&gt;
&#22312;&#21442;&#25968;&#21270;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#25805;&#20316;&#22120;&#22312;&#30495;&#23454;&#30340;&#20132;&#20114;&#24335;3D&#22330;&#26223;&#20013;&#27169;&#25311;&#22768;&#38899;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Sound propagation in realistic interactive 3D scenes with parameterized sources using deep neural operators. (arXiv:2308.05141v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#25805;&#20316;&#22120;&#32593;&#32476;&#26469;&#27169;&#25311;&#22312;&#30495;&#23454;&#30340;3D&#22330;&#26223;&#20013;&#24102;&#26377;&#31227;&#21160;&#28304;&#30340;&#22768;&#38899;&#20256;&#25773;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#32039;&#20945;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#24555;&#36895;&#39044;&#27979;&#22768;&#38899;&#20256;&#25773;&#65292;&#36991;&#20813;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#33033;&#20914;&#21709;&#24212;&#30340;&#31163;&#32447;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#20855;&#26377;&#31227;&#21160;&#28304;&#30340;&#19977;&#32500;&#34394;&#25311;&#25151;&#38388;&#20013;&#36827;&#34892;&#22768;&#38899;&#20256;&#25773;&#27169;&#25311;&#30340;&#25361;&#25112;&#65292;&#36825;&#22312;&#34394;&#25311;/&#22686;&#24378;&#29616;&#23454;&#12289;&#28216;&#25103;&#38899;&#39057;&#21644;&#31354;&#38388;&#35745;&#31639;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#12290;&#36890;&#36807;&#27714;&#35299;&#27874;&#21160;&#26041;&#31243;&#65292;&#21487;&#20197;&#25551;&#36848;&#34893;&#23556;&#21644;&#24178;&#28041;&#31561;&#27874;&#21160;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20256;&#32479;&#30340;&#25968;&#20540;&#31163;&#25955;&#21270;&#26041;&#27861;&#27169;&#25311;&#28041;&#21450;&#25968;&#30334;&#20010;&#28304;&#21644;&#25509;&#25910;&#22120;&#20301;&#32622;&#30340;&#27874;&#21160;&#26041;&#31243;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#20351;&#24471;&#20351;&#29992;&#31227;&#21160;&#28304;&#21050;&#28608;&#22768;&#22330;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#25805;&#20316;&#22120;&#32593;&#32476;&#26469;&#36924;&#36817;&#32447;&#24615;&#27874;&#21160;&#26041;&#31243;&#25805;&#20316;&#22120;&#12290;&#36825;&#20351;&#24471;&#33021;&#22815;&#24555;&#36895;&#39044;&#27979;&#22312;&#20855;&#26377;&#31227;&#21160;&#28304;&#30340;&#30495;&#23454;&#19977;&#32500;&#22768;&#23398;&#22330;&#26223;&#20013;&#30340;&#22768;&#38899;&#20256;&#25773;&#65292;&#36798;&#21040;&#27627;&#31186;&#32423;&#30340;&#35745;&#31639;&#12290;&#36890;&#36807;&#23398;&#20064;&#32039;&#20945;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#20026;&#25152;&#26377;&#30456;&#20851;&#30340;&#28304;/&#21548;&#32773;&#23545;&#35745;&#31639;&#21644;&#23384;&#20648;&#33033;&#20914;&#21709;&#24212;&#30340;&#31163;&#32447;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21253;&#21547;&#21508;&#31181;&#22797;&#26434;&#22330;&#26223;&#20960;&#20309;&#24418;&#29366;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#21442;&#32771;&#32467;&#26524;&#36798;&#25104;&#20102;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the challenge of sound propagation simulations in $3$D virtual rooms with moving sources, which have applications in virtual/augmented reality, game audio, and spatial computing. Solutions to the wave equation can describe wave phenomena such as diffraction and interference. However, simulating them using conventional numerical discretization methods with hundreds of source and receiver positions is intractable, making stimulating a sound field with moving sources impractical. To overcome this limitation, we propose using deep operator networks to approximate linear wave-equation operators. This enables the rapid prediction of sound propagation in realistic 3D acoustic scenes with moving sources, achieving millisecond-scale computations. By learning a compact surrogate model, we avoid the offline calculation and storage of impulse responses for all relevant source/listener pairs. Our experiments, including various complex scene geometries, show good agreement with reference 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Fuzz4All&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#38024;&#23545;&#35768;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#35821;&#35328;&#21644;&#36825;&#20123;&#35821;&#35328;&#30340;&#35768;&#22810;&#19981;&#21516;&#21151;&#33021;&#36827;&#34892;&#27169;&#31946;&#27979;&#35797;&#30340;&#36890;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2308.04748</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36890;&#29992;&#27169;&#31946;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Universal Fuzzing via Large Language Models. (arXiv:2308.04748v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fuzz4All&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#38024;&#23545;&#35768;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#35821;&#35328;&#21644;&#36825;&#20123;&#35821;&#35328;&#30340;&#35768;&#22810;&#19981;&#21516;&#21151;&#33021;&#36827;&#34892;&#27169;&#31946;&#27979;&#35797;&#30340;&#36890;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#27979;&#35797;&#22312;&#21457;&#29616;&#21508;&#31181;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#21644;&#33030;&#24369;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#25509;&#21463;&#32534;&#31243;&#25110;&#24418;&#24335;&#35821;&#35328;&#20316;&#20026;&#36755;&#20837;&#30340;&#27979;&#35797;&#31995;&#32479;&#65288;SUTs&#65289;&#65292;&#22914;&#32534;&#35793;&#22120;&#65292;&#36816;&#34892;&#26102;&#24341;&#25806;&#65292;&#32422;&#26463;&#27714;&#35299;&#22120;&#21644;&#20855;&#26377;&#21487;&#35775;&#38382;API&#30340;&#36719;&#20214;&#24211;&#65292;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#36719;&#20214;&#24320;&#21457;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#29616;&#26377;&#27169;&#31946;&#27979;&#35797;&#24037;&#20855;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#35821;&#35328;&#65292;&#22240;&#27492;&#26080;&#27861;&#36731;&#26131;&#24212;&#29992;&#20110;&#20854;&#20182;&#35821;&#35328;&#29978;&#33267;&#21516;&#19968;&#35821;&#35328;&#30340;&#20854;&#20182;&#29256;&#26412;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#27169;&#31946;&#27979;&#35797;&#24037;&#20855;&#29983;&#25104;&#30340;&#36755;&#20837;&#36890;&#24120;&#23616;&#38480;&#20110;&#36755;&#20837;&#35821;&#35328;&#30340;&#29305;&#23450;&#21151;&#33021;&#65292;&#22240;&#27492;&#24456;&#38590;&#25581;&#31034;&#19982;&#20854;&#20182;&#21151;&#33021;&#30456;&#20851;&#30340;&#28431;&#27934;&#25110;&#26032;&#21151;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Fuzz4All&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#29992;&#30340;&#27169;&#31946;&#27979;&#35797;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#38024;&#23545;&#35768;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#35821;&#35328;&#21644;&#36825;&#20123;&#35821;&#35328;&#30340;&#35768;&#22810;&#19981;&#21516;&#21151;&#33021;&#36827;&#34892;&#27979;&#35797;&#12290;Fuzz4All&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#36755;&#20837;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input genera
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LSTM&#21644;DDPM&#30456;&#32467;&#21512;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.16149</link><description>&lt;p&gt;
&#26234;&#33021;&#30005;&#32593;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#30340;LSTM-DDPM&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid. (arXiv:2307.16149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16149
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LSTM&#21644;DDPM&#30456;&#32467;&#21512;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#65288;ETD&#65289;&#21644;&#33021;&#37327;&#28040;&#32791;&#39044;&#27979;&#65288;ECF&#65289;&#26159;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#25361;&#25112;&#12290;&#20849;&#21516;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#30830;&#20445;&#31995;&#32479;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;ETD&#21644;ECF&#30340;&#30456;&#20114;&#20851;&#32852;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#36755;&#20837;&#37325;&#26500;&#21644;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#21644;&#39044;&#27979;&#35823;&#24046;&#30340;&#26041;&#27861;&#30456;&#20114;&#34917;&#20805;&#65292;&#21487;&#20197;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;ETD&#21644;ECF&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#38598;&#25104;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;ETD&#24615;&#33021;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#21040;&#22522;&#20934;&#26041;&#27861;&#26410;&#33021;&#26816;&#27979;&#21040;&#30340;&#33021;&#37327;&#30423;&#31363;&#25915;&#20987;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;ETD&#21644;ECF&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy theft detection (ETD) and energy consumption forecasting (ECF) are two interconnected challenges in smart grid systems. Addressing these issues collectively is crucial for ensuring system security. This paper addresses the interconnected challenges of ETD and ECF in smart grid systems. The proposed solution combines long short-term memory (LSTM) and a denoising diffusion probabilistic model (DDPM) to generate input reconstruction and forecasting. By leveraging the reconstruction and forecasting errors, the system identifies instances of energy theft, with the methods based on reconstruction error and forecasting error complementing each other in detecting different types of attacks. Through extensive experiments on real-world and synthetic datasets, the proposed scheme outperforms baseline methods in ETD and ECF problems. The ensemble method significantly enhances ETD performance, accurately detecting energy theft attacks that baseline methods fail to detect. The research offers
&lt;/p&gt;</description></item><item><title>FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13716</link><description>&lt;p&gt;
FedDRL: &#19968;&#31181;&#22522;&#20110;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13716
&lt;/p&gt;
&lt;p&gt;
FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#26679;&#26412;&#25968;&#37327;&#35745;&#31639;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#36825;&#20010;&#22266;&#23450;&#26435;&#37325;&#20540;&#26469;&#34701;&#21512;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23548;&#33268;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#36129;&#29486;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#26679;&#26412;&#37327;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#23458;&#25143;&#31471;&#25925;&#24847;&#19978;&#20256;&#20302;&#36136;&#37327;&#25110;&#24694;&#24847;&#27169;&#22411;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32858;&#21512;&#23558;&#20005;&#37325;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#27809;&#26377;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDRL&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36807;&#28388;&#25481;&#24694;&#24847;&#27169;&#22411;&#65292;&#24182;&#36873;&#25321;&#21487;&#20449;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#19982;&#27169;&#22411;&#34701;&#21512;&#12290;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#65292;FedDRL&#31639;&#27861;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#21487;&#20449;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#24182;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and ag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26080;&#20851;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21442;&#25968;&#37327;&#21270;&#26041;&#27861;QuIP&#65292;&#36890;&#36807;&#20351;&#26435;&#37325;&#21644;Hessian&#30697;&#38453;&#19982;&#22352;&#26631;&#36724;&#19981;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#37327;&#21270;&#32467;&#26524;&#12290;&#32463;&#36807;&#32463;&#39564;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29616;&#26377;&#30340;&#37327;&#21270;&#31639;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#22312;&#20165;&#20351;&#29992;&#20004;&#27604;&#29305;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#21487;&#34892;&#30340;LLM&#37327;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13304</link><description>&lt;p&gt;
QuIP&#65306;&#20855;&#26377;&#20445;&#35777;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;2&#27604;&#29305;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QuIP: 2-Bit Quantization of Large Language Models With Guarantees. (arXiv:2307.13304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26080;&#20851;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21442;&#25968;&#37327;&#21270;&#26041;&#27861;QuIP&#65292;&#36890;&#36807;&#20351;&#26435;&#37325;&#21644;Hessian&#30697;&#38453;&#19982;&#22352;&#26631;&#36724;&#19981;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#37327;&#21270;&#32467;&#26524;&#12290;&#32463;&#36807;&#32463;&#39564;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29616;&#26377;&#30340;&#37327;&#21270;&#31639;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#22312;&#20165;&#20351;&#29992;&#20004;&#27604;&#29305;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#21487;&#34892;&#30340;LLM&#37327;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#35757;&#32451;&#21518;&#21442;&#25968;&#37327;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26080;&#20851;&#22788;&#29702;&#65288;QuIP&#65289;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#20197;&#19979;&#35265;&#35299;&#65306;&#37327;&#21270;&#20174;&#19981;&#30456;&#20851;&#30340;&#26435;&#37325;&#21644; Hessian &#30697;&#38453;&#20013;&#25910;&#30410;&#65292;&#21363;&#36890;&#36807;&#20934;&#30830;&#22320;&#23558;&#23427;&#20204;&#33293;&#20837;&#20026;&#19982;&#22352;&#26631;&#36724;&#19981;&#23545;&#40784;&#30340;&#26041;&#21521;&#65292;&#20351;&#24471;&#33719;&#21462;&#37325;&#35201;&#30340;&#37327;&#21270;&#32467;&#26524;&#12290;QuIP &#21253;&#21547;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#26368;&#23567;&#21270;&#20108;&#27425;&#36817;&#20284;&#30446;&#26631;&#30340;&#33258;&#36866;&#24212;&#33293;&#20837;&#36807;&#31243;&#65307;&#65288;2&#65289;&#36890;&#36807;&#19982;&#38543;&#26426;&#27491;&#20132;&#30697;&#38453;&#30456;&#20056;&#26469;&#30830;&#20445;&#26435;&#37325;&#21644; Hessian &#26080;&#20851;&#30340;&#39640;&#25928;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#31532;&#19968;&#27425;&#38024;&#23545; LLM &#35268;&#27169;&#30340;&#37327;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#19988;&#35777;&#26126;&#25105;&#20204;&#30340;&#29702;&#35770;&#20063;&#36866;&#29992;&#20110;&#29616;&#26377;&#26041;&#27861; OPTQ&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26080;&#20851;&#39044;&#22788;&#29702;&#25913;&#21892;&#20102;&#29616;&#26377;&#30340;&#22810;&#20010;&#37327;&#21270;&#31639;&#27861;&#65292;&#24182;&#39318;&#27425;&#23454;&#29616;&#20102;&#20165;&#20351;&#29992;&#27599;&#20010;&#26435;&#37325;2&#27604;&#29305;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#20351;&#29992;&#22270;&#22686;&#24378;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#20174;&#24322;&#26500;&#22270;&#20013;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#12289;&#22270;&#27880;&#24847;&#21147;&#21644;&#20851;&#31995;&#31867;&#22411;&#32771;&#34385;&#65292;&#20248;&#21270;&#20102;&#23454;&#20307;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.10443</link><description>&lt;p&gt;
&#20351;&#29992;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#23558;&#24322;&#26500;&#22270;&#19982;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#30340;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model. (arXiv:2307.10443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#20351;&#29992;&#22270;&#22686;&#24378;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#20174;&#24322;&#26500;&#22270;&#20013;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#12289;&#22270;&#27880;&#24847;&#21147;&#21644;&#20851;&#31995;&#31867;&#22411;&#32771;&#34385;&#65292;&#20248;&#21270;&#20102;&#23454;&#20307;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36755;&#20837;&#24207;&#21015;&#20013;&#32570;&#23569;&#26174;&#24335;&#30693;&#35782;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#26469;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#23427;&#21033;&#29992;&#22686;&#24378;&#22270;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#30001;&#24322;&#26500;&#22270;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#12290;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;&#21333;&#35789;&#26631;&#35760;&#30340;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#65292;&#23545;&#23454;&#20307;&#26631;&#35760;&#30340;&#22270;&#27880;&#24847;&#21147;&#65292;&#23454;&#20307;&#26631;&#35760;&#23545;&#30456;&#20851;&#32852;&#30340;&#26631;&#35760;&#26174;&#31034;&#24378;&#28872;&#30340;&#27880;&#24847;&#21147;&#32780;&#23545;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#26174;&#31034;&#36739;&#24369;&#30340;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32771;&#34385;&#27599;&#20010;&#23454;&#20307;&#26631;&#35760;&#19982;&#21333;&#35789;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#31867;&#22411;&#12290;&#36825;&#26679;&#65292;&#22914;&#26524;&#23384;&#22312;&#20851;&#31995;&#65292;&#21017;&#21487;&#20197;&#20248;&#21270;&#20004;&#32773;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#29305;&#27530;&#30340;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant progress made by transformer models in machine reading comprehension tasks, they still face limitations in handling complex reasoning tasks due to the absence of explicit knowledge in the input sequence. This paper proposes a novel attention pattern to overcome this limitation, which integrates reasoning knowledge derived from a heterogeneous graph into the transformer architecture using a graph-enhanced self-attention mechanism. The proposed attention pattern comprises three key elements: global-local attention for word tokens, graph attention for entity tokens that exhibit strong attention towards tokens connected in the graph as opposed to those unconnected, and the consideration of the type of relationship between each entity token and word token. This results in optimized attention between the two if a relationship exists. The pattern is coupled with special relative position labels, allowing it to integrate with LUKE's entity-aware self-attention mechanism
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#22788;&#29702;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#30340;&#21152;&#36895;&#20248;&#21270;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;SLQR&#21644;OLQR&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;LQR&#24615;&#33021;&#20934;&#21017;&#30340;Lipschitz Hessian&#29305;&#24615;&#65292;&#20026;&#29616;&#20195;&#20248;&#21270;&#25216;&#26415;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2307.03590</link><description>&lt;p&gt;
&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#30340;&#21152;&#36895;&#20248;&#21270;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
Accelerated Optimization Landscape of Linear-Quadratic Regulator. (arXiv:2307.03590v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#22788;&#29702;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#30340;&#21152;&#36895;&#20248;&#21270;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;SLQR&#21644;OLQR&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;LQR&#24615;&#33021;&#20934;&#21017;&#30340;Lipschitz Hessian&#29305;&#24615;&#65292;&#20026;&#29616;&#20195;&#20248;&#21270;&#25216;&#26415;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#26159;&#26368;&#20248;&#25511;&#21046;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22788;&#29702;LQR&#38382;&#39064;&#30340;&#19968;&#38454;&#21152;&#36895;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#20998;&#21035;&#32473;&#20986;&#20102;SLQR&#21644;OLQR&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LQR&#24615;&#33021;&#20934;&#21017;&#30340;Lipschitz Hessian&#29305;&#24615;&#65292;&#36825;&#23545;&#20110;&#24212;&#29992;&#29616;&#20195;&#20248;&#21270;&#25216;&#26415;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23545;&#20110;SLQR&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#28151;&#21512;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#35777;&#26126;&#20854;&#35299;&#36712;&#36857;&#25351;&#25968;&#32423;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear-quadratic regulator (LQR) is a landmark problem in the field of optimal control, which is the concern of this paper. Generally, LQR is classified into state-feedback LQR (SLQR) and output-feedback LQR (OLQR) based on whether the full state is obtained. It has been suggested in existing literature that both the SLQR and the OLQR could be viewed as \textit{constrained nonconvex matrix optimization} problems in which the only variable to be optimized is the feedback gain matrix. In this paper, we introduce a first-order accelerated optimization framework of handling the LQR problem, and give its convergence analysis for the cases of SLQR and OLQR, respectively.  Specifically, a Lipschiz Hessian property of LQR performance criterion is presented, which turns out to be a crucial property for the application of modern optimization techniques. For the SLQR problem, a continuous-time hybrid dynamic system is introduced, whose solution trajectory is shown to converge exponentially to the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.03393</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#22240;&#20854;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20197;&#25991;&#26412;&#33410;&#28857;&#23646;&#24615;&#20026;&#20027;&#30340;&#22270;&#23398;&#20064;&#26368;&#27969;&#34892;&#30340;&#27969;&#31243;&#20027;&#35201;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#24182;&#21033;&#29992;&#27973;&#23618;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#21021;&#22987;&#33410;&#28857;&#34920;&#31034;&#65292;&#20294;&#23384;&#22312;&#36890;&#29992;&#30693;&#35782;&#21644;&#28145;&#21051;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35777;&#26126;&#20855;&#26377;&#24191;&#27867;&#30340;&#24120;&#35782;&#21644;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#24050;&#32463;&#39072;&#35206;&#20102;&#29616;&#26377;&#30340;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20004;&#31181;&#21487;&#33021;&#30340;&#27969;&#31243;&#65306;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#21644;LLMs&#20316;&#20026;&#39044;&#27979;&#22120;&#12290;&#21069;&#32773;&#21033;&#29992;LLMs&#36890;&#36807;&#20854;&#28023;&#37327;&#30693;&#35782;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;GNNs&#29983;&#25104;&#39044;&#27979;&#12290;&#21518;&#32773;&#35797;&#22270;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#22120;&#21644;&#31867;&#21407;&#22411;&#32047;&#31215;&#26469;&#36991;&#20813;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02251</link><description>&lt;p&gt;
RanPAC: &#38543;&#26426;&#25237;&#24433;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
RanPAC: Random Projections and Pre-trained Models for Continual Learning. (arXiv:2307.02251v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02251
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#22120;&#21644;&#31867;&#21407;&#22411;&#32047;&#31215;&#26469;&#36991;&#20813;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26088;&#22312;&#22312;&#38750;&#31283;&#24577;&#25968;&#25454;&#27969;&#20013;&#22686;&#37327;&#23398;&#20064;&#19981;&#21516;&#30340;&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#65289;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#26087;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#37117;&#33268;&#21147;&#20110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#26085;&#30410;&#37325;&#35201;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#38656;&#27714;&#20013;&#37117;&#21487;&#20197;&#21033;&#29992;&#20855;&#26377;&#20449;&#24687;&#20016;&#23500;&#34920;&#31034;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#24050;&#32463;&#25506;&#32034;&#20102;&#19968;&#20123;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#35201;&#20040;&#30452;&#25509;&#21033;&#29992;&#39044;&#25552;&#21462;&#30340;&#29305;&#24449;&#65288;&#36825;&#20351;&#24471;&#24357;&#21512;&#20998;&#24067;&#24046;&#36317;&#21464;&#24471;&#22256;&#38590;&#65289;&#65292;&#35201;&#20040;&#34701;&#20837;&#36866;&#37197;&#22120;&#65288;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#36951;&#24536;&#38382;&#39064;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#32780;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#37492;&#20110;&#36951;&#24536;&#38382;&#39064;&#21457;&#29983;&#22312;&#21442;&#25968;&#26356;&#26032;&#26399;&#38388;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#30340;&#38543;&#26426;&#25237;&#24433;&#22120;&#21644;&#31867;&#21407;&#22411;&#32047;&#31215;&#26469;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27880;&#20837;&#20102;&#19968;&#20010;&#22266;&#23450;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) aims to incrementally learn different tasks (such as classification) in a non-stationary data stream without forgetting old ones. Most CL works focus on tackling catastrophic forgetting under a learning-from-scratch paradigm. However, with the increasing prominence of foundation models, pre-trained models equipped with informative representations have become available for various downstream requirements. Several CL methods based on pre-trained models have been explored, either utilizing pre-extracted features directly (which makes bridging distribution gaps challenging) or incorporating adaptors (which may be subject to forgetting). In this paper, we propose a concise and effective approach for CL with pre-trained models. Given that forgetting occurs during parameter updating, we contemplate an alternative approach that exploits training-free random projectors and class-prototype accumulation, which thus bypasses the issue. Specifically, we inject a frozen Rando
&lt;/p&gt;</description></item><item><title>CardiGraphormer&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20445;&#25345;&#22522;&#25968;&#27880;&#24847;&#21147;&#65292;&#39072;&#35206;&#20102;&#33647;&#29289;&#21457;&#29616;&#30340;&#26041;&#24335;&#12290;&#23427;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#20998;&#23376;&#25351;&#32441;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#22312;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#21644;&#25191;&#34892;&#21508;&#31181;&#19982;&#22270;&#32467;&#26500;&#30456;&#20851;&#30340;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.00859</link><description>&lt;p&gt;
CardiGraphormer: &#25581;&#31034;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#39072;&#35206;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery. (arXiv:2307.00859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00859
&lt;/p&gt;
&lt;p&gt;
CardiGraphormer&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20445;&#25345;&#22522;&#25968;&#27880;&#24847;&#21147;&#65292;&#39072;&#35206;&#20102;&#33647;&#29289;&#21457;&#29616;&#30340;&#26041;&#24335;&#12290;&#23427;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#20998;&#23376;&#25351;&#32441;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#22312;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#21644;&#25191;&#34892;&#21508;&#31181;&#19982;&#22270;&#32467;&#26500;&#30456;&#20851;&#30340;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#38420;&#30340;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#20013;&#65292;&#24050;&#30693;&#33647;&#29289;&#32422;&#26377;15,000&#31181;&#65292;&#20294;&#21482;&#26377;&#22823;&#32422;4,200&#31181;&#24471;&#21040;&#20102;&#25209;&#20934;&#65292;&#21270;&#23398;&#31354;&#38388;&#30340;&#32452;&#21512;&#24615;&#36136;&#25552;&#20379;&#20102;&#19968;&#39033;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#25104;&#20026;&#20102;&#26377;&#21147;&#30340;&#20249;&#20276;&#65292;&#20256;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#20173;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CardiGraphormer&#65292;&#36825;&#26159;&#19968;&#31181;&#21010;&#26102;&#20195;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#20445;&#25345;&#22522;&#25968;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#39072;&#35206;&#33647;&#29289;&#21457;&#29616;&#12290;CardiGraphormer&#26159;Graphormer&#21644;&#20445;&#25345;&#22522;&#25968;&#27880;&#24847;&#21147;&#30340;&#26032;&#39062;&#32452;&#21512;&#65292;&#21033;&#29992;SSL&#23398;&#20064;&#26377;&#25928;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;GNN&#25552;&#21462;&#20998;&#23376;&#25351;&#32441;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#26102;&#38388;&#12290;&#23427;&#22312;&#22788;&#29702;&#20998;&#23376;&#32467;&#26500;&#31561;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#33021;&#25191;&#34892;&#19982;&#33410;&#28857;&#12289;&#33410;&#28857;&#23545;&#12289;&#23376;&#22270;&#25110;&#25972;&#20010;&#22270;&#32467;&#26500;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the expansive realm of drug discovery, with approximately 15,000 known drugs and only around 4,200 approved, the combinatorial nature of the chemical space presents a formidable challenge. While Artificial Intelligence (AI) has emerged as a powerful ally, traditional AI frameworks face significant hurdles. This manuscript introduces CardiGraphormer, a groundbreaking approach that synergizes self-supervised learning (SSL), Graph Neural Networks (GNNs), and Cardinality Preserving Attention to revolutionize drug discovery. CardiGraphormer, a novel combination of Graphormer and Cardinality Preserving Attention, leverages SSL to learn potent molecular representations and employs GNNs to extract molecular fingerprints, enhancing predictive performance and interpretability while reducing computation time. It excels in handling complex data like molecular structures and performs tasks associated with nodes, pairs of nodes, subgraphs, or entire graph structures. CardiGraphormer's potential a
&lt;/p&gt;</description></item><item><title>ENN&#26159;&#19968;&#31181;&#20855;&#26377;DCT&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#28608;&#27963;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#39640;&#24230;&#30340;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#22312;&#35299;&#37322;&#32593;&#32476;&#25910;&#25947;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#27599;&#20010;&#28608;&#27963;&#20989;&#25968;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#21709;&#24212;&#65292;&#21363;&#8220;&#20984;&#36215;&#8221;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.00673</link><description>&lt;p&gt;
ENN: &#19968;&#31181;&#20855;&#26377;DCT&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ENN: A Neural Network with DCT Adaptive Activation Functions. (arXiv:2307.00673v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00673
&lt;/p&gt;
&lt;p&gt;
ENN&#26159;&#19968;&#31181;&#20855;&#26377;DCT&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#28608;&#27963;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#39640;&#24230;&#30340;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#22312;&#35299;&#37322;&#32593;&#32476;&#25910;&#25947;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#27599;&#20010;&#28608;&#27963;&#20989;&#25968;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#21709;&#24212;&#65292;&#21363;&#8220;&#20984;&#36215;&#8221;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#39640;&#24230;&#21462;&#20915;&#20110;&#28608;&#27963;&#20989;&#25968;&#30340;&#24615;&#36136;&#65292;&#23613;&#31649;&#36825;&#20123;&#36890;&#24120;&#22312;&#35757;&#32451;&#38454;&#27573;&#34987;&#20551;&#23450;&#20026;&#39044;&#23450;&#20041;&#21644;&#22266;&#23450;&#30340;&#12290;&#22312;&#20449;&#21495;&#22788;&#29702;&#30340;&#35270;&#35282;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#8212;&#8212;&#34920;&#36798;&#31070;&#32463;&#32593;&#32476;(ENN)&#65292;&#20854;&#20013;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#20351;&#29992;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;(DCT)&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#36825;&#31181;&#21442;&#25968;&#21270;&#26041;&#27861;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#20445;&#25345;&#36739;&#20302;&#65292;&#36866;&#21512;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#26696;&#65292;&#24182;&#33021;&#36866;&#24212;&#19981;&#21516;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#28608;&#27963;&#20989;&#25968;&#26041;&#38754;&#20381;&#36182;&#20110;&#20449;&#21495;&#22788;&#29702;&#35270;&#35282;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#65292;&#20026;&#32593;&#32476;&#25552;&#20379;&#20102;&#39640;&#24230;&#30340;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#8220;&#20984;&#36215;&#8221;&#30340;&#27010;&#24565;&#26469;&#20026;&#32593;&#32476;&#22312;&#25910;&#25947;&#26102;&#30340;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#27599;&#20010;&#28608;&#27963;&#20989;&#25968;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#21709;&#24212;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35814;&#23613;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expressiveness of neural networks highly depends on the nature of the activation function, although these are usually assumed predefined and fixed during the training stage. Under a signal processing perspective, in this paper we present Expressive Neural Network (ENN), a novel model in which the non-linear activation functions are modeled using the Discrete Cosine Transform (DCT) and adapted using backpropagation during training. This parametrization keeps the number of trainable parameters low, is appropriate for gradient-based schemes, and adapts to different learning tasks. This is the first non-linear model for activation functions that relies on a signal processing perspective, providing high flexibility and expressiveness to the network. We contribute with insights in the explainability of the network at convergence by recovering the concept of bump, this is, the response of each activation function in the output space. Finally, through exhaustive experiments we show that th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#22240;&#26524;&#26426;&#21046;&#36716;&#21464;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#27880;&#20110;&#22312;&#30456;&#20851;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35782;&#21035;&#21151;&#33021;&#26426;&#21046;&#30340;&#21464;&#21270;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#25972;&#20010;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.17361</link><description>&lt;p&gt;
iSCAN&#65306;&#35782;&#21035;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#26426;&#21046;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models. (arXiv:2306.17361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#22240;&#26524;&#26426;&#21046;&#36716;&#21464;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#27880;&#20110;&#22312;&#30456;&#20851;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35782;&#21035;&#21151;&#33021;&#26426;&#21046;&#30340;&#21464;&#21270;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#25972;&#20010;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;(SCM)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#20197;&#34920;&#31034;&#22797;&#26434;&#31995;&#32479;&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#30495;&#27491;&#30340;&#24213;&#23618;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#32467;&#26500;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#20174;&#35266;&#27979;&#25968;&#25454;&#25110;&#24178;&#39044;&#25968;&#25454;&#20013;&#30830;&#23450;&#23427;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#35782;&#21035;&#30456;&#20851;SCM&#20043;&#38388;&#30340;&#22240;&#26524;&#26426;&#21046;&#30340;&#21464;&#21270;(&#36716;&#21464;)&#32780;&#19981;&#26159;&#24674;&#22797;&#25972;&#20010;&#24213;&#23618;DAG&#32467;&#26500;&#12290;&#20363;&#23376;&#21253;&#25324;&#20998;&#26512;&#20581;&#24247;&#21644;&#30284;&#30151;&#24739;&#32773;&#20043;&#38388;&#30340;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#32467;&#26500;&#21464;&#21270;&#65292;&#25110;&#32773;&#22312;&#19981;&#21516;&#32454;&#32990;&#29615;&#22659;&#19979;&#29702;&#35299;&#29983;&#29289;&#36884;&#24452;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#30456;&#21516;&#30340;&#21464;&#37327;&#38598;&#19978;&#35782;&#21035;&#20004;&#20010;&#25110;&#22810;&#20010;&#30456;&#20851;SCM&#20013;&#30340;$\textit{&#21151;&#33021;}$&#26426;&#21046;&#36716;&#21464;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#27599;&#20010;SCM&#30340;&#25972;&#20010;DAG&#32467;&#26500;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20551;&#35774;&#20351;&#29992;&#20102;&#20855;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#32447;&#24615;&#27169;&#22411;&#65307;&#32780;&#26412;&#25991;&#20013;&#25105;&#20204;&#21017;&#32771;&#34385;&#20102;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structural causal models (SCMs) are widely used in various disciplines to represent causal relationships among variables in complex systems. Unfortunately, the true underlying directed acyclic graph (DAG) structure is often unknown, and determining it from observational or interventional data remains a challenging task. However, in many situations, the end goal is to identify changes (shifts) in causal mechanisms between related SCMs rather than recovering the entire underlying DAG structure. Examples include analyzing gene regulatory network structure changes between healthy and cancerous individuals or understanding variations in biological pathways under different cellular contexts. This paper focuses on identifying $\textit{functional}$ mechanism shifts in two or more related SCMs over the same set of variables -$\textit{without estimating the entire DAG structure of each SCM}$. Prior work under this setting assumed linear models with Gaussian noises; instead, in this work we ass
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#27969;&#30340;&#33258;&#36866;&#24212;&#20271;&#24681;&#26031;&#22374;&#21464;&#21270;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#20934;&#30830;&#22320;&#35782;&#21035;&#21464;&#21270;&#21457;&#29983;&#30340;&#26102;&#38388;&#19982;&#23376;&#31354;&#38388;&#65292;&#24182;&#33021;&#22815;&#37327;&#21270;&#20005;&#37325;&#31243;&#24230;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12974</link><description>&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#27969;&#33258;&#36866;&#24212;&#20271;&#24681;&#26031;&#22374;&#21464;&#21270;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Adaptive Bernstein Change Detector for High-Dimensional Data Streams. (arXiv:2306.12974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#27969;&#30340;&#33258;&#36866;&#24212;&#20271;&#24681;&#26031;&#22374;&#21464;&#21270;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#20934;&#30830;&#22320;&#35782;&#21035;&#21464;&#21270;&#21457;&#29983;&#30340;&#26102;&#38388;&#19982;&#23376;&#31354;&#38388;&#65292;&#24182;&#33021;&#22815;&#37327;&#21270;&#20005;&#37325;&#31243;&#24230;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20998;&#26512;&#25968;&#25454;&#27969;&#26102;&#65292;&#21464;&#21270;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#24555;&#36895;&#21644;&#20934;&#30830;&#22320;&#26816;&#27979;&#21040;&#21464;&#21270;&#21487;&#20197;&#20351;&#30417;&#27979;&#21644;&#39044;&#27979;&#31995;&#32479;&#20570;&#20986;&#21453;&#24212;&#65292;&#20363;&#22914;&#21457;&#20986;&#35686;&#25253;&#25110;&#26356;&#26032;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#25968;&#25454;&#20013;&#26816;&#27979;&#21464;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#39640;&#32500;&#25968;&#25454;&#20013;&#65292;&#21464;&#21270;&#26816;&#27979;&#22120;&#19981;&#20165;&#24212;&#33021;&#22815;&#35782;&#21035;&#21464;&#21270;&#21457;&#29983;&#30340;&#26102;&#38388;&#65292;&#36824;&#24212;&#33021;&#22815;&#35782;&#21035;&#21457;&#29983;&#22312;&#21738;&#20010;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#19988;&#26368;&#22909;&#36824;&#24212;&#33021;&#37327;&#21270;&#23427;&#20204;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;ABCd&#26041;&#27861;&#20855;&#22791;&#36825;&#20123;&#29305;&#24615;&#12290;ABCD&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#30417;&#27979;&#20854;&#22312;&#33258;&#36866;&#24212;&#22823;&#23567;&#30340;&#31383;&#21475;&#20869;&#30340;&#20934;&#30830;&#24615;&#12290;ABCD&#26681;&#25454;&#20271;&#24681;&#26031;&#22374;&#19981;&#31561;&#24335;&#35745;&#31639;&#21464;&#21270;&#24471;&#20998;&#65292;&#20197;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20559;&#24046;&#65292;&#25351;&#31034;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ABCD&#22312;&#24179;&#22343;F1-score&#20013;&#33267;&#23569;&#27604;&#20854;&#26368;&#20339;&#31454;&#20105;&#23545;&#25163;&#34920;&#29616;&#20248;&#36234;&#20102;8&#65285;&#65292;&#26368;&#22810;&#21487;&#25552;&#39640;23&#65285;&#12290;&#23427;&#36824;&#21487;&#20197;&#20934;&#30830;&#22320;&#20272;&#35745;&#21464;&#21270;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#21450;&#19982;&#21464;&#21270;&#22823;&#23567;&#30456;&#20851;&#30340;&#20005;&#37325;&#31243;&#24230;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Change detection is of fundamental importance when analyzing data streams. Detecting changes both quickly and accurately enables monitoring and prediction systems to react, e.g., by issuing an alarm or by updating a learning algorithm. However, detecting changes is challenging when observations are high-dimensional. In high-dimensional data, change detectors should not only be able to identify when changes happen, but also in which subspace they occur. Ideally, one should also quantify how severe they are. Our approach, ABCD, has these properties. ABCD learns an encoder-decoder model and monitors its accuracy over a window of adaptive size. ABCD derives a change score based on Bernstein's inequality to detect deviations in terms of accuracy, which indicate changes. Our experiments demonstrate that ABCD outperforms its best competitor by at least 8% and up to 23% in F1-score on average. It can also accurately estimate changes' subspace, together with a severity measure that correlates w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20174;&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#20013;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35745;&#31639;&#39640;&#25928;&#19988;&#33021;&#22312;&#36275;&#22815;&#35206;&#30422;&#25968;&#25454;&#30340;&#21306;&#22495;&#21644;&#36275;&#22815;&#36828;&#31163;&#25968;&#25454;&#30340;&#21306;&#22495;&#20013;&#20135;&#29983;&#25509;&#36817;&#30495;&#23454;&#21518;&#39564;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.11589</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20174;&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#20013;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Sampling from Gaussian Process Posteriors using Stochastic Gradient Descent. (arXiv:2306.11589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20174;&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#20013;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35745;&#31639;&#39640;&#25928;&#19988;&#33021;&#22312;&#36275;&#22815;&#35206;&#30422;&#25968;&#25454;&#30340;&#21306;&#22495;&#21644;&#36275;&#22815;&#36828;&#31163;&#25968;&#25454;&#30340;&#21306;&#22495;&#20013;&#20135;&#29983;&#25509;&#36817;&#30495;&#23454;&#21518;&#39564;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#26159;&#29992;&#20110;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#21644;&#39034;&#24207;&#20915;&#31574;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#20294;&#20854;&#38656;&#35201;&#27714;&#35299;&#32447;&#24615;&#31995;&#32479;&#65292;&#27599;&#24403;&#25968;&#25454;&#38598;&#22823;&#23567;&#22686;&#21152;&#26102;&#20195;&#20215;&#26159;&#31435;&#26041;&#32423;&#21035;&#30340;&#19988;&#23545;&#26465;&#20214;&#25935;&#24863;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#20316;&#20026;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#36825;&#20123;&#32447;&#24615;&#31995;&#32479;&#65306;&#25105;&#20204;&#24320;&#21457;&#20102;&#20302;&#26041;&#24046;&#30340;&#26368;&#20248;&#21270;&#30446;&#26631;&#20197;&#20174;&#21518;&#39564;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;&#24341;&#20837;&#28857;&#12290;&#20196;&#20154;&#24847;&#24819;&#19981;&#21040;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#19981;&#24555;&#36895;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36890;&#24120;&#20063;&#20250;&#20135;&#29983;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#38750;&#25910;&#25947;&#30340;&#38544;&#24335;&#20559;&#32622;&#30340;&#35889;&#29305;&#24449;&#26469;&#35299;&#37322;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20250;&#22312;&#36275;&#22815;&#35206;&#30422;&#25968;&#25454;&#30340;&#21306;&#22495;&#21644;&#36275;&#22815;&#36828;&#31163;&#25968;&#25454;&#30340;&#21306;&#22495;&#20013;&#20135;&#29983;&#25509;&#36817;&#30495;&#23454;&#21518;&#39564;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are a powerful framework for quantifying uncertainty and for sequential decision-making but are limited by the requirement of solving linear systems. In general, this has a cubic cost in dataset size and is sensitive to conditioning. We explore stochastic gradient algorithms as a computationally efficient method of approximately solving these linear systems: we develop low-variance optimization objectives for sampling from the posterior and extend these to inducing points. Counterintuitively, stochastic gradient descent often produces accurate predictions, even in cases where it does not converge quickly to the optimum. We explain this through a spectral characterization of the implicit bias from non-convergence. We show that stochastic gradient descent produces predictive distributions close to the true posterior both in regions with sufficient data coverage, and in regions sufficiently far away from the data. Experimentally, stochastic gradient descent achieves sta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#30340;MCMC&#31639;&#27861;&#30340;&#26465;&#20214;&#28151;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;Poincar\'e&#22411;&#19981;&#31561;&#24335;&#22312;&#29366;&#24577;&#31354;&#38388;&#23376;&#38598;&#19978;&#30340;&#25104;&#31435;&#24773;&#20917;&#65292;&#21457;&#29616;&#26465;&#20214;&#20998;&#24067;&#30340;&#28151;&#21512;&#36895;&#24230;&#24555;&#20110;&#20840;&#23616;&#28151;&#21512;&#36895;&#24230;&#65292;&#23545;&#20110;&#26041;&#24046;&#28151;&#21512;&#27169;&#22411;&#30340;&#37319;&#26679;&#12289;&#21442;&#25968;&#20272;&#35745;&#20197;&#21450;&#20855;&#26377;&#33391;&#22909;&#36830;&#25509;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#21513;&#24067;&#26031;&#37319;&#26679;&#31561;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.10506</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#30340;MCMC&#31639;&#27861;&#24555;&#36895;&#26465;&#20214;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Fast Conditional Mixing of MCMC Algorithms for Non-log-concave Distributions. (arXiv:2306.10506v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#30340;MCMC&#31639;&#27861;&#30340;&#26465;&#20214;&#28151;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;Poincar\'e&#22411;&#19981;&#31561;&#24335;&#22312;&#29366;&#24577;&#31354;&#38388;&#23376;&#38598;&#19978;&#30340;&#25104;&#31435;&#24773;&#20917;&#65292;&#21457;&#29616;&#26465;&#20214;&#20998;&#24067;&#30340;&#28151;&#21512;&#36895;&#24230;&#24555;&#20110;&#20840;&#23616;&#28151;&#21512;&#36895;&#24230;&#65292;&#23545;&#20110;&#26041;&#24046;&#28151;&#21512;&#27169;&#22411;&#30340;&#37319;&#26679;&#12289;&#21442;&#25968;&#20272;&#35745;&#20197;&#21450;&#20855;&#26377;&#33391;&#22909;&#36830;&#25509;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#21513;&#24067;&#26031;&#37319;&#26679;&#31561;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MCMC&#31639;&#27861;&#20026;&#20174;&#30446;&#26631;&#20998;&#24067;$\pi(x) \propto \exp(-V(x))$&#20013;&#37319;&#26679;&#25552;&#20379;&#20102;&#32463;&#39564;&#39640;&#25928;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#29702;&#35770;&#26041;&#38754;&#65292;&#24403;$\pi(x)$&#26159;&#38750;&#23545;&#25968;&#20985;&#26102;&#65292;MCMC&#31639;&#27861;&#30340;&#28151;&#21512;&#36895;&#24230;&#36739;&#24930;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#36825;&#19968;&#24046;&#36317;&#65292;&#24182;&#34920;&#26126;&#24403;Poincar\'e&#22411;&#19981;&#31561;&#24335;&#25104;&#31435;&#26102;&#65292;&#29366;&#24577;&#31354;&#38388;&#23376;&#38598;$\mathcal{X}$&#19978;&#30340;MCMC&#36845;&#20195;&#30340;&#26465;&#20214;&#20998;&#24067;&#28151;&#21512;&#24555;&#36895;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#36825;&#31181;&#24555;&#36895;&#28151;&#21512;&#20445;&#35777;&#21487;&#20197;&#22312;&#20840;&#23616;&#28151;&#21512;&#24050;&#34987;&#35777;&#26126;&#36739;&#24930;&#30340;&#24773;&#20917;&#19979;&#25104;&#31435;&#12290;&#25105;&#20204;&#23558;&#35813;&#22768;&#26126;&#24418;&#24335;&#21270;&#24182;&#37327;&#21270;&#20102;&#26465;&#20214;&#28151;&#21512;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26465;&#20214;&#28151;&#21512;&#23545;&#20110;&#20174;&#39640;&#26031;&#28151;&#21512;&#29289;&#20013;&#37319;&#26679;&#12289;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#20197;&#21450;&#20855;&#26377;&#33391;&#22909;&#36830;&#25509;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#21513;&#24067;&#26031;&#37319;&#26679;&#20250;&#20135;&#29983;&#26377;&#36259;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
MCMC algorithms offer empirically efficient tools for sampling from a target distribution $\pi(x) \propto \exp(-V(x))$. However, on the theory side, MCMC algorithms suffer from slow mixing rate when $\pi(x)$ is non-log-concave. Our work examines this gap and shows that when Poincar\'e-style inequality holds on a subset $\mathcal{X}$ of the state space, the conditional distribution of MCMC iterates over $\mathcal{X}$ mixes fast to the true conditional distribution. This fast mixing guarantee can hold in cases when global mixing is provably slow. We formalize the statement and quantify the conditional mixing rate. We further show that conditional mixing can have interesting implications for sampling from mixtures of Gaussians, parameter estimation for Gaussian mixture models and Gibbs-sampling with well-connected local minima.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#19988;&#20984;&#30340;&#26367;&#20195;&#20108;&#36827;&#21046;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#20013;&#21457;&#24067;&#31169;&#26377;&#21069;&#32512;&#27714;&#21644;&#39064;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#27604;&#26631;&#24535;&#24615;&#30340;&#20108;&#36827;&#21046;&#26426;&#21046;&#20855;&#26377;&#26356;&#20302;&#30340;&#26041;&#24046;&#21644;&#26356;&#22909;&#30340;&#35266;&#27979;&#26102;&#38388;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#21644;&#31354;&#38388;&#35201;&#27714;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.09666</link><description>&lt;p&gt;
&#19968;&#31181;&#39640;&#25928;&#30340;&#31169;&#26377;&#36830;&#32493;&#35266;&#27979;&#24179;&#28369;&#20108;&#20803;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Smooth Binary Mechanism for Efficient Private Continual Observation. (arXiv:2306.09666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#19988;&#20984;&#30340;&#26367;&#20195;&#20108;&#36827;&#21046;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#20013;&#21457;&#24067;&#31169;&#26377;&#21069;&#32512;&#27714;&#21644;&#39064;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#27604;&#26631;&#24535;&#24615;&#30340;&#20108;&#36827;&#21046;&#26426;&#21046;&#20855;&#26377;&#26356;&#20302;&#30340;&#26041;&#24046;&#21644;&#26356;&#22909;&#30340;&#35266;&#27979;&#26102;&#38388;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#21644;&#31354;&#38388;&#35201;&#27714;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#35266;&#27979;&#30340;&#38544;&#31169;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#22522;&#20110;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#25968;&#25454;&#38598;&#21457;&#24067;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#12290;&#21457;&#24067;&#31169;&#26377;&#21069;&#32512;&#27714;&#21644;$x_1,x_2,x_3,\dots\in\{0,1\}$&#38382;&#39064;&#26159;&#19968;&#20010;&#29305;&#21035;&#22909;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#31169;&#26377;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26368;&#26032;&#26041;&#27861;&#20013;&#20351;&#29992;&#20102;&#19968;&#20010;&#24191;&#20041;&#24418;&#24335;&#12290;&#26631;&#24535;&#24615;&#30340;&#20108;&#36827;&#21046;&#26426;&#21046;&#33258;&#21160;&#21152;&#20837;&#30340;&#22122;&#22768;&#30340;&#26041;&#24046;&#26159;&#22810;&#23545;&#25968;&#30340;&#12290;&#26368;&#36817;&#65292;Henzinger et al.&#21644;Denisov et al.&#34920;&#31034;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#25552;&#39640;&#20108;&#36827;&#21046;&#26426;&#21046;&#65306;&#22122;&#22768;&#30340;&#26041;&#24046;&#21487;&#20197;&#20943;&#23567;&#19968;&#20010;&#36739;&#22823;&#30340;&#24120;&#25968;&#22240;&#23376;&#65292;&#24182;&#19988;&#22312;&#26102;&#38388;&#27493;&#38271;&#20869;&#20063;&#21487;&#20197;&#26356;&#21152;&#24179;&#22343;&#12290;&#20294;&#26159;&#65292;&#20182;&#20204;&#29983;&#25104;&#22122;&#22768;&#20998;&#24067;&#30340;&#31639;&#27861;&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#65288;&#29305;&#21035;&#26159;&#65289;&#31354;&#38388;&#26041;&#38754;&#37117;&#19981;&#22914;&#20154;&#24847;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#21487;&#24494;&#19988;&#20984;&#30340;&#31616;&#21333;&#26367;&#20195;&#20108;&#36827;&#21046;&#26426;&#21046;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25928;&#29575;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#30456;&#23545;&#20110;&#28155;&#21152;&#30340;&#22122;&#22768;&#37327;&#23545;&#26426;&#21046;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26426;&#21046;&#27604;&#26631;&#24535;&#24615;&#30340;&#20108;&#36827;&#21046;&#26426;&#21046;&#20855;&#26377;&#26356;&#20302;&#30340;&#26041;&#24046;&#21644;&#26356;&#22909;&#30340;&#35266;&#27979;&#26102;&#38388;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#21644;&#31354;&#38388;&#35201;&#27714;&#26041;&#38754;&#27604;&#20197;&#21069;&#30340;&#25913;&#36827;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In privacy under continual observation we study how to release differentially private estimates based on a dataset that evolves over time. The problem of releasing private prefix sums of $x_1,x_2,x_3,\dots \in\{0,1\}$ (where the value of each $x_i$ is to be private) is particularly well-studied, and a generalized form is used in state-of-the-art methods for private stochastic gradient descent (SGD). The seminal binary mechanism privately releases the first $t$ prefix sums with noise of variance polylogarithmic in $t$. Recently, Henzinger et al. and Denisov et al. showed that it is possible to improve on the binary mechanism in two ways: The variance of the noise can be reduced by a (large) constant factor, and also made more even across time steps. However, their algorithms for generating the noise distribution are not as efficient as one would like in terms of computation time and (in particular) space. We address the efficiency problem by presenting a simple alternative to the binary
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26469;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#31574;&#30053;&#23450;&#21046;&#65292;&#26080;&#38656;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.09526</link><description>&lt;p&gt;
&#27531;&#24046; Q &#23398;&#20064;&#65306;&#26080;&#38656;&#20215;&#20540;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Residual Q-Learning: Offline and Online Policy Customization without Value. (arXiv:2306.09526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26469;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#31574;&#30053;&#23450;&#21046;&#65292;&#26080;&#38656;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#27169;&#20223;&#34892;&#20026;&#12290;&#24403;&#25163;&#24037;&#21046;&#20316;&#22870;&#21169;&#20989;&#25968;&#22256;&#38590;&#25110;&#30446;&#26631;&#26159;&#27169;&#20223;&#20154;&#31867;&#19987;&#23478;&#34892;&#20026;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#12290;&#20294;&#26159;&#65292;&#23398;&#20064;&#30340;&#27169;&#20223;&#31574;&#30053;&#21482;&#33021;&#36981;&#24490;&#28436;&#31034;&#20013;&#30340;&#34892;&#20026;&#12290;&#22312;&#24212;&#29992;&#27169;&#20223;&#31574;&#30053;&#26102;&#65292;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#26681;&#25454;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#35201;&#27714;&#23450;&#21046;&#31574;&#30053;&#34892;&#20026;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20173;&#24076;&#26395;&#23450;&#21046;&#30340;&#31574;&#30053;&#20445;&#25345;&#20854;&#27169;&#20223;&#24615;&#36136;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#31216;&#20026;&#31574;&#30053;&#23450;&#21046;&#12290;&#23427;&#23558;&#23398;&#20064;&#20219;&#21153;&#23450;&#20041;&#20026;&#35757;&#32451;&#19968;&#31181;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32487;&#25215;&#20808;&#21069;&#31574;&#30053;&#30340;&#29305;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#24378;&#21152;&#30340;&#19968;&#20123;&#38468;&#21152;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#30830;&#23450;&#20004;&#20010;&#20219;&#21153;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) is a widely used framework for learning imitative behavior from demonstrations. It is especially appealing for solving complex real-world tasks where handcrafting reward function is difficult, or when the goal is to mimic human expert behavior. However, the learned imitative policy can only follow the behavior in the demonstration. When applying the imitative policy, we may need to customize the policy behavior to meet different requirements coming from diverse downstream tasks. Meanwhile, we still want the customized policy to maintain its imitative nature. To this end, we formulate a new problem setting called policy customization. It defines the learning task as training a policy that inherits the characteristics of the prior policy while satisfying some additional requirements imposed by a target downstream task. We propose a novel and principled approach to interpret and determine the trade-off between the two task objectives. Specifically, we formulate the
&lt;/p&gt;</description></item><item><title>AQuA&#26159;&#19968;&#27454;&#29992;&#20110;&#26631;&#31614;&#36136;&#37327;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#30446;&#30340;&#26159;&#35780;&#20272;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#21253;&#25324;&#25968;&#25454;&#27169;&#25311;&#12289;&#36136;&#37327;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26631;&#31614;&#22122;&#22768;&#28040;&#38500;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09467</link><description>&lt;p&gt;
AQuA&#65306;&#19968;&#31181;&#29992;&#20110;&#26631;&#31614;&#36136;&#37327;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
AQuA: A Benchmarking Tool for Label Quality Assessment. (arXiv:2306.09467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09467
&lt;/p&gt;
&lt;p&gt;
AQuA&#26159;&#19968;&#27454;&#29992;&#20110;&#26631;&#31614;&#36136;&#37327;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#30446;&#30340;&#26159;&#35780;&#20272;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#21253;&#25324;&#25968;&#25454;&#27169;&#25311;&#12289;&#36136;&#37327;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26631;&#31614;&#22122;&#22768;&#28040;&#38500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22909;&#22351;&#21462;&#20915;&#20110;&#29992;&#20110;&#35757;&#32451;&#23427;&#20204;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;ImageNet&#65292;&#23384;&#22312;&#26222;&#36941;&#30340;&#26631;&#27880;&#38169;&#35823;&#12290;&#35757;&#32451;&#38598;&#20013;&#30340;&#38169;&#35823;&#26631;&#31614;&#20250;&#21066;&#24369;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#24433;&#21709;&#20351;&#29992;&#27979;&#35797;&#38598;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#21644;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#22312;&#23384;&#22312;&#26631;&#31614;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26159;&#30740;&#31350;&#30340;&#19968;&#20010;&#27963;&#36291;&#39046;&#22495;&#65292;&#20294;&#35813;&#39046;&#22495;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AQuA&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#65292;&#20197;&#20005;&#26684;&#35780;&#20272;&#22312;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models are only as good as the data they are trained on. But recent studies have found datasets widely used to train and evaluate ML models, e.g. ImageNet, to have pervasive labeling errors. Erroneous labels on the train set hurt ML models' ability to generalize, and they impact evaluation and model selection using the test set. Consequently, learning in the presence of labeling errors is an active area of research, yet this field lacks a comprehensive benchmark to evaluate these methods. Most of these methods are evaluated on a few computer vision datasets with significant variance in the experimental protocols. With such a large pool of methods and inconsistent evaluation, it is also unclear how ML practitioners can choose the right models to assess label quality in their data. To this end, we propose a benchmarking environment AQuA to rigorously evaluate methods that enable machine learning in the presence of label noise. We also introduce a design space to del
&lt;/p&gt;</description></item><item><title>ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.06446</link><description>&lt;p&gt;
ShiftAddViT&#65306;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#28151;&#21512;&#23454;&#29616;&#39640;&#25928;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06446
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#30340;&#32479;&#19968;&#39592;&#24178;&#12290;&#20294;&#26159;&#65292;ViTs&#20013;&#30340;&#27880;&#24847;&#21147;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#30001;&#20110;&#23494;&#38598;&#30340;&#20056;&#27861;&#32780;&#19981;&#22815;&#39640;&#25928;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;ViT&#20197;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#65288;&#20363;&#22914;&#20301;&#31227;&#21644;&#21152;&#27861;&#65289;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20840;&#26032;&#31867;&#22411;&#30340;&#20943;&#23569;&#20056;&#27861;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;ShiftAddViT&#65292;&#26088;&#22312;&#23454;&#29616;GPU&#19978;&#30340;&#31471;&#21040;&#31471;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#26597;&#35810;&#21644;&#38190;&#26144;&#23556;&#20026;&#27721;&#26126;&#31354;&#38388;&#20013;&#30340;&#20108;&#36827;&#21046;&#30721;&#20043;&#21518;&#65292;&#37319;&#29992;&#21152;&#27861;&#26680;&#23545;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#20043;&#38388;&#30340;MatMul&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#21097;&#20313;&#30340;MLPs&#25110;&#32447;&#24615;&#23618;&#21017;&#37319;&#29992;&#20301;&#31227;&#26680;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;TVM&#22312;GPU&#19978;&#23454;&#26045;&#24182;&#20248;&#21270;&#36825;&#20123;&#23450;&#21046;&#26680;&#65292;&#20197;&#23454;&#29616;&#23454;&#38469;&#30828;&#20214;&#37096;&#32626;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. But both attention and multi-layer perceptions (MLPs) in ViTs are not efficient enough due to dense multiplications, resulting in costly training and inference. To this end, we propose to reparameterize the pre-trained ViT with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\textbf{ShiftAddViT}$, which aims for end-to-end inference speedups on GPUs without the need of training from scratch. Specifically, all $\texttt{MatMuls}$ among queries, keys, and values are reparameterized by additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized by shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameter
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20986;&#22810;&#20013;&#24515;&#35782;&#21035;&#27169;&#22411;&#65292;&#25972;&#20307; F1&#24471;&#20998;&#20026;84.77%&#12290;&#35813;&#27169;&#22411;&#23558;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05323</link><description>&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#24212;&#29992;&#65306;&#26041;&#27861;&#35770;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#22810;&#20013;&#24515;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application. (arXiv:2306.05323v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20986;&#22810;&#20013;&#24515;&#35782;&#21035;&#27169;&#22411;&#65292;&#25972;&#20307; F1&#24471;&#20998;&#20026;84.77%&#12290;&#35813;&#27169;&#22411;&#23558;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#38498;&#24341;&#20837;&#35745;&#31639;&#26426;&#21270;&#21307;&#30103;&#35760;&#24405;&#26377;&#21161;&#20110;&#20943;&#23569;&#25163;&#20889;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#32321;&#29712;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#21307;&#30103;&#35760;&#24405;&#20013;&#25552;&#21462;&#25968;&#25454;&#38656;&#35201;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#22240;&#27492;&#21307;&#30103;&#35760;&#24405;&#20013;&#21253;&#21547;&#30340;&#25968;&#25454;&#20173;&#28982;&#34987;&#20805;&#20998;&#21033;&#29992;&#31243;&#24230;&#20302;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#39046;&#22495;&#20449;&#24687;&#25552;&#21462;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#20351;&#29992;&#33258;&#21160;&#21270;&#25991;&#26412;&#25366;&#25496;&#27969;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598; PsyNIT&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#24320;&#21457;&#36825;&#19968;&#20219;&#21153;&#30340;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#20351;&#29992;&#19977;&#20010;&#22806;&#37096;&#29420;&#31435;&#25968;&#25454;&#38598;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#20013;&#24515;&#27169;&#22411;&#65292;&#25972;&#20307; F1 &#24471;&#20998;&#20026; 84.77%&#65292;&#31934;&#30830;&#29575;&#20026; 83.16%&#65292;&#21484;&#22238;&#29575;&#20026; 86.44%&#12290;&#25105;&#20204;&#23398;&#21040;&#30340;&#32463;&#39564;&#26159;: (i) &#19968;&#33268;&#30340;&#27880;&#37322;&#36807;&#31243;&#30340;&#20851;&#38190;&#20316;&#29992;&#21644; (ii) &#32467;&#21512;&#32463;&#20856;&#26041;&#27861;&#21644;&#8220;&#23569;&#37327;&#35757;&#32451;&#8221;&#30340; fine-tuning &#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of computerized medical records in hospitals has reduced burdensome operations like manual writing and information fetching. However, the data contained in medical records are still far underutilized, primarily because extracting them from unstructured textual medical records takes time and effort. Information Extraction, a subfield of Natural Language Processing, can help clinical practitioners overcome this limitation, using automated text-mining pipelines. In this work, we created the first Italian neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to develop a Large Language Model for this task. Moreover, we conducted several experiments with three external independent datasets to implement an effective multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned are: (i) the crucial role of a consistent annotation process and (ii) a fine-tuning strategy that combines classical methods with a "few-shot" a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#65292;&#24182;&#38477;&#20302;&#26631;&#31614;&#33719;&#21462;&#30340;&#30740;&#31350;&#25104;&#26412;80&#65285;&#65292;&#21516;&#26102;&#20445;&#35777;CSS&#30740;&#31350;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04746</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#36827;&#34892;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#26377;&#25928;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;: &#22522;&#20110;&#35774;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning. (arXiv:2306.04746v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04746
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#65292;&#24182;&#38477;&#20302;&#26631;&#31614;&#33719;&#21462;&#30340;&#30740;&#31350;&#25104;&#26412;80&#65285;&#65292;&#21516;&#26102;&#20445;&#35777;CSS&#30740;&#31350;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#65288;CSS&#65289;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#25991;&#26723;&#26469;&#35299;&#37322;&#31038;&#20250;&#21644;&#25919;&#27835;&#29616;&#35937;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;CSS&#30740;&#31350;&#20154;&#21592;&#39318;&#20808;&#33719;&#21462;&#25991;&#26723;&#30340;&#26631;&#31614;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22238;&#24402;&#20998;&#26512;&#26469;&#35299;&#37322;&#26631;&#31614;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#36827;&#23637;&#21487;&#20197;&#36890;&#36807;&#22312;&#35268;&#27169;&#19978;&#20415;&#23452;&#22320;&#27880;&#37322;&#25991;&#26723;&#26469;&#38477;&#20302;CSS&#30740;&#31350;&#25104;&#26412;&#65292;&#20294;&#36825;&#20123;&#26367;&#20195;&#26631;&#31614;&#36890;&#24120;&#26159;&#19981;&#23436;&#32654;&#21644;&#26377;&#20559;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;LLMs&#30340;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#21516;&#26102;&#20445;&#35777;&#19982;CSS&#30740;&#31350;&#22522;&#26412;&#30456;&#20851;&#30340;&#32479;&#35745;&#23646;&#24615;-&#22914;&#28176;&#36817;&#26080;&#20559;&#24615;&#21644;&#27491;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#30452;&#25509;&#22312;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#20013;&#20351;&#29992;LLM&#39044;&#27979;&#30340;&#26367;&#20195;&#26631;&#31614;&#20250;&#23548;&#33268;&#23454;&#36136;&#24615;&#20559;&#24046;&#21644;&#26080;&#25928;&#32622;&#20449;&#21306;&#38388;&#65292;&#21363;&#20351;&#26367;&#20195;&#20934;&#30830;&#24615;&#39640;&#36798;80-90&#65285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#22522;&#20110;&#35774;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;D-SSL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;LLM&#27880;&#37322;&#19982;&#26377;&#38024;&#23545;&#24615;&#30340;&#37319;&#26679;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#26631;&#31614;&#33719;&#21462;&#30340;CSS&#30740;&#31350;&#25104;&#26412;&#38477;&#20302;80&#65285;&#65292;&#32780;&#19981;&#24433;&#21709;&#32479;&#35745;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#34920;&#26126;&#65292;&#19982;&#30452;&#25509;&#20351;&#29992;LLM&#39044;&#27979;&#26631;&#31614;&#30456;&#27604;&#65292;D-SSL&#21487;&#20197;&#23558;&#22238;&#24402;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#22810;&#36798;40&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. The recent advancements in large language models (LLMs) can lower costs for CSS research by annotating documents cheaply at scale, but such surrogate labels are often imperfect and biased. We present a new algorithm for using outputs from LLMs for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to CSS research. We show that direct use of LLM-predicted surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80--90\%. To address this, we build on debiased machine learning to propose the design-based semi-supervised le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$(k, t)$-FWL&#21644;$k$-FWL+&#20004;&#31181;&#26041;&#27861;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#21487;&#20197;&#22312;$O(n^2)$&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#19979;&#65292;&#35299;&#20915;&#22270;&#21516;&#26500;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03266</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#27665;&#38388;&#23041;&#26031;&#36153;&#21202;-&#33713;&#26364;&#31639;&#27861;&#65292;&#23454;&#29616;$O(n^2)$&#31354;&#38388;&#20869;&#20219;&#24847;&#34920;&#36798;&#33021;&#21147;&#30340;GNNs
&lt;/p&gt;
&lt;p&gt;
Towards Arbitrarily Expressive GNNs in $O(n^2)$ Space by Rethinking Folklore Weisfeiler-Lehman. (arXiv:2306.03266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$(k, t)$-FWL&#21644;$k$-FWL+&#20004;&#31181;&#26041;&#27861;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#21487;&#20197;&#22312;$O(n^2)$&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#19979;&#65292;&#35299;&#20915;&#22270;&#21516;&#26500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#24050;&#25104;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20854;&#34920;&#36798;&#33021;&#21147;&#21463;&#21040;&#19968;&#32500;&#23041;&#26031;&#36153;&#21202;-&#33713;&#26364;&#65288;1-WL&#65289;&#27979;&#35797;&#30340;&#38480;&#21046;&#12290;&#19968;&#20123;&#30740;&#31350;&#21463;&#21040;$k$-WL/FWL&#65288;&#27665;&#38388;WL&#65289;&#30340;&#21551;&#21457;&#24182;&#35774;&#35745;&#20854;&#30456;&#24212;&#30340;&#31070;&#32463;&#29256;&#26412;&#12290;&#23613;&#31649;&#20855;&#26377;&#24456;&#39640;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#23384;&#22312;&#20005;&#37325;&#23616;&#38480;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;$(k, t)$-FWL&#21644;$k$-FWL+&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing neural networks (MPNNs) have emerged as the most popular framework of graph neural networks (GNNs) in recent years. However, their expressive power is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Some works are inspired by $k$-WL/FWL (Folklore WL) and design the corresponding neural versions. Despite the high expressive power, there are serious limitations in this line of research. In particular, (1) $k$-WL/FWL requires at least $O(n^k)$ space complexity, which is impractical for large graphs even when $k=3$; (2) The design space of $k$-WL/FWL is rigid, with the only adjustable hyper-parameter being $k$. To tackle the first limitation, we propose an extension, $(k, t)$-FWL. We theoretically prove that even if we fix the space complexity to $O(n^2)$ in $(k, t)$-FWL, we can construct an expressiveness hierarchy up to solving the graph isomorphism problem. To tackle the second problem, we propose $k$-FWL+, which considers any equivariant set as neighbors ins
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#26377;&#25928;&#25506;&#32034;&#39046;&#22495;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22312;&#20687;ProcGen Maze&#36825;&#26679;&#30340;&#38382;&#39064;&#19978;&#22522;&#20110;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#22833;&#36133;&#30340;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2306.03072</link><description>&lt;p&gt;
&#22312;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Explore to Generalize in Zero-Shot RL. (arXiv:2306.03072v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#26377;&#25928;&#25506;&#32034;&#39046;&#22495;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22312;&#20687;ProcGen Maze&#36825;&#26679;&#30340;&#38382;&#39064;&#19978;&#22522;&#20110;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#22833;&#36133;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#38382;&#39064;&#65292;&#21363;&#22312;&#19968;&#32452;&#35757;&#32451;&#20219;&#21153;&#19978;&#20248;&#21270;&#31574;&#30053;&#20197;&#22312;&#31867;&#20284;&#20294;&#26410;&#35265;&#36807;&#30340;&#27979;&#35797;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#20026;&#20102;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#20219;&#21153;&#19981;&#21464;&#24615;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#22312;&#20687;ProcGen Maze&#36825;&#26679;&#30340;&#38382;&#39064;&#19978;&#65292;&#19981;&#23384;&#22312;&#19968;&#31181;&#19982;&#20219;&#21153;&#21487;&#35270;&#21270;&#26080;&#20851;&#30340;&#36866;&#24403;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#27492;&#22522;&#20110;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#22833;&#36133;&#20102;&#12290;&#25105;&#20204;&#30340;&#27934;&#23519;&#21147;&#26159;&#65292;&#23398;&#20064;&#19968;&#31181;&#26377;&#25928;&#22320;&#25506;&#32034;&#39046;&#22495;&#30340;&#31574;&#30053;&#27604;&#23398;&#20064;&#19968;&#31181;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#31574;&#30053;&#26356;&#38590;&#34987;&#35760;&#24518;&#65292;&#22240;&#27492;&#25105;&#20204;&#26399;&#26395;&#36825;&#31181;&#23398;&#20064;&#21040;&#30340;&#34892;&#20026;&#33021;&#22815;&#33391;&#22909;&#27867;&#21270;&#65307;&#25105;&#20204;&#22312;&#20960;&#20010;&#23545;&#20110;&#22522;&#20110;&#19981;&#21464;&#24615;&#26041;&#27861;&#26469;&#35828;&#22256;&#38590;&#30340;&#39046;&#22495;&#19978;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#30340;&#8220;&#25506;&#32034;&#27867;&#21270;&#8221;&#31639;&#27861;&#65288;ExpGen&#65289;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65306;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#20195;&#29702;&#38598;&#21512;&#26469;&#20248;&#21270;&#22870;&#21169;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#22914;&#26524;&#20195;&#29702;&#38598;&#21512;&#23545;&#19968;&#20010;&#21160;&#20316;&#36798;&#25104;&#19968;&#33268;&#65292;&#25105;&#20204;&#23558;&#33021;&#22815;&#33391;&#22909;&#27867;&#21270;&#65292;&#21542;&#21017;&#25105;&#20204;&#23558;&#37319;&#21462;&#20854;&#20182;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study zero-shot generalization in reinforcement learning-optimizing a policy on a set of training tasks to perform well on a similar but unseen test task. To mitigate overfitting, previous work explored different notions of invariance to the task. However, on problems such as the ProcGen Maze, an adequate solution that is invariant to the task visualization does not exist, and therefore invariance-based approaches fail. Our insight is that learning a policy that effectively $\textit{explores}$ the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore we expect such learned behavior to generalize well; we indeed demonstrate this empirically on several domains that are difficult for invariance-based approaches. Our $\textit{Explore to Generalize}$ algorithm (ExpGen) builds on this insight: we train an additional ensemble of agents that optimize reward. At test time, either the ensemble agrees on an action, and we generalize well, or we tak
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;UnMixMatch&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#38750;&#32422;&#26463;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#24449;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#22823;&#22810;&#25968;&#21322;&#30417;&#30563;&#26041;&#27861;&#22522;&#20110;&#26377;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#26679;&#26412;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#30340;&#20551;&#35774;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#24378;&#27491;&#21017;&#21270;&#20316;&#29992;&#30340;&#30828;&#22686;&#24378;&#30417;&#30563;&#23398;&#20064;&#22120;&#12289;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#30784;&#34920;&#24449;&#30340;&#23545;&#27604;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#22120;&#20197;&#21450;&#36890;&#36807;&#33258;&#30417;&#30563;&#25439;&#22833;&#26469;&#22686;&#24378;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#30340;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.01222</link><description>&lt;p&gt;
&#29992;&#38750;&#32422;&#26463;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#25193;&#23637;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data. (arXiv:2306.01222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;UnMixMatch&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#38750;&#32422;&#26463;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#24449;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#22823;&#22810;&#25968;&#21322;&#30417;&#30563;&#26041;&#27861;&#22522;&#20110;&#26377;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#26679;&#26412;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#30340;&#20551;&#35774;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#24378;&#27491;&#21017;&#21270;&#20316;&#29992;&#30340;&#30828;&#22686;&#24378;&#30417;&#30563;&#23398;&#20064;&#22120;&#12289;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#30784;&#34920;&#24449;&#30340;&#23545;&#27604;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#22120;&#20197;&#21450;&#36890;&#36807;&#33258;&#30417;&#30563;&#25439;&#22833;&#26469;&#22686;&#24378;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;UnMixMatch&#65292;&#21487;&#20197;&#20174;&#38750;&#32422;&#26463;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#24449;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#22522;&#20110;&#26377;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#26679;&#26412;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#36825;&#38480;&#21046;&#20102;&#36890;&#36807;&#20351;&#29992;&#33258;&#30001;&#29983;&#27963;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#35813;&#20551;&#35774;&#32463;&#24120;&#38480;&#21046;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21487;&#25512;&#24191;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#24182;&#26377;&#25928;&#21033;&#29992;&#38750;&#32422;&#26463;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;UnMixMatch&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#20855;&#26377;&#24378;&#27491;&#21017;&#21270;&#20316;&#29992;&#30340;&#30828;&#22686;&#24378;&#30417;&#30563;&#23398;&#20064;&#22120;&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#30784;&#34920;&#24449;&#30340;&#23545;&#27604;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#22120;&#20197;&#21450;&#36890;&#36807;&#33258;&#30417;&#30563;&#25439;&#22833;&#26469;&#22686;&#24378;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35777;&#26126;UnMixMatch&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose UnMixMatch, a semi-supervised learning framework which can learn effective representations from unconstrained unlabelled data in order to scale up performance. Most existing semi-supervised methods rely on the assumption that labelled and unlabelled samples are drawn from the same distribution, which limits the potential for improvement through the use of free-living unlabeled data. Consequently, the generalizability and scalability of semi-supervised learning are often hindered by this assumption. Our method aims to overcome these constraints and effectively utilize unconstrained unlabelled data in semi-supervised learning. UnMixMatch consists of three main components: a supervised learner with hard augmentations that provides strong regularization, a contrastive consistency regularizer to learn underlying representations from the unlabelled data, and a self-supervised loss to enhance the representations that are learnt from the unlabelled data. We perform extensive experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#21333;&#33218;&#31574;&#30053;&#36716;&#21270;&#20026;&#21407;&#22987;&#30340;$N$&#33218;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20381;&#36182;&#20110;&#22797;&#26434;UGAP&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;$O(1/\sqrt{N})$&#26368;&#20248;&#24615;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.00196</link><description>&lt;p&gt;
&#20855;&#26377;&#24179;&#22343;&#22870;&#21169;&#30340;&#19981;&#23433;&#23450;&#36172;&#24466;&#38382;&#39064;&#65306;&#25171;&#30772;&#32479;&#19968;&#20840;&#23616;&#24341;&#23376;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption. (arXiv:2306.00196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#21333;&#33218;&#31574;&#30053;&#36716;&#21270;&#20026;&#21407;&#22987;&#30340;$N$&#33218;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20381;&#36182;&#20110;&#22797;&#26434;UGAP&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;$O(1/\sqrt{N})$&#26368;&#20248;&#24615;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#24179;&#22343;&#22870;&#21169;&#26631;&#20934;&#19979;&#30340;&#26080;&#38480;&#26102;&#19981;&#23433;&#23450;&#36172;&#24466;&#38382;&#39064;&#65292;&#21253;&#25324;&#31163;&#25955;&#26102;&#38388;&#21644;&#36830;&#32493;&#26102;&#38388;&#35774;&#32622;&#12290;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#22914;&#20309;&#35774;&#35745;&#35745;&#31639;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#20351;&#24471;&#20248;&#21270;&#24046;&#36317;&#38543;&#30528;&#33218;&#30340;&#25968;&#37327;$N$&#30340;&#22686;&#21152;&#32780;&#20943;&#23567;&#12290;&#29616;&#26377;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#32467;&#26524;&#37117;&#20381;&#36182;&#20110;&#32479;&#19968;&#20840;&#23616;&#24341;&#23376;&#24615;&#36136;(UGAP)&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#38590;&#20197;&#39564;&#35777;&#30340;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#22522;&#20110;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#21333;&#33218;&#31574;&#30053;&#36716;&#21270;&#20026;&#21407;&#22987;&#30340;$N$&#33218;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#27599;&#20010;&#33218;&#19978;&#27169;&#25311;&#21333;&#33218;&#31574;&#30053;&#65292;&#24182;&#20180;&#32454;&#22320;&#23558;&#30495;&#23454;&#29366;&#24577;&#24341;&#23548;&#21521;&#27169;&#25311;&#29366;&#24577;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#23454;&#20363;&#21270;&#65292;&#20135;&#29983;&#19968;&#20010;&#20855;&#26377;$O(1/\sqrt{N})$&#30340;&#26368;&#20248;&#35299;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#22312;&#31163;&#25955;&#26102;&#38388;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#26356;&#31616;&#21333;&#30340;&#21516;&#27493;&#20551;&#35774;&#19979;&#25104;&#31435;&#65292;&#28085;&#30422;&#20102;&#19968;&#20123;&#19981;&#28385;&#36275;UGAP&#30340;&#38382;&#39064;&#23454;&#20363;&#12290;&#26356;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22823;&#30340;&#38382;&#39064;&#31867;&#65292;&#32780;&#19981;&#38656;&#23545;&#38382;&#39064;&#23454;&#20363;&#20570;&#20219;&#20309;&#29305;&#23450;&#30340;&#32467;&#26500;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the infinite-horizon restless bandit problem with the average reward criterion, under both discrete-time and continuous-time settings. A fundamental question is how to design computationally efficient policies that achieve a diminishing optimality gap as the number of arms, $N$, grows large. Existing results on asymptotical optimality all rely on the uniform global attractor property (UGAP), a complex and challenging-to-verify assumption. In this paper, we propose a general, simulation-based framework that converts any single-armed policy into a policy for the original $N$-armed problem. This is accomplished by simulating the single-armed policy on each arm and carefully steering the real state towards the simulated state. Our framework can be instantiated to produce a policy with an $O(1/\sqrt{N})$ optimality gap. In the discrete-time setting, our result holds under a simpler synchronization assumption, which covers some problem instances that do not satisfy UGAP. More notabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#34920;&#31034;&#21518;&#39564;&#20998;&#24067;&#30340;&#25674;&#38144;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23454;&#26102;&#25512;&#29702;&#30340;&#30446;&#30340;&#65292;&#21487;&#24212;&#29992;&#20110;&#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#21442;&#25968;&#20272;&#35745;&#21644;&#27969;&#22330;&#37325;&#26500;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.20004</link><description>&lt;p&gt;
&#23398;&#20064;&#35299;&#20915;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#65306;&#19968;&#31181;&#25674;&#38144;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to solve Bayesian inverse problems: An amortized variational inference approach. (arXiv:2305.20004v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#34920;&#31034;&#21518;&#39564;&#20998;&#24067;&#30340;&#25674;&#38144;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23454;&#26102;&#25512;&#29702;&#30340;&#30446;&#30340;&#65292;&#21487;&#24212;&#29992;&#20110;&#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#21442;&#25968;&#20272;&#35745;&#21644;&#27969;&#22330;&#37325;&#26500;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#65292;&#21363;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#20272;&#35745;&#29289;&#29702;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#36125;&#21494;&#26031;&#20844;&#24335;&#26159;&#40644;&#37329;&#26631;&#20934;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#32531;&#35299;&#30149;&#24577;&#24615;&#38382;&#39064;&#24182;&#37327;&#21270;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35299;&#26512;&#21518;&#39564;&#19981;&#36890;&#24120;&#21487;&#29992;&#65292;&#20154;&#20204;&#37319;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#25110;&#36817;&#20284;&#21464;&#20998;&#25512;&#29702;&#12290;&#20294;&#26159;&#65292;&#38656;&#35201;&#37325;&#26032;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#25512;&#29702;&#20197;&#36866;&#24212;&#27599;&#32452;&#26032;&#25968;&#25454;&#12290;&#36825;&#31181;&#32570;&#28857;&#38480;&#21046;&#20102;&#36125;&#21494;&#26031;&#20844;&#24335;&#22312;&#23454;&#26102;&#35774;&#32622;&#65292;&#20363;&#22914;&#24037;&#31243;&#31995;&#32479;&#30340;&#20581;&#24247;&#30417;&#27979;&#21644;&#21307;&#30103;&#35786;&#26029;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20174;&#25968;&#25454;&#21040;&#21518;&#39564;&#30340;&#36125;&#21494;&#26031;&#36870;&#26144;&#23556;&#65292;&#21363;&#20174;&#25968;&#25454;&#21040;&#21518;&#39564;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#23454;&#26102;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#19979;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#34920;&#31034;&#21518;&#39564;&#20998;&#24067;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#25674;&#38144;&#21464;&#20998;&#25512;&#29702;&#23398;&#20064;&#32593;&#32476;&#21442;&#25968;&#65292;&#20854;&#20013;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#39044;&#27979;&#20174;&#25968;&#25454;&#20013;&#39044;&#27979;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#19968;&#20123;&#36870;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#21253;&#25324;&#21442;&#25968;&#20272;&#35745;&#21644;&#27969;&#22330;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems, i.e., estimating parameters of physical models from experimental data, are ubiquitous in science and engineering. The Bayesian formulation is the gold standard because it alleviates ill-posedness issues and quantifies epistemic uncertainty. Since analytical posteriors are not typically available, one resorts to Markov chain Monte Carlo sampling or approximate variational inference. However, inference needs to be rerun from scratch for each new set of data. This drawback limits the applicability of the Bayesian formulation to real-time settings, e.g., health monitoring of engineered systems, and medical diagnosis. The objective of this paper is to develop a methodology that enables real-time inference by learning the Bayesian inverse map, i.e., the map from data to posteriors. Our approach is as follows. We represent the posterior distribution using a parameterization based on deep neural networks. Next, we learn the network parameters by amortized variational inferenc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#20309;&#32452;&#21512;&#30340;&#21487;&#20998;&#31163;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.19706</link><description>&lt;p&gt;
&#21487;&#20998;&#30446;&#26631;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65306;&#25512;&#21160;&#21160;&#24577;&#35268;&#21010;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Optimal Decision Trees for Separable Objectives: Pushing the Limits of Dynamic Programming. (arXiv:2305.19706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#20309;&#32452;&#21512;&#30340;&#21487;&#20998;&#31163;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#30340;&#20840;&#23616;&#20248;&#21270;&#22312;&#20934;&#30830;&#24615;&#65292;&#22823;&#23567;&#21644;&#20154;&#31867;&#21487;&#29702;&#35299;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#36890;&#29992;&#27714;&#35299;&#22120;&#65292;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#23558;&#23376;&#26641;&#20316;&#20026;&#29420;&#31435;&#30340;&#23376;&#38382;&#39064;&#35299;&#20915;&#26469;&#21033;&#29992;&#26641;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20165;&#36866;&#29992;&#20110;&#21487;&#20197;&#20998;&#21035;&#20248;&#21270;&#23376;&#26641;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35814;&#32454;&#30740;&#31350;&#20102;&#36825;&#31181;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#23454;&#29616;&#36825;&#31181;&#21487;&#20998;&#31163;&#32422;&#26463;&#21644;&#30446;&#26631;&#20219;&#24847;&#32452;&#21512;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#22235;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#65292;&#21516;&#26102;&#20063;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. However, many of the methods used rely on general-purpose solvers for which scalability remains an issue. Dynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems. However, this only works when an objective can be optimized separately for subtrees. We explore this relationship in detail and show necessary and sufficient conditions for such separability and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints. Experiments on four application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550; Diff-Instruct&#65292;&#33021;&#22815;&#20197;&#26080;&#38656;&#25968;&#25454;&#26041;&#24335;&#23558;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#65292;&#20165;&#38656;&#39044;&#35757;&#32451; DM &#21644;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#26159;&#24314;&#31435;&#22312;&#20005;&#35880;&#30340;&#25968;&#23398;&#22522;&#30784;&#19978;&#30340;&#65292;&#25351;&#23548;&#36807;&#31243;&#30452;&#25509;&#23545;&#24212;&#20110;&#26368;&#23567;&#21270;&#19968;&#31181;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;Integral Kullback-Leibler (IKL) &#25955;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22270;&#20687;&#21512;&#25104;&#21644;&#35270;&#39057;&#39044;&#27979;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18455</link><description>&lt;p&gt;
Diff-Instruct: &#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20013;&#20256;&#36882;&#30693;&#35782;&#30340;&#36890;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models. (arXiv:2305.18455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550; Diff-Instruct&#65292;&#33021;&#22815;&#20197;&#26080;&#38656;&#25968;&#25454;&#26041;&#24335;&#23558;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#65292;&#20165;&#38656;&#39044;&#35757;&#32451; DM &#21644;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#26159;&#24314;&#31435;&#22312;&#20005;&#35880;&#30340;&#25968;&#23398;&#22522;&#30784;&#19978;&#30340;&#65292;&#25351;&#23548;&#36807;&#31243;&#30452;&#25509;&#23545;&#24212;&#20110;&#26368;&#23567;&#21270;&#19968;&#31181;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;Integral Kullback-Leibler (IKL) &#25955;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22270;&#20687;&#21512;&#25104;&#21644;&#35270;&#39057;&#39044;&#27979;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35757;&#32451;&#23481;&#26131;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#26679;&#26412;&#36136;&#37327;&#39640;&#65292;&#25193;&#25955;&#27169;&#22411; (DMs) &#24050;&#25104;&#20026;&#29983;&#25104;&#24314;&#27169;&#30340;&#39318;&#36873;&#26041;&#27861;&#65292;&#24182;&#26377;&#22823;&#37327;&#24050;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#38598;&#12290;&#39044;&#35757;&#32451; DMs &#21253;&#21547;&#26377;&#20851;&#25968;&#25454;&#20998;&#24067;&#30340;&#22797;&#26434;&#20449;&#24687;&#65292;&#23545;&#19979;&#28216;&#24212;&#29992;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#32771;&#34385;&#20174;&#39044;&#35757;&#32451; DMs &#20013;&#23398;&#20064;&#24182;&#20197;&#26080;&#38656;&#25968;&#25454;&#26041;&#24335;&#23558;&#20854;&#30693;&#35782;&#20256;&#36882;&#32473;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550; Diff-Instruct&#65292;&#35813;&#26694;&#26550;&#33021;&#25351;&#23548;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#21482;&#35201;&#29983;&#25104;&#30340;&#26679;&#26412;&#22312;&#27169;&#22411;&#21442;&#25968;&#26041;&#38754;&#26159;&#21487;&#24494;&#30340;&#12290;&#25105;&#20204;&#30340; Diff-Instruct &#24314;&#31435;&#22312;&#19968;&#20010;&#20005;&#35880;&#30340;&#25968;&#23398;&#22522;&#30784;&#19978;&#65292;&#20854;&#20013;&#25351;&#23548;&#36807;&#31243;&#30452;&#25509;&#23545;&#24212;&#20110;&#26368;&#23567;&#21270;&#31216;&#20026;&#31215;&#20998;Kullback-Leibler (IKL) &#25955;&#24230;&#30340;&#26032;&#22411;&#25955;&#24230;&#12290;IKL &#26159;&#38024;&#23545; DMs &#23450;&#21046;&#30340;&#65292;&#36890;&#36807;&#35745;&#31639;&#27839;&#25193;&#25955;&#36712;&#36857;&#30340; KL &#25955;&#24230;&#30340;&#31215;&#20998;&#26469;&#25429;&#33719;&#25193;&#25955;&#36807;&#31243;&#20449;&#24687;&#65292;&#22240;&#27492;&#21482;&#38656;&#35201;&#19968;&#20010;&#39044;&#35757;&#32451; DM &#21644;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24212;&#29992;&#31243;&#24207;&#65306;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22270;&#20687;&#21512;&#25104;&#21644;&#35270;&#39057;&#39044;&#27979;&#20013;&#23637;&#31034;&#20854;&#20248;&#36234;&#24615;&#26469;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the ease of training, ability to scale, and high sample quality, diffusion models (DMs) have become the preferred option for generative modeling, with numerous pre-trained models available for a wide variety of datasets. Containing intricate information about data distributions, pre-trained DMs are valuable assets for downstream applications. In this work, we consider learning from pre-trained DMs and transferring their knowledge to other generative models in a data-free fashion. Specifically, we propose a general framework called Diff-Instruct to instruct the training of arbitrary generative models as long as the generated samples are differentiable with respect to the model parameters. Our proposed Diff-Instruct is built on a rigorous mathematical foundation where the instruction process directly corresponds to minimizing a novel divergence we call Integral Kullback-Leibler (IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL divergence along a diffu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18342</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#32534;&#31243;&#20013;&#31070;&#32463;&#20219;&#21153;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#26032;&#30340;&#20869;&#23481;&#65292;&#29983;&#25104;&#24335;&#31070;&#32463;&#27169;&#22411;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31070;&#32463;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#21487;&#35270;&#21270;&#32534;&#31243;&#29615;&#22659;&#19979;&#32473;&#23450;&#30340;&#35268;&#33539;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#20687; GPT-4 &#36825;&#26679;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21512;&#25104;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#36923;&#36753;&#21644;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415; NeurTaskSyn&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#21512;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;NeurTaskSyn &#30001;&#20004;&#20010;&#37096;&#20998;&#26500;&#25104;&#65306;&#31532;&#19968;&#20010;&#37096;&#20998;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#29983;&#25104;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#25351;&#23548;&#24213;&#23618;&#31526;&#21495;&#25191;&#34892;&#24341;&#25806;&#29983;&#25104;&#21487;&#35270;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visua
&lt;/p&gt;</description></item><item><title>Translatotron 3&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#36827;&#34892;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65292;&#26080;&#38656;&#37197;&#23545;&#30340;&#25968;&#25454;&#25110;&#19987;&#19994;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#20445;&#30041;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.17547</link><description>&lt;p&gt;
Translatotron 3: &#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#36827;&#34892;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Translatotron 3: Speech to Speech Translation with Monolingual Data. (arXiv:2305.17547v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17547
&lt;/p&gt;
&lt;p&gt;
Translatotron 3&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#36827;&#34892;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65292;&#26080;&#38656;&#37197;&#23545;&#30340;&#25968;&#25454;&#25110;&#19987;&#19994;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#20445;&#30041;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Translatotron 3&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#12289;&#26080;&#30417;&#30563;&#30340;&#23884;&#20837;&#26144;&#23556;&#21644;&#22238;&#35793;&#23558;&#30452;&#25509;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#20174;&#21333;&#22768;&#36947;&#35821;&#38899;-&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#23436;&#20840;&#26080;&#30417;&#30563;&#22320;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#33521;&#35821;&#20043;&#38388;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Translatotron 3&#20248;&#20110;&#22522;&#20934;&#32423;&#32852;&#31995;&#32479;&#65292;&#22312; synthesized Unpaired-Conversational &#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;18.14 BLEU&#20998;&#25968;&#30340;&#25552;&#39640;&#12290;&#19982;&#38656;&#35201;&#30495;&#23454;&#37197;&#23545;&#25968;&#25454;&#25110;&#19987;&#19994;&#24314;&#27169;&#26469;&#22797;&#21046;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#30417;&#30563;&#26041;&#27861;&#19981;&#21516;&#65292;Translatotron 3&#23637;&#31034;&#20102;&#23427;&#20445;&#30041;&#20102;&#20687;&#26242;&#20572;&#12289;&#35828;&#35805;&#36895;&#24230;&#21644;&#35828;&#35805;&#20154;&#36523;&#20221;&#31561;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Translatotron 3, a novel approach to train a direct speech-to-speech translation model from monolingual speech-text datasets only in a fully unsupervised manner. Translatotron 3 combines masked autoencoder, unsupervised embedding mapping, and back-translation to achieve this goal. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting 18.14 BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, which is unavailable, or specialized modeling to replicate para-/non-linguistic information, Translatotron 3 showcases its capability to retain para-/non-linguistic such as pauses, speaking rates, and speaker identity. Audio samples can be found in our website this http URL
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#21333;&#20301;&#25293;&#21334;&#20013;&#23398;&#20064;&#21644;&#21246;&#32467;&#30340;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#25293;&#21334;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#35774;&#32622;&#20013;&#30340;&#29609;&#23478;&#20986;&#20215;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17402</link><description>&lt;p&gt;
&#23398;&#20064;&#19982;&#21246;&#32467;&#22312;&#22810;&#21333;&#20301;&#25293;&#21334;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning and Collusion in Multi-unit Auctions. (arXiv:2305.17402v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#21333;&#20301;&#25293;&#21334;&#20013;&#23398;&#20064;&#21644;&#21246;&#32467;&#30340;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#25293;&#21334;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#35774;&#32622;&#20013;&#30340;&#29609;&#23478;&#20986;&#20215;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#25293;&#21334;&#19982;&#32479;&#19968;&#23450;&#20215;&#65292;&#36825;&#31181;&#25293;&#21334;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#29992;&#20110;&#20998;&#37197;&#35832;&#22914;&#30899;&#25490;&#25918;&#35768;&#21487;&#35777;&#20043;&#31867;&#30340;&#21830;&#21697;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;$K$&#20010;&#30456;&#21516;&#30340;&#21830;&#21697;&#21333;&#20803;&#34987;&#21334;&#32473;&#19968;&#32452;&#20855;&#26377;&#36882;&#20943;&#36793;&#38469;&#22238;&#25253;&#20215;&#20540;&#30340;&#20080;&#23478;&#12290;&#20080;&#23478;&#20026;&#21830;&#21697;&#21333;&#20301;&#25552;&#20132;&#20986;&#20215;&#65292;&#28982;&#21518;&#26681;&#25454;&#20986;&#20215;&#30830;&#23450;&#19968;&#20010;&#21333;&#20301;&#30340;&#20215;&#26684;$p$&#65292;&#20351;&#24471;&#25152;&#26377;&#21333;&#20301;&#37117;&#33021;&#34987;&#21806;&#20986;&#12290;&#25105;&#20204;&#32771;&#34385;&#25293;&#21334;&#30340;&#20004;&#20010;&#21464;&#20307;&#65292;&#20854;&#20013;&#20215;&#26684;&#20998;&#21035;&#35774;&#20026;&#31532;$K$&#39640;&#20986;&#20215;&#21644;&#31532;$(K+1)$&#39640;&#20986;&#20215;&#12290;&#25105;&#20204;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#20013;&#20998;&#26512;&#20102;&#25293;&#21334;&#30340;&#24615;&#36136;&#12290;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#29609;&#23478;$i$&#38754;&#20020;&#30340;&#38382;&#39064;&#65306;&#22312;&#36807;&#21435;&#30340;&#25293;&#21334;&#20013;&#65292;&#32473;&#23450;&#21253;&#21547;&#31454;&#20105;&#23545;&#25163;&#25552;&#20132;&#30340;&#20986;&#20215;&#30340;&#25968;&#25454;&#38598;&#65292;&#25214;&#21040;&#19968;&#20010;&#20986;&#20215;&#21521;&#37327;&#65292;&#20351;&#24471;&#29609;&#23478;$i$&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#32047;&#31215;&#25928;&#29992;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20854;&#31561;&#20215;&#20110;&#22312;&#20180;&#32454;&#26500;&#36896;&#30340;&#26377;&#21521;&#22270;&#19978;&#25214;&#21040;&#26368;&#22823;&#26435;&#37325;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider repeated multi-unit auctions with uniform pricing, which are widely used in practice for allocating goods such as carbon licenses. In each round, $K$ identical units of a good are sold to a group of buyers that have valuations with diminishing marginal returns. The buyers submit bids for the units, and then a price $p$ is set per unit so that all the units are sold. We consider two variants of the auction, where the price is set to the $K$-th highest bid and $(K+1)$-st highest bid, respectively.  We analyze the properties of this auction in both the offline and online settings. In the offline setting, we consider the problem that one player $i$ is facing: given access to a data set that contains the bids submitted by competitors in past auctions, find a bid vector that maximizes player $i$'s cumulative utility on the data set. We design a polynomial time algorithm for this problem, by showing it is equivalent to finding a maximum-weight path on a carefully constructed direc
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#22312;&#26410;&#30693;&#19988;&#20010;&#24615;&#21270;&#25805;&#32437;&#24433;&#21709;&#19979;&#30340;&#25112;&#30053;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#30340;&#23450;&#20041;&#65292;&#26088;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#25805;&#32437;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16501</link><description>&lt;p&gt;
&#26410;&#30693;&#20010;&#24615;&#21270;&#25805;&#32437;&#19979;&#30340;&#25112;&#30053;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Strategic Classification under Unknown Personalized Manipulation. (arXiv:2305.16501v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16501
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#22312;&#26410;&#30693;&#19988;&#20010;&#24615;&#21270;&#25805;&#32437;&#24433;&#21709;&#19979;&#30340;&#25112;&#30053;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#30340;&#23450;&#20041;&#65292;&#26088;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#25805;&#32437;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25112;&#30053;&#20998;&#31867;&#20013;&#30340;&#22522;&#30784;&#38169;&#35823;&#30028;&#38480;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#20195;&#29702;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25112;&#30053;&#24615;&#22320;&#25805;&#32437;&#20854;&#29305;&#24449;&#21521;&#37327;&#20197;&#39044;&#27979;&#20026;&#27491;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#20010;&#30830;&#23450;&#22823;&#23398;&#24405;&#21462;&#30340;&#20998;&#31867;&#22120;&#65292;&#23398;&#29983;&#20505;&#36873;&#20154;&#21487;&#33021;&#20250;&#23581;&#35797;&#36873;&#25321;&#26356;&#23481;&#26131;&#30340;&#35838;&#31243;&#26469;&#25552;&#39640;&#20182;&#20204;&#30340;GPA&#65292;&#37325;&#26032;&#21442;&#21152;SAT&#24182;&#26356;&#25442;&#23398;&#26657;&#65292;&#20197;&#23581;&#35797;&#27450;&#39575;&#20998;&#31867;&#22120;&#12290; &#22312;&#25991;&#29486;&#20013;&#65292;&#29699;&#25805;&#32437;&#26159;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#25805;&#32437;&#31867;&#21035;&#65292;&#20195;&#29702;&#21487;&#20197;&#22312;&#26377;&#30028;&#21322;&#24452;&#29699;&#20869;&#20462;&#25913;&#20854;&#29305;&#24449;&#21521;&#37327;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#35748;&#20026;&#25805;&#32437;&#26159;&#20010;&#24615;&#21270;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#20195;&#29702;&#21487;&#20197;&#25317;&#26377;&#19981;&#21516;&#27700;&#24179;&#30340;&#25805;&#32437;&#33021;&#21147;&#65288;&#20363;&#22914;&#65292;&#29699;&#20307;&#25805;&#32437;&#30340;&#21464;&#21270;&#21322;&#24452;&#65289;&#65292;&#24182;&#19988;&#23545;&#23398;&#20064;&#32773;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20132;&#20114;&#27169;&#22411;&#20013;&#24418;&#24335;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#39318;&#20808;&#37096;&#32626;&#20998;&#31867;&#22120;&#65292;&#20195;&#29702;&#22312;&#20854;&#25805;&#32437;&#38598;&#21512;&#20869;&#25805;&#32437;&#29305;&#24449;&#21521;&#37327;&#20197;&#25805;&#20316;&#37096;&#32626;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the fundamental mistake bound and sample complexity in the strategic classification, where agents can strategically manipulate their feature vector up to an extent in order to be predicted as positive. For example, given a classifier determining college admission, student candidates may try to take easier classes to improve their GPA, retake SAT and change schools in an effort to fool the classifier. Ball manipulations are a widely studied class of manipulations in the literature, where agents can modify their feature vector within a bounded radius ball. Unlike most prior work, our work considers manipulations to be personalized, meaning that agents can have different levels of manipulation abilities (e.g., varying radii for ball manipulations), and unknown to the learner.  We formalize the learning problem in an interaction model where the learner first deploys a classifier and the agent manipulates the feature vector within their manipulation set to game the deployed classif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DoWG&#30340;&#26080;&#21442;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#26082;&#39640;&#25928;&#21448;&#36890;&#29992;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#20110;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#38382;&#39064;&#65292;&#24182;&#19988;&#26080;&#38656;&#22238;&#28335;&#25628;&#32034;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.16284</link><description>&lt;p&gt;
DoWG&#23637;&#31034;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#36890;&#29992;&#26080;&#21442;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method. (arXiv:2305.16284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DoWG&#30340;&#26080;&#21442;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#26082;&#39640;&#25928;&#21448;&#36890;&#29992;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#20110;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#38382;&#39064;&#65292;&#24182;&#19988;&#26080;&#38656;&#22238;&#28335;&#25628;&#32034;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26131;&#20110;&#23454;&#29616;&#30340;&#26080;&#21442;&#25968;&#26799;&#24230;&#20248;&#21270;&#22120;&#65306;DoWG&#65288;Weighted Gradients&#30340;&#36317;&#31163;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#26159;&#39640;&#25928;&#30340;&#8212;&#8212;&#22312;&#19981;&#35843;&#25972;&#20219;&#20309;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#21305;&#37197;&#20248;&#21270;&#20984;&#20248;&#21270;&#20013;&#26368;&#20248;&#35843;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#30452;&#21040;&#23545;&#25968;&#22240;&#23376;&#65292;&#24182;&#19988;&#26159;&#36890;&#29992;&#30340;&#8212;&#8212;&#33258;&#21160;&#36866;&#24212;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#38382;&#39064;&#12290;&#19982;AdaGrad&#65292;Adam&#25110;DoG&#31561;&#27969;&#34892;&#31639;&#27861;&#35745;&#31639;&#24179;&#26041;&#26799;&#24230;&#30340;&#36816;&#34892;&#24179;&#22343;&#20540;&#19981;&#21516;&#65292;DoWG&#20445;&#25345;&#36816;&#34892;&#24179;&#22343;&#20540;&#30340;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#21152;&#26435;&#29256;&#26412;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#25152;&#38656;&#30340;&#24615;&#36136;&#33267;&#20851;&#37325;&#35201;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;DoWG&#26159;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#22238;&#28335;&#25628;&#32034;&#36807;&#31243;&#30340;&#26080;&#21442;&#25968;&#65292;&#39640;&#25928;&#21644;&#36890;&#29992;&#31639;&#27861;&#12290;&#23427;&#36824;&#26159;&#31532;&#19968;&#20010;&#36866;&#24212;&#20110;&#24179;&#31283;&#20248;&#21270;&#30340;&#26080;&#21442;&#25968;AdaGrad&#26679;&#24335;&#31639;&#27861;&#12290;&#20026;&#20102;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;DoWG&#22312;&#31283;&#23450;&#30340;&#36793;&#32536;&#35757;&#32451;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new easy-to-implement parameter-free gradient-based optimizer: DoWG (Distance over Weighted Gradients). We prove that DoWG is efficient -- matching the convergence rate of optimally tuned gradient descent in convex optimization up to a logarithmic factor without tuning any parameters, and universal -- automatically adapting to both smooth and nonsmooth problems. While popular algorithms such as AdaGrad, Adam, or DoG compute a running average of the squared gradients, DoWG maintains a new distance-based weighted version of the running average, which is crucial to achieve the desired properties. To our best knowledge, DoWG is the first parameter-free, efficient, and universal algorithm that does not require backtracking search procedures. It is also the first parameter-free AdaGrad style algorithm that adapts to smooth optimization. To complement our theory, we also show empirically that DoWG trains at the edge of stability, and validate its effectiveness on practic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Koopman&#26680;&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21464;&#12290;&#35813;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#35270;&#39057;&#39044;&#27979;&#21644;&#20132;&#36890;&#39044;&#27979;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#22343;&#26377;&#20248;&#24322;&#34920;&#29616;&#65292;&#24182;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#23398;&#20064;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.16215</link><description>&lt;p&gt;
Koopman&#26680;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Koopman Kernel Regression. (arXiv:2305.16215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16215
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Koopman&#26680;&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21464;&#12290;&#35813;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#35270;&#39057;&#39044;&#27979;&#21644;&#20132;&#36890;&#39044;&#27979;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#22343;&#26377;&#20248;&#24322;&#34920;&#29616;&#65292;&#24182;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#23398;&#20064;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20915;&#31574;&#21046;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#24378;&#21270;&#23398;&#20064;&#65292;&#20381;&#36182;&#20110;&#27169;&#25311;&#22120;&#25110;&#39044;&#27979;&#27169;&#22411;&#26469;&#39044;&#27979;&#24863;&#20852;&#36259;&#30340;&#37327;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#20363;&#22914;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#25110;&#31574;&#30053;&#30340;&#22870;&#21169;&#12290;&#36825;&#20123;&#22797;&#26434;&#29616;&#35937;&#30340;&#39044;&#27979;&#36890;&#24120;&#30001;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#21160;&#21147;&#31995;&#32479;&#25551;&#36848;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#22522;&#20110;&#20248;&#21270;&#30340;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#20351;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;Koopman&#31639;&#23376;&#29702;&#35770;&#36890;&#36807;&#36890;&#36807;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#25551;&#36848;&#39044;&#27979;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#31995;&#32479;&#20998;&#26512;&#21644;&#38271;&#26399;&#39044;&#27979;&#21464;&#24471;&#31616;&#21333;--&#21482;&#28041;&#21450;&#30697;&#38453;&#20056;&#27861;&#12290;&#28982;&#32780;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#32447;&#24615;&#31995;&#32479;&#36890;&#24120;&#26159;&#38750;&#24179;&#20961;&#30340;&#21644;&#26410;&#30693;&#30340;&#65292;&#38656;&#35201;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#23384;&#22312;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#32570;&#20047;&#20851;&#38190;&#30340;&#23398;&#20064;&#29702;&#35770;&#20445;&#35777;&#65292;&#22240;&#27492;&#25152;&#33719;&#24471;&#30340;&#27169;&#22411;&#22312;&#25968;&#25454;&#21644;&#32500;&#24230;&#22686;&#21152;&#26102;&#30340;&#34892;&#20026;&#36890;&#24120;&#19981;&#28165;&#26970;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Koopman&#26680;&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#30452;&#25509;&#20174;&#21382;&#21490;&#35266;&#23519;&#20013;&#23398;&#20064;&#21040;&#26410;&#26469;&#39044;&#27979;&#22312;Koopman&#31639;&#23376;&#31354;&#38388;&#20013;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20139;&#26377;&#21487;&#35777;&#26126;&#30340;&#23398;&#20064;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#21305;&#37197;&#65288;&#25110;&#20248;&#20110;&#65289;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#35270;&#39057;&#39044;&#27979;&#21644;&#20132;&#36890;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning approaches for decision making, such as reinforcement learning, rely on simulators or predictive models to forecast the time-evolution of quantities of interest, e.g., the state of an agent or the reward of a policy. Forecasts of such complex phenomena are commonly described by highly nonlinear dynamical systems, making their use in optimization-based decision-making challenging. Koopman operator theory offers a beneficial paradigm for addressing this problem by characterizing forecasts via linear dynamical systems. This makes system analysis and long-term predictions simple -- involving only matrix multiplications. However, the transformation to a linear system is generally non-trivial and unknown, requiring learning-based approaches. While there exists a variety of approaches, they usually lack crucial learning-theoretic guarantees, such that the behavior of the obtained models with increasing data and dimensionality is often unclear. We address the aforemention
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#31934;&#30830;MLE&#23398;&#20064;&#12289;&#26377;&#25928;&#25277;&#26679;&#26032;&#30340;&#19977;&#20803;&#32452;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#65292;&#33719;&#24471;&#20102;&#27604;&#21407;&#22987;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15944</link><description>&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;&#27010;&#29575;&#30005;&#36335;&#23558;&#24744;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits. (arXiv:2305.15944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#31934;&#30830;MLE&#23398;&#20064;&#12289;&#26377;&#25928;&#25277;&#26679;&#26032;&#30340;&#19977;&#20803;&#32452;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#65292;&#33719;&#24471;&#20102;&#27604;&#21407;&#22987;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#25104;&#21151;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#21487;&#29992;&#20316;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#23558;&#20854;&#37325;&#26032;&#35299;&#37322;&#25104;&#20026;&#30005;&#36335;&#24418;&#24335;--&#36825;&#26159;&#19968;&#31181;&#20801;&#35768;&#26377;&#25928;&#36793;&#38469;&#21270;&#30340;&#32422;&#26463;&#35745;&#31639;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#26041;&#27861;&#26469;&#33719;&#24471;&#26377;&#25928;&#30340;&#29983;&#25104;&#30005;&#36335;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#26041;&#27861;&#26159;&#23558;&#20854;&#28608;&#27963;&#38480;&#21046;&#20026;&#38750;&#36127;&#25968;&#65292;&#21478;&#19968;&#20010;&#26041;&#27861;&#26159;&#23558;&#20854;&#36755;&#20986;&#24179;&#26041;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#19981;&#20250;&#24433;&#21709;&#21040;&#39044;&#27979;&#33410;&#28857;&#36830;&#36793;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#30005;&#36335;&#26694;&#26550;&#20351;&#24471;MLE&#30340;&#31934;&#30830;&#23398;&#20064;&#12289;&#26032;&#19977;&#20803;&#32452;&#30340;&#26377;&#25928;&#25277;&#26679;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#24471;&#20197;&#28385;&#36275;&#25104;&#20026;&#21487;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25317;&#26377;&#25968;&#30334;&#19975;&#20010;&#23454;&#20307;&#30340;&#22270;&#19978;&#27604;&#21407;&#22987;&#30340;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some of the most successful knowledge graph embedding (KGE) models for link prediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits -- constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#25391;&#24133;&#30340;&#20809; plethysmography (PPG) &#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35265;&#24615;&#22270;&#21644;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;&#23545;&#24515;&#29575;&#21644;&#34880;&#31649;&#32769;&#21270;&#31561;&#29983;&#29289;&#29305;&#24449;&#30340;&#31283;&#20581;&#20272;&#35745;&#21644;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.14062</link><description>&lt;p&gt;
&#19981;&#20381;&#36182;&#20110;&#25391;&#24133;&#30340;&#20809; plethysmography (PPG) &#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65306;&#36890;&#36807;&#21487;&#35265;&#24615;&#22270;&#21644;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning. (arXiv:2305.14062v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#25391;&#24133;&#30340;&#20809; plethysmography (PPG) &#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35265;&#24615;&#22270;&#21644;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;&#23545;&#24515;&#29575;&#21644;&#34880;&#31649;&#32769;&#21270;&#31561;&#29983;&#29289;&#29305;&#24449;&#30340;&#31283;&#20581;&#20272;&#35745;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#20307;&#31215;&#25551;&#35760;&#27861; (PPG) &#26159;&#20351;&#29992;&#20809;&#27979;&#37327;&#34880;&#28082;&#20307;&#31215;&#30340;&#21464;&#21270;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#26159;&#22823;&#22810;&#25968;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#29305;&#24449;&#12290;PPG&#20449;&#21495;&#33021;&#22815;&#25552;&#20379;&#23545;&#20154;&#20307;&#24490;&#29615;&#31995;&#32479;&#30340;&#27934;&#23519;&#65292;&#24182;&#21487;&#29992;&#20110;&#25552;&#21462;&#21508;&#31181;&#29983;&#29289;&#29305;&#24449;&#65292;&#20363;&#22914;&#24515;&#29575;&#21644;&#34880;&#31649;&#32769;&#21270;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#65292;&#20294;&#35768;&#22810;&#31639;&#27861;&#23384;&#22312;&#38480;&#21046;&#65292;&#21253;&#25324;&#36807;&#22810;&#22320;&#20381;&#36182;&#20154;&#24037;&#26657;&#20934;&#12289;&#39640;&#20449;&#21495;&#36136;&#37327;&#35201;&#27714;&#21644;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22270;&#35770;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#30340;PPG&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#25391;&#24133;&#26080;&#20851;&#24182;&#19988;&#23545;&#20223;&#23556;&#21464;&#25442;&#19981;&#21464;&#12290;&#23427;&#36824;&#38656;&#35201;&#26368;&#23569;&#30340;&#39044;&#22788;&#29702;&#65292;&#36890;&#36807;RGB&#36890;&#36947;&#34701;&#21512;&#20449;&#24687;&#65292;&#24182;&#22312;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#31283;&#20581;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;VGTL-net&#22312;&#34880;&#31649;&#32769;&#21270;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#31283;&#20581;&#30340;&#20272;&#35745;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) refers to the measurement of variations in blood volume using light and is a feature of most wearable devices. The PPG signals provide insight into the body's circulatory system and can be employed to extract various bio-features, such as heart rate and vascular ageing. Although several algorithms have been proposed for this purpose, many exhibit limitations, including heavy reliance on human calibration, high signal quality requirements, and a lack of generalisation. In this paper, we introduce a PPG signal processing framework that integrates graph theory and computer vision algorithms, to provide an analysis framework which is amplitude-independent and invariant to affine transformations. It also requires minimal preprocessing, fuses information through RGB channels and exhibits robust generalisation across tasks and datasets. The proposed VGTL-net achieves state-of-the-art performance in the prediction of vascular ageing and demonstrates robust estimation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11554</link><description>&lt;p&gt;
ToolkenGPT&#65306;&#36890;&#36807;&#24037;&#20855;&#23884;&#20837;&#25193;&#20805;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#29992;&#24037;&#20855;&#28436;&#31034;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#26082;&#36153;&#26102;&#21448;&#21463;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#24037;&#20855;&#38598;&#12290;&#26368;&#36817;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#20363;&#32531;&#35299;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#26159;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21482;&#20801;&#35768;&#28436;&#31034;&#20960;&#27425;&#65292;&#23548;&#33268;&#23545;&#24037;&#20855;&#30340;&#29702;&#35299;&#19981;&#22815;&#20805;&#20998;&#12290;&#27492;&#22806;&#65292;&#24403;&#26377;&#22823;&#37327;&#24037;&#20855;&#21487;&#20379;&#36873;&#25321;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#33021;&#23436;&#20840;&#26080;&#27861;&#27491;&#24120;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$\textbf{ToolkenGPT}$&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;$\underline{&#24037;&#20855;}$&#34920;&#31034;&#20026;&#19968;&#20010;$\underline{token}$&#65288;$\textit{toolken}$&#65289;&#65292;&#24182;&#20026;&#20854;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#65292;&#20351;&#24471;&#24037;&#20855;&#35843;&#29992;&#19982;&#29983;&#25104;&#24120;&#35268;&#21333;&#35789;&#26631;&#35760;&#30340;&#26041;&#24335;&#30456;&#21516;&#12290;&#19968;&#26086;&#35302;&#21457;&#20102;toolken&#65292;LLM&#34987;&#25552;&#31034;&#23436;&#25104;&#24037;&#20855;&#25191;&#34892;&#25152;&#38656;&#30340;&#21442;&#25968;&#12290;ToolkenGPT&#25552;&#20379;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;1&#65289;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#20197;&#25193;&#20805;LLM&#19982;&#22806;&#37096;&#24037;&#20855;&#30340;&#20132;&#20114;&#65292;2&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#20363;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;3&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\underline{tool}$ as a to$\underline{ken}$ ($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#21151;&#33021;&#31561;&#20215;&#30340;&#35282;&#24230;&#20986;&#21457;&#30740;&#31350;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#21457;&#29616;&#21033;&#29992;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#29305;&#24615;&#21487;&#20197;&#38477;&#20302;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#65292;&#36890;&#36807;&#36807;&#21442;&#25968;&#21270;&#21487;&#20197;&#22686;&#21152;&#35757;&#32451;&#32593;&#32476;&#30340;&#23481;&#26131;&#31243;&#24230;&#65292;&#24182;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#29702;&#35299;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.11417</link><description>&lt;p&gt;
&#20174;&#21151;&#33021;&#31561;&#20215;&#30340;&#35282;&#24230;&#30475;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complexity of Feed-Forward Neural Networks from the Perspective of Functional Equivalence. (arXiv:2305.11417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#21151;&#33021;&#31561;&#20215;&#30340;&#35282;&#24230;&#20986;&#21457;&#30740;&#31350;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#21457;&#29616;&#21033;&#29992;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#29305;&#24615;&#21487;&#20197;&#38477;&#20302;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#65292;&#36890;&#36807;&#36807;&#21442;&#25968;&#21270;&#21487;&#20197;&#22686;&#21152;&#35757;&#32451;&#32593;&#32476;&#30340;&#23481;&#26131;&#31243;&#24230;&#65292;&#24182;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#29702;&#35299;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32771;&#23519;&#21151;&#33021;&#31561;&#20215;&#30340;&#27010;&#24565;&#26469;&#30740;&#31350;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#35813;&#27010;&#24565;&#34920;&#26126;&#19981;&#21516;&#30340;&#32593;&#32476;&#21442;&#25968;&#21270;&#21487;&#20197;&#23548;&#33268;&#30456;&#21516;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#29305;&#24615;&#20026;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#31867;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35206;&#30422;&#25968;&#19978;&#30028;&#65292;&#21457;&#29616;&#21033;&#29992;&#35813;&#24615;&#36136;&#21487;&#20197;&#38477;&#20302;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#30340;&#23545;&#31216;&#32467;&#26500;&#65292;&#25105;&#20204;&#35777;&#26126;&#36866;&#24403;&#30340;&#38543;&#26426;&#21442;&#25968;&#21021;&#22987;&#21270;&#31574;&#30053;&#21487;&#20197;&#22686;&#21152;&#20248;&#21270;&#25910;&#25947;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36807;&#21442;&#25968;&#21270;&#30340;&#32593;&#32476;&#24448;&#24448;&#26356;&#23481;&#26131;&#35757;&#32451;&#65292;&#21363;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#23485;&#24230;&#20250;&#23548;&#33268;&#26377;&#25928;&#21442;&#25968;&#31354;&#38388;&#30340;&#20307;&#31215;&#36235;&#36817;&#20110;&#38646;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36807;&#21442;&#25968;&#21270;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#21644;&#20248;&#21270;&#29702;&#35299;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the complexity of feed-forward neural networks by examining the concept of functional equivalence, which suggests that different network parameterizations can lead to the same function. We utilize the permutation invariance property to derive a novel covering number bound for the class of feedforward neural networks, which reveals that the complexity of a neural network can be reduced by exploiting this property. Furthermore, based on the symmetric structure of parameter space, we demonstrate that an appropriate strategy of random parameter initialization can increase the probability of convergence for optimization. We found that overparameterized networks tend to be easier to train in the sense that increasing the width of neural networks leads to a vanishing volume of the effective parameter space. Our findings offer new insights into overparameterization and have significant implications for understanding generalization and optimization in deep learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10818</link><description>&lt;p&gt;
&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26377;&#28508;&#22312;&#22909;&#22788;&#65292;&#20294;&#30446;&#21069;&#20844;&#24320;&#30340;&#23454;&#29616;&#12289;&#35757;&#32451;&#27169;&#22411;&#25110;&#21487;&#37325;&#29616;&#30340;&#35757;&#32451;&#31243;&#24207;&#24182;&#19981;&#23384;&#22312;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;C4&#25968;&#25454;&#38598;&#31616;&#21270;&#30340;DDLM&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#36895;&#24230;&#26356;&#24555;&#30340;&#37319;&#26679;&#30340;&#26032;&#22411;&#26089;&#26399;&#36864;&#20986;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#38024;&#23545;&#20351;&#29992;&#24471;&#20998;&#25554;&#20540;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#27492;&#21069;&#27809;&#26377;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;LM&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#31867;&#20219;&#21153;&#65289;&#65292;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;DDLM&#30340;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20379;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#30340;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#39044;&#20808;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#22312;&#26410;&#26469;&#30340;D&#30456;&#20851;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the potential benefits of Diffusion Models for NLP applications, publicly available implementations, trained models, or reproducible training procedures currently need to be publicly available. We present the Democratized Diffusion Language Model (DDLM), based on the Continuous Diffusion for Categorical Data (CDCD) framework, to address these challenges. We propose a simplified training procedure for DDLM using the C4 dataset and perform an in-depth analysis of the trained model's behavior. Furthermore, we introduce a novel early-exiting strategy for faster sampling with models trained with score interpolation. Since no previous works aimed at solving downstream tasks with pre-trained Diffusion LM (e.g., classification tasks), we experimented with GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this paper, we propose available training and evaluation pipelines to other researchers and pre-trained DDLM models, which could be used in future research with D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#22270;&#20687;&#30340;&#24635;&#20307;&#32654;&#23398;&#35780;&#20998;&#21644;&#32654;&#23398;&#23646;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#25509;&#36817;&#20154;&#31867;&#34920;&#29616;&#30340;&#25972;&#20307;&#32654;&#23398;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.09373</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22270;&#20687;&#32654;&#23398;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multi-task convolutional neural network for image aesthetic assessment. (arXiv:2305.09373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#22270;&#20687;&#30340;&#24635;&#20307;&#32654;&#23398;&#35780;&#20998;&#21644;&#32654;&#23398;&#23646;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#25509;&#36817;&#20154;&#31867;&#34920;&#29616;&#30340;&#25972;&#20307;&#32654;&#23398;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#20204;&#23545;&#22270;&#20687;&#32654;&#23398;&#20559;&#22909;&#30340;&#29702;&#35299;&#36824;&#36828;&#36828;&#19981;&#22815;&#65292;&#22270;&#20687;&#32654;&#23398;&#35780;&#20272;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#32771;&#34385;&#20102;&#24433;&#21709;&#22270;&#20687;&#32654;&#23398;&#30340;&#22240;&#32032;&#12290;&#25152;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#21516;&#26102;&#23398;&#20064;&#20102;&#22270;&#20687;&#30340;&#24635;&#20307;&#32654;&#23398;&#35780;&#20998;&#20197;&#21450;&#36825;&#20123;&#24050;&#30693;&#32654;&#23398;&#23646;&#24615;&#12290;&#36825;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#36890;&#36807;&#20849;&#20139;&#34920;&#31034;&#23454;&#29616;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#22270;&#20687;&#32654;&#23398;&#30340;&#25972;&#20307;&#35780;&#20998;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#22312;&#32771;&#34385;Spearman&#31561;&#32423;&#30456;&#20851;&#24615;&#26102;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#20154;&#31867;&#34920;&#29616;&#30340;&#25972;&#20307;&#32654;&#23398;&#35780;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21478;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36824;&#36890;&#36807;&#39044;&#27979;&#22270;&#20687;&#30340;&#29305;&#23450;&#32654;&#23398;&#23646;&#24615;&#24320;&#21019;&#20102;&#22810;&#20219;&#21153;&#24212;&#29992;&#30340;&#20808;&#27827;&#12290;
&lt;/p&gt;
&lt;p&gt;
As people's aesthetic preferences for images are far from understood, image aesthetic assessment is a challenging artificial intelligence task. The range of factors underlying this task is almost unlimited, but we know that some aesthetic attributes affect those preferences. In this study, we present a multi-task convolutional neural network that takes into account these attributes. The proposed neural network jointly learns the attributes along with the overall aesthetic scores of images. This multi-task learning framework allows for effective generalization through the utilization of shared representations. Our experiments demonstrate that the proposed method outperforms the state-of-the-art approaches in predicting overall aesthetic scores for images in one benchmark of image aesthetics. We achieve near-human performance in terms of overall aesthetic scores when considering the Spearman's rank correlations. Moreover, our model pioneers the application of multi-tasking in another ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#22823;&#23567;&#23545;&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20020;&#30028;&#20540;&#22788;&#20250;&#20986;&#29616;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#38416;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.06435</link><description>&lt;p&gt;
&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Phase transitions in the mini-batch size for sparse and dense neural networks. (arXiv:2305.06435v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#22823;&#23567;&#23545;&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20020;&#30028;&#20540;&#22788;&#20250;&#20986;&#29616;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#38416;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#25968;&#25454;&#29616;&#22312;&#38750;&#24120;&#26222;&#36941;&#12290;&#23613;&#31649;&#24050;&#32463;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#32570;&#23569;&#23450;&#37327;&#35299;&#37322;&#26368;&#20339;&#23567;&#25209;&#37327;&#22823;&#23567;&#24212;&#35813;&#26159;&#22810;&#22823;&#30340;&#29702;&#35770;&#12290;&#26412;&#25991;&#23581;&#35797;&#31995;&#32479;&#22320;&#29702;&#35299;&#23567;&#25209;&#37327;&#22823;&#23567;&#22312;&#35757;&#32451;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#12290;&#22312;&#25945;&#24072;-&#23398;&#29983;&#24773;&#22659;&#19979;&#65292;&#20351;&#29992;&#31232;&#30095;&#25945;&#24072;&#65292;&#24182;&#32858;&#28966;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#25913;&#21464;&#23567;&#25209;&#37327;&#22823;&#23567;m&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#23398;&#29983;&#30340;&#27867;&#21270;&#24615;&#33021;&#24378;&#28872;&#20381;&#36182;&#20110;m&#65292;&#24182;&#19988;&#21487;&#33021;&#22312;&#20020;&#30028;&#20540;mc&#22788;&#32463;&#21382;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#36825;&#26679;&#24403;m&lt; mc&#26102;&#65292;&#35757;&#32451;&#36807;&#31243;&#22833;&#36133;&#65292;&#32780;&#24403;m&gt; mc&#26102;&#65292;&#23398;&#29983;&#21487;&#20197;&#23436;&#32654;&#22320;&#23398;&#20064;&#25110;&#24456;&#22909;&#22320;&#27867;&#21270;&#25945;&#24072;&#12290;&#30456;&#21464;&#26159;&#30001;&#32479;&#35745;&#21147;&#23398;&#39318;&#27425;&#21457;&#29616;&#30340;&#38598;&#20307;&#29616;&#35937;&#65292;&#24182;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#35266;&#23519;&#21040;&#12290;&#25214;&#21040;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#25913;&#21464;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#30456;&#21464;&#65292;&#21487;&#20197;&#38416;&#26126;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of mini-batches of data in training artificial neural networks is nowadays very common. Despite its broad usage, theories explaining quantitatively how large or small the optimal mini-batch size should be are missing. This work presents a systematic attempt at understanding the role of the mini-batch size in training two-layer neural networks. Working in the teacher-student scenario, with a sparse teacher, and focusing on tasks of different complexity, we quantify the effects of changing the mini-batch size $m$. We find that often the generalization performances of the student strongly depend on $m$ and may undergo sharp phase transitions at a critical value $m_c$, such that for $m&lt;m_c$ the training process fails, while for $m&gt;m_c$ the student learns perfectly or generalizes very well the teacher. Phase transitions are induced by collective phenomena firstly discovered in statistical mechanics and later observed in many fields of science. Finding a phase transition varying the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30740;&#31350;&#20102;&#35299;&#26512;&#24230;&#20989;&#25968;&#20026;&#38750;&#20984;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#24403;&#26426;&#22120;&#23398;&#20064;&#22122;&#22768;&#30340;&#23610;&#24230;&#19982;&#30446;&#26631;&#20989;&#25968;&#30456;&#31561;&#26102;&#65292;&#22312;&#23616;&#37096;&#21306;&#22495;&#20869;&#21021;&#22987;&#21270;&#21518;&#65292;&#20197;&#27491;&#30340;&#27010;&#29575;&#33021;&#22815;&#25910;&#25947;&#21040;&#35813;&#21306;&#22495;&#20869;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.09221</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;Lajasiewicz&#26465;&#20214;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence of stochastic gradient descent under a local Lajasiewicz condition for deep neural networks. (arXiv:2304.09221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30740;&#31350;&#20102;&#35299;&#26512;&#24230;&#20989;&#25968;&#20026;&#38750;&#20984;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#24403;&#26426;&#22120;&#23398;&#20064;&#22122;&#22768;&#30340;&#23610;&#24230;&#19982;&#30446;&#26631;&#20989;&#25968;&#30456;&#31561;&#26102;&#65292;&#22312;&#23616;&#37096;&#21306;&#22495;&#20869;&#21021;&#22987;&#21270;&#21518;&#65292;&#20197;&#27491;&#30340;&#27010;&#29575;&#33021;&#22815;&#25910;&#25947;&#21040;&#35813;&#21306;&#22495;&#20869;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#35299;&#26512;&#24230;&#20989;&#25968;&#20026;&#38750;&#20984;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#26377;&#38480;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#21152;&#20837;&#26368;&#23567;&#30340;&#39069;&#22806;&#20551;&#35774;&#24182;&#20445;&#35777;&#26426;&#22120;&#23398;&#20064;&#22122;&#22768;&#30340;&#23610;&#24230;&#19982;&#30446;&#26631;&#20989;&#25968;&#30456;&#31561;&#65292;&#35777;&#26126;&#20102;&#22312;&#23616;&#37096;&#21306;&#22495;&#20869;&#21021;&#22987;&#21270;&#26102;&#65292;&#20197;&#27491;&#30340;&#27010;&#29575;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#25910;&#25947;&#21040;&#35813;&#21306;&#22495;&#20869;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#26159;&#30830;&#20445;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#25972;&#20010;&#36712;&#36857;&#20197;&#27491;&#30340;&#27010;&#29575;&#20445;&#30041;&#22312;&#23616;&#37096;&#21306;&#22495;&#20869;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#36127;&#38754;&#20998;&#26512;&#65292;&#34920;&#26126;&#20351;&#29992;Robbins-Monro&#31867;&#22411;&#30340;&#27493;&#38271;&#20043;&#38388;&#20855;&#26377;&#26377;&#30028;&#22122;&#22768;&#30340;&#20551;&#35774;&#19981;&#36275;&#20197;&#20445;&#25345;&#35813;&#20851;&#38190;&#37096;&#20998;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend the global convergence result of Chatterjee \cite{chatterjee2022convergence} by considering the stochastic gradient descent (SGD) for non-convex objective functions. With minimal additional assumptions that can be realized by finitely wide neural networks, we prove that if we initialize inside a local region where the \L{}ajasiewicz condition holds, with a positive probability, the stochastic gradient iterates converge to a global minimum inside this region. A key component of our proof is to ensure that the whole trajectories of SGD stay inside the local region with a positive probability. For that, we assume the SGD noise scales with the objective function, which is called machine learning noise and achievable in many real examples. Furthermore, we provide a negative argument to show why using the boundedness of noise with Robbins-Monro type step sizes is not enough to keep the key component valid.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32423;&#32852;&#25351;&#23548;&#19979;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#21462;&#24471;&#20102;&#27604;&#24050;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05492</link><description>&lt;p&gt;
&#25913;&#36827;&#20018;&#32852;&#25512;&#33616;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;: &#20276;&#38543;&#32423;&#32852;&#25351;&#23548;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards More Robust and Accurate Sequential Recommendation with Cascade-guided Adversarial Training. (arXiv:2304.05492v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32423;&#32852;&#25351;&#23548;&#19979;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#21462;&#24471;&#20102;&#27604;&#24050;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#19982;&#29289;&#21697;&#38388;&#30340;&#26102;&#38388;&#39034;&#24207;&#20114;&#21160;&#26469;&#36827;&#34892;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#20854;&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36817;&#26399;&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#22791;&#21463;&#36136;&#30097;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#20004;&#20010;&#29305;&#24615;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987; - &#22312;&#35757;&#32451;&#20013;&#20250;&#20135;&#29983;&#32423;&#32852;&#25928;&#24212;&#65292;&#22312;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#26102;&#38388;&#20449;&#24687;&#30340;&#21516;&#26102;&#20250;&#24573;&#30053;&#20854;&#20182;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#30340;&#32423;&#32852;&#25351;&#23548;&#19979;&#30340;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20018;&#32852;&#24314;&#27169;&#20013;&#30340;&#20869;&#22312;&#32423;&#32852;&#25928;&#24212;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#25112;&#30053;&#24615;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#26469;&#24433;&#21709;&#29289;&#21697;&#23884;&#20837;&#12290;&#22312;&#20351;&#29992;&#19981;&#21516;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#35757;&#32451;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#20018;&#32852;&#27169;&#22411;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#20135;&#29983;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation models, models that learn from chronological user-item interactions, outperform traditional recommendation models in many settings. Despite the success of sequential recommendation models, their robustness has recently come into question. Two properties unique to the nature of sequential recommendation models may impair their robustness - the cascade effects induced during training and the model's tendency to rely too heavily on temporal information. To address these vulnerabilities, we propose Cascade-guided Adversarial training, a new adversarial training procedure that is specifically designed for sequential recommendation models. Our approach harnesses the intrinsic cascade effects present in sequential modeling to produce strategic adversarial perturbations to item embeddings during training. Experiments on training state-of-the-art sequential models on four public datasets from different domains show that our training approach produces superior model ran
&lt;/p&gt;</description></item><item><title>Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;</title><link>http://arxiv.org/abs/2303.17503</link><description>&lt;p&gt;
Pgx:&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#30340;&#24182;&#34892;&#28216;&#25103;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pgx: Hardware-accelerated parallel game simulation for reinforcement learning. (arXiv:2303.17503v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17503
&lt;/p&gt;
&lt;p&gt;
Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Pgx&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#26827;&#30424;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#12290;&#30001;&#20110;JAX&#30340;&#33258;&#21160;&#21521;&#37327;&#21270;&#21644;&#21363;&#26102;&#32534;&#35793;&#21151;&#33021;&#65292;Pgx&#26131;&#20110;&#22312;GPU/TPU&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#24182;&#34892;&#25191;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21333;&#20010;A100 GPU&#19978;&#30340;Pgx&#27169;&#25311;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290;Pgx&#23454;&#29616;&#20102;&#34987;&#35748;&#20026;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#28216;&#25103;&#65292;&#22914;Backgammon&#65292;Shogi&#21644;Go&#12290; Pgx&#21487;&#22312;https://github.com/sotetsuk/pgx&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Pgx, a collection of board game simulators written in JAX. Thanks to auto-vectorization and Just-In-Time compilation of JAX, Pgx scales easily to thousands of parallel execution on GPU/TPU accelerators. We found that the simulation of Pgx on a single A100 GPU is 10x faster than that of existing reinforcement learning libraries. Pgx implements games considered vital benchmarks in artificial intelligence research, such as Backgammon, Shogi, and Go. Pgx is available at https://github.com/sotetsuk/pgx.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MGTBench&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#20013;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24191;&#27867;&#35780;&#20272;&#21644;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;MGT&#26816;&#27979;&#35780;&#20272;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#27604;&#36739;&#19981;&#21516;&#26816;&#27979;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14822</link><description>&lt;p&gt;
MGTBench&#65306;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MGTBench: Benchmarking Machine-Generated Text Detection. (arXiv:2303.14822v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MGTBench&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#20013;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24191;&#27867;&#35780;&#20272;&#21644;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;MGT&#26816;&#27979;&#35780;&#20272;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#27604;&#36739;&#19981;&#21516;&#26816;&#27979;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#38761;&#21629;&#24615;&#30340;&#21147;&#37327;&#65292;&#20363;&#22914;&#25991;&#26412;&#20998;&#31867;&#65292;&#24773;&#24863;&#20998;&#26512;&#65292;&#35821;&#35328;&#32763;&#35793;&#21644;&#38382;&#31572;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65288;MGT&#65289;&#38543;&#30528;LLM&#21464;&#24471;&#36234;&#26469;&#36234;&#20808;&#36827;&#21644;&#26222;&#36941;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#20889;&#20316;&#30340;&#35821;&#35328;&#65292;&#24456;&#38590;&#19982;&#20154;&#31867;&#20889;&#30340;&#25991;&#26412;&#21306;&#20998;&#24320;&#26469;&#65292;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#30495;&#23454;&#24615;&#65292;&#38382;&#36131;&#21644;&#28508;&#22312;&#20559;&#35265;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MGT&#26816;&#27979;&#26041;&#27861;&#26159;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#35774;&#32622;&#19979;&#36827;&#34892;&#35780;&#20272;&#30340;&#65292;&#23548;&#33268;&#32570;&#20047;&#36328;&#19981;&#21516;&#26041;&#27861;&#23398;&#30340;&#20840;&#38754;&#35780;&#20272;&#26694;&#26550;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#21517;&#20026;MGTBench&#30340;MGT&#26816;&#27979;&#22522;&#20934;&#26694;&#26550;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#12290;&#23545;&#30001;ChatGPT&#29983;&#25104;&#30340;&#31572;&#26696;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#35813;&#27169;&#22411;&#26159;&#20013;&#22269;&#26368;&#20855;&#20195;&#34920;&#24615;&#21644;&#26368;&#24378;&#22823;&#30340;LLM&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;MGTBench&#25552;&#20379;&#20102;&#20844;&#24179;&#21644;&#20840;&#38754;&#30340;MGT&#26816;&#27979;&#35780;&#20272;&#65292;&#24182;&#20351;&#30740;&#31350;&#20154;&#21592;&#27604;&#36739;&#20102;&#19981;&#21516;&#26816;&#27979;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays large language models (LLMs) have shown revolutionary power in a variety of natural language processing (NLP) tasks such as text classification, sentiment analysis, language translation, and question-answering. In this way, detecting machine-generated texts (MGTs) is becoming increasingly important as LLMs become more advanced and prevalent. These models can generate human-like language that can be difficult to distinguish from text written by a human, which raises concerns about authenticity, accountability, and potential bias. However, existing detection methods against MGTs are evaluated under different model architectures, datasets, and experimental settings, resulting in a lack of a comprehensive evaluation framework across different methodologies  In this paper, we fill this gap by proposing the first benchmark framework for MGT detection, named MGTBench. Extensive evaluations on public datasets with curated answers generated by ChatGPT (the most representative and power
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#24178;&#39044;&#19979;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#26045;&#21152;&#28508;&#22312;&#32467;&#26500;&#36328;&#36234;&#21333;&#20301;&#21644;&#32452;&#21512;&#65292;&#22312;&#38477;&#20302;&#23454;&#39564;&#25968;&#37327;&#21644;&#22788;&#29702;&#28151;&#26434;&#38382;&#39064;&#26041;&#38754;&#26377;&#30528;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.14226</link><description>&lt;p&gt;
&#32452;&#21512;&#24178;&#39044;&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;:&#21512;&#25104;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions. (arXiv:2303.14226v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14226
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#24178;&#39044;&#19979;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#26045;&#21152;&#28508;&#22312;&#32467;&#26500;&#36328;&#36234;&#21333;&#20301;&#21644;&#32452;&#21512;&#65292;&#22312;&#38477;&#20302;&#23454;&#39564;&#25968;&#37327;&#21644;&#22788;&#29702;&#28151;&#26434;&#38382;&#39064;&#26041;&#38754;&#26377;&#30528;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21253;&#21547;N&#20010;&#24322;&#36136;&#21333;&#20301;&#21644;p&#20010;&#24178;&#39044;&#30340;&#35774;&#32622;&#12290; &#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#20219;&#24847;&#32452;&#21512;&#30340;&#21333;&#20301;&#29305;&#23450;&#28508;&#22312;&#32467;&#26524;&#65292;&#21363;N&#215;2 ^ p&#20010;&#22240;&#26524;&#21442;&#25968;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#33258;&#28982;&#20986;&#29616;&#20102;&#36873;&#25321;&#24178;&#39044;&#32452;&#21512;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#22240;&#23376;&#35774;&#35745;&#35797;&#39564;&#65292;&#25512;&#33616;&#24341;&#25806;(&#20363;&#22914;&#65292;&#20026;&#29992;&#25143;&#26174;&#31034;&#26368;&#22823;&#31243;&#24230;&#30340;&#21442;&#19982;&#24230;&#30340;&#19968;&#32452;&#30005;&#24433;)&#65292;&#21307;&#23398;&#20013;&#30340;&#32452;&#21512;&#30103;&#27861;&#65292;&#36873;&#25321;ML&#27169;&#22411;&#30340;&#37325;&#35201;&#29305;&#24449;&#31561;&#31561;&#12290;&#24403;N&#21644;p&#22686;&#38271;&#26102;&#65292;&#36827;&#34892;N&#215;2 ^ p&#20010;&#23454;&#39564;&#26469;&#20272;&#35745;&#21508;&#31181;&#21442;&#25968;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#32780;&#19988;&#65292;&#35266;&#27979;&#25968;&#25454;&#24456;&#21487;&#33021;&#23384;&#22312;&#28151;&#26434;&#65292;&#21363;&#21333;&#20301;&#26159;&#21542;&#22312;&#32452;&#21512;&#19979;&#20986;&#29616;&#19982;&#20854;&#22312;&#35813;&#32452;&#21512;&#19979;&#30340;&#28508;&#22312;&#32467;&#26524;&#30456;&#20851;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#22312;&#21333;&#20301;&#21644;&#32452;&#21512;&#20043;&#38388;&#37117;&#26045;&#21152;&#20102;&#28508;&#22312;&#32467;&#26500;&#12290;&#25105;&#20204;&#20551;&#35774;&#21333;&#20301;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#30456;&#20284;&#24615;(&#21363;&#31867;&#20284;&#21333;&#20301;&#30340;&#28508;&#22312;&#32467;&#26524;&#26159;&#30456;&#20284;&#30340;)&#65292;&#24182;&#19988;&#32452;&#21512;&#20043;&#38388;&#20063;&#23384;&#22312;&#28508;&#22312;&#30340;&#30456;&#20284;&#24615;(&#21363;&#31867;&#20284;&#32452;&#21512;&#30340;&#25928;&#26524;&#26159;&#30456;&#20284;&#30340;)&#12290;&#25105;&#20204;&#20351;&#29992;&#23618;&#27425;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#27169;&#22411;&#26469;&#24418;&#24335;&#21270;&#36825;&#19968;&#28857;&#65292;&#35813;&#27169;&#22411;&#32852;&#21512;&#32858;&#31867;&#21333;&#20803;&#21644;&#32452;&#21512;&#65292;&#24182;&#19988;&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#20197;&#27169;&#25311;&#36830;&#32493;&#25110;&#31163;&#25955;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#28436;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#26174;&#30528;&#20943;&#23569;&#23398;&#20064;&#22240;&#26524;&#21442;&#25968;&#25152;&#38656;&#30340;&#23454;&#39564;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a setting with $N$ heterogeneous units and $p$ interventions. Our goal is to learn unit-specific potential outcomes for any combination of these $p$ interventions, i.e., $N \times 2^p$ causal parameters. Choosing combinations of interventions is a problem that naturally arises in many applications such as factorial design experiments, recommendation engines (e.g., showing a set of movies that maximizes engagement for users), combination therapies in medicine, selecting important features for ML models, etc. Running $N \times 2^p$ experiments to estimate the various parameters is infeasible as $N$ and $p$ grow. Further, with observational data there is likely confounding, i.e., whether or not a unit is seen under a combination is correlated with its potential outcome under that combination. To address these challenges, we propose a novel model that imposes latent structure across both units and combinations. We assume latent similarity across units (i.e., the potential outco
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#30340;&#37327;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#21151;&#33021;&#20998;&#20026;&#37327;&#23376;&#24182;&#23545;&#37327;&#23376;&#36827;&#34892;&#36882;&#20943;&#20351;&#29992;&#39057;&#29575;&#30340;&#39034;&#24207;&#23398;&#20064;&#65292;&#21487;&#20197;&#35299;&#37322;&#25439;&#22833;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#33258;&#21160;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26679;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.13506</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#30340;&#37327;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Quantization Model of Neural Scaling. (arXiv:2303.13506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13506
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#30340;&#37327;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#21151;&#33021;&#20998;&#20026;&#37327;&#23376;&#24182;&#23545;&#37327;&#23376;&#36827;&#34892;&#36882;&#20943;&#20351;&#29992;&#39057;&#29575;&#30340;&#39034;&#24207;&#23398;&#20064;&#65292;&#21487;&#20197;&#35299;&#37322;&#25439;&#22833;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#33258;&#21160;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26679;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#23450;&#24459;&#30340;&#37327;&#21270;&#27169;&#22411;&#65292;&#35299;&#37322;&#20102;&#35266;&#23519;&#21040;&#30340;&#25439;&#22833;&#20989;&#25968;&#38543;&#30528;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#24130;&#24459;&#19979;&#38477;&#20197;&#21450;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#20986;&#29616;&#26032;&#33021;&#21147;&#30340;&#31361;&#28982;&#31361;&#30772;&#12290;&#25105;&#20204;&#20174;&#25152;&#35859;&#30340;&#8220;&#37327;&#21270;&#20551;&#35774;&#8221;&#20013;&#25512;&#23548;&#20986;&#36825;&#20010;&#27169;&#22411;&#65292;&#20854;&#20013;&#23398;&#20064;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#21151;&#33021;&#34987;&#37327;&#21270;&#20026;&#31163;&#25955;&#22359;&#65288;&#8220;&#37327;&#23376;&#8221;&#65289;&#12290;&#25105;&#20204;&#22312;&#38477;&#24207;&#23398;&#20064;&#39057;&#29575;&#20013;&#23398;&#20064;&#37327;&#23376;&#65292;&#24182;&#34920;&#26126;&#24403;&#37327;&#23376;&#34987;&#20197;&#36882;&#20943;&#20351;&#29992;&#39057;&#29575;&#30340;&#39034;&#24207;&#23398;&#20064;&#26102;&#65292;&#22312;&#20351;&#29992;&#39057;&#29575;&#20013;&#20351;&#29992;&#24130;&#24459;&#21487;&#20197;&#35299;&#37322;&#35266;&#23519;&#21040;&#30340;&#25439;&#22833;&#32553;&#25918;&#23450;&#24459;&#12290;&#25105;&#20204;&#22312;&#29609;&#20855;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#36825;&#20010;&#39044;&#27979;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32553;&#25918;&#26354;&#32447;&#22914;&#20309;&#20998;&#35299;&#12290;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#65292;&#25105;&#20204;&#33258;&#21160;&#21457;&#29616;&#22810;&#26679;&#30340;&#27169;&#22411;&#33021;&#21147;&#65288;&#37327;&#23376;&#65289;&#65292;&#24182;&#21457;&#29616;&#23545;&#24212;&#23376;&#38382;&#39064;&#30340;&#20998;&#24067;&#19982;&#25105;&#20204;&#29702;&#35770;&#39044;&#27979;&#30340;&#31070;&#32463;&#32553;&#25918;&#25351;&#25968;&#20135;&#29983;&#20102;&#20860;&#23481;&#24615;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the $\textit{Quantization Model}$ of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the $\textit{Quantization Hypothesis}$, where learned network capabilities are quantized into discrete chunks ($\textit{quanta}$). We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model internals, we auto-discover diverse model capabilities (quanta) and find tentative evidence that the distribution over corresponding subproblems in the prediction of natural text is compatible with the power law predicted from the neural scaling exponent as predicted from our theory.
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.11593</link><description>&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#25163;&#24615;&#26102;&#23384;&#22312;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Difficulty in learning chirality for Transformer fed with SMILES. (arXiv:2303.11593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11593
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23545;&#26497;&#20854;&#22810;&#26679;&#30340;&#20998;&#23376;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#25551;&#36848;&#31526;&#29983;&#25104;&#24050;&#32463;&#24471;&#21040;&#20102;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;SMILES&#65292;&#21363;&#20998;&#23376;&#32467;&#26500;&#30340;&#25991;&#23383;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21270;&#23398;&#32467;&#26500;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;NLP&#27169;&#22411;&#8212;&#8212;Transformer&#65292;&#22312;&#23398;&#20064;SMILES&#21644;&#21270;&#23398;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;Transformer&#24555;&#36895;&#23398;&#20064;&#20998;&#23376;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#25165;&#33021;&#29702;&#35299;&#25972;&#20307;&#32467;&#26500;&#12290;&#19982;&#20043;&#19968;&#33268;&#30340;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#23398;&#20064;&#27493;&#39588;&#20013;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;Transformer&#38656;&#35201;&#29305;&#21035;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#25165;&#33021;&#23398;&#20064;&#25163;&#24615;&#65292;&#24182;&#19988;&#26377;&#26102;&#20250;&#20986;&#29616;&#20302;&#32763;&#35793;&#20934;&#30830;&#29575;&#30340;&#20572;&#28382;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen development of descriptor generation based on representation learning of extremely diverse molecules, especially those that apply natural language processing (NLP) models to SMILES, a literal representation of molecular structure. However, little research has been done on how these models understand chemical structure. To address this, we investigated the relationship between the learning progress of SMILES and chemical structure using a representative NLP model, the Transformer. The results suggest that while the Transformer learns partial structures of molecules quickly, it requires extended training to understand overall structures. Consistently, the accuracy of molecular property predictions using descriptors generated from models at different learning steps was similar from the beginning to the end of training. Furthermore, we found that the Transformer requires particularly long training to learn chirality and sometimes stagnates with low translation accura
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#20351;&#29992;&#21333;&#20010;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#24494;&#35843;&#30340;&#27169;&#22411;&#38598;&#21512;&#65292;&#21457;&#29616;&#36890;&#36807;&#26356;&#22909;&#22320;&#25506;&#32034;&#39044;&#35757;&#32451;&#22522;&#22495;&#21487;&#20197;&#25913;&#36827;&#38598;&#25104;&#27169;&#22411;&#65292;&#20294;&#31163;&#24320;&#22522;&#22495;&#20250;&#23548;&#33268;&#22833;&#21435;&#36801;&#31227;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#24182;&#19988;&#38477;&#20302;&#38598;&#25104;&#36136;&#37327;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#20462;&#25913;&#26041;&#27861;StarSSE&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#24378;&#30340;&#38598;&#25104;&#27169;&#22411;&#21644;&#22343;&#21248;&#30340;&#27169;&#22411;&#28151;&#21512;&#12290;</title><link>http://arxiv.org/abs/2303.03374</link><description>&lt;p&gt;
&#20572;&#30041;&#36824;&#26159;&#31163;&#24320;&#39044;&#35757;&#32451;&#22522;&#22495;&#65306;&#20851;&#20110;&#38598;&#25104;&#23398;&#20064;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning. (arXiv:2303.03374v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#20351;&#29992;&#21333;&#20010;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#24494;&#35843;&#30340;&#27169;&#22411;&#38598;&#21512;&#65292;&#21457;&#29616;&#36890;&#36807;&#26356;&#22909;&#22320;&#25506;&#32034;&#39044;&#35757;&#32451;&#22522;&#22495;&#21487;&#20197;&#25913;&#36827;&#38598;&#25104;&#27169;&#22411;&#65292;&#20294;&#31163;&#24320;&#22522;&#22495;&#20250;&#23548;&#33268;&#22833;&#21435;&#36801;&#31227;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#24182;&#19988;&#38477;&#20302;&#38598;&#25104;&#36136;&#37327;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#20462;&#25913;&#26041;&#27861;StarSSE&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#24378;&#30340;&#38598;&#25104;&#27169;&#22411;&#21644;&#22343;&#21248;&#30340;&#27169;&#22411;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#21644;&#38598;&#25104;&#23398;&#20064;&#26159;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#30340;&#20004;&#31181;&#28909;&#38376;&#25216;&#26415;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#65292;&#36890;&#24120;&#23454;&#36341;&#20013;&#20351;&#29992;&#20174;&#21333;&#20010;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#24494;&#35843;&#30340;&#27169;&#22411;&#38598;&#21512;&#12290;&#36825;&#20123;&#27169;&#22411;&#26368;&#32456;&#20250;&#36827;&#20837;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#19979;&#38477;&#31354;&#38388;&#30340;&#30456;&#21516;&#21306;&#22495;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39044;&#35757;&#32451;&#22522;&#22495;&#65292;&#22240;&#27492;&#20855;&#26377;&#26377;&#38480;&#30340;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#21333;&#20010;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#35757;&#32451;&#30340;&#38598;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26356;&#22909;&#22320;&#25506;&#32034;&#39044;&#35757;&#32451;&#22522;&#22495;&#26469;&#25913;&#36827;&#65292;&#28982;&#32780;&#65292;&#31163;&#24320;&#22522;&#22495;&#20250;&#23548;&#33268;&#22833;&#21435;&#36801;&#31227;&#23398;&#20064;&#30340;&#22909;&#22788;&#24182;&#23548;&#33268;&#38598;&#25104;&#36136;&#37327;&#30340;&#19979;&#38477;&#12290;&#22522;&#20110;&#23545;&#29616;&#26377;&#25506;&#32034;&#26041;&#27861;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#20462;&#25913;Transfer Learning Setup&#20013;&#30340;Snapshot Ensembles&#65288;SSE&#65289;&#26041;&#27861;&#65292;&#21517;&#20026;StarSSE&#65292;&#23427;&#33021;&#20135;&#29983;&#26356;&#24378;&#30340;&#38598;&#25104;&#27169;&#22411;&#21644;&#22343;&#21248;&#30340;&#27169;&#22411;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning and ensembling are two popular techniques for improving the performance and robustness of neural networks. Due to the high cost of pre-training, ensembles of models fine-tuned from a single pre-trained checkpoint are often used in practice. Such models end up in the same basin of the loss landscape, which we call the pre-train basin, and thus have limited diversity. In this work, we show that ensembles trained from a single pre-trained checkpoint may be improved by better exploring the pre-train basin, however, leaving the basin results in losing the benefits of transfer learning and in degradation of the ensemble quality. Based on the analysis of existing exploration methods, we propose a more effective modification of the Snapshot Ensembles (SSE) for transfer learning setup, StarSSE, which results in stronger ensembles and uniform model soups.
&lt;/p&gt;</description></item><item><title>CAMEL&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#25299;&#25169;&#24230;&#37327;&#21644;&#29420;&#29305;&#30340;&#40654;&#26364;&#24230;&#37327;&#36827;&#34892;&#39640;&#32500;&#25968;&#25454;&#20998;&#31867;&#12289;&#38477;&#32500;&#21644;&#21487;&#35270;&#21270;&#12290;&#23427;&#36890;&#36807;&#24179;&#28369;&#20998;&#21306;&#32479;&#19968;&#31639;&#23376;&#23558;&#23616;&#37096;&#27491;&#20132;&#25237;&#24433;&#36716;&#25442;&#20026;&#20840;&#23616;&#23884;&#20837;&#65292;&#24182;&#25552;&#20379;&#20102;&#32858;&#31867;&#26174;&#33879;&#29305;&#24449;&#30340;&#29289;&#29702;&#35299;&#37322;&#12290;CAMEL&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.02561</link><description>&lt;p&gt;
CAMEL: &#26354;&#29575;&#22686;&#24378;&#30340;&#27969;&#24418;&#23884;&#20837;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CAMEL: Curvature-Augmented Manifold Embedding and Learning. (arXiv:2303.02561v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02561
&lt;/p&gt;
&lt;p&gt;
CAMEL&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#25299;&#25169;&#24230;&#37327;&#21644;&#29420;&#29305;&#30340;&#40654;&#26364;&#24230;&#37327;&#36827;&#34892;&#39640;&#32500;&#25968;&#25454;&#20998;&#31867;&#12289;&#38477;&#32500;&#21644;&#21487;&#35270;&#21270;&#12290;&#23427;&#36890;&#36807;&#24179;&#28369;&#20998;&#21306;&#32479;&#19968;&#31639;&#23376;&#23558;&#23616;&#37096;&#27491;&#20132;&#25237;&#24433;&#36716;&#25442;&#20026;&#20840;&#23616;&#23884;&#20837;&#65292;&#24182;&#25552;&#20379;&#20102;&#32858;&#31867;&#26174;&#33879;&#29305;&#24449;&#30340;&#29289;&#29702;&#35299;&#37322;&#12290;CAMEL&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Curvature-Augmented Manifold Embedding and Learning (CAMEL)&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#20998;&#31867;&#12289;&#38477;&#32500;&#21644;&#21487;&#35270;&#21270;&#12290;CAMEL&#21033;&#29992;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#23450;&#20041;&#30340;&#25299;&#25169;&#24230;&#37327;&#20197;&#21450;&#29992;&#20110;&#36317;&#31163;&#21644;&#26354;&#29575;&#30340;&#29420;&#29305;&#40654;&#26364;&#24230;&#37327;&#26469;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36824;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#20351;&#29992;&#24179;&#28369;&#20998;&#21306;&#32479;&#19968;&#31639;&#23376;&#65292;&#23558;&#23616;&#37096;&#27491;&#20132;&#25237;&#24433;&#36716;&#25442;&#20026;&#20840;&#23616;&#23884;&#20837;&#65292;&#21516;&#26102;&#25429;&#25417;&#25972;&#20307;&#25299;&#25169;&#32467;&#26500;&#21644;&#23616;&#37096;&#30456;&#20284;&#24615;&#12290;&#23616;&#37096;&#27491;&#20132;&#21521;&#37327;&#25552;&#20379;&#20102;&#32858;&#31867;&#30340;&#26174;&#33879;&#29305;&#24449;&#30340;&#29289;&#29702;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;CAMEL&#19981;&#20165;&#25552;&#20379;&#20102;&#20302;&#32500;&#23884;&#20837;&#65292;&#36824;&#35299;&#37322;&#20102;&#27492;&#23884;&#20837;&#32972;&#21518;&#30340;&#29289;&#29702;&#24773;&#20917;&#12290;CAMEL&#24050;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#26174;&#31034;&#20986;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#30340;&#26174;&#33879;&#20248;&#21183;&#26159;&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel method, named Curvature-Augmented Manifold Embedding and Learning (CAMEL), is proposed for high dimensional data classification, dimension reduction, and visualization. CAMEL utilizes a topology metric defined on the Riemannian manifold, and a unique Riemannian metric for both distance and curvature to enhance its expressibility. The method also employs a smooth partition of unity operator on the Riemannian manifold to convert localized orthogonal projection to global embedding, which captures both the overall topological structure and local similarity simultaneously. The local orthogonal vectors provide a physical interpretation of the significant characteristics of clusters. Therefore, CAMEL not only provides a low-dimensional embedding but also interprets the physics behind this embedding. CAMEL has been evaluated on various benchmark datasets and has shown to outperform state-of-the-art methods, especially for high-dimensional datasets. The method's distinct benefits are it
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#32806;&#21512;&#27969;&#21644;&#33258;&#22238;&#24402;&#27969;&#30340;&#19981;&#21516;&#26550;&#26500;&#21644;&#22810;&#26679;&#30446;&#26631;&#20998;&#24067;&#65292;&#21033;&#29992;&#21508;&#31181;&#27979;&#35797;&#32479;&#35745;&#37327;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#65292;&#20026;&#27491;&#35268;&#21270;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2302.12024</link><description>&lt;p&gt;
&#27604;&#36739;&#32806;&#21512;&#27969;&#21644;&#33258;&#22238;&#24402;&#27969;&#30340;&#40065;&#26834;&#32479;&#35745;&#26816;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study of Coupling and Autoregressive Flows through Robust Statistical Tests. (arXiv:2302.12024v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#32806;&#21512;&#27969;&#21644;&#33258;&#22238;&#24402;&#27969;&#30340;&#19981;&#21516;&#26550;&#26500;&#21644;&#22810;&#26679;&#30446;&#26631;&#20998;&#24067;&#65292;&#21033;&#29992;&#21508;&#31181;&#27979;&#35797;&#32479;&#35745;&#37327;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#65292;&#20026;&#27491;&#35268;&#21270;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#35268;&#21270;&#27969;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#20165;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#22797;&#26434;&#30446;&#26631;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#65292;&#32780;&#19988;&#36824;&#36890;&#36807;&#26500;&#36896;&#25552;&#20379;&#23494;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#23545;&#32806;&#21512;&#27969;&#21644;&#33258;&#22238;&#24402;&#27969;&#36827;&#34892;&#28145;&#20837;&#27604;&#36739;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#20223;&#23556;&#21644;&#26377;&#29702;&#20108;&#27425;&#26679;&#26465;&#31867;&#22411;&#30340;&#22235;&#31181;&#19981;&#21516;&#26550;&#26500;&#65306;&#23454;&#20540;&#38750;&#20307;&#31215;&#20445;&#25345;&#65288;RealNVP&#65289;&#12289;&#25513;&#34109;&#33258;&#22238;&#24402;&#27969;&#65288;MAF&#65289;&#12289;&#32806;&#21512;&#26377;&#29702;&#20108;&#27425;&#26679;&#26465;&#65288;C-RQS&#65289;&#21644;&#33258;&#22238;&#24402;&#26377;&#29702;&#20108;&#27425;&#26679;&#26465;&#65288;A-RQS&#65289;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#32452;&#20174;4&#32500;&#21040;400&#32500;&#36882;&#22686;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#20998;&#24067;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#20004;&#26679;&#26412;&#27979;&#35797;&#30340;&#27979;&#35797;&#32479;&#35745;&#37327;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24050;&#30693;&#36317;&#31163;&#24230;&#37327;&#30340;&#27979;&#35797;&#32479;&#35745;&#37327;&#65306;&#20999;&#29255;Wasserstein&#36317;&#31163;&#12289;&#32500;&#24230;&#24179;&#22343;&#19968;&#32500;Kolmogorov-Smirnov&#26816;&#39564;&#21644;&#30456;&#20851;&#30697;&#38453;&#20043;&#24046;&#30340;Frobenius&#33539;&#25968;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#21253;&#25324;&#20102;&#20197;&#19979;&#20272;&#35745;&#65306;
&lt;/p&gt;
&lt;p&gt;
Normalizing Flows have emerged as a powerful brand of generative models, as they not only allow for efficient sampling of complicated target distributions, but also deliver density estimation by construction. We propose here an in-depth comparison of coupling and autoregressive flows, both of the affine and rational quadratic spline type, considering four different architectures: Real-valued Non-Volume Preserving (RealNVP), Masked Autoregressive Flow (MAF), Coupling Rational Quadratic Spline (C-RQS), and Autoregressive Rational Quadratic Spline (A-RQS). We focus on a set of multimodal target distributions of increasing dimensionality ranging from 4 to 400. The performances are compared by means of different test-statistics for two-sample tests, built from known distance measures: the sliced Wasserstein distance, the dimension-averaged one-dimensional Kolmogorov-Smirnov test, and the Frobenius norm of the difference between correlation matrices. Furthermore, we include estimations of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#65292;&#36755;&#20986;&#27169;&#22411;&#21644;&#32463;&#39564;&#26679;&#26412;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#19982;&#31639;&#27861;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#20449;&#24687;&#29702;&#35770;&#27867;&#21270;&#30028;&#38480;&#19981;&#36275;&#20197;&#25429;&#25417;&#21040;&#20687;SGD&#21644;&#27491;&#21017;&#21270;ERM&#36825;&#26679;&#20855;&#26377;&#32500;&#24230;&#26080;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.04925</link><description>&lt;p&gt;
&#20449;&#24687;&#29702;&#35770;&#19978;&#30028;&#23545;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#30340;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
Information Theoretic Lower Bounds for Information Theoretic Upper Bounds. (arXiv:2302.04925v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#65292;&#36755;&#20986;&#27169;&#22411;&#21644;&#32463;&#39564;&#26679;&#26412;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#19982;&#31639;&#27861;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#20449;&#24687;&#29702;&#35770;&#27867;&#21270;&#30028;&#38480;&#19981;&#36275;&#20197;&#25429;&#25417;&#21040;&#20687;SGD&#21644;&#27491;&#21017;&#21270;ERM&#36825;&#26679;&#20855;&#26377;&#32500;&#24230;&#26080;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#30340;&#32972;&#26223;&#19979;&#30740;&#31350;&#20102;&#36755;&#20986;&#27169;&#22411;&#21644;&#32463;&#39564;&#26679;&#26412;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#19982;&#31639;&#27861;&#30340;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#23545;&#20449;&#24687;&#29702;&#35770;&#27867;&#21270;&#30028;&#38480;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#65292;&#20294;&#36825;&#20123;&#30028;&#38480;&#33021;&#21542;&#25581;&#31034;&#21508;&#31181;&#23398;&#20064;&#31639;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#36824;&#19981;&#30830;&#23450;&#12290;&#25105;&#20204;&#23545;&#38543;&#26426;&#20984;&#20248;&#21270;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#30495;&#27491;&#30340;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#20381;&#36182;&#20110;&#32500;&#24230;&#30340;&#20114;&#20449;&#24687;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#20449;&#24687;&#29702;&#35770;&#27867;&#21270;&#30028;&#38480;&#19981;&#33021;&#23436;&#20840;&#25429;&#25417;&#21040;&#20687;SGD&#21644;&#27491;&#21017;&#21270;ERM&#36825;&#26679;&#20855;&#26377;&#32500;&#24230;&#26080;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the relationship between the mutual information between the output model and the empirical sample and the generalization of the algorithm in the context of stochastic convex optimization. Despite increasing interest in information-theoretic generalization bounds, it is uncertain if these bounds can provide insight into the exceptional performance of various learning algorithms. Our study of stochastic convex optimization reveals that, for true risk minimization, dimension-dependent mutual information is necessary. This indicates that existing information-theoretic generalization bounds fall short in capturing the generalization capabilities of algorithms like SGD and regularized ERM, which have dimension-independent sample complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04391</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#26032;&#26631;&#31614;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#24320;&#21457;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;90&#20998;&#20197;&#19978;&#30340;&#25104;&#32489;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#20986;&#22122;&#22768;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#20154;&#31867;&#26631;&#35760;&#30340;&#21442;&#32771;&#26469;&#37325;&#26032;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24819;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24207;&#21015;&#26631;&#35760;&#12289;&#29289;&#20307;&#26816;&#27979;&#12289;&#24207;&#21015;&#29983;&#25104;&#12289;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#26799;&#24230;&#19979;&#38477;&#22312;&#32447;&#24615;&#30456;&#20851;&#22122;&#22768;&#19979;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#19981;&#21516;ially private optimization&#12290;</title><link>http://arxiv.org/abs/2302.01463</link><description>&lt;p&gt;
&#32447;&#24615;&#30456;&#20851;&#22122;&#22768;&#19979;&#30340;&#26799;&#24230;&#19979;&#38477;&#65306;&#29702;&#35770;&#21450;&#24212;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#20013;
&lt;/p&gt;
&lt;p&gt;
Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy. (arXiv:2302.01463v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26799;&#24230;&#19979;&#38477;&#22312;&#32447;&#24615;&#30456;&#20851;&#22122;&#22768;&#19979;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#19981;&#21516;ially private optimization&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#30456;&#20851;&#22122;&#22768;&#19979;&#30340;&#26799;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#30001;&#26368;&#36817;&#38024;&#23545;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#20248;&#21270;&#30340;&#23454;&#36341;&#26041;&#27861;&#25152;&#21551;&#21457;&#30340;&#65292;&#20363;&#22914;DP-FTRL&#65292;&#22312;&#26080;&#27861;&#20351;&#29992;&#38544;&#31169;&#25918;&#22823;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65289;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#30697;&#38453;&#20998;&#35299;&#26426;&#21046;&#27880;&#20837;&#38544;&#31169;&#22122;&#22768;&#65292;&#20351;&#22122;&#22768;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#21576;&#32447;&#24615;&#30456;&#20851;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#29615;&#22659;&#65292;&#31934;&#31616;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#20851;&#38190;&#38754;&#35980;&#65292;&#24182;&#20998;&#31163;&#20102;&#32447;&#24615;&#30456;&#20851;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26799;&#24230;&#19979;&#38477;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#65292;&#26080;&#35770;&#26159;&#20984;&#20989;&#25968;&#36824;&#26159;&#38750;&#20984;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26126;&#26174;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#26356;&#32039;&#65292;&#24182;&#31934;&#30830;&#22320;&#24674;&#22797;&#20102;&#22810;&#20010;&#37325;&#35201;&#30340;&#29305;&#27530;&#24773;&#20917;&#65288;&#21253;&#25324;&#21453;&#30456;&#20851;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#32467;&#26524;&#24320;&#21457;&#20102;&#26032;&#30340;&#65292;&#26377;&#25928;&#30340;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#19981;&#21516;ially private optimization&#65292;&#24182;&#31361;&#20986;&#20102;&#36825;&#20123;&#22240;&#23376;&#20998;&#35299;&#26041;&#27861;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
We study gradient descent under linearly correlated noise. Our work is motivated by recent practical methods for optimization with differential privacy (DP), such as DP-FTRL, which achieve strong performance in settings where privacy amplification techniques are infeasible (such as in federated learning). These methods inject privacy noise through a matrix factorization mechanism, making the noise linearly correlated over iterations. We propose a simplified setting that distills key facets of these methods and isolates the impact of linearly correlated noise. We analyze the behavior of gradient descent in this setting, for both convex and non-convex functions. Our analysis is demonstrably tighter than prior work and recovers multiple important special cases exactly (including anticorrelated perturbed gradient descent). We use our results to develop new, effective matrix factorizations for differentially private optimization, and highlight the benefits of these factorizations theoretica
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#31169;&#20154;&#20010;&#24615;&#21270;&#20998;&#25955;&#23398;&#20064;&#65288;PPDL&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23433;&#20840;&#32858;&#21512;&#21644;&#30456;&#20851;&#23545;&#25239;&#24335;&#22810;&#33218;&#32769;&#34382;&#26426;&#20248;&#21270;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#33410;&#28857;&#36873;&#25321;&#65292;&#24182;&#20445;&#25252;&#33410;&#28857;&#30340;&#38544;&#31169;&#12290;&#20316;&#32773;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#21512;&#20316;&#32773;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20165;&#22522;&#20110;&#32858;&#21512;&#27169;&#22411;&#23601;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21512;&#36866;&#30340;&#21512;&#20316;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;PPDL&#22312;&#26631;&#31614;&#21644;&#21327;&#21464;&#37327;&#20559;&#31227;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#20248;&#20110;&#20808;&#21069;&#30340;&#38750;&#31169;&#23494;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12755</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#31169;&#20154;&#20010;&#24615;&#21270;&#20998;&#25955;&#23398;&#20064;&#20013;&#33410;&#28857;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Efficient Node Selection in Private Personalized Decentralized Learning. (arXiv:2301.12755v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12755
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#31169;&#20154;&#20010;&#24615;&#21270;&#20998;&#25955;&#23398;&#20064;&#65288;PPDL&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23433;&#20840;&#32858;&#21512;&#21644;&#30456;&#20851;&#23545;&#25239;&#24335;&#22810;&#33218;&#32769;&#34382;&#26426;&#20248;&#21270;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#33410;&#28857;&#36873;&#25321;&#65292;&#24182;&#20445;&#25252;&#33410;&#28857;&#30340;&#38544;&#31169;&#12290;&#20316;&#32773;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#21512;&#20316;&#32773;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20165;&#22522;&#20110;&#32858;&#21512;&#27169;&#22411;&#23601;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21512;&#36866;&#30340;&#21512;&#20316;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;PPDL&#22312;&#26631;&#31614;&#21644;&#21327;&#21464;&#37327;&#20559;&#31227;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#20248;&#20110;&#20808;&#21069;&#30340;&#38750;&#31169;&#23494;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#20998;&#25955;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#27599;&#20010;&#33410;&#28857;&#33021;&#22815;&#22312;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65292;&#24182;&#19982;&#20854;&#20182;&#33410;&#28857;&#21512;&#20316;&#20197;&#25913;&#36827;&#32780;&#19981;&#20849;&#20139;&#20219;&#20309;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#30528;&#37325;&#22823;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#33410;&#28857;&#21487;&#33021;&#20250;&#36890;&#36807;&#20854;&#21512;&#20316;&#36873;&#25321;&#26080;&#24847;&#20013;&#25259;&#38706;&#26377;&#20851;&#20854;&#25968;&#25454;&#25110;&#20559;&#22909;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31169;&#20154;&#20010;&#24615;&#21270;&#20998;&#25955;&#23398;&#20064;&#65288;PPDL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#23433;&#20840;&#32858;&#21512;&#21644;&#30456;&#20851;&#23545;&#25239;&#24335;&#22810;&#33218;&#32769;&#34382;&#26426;&#20248;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#20445;&#25252;&#33410;&#28857;&#30340;&#38544;&#31169;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#30340;&#33410;&#28857;&#36873;&#25321;&#12290;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#33218;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21363;&#28508;&#22312;&#30340;&#21512;&#20316;&#32773;&#65292;&#25105;&#20204;&#35777;&#26126;PPDL&#21487;&#20197;&#20165;&#22522;&#20110;&#32858;&#21512;&#27169;&#22411;&#26377;&#25928;&#22320;&#35782;&#21035;&#21512;&#36866;&#30340;&#21512;&#20316;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PPDL&#22312;&#26631;&#31614;&#21644;&#21327;&#21464;&#37327;&#20559;&#31227;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#20248;&#20110;&#20808;&#21069;&#30340;&#38750;&#31169;&#23494;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized decentralized learning is a promising paradigm for distributed learning, enabling each node to train a local model on its own data and collaborate with other nodes to improve without sharing any data. However, this approach poses significant privacy risks, as nodes may inadvertently disclose sensitive information about their data or preferences through their collaboration choices. In this paper, we propose Private Personalized Decentralized Learning (PPDL), a novel approach that combines secure aggregation and correlated adversarial multi-armed bandit optimization to protect node privacy while facilitating efficient node selection. By leveraging dependencies between different arms, represented by potential collaborators, we demonstrate that PPDL can effectively identify suitable collaborators solely based on aggregated models. Additionally, we show that PPDL surpasses previous non-private methods in model performance on standard benchmarks under label and covariate shift s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20174;SPIDER&#20202;&#22120;&#27979;&#37327;&#25968;&#25454;&#20013;&#37325;&#24314;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20808;&#39564;&#20449;&#24687;&#26469;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#23558;&#37325;&#24314;&#26102;&#38388;&#32553;&#30701;&#21040;&#32422;10&#27627;&#31186;&#65292;&#23454;&#29616;&#20102;SPIDER&#20202;&#22120;&#30340;&#23454;&#26102;&#25104;&#20687;&#12290;</title><link>http://arxiv.org/abs/2301.10260</link><description>&lt;p&gt;
SPIDER&#20202;&#22120;&#30340;&#23398;&#20064;&#24178;&#28041;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Learned Interferometric Imaging for the SPIDER Instrument. (arXiv:2301.10260v2 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20174;SPIDER&#20202;&#22120;&#27979;&#37327;&#25968;&#25454;&#20013;&#37325;&#24314;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20808;&#39564;&#20449;&#24687;&#26469;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#23558;&#37325;&#24314;&#26102;&#38388;&#32553;&#30701;&#21040;&#32422;10&#27627;&#31186;&#65292;&#23454;&#29616;&#20102;SPIDER&#20202;&#22120;&#30340;&#23454;&#26102;&#25104;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#27573;&#24179;&#38754;&#25104;&#20687;&#20809;&#23398;&#24178;&#28041;&#20202;&#65288;SPIDER&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#24178;&#28041;&#25104;&#20687;&#26469;&#20943;&#23567;&#20307;&#31215;&#12289;&#37325;&#37327;&#21644;&#21151;&#32791;&#30340;&#22823;&#22411;&#31354;&#38388;&#26395;&#36828;&#38236;&#35774;&#35745;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#30446;&#21069;&#65292;&#20174;&#24178;&#28041;&#27979;&#37327;&#20013;&#37325;&#24314;&#22270;&#20687;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#37319;&#29992;&#36817;&#31471;&#20248;&#21270;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#35745;&#31639;&#37327;&#22823;&#19988;&#38656;&#35201;&#25163;&#24037;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#37325;&#24314;SPIDER&#20202;&#22120;&#27979;&#37327;&#25152;&#24471;&#22270;&#29255;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#20808;&#39564;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#23558;&#24674;&#22797;&#22270;&#29255;&#25152;&#38656;&#30340;&#35745;&#31639;&#26102;&#38388;&#22823;&#24133;&#32553;&#30701;&#22810;&#20010;&#25968;&#37327;&#32423;&#12290;&#37325;&#24314;&#26102;&#38388;&#20943;&#23569;&#21040;&#32422;10&#27627;&#31186;&#65292;&#39318;&#27425;&#20026;SPIDER&#23454;&#29616;&#20102;&#23454;&#26102;&#25104;&#20687;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segmented Planar Imaging Detector for Electro-Optical Reconnaissance (SPIDER) is an optical interferometric imaging device that aims to offer an alternative to the large space telescope designs of today with reduced size, weight and power consumption. This is achieved through interferometric imaging. State-of-the-art methods for reconstructing images from interferometric measurements adopt proximal optimization techniques, which are computationally expensive and require handcrafted priors. In this work we present two data-driven approaches for reconstructing images from measurements made by the SPIDER instrument. These approaches use deep learning to learn prior information from training data, increasing the reconstruction quality, and significantly reducing the computation time required to recover images by orders of magnitude. Reconstruction time is reduced to ${\sim} 10$ milliseconds, opening up the possibility of real-time imaging with SPIDER for the first time. Furthermore, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#24212;&#29992;&#20110;&#22810;&#31867;&#25968;&#25454;&#38598;&#20013;&#65292;&#36890;&#36807;&#26500;&#24314;&#25299;&#25169;&#20998;&#31867;&#22120;&#21644;&#31616;&#21333;&#22797;&#21512;&#20307;&#65292;&#30740;&#31350;&#20102;&#25299;&#25169;&#22797;&#26434;&#24615;&#23545;&#21069;&#39304;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#24182;&#39564;&#35777;&#20102;&#25299;&#25169;&#22797;&#26434;&#24615;&#19982;DNN&#23398;&#20064;&#20043;&#38388;&#30340;&#36127;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.09734</link><description>&lt;p&gt;
&#22810;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#25299;&#25169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Topological Learning in Multi-Class Data Sets. (arXiv:2301.09734v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#24212;&#29992;&#20110;&#22810;&#31867;&#25968;&#25454;&#38598;&#20013;&#65292;&#36890;&#36807;&#26500;&#24314;&#25299;&#25169;&#20998;&#31867;&#22120;&#21644;&#31616;&#21333;&#22797;&#21512;&#20307;&#65292;&#30740;&#31350;&#20102;&#25299;&#25169;&#22797;&#26434;&#24615;&#23545;&#21069;&#39304;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#24182;&#39564;&#35777;&#20102;&#25299;&#25169;&#22797;&#26434;&#24615;&#19982;DNN&#23398;&#20064;&#20043;&#38388;&#30340;&#36127;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#24212;&#29992;&#20110;&#22810;&#31867;&#25968;&#25454;&#38598;&#30340;&#25299;&#25169;&#22797;&#26434;&#24615;&#34920;&#24449;&#38382;&#39064;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#25299;&#25169;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;&#25968;&#25454;&#38598;&#30340;&#24320;&#25918;&#23376;&#35206;&#30422;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#31616;&#21333;&#22797;&#21512;&#20307;&#65292;&#20854;&#25299;&#25169;&#29305;&#24449;&#65288;&#22914;Betti&#25968;&#65289;&#25552;&#20379;&#20851;&#20110;&#20998;&#31867;&#38382;&#39064;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25299;&#25169;&#32467;&#26500;&#26469;&#30740;&#31350;&#25299;&#25169;&#22797;&#26434;&#24615;&#23545;&#21069;&#39304;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20551;&#35774;&#25299;&#25169;&#22797;&#26434;&#24615;&#19982;&#20840;&#36830;&#25509;&#21069;&#39304;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#30830;&#20998;&#31867;&#25968;&#25454;&#30340;&#33021;&#21147;&#21576;&#36127;&#30456;&#20851;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26500;&#24314;&#21644;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#25299;&#25169;&#20998;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#25299;&#25169;&#22797;&#26434;&#24615;&#19982;DNN&#23398;&#20064;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We specialize techniques from topological data analysis to the problem of characterizing the topological complexity (as defined in the body of the paper) of a multi-class data set. As a by-product, a topological classifier is defined that uses an open sub-covering of the data set. This sub-covering can be used to construct a simplicial complex whose topological features (e.g., Betti numbers) provide information about the classification problem. We use these topological constructs to study the impact of topological complexity on learning in feedforward deep neural networks (DNNs). We hypothesize that topological complexity is negatively correlated with the ability of a fully connected feedforward deep neural network to learn to classify data correctly. We evaluate our topological classification algorithm on multiple constructed and open source data sets. We also validate our hypothesis regarding the relationship between topological complexity and learning in DNN's on multiple data sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#22823;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26679;&#26412;&#39640;&#25928;&#30340;&#26694;&#26550;NN-PMP-Gradient&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20855;&#26377;&#26410;&#30693;&#21644;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#37319;&#29992;&#36845;&#20195;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#19981;&#20165;&#21033;&#29992;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#36824;&#39640;&#25928;&#22320;&#24674;&#22797;&#20102;&#26368;&#20248;&#24615;&#26465;&#20214;&#21644;&#26368;&#20248;&#21160;&#20316;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2212.14566</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#20248;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Pontryagin Optimal Control via Neural Networks. (arXiv:2212.14566v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#22823;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26679;&#26412;&#39640;&#25928;&#30340;&#26694;&#26550;NN-PMP-Gradient&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20855;&#26377;&#26410;&#30693;&#21644;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#37319;&#29992;&#36845;&#20195;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#19981;&#20165;&#21033;&#29992;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#36824;&#39640;&#25928;&#22320;&#24674;&#22797;&#20102;&#26368;&#20248;&#24615;&#26465;&#20214;&#21644;&#26368;&#20248;&#21160;&#20316;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#22797;&#26434;&#30340;&#39640;&#32500;&#31995;&#32479;&#21160;&#21147;&#23398;&#36890;&#24120;&#23545;&#20915;&#31574;&#32773;&#26469;&#35828;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#25968;&#20540;&#26041;&#27861;&#25214;&#21040;&#26368;&#20248;&#25511;&#21046;&#21160;&#20316;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#24314;&#27169;&#21644;&#35745;&#31639;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#22823;&#21407;&#29702;&#65288;PMP&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26694;&#26550;NN-PMP-Gradient&#12290;&#25152;&#24471;&#21040;&#30340;&#25511;&#21046;&#22120;&#21487;&#20197;&#29992;&#20110;&#20855;&#26377;&#26410;&#30693;&#21644;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#37319;&#29992;&#36845;&#20195;&#26041;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#19981;&#20165;&#21033;&#29992;&#20102;&#30001;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#20934;&#30830;&#26367;&#20195;&#27169;&#22411;&#65292;&#36824;&#36890;&#36807;PMP&#26465;&#20214;&#39640;&#25928;&#22320;&#24674;&#22797;&#20102;&#26368;&#20248;&#24615;&#26465;&#20214;&#20197;&#21450;&#26368;&#20248;&#21160;&#20316;&#24207;&#21015;&#12290;&#22312;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#12289;&#19982;&#32593;&#26684;&#36830;&#25509;&#30340;&#26377;&#25439;&#30005;&#27744;&#30340;&#33021;&#28304;&#22871;&#21033;&#12289;&#21333;&#25670;&#30340;&#25511;&#21046;&#21644;&#20004;&#20010;MuJoCo&#36816;&#21160;&#20219;&#21153;&#30340;&#25968;&#20540;&#20223;&#30495;&#20013;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;NN-PMP-Gradient&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#21644;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving real-world optimal control problems are challenging tasks, as the complex, high-dimensional system dynamics are usually unrevealed to the decision maker. It is thus hard to find the optimal control actions numerically. To deal with such modeling and computation challenges, in this paper, we integrate Neural Networks with the Pontryagin's Maximum Principle (PMP), and propose a sample efficient framework NN-PMP-Gradient. The resulting controller can be implemented for systems with unknown and complex dynamics. By taking an iterative approach, the proposed framework not only utilizes the accurate surrogate models parameterized by neural networks, it also efficiently recovers the optimality conditions along with the optimal action sequences via PMP conditions. Numerical simulations on Linear Quadratic Regulator, energy arbitrage of grid-connected lossy battery, control of single pendulum, and two MuJoCo locomotion tasks demonstrate our proposed NN-PMP-Gradient is a general and vers
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#20248;&#25511;&#21046;&#20013;&#35266;&#27979;&#26102;&#38388;&#20197;&#31163;&#25955;&#26102;&#38388;&#28857;&#22266;&#23450;&#21608;&#26399;&#21040;&#36798;&#30340;&#40664;&#35748;&#20551;&#35774;&#19982;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#22312;LQR&#31995;&#32479;&#20013;&#25581;&#31034;&#20102;&#36817;&#20284;&#35823;&#24046;&#21644;&#32479;&#35745;&#35823;&#24046;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#22312;&#26377;&#38480;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#31649;&#29702;&#26102;&#38388;&#20998;&#36776;&#29575;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#31574;&#30053;&#35780;&#20272;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.08949</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#20540;&#20272;&#35745;&#20013;&#31649;&#29702;&#26102;&#38388;&#20998;&#36776;&#29575;: &#19968;&#39033;&#22522;&#26412;&#26435;&#34913;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Managing Temporal Resolution in Continuous Value Estimation: A Fundamental Trade-off. (arXiv:2212.08949v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#20248;&#25511;&#21046;&#20013;&#35266;&#27979;&#26102;&#38388;&#20197;&#31163;&#25955;&#26102;&#38388;&#28857;&#22266;&#23450;&#21608;&#26399;&#21040;&#36798;&#30340;&#40664;&#35748;&#20551;&#35774;&#19982;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#22312;LQR&#31995;&#32479;&#20013;&#25581;&#31034;&#20102;&#36817;&#20284;&#35823;&#24046;&#21644;&#32479;&#35745;&#35823;&#24046;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#22312;&#26377;&#38480;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#31649;&#29702;&#26102;&#38388;&#20998;&#36776;&#29575;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#31574;&#30053;&#35780;&#20272;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#26368;&#20248;&#25511;&#21046;&#20013;&#30340;&#40664;&#35748;&#20551;&#35774;&#26159;&#35266;&#27979;&#20197;&#22266;&#23450;&#30340;&#26102;&#38047;&#21608;&#26399;&#22312;&#31163;&#25955;&#30340;&#26102;&#38388;&#28857;&#21040;&#36798;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24212;&#29992;&#28041;&#21450;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#65292;&#29702;&#35770;&#19978;&#21487;&#20197;&#23545;&#26102;&#38388;&#31163;&#25955;&#21270;&#36827;&#34892;&#31649;&#29702;&#12290;&#26102;&#38388;&#31163;&#25955;&#21270;&#23545;RL&#26041;&#27861;&#30340;&#24433;&#21709;&#23578;&#26410;&#22312;&#29616;&#26377;&#29702;&#35770;&#20013;&#23436;&#20840;&#34920;&#24449;&#65292;&#20294;&#23545;&#20854;&#24433;&#21709;&#36827;&#34892;&#26356;&#35814;&#32454;&#30340;&#20998;&#26512;&#21487;&#33021;&#25581;&#31034;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;LQR&#31995;&#32479;&#30340;Monte-Carlo&#31574;&#30053;&#35780;&#20272;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24182;&#21457;&#29616;&#20102;&#20272;&#20540;&#36807;&#31243;&#20013;&#36817;&#20284;&#35823;&#24046;&#21644;&#32479;&#35745;&#35823;&#24046;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20004;&#31181;&#38169;&#35823;&#23545;&#26102;&#38388;&#31163;&#25955;&#21270;&#30340;&#34920;&#29616;&#19981;&#21516;&#65292;&#36825;&#23548;&#33268;&#20102;&#23545;&#20110;&#32473;&#23450;&#25968;&#25454;&#39044;&#31639;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#26377;&#38480;&#25968;&#25454;&#30340;LQR&#31995;&#32479;&#20013;&#65292;&#31649;&#29702;&#26102;&#38388;&#20998;&#36776;&#29575;&#21487;&#20197;&#25913;&#21892;&#31574;&#30053;&#35780;&#20272;&#30340;&#25928;&#29575;&#12290;&#20174;&#23454;&#35777;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#23637;&#31034;&#20102;&#36825;&#31181;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
A default assumption in reinforcement learning (RL) and optimal control is that observations arrive at discrete time points on a fixed clock cycle. Yet, many applications involve continuous-time systems where the time discretization, in principle, can be managed. The impact of time discretization on RL methods has not been fully characterized in existing theory, but a more detailed analysis of its effect could reveal opportunities for improving data-efficiency. We address this gap by analyzing Monte-Carlo policy evaluation for LQR systems and uncover a fundamental trade-off between approximation and statistical error in value estimation. Importantly, these two errors behave differently to time discretization, leading to an optimal choice of temporal resolution for a given data budget. These findings show that managing the temporal resolution can provably improve policy evaluation efficiency in LQR systems with finite data. Empirically, we demonstrate the trade-off in numerical simulati
&lt;/p&gt;</description></item><item><title>DeepSpeed Data Efficiency&#25552;&#20986;&#20102;&#20004;&#31181;&#25968;&#25454;&#25928;&#29575;&#25216;&#26415;&#65306;&#39640;&#25928;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#39640;&#25928;&#30340;&#25968;&#25454;&#36335;&#30001;&#65292;&#33021;&#22815;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2212.03597</link><description>&lt;p&gt;
DeepSpeed&#25968;&#25454;&#25928;&#29575;&#65306;&#36890;&#36807;&#39640;&#25928;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#36335;&#30001;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36136;&#37327;&#21644;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. (arXiv:2212.03597v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03597
&lt;/p&gt;
&lt;p&gt;
DeepSpeed Data Efficiency&#25552;&#20986;&#20102;&#20004;&#31181;&#25968;&#25454;&#25928;&#29575;&#25216;&#26415;&#65306;&#39640;&#25928;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#39640;&#25928;&#30340;&#25968;&#25454;&#36335;&#30001;&#65292;&#33021;&#22815;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36827;&#27493;&#20197;&#24040;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#20026;&#20195;&#20215;&#12290;&#27169;&#22411;&#23610;&#23544;&#30340;&#22686;&#21152;&#26159;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#65292;&#20294;&#21478;&#19968;&#20010;&#19981;&#22826;&#34987;&#37325;&#35270;&#30340;&#20107;&#23454;&#26159;&#65292;&#25968;&#25454;&#35268;&#27169;&#23454;&#38469;&#19978;&#19982;&#27169;&#22411;&#35268;&#27169;&#20197;&#30456;&#20284;&#30340;&#36895;&#24230;&#22686;&#21152;&#65292;&#32780;&#35757;&#32451;&#25104;&#26412;&#19982;&#20004;&#32773;&#25104;&#27604;&#20363;&#12290;&#19982;&#19981;&#26029;&#21457;&#23637;&#30340;&#27169;&#22411;&#26550;&#26500;&#30456;&#27604;&#65292;&#22914;&#20309;&#39640;&#25928;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#65288;&#29305;&#21035;&#26159;&#23545;&#20110;&#26114;&#36149;&#30340;&#22522;&#30784;&#27169;&#22411;&#39044;&#35757;&#32451;&#65289;&#22312;&#36739;&#23569;&#34987;&#25506;&#32034;&#19988;&#38590;&#20197;&#23454;&#29616;&#65292;&#36825;&#26159;&#22240;&#20026;&#32570;&#20047;&#19968;&#20010;&#19987;&#27880;&#20110;&#25968;&#25454;&#25928;&#29575;&#33021;&#21147;&#30340;&#26041;&#20415;&#26694;&#26550;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepSpeed&#25968;&#25454;&#25928;&#29575;&#65292;&#19968;&#31181;&#33021;&#26356;&#22909;&#22320;&#21033;&#29992;&#25968;&#25454;&#12289;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#24182;&#25913;&#21892;&#27169;&#22411;&#36136;&#37327;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#32467;&#21512;&#20102;&#20004;&#31181;&#25968;&#25454;&#25928;&#29575;&#25216;&#26415;&#65306;&#36890;&#36807;&#36890;&#29992;&#35838;&#31243;&#23398;&#20064;&#24211;&#23454;&#29616;&#39640;&#25928;&#25968;&#25454;&#37319;&#26679;&#65292;&#20197;&#21450;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#36880;&#23618;&#21024;&#38500;&#20196;&#29260;&#30340;&#25216;&#26415;&#23454;&#29616;&#39640;&#25928;&#25968;&#25454;&#36335;&#30001;&#12290;&#38024;&#23545;GPT-3 1.3B&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focuses on data efficiency capabilities. To this end, we present DeepSpeed Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#27431;&#24335;&#22411;&#21644;&#32654;&#24335;&#22411;&#26399;&#26435;&#23450;&#20215;&#38382;&#39064;&#30340;&#22810;&#32500;&#36335;&#24452;&#20381;&#36182;&#26399;&#26435;&#28145;&#24230;&#31614;&#21517;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36335;&#24452;&#20381;&#36182;&#22411;FBSDE&#38382;&#39064;&#65292;&#25968;&#20540;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#24471;&#21040;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.11691</link><description>&lt;p&gt;
&#22810;&#32500;&#36335;&#24452;&#20381;&#36182;&#26399;&#26435;&#30340;&#28145;&#24230;&#31614;&#21517;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Signature Algorithm for Multi-dimensional Path-Dependent Options. (arXiv:2211.11691v2 [q-fin.CP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#27431;&#24335;&#22411;&#21644;&#32654;&#24335;&#22411;&#26399;&#26435;&#23450;&#20215;&#38382;&#39064;&#30340;&#22810;&#32500;&#36335;&#24452;&#20381;&#36182;&#26399;&#26435;&#28145;&#24230;&#31614;&#21517;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36335;&#24452;&#20381;&#36182;&#22411;FBSDE&#38382;&#39064;&#65292;&#25968;&#20540;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36335;&#24452;&#20381;&#36182;&#26399;&#26435;&#30340;&#28145;&#24230;&#31614;&#21517;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;Hur\'e-Pham-Warin&#22312;2020&#24180;&#38024;&#23545;&#20855;&#26377;&#21453;&#23556;&#30340;&#29366;&#24577;&#20381;&#36182;&#22411;FBSDE&#30340;&#21453;&#21521;&#26041;&#26696;&#25193;&#23637;&#21040;&#20855;&#26377;&#21453;&#23556;&#30340;&#36335;&#24452;&#20381;&#36182;&#22411;FBSDE&#65292;&#36890;&#36807;&#21521;&#21453;&#21521;&#26041;&#26696;&#28155;&#21152;&#31614;&#21517;&#23618;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36866;&#29992;&#20110;&#27431;&#24335;&#22411;&#21644;&#32654;&#24335;&#22411;&#26399;&#26435;&#23450;&#20215;&#38382;&#39064;&#65292;&#32780;&#25903;&#20184;&#20989;&#25968;&#21462;&#20915;&#20110;&#22522;&#30784;&#27491;&#21521;&#32929;&#31080;&#36807;&#31243;&#30340;&#25972;&#20010;&#36335;&#24452;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25968;&#20540;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#26126;&#30830;&#20381;&#36182;&#20110;&#31614;&#21517;&#30340;&#25130;&#26029;&#38454;&#25968;&#21644;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#35823;&#24046;&#12290;&#31639;&#27861;&#30340;&#25968;&#20540;&#31034;&#20363;&#21253;&#25324;&#65306;Black-Scholes&#27169;&#22411;&#19979;&#30340;Amerasian&#26399;&#26435;&#12289;&#20855;&#26377;&#36335;&#24452;&#20381;&#36182;&#20960;&#20309;&#24179;&#22343;&#25910;&#30410;&#20989;&#25968;&#30340;&#32654;&#24335;&#26399;&#26435;&#20197;&#21450;Shiryaev&#30340;&#26368;&#20248;&#20572;&#27490;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the deep signature algorithms for path-dependent options. We extend the backward scheme in [Hur\'e-Pham-Warin. Mathematics of Computation 89, no. 324 (2020)] for state-dependent FBSDEs with reflections to path-dependent FBSDEs with reflections, by adding the signature layer to the backward scheme. Our algorithm applies to both European and American type option pricing problems while the payoff function depends on the whole paths of the underlying forward stock process. We prove the convergence analysis of our numerical algorithm with explicit dependence on the truncation order of the signature and the neural network approximation errors. Numerical examples for the algorithm are provided including: Amerasian option under the Black-Scholes model, American option with a path-dependent geometric mean payoff function, and the Shiryaev's optimal stopping problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#20013;&#30340;&#39564;&#35777;&#25351;&#26631;&#12290;&#22312;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27809;&#26377;&#35775;&#38382;&#30495;&#20540;&#39044;&#35328;&#26426;&#30340;&#24773;&#20917;&#19979;&#35774;&#35745;&#20505;&#36873;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#39564;&#35777;&#25351;&#26631;&#26159;&#23545;&#39044;&#35328;&#26426;&#30340;&#36817;&#20284;&#65292;&#25105;&#20204;&#24076;&#26395;&#25214;&#21040;&#19982;&#30495;&#20540;&#39044;&#35328;&#26426;&#26368;&#30456;&#20851;&#30340;&#39564;&#35777;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2211.10747</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#30340;&#39564;&#35777;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Exploring validation metrics for offline model-based optimisation with diffusion models. (arXiv:2211.10747v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#20013;&#30340;&#39564;&#35777;&#25351;&#26631;&#12290;&#22312;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27809;&#26377;&#35775;&#38382;&#30495;&#20540;&#39044;&#35328;&#26426;&#30340;&#24773;&#20917;&#19979;&#35774;&#35745;&#20505;&#36873;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#39564;&#35777;&#25351;&#26631;&#26159;&#23545;&#39044;&#35328;&#26426;&#30340;&#36817;&#20284;&#65292;&#25105;&#20204;&#24076;&#26395;&#25214;&#21040;&#19982;&#30495;&#20540;&#39044;&#35328;&#26426;&#26368;&#30456;&#20851;&#30340;&#39564;&#35777;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#20505;&#36873;&#26041;&#26696;&#65292;&#20197;&#26368;&#22823;&#21270;&#23545;&#20110;&#19968;&#20010;&#31216;&#20026;&#65288;&#22320;&#38754;&#30495;&#20540;&#65289;&#39044;&#35328;&#26426;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#26576;&#31181;&#22870;&#21169;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#21040;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#36807;&#31243;&#65292;&#35745;&#31639;&#39044;&#35328;&#26426;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#35757;&#32451;&#25110;&#39564;&#35777;&#36807;&#31243;&#20013;&#19981;&#20551;&#35774;&#23545;&#39044;&#35328;&#26426;&#26377;&#35775;&#38382;&#26435;&#38480;&#65292;&#36825;&#20351;&#24471;&#35780;&#20272;&#21464;&#24471;&#22797;&#26434;&#12290;&#34429;&#28982;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#39044;&#35328;&#26426;&#30340;&#36817;&#20284;&#27169;&#22411;&#24182;&#22312;&#27169;&#22411;&#39564;&#35777;&#36807;&#31243;&#20013;&#20351;&#29992;&#23427;&#20195;&#26367;&#30495;&#20540;&#39044;&#35328;&#26426;&#26469;&#27979;&#37327;&#29983;&#25104;&#20505;&#36873;&#26041;&#26696;&#30340;&#24179;&#22343;&#22870;&#21169;&#65292;&#20294;&#36825;&#31181;&#35780;&#20272;&#26159;&#36817;&#20284;&#30340;&#19988;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#36817;&#20284;&#19979;&#29983;&#25104;&#20505;&#36873;&#26041;&#26696;&#30340;&#24179;&#22343;&#22870;&#21169;&#20316;&#20026;&#19968;&#31181;&#8220;&#39564;&#35777;&#25351;&#26631;&#8221;&#65292;&#32780;&#25105;&#20204;&#26356;&#20851;&#24515;&#30340;&#26159;&#19968;&#20010;&#26356;&#22522;&#26412;&#30340;&#38382;&#39064;&#65292;&#21363;&#25214;&#21040;&#19982;&#30495;&#20540;&#39044;&#35328;&#26426;&#26368;&#30456;&#20851;&#30340;&#39564;&#35777;&#25351;&#26631;&#12290;&#36825;&#28041;&#21450;&#21040;&#25552;&#20986;&#39564;&#35777;&#25351;&#26631;&#24182;&#23545;&#35768;&#22810;&#25968;&#25454;&#38598;&#36827;&#34892;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In model-based optimisation (MBO) we are interested in using machine learning to design candidates that maximise some measure of reward with respect to a black box function called the (ground truth) oracle, which is expensive to compute since it involves executing a real world process. In offline MBO we wish to do so without assuming access to such an oracle during training or validation, with makes evaluation non-straightforward. While an approximation to the ground oracle can be trained and used in place of it during model validation to measure the mean reward over generated candidates, the evaluation is approximate and vulnerable to adversarial examples. Measuring the mean reward of generated candidates over this approximation is one such `validation metric', whereas we are interested in a more fundamental question which is finding which validation metrics correlate the most with the ground truth. This involves proposing validation metrics and quantifying them over many datasets for
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#25104;&#20026;&#28909;&#38376;&#26041;&#27861;&#65292;&#20294;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20250;&#38754;&#20020;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;"&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;</title><link>http://arxiv.org/abs/2211.02658</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#22788;&#29702;&#23398;&#20064;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation. (arXiv:2211.02658v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02658
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#25104;&#20026;&#28909;&#38376;&#26041;&#27861;&#65292;&#20294;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20250;&#38754;&#20020;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;"&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064; (ML) &#24050;&#25104;&#20026;&#25903;&#25345;&#33258;&#36866;&#24212;&#30340;&#28909;&#38376;&#26041;&#27861;&#12290;ML &#24050;&#34987;&#29992;&#26469;&#22788;&#29702;&#33258;&#36866;&#24212;&#20013;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#32500;&#25252;&#26368;&#26032;&#30340;&#36816;&#34892;&#26102;&#27169;&#22411;&#21644;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992; ML &#23384;&#22312;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#38754;&#21521;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31995;&#32479;&#30340;&#19968;&#20010;&#29305;&#21035;&#37325;&#35201;&#30340;&#25361;&#25112;&#65306;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#12290;&#36890;&#36807;&#36866;&#24212;&#31354;&#38388;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#33258;&#36866;&#24212;&#31995;&#32479;&#22312;&#26576;&#19968;&#29305;&#23450;&#26102;&#38388;&#21487;&#20197;&#36873;&#25321;&#30340;&#36866;&#24212;&#36873;&#39033;&#30340;&#38598;&#21512;&#65292;&#20197;&#26681;&#25454;&#36866;&#24212;&#36873;&#39033;&#30340;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#36866;&#24212;&#12290;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#28304;&#20110;&#24433;&#21709;&#36866;&#24212;&#36873;&#39033;&#36136;&#37327;&#23646;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#28418;&#31227;&#21487;&#33021;&#24847;&#21619;&#30528;&#26368;&#32456;&#27809;&#26377;&#36866;&#24212;&#36873;&#39033;&#33021;&#22815;&#28385;&#36275;&#26368;&#21021;&#30340;&#36866;&#24212;&#30446;&#26631;&#65292;&#20174;&#32780;&#38477;&#20302;&#31995;&#32479;&#30340;&#36136;&#37327;&#65292;&#25110;&#32773;&#21487;&#33021;&#20986;&#29616;&#20801;&#35768;&#22686;&#24378;&#36866;&#24212;&#30446;&#26631;&#30340;&#36866;&#24212;&#36873;&#39033;&#12290;&#22312; ML &#20013;&#65292;&#36825;&#31181;&#28418;&#31227;&#36890;&#24120;&#34987;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#25110;&#23454;&#20363;&#28418;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#8221;&#30340;&#26032;&#26041;&#27861;&#12290;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#23545; ML powered self-adaptation &#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, machine learning (ML) has become a popular approach to support self-adaptation. ML has been used to deal with several problems in self-adaptation, such as maintaining an up-to-date runtime model under uncertainty and scalable decision-making. Yet, exploiting ML comes with inherent challenges. In this paper, we focus on a particularly important challenge for learning-based self-adaptive systems: drift in adaptation spaces. With adaptation space we refer to the set of adaptation options a self-adaptive system can select from at a given time to adapt based on the estimated quality properties of the adaptation options. Drift of adaptation spaces originates from uncertainties, affecting the quality properties of the adaptation options. Such drift may imply that eventually no adaptation option can satisfy the initial set of the adaptation goals, deteriorating the quality of the system, or adaptation options may emerge that allow enhancing the adaptation goals. In ML, such shift cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#32954;&#28814;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32423;&#33258;&#27880;&#24847;&#26426;&#21046;&#21152;&#36895;&#25910;&#25947;&#24182;&#24378;&#35843;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#21306;&#22495;&#12290;&#37319;&#29992;&#23454;&#29992;&#30340;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#20026;&#21307;&#23398;&#23454;&#36341;&#25552;&#20379;&#39640;&#36895;&#20998;&#26512;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2210.16584</link><description>&lt;p&gt;
&#29992;&#20110;&#24555;&#36895;&#35782;&#21035;&#33016;&#37096; X &#23556;&#32447;&#22270;&#20687;&#20013;&#32954;&#28814;&#30340;&#21487;&#35299;&#37322; CNN-Multilevel Attention Transformer
&lt;/p&gt;
&lt;p&gt;
Interpretable CNN-Multilevel Attention Transformer for Rapid Recognition of Pneumonia from Chest X-Ray Images. (arXiv:2210.16584v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#32954;&#28814;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32423;&#33258;&#27880;&#24847;&#26426;&#21046;&#21152;&#36895;&#25910;&#25947;&#24182;&#24378;&#35843;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#21306;&#22495;&#12290;&#37319;&#29992;&#23454;&#29992;&#30340;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#20026;&#21307;&#23398;&#23454;&#36341;&#25552;&#20379;&#39640;&#36895;&#20998;&#26512;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;&#25104;&#20687;&#22312;&#35786;&#26029;&#21644;&#39044;&#27979; COVID-19 &#24739;&#32773;&#21628;&#21560;&#29366;&#20917;&#24694;&#21270;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#24050;&#32463;&#24320;&#21457;&#20986;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32954;&#28814;&#35782;&#21035;&#26041;&#27861;&#26469;&#23454;&#29616;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#20351;&#23427;&#20204;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#32780;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#38477;&#20302;&#20102;&#23427;&#20204;&#22312;&#20020;&#24202;&#21307;&#23398;&#23454;&#36341;&#20013;&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#32954;&#28814;&#35782;&#21035;&#26694;&#26550;&#65292;&#21487;&#20197;&#29702;&#35299;&#33016;&#37096; X &#23556;&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#20013;&#32954;&#37096;&#29305;&#24449;&#19982;&#30456;&#20851;&#30142;&#30149;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20026;&#21307;&#23398;&#23454;&#36341;&#25552;&#20379;&#39640;&#36895;&#20998;&#26512;&#25903;&#25345;&#12290;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#20197;&#21152;&#36895;&#35782;&#21035;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110; Transformer &#30340;&#22810;&#32423;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#21152;&#36895;&#25910;&#25947;&#24182;&#24378;&#35843;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#20102;&#23454;&#29992;&#30340; CXR &#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chest imaging plays an essential role in diagnosing and predicting patients with COVID-19 with evidence of worsening respiratory status. Many deep learning-based approaches for pneumonia recognition have been developed to enable computer-aided diagnosis. However, the long training and inference time makes them inflexible, and the lack of interpretability reduces their credibility in clinical medical practice. This paper aims to develop a pneumonia recognition framework with interpretability, which can understand the complex relationship between lung features and related diseases in chest X-ray (CXR) images to provide high-speed analytics support for medical practice. To reduce the computational complexity to accelerate the recognition process, a novel multi-level self-attention mechanism within Transformer has been proposed to accelerate convergence and emphasize the task-related feature regions. Moreover, a practical CXR image data augmentation has been adopted to address the scarcity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#24863;&#20852;&#36259;&#28857;&#30340;&#36873;&#25321;&#21644;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#31283;&#23450;&#21487;&#38752;&#30340;&#31232;&#30095;&#36924;&#36817;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.07893</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Cover Trees&#30340;&#26368;&#23567;&#38388;&#38548;&#23454;&#29616;&#25968;&#20540;&#31283;&#23450;&#30340;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees. (arXiv:2210.07893v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#24863;&#20852;&#36259;&#28857;&#30340;&#36873;&#25321;&#21644;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#31283;&#23450;&#21487;&#38752;&#30340;&#31232;&#30095;&#36924;&#36817;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#24120;&#29992;&#20110;&#36739;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20915;&#31574;&#31995;&#32479;&#20013;&#65292;&#20363;&#22914;&#22320;&#29702;&#31354;&#38388;&#24314;&#27169;&#12289;&#36125;&#21494;&#26031;&#20248;&#21270;&#25110;&#28508;&#22312;&#39640;&#26031;&#27169;&#22411;&#20013;&#12290;&#22312;&#19968;&#20010;&#31995;&#32479;&#20013;&#65292;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#38656;&#35201;&#20197;&#31283;&#23450;&#21487;&#38752;&#30340;&#26041;&#24335;&#36816;&#34892;&#65292;&#20197;&#30830;&#20445;&#19982;&#31995;&#32479;&#30340;&#20854;&#20182;&#37096;&#20998;&#27491;&#30830;&#20132;&#20114;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24863;&#20852;&#36259;&#28857;&#30340;&#21487;&#25193;&#23637;&#31232;&#30095;&#36924;&#36817;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#21487;&#33021;&#19981;&#31283;&#23450;&#30340;&#20856;&#22411;&#24773;&#20917;&#12290;&#22312;&#25554;&#20540;&#25991;&#29486;&#20013;&#21407;&#22987;&#24320;&#21457;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#23545;&#24863;&#20852;&#36259;&#28857;&#36827;&#34892;&#35745;&#31639;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#21644;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#23545;&#20110;&#22320;&#29702;&#31354;&#38388;&#24314;&#27169;&#31561;&#20302;&#32500;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35745;&#31639;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#30340;&#24863;&#20852;&#36259;&#28857;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are frequently deployed as part of larger machine learning and decision-making systems, for instance in geospatial modeling, Bayesian optimization, or in latent Gaussian models. Within a system, the Gaussian process model needs to perform in a stable and reliable manner to ensure it interacts correctly with other parts of the system. In this work, we study the numerical stability of scalable sparse approximations based on inducing points. To do so, we first review numerical stability, and illustrate typical situations in which Gaussian process models can be unstable. Building on stability theory originally developed in the interpolation literature, we derive sufficient and in certain cases necessary conditions on the inducing points for the computations performed to be numerically stable. For low-dimensional tasks such as geospatial modeling, we propose an automated method for computing inducing points satisfying these conditions. This is done via a modification of t
&lt;/p&gt;</description></item><item><title>RALACs&#26159;&#19968;&#31181;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#21160;&#20316;&#35782;&#21035;&#30340;&#26032;&#39062;&#31995;&#32479;&#65292;&#36890;&#36807;&#20132;&#20114;&#32534;&#30721;&#21644;&#20809;&#27969;&#25216;&#26415;&#23558;&#21160;&#20316;&#35782;&#21035;&#24212;&#29992;&#20110;&#36947;&#36335;&#22330;&#26223;&#65292;&#24182;&#24357;&#21512;&#20102;&#20854;&#19982;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2209.14408</link><description>&lt;p&gt;
RALACs: &#20351;&#29992;&#20132;&#20114;&#32534;&#30721;&#21644;&#20809;&#27969;&#36827;&#34892;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
RALACs: Action Recognition in Autonomous Vehicles using Interaction Encoding and Optical Flow. (arXiv:2209.14408v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14408
&lt;/p&gt;
&lt;p&gt;
RALACs&#26159;&#19968;&#31181;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#21160;&#20316;&#35782;&#21035;&#30340;&#26032;&#39062;&#31995;&#32479;&#65292;&#36890;&#36807;&#20132;&#20114;&#32534;&#30721;&#21644;&#20809;&#27969;&#25216;&#26415;&#23558;&#21160;&#20316;&#35782;&#21035;&#24212;&#29992;&#20110;&#36947;&#36335;&#22330;&#26223;&#65292;&#24182;&#24357;&#21512;&#20102;&#20854;&#19982;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21160;&#20316;&#35782;&#21035;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#29615;&#22659;&#21487;&#20197;&#22686;&#24378;&#29615;&#22659;&#27169;&#22411;&#30340;&#24773;&#22659;&#24863;&#30693;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#21160;&#20316;&#35782;&#21035;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#20154;&#31867;&#65292;&#20854;&#23545;&#20110;&#22024;&#26434;&#12289;&#26410;&#21098;&#36753;&#12289;&#21407;&#22987;&#30340;RGB&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#26377;&#38480;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#20026;&#25512;&#21160;&#21160;&#20316;&#35782;&#21035;&#22312;AVs&#20013;&#30340;&#36827;&#23637;&#21644;&#24212;&#29992;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#38454;&#27573;&#21160;&#20316;&#35782;&#21035;&#31995;&#32479;&#65292;&#21629;&#21517;&#20026;RALACs&#12290;RALACs&#23558;&#21160;&#20316;&#35782;&#21035;&#38382;&#39064;&#24212;&#29992;&#20110;&#36947;&#36335;&#22330;&#26223;&#65292;&#24182;&#24357;&#21512;&#20102;&#20854;&#19982;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#27880;&#24847;&#21147;&#23618;&#22914;&#20309;&#26377;&#21161;&#20110;&#32534;&#30721;agent&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#24378;&#35843;&#36825;&#31181;&#26041;&#26696;&#22914;&#20309;&#19982;&#31867;&#21035;&#26080;&#20851;&#12290;&#27492;&#22806;&#65292;&#20026;&#35299;&#20915;&#36947;&#36335;&#19978;agent&#30340;&#21160;&#24577;&#24615;&#65292;RALACs&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
When applied to autonomous vehicle (AV) settings, action recognition can enhance an environment model's situational awareness. This is especially prevalent in scenarios where traditional geometric descriptions and heuristics in AVs are insufficient. However, action recognition has traditionally been studied for humans, and its limited adaptability to noisy, un-clipped, un-pampered, raw RGB data has limited its application in other fields. To push for the advancement and adoption of action recognition into AVs, this work proposes a novel two-stage action recognition system, termed RALACs. RALACs formulates the problem of action recognition for road scenes, and bridges the gap between it and the established field of human action recognition. This work shows how attention layers can be useful for encoding the relations across agents, and stresses how such a scheme can be class-agnostic. Furthermore, to address the dynamic nature of agents on the road, RALACs constructs a novel approach to
&lt;/p&gt;</description></item><item><title>MIXRTs&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#28151;&#21512;&#24490;&#29615;&#36719;&#20915;&#31574;&#26641;&#30340;&#26041;&#24335;&#65292;&#33021;&#22815;&#34920;&#36798;&#26126;&#30830;&#30340;&#20915;&#31574;&#36807;&#31243;&#24182;&#23637;&#31034;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2209.07225</link><description>&lt;p&gt;
MIXRTs:&#36890;&#36807;&#28151;&#21512;&#24490;&#29615;&#36719;&#20915;&#31574;&#26641;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MIXRTs: Toward Interpretable Multi-Agent Reinforcement Learning via Mixing Recurrent Soft Decision Trees. (arXiv:2209.07225v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07225
&lt;/p&gt;
&lt;p&gt;
MIXRTs&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#28151;&#21512;&#24490;&#29615;&#36719;&#20915;&#31574;&#26641;&#30340;&#26041;&#24335;&#65292;&#33021;&#22815;&#34920;&#36798;&#26126;&#30830;&#30340;&#20915;&#31574;&#36807;&#31243;&#24182;&#23637;&#31034;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#30340;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#20855;&#26377;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20197;&#19981;&#36879;&#26126;&#30340;&#26041;&#24335;&#20570;&#20986;&#20915;&#31574;&#65292;&#38459;&#30861;&#20102;&#20154;&#20204;&#29702;&#35299;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#20197;&#21450;&#36755;&#20837;&#35266;&#27979;&#22914;&#20309;&#24433;&#21709;&#20915;&#31574;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#22914;&#20256;&#32479;&#30340;&#32447;&#24615;&#27169;&#22411;&#21644;&#20915;&#31574;&#26641;&#65292;&#24448;&#24448;&#22312;&#34920;&#36798;&#33021;&#21147;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26126;&#26174;&#20108;&#20803;&#23545;&#31435;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#32467;&#26500;&#8212;&#8212;&#28151;&#21512;&#24490;&#29615;&#36719;&#20915;&#31574;&#26641;&#65288;MIXRTs&#65289;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#20174;&#26681;&#33410;&#28857;&#21040;&#21494;&#33410;&#28857;&#30340;&#36335;&#24452;&#34920;&#31034;&#26126;&#30830;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21453;&#26144;&#27599;&#20010;&#26234;&#33021;&#20307;&#23545;&#22242;&#38431;&#30340;&#36129;&#29486;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36719;&#20915;&#31574;&#26641;&#26469;&#35299;&#20915;&#23616;&#37096;&#21487;&#35266;&#23519;&#24615;&#38382;&#39064;&#65292;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#23637;&#31034;&#21738;&#20123;&#29305;&#24449;&#24433;&#21709;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
While achieving tremendous success in various fields, existing multi-agent reinforcement learning (MARL) with a black-box neural network architecture makes decisions in an opaque manner that hinders humans from understanding the learned knowledge and how input observations influence decisions. Instead, existing interpretable approaches, such as traditional linear models and decision trees, usually suffer from weak expressivity and low accuracy. To address this apparent dichotomy between performance and interpretability, our solution, MIXing Recurrent soft decision Trees (MIXRTs), is a novel interpretable architecture that can represent explicit decision processes via the root-to-leaf path and reflect each agent's contribution to the team. Specifically, we construct a novel soft decision tree to address partial observability by leveraging the advances in recurrent neural networks, and demonstrate which features influence the decision-making process through the tree-based model. Then, ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#20869;&#37096;&#24230;&#37327;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2209.02935</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#32858;&#31867;&#20934;&#30830;&#24230;&#65306;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Normalised clustering accuracy: An asymmetric external cluster validity measure. (arXiv:2209.02935v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#20869;&#37096;&#24230;&#37327;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#19968;&#20010;&#26368;&#22909;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#25105;&#20204;&#20173;&#28982;&#24076;&#26395;&#33021;&#22815;&#21306;&#20998;&#20986;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#32858;&#31867;&#31639;&#27861;&#20351;&#29992;&#20869;&#37096;&#25110;&#22806;&#37096;&#26377;&#25928;&#24230;&#37327;&#36827;&#34892;&#35780;&#20272;&#12290;&#20869;&#37096;&#24230;&#37327;&#37327;&#21270;&#25152;&#24471;&#20998;&#21306;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#20363;&#22914;&#65292;&#31751;&#32039;&#23494;&#24230;&#30340;&#24179;&#22343;&#31243;&#24230;&#25110;&#28857;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#20419;&#20351;&#30340;&#32858;&#31867;&#26377;&#26102;&#21487;&#33021;&#26159;&#26080;&#24847;&#20041;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22806;&#37096;&#24230;&#37327;&#23558;&#31639;&#27861;&#30340;&#36755;&#20986;&#19982;&#30001;&#19987;&#23478;&#25552;&#20379;&#30340;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24120;&#29992;&#30340;&#32463;&#20856;&#20998;&#21306;&#30456;&#20284;&#24615;&#35780;&#20998;&#65292;&#20363;&#22914;&#35268;&#33539;&#21270;&#20114;&#20449;&#24687;&#12289;Fowlkes-Mallows&#25110;&#35843;&#25972;&#20848;&#24503;&#25351;&#25968;&#65292;&#32570;&#23569;&#19968;&#20123;&#21487;&#21462;&#30340;&#23646;&#24615;&#65292;&#20363;&#22914;&#65292;&#23427;&#20204;&#19981;&#33021;&#27491;&#30830;&#35782;&#21035;&#26368;&#22351;&#24773;&#20917;&#65292;&#20063;&#19981;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is no, nor will there ever be, single best clustering algorithm, but we would still like to be able to distinguish between methods which work well on certain task types and those that systematically underperform. Clustering algorithms are traditionally evaluated using either internal or external validity measures. Internal measures quantify different aspects of the obtained partitions, e.g., the average degree of cluster compactness or point separability. Yet, their validity is questionable, because the clusterings they promote can sometimes be meaningless. External measures, on the other hand, compare the algorithms' outputs to the reference, ground truth groupings that are provided by experts. In this paper, we argue that the commonly-used classical partition similarity scores, such as the normalised mutual information, Fowlkes-Mallows, or adjusted Rand index, miss some desirable properties, e.g., they do not identify worst-case scenarios correctly or are not easily interpretab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#21017;&#21270;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#39044;&#27979;&#33391;&#22909;&#30340;&#21453;&#24212;&#22352;&#26631;&#20197;&#21450;MD&#36712;&#36857;&#30340;&#28436;&#21270;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#27491;&#21017;&#21270;&#32422;&#26463;&#23545;&#20110;&#36873;&#25321;&#37325;&#35201;&#21453;&#24212;&#22352;&#26631;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2208.10962</link><description>&lt;p&gt;
&#20351;&#29992;&#27491;&#21017;&#21270;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#39044;&#27979;&#33391;&#22909;&#21453;&#24212;&#22352;&#26631;&#21644;MD&#36712;&#36857;&#30340;&#26410;&#26469;&#28436;&#21270;&#65306;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prediction of good reaction coordinates and future evolution of MD trajectories using Regularized Sparse Autoencoders: A novel deep learning approach. (arXiv:2208.10962v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#21017;&#21270;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#39044;&#27979;&#33391;&#22909;&#30340;&#21453;&#24212;&#22352;&#26631;&#20197;&#21450;MD&#36712;&#36857;&#30340;&#28436;&#21270;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#27491;&#21017;&#21270;&#32422;&#26463;&#23545;&#20110;&#36873;&#25321;&#37325;&#35201;&#21453;&#24212;&#22352;&#26631;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#23450;&#21453;&#24212;&#22352;&#26631;(RCs)&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20026;RCs&#22312;&#30830;&#23450;&#21270;&#23398;&#21453;&#24212;&#30340;&#36827;&#23637;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36873;&#25321;&#21453;&#24212;&#22352;&#26631;&#36890;&#24120;&#22522;&#20110;&#21551;&#21457;&#24335;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#30340;&#26631;&#20934;&#20043;&#19968;&#26159;&#35813;&#22352;&#26631;&#24212;&#28165;&#26224;&#22320;&#25429;&#33719;&#21453;&#24212;&#29289;&#21644;&#29983;&#25104;&#29289;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#22352;&#26631;&#24212;&#35813;&#26159;&#26368;&#24930;&#30340;&#65292;&#20351;&#24471;&#25152;&#26377;&#20854;&#20182;&#33258;&#30001;&#24230;&#21487;&#20197;&#27839;&#30528;&#21453;&#24212;&#22352;&#26631;&#36731;&#26494;&#36798;&#21040;&#24179;&#34913;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#21363;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#26469;&#21457;&#29616;&#19968;&#32452;&#20851;&#38190;&#30340;&#21453;&#24212;&#22352;&#26631;&#12290;&#38500;&#20102;&#21457;&#29616;&#21453;&#24212;&#22352;&#26631;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#39044;&#27979;&#20998;&#23376;&#21160;&#21147;&#23398;(MD)&#36712;&#36857;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21253;&#25324;&#31232;&#30095;&#32422;&#26463;&#27491;&#21017;&#21270;&#26377;&#21161;&#20110;&#36873;&#25321;&#19968;&#20010;&#23567;&#20294;&#37325;&#35201;&#30340;&#19968;&#32452;&#21453;&#24212;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying reaction coordinates(RCs) is an active area of research, given the crucial role RCs play in determining the progress of a chemical reaction. The choice of the reaction coordinate is often based on heuristic knowledge. However, an essential criterion for the choice is that the coordinate should capture both the reactant and product states unequivocally. Also, the coordinate should be the slowest one so that all the other degrees of freedom can easily equilibrate along the reaction coordinate. Also, the coordinate should be the slowest one so that all the other degrees of freedom can easily equilibrate along the reaction coordinate. We used a regularised sparse autoencoder, an energy-based model, to discover a crucial set of reaction coordinates. Along with discovering reaction coordinates, our model also predicts the evolution of a molecular dynamics(MD) trajectory. We showcased that including sparsity enforcing regularisation helps in choosing a small but important set of r
&lt;/p&gt;</description></item><item><title>NAPA&#26159;&#19968;&#31181;&#20013;&#32423;&#21487;&#21464;&#20998;&#30340;&#26412;&#22320;&#33033;&#20914;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#33033;&#20914;&#24207;&#21015;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2208.01215</link><description>&lt;p&gt;
NAPA: &#20013;&#32423;&#21487;&#21464;&#20998;&#30340;&#26412;&#22320;&#33033;&#20914;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
NAPA: Intermediate-level Variational Native-pulse Ansatz for Variational Quantum Algorithms. (arXiv:2208.01215v5 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01215
&lt;/p&gt;
&lt;p&gt;
NAPA&#26159;&#19968;&#31181;&#20013;&#32423;&#21487;&#21464;&#20998;&#30340;&#26412;&#22320;&#33033;&#20914;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#33033;&#20914;&#24207;&#21015;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#22312;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#33258;&#21464;&#37327;&#26469;&#36924;&#36817;&#26399;&#26395;&#30340;&#37327;&#23376;&#24577;&#12290;&#22312;&#35774;&#35745;&#26356;&#22909;&#30340;&#33258;&#21464;&#37327;&#26041;&#38754;&#65292;&#25105;&#20204;&#24050;&#32463;&#30475;&#21040;&#20102;&#21508;&#31181;&#21162;&#21147;&#65292;&#20197;&#20943;&#23569;&#38376;&#25968;&#37327;&#12290;&#26377;&#20123;&#26041;&#27861;&#32771;&#34385;&#20102;&#24213;&#23618;&#30005;&#36335;&#30340;&#29289;&#29702;&#24847;&#20041;&#65292;&#32780;&#21478;&#19968;&#20123;&#26041;&#27861;&#21017;&#37319;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#30340;&#24605;&#24819;&#26469;&#29983;&#25104;&#33258;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#35745;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#20248;&#21183;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#25216;&#26415;&#37117;&#38024;&#23545;&#38376;&#33258;&#21464;&#37327;&#65292;&#24182;&#19988;&#21442;&#25968;&#36890;&#24120;&#26159;&#38376;&#30340;&#26059;&#36716;&#35282;&#24230;&#12290;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#20013;&#65292;&#38376;&#33258;&#21464;&#37327;&#26368;&#32456;&#23558;&#36716;&#25442;&#20026;&#25511;&#21046;&#20449;&#21495;&#65292;&#20363;&#22914;&#36229;&#23548;&#37327;&#23376;&#27604;&#29305;&#19978;&#30340;&#24494;&#27874;&#33033;&#20914;&#12290;&#36825;&#20123;&#25511;&#21046;&#33033;&#20914;&#38656;&#35201;&#31934;&#30830;&#26657;&#20934;&#20197;&#26368;&#23567;&#21270;&#35832;&#22914;&#36807;&#26059;&#36716;&#21644;&#27424;&#26059;&#36716;&#20043;&#31867;&#30340;&#35823;&#24046;&#12290;&#22312;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#36807;&#31243;&#20250;&#24341;&#20837;&#20887;&#20313;&#65292;&#20294;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#21487;&#21464;&#24615;&#36136;&#21487;&#20197;&#29992;&#26469;&#26356;&#22909;&#22320;&#20248;&#21270;&#33033;&#20914;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational quantum algorithms (VQAs) have demonstrated great potentials in the Noisy Intermediate Scale Quantum (NISQ) era. In the workflow of VQA, the parameters of ansatz are iteratively updated to approximate the desired quantum states. We have seen various efforts to draft better ansatz with less gates. Some works consider the physical meaning of the underlying circuits, while others adopt the ideas of neural architecture search (NAS) for ansatz generator. However, these designs do not exploit the full advantages of VQAs. Because most techniques target gate ansatz, and the parameters are usually rotation angles of the gates. In quantum computers, the gate ansatz will eventually be transformed into control signals such as microwave pulses on superconducting qubits. These control pulses need elaborate calibrations to minimize the errors such as over-rotation and under-rotation. In the case of VQAs, this procedure will introduce redundancy, but the variational properties of VQAs can 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#31216;&#24615;&#21644;&#36864;&#28779;&#34917;&#20805;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#31934;&#30830;&#22320;&#20272;&#35745;&#20108;&#32500;&#28023;&#26862;&#22561;&#27169;&#22411;&#30340;&#22522;&#24577;&#33021;&#37327;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#31995;&#32479;&#23610;&#23544;&#19978;&#20855;&#26377;&#37325;&#35201;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2207.14314</link><description>&lt;p&gt;
&#29992;&#23545;&#31216;&#24615;&#21644;&#36864;&#28779;&#34917;&#20805;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#20197;&#25552;&#39640;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Supplementing Recurrent Neural Network Wave Functions with Symmetry and Annealing to Improve Accuracy. (arXiv:2207.14314v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#31216;&#24615;&#21644;&#36864;&#28779;&#34917;&#20805;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#31934;&#30830;&#22320;&#20272;&#35745;&#20108;&#32500;&#28023;&#26862;&#22561;&#27169;&#22411;&#30340;&#22522;&#24577;&#33021;&#37327;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#31995;&#32479;&#23610;&#23544;&#19978;&#20855;&#26377;&#37325;&#35201;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26159;&#19968;&#31867;&#20174;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#20013;&#20986;&#29616;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#35768;&#22810;&#26377;&#36259;&#30340;&#36827;&#23637;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#20123;&#26550;&#26500;&#34987;&#35777;&#26126;&#26159;&#36817;&#20284;&#37327;&#23376;&#31995;&#32479;&#22522;&#24577;&#30340;&#24378;&#22823;ansatze&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#24314;&#31435;&#22312;[Phys. Rev. Research 2, 023358 (2020)]&#30340;&#32467;&#26524;&#20043;&#19978;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#20108;&#32500;RNN&#27874;&#20989;&#25968;ansatz&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#31216;&#24615;&#21644;&#36864;&#28779;&#26469;&#33719;&#24471;&#22312;&#26041;&#24418;&#26684;&#21644;&#19977;&#35282;&#24418;&#26684;&#19978;&#20108;&#32500;&#28023;&#26862;&#22561;&#27169;&#22411;&#22522;&#24577;&#33021;&#37327;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#23494;&#24230;&#30697;&#38453;&#37325;&#25972;&#21270;&#32676;&#65288;DMRG&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#35282;&#24418;&#26684;&#30340;&#31995;&#32479;&#23610;&#23544;&#22823;&#20110;&#25110;&#31561;&#20110;$14 \times 14$&#26102;&#26356;&#20026;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural networks (RNNs) are a class of neural networks that have emerged from the paradigm of artificial intelligence and has enabled lots of interesting advances in the field of natural language processing. Interestingly, these architectures were shown to be powerful ansatze to approximate the ground state of quantum systems. Here, we build over the results of [Phys. Rev. Research 2, 023358 (2020)] and construct a more powerful RNN wave function ansatz in two dimensions. We use symmetry and annealing to obtain accurate estimates of ground state energies of the two-dimensional (2D) Heisenberg model, on the square lattice and on the triangular lattice. We show that our method is superior to Density Matrix Renormalisation Group (DMRG) for system sizes larger than or equal to $14 \times 14$ on the triangular lattice.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#38024;&#23545;ElasticNet&#30340;&#26032;&#39062;&#32467;&#26500;&#32467;&#26524;&#65292;&#29992;&#20197;&#35777;&#26126;&#22312;&#22810;&#20010;&#38382;&#39064;&#23454;&#20363;&#20013;&#35843;&#25972;&#27491;&#35268;&#21270;&#21442;&#25968;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#32479;&#35745;&#21644;&#22312;&#32447;&#23398;&#20064;&#24773;&#26223;&#19979;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2207.10199</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#22320;&#35843;&#25972;ElasticNet&#22312;&#22810;&#20010;&#23454;&#20363;&#38388;&#30340;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Provably tuning the ElasticNet across instances. (arXiv:2207.10199v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10199
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#38024;&#23545;ElasticNet&#30340;&#26032;&#39062;&#32467;&#26500;&#32467;&#26524;&#65292;&#29992;&#20197;&#35777;&#26126;&#22312;&#22810;&#20010;&#38382;&#39064;&#23454;&#20363;&#20013;&#35843;&#25972;&#27491;&#35268;&#21270;&#21442;&#25968;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#32479;&#35745;&#21644;&#22312;&#32447;&#23398;&#20064;&#24773;&#26223;&#19979;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#35268;&#21270;&#29702;&#35770;&#20013;&#19968;&#20010;&#37325;&#35201;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#35774;&#32622;&#24120;&#29992;&#25216;&#26415;&#65288;&#22914;ElasticNet&#65289;&#30340;&#27491;&#35268;&#21270;&#31995;&#25968;&#65292;&#24182;&#25552;&#20379;&#19968;&#33324;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#22810;&#20010;&#38382;&#39064;&#23454;&#20363;&#20013;&#35843;&#25972;Ridge&#22238;&#24402;&#12289;LASSO&#21644;ElasticNet&#30340;&#27491;&#35268;&#21270;&#21442;&#25968;&#30340;&#38382;&#39064;&#65292;&#36825;&#31181;&#35774;&#32622;&#21253;&#25324;&#20102;&#20132;&#21449;&#39564;&#35777;&#21644;&#22810;&#20219;&#21153;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#38024;&#23545;ElasticNet&#30340;&#26032;&#39062;&#32467;&#26500;&#32467;&#26524;&#65292;&#23558;&#25439;&#22833;&#20989;&#25968;&#34920;&#36798;&#20026;&#20197;&#35843;&#25972;&#21442;&#25968;&#20026;&#20989;&#25968;&#30340;&#20998;&#27573;&#26377;&#29702;&#20989;&#25968;&#65292;&#20854;&#20013;&#21253;&#21547;&#20195;&#25968;&#36793;&#30028;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#23545;&#27491;&#35268;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#36827;&#34892;&#20102;&#30028;&#23450;&#65292;&#24182;&#22312;&#32479;&#35745;&#24773;&#26223;&#19979;&#23637;&#31034;&#20102;&#35843;&#25972;ElasticNet&#22238;&#24402;&#31995;&#25968;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22312;&#32447;&#23398;&#20064;&#24773;&#26223;&#65292;&#23637;&#31034;&#20102;&#19982;&#26368;&#20248;&#21442;&#25968;&#23545;&#30456;&#23545;&#32780;&#35328;&#65292;&#28040;&#22833;&#30340;&#24179;&#22343;&#39044;&#26399;&#36951;&#25022;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#35843;&#25972;&#20998;&#31867;&#38382;&#39064;&#30340;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important unresolved challenge in the theory of regularization is to set the regularization coefficients of popular techniques like the ElasticNet with general provable guarantees. We consider the problem of tuning the regularization parameters of Ridge regression, LASSO, and the ElasticNet across multiple problem instances, a setting that encompasses both cross-validation and multi-task hyperparameter optimization. We obtain a novel structural result for the ElasticNet which characterizes the loss as a function of the tuning parameters as a piecewise-rational function with algebraic boundaries. We use this to bound the structural complexity of the regularized loss functions and show generalization guarantees for tuning the ElasticNet regression coefficients in the statistical setting. We also consider the more challenging online learning setting, where we show vanishing average expected regret relative to the optimal parameter pair. We further extend our results to tuning classific
&lt;/p&gt;</description></item><item><title>SPIRAL&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#27491;&#21017;&#21270;&#26377;&#38480;&#21644;&#38382;&#39064;&#30340;&#36229;&#32447;&#24615;&#25910;&#25947;&#22686;&#37327;&#36817;&#31471;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22686;&#37327;&#26799;&#24230;&#26356;&#26032;&#21644;&#27704;&#19981;&#35302;&#21457;&#28176;&#36817;&#34892;&#25628;&#32034;&#29305;&#24615;&#65292;&#22312;&#26497;&#38480;&#28857;&#19979;&#23454;&#29616;&#20102;&#36229;&#32447;&#24615;&#25910;&#25947;&#12290;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#20984;&#12289;&#38750;&#20984;&#21644;&#38750;Lipschitz&#21487;&#24494;&#38382;&#39064;&#65292;SPIRAL&#31639;&#27861;&#21450;&#20854;&#33258;&#36866;&#24212;&#21464;&#20307;&#34920;&#29616;&#20986;&#20102;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2207.08195</link><description>&lt;p&gt;
SPIRAL&#65306;&#19968;&#31181;&#29992;&#20110;&#38750;&#20984;&#26377;&#38480;&#21644;&#26368;&#23567;&#21270;&#30340;&#36229;&#32447;&#24615;&#25910;&#25947;&#22686;&#37327;&#36817;&#31471;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SPIRAL: A superlinearly convergent incremental proximal algorithm for nonconvex finite sum minimization. (arXiv:2207.08195v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08195
&lt;/p&gt;
&lt;p&gt;
SPIRAL&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#27491;&#21017;&#21270;&#26377;&#38480;&#21644;&#38382;&#39064;&#30340;&#36229;&#32447;&#24615;&#25910;&#25947;&#22686;&#37327;&#36817;&#31471;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22686;&#37327;&#26799;&#24230;&#26356;&#26032;&#21644;&#27704;&#19981;&#35302;&#21457;&#28176;&#36817;&#34892;&#25628;&#32034;&#29305;&#24615;&#65292;&#22312;&#26497;&#38480;&#28857;&#19979;&#23454;&#29616;&#20102;&#36229;&#32447;&#24615;&#25910;&#25947;&#12290;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#20984;&#12289;&#38750;&#20984;&#21644;&#38750;Lipschitz&#21487;&#24494;&#38382;&#39064;&#65292;SPIRAL&#31639;&#27861;&#21450;&#20854;&#33258;&#36866;&#24212;&#21464;&#20307;&#34920;&#29616;&#20986;&#20102;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SPIRAL&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#30456;&#23545;&#24179;&#28369;&#24615;&#20551;&#35774;&#19979;&#35299;&#20915;&#38750;&#20984;&#27491;&#21017;&#21270;&#26377;&#38480;&#21644;&#38382;&#39064;&#30340;&#36229;&#32447;&#24615;&#25910;&#25947;&#22686;&#37327;&#36817;&#31471;&#31639;&#27861;&#12290;SPIRAL&#30340;&#27599;&#27425;&#36845;&#20195;&#21253;&#25324;&#20869;&#24490;&#29615;&#21644;&#22806;&#24490;&#29615;&#12290;&#23427;&#32467;&#21512;&#20102;&#22686;&#37327;&#26799;&#24230;&#26356;&#26032;&#21644;&#20855;&#26377;&#27704;&#19981;&#35302;&#21457;&#28176;&#36817;&#34892;&#25628;&#32034;&#29305;&#24615;&#30340;&#32447;&#25628;&#32034;&#65292;&#22312;&#26497;&#38480;&#28857;&#19979;&#20197;&#36229;&#32447;&#24615;&#25910;&#25947;&#12290;&#22312;&#19981;&#21516;&#30340;&#20984;&#12289;&#38750;&#20984;&#21644;&#38750;Lipschitz&#21487;&#24494;&#38382;&#39064;&#19978;&#20351;&#29992;L-BFGS&#26041;&#21521;&#36827;&#34892;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20197;&#21450;&#20854;&#33258;&#36866;&#24212;&#21464;&#20307;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SPIRAL, a SuPerlinearly convergent Incremental pRoximal ALgorithm, for solving nonconvex regularized finite sum problems under a relative smoothness assumption. Each iteration of SPIRAL consists of an inner and an outer loop. It combines incremental gradient updates with a linesearch that has the remarkable property of never being triggered asymptotically, leading to superlinear convergence under mild assumptions at the limit point. Simulation results with L-BFGS directions on different convex, nonconvex, and non-Lipschitz differentiable problems show that our algorithm, as well as its adaptive variant, are competitive to the state of the art.
&lt;/p&gt;</description></item><item><title>KeyCLD&#26159;&#19968;&#20010;&#20174;&#22270;&#20687;&#20013;&#23398;&#20064;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#28857;&#22352;&#26631;&#34920;&#31034;&#29366;&#24577;&#21160;&#21147;&#23398;&#65292;&#24182;&#32467;&#21512;&#26174;&#24335;&#30340;&#23436;&#25972;&#32422;&#26463;&#26469;&#34920;&#31034;&#21160;&#21147;&#23398;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#26080;&#30417;&#30563;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20934;&#30830;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#20102;&#38271;&#26399;&#35270;&#39057;&#39044;&#27979;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#30740;&#31350;&#20102;&#25289;&#26684;&#26391;&#26085;&#20808;&#39564;&#21644;&#32422;&#26463;&#20989;&#25968;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2206.11030</link><description>&lt;p&gt;
KeyCLD: &#20174;&#22270;&#20687;&#20013;&#23398;&#20064;&#22522;&#20110;&#20851;&#38190;&#28857;&#22352;&#26631;&#30340;&#21463;&#38480;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
KeyCLD: Learning Constrained Lagrangian Dynamics in Keypoint Coordinates from Images. (arXiv:2206.11030v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11030
&lt;/p&gt;
&lt;p&gt;
KeyCLD&#26159;&#19968;&#20010;&#20174;&#22270;&#20687;&#20013;&#23398;&#20064;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#28857;&#22352;&#26631;&#34920;&#31034;&#29366;&#24577;&#21160;&#21147;&#23398;&#65292;&#24182;&#32467;&#21512;&#26174;&#24335;&#30340;&#23436;&#25972;&#32422;&#26463;&#26469;&#34920;&#31034;&#21160;&#21147;&#23398;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#26080;&#30417;&#30563;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20934;&#30830;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#20102;&#38271;&#26399;&#35270;&#39057;&#39044;&#27979;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#30740;&#31350;&#20102;&#25289;&#26684;&#26391;&#26085;&#20808;&#39564;&#21644;&#32422;&#26463;&#20989;&#25968;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;KeyCLD&#65292;&#19968;&#20010;&#20174;&#22270;&#20687;&#20013;&#23398;&#20064;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#30340;&#26694;&#26550;&#12290;&#23398;&#20064;&#21040;&#30340;&#20851;&#38190;&#28857;&#22312;&#22270;&#20687;&#20013;&#20195;&#34920;&#35821;&#20041;&#22320;&#26631;&#65292;&#21487;&#20197;&#30452;&#25509;&#34920;&#31034;&#29366;&#24577;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;&#36825;&#20010;&#29366;&#24577;&#35299;&#37322;&#20026;&#31515;&#21345;&#23572;&#22352;&#26631;&#65292;&#32467;&#21512;&#26174;&#24335;&#30340;&#23436;&#25972;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#21463;&#38480;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#26469;&#34920;&#31034;&#21160;&#21147;&#23398;&#12290;KeyCLD&#22312;&#22270;&#20687;&#24207;&#21015;&#19978;&#36827;&#34892;&#26080;&#30417;&#30563;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#36136;&#37327;&#30697;&#38453;&#12289;&#21183;&#33021;&#21644;&#36755;&#20837;&#30697;&#38453;&#65292;&#20174;&#32780;&#23454;&#29616;&#22522;&#20110;&#33021;&#37327;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#22312;dm_control&#25670;&#12289;&#20498;&#31435;&#25670;&#21644;&#20498;&#31435;&#25670;&#26426;&#22120;&#20154;&#29615;&#22659;&#19978;&#23637;&#31034;&#20102;&#20174;&#22270;&#20687;&#20013;&#23398;&#20064;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#30340;&#33021;&#21147;&#12290;KeyCLD&#21487;&#20197;&#22312;&#36825;&#20123;&#31995;&#32479;&#19978;&#36827;&#34892;&#23398;&#20064;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#26080;&#39537;&#21160;&#12289;&#20122;&#39537;&#21160;&#36824;&#26159;&#20840;&#39537;&#21160;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38271;&#26399;&#35270;&#39057;&#39044;&#27979;&#65292;&#26174;&#31034;&#20854;&#20934;&#30830;&#23398;&#20064;&#20102;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#19982;Lag-VAE&#12289;Lag-caVAE&#21644;HGN&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#30740;&#31350;&#20102;&#25289;&#26684;&#26391;&#26085;&#20808;&#39564;&#21644;&#32422;&#26463;&#20989;&#25968;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present KeyCLD, a framework to learn Lagrangian dynamics from images. Learned keypoints represent semantic landmarks in images and can directly represent state dynamics. We show that interpreting this state as Cartesian coordinates, coupled with explicit holonomic constraints, allows expressing the dynamics with a constrained Lagrangian. KeyCLD is trained unsupervised end-to-end on sequences of images. Our method explicitly models the mass matrix, potential energy and the input matrix, thus allowing energy based control. We demonstrate learning of Lagrangian dynamics from images on the dm_control pendulum, cartpole and acrobot environments. KeyCLD can be learned on these systems, whether they are unactuated, underactuated or fully actuated. Trained models are able to produce long-term video predictions, showing that the dynamics are accurately learned. We compare with Lag-VAE, Lag-caVAE and HGN, and investigate the benefit of the Lagrangian prior and the constraint function. KeyCLD 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#20998;&#31163;Transformer&#65288;RE-SepFormer&#65289;&#65292;&#36890;&#36807;&#38750;&#37325;&#21472;&#30340;&#28508;&#31354;&#38388;&#22359;&#21644;&#32039;&#20945;&#30340;&#28508;&#21464;&#37327;&#25688;&#35201;&#25805;&#20316;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#12290;&#22312;&#35821;&#38899;&#20998;&#31163;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#36739;&#22909;&#30340;&#20869;&#23384;&#21644;&#25512;&#26029;&#26102;&#38388;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.09507</link><description>&lt;p&gt;
&#36164;&#28304;&#39640;&#25928;&#30340;&#20998;&#31163;Transformer
&lt;/p&gt;
&lt;p&gt;
Resource-Efficient Separation Transformer. (arXiv:2206.09507v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#20998;&#31163;Transformer&#65288;RE-SepFormer&#65289;&#65292;&#36890;&#36807;&#38750;&#37325;&#21472;&#30340;&#28508;&#31354;&#38388;&#22359;&#21644;&#32039;&#20945;&#30340;&#28508;&#21464;&#37327;&#25688;&#35201;&#25805;&#20316;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#12290;&#22312;&#35821;&#38899;&#20998;&#31163;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#36739;&#22909;&#30340;&#20869;&#23384;&#21644;&#25512;&#26029;&#26102;&#38388;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;Transformer&#22312;&#35821;&#38899;&#20998;&#31163;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#39640;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#20998;&#31163;Transformer&#65288;RE-SepFormer&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#12290;&#39318;&#20808;&#65292;&#23427;&#22312;&#28508;&#31354;&#38388;&#20013;&#20351;&#29992;&#38750;&#37325;&#21472;&#30340;&#22359;&#12290;&#20854;&#27425;&#65292;&#23427;&#23545;&#27599;&#20010;&#22359;&#35745;&#31639;&#30340;&#32039;&#20945;&#28508;&#21464;&#37327;&#25688;&#35201;&#36827;&#34892;&#25805;&#20316;&#12290;RE-SepFormer&#22312;&#27969;&#34892;&#30340;WSJ0-2Mix&#21644;WHAM&#65281;&#25968;&#25454;&#38598;&#19978;&#30340;&#22240;&#26524;&#21644;&#38750;&#22240;&#26524;&#35774;&#32622;&#19979;&#22343;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#22312;&#20869;&#23384;&#21644;&#25512;&#26029;&#26102;&#38388;&#26041;&#38754;&#27604;&#20808;&#21069;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#26377;&#26174;&#33879;&#30340;&#25193;&#23637;&#24615;&#65292;&#26356;&#36866;&#29992;&#20110;&#22788;&#29702;&#38271;&#30340;&#28151;&#21512;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have recently achieved state-of-the-art performance in speech separation. These models, however, are computationally demanding and require a lot of learnable parameters. This paper explores Transformer-based speech separation with a reduced computational cost. Our main contribution is the development of the Resource-Efficient Separation Transformer (RE-SepFormer), a self-attention-based architecture that reduces the computational burden in two ways. First, it uses non-overlapping blocks in the latent space. Second, it operates on compact latent summaries calculated from each chunk. The RE-SepFormer reaches a competitive performance on the popular WSJ0-2Mix and WHAM! datasets in both causal and non-causal settings. Remarkably, it scales significantly better than the previous Transformer-based architectures in terms of memory and inference time, making it more suitable for processing long mixtures.
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#23545;&#24352;&#37327;&#22238;&#24402;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40654;&#26364;&#20248;&#21270;&#26041;&#27861;&#21644;&#31209;&#36807;&#21442;&#25968;&#21270;&#30340;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#40654;&#26364;&#20248;&#21270;&#26041;&#27861;&#30340;&#32447;&#24615;&#21644;&#20108;&#27425;&#25910;&#25947;&#24615;&#20197;&#21450;&#36866;&#24212;&#36807;&#21442;&#25968;&#21270;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26631;&#37327;&#23545;&#24352;&#37327;&#22238;&#24402;&#20013;&#30340;&#32479;&#35745;&#35745;&#31639;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2206.08756</link><description>&lt;p&gt;
&#24352;&#37327;&#23545;&#24352;&#37327;&#22238;&#24402;: &#40654;&#26364;&#20248;&#21270;&#65292;&#36807;&#21442;&#25968;&#21270;&#65292;&#32479;&#35745;&#35745;&#31639;&#24046;&#24322;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tensor-on-Tensor Regression: Riemannian Optimization, Over-parameterization, Statistical-computational Gap, and Their Interplay. (arXiv:2206.08756v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08756
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#23545;&#24352;&#37327;&#22238;&#24402;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40654;&#26364;&#20248;&#21270;&#26041;&#27861;&#21644;&#31209;&#36807;&#21442;&#25968;&#21270;&#30340;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#40654;&#26364;&#20248;&#21270;&#26041;&#27861;&#30340;&#32447;&#24615;&#21644;&#20108;&#27425;&#25910;&#25947;&#24615;&#20197;&#21450;&#36866;&#24212;&#36807;&#21442;&#25968;&#21270;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26631;&#37327;&#23545;&#24352;&#37327;&#22238;&#24402;&#20013;&#30340;&#32479;&#35745;&#35745;&#31639;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24352;&#37327;&#23545;&#24352;&#37327;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#19981;&#30693;&#36947;&#20854;&#20869;&#22312;&#31209;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24352;&#37327;&#21709;&#24212;&#19982;&#24352;&#37327;&#21327;&#21464;&#37327;&#30456;&#36830;&#25509;&#65292;&#36890;&#36807;&#20302;Tucker&#31209;&#21442;&#25968;&#24352;&#37327;/&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#65288;RGD&#65289;&#21644;&#40654;&#26364;&#39640;&#26031;&#29275;&#39039;&#65288;RGN&#65289;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#31209;&#36807;&#21442;&#25968;&#21270;&#30340;&#24433;&#21709;&#26469;&#24212;&#23545;&#26410;&#30693;&#31209;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#20851;&#20110;&#19968;&#33324;&#24352;&#37327;&#23545;&#24352;&#37327;&#22238;&#24402;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#34920;&#26126;RGD&#21644;RGN&#20998;&#21035;&#22312;&#27491;&#30830;&#21442;&#25968;&#21270;&#21644;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#19979;&#32447;&#24615;&#21644;&#20108;&#27425;&#25910;&#25947;&#21040;&#32479;&#35745;&#19978;&#30340;&#26368;&#20248;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65306;&#40654;&#26364;&#20248;&#21270;&#26041;&#27861;&#22312;&#19981;&#20462;&#25913;&#23454;&#29616;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#33258;&#28982;&#22320;&#36866;&#24212;&#36807;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#30452;&#25509;&#30340;&#20302;&#27425;&#22810;&#39033;&#24335;&#35770;&#35777;&#35777;&#26126;&#20102;&#26631;&#37327;&#23545;&#24352;&#37327;&#22238;&#24402;&#20013;&#30340;&#32479;&#35745;&#35745;&#31639;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#35777;&#26126;&#20102;&#32479;&#35745;&#23398;&#30340;&#31119;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the tensor-on-tensor regression, where the goal is to connect tensor responses to tensor covariates with a low Tucker rank parameter tensor/matrix without the prior knowledge of its intrinsic rank. We propose the Riemannian gradient descent (RGD) and Riemannian Gauss-Newton (RGN) methods and cope with the challenge of unknown rank by studying the effect of rank over-parameterization. We provide the first convergence guarantee for the general tensor-on-tensor regression by showing that RGD and RGN respectively converge linearly and quadratically to a statistically optimal estimate in both rank correctly-parameterized and over-parameterized settings. Our theory reveals an intriguing phenomenon: Riemannian optimization methods naturally adapt to over-parameterization without modifications to their implementation. We also prove the statistical-computational gap in scalar-on-tensor regression by a direct low-degree polynomial argument. Our theory demonstrates a "blessing of statist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#39044;&#26465;&#20214;&#26356;&#26032;&#30340;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#65292;&#21253;&#25324;&#20102;&#22522;&#20110;Hutchinson&#26041;&#27861;&#30340;&#39044;&#26465;&#20214;&#22120;&#21644;&#20960;&#31181;&#26799;&#24230;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#23610;&#24230;&#19981;&#22909;&#21644;&#26465;&#20214;&#19981;&#33391;&#38382;&#39064;&#65292;&#24182;&#22312;&#20809;&#28369;&#24615;&#21644;PL&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.00285</link><description>&lt;p&gt;
&#20855;&#26377;&#39044;&#26465;&#20214;&#26356;&#26032;&#30340;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Methods with Preconditioned Updates. (arXiv:2206.00285v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#39044;&#26465;&#20214;&#26356;&#26032;&#30340;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#65292;&#21253;&#25324;&#20102;&#22522;&#20110;Hutchinson&#26041;&#27861;&#30340;&#39044;&#26465;&#20214;&#22120;&#21644;&#20960;&#31181;&#26799;&#24230;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#23610;&#24230;&#19981;&#22909;&#21644;&#26465;&#20214;&#19981;&#33391;&#38382;&#39064;&#65292;&#24182;&#22312;&#20809;&#28369;&#24615;&#21644;PL&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#38750;&#20984;&#26377;&#38480;&#27714;&#21644;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#30446;&#21069;&#24050;&#26377;&#19968;&#20123;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#65292;&#20294;&#24403;&#38382;&#39064;&#30340;&#23610;&#24230;&#21644;/&#25110;&#26465;&#20214;&#19981;&#33391;&#26102;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24037;&#20316;&#25928;&#26524;&#36739;&#24046;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24341;&#20837;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;Hutchinson&#26041;&#27861;&#36817;&#20284;Hessian&#23545;&#35282;&#20803;&#32032;&#30340;&#39044;&#26465;&#20214;&#22120;&#65292;&#24182;&#23558;&#20854;&#19982;&#20960;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#23610;&#24230;&#31639;&#27861;&#65306;Scaled SARAH&#21644;Scaled L-SVRG&#12290;&#22312;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#32473;&#20986;&#20102;&#29702;&#35770;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;&#24403;&#21516;&#26102;&#20551;&#35774;&#20809;&#28369;&#24615;&#21644;PL&#26465;&#20214;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#23610;&#24230;&#26041;&#27861;&#20351;&#29992;&#20102;&#36817;&#20284;&#30340;&#20108;&#38454;&#26354;&#29575;&#20449;&#24687;&#65292;&#22240;&#27492;&#21487;&#20197;&#26356;&#22909;&#22320;&#20943;&#36731;&#23610;&#24230;&#19981;&#22909;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#25913;&#36827;&#30340;&#23454;&#38469;&#24615;&#33021;&#22312;&#26412;&#25991;&#20013;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#20063;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work considers the non-convex finite sum minimization problem. There are several algorithms for such problems, but existing methods often work poorly when the problem is badly scaled and/or ill-conditioned, and a primary goal of this work is to introduce methods that alleviate this issue. Thus, here we include a preconditioner based on Hutchinson's approach to approximating the diagonal of the Hessian, and couple it with several gradient-based methods to give new scaled algorithms: Scaled SARAH and Scaled L-SVRG. Theoretical complexity guarantees under smoothness assumptions are presented. We prove linear convergence when both smoothness and the PL condition are assumed. Our adaptively scaled methods use approximate partial second-order curvature information and, therefore, can better mitigate the impact of badly scaled problems. This improved practical performance is demonstrated in the numerical experiments also presented in this work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;leave-one-out&#22855;&#24322;&#23376;&#31354;&#38388;&#25200;&#21160;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25200;&#21160;&#19978;&#30028;&#26469;&#27979;&#37327;&#20004;&#20010;&#30697;&#38453;&#30340;&#22855;&#24322;&#23376;&#31354;&#38388;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#28151;&#21512;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#25200;&#21160;&#30028;&#38480;&#20855;&#26377;&#26356;&#23574;&#38160;&#21644;&#31934;&#32454;&#30340;&#32479;&#35745;&#20998;&#26512;&#33021;&#21147;&#12290;&#22312;&#20122;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#27492;&#26041;&#27861;&#36827;&#34892;&#30340;&#35889;&#32858;&#31867;&#20855;&#26377;&#26174;&#24335;&#30340;&#25351;&#25968;&#35823;&#24046;&#29575;&#65292;&#19988;&#22312;&#36739;&#24369;&#30340;&#20449;&#22122;&#27604;&#26465;&#20214;&#19979;&#20248;&#20110;&#20043;&#21069;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.14855</link><description>&lt;p&gt;
leave-one-out&#22855;&#24322;&#23376;&#31354;&#38388;&#25200;&#21160;&#20998;&#26512;&#29992;&#20110;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Leave-one-out Singular Subspace Perturbation Analysis for Spectral Clustering. (arXiv:2205.14855v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14855
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;leave-one-out&#22855;&#24322;&#23376;&#31354;&#38388;&#25200;&#21160;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25200;&#21160;&#19978;&#30028;&#26469;&#27979;&#37327;&#20004;&#20010;&#30697;&#38453;&#30340;&#22855;&#24322;&#23376;&#31354;&#38388;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#28151;&#21512;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#25200;&#21160;&#30028;&#38480;&#20855;&#26377;&#26356;&#23574;&#38160;&#21644;&#31934;&#32454;&#30340;&#32479;&#35745;&#20998;&#26512;&#33021;&#21147;&#12290;&#22312;&#20122;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#27492;&#26041;&#27861;&#36827;&#34892;&#30340;&#35889;&#32858;&#31867;&#20855;&#26377;&#26174;&#24335;&#30340;&#25351;&#25968;&#35823;&#24046;&#29575;&#65292;&#19988;&#22312;&#36739;&#24369;&#30340;&#20449;&#22122;&#27604;&#26465;&#20214;&#19979;&#20248;&#20110;&#20043;&#21069;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22855;&#24322;&#23376;&#31354;&#38388;&#25200;&#21160;&#29702;&#35770;&#22312;&#27010;&#29575;&#19982;&#32479;&#35745;&#23398;&#20013;&#20855;&#26377;&#22522;&#30784;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#26377;&#30528;&#21508;&#31181;&#24212;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#20219;&#24847;&#30340;&#30697;&#38453;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#21478;&#19968;&#20010;&#30697;&#38453;&#30340;leave-one-column-out&#23376;&#30697;&#38453;&#65292;&#24182;&#20026;&#20004;&#20010;&#30456;&#24212;&#30340;&#22855;&#24322;&#23376;&#31354;&#38388;&#20043;&#38388;&#30340;&#36317;&#31163;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#25200;&#21160;&#19978;&#30028;&#12290;&#35813;&#26041;&#27861;&#38750;&#24120;&#36866;&#29992;&#20110;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#20135;&#29983;&#27604;&#32463;&#20856;&#25200;&#21160;&#30028;&#38480;&#65288;&#22914;Wedin&#23450;&#29702;&#65289;&#26356;&#23574;&#38160;&#21644;&#31934;&#32454;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#22522;&#20110;&#36825;&#31181;leave-one-out&#25200;&#21160;&#29702;&#35770;&#65292;&#25105;&#20204;&#23545;&#28151;&#21512;&#27169;&#22411;&#19979;&#35889;&#32858;&#31867;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#30830;&#23450;&#24615;&#30340;&#36880;&#20803;&#32032;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#20122;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#35889;&#32858;&#31867;&#30340;&#25351;&#25968;&#35823;&#24046;&#29575;&#12290;&#23545;&#20110;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35813;&#36895;&#29575;&#22312;&#36739;&#24369;&#30340;&#20449;&#22122;&#27604;&#26465;&#20214;&#19979;&#20248;&#20110;L{\"o}ffler&#31561;&#20154;&#30340;&#36895;&#29575;&#65288;2021&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The singular subspaces perturbation theory is of fundamental importance in probability and statistics. It has various applications across different fields. We consider two arbitrary matrices where one is a leave-one-column-out submatrix of the other one and establish a novel perturbation upper bound for the distance between the two corresponding singular subspaces. It is well-suited for mixture models and results in a sharper and finer statistical analysis than classical perturbation bounds such as Wedin's Theorem. Empowered by this leave-one-out perturbation theory, we provide a deterministic entrywise analysis for the performance of spectral clustering under mixture models. Our analysis leads to an explicit exponential error rate for spectral clustering of sub-Gaussian mixture models. For the mixture of isotropic Gaussians, the rate is optimal under a weaker signal-to-noise condition than that of L{\"o}ffler et al. (2021).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24102;&#26377;&#19978;&#19979;&#25991;&#30340;&#22312;&#32447;&#24773;&#22659;&#19979;&#30340;&#28504;&#22810;&#25289;&#20043;&#30418;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#36951;&#25022;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#23436;&#20840;&#20102;&#35299;&#20808;&#21069;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#26368;&#20248;&#31639;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.13114</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#24773;&#22659;&#19979;&#30340;&#28504;&#22810;&#25289;&#20043;&#30418;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Contextual Pandora's Box. (arXiv:2205.13114v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24102;&#26377;&#19978;&#19979;&#25991;&#30340;&#22312;&#32447;&#24773;&#22659;&#19979;&#30340;&#28504;&#22810;&#25289;&#20043;&#30418;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#36951;&#25022;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#23436;&#20840;&#20102;&#35299;&#20808;&#21069;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#26368;&#20248;&#31639;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28504;&#22810;&#25289;&#20043;&#30418;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20915;&#31574;&#32773;&#38656;&#35201;&#25214;&#21040;&#19968;&#20010;&#22909;&#30340;&#36873;&#25321;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#25506;&#32034;&#27599;&#20010;&#36873;&#25321;&#20215;&#20540;&#30340;&#25628;&#32034;&#25104;&#26412;&#12290;&#22312;&#26368;&#21021;&#30340;&#23450;&#20041;&#20013;&#65292;&#20551;&#23450;&#25152;&#26377;&#36873;&#25321;&#30340;&#20215;&#20540;&#30340;&#20934;&#30830;&#20998;&#24067;&#24050;&#30693;&#65292;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#21017;&#30740;&#31350;&#20102;&#28504;&#22810;&#25289;&#20043;&#30418;&#30340;&#22312;&#32447;&#21464;&#20307;&#65292;&#20854;&#20013;&#26368;&#21021;&#26410;&#30693;&#36873;&#25321;&#30340;&#20998;&#24067;&#24773;&#20917;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#19978;&#19979;&#25991;&#30340;&#22312;&#32447;&#24773;&#22659;&#19979;&#30340;&#28504;&#22810;&#25289;&#20043;&#30418;&#38382;&#39064;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#25105;&#20204;&#20250;&#23637;&#31034;&#19968;&#20123;&#20855;&#26377;&#19978;&#19979;&#25991;&#12289;&#25506;&#32034;&#25104;&#26412;&#21644;&#26410;&#30693;&#20215;&#20540;&#30340;&#36873;&#25321;&#65292;&#36825;&#20123;&#26410;&#30693;&#20215;&#20540;&#26159;&#20174;&#21487;&#33021;&#22312;&#27599;&#19968;&#36718;&#21457;&#29983;&#21464;&#21270;&#30340;&#26410;&#30693;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#31181;&#26080;&#36951;&#25022;&#31639;&#27861;&#65292;&#20854;&#24615;&#33021;&#19982;&#23436;&#20840;&#20102;&#35299;&#25152;&#26377;&#20808;&#21069;&#20998;&#24067;&#30340;&#26368;&#20248;&#31639;&#27861;&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#29978;&#33267;&#36866;&#29992;&#20110;&#21482;&#26377;&#37096;&#20998;&#36873;&#25321;&#26410;&#34987;&#25506;&#32034;&#30340;&#25176;&#31649;&#35774;&#32622;&#12290;&#20851;&#38190;&#30340;&#25216;&#26415;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Pandora's Box is a fundamental stochastic optimization problem, where the decision-maker must find a good alternative while minimizing the search cost of exploring the value of each alternative. In the original formulation, it is assumed that accurate distributions are given for the values of all the alternatives, while recent work studies the online variant of Pandora's Box where the distributions are originally unknown. In this work, we study Pandora's Box in the online setting, while incorporating context. At every round, we are presented with a number of alternatives each having a context, an exploration cost and an unknown value drawn from an unknown distribution that may change at every round. Our main result is a no-regret algorithm that performs comparably well to the optimal algorithm which knows all prior distributions exactly. Our algorithm works even in the bandit setting where the algorithm never learns the values of the alternatives that were not explored. The key techniq
&lt;/p&gt;</description></item><item><title>AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.</title><link>http://arxiv.org/abs/2205.12787</link><description>&lt;p&gt;
&#20844;&#27491;&#28216;&#25103;&#65306;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Impartial Games: A Challenge for Reinforcement Learning. (arXiv:2205.12787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12787
&lt;/p&gt;
&lt;p&gt;
AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;AlphaZero&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#26827;&#30424;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20844;&#27491;&#28216;&#25103;&#20013;&#21364;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#20123;&#28216;&#25103;&#20013;&#29609;&#23478;&#20849;&#20139;&#26827;&#23376;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#28216;&#25103;&#20363;&#23376;&#65292;&#21363;&#23567;&#23401;&#20204;&#29609;&#30340;&#23612;&#22982;&#28216;&#25103;&#65292;&#20197;&#21450;&#20854;&#20182;&#19968;&#20123;&#20844;&#27491;&#28216;&#25103;&#65292;&#36825;&#20123;&#28216;&#25103;&#20284;&#20046;&#25104;&#20026;AlphaZero&#21644;&#31867;&#20284;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32458;&#33050;&#30707;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#19968;&#33268;&#65292;&#34920;&#26126;AlphaZero-style&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#21644;&#25932;&#23545;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#26174;&#31034;&#20102;&#22312;&#25152;&#26377;&#21512;&#27861;&#29366;&#24577;&#19979;&#23398;&#20064;&#25484;&#25569;&#36825;&#20123;&#28216;&#25103;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#21457;&#29616;&#23612;&#22982;&#28216;&#25103;&#22312;&#23567;&#22411;&#26827;&#30424;&#19978;&#21487;&#20197;&#23398;&#20064;&#65292;&#20294;&#24403;&#26827;&#30424;&#23610;&#23544;&#22686;&#22823;&#26102;&#65292;AlphaZero-style&#31639;&#27861;&#30340;&#23398;&#20064;&#36895;&#24230;&#26174;&#33879;&#20943;&#24930;&#12290;&#30452;&#35266;&#19978;&#65292;&#23612;&#22982;&#31561;&#20844;&#27491;&#28216;&#25103;&#19982;&#35937;&#26827;&#21644;&#22260;&#26827;&#31561;&#20826;&#27966;&#28216;&#25103;&#20043;&#38388;&#30340;&#21306;&#21035;&#22312;&#20110;&#65292;&#22914;&#26524;&#31995;&#32479;&#20013;&#28155;&#21152;&#20102;&#24494;&#23567;&#30340;&#22122;&#38899;&#65288;&#20363;&#22914;&#65292;&#26827;&#30424;&#30340;&#19968;&#23567;&#37096;&#20998;&#34987;&#35206;&#30422;&#65289;&#65292;&#23545;&#20110;&#20844;&#27491;&#28216;&#25103;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
AlphaZero-style reinforcement learning (RL) algorithms excel in various board games but face challenges with impartial games, where players share pieces. We present a concrete example of a game - namely the children's game of nim - and other impartial games that seem to be a stumbling block for AlphaZero-style and similar reinforcement learning algorithms.  Our findings are consistent with recent studies showing that AlphaZero-style algorithms are vulnerable to adversarial attacks and adversarial perturbations, showing the difficulty of learning to master the games in all legal states.  We show that nim can be learned on small boards, but AlphaZero-style algorithms learning dramatically slows down when the board size increases. Intuitively, the difference between impartial games like nim and partisan games like Chess and Go can be explained by the fact that if a tiny amount of noise is added to the system (e.g. if a small part of the board is covered), for impartial games, it is typica
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#30340;&#22810;&#20998;&#36776;&#29575;&#20559;&#24494;&#20998;&#26041;&#31243;&#20445;&#30041;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#31163;&#25955;&#21270;&#30340;&#25511;&#21046;&#26041;&#31243;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#38271;&#26399;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.03990</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#20559;&#24494;&#20998;&#26041;&#31243;&#20445;&#30041;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#26102;&#31354;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Multi-resolution partial differential equations preserved learning framework for spatiotemporal dynamics. (arXiv:2205.03990v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#30340;&#22810;&#20998;&#36776;&#29575;&#20559;&#24494;&#20998;&#26041;&#31243;&#20445;&#30041;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#31163;&#25955;&#21270;&#30340;&#25511;&#21046;&#26041;&#31243;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#38271;&#26399;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22797;&#26434;&#29289;&#29702;&#36807;&#31243;&#20013;&#24448;&#24448;&#38754;&#20020;&#39640;&#35757;&#32451;&#25104;&#26412;&#12289;&#35823;&#24046;&#31215;&#32047;&#21644;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;&#29289;&#29702;&#21551;&#21457;&#24335;&#28145;&#24230;&#23398;&#20064;&#65288;PiDL&#65289;&#36890;&#36807;&#23558;&#29289;&#29702;&#21407;&#29702;&#34701;&#20837;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;PiDL&#26041;&#27861;&#36890;&#36807;&#23558;&#25511;&#21046;&#26041;&#31243;&#23884;&#20837;&#25439;&#22833;&#20989;&#25968;&#26469;&#36827;&#34892;&#35757;&#32451;&#30340;&#35268;&#33539;&#21270;&#65292;&#20294;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26469;&#26435;&#34913;&#27599;&#20010;&#25439;&#22833;&#39033;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#65292;&#36890;&#36807;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#31639;&#23376;&#21644;&#32593;&#32476;&#32467;&#26500;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#23558;&#31163;&#25955;&#21270;&#30340;&#25511;&#21046;&#26041;&#31243;&#8220;&#28888;&#28900;&#8221;&#21040;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#24418;&#25104;&#19968;&#20010;&#20445;&#30041;PDE&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PPNN&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#22810;&#20998;&#36776;&#29575;&#29615;&#22659;&#20013;&#23558;&#31163;&#25955;&#21270;&#30340;PDE&#23884;&#20837;&#21367;&#31215;&#27531;&#24046;&#32593;&#32476;&#20013;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#38271;&#26399;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#40657;&#30418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional data-driven deep learning models often struggle with high training costs, error accumulation, and poor generalizability in complex physical processes. Physics-informed deep learning (PiDL) addresses these challenges by incorporating physical principles into the model. Most PiDL approaches regularize training by embedding governing equations into the loss function, yet this depends heavily on extensive hyperparameter tuning to weigh each loss term. To this end, we propose to leverage physics prior knowledge by ``baking'' the discretized governing equations into the neural network architecture via the connection between the partial differential equations (PDE) operators and network structures, resulting in a PDE-preserved neural network (PPNN). This method, embedding discretized PDEs through convolutional residual networks in a multi-resolution setting, largely improves the generalizability and long-term prediction accuracy, outperforming conventional black-box models. The ef
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#28608;&#21169;&#35282;&#24230;&#29702;&#35299;CNNs&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#27491;&#36127;&#28608;&#21169;&#24182;&#24341;&#20837;&#21452;&#38142;&#21453;&#21521;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26080;&#26799;&#24230;&#30340;&#36880;&#23618;&#20449;&#24687;&#21033;&#29992;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35299;&#37322;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.00932</link><description>&lt;p&gt;
&#20174;&#28608;&#21169;&#35282;&#24230;&#29702;&#35299;CNNs
&lt;/p&gt;
&lt;p&gt;
Understanding CNNs from excitations. (arXiv:2205.00932v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00932
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#28608;&#21169;&#35282;&#24230;&#29702;&#35299;CNNs&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#27491;&#36127;&#28608;&#21169;&#24182;&#24341;&#20837;&#21452;&#38142;&#21453;&#21521;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26080;&#26799;&#24230;&#30340;&#36880;&#23618;&#20449;&#24687;&#21033;&#29992;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35299;&#37322;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#24615;&#22270;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26799;&#24230;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#35299;&#37322;&#22797;&#26434;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21033;&#29992;&#36127;&#26799;&#24230;&#20449;&#24687;&#26469;&#25552;&#39640;&#35299;&#37322;&#20934;&#30830;&#24615;&#26041;&#38754;&#24182;&#19981;&#23436;&#20840;&#29087;&#32451;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#65292;&#31216;&#20043;&#20026;&#27491;&#36127;&#28608;&#21169;&#65292;&#23427;&#33021;&#22815;&#30452;&#25509;&#25552;&#21462;&#27599;&#20010;&#23618;&#30340;&#27491;&#36127;&#28608;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#23436;&#20840;&#26080;&#26799;&#24230;&#30340;&#36880;&#23618;&#20449;&#24687;&#21033;&#29992;&#12290;&#20026;&#20102;&#23558;&#36825;&#20123;&#28608;&#21169;&#32452;&#32455;&#25104;&#26368;&#32456;&#30340;&#26174;&#33879;&#24615;&#22270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21452;&#38142;&#21453;&#21521;&#20256;&#25773;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20108;&#20998;&#31867;&#21644;&#22810;&#20998;&#31867;&#20219;&#21153;&#30340;&#32508;&#21512;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20196;&#20154;&#40723;&#33310;&#30340;&#26159;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#37322;&#20934;&#30830;&#24615;&#26041;&#38754;&#26377;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Saliency maps have proven to be a highly efficacious approach for explicating the decisions of Convolutional Neural Networks. However, extant methodologies predominantly rely on gradients, which constrain their ability to explicate complex models. Furthermore, such approaches are not fully adept at leveraging negative gradient information to improve interpretive veracity. In this study, we present a novel concept, termed positive and negative excitation, which enables the direct extraction of positive and negative excitation for each layer, thus enabling complete layer-by-layer information utilization sans gradients. To organize these excitations into final saliency maps, we introduce a double-chain backpropagation procedure. A comprehensive experimental evaluation, encompassing both binary classification and multi-classification tasks, was conducted to gauge the effectiveness of our proposed method. Encouragingly, the results evince that our approach offers a significant improvement o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#28304;&#22495;&#36866;&#24212;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#26679;&#26412;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#23581;&#35797;&#22312;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#26377;&#29992;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#38754;&#20020;&#39044;&#35757;&#32451;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20851;&#32852;&#24615;&#19981;&#24378;&#20197;&#21450;&#20449;&#24687;&#20132;&#25442;&#26080;&#25928;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2204.05104</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22810;&#28304;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Graph Neural Network for Multi-Source Domain Adaptation. (arXiv:2204.05104v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#28304;&#22495;&#36866;&#24212;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#26679;&#26412;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#23581;&#35797;&#22312;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#26377;&#29992;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#38754;&#20020;&#39044;&#35757;&#32451;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20851;&#32852;&#24615;&#19981;&#24378;&#20197;&#21450;&#20449;&#24687;&#20132;&#25442;&#26080;&#25928;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#36866;&#24212;&#35797;&#22270;&#35299;&#20915;&#27979;&#35797;&#25968;&#25454;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#23436;&#20840;&#36981;&#24490;&#30456;&#21516;&#20998;&#24067;&#30340;&#24773;&#20917;&#65292;&#32780;&#22810;&#28304;&#22495;&#36866;&#24212;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#26377;&#21560;&#24341;&#21147;&#12290;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#29616;&#22312;&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26032;&#36235;&#21183;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22810;&#28304;&#22495;&#36866;&#24212;&#26377;&#19968;&#20010;&#30456;&#20284;&#30340;&#30446;&#26631;&#65306;&#23427;&#20204;&#37117;&#26088;&#22312;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#23398;&#20064;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#34920;&#31034;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20256;&#32479;&#30340;&#22810;&#20219;&#21153;&#33258;&#30417;&#30563;&#23398;&#20064;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#39044;&#35757;&#32451;&#20219;&#21153;&#21487;&#33021;&#19982;&#19979;&#28216;&#20219;&#21153;&#20851;&#32852;&#19981;&#24378;&#65292;&#22240;&#27492;&#24456;&#38590;&#20174;&#39044;&#35757;&#32451;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#19982;&#30446;&#26631;&#20219;&#21153;&#20849;&#20139;&#30340;&#26377;&#29992;&#30693;&#35782;&#65307;&#65288;2&#65289;&#24403;&#30456;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#22312;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#65292;&#24182;&#19988;&#21482;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#27979;&#22836;&#26102;&#65292;&#20351;&#20219;&#21153;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#21464;&#24471;&#26080;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation (DA) tries to tackle the scenarios when the test data does not fully follow the same distribution of the training data, and multi-source domain adaptation (MSDA) is very attractive for real world applications. By learning from large-scale unlabeled samples, self-supervised learning has now become a new trend in deep learning. It is worth noting that both self-supervised learning and multi-source domain adaptation share a similar goal: they both aim to leverage unlabeled data to learn more expressive representations. Unfortunately, traditional multi-task self-supervised learning faces two challenges: (1) the pretext task may not strongly relate to the downstream task, thus it could be difficult to learn useful knowledge being shared from the pretext task to the target task; (2) when the same feature extractor is shared between the pretext task and the downstream one and only different prediction heads are used, it is ineffective to enable inter-task information exchang
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#23545;&#20110;&#20855;&#26377;&#36890;&#20449;&#25110;&#38544;&#31169;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#19982;&#33410;&#28857;&#25968;&#37327;&#25104;&#21453;&#27604;&#30340;&#25913;&#36827;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2202.02423</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Improved Information Theoretic Generalization Bounds for Distributed and Federated Learning. (arXiv:2202.02423v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02423
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#23545;&#20110;&#20855;&#26377;&#36890;&#20449;&#25110;&#38544;&#31169;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#19982;&#33410;&#28857;&#25968;&#37327;&#25104;&#21453;&#27604;&#30340;&#25913;&#36827;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#32593;&#32476;&#29615;&#22659;&#19979;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#30340;&#20449;&#24687;&#35770;&#30028;&#38480;&#65292;&#35813;&#29615;&#22659;&#20013;&#26377;K&#20010;&#33410;&#28857;&#65292;&#27599;&#20010;&#33410;&#28857;&#37117;&#26377;&#33258;&#24049;&#30340;&#29420;&#31435;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#27599;&#20010;&#33410;&#28857;&#30340;&#27169;&#22411;&#24517;&#39035;&#34987;&#32858;&#21512;&#25104;&#19968;&#20010;&#26368;&#32456;&#30340;&#38598;&#20013;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#31616;&#21333;&#30340;&#27169;&#22411;&#24179;&#22343;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#22810;&#36718;&#31639;&#27861;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31995;&#21015;&#38382;&#39064;&#30340;&#26399;&#26395;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#20363;&#22914;&#20855;&#26377;Bregman&#25955;&#24230;&#25110;Lipschitz&#36830;&#32493;&#25439;&#22833;&#30340;&#38382;&#39064;&#65292;&#22312;&#33410;&#28857;&#25968;&#37327;&#19978;&#23637;&#31034;&#20102;&#23545;1/K&#30340;&#25913;&#36827;&#20381;&#36182;&#12290;&#36825;&#20123;&#8220;&#27599;&#20010;&#33410;&#28857;&#8221;&#30340;&#30028;&#38480;&#26159;&#20851;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19982;&#27599;&#20010;&#33410;&#28857;&#35757;&#32451;&#26435;&#37325;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#22240;&#27492;&#23545;&#20110;&#25551;&#36848;&#22312;&#27599;&#20010;&#33410;&#28857;&#20855;&#26377;&#36890;&#20449;&#25110;&#38544;&#31169;&#32422;&#26463;&#30340;&#27867;&#21270;&#24615;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider information-theoretic bounds on expected generalization error for statistical learning problems in a networked setting. In this setting, there are $K$ nodes, each with its own independent dataset, and the models from each node have to be aggregated into a final centralized model. We consider both simple averaging of the models as well as more complicated multi-round algorithms. We give upper bounds on the expected generalization error for a variety of problems, such as those with Bregman divergence or Lipschitz continuous losses, that demonstrate an improved dependence of $1/K$ on the number of nodes. These "per node" bounds are in terms of the mutual information between the training dataset and the trained weights at each node, and are therefore useful in describing the generalization properties inherent to having communication or privacy constraints at each node.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#37325;&#25490;&#31574;&#30053;&#21644;&#32858;&#21512;&#38543;&#26426;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#23436;&#25972;&#26799;&#24230;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#38543;&#26426;&#36882;&#24402;&#26799;&#24230;&#31639;&#27861;&#65288;SARAH&#65289;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2111.13322</link><description>&lt;p&gt;
&#38543;&#26426;&#37325;&#25490;&#30340;SARAH&#31639;&#27861;&#19981;&#38656;&#35201;&#36827;&#34892;&#23436;&#25972;&#30340;&#26799;&#24230;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Random-reshuffled SARAH does not need a full gradient computations. (arXiv:2111.13322v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.13322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#37325;&#25490;&#31574;&#30053;&#21644;&#32858;&#21512;&#38543;&#26426;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#23436;&#25972;&#26799;&#24230;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#38543;&#26426;&#36882;&#24402;&#26799;&#24230;&#31639;&#27861;&#65288;SARAH&#65289;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36882;&#24402;&#26799;&#24230;&#31639;&#27861;&#65288;SARAH&#65289;&#26159;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#30340;&#19968;&#31181;&#26041;&#24046;&#32553;&#20943;&#21464;&#20307;&#65292;&#38656;&#35201;&#23450;&#26399;&#35745;&#31639;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28040;&#38500;&#20102;&#36827;&#34892;&#23436;&#25972;&#26799;&#24230;&#35745;&#31639;&#30340;&#24517;&#35201;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#37325;&#25490;&#31574;&#30053;&#21644;&#22312;&#27599;&#20010;&#26102;&#26399;&#33719;&#24471;&#30340;&#38543;&#26426;&#26799;&#24230;&#30340;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#32858;&#21512;&#30340;&#38543;&#26426;&#26799;&#24230;&#22312;SARAH&#31639;&#27861;&#20013;&#20316;&#20026;&#23436;&#25972;&#26799;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The StochAstic Recursive grAdient algoritHm (SARAH) algorithm is a variance reduced variant of the Stochastic Gradient Descent (SGD) algorithm that needs a gradient of the objective function from time to time. In this paper, we remove the necessity of a full gradient computation. This is achieved by using a randomized reshuffling strategy and aggregating stochastic gradients obtained in each epoch. The aggregated stochastic gradients serve as an estimate of a full gradient in the SARAH algorithm. We provide a theoretical analysis of the proposed approach and conclude the paper with numerical experiments that demonstrate the efficiency of this approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPViT&#30340;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;ViTs&#21387;&#32553;&#20026;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#26041;&#26696;&#23558;&#33258;&#27880;&#24847;&#21147;&#23618;&#21644;&#21367;&#31215;&#25805;&#20316;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#21644;&#24314;&#27169;&#26412;&#22320;&#35270;&#35273;&#27169;&#24335;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2111.11802</link><description>&lt;p&gt;
&#22312;&#21333;&#36335;&#24452;&#20013;&#23558;&#33258;&#27880;&#24847;&#21147;&#20462;&#21098;&#21040;&#21367;&#31215;&#23618;&#20013;
&lt;/p&gt;
&lt;p&gt;
Pruning Self-attentions into Convolutional Layers in Single Path. (arXiv:2111.11802v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.11802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPViT&#30340;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;ViTs&#21387;&#32553;&#20026;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#26041;&#26696;&#23558;&#33258;&#27880;&#24847;&#21147;&#23618;&#21644;&#21367;&#31215;&#25805;&#20316;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#21644;&#24314;&#27169;&#26412;&#22320;&#35270;&#35273;&#27169;&#24335;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22810;&#22836;&#33258;&#27880;&#24847;&#65288;MSA&#65289;&#23618;&#24314;&#27169;&#20840;&#23616;&#20851;&#32852;&#23384;&#22312;&#20004;&#20010;&#24191;&#20026;&#35748;&#21487;&#30340;&#38382;&#39064;&#65306;&#24040;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#21644;&#32570;&#20047;&#23545;&#24314;&#27169;&#26412;&#22320;&#35270;&#35273;&#27169;&#24335;&#30340;&#20869;&#22312;&#24402;&#32435;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21333;&#36335;&#24452;&#35270;&#35273;Transformer&#20462;&#21098;&#65288;SPViT&#65289;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;ViTs&#21387;&#32553;&#20026;&#20855;&#26377;&#36866;&#24403;&#23616;&#37096;&#24615;&#30340;&#32039;&#20945;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MSA&#21644;&#21367;&#31215;&#25805;&#20316;&#20043;&#38388;&#30340;&#26435;&#37325;&#20849;&#20139;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21333;&#36335;&#24452;&#31354;&#38388;&#26469;&#32534;&#30721;&#25152;&#26377;&#20505;&#36873;&#25805;&#20316;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23558;&#25805;&#20316;&#25628;&#32034;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#27599;&#20010;MSA&#23618;&#20013;&#25214;&#21040;&#35201;&#20351;&#29992;&#30340;&#21442;&#25968;&#23376;&#38598;&#65292;&#36825;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;&#20248;&#21270;&#38590;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#39044;&#20808;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have achieved impressive performance over various computer vision tasks. However, modeling global correlations with multi-head self-attention (MSA) layers leads to two widely recognized issues: the massive computational resource consumption and the lack of intrinsic inductive bias for modeling local visual patterns. To solve both issues, we devise a simple yet effective method named Single-Path Vision Transformer pruning (SPViT), to efficiently and automatically compress the pre-trained ViTs into compact models with proper locality added. Specifically, we first propose a novel weight-sharing scheme between MSA and convolutional operations, delivering a single-path space to encode all candidate operations. In this way, we cast the operation search problem as finding which subset of parameters to use in each MSA layer, which significantly reduces the computational cost and optimization difficulty, and the convolution kernels can be well initialized using pre-tr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#20027;&#21160;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#35268;&#21010;&#26410;&#26469;&#34892;&#21160;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22270;&#20687;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#19988;&#26131;&#20110;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2110.10083</link><description>&lt;p&gt;
&#23545;&#27604;&#20027;&#21160;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Contrastive Active Inference. (arXiv:2110.10083v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.10083
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#20027;&#21160;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#35268;&#21010;&#26410;&#26469;&#34892;&#21160;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22270;&#20687;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#19988;&#26131;&#20110;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#25512;&#26029;&#26159;&#19968;&#20010;&#20851;&#20110;&#24863;&#30693;&#21644;&#34892;&#21160;&#30340;&#32479;&#19968;&#29702;&#35770;&#65292;&#20854;&#22522;&#26412;&#24605;&#24819;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#33258;&#30001;&#33021;&#26469;&#32500;&#25345;&#22823;&#33041;&#23545;&#19990;&#30028;&#30340;&#20869;&#37096;&#27169;&#22411;&#12290;&#20174;&#34892;&#20026;&#23398;&#35282;&#24230;&#26469;&#30475;&#65292;&#20027;&#21160;&#25512;&#26029;&#20195;&#29702;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#33258;&#35777;&#26126;&#30340;&#23384;&#22312;&#65292;&#20182;&#20204;&#34892;&#21160;&#20197;&#23454;&#29616;&#20182;&#20204;&#20048;&#35266;&#30340;&#39044;&#27979;&#65292;&#21363;&#20248;&#36873;&#32467;&#26524;&#25110;&#30446;&#26631;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#20154;&#24037;&#35774;&#35745;&#30340;&#22870;&#21169;&#26469;&#23454;&#29616;&#20219;&#20309;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#20027;&#21160;&#25512;&#26029;&#21487;&#20197;&#20026;&#25511;&#21046;&#25552;&#20379;&#19968;&#31181;&#26356;&#33258;&#28982;&#30340;&#33258;&#30417;&#30563;&#30446;&#26631;&#65292;&#20294;&#20854;&#36866;&#29992;&#24615;&#21463;&#21040;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#25193;&#23637;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#30446;&#26631;&#65292;&#29992;&#20110;&#20027;&#21160;&#25512;&#26029;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#23398;&#20064;&#20195;&#29702;&#29983;&#25104;&#27169;&#22411;&#21644;&#35268;&#21010;&#26410;&#26469;&#34892;&#21160;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#20013;&#27604;&#22522;&#20110;&#20284;&#28982;&#30340;&#20027;&#21160;&#25512;&#26029;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#19988;&#26356;&#26131;&#20110;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active inference is a unifying theory for perception and action resting upon the idea that the brain maintains an internal model of the world by minimizing free energy. From a behavioral perspective, active inference agents can be seen as self-evidencing beings that act to fulfill their optimistic predictions, namely preferred outcomes or goals. In contrast, reinforcement learning requires human-designed rewards to accomplish any desired outcome. Although active inference could provide a more natural self-supervised objective for control, its applicability has been limited because of the shortcomings in scaling the approach to complex environments. In this work, we propose a contrastive objective for active inference that strongly reduces the computational burden in learning the agent's generative model and planning future actions. Our method performs notably better than likelihood-based active inference in image-based tasks, while also being computationally cheaper and easier to train
&lt;/p&gt;</description></item><item><title>CoDA Nets&#26159;&#19968;&#31181;&#24615;&#33021;&#33391;&#22909;&#30340;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#39640;&#24230;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;&#23427;&#20204;&#36890;&#36807;&#21160;&#24577;&#23545;&#40784;&#21333;&#20803;&#23454;&#29616;&#36755;&#20837;&#20381;&#36182;&#30340;&#32447;&#24615;&#21464;&#25442;&#65292;&#24182;&#23558;&#36755;&#20986;&#32447;&#24615;&#20998;&#35299;&#20026;&#21508;&#20010;&#36755;&#20837;&#30340;&#36129;&#29486;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#20998;&#31867;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;CIFAR-10&#21644;TinyImagenet&#31561;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;ResNet&#21644;VGG&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2109.13004</link><description>&lt;p&gt;
&#20248;&#21270;&#21487;&#35299;&#37322;&#24615;&#65306;&#21367;&#31215;&#21160;&#24577;&#23545;&#40784;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Optimising for Interpretability: Convolutional Dynamic Alignment Networks. (arXiv:2109.13004v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.13004
&lt;/p&gt;
&lt;p&gt;
CoDA Nets&#26159;&#19968;&#31181;&#24615;&#33021;&#33391;&#22909;&#30340;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#39640;&#24230;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;&#23427;&#20204;&#36890;&#36807;&#21160;&#24577;&#23545;&#40784;&#21333;&#20803;&#23454;&#29616;&#36755;&#20837;&#20381;&#36182;&#30340;&#32447;&#24615;&#21464;&#25442;&#65292;&#24182;&#23558;&#36755;&#20986;&#32447;&#24615;&#20998;&#35299;&#20026;&#21508;&#20010;&#36755;&#20837;&#30340;&#36129;&#29486;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#20998;&#31867;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;CIFAR-10&#21644;TinyImagenet&#31561;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;ResNet&#21644;VGG&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;&#21367;&#31215;&#21160;&#24577;&#23545;&#40784;&#32593;&#32476;&#65288;CoDA Nets&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#39640;&#24230;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#30340;&#24615;&#33021;&#20998;&#31867;&#22120;&#12290;&#23427;&#20204;&#30340;&#26680;&#24515;&#26500;&#24314;&#27169;&#22359;&#26159;&#21160;&#24577;&#23545;&#40784;&#21333;&#20803;&#65288;DAUs&#65289;&#65292;&#20854;&#32463;&#36807;&#20248;&#21270;&#21518;&#33021;&#22815;&#36890;&#36807;&#21160;&#24577;&#35745;&#31639;&#30340;&#26435;&#37325;&#21521;&#37327;&#23558;&#20854;&#36755;&#20837;&#36716;&#25442;&#20026;&#19982;&#20219;&#21153;&#30456;&#20851;&#27169;&#24335;&#23545;&#40784;&#30340;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;CoDA Nets&#36890;&#36807;&#19968;&#31995;&#21015;&#20381;&#36182;&#20110;&#36755;&#20837;&#30340;&#32447;&#24615;&#21464;&#25442;&#26469;&#27169;&#25311;&#20998;&#31867;&#39044;&#27979;&#65292;&#20801;&#35768;&#23558;&#36755;&#20986;&#32447;&#24615;&#20998;&#35299;&#20026;&#21508;&#20010;&#36755;&#20837;&#30340;&#36129;&#29486;&#12290;&#26681;&#25454;DAUs&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#24471;&#21040;&#30340;&#36129;&#29486;&#26144;&#23556;&#19982;&#37492;&#21035;&#24615;&#36755;&#20837;&#27169;&#24335;&#30456;&#19968;&#33268;&#12290;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20998;&#35299;&#20855;&#26377;&#24456;&#39640;&#30340;&#35270;&#35273;&#36136;&#37327;&#65292;&#22312;&#23450;&#37327;&#25351;&#26631;&#19979;&#20248;&#20110;&#29616;&#26377;&#30340;&#24402;&#22240;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;CoDA Nets&#26159;&#24615;&#33021;&#20986;&#33394;&#30340;&#20998;&#31867;&#22120;&#65292;&#22312;CIFAR-10&#21644;TinyImagenet&#31561;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19982;ResNet&#21644;VGG&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new family of neural network models called Convolutional Dynamic Alignment Networks (CoDA Nets), which are performant classifiers with a high degree of inherent interpretability. Their core building blocks are Dynamic Alignment Units (DAUs), which are optimised to transform their inputs with dynamically computed weight vectors that align with task-relevant patterns. As a result, CoDA Nets model the classification prediction through a series of input-dependent linear transformations, allowing for linear decomposition of the output into individual input contributions. Given the alignment of the DAUs, the resulting contribution maps align with discriminative input patterns. These model-inherent decompositions are of high visual quality and outperform existing attribution methods under quantitative metrics. Further, CoDA Nets constitute performant classifiers, achieving on par results to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet. Lastly, CoDA Nets can be combin
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;SubseasonalClimateUSA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#32654;&#22269;&#30340;&#20122;&#23395;&#33410;&#39044;&#27979;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#12290;&#20316;&#32773;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#23545;&#22810;&#31181;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2109.10399</link><description>&lt;p&gt;
SubseasonalClimateUSA: &#29992;&#20110;&#20122;&#23395;&#33410;&#39044;&#27979;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking. (arXiv:2109.10399v3 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.10399
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;SubseasonalClimateUSA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#32654;&#22269;&#30340;&#20122;&#23395;&#33410;&#39044;&#27979;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#12290;&#20316;&#32773;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#23545;&#22810;&#31181;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#30340;&#20122;&#23395;&#33410;&#39044;&#27979;&#23545;&#36164;&#28304;&#37197;&#32622;&#21644;&#27668;&#20505;&#36866;&#24212;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#39044;&#27979;&#31038;&#21306;&#25552;&#20986;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#22312;&#36825;&#20010;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#25216;&#33021;&#26377;&#38480;&#65292;&#24182;&#19988;&#39044;&#27979;&#30446;&#26631;&#20197;&#19968;&#31181;&#22797;&#26434;&#30340;&#26041;&#24335;&#20381;&#36182;&#20110;&#26412;&#22320;&#22825;&#27668;&#21644;&#20840;&#29699;&#27668;&#20505;&#21464;&#37327;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26174;&#31034;&#20986;&#25512;&#36827;&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25972;&#29702;&#65292;&#23558;&#19987;&#23478;&#30693;&#35782;&#19982;&#22810;&#20010;&#30456;&#20851;&#25968;&#25454;&#26469;&#28304;&#12289;&#25991;&#20214;&#26684;&#24335;&#21644;&#26102;&#38388;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#32858;&#21512;&#36827;&#34892;&#25972;&#21512;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#24182;&#21152;&#36895;&#26410;&#26469;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SubseasonalClimateUSA&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#32654;&#22269;&#30340;&#20122;&#23395;&#33410;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#26469;&#23545;&#21508;&#31181;&#19981;&#21516;&#30340;&#20122;&#23395;&#33410;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#25805;&#20316;&#24615;&#21160;&#21147;&#23398;&#27169;&#22411;&#12289;&#21476;&#20856;&#30340;&#27668;&#35937;&#22522;&#32447;&#20197;&#21450;&#21313;&#20010;&#32479;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subseasonal forecasting of the weather two to six weeks in advance is critical for resource allocation and climate adaptation but poses many challenges for the forecasting community. At this forecast horizon, physics-based dynamical models have limited skill, and the targets for prediction depend in a complex manner on both local weather and global climate variables. Recently, machine learning methods have shown promise in advancing the state of the art but only at the cost of complex data curation, integrating expert knowledge with aggregation across multiple relevant data sources, file formats, and temporal and spatial resolutions. To streamline this process and accelerate future development, we introduce SubseasonalClimateUSA, a curated dataset for training and benchmarking subseasonal forecasting models in the United States. We use this dataset to benchmark a diverse suite of subseasonal models, including operational dynamical models, classical meteorological baselines, and ten sta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26032;&#26102;&#23578;&#20135;&#21697;&#38144;&#37327;&#39044;&#27979;&#20013;&#32467;&#21512;&#35895;&#27468;&#36235;&#21183;&#21644;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23558;&#35895;&#27468;&#36235;&#21183;&#32534;&#30721;&#19982;&#35270;&#35273;&#21644;&#20803;&#25968;&#25454;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#32570;&#20047;&#36807;&#21435;&#25968;&#25454;&#30340;&#26032;&#26102;&#23578;&#20135;&#21697;&#38144;&#37327;&#12290;</title><link>http://arxiv.org/abs/2109.09824</link><description>&lt;p&gt;
&#35895;&#27468;&#36235;&#21183;&#21644;&#22810;&#27169;&#24577;&#20449;&#24687;&#22312;&#26032;&#26102;&#23578;&#20135;&#21697;&#38144;&#37327;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#22810;&#27169;&#24577;&#22522;&#20110;&#22270;&#20687;&#30340;&#35895;&#27468;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Well Googled is Half Done: Multimodal Forecasting of New Fashion Product Sales with Image-based Google Trends. (arXiv:2109.09824v6 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.09824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26032;&#26102;&#23578;&#20135;&#21697;&#38144;&#37327;&#39044;&#27979;&#20013;&#32467;&#21512;&#35895;&#27468;&#36235;&#21183;&#21644;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23558;&#35895;&#27468;&#36235;&#21183;&#32534;&#30721;&#19982;&#35270;&#35273;&#21644;&#20803;&#25968;&#25454;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#32570;&#20047;&#36807;&#21435;&#25968;&#25454;&#30340;&#26032;&#26102;&#23578;&#20135;&#21697;&#38144;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#26102;&#23578;&#20135;&#21697;&#38144;&#37327;&#39044;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#35768;&#22810;&#21830;&#19994;&#21160;&#24577;&#65292;&#24182;&#19988;&#19981;&#33021;&#36890;&#36807;&#20256;&#32479;&#30340;&#39044;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31995;&#32479;&#22320;&#25506;&#32034;&#22806;&#37096;&#30693;&#35782;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#35895;&#27468;&#36235;&#21183;&#26102;&#38388;&#24207;&#21015;&#30340;&#24418;&#24335;&#32467;&#21512;&#19982;&#20840;&#26032;&#26102;&#23578;&#20135;&#21697;&#30456;&#20851;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#32570;&#20047;&#36807;&#21435;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#39044;&#27979;&#20854;&#38144;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#32534;&#30721;&#22120;&#23398;&#20064;&#22806;&#37096;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#30340;&#21516;&#26102;&#65292;&#35299;&#30721;&#22120;&#26681;&#25454;&#35895;&#27468;&#36235;&#21183;&#32534;&#30721;&#21644;&#21487;&#29992;&#30340;&#35270;&#35273;&#21644;&#20803;&#25968;&#25454;&#20449;&#24687;&#26469;&#39044;&#27979;&#38144;&#37327;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#38750;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#24037;&#20316;&#65292;&#36991;&#20813;&#20102;&#22823;&#30340;&#31532;&#19968;&#27493;&#35823;&#24046;&#30340;&#32047;&#31215;&#25928;&#24212;&#12290;&#20316;&#20026;&#31532;&#20108;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;VISUELLE&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26032;&#26102;&#23578;&#20135;&#21697;&#38144;&#37327;&#39044;&#27979;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#21547;5577&#20010;&#30495;&#23454;&#30340;&#12289;&#26032;&#30340;&#20135;&#21697;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
New fashion product sales forecasting is a challenging problem that involves many business dynamics and cannot be solved by classical forecasting approaches. In this paper, we investigate the effectiveness of systematically probing exogenous knowledge in the form of Google Trends time series and combining it with multi-modal information related to a brand-new fashion item, in order to effectively forecast its sales despite the lack of past data. In particular, we propose a neural network-based approach, where an encoder learns a representation of the exogenous time series, while the decoder forecasts the sales based on the Google Trends encoding and the available visual and metadata information. Our model works in a non-autoregressive manner, avoiding the compounding effect of large first-step errors. As a second contribution, we present VISUELLE, a publicly available dataset for the task of new fashion product sales forecasting, containing multimodal information for 5577 real, new pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#31232;&#30095;&#31361;&#21457;&#31070;&#32463;&#31361;&#35302;&#65292;&#22312;&#39046;&#22495;&#36716;&#31227;&#20013;&#33021;&#22815;&#22312;&#20808;&#21069;&#26410;&#35265;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#20943;&#23569;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2108.12056</link><description>&lt;p&gt;
&#22312;&#31232;&#30095;&#31361;&#21457;&#31070;&#32463;&#31361;&#35302;&#19979;&#30340;&#39046;&#22495;&#36716;&#31227;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v9 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.12056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#31232;&#30095;&#31361;&#21457;&#31070;&#32463;&#31361;&#35302;&#65292;&#22312;&#39046;&#22495;&#36716;&#31227;&#20013;&#33021;&#22815;&#22312;&#20808;&#21069;&#26410;&#35265;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#20943;&#23569;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26426;&#22120;&#26159;&#21151;&#33021;&#29305;&#23450;&#30340;&#24037;&#20855;&#65292;&#26131;&#20110;&#39044;&#27979;&#21644;&#25511;&#21046;&#12290;&#26126;&#22825;&#30340;&#26426;&#22120;&#21487;&#33021;&#26356;&#25509;&#36817;&#29983;&#29289;&#31995;&#32479;&#65292;&#20855;&#26377;&#21487;&#21464;&#24615;&#12289;&#38887;&#24615;&#21644;&#33258;&#20027;&#24615;&#12290;&#20294;&#26159;&#39318;&#20808;&#65292;&#23427;&#20204;&#24517;&#39035;&#33021;&#22815;&#23398;&#20064;&#21644;&#20445;&#30041;&#26032;&#20449;&#24687;&#65292;&#32780;&#19981;&#24517;&#38543;&#26426;&#25509;&#35302;&#23427;&#12290;&#36807;&#21435;&#35774;&#35745;&#36825;&#26679;&#30340;&#31995;&#32479;&#30340;&#21162;&#21147;&#26159;&#36890;&#36807;&#26500;&#24314;&#25110;&#35843;&#33410;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#21807;&#19968;&#25935;&#24863;&#20110;&#29305;&#23450;&#20219;&#21153;&#25110;&#36755;&#20837;&#30340;&#19981;&#30456;&#20132;&#30340;&#26435;&#37325;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#23578;&#26410;&#23454;&#29616;&#22312;&#38271;&#26102;&#38388;&#30340;&#20808;&#21069;&#26410;&#35265;&#25968;&#25454;&#24207;&#21015;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#32780;&#19981;&#30772;&#22351;&#29616;&#26377;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#36825;&#34987;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20808;&#21069;&#26410;&#35265;&#30340;&#25968;&#25454;&#38598;&#65288;ImageNet&#65292;CIFAR-100&#65289;&#19978;&#20197;&#36739;&#23567;&#30340;&#36951;&#24536;&#36880;&#27493;&#23398;&#20064;&#12290;&#36825;&#26159;&#36890;&#36807;&#22522;&#20110;&#36755;&#20837;&#25511;&#21046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#26435;&#37325;&#30340;&#27963;&#21160;&#65292;&#20351;&#29992;&#30001;&#31532;&#20108;&#20010;&#21069;&#39304;&#29983;&#25104;&#30340;&#33258;&#19978;&#32780;&#19979;&#35843;&#33410;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing machines are functionally specific tools that were made for easy prediction and control. Tomorrow's machines may be closer to biological systems in their mutability, resilience, and autonomy. But first they must be capable of learning and retaining new information without being exposed to it arbitrarily often. Past efforts to engineer such systems have sought to build or regulate artificial neural networks using disjoint sets of weights that are uniquely sensitive to specific tasks or inputs. This has not yet enabled continual learning over long sequences of previously unseen data without corrupting existing knowledge: a problem known as catastrophic forgetting. In this paper, we introduce a system that can learn sequentially over previously unseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This is done by controlling the activity of weights in a convolutional neural network on the basis of inputs using top-down regulation generated by a second feed-forwa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#24341;&#20837;&#23616;&#37096;Lipschitz&#24615;&#36136;&#23545;&#40065;&#26834;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#30340;&#30410;&#22788;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#20855;&#26377;Lipschitz&#24615;&#36136;&#30340;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#26356;&#21152;&#40065;&#26834;&#30340;&#31574;&#30053;&#65292;&#25552;&#39640;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2107.00116</link><description>&lt;p&gt;
&#20851;&#20110;&#24341;&#20837;&#23616;&#37096;Lipschitz&#24615;&#36136;&#30340;&#40065;&#26834;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#30340;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
On the Benefits of Inducing Local Lipschitzness for Robust Generative Adversarial Imitation Learning. (arXiv:2107.00116v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.00116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#24341;&#20837;&#23616;&#37096;Lipschitz&#24615;&#36136;&#23545;&#40065;&#26834;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#30340;&#30410;&#22788;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#20855;&#26377;Lipschitz&#24615;&#36136;&#30340;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#26356;&#21152;&#40065;&#26834;&#30340;&#31574;&#30053;&#65292;&#25552;&#39640;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#25913;&#36827;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;GAIL&#65289;&#31639;&#27861;&#23545;&#35266;&#27979;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#23616;&#37096;Lipschitz&#24615;&#36136;&#23545;GAIL&#23398;&#20064;&#30340;&#31574;&#30053;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#35768;&#22810;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#29615;&#22659;&#35266;&#27979;&#21487;&#33021;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;GAIL&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#27979;&#35797;&#26102;&#36890;&#24120;&#34920;&#29616;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#23545;&#25239;&#35266;&#27979;&#22122;&#22768;&#30340;&#23398;&#20064;&#31574;&#30053;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24341;&#20837;&#23616;&#37096;Lipschitz&#24615;&#36136;&#21040;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#30340;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20462;&#25913;&#21518;&#30340;&#30446;&#26631;&#23548;&#33268;&#23398;&#20064;&#21040;&#26356;&#21152;&#40065;&#26834;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#35777;&#26126;&#20102;&#35757;&#32451;&#19968;&#20010;&#23616;&#37096;Lipschitz&#37492;&#21035;&#22120;&#20250;&#23548;&#33268;&#19968;&#20010;&#23616;&#37096;Lipschitz&#29983;&#25104;&#22120;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore methodologies to improve the robustness of generative adversarial imitation learning (GAIL) algorithms to observation noise. Towards this objective, we study the effect of local Lipschitzness of the discriminator and the generator on the robustness of policies learned by GAIL. In many robotics applications, the learned policies by GAIL typically suffer from a degraded performance at test time since the observations from the environment might be corrupted by noise. Hence, robustifying the learned policies against the observation noise is of critical importance. To this end, we propose a regularization method to induce local Lipschitzness in the generator and the discriminator of adversarial imitation learning methods. We show that the modified objective leads to learning significantly more robust policies. Moreover, we demonstrate -- both theoretically and experimentally -- that training a locally Lipschitz discriminator leads to a locally Lipschitz generator, thereby improvi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;CoDA-Nets&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#30340;&#24615;&#33021;&#20998;&#31867;&#22120;&#12290;CoDA-Nets&#20351;&#29992;&#21160;&#24577;&#23545;&#40784;&#21333;&#20803;&#23545;&#36755;&#20837;&#36827;&#34892;&#32447;&#24615;&#21464;&#25442;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#20998;&#35299;&#23558;&#36755;&#20986;&#35299;&#37322;&#20026;&#21333;&#20010;&#36755;&#20837;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;CoDA-Nets&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19982;ResNet&#21644;VGG&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2104.00032</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20998;&#31867;&#30340;&#21367;&#31215;&#21160;&#24577;&#23545;&#40784;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Convolutional Dynamic Alignment Networks for Interpretable Classifications. (arXiv:2104.00032v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.00032
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;CoDA-Nets&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#30340;&#24615;&#33021;&#20998;&#31867;&#22120;&#12290;CoDA-Nets&#20351;&#29992;&#21160;&#24577;&#23545;&#40784;&#21333;&#20803;&#23545;&#36755;&#20837;&#36827;&#34892;&#32447;&#24615;&#21464;&#25442;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#20998;&#35299;&#23558;&#36755;&#20986;&#35299;&#37322;&#20026;&#21333;&#20010;&#36755;&#20837;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;CoDA-Nets&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19982;ResNet&#21644;VGG&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#21367;&#31215;&#21160;&#24577;&#23545;&#40784;&#32593;&#32476;&#65288;CoDA-Nets&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#39640;&#24230;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#30340;&#24615;&#33021;&#20998;&#31867;&#22120;&#12290;&#23427;&#20204;&#30340;&#26680;&#24515;&#26500;&#24314;&#27169;&#22359;&#26159;&#21160;&#24577;&#23545;&#40784;&#21333;&#20803;&#65288;DAUs&#65289;&#65292;&#23427;&#20204;&#20351;&#29992;&#26435;&#37325;&#21521;&#37327;&#32447;&#24615;&#21464;&#25442;&#20854;&#36755;&#20837;&#65292;&#24182;&#19982;&#20219;&#21153;&#30456;&#20851;&#27169;&#24335;&#21160;&#24577;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;CoDA-Nets&#36890;&#36807;&#19968;&#31995;&#21015;&#20381;&#36182;&#20110;&#36755;&#20837;&#30340;&#32447;&#24615;&#21464;&#25442;&#23545;&#20998;&#31867;&#39044;&#27979;&#24314;&#27169;&#65292;&#20801;&#35768;&#32447;&#24615;&#20998;&#35299;&#36755;&#20986;&#20026;&#21333;&#20010;&#36755;&#20837;&#36129;&#29486;&#12290;&#26681;&#25454;DAUs&#30340;&#23545;&#40784;&#65292;&#24471;&#21040;&#30340;&#36129;&#29486;&#22270;&#19982;&#20855;&#26377;&#37492;&#21035;&#24615;&#30340;&#36755;&#20837;&#27169;&#24335;&#23545;&#40784;&#12290;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20998;&#35299;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#21487;&#35270;&#21270;&#25928;&#26524;&#65292;&#24182;&#22312;&#23450;&#37327;&#25351;&#26631;&#19979;&#32988;&#36807;&#29616;&#26377;&#30340;&#24402;&#22240;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;CoDA-Nets&#26159;&#19968;&#31181;&#24615;&#33021;&#33391;&#22909;&#30340;&#20998;&#31867;&#22120;&#65292;&#22312;CIFAR-10&#21644;TinyImagenet&#31561;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#19982;ResNet&#21644;VGG&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new family of neural network models called Convolutional Dynamic Alignment Networks (CoDA-Nets), which are performant classifiers with a high degree of inherent interpretability. Their core building blocks are Dynamic Alignment Units (DAUs), which linearly transform their input with weight vectors that dynamically align with task-relevant patterns. As a result, CoDA-Nets model the classification prediction through a series of input-dependent linear transformations, allowing for linear decomposition of the output into individual input contributions. Given the alignment of the DAUs, the resulting contribution maps align with discriminative input patterns. These model-inherent decompositions are of high visual quality and outperform existing attribution methods under quantitative metrics. Further, CoDA-Nets constitute performant classifiers, achieving on par results to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#20195;&#30721;&#24322;&#21619;&#23545;&#20195;&#30721;&#36136;&#37327;&#21644;&#24320;&#21457;&#29983;&#20135;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20165;&#26377;&#23569;&#25968;&#20195;&#30721;&#24322;&#21619;&#26159;&#30495;&#27491;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#65292;&#20854;&#20013;&#19982;&#31616;&#21333;&#24615;&#12289;&#38450;&#24481;&#24615;&#32534;&#31243;&#21644;&#25277;&#35937;&#30456;&#20851;&#30340;&#24322;&#21619;&#26159;&#26368;&#24378;&#28872;&#30340;&#12290;&#24320;&#21457;&#20154;&#21592;&#20542;&#21521;&#20110;&#21435;&#38500;&#23481;&#26131;&#30340;&#24322;&#21619;&#32780;&#19981;&#26159;&#30495;&#27491;&#26377;&#25928;&#30340;&#24322;&#21619;&#12290;</title><link>http://arxiv.org/abs/2103.01861</link><description>&lt;p&gt;
&#36861;&#36394;&#20195;&#30721;&#24322;&#21619; - &#21738;&#20123;&#20195;&#30721;&#24322;&#21619;&#20540;&#24471;&#36861;&#36394;&#65311;
&lt;/p&gt;
&lt;p&gt;
Follow Your Nose -- Which Code Smells are Worth Chasing?. (arXiv:2103.01861v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.01861
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#20195;&#30721;&#24322;&#21619;&#23545;&#20195;&#30721;&#36136;&#37327;&#21644;&#24320;&#21457;&#29983;&#20135;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20165;&#26377;&#23569;&#25968;&#20195;&#30721;&#24322;&#21619;&#26159;&#30495;&#27491;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#65292;&#20854;&#20013;&#19982;&#31616;&#21333;&#24615;&#12289;&#38450;&#24481;&#24615;&#32534;&#31243;&#21644;&#25277;&#35937;&#30456;&#20851;&#30340;&#24322;&#21619;&#26159;&#26368;&#24378;&#28872;&#30340;&#12290;&#24320;&#21457;&#20154;&#21592;&#20542;&#21521;&#20110;&#21435;&#38500;&#23481;&#26131;&#30340;&#24322;&#21619;&#32780;&#19981;&#26159;&#30495;&#27491;&#26377;&#25928;&#30340;&#24322;&#21619;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#24322;&#21619;&#30340;&#24120;&#35265;&#29992;&#20363;&#35748;&#20026;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#65306;&#35782;&#21035;&#24322;&#21619;&#65292;&#21435;&#38500;&#23427;&#65292;&#20174;&#32780;&#25913;&#21892;&#20195;&#30721;&#12290;&#25105;&#20204;&#23545;&#20854;&#36866;&#24212;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20195;&#30721;&#24322;&#21619;&#30340;&#29305;&#24615;&#65292;&#22914;&#26524;&#23427;&#20204;&#30830;&#23454;&#23548;&#33268;&#36739;&#20302;&#30340;&#20195;&#30721;&#36136;&#37327;&#65292;&#37027;&#20040;&#23427;&#20204;&#24212;&#35813;&#20855;&#22791;&#36825;&#20123;&#29305;&#24615;&#12290;&#25105;&#20204;&#23545;&#26469;&#33258;677&#20010;GitHub&#23384;&#20648;&#24211;&#20013;31,687&#20010;Java&#25991;&#20214;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36825;&#20123;&#23384;&#20648;&#24211;&#22312;2019&#24180;&#26377;200&#20010;&#20197;&#19978;&#30340;&#25552;&#20132;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#36825;&#20123;&#24322;&#21619;&#23545;&#36136;&#37327;&#12289;&#29983;&#20135;&#21147;&#21644;&#38169;&#35823;&#26816;&#27979;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;CheckStyle&#24322;&#21619;&#26816;&#27979;&#22120;&#35745;&#31639;&#20986;&#30340;151&#20010;&#20195;&#30721;&#24322;&#21619;&#20013;&#65292;&#21482;&#26377;&#19981;&#21040;20%&#34987;&#21457;&#29616;&#21487;&#33021;&#24341;&#36215;&#36136;&#37327;&#19979;&#38477;&#65292;&#24182;&#19988;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#30456;&#23545;&#31283;&#23450;&#12290;&#26368;&#24378;&#28872;&#30340;&#24322;&#21619;&#19982;&#31616;&#21333;&#24615;&#12289;&#38450;&#24481;&#24615;&#32534;&#31243;&#21644;&#25277;&#35937;&#26377;&#20851;&#12290;&#27809;&#26377;&#21487;&#33021;&#24341;&#36215;&#36136;&#37327;&#19979;&#38477;&#30340;&#24322;&#21619;&#30340;&#25991;&#20214;&#26356;&#26377;&#21487;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#36136;&#37327;&#65292;&#27010;&#29575;&#22686;&#21152;&#20102;50%&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#24322;&#21619;&#27809;&#26377;&#34987;&#21435;&#38500;&#65292;&#24320;&#21457;&#20154;&#21592;&#20542;&#21521;&#20110;&#21435;&#38500;&#23481;&#26131;&#30340;&#24322;&#21619;&#32780;&#19981;&#26159;&#26377;&#25928;&#30340;&#24322;&#21619;&#12290;
&lt;/p&gt;
&lt;p&gt;
The common use case of code smells assumes causality: Identify a smell, remove it, and by doing so improve the code. We empirically investigate their fitness to this use. We present a list of properties that code smells should have if they indeed cause lower quality. We evaluated the smells in 31,687 Java files from 677 GitHub repositories, all the repositories with 200+ commits in 2019. We measured the influence of smells on four metrics for quality, productivity, and bug detection efficiency. Out of 151 code smells computed by the CheckStyle smell detector, less than 20% were found to be potentially causal, and only a handful are rather robust. The strongest smells deal with simplicity, defensive programming, and abstraction. Files without the potentially causal smells are 50% more likely to be of high quality. Unfortunately, most smells are not removed, and developers tend to remove the easy ones and not the effective ones.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#23481;&#37327;&#27133;&#27880;&#24847;&#21147;&#27169;&#22411;&#20174;&#23383;&#31526;&#24207;&#21015;&#20013;&#23398;&#20064;&#25277;&#35937;&#26377;&#24847;&#20041;&#21333;&#20803;&#65292;&#25104;&#21151;&#21457;&#29616;&#20102;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#21333;&#20803;&#30456;&#20284;&#30340;&#12289;&#36866;&#29992;&#20110;&#26356;&#39640;&#32423;&#21035;&#25277;&#35937;&#30340;&#21487;&#25429;&#25417;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#21333;&#20803;&#12290;</title><link>http://arxiv.org/abs/2102.01223</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#23481;&#37327;&#27133;&#27880;&#24847;&#21147;&#20174;&#23383;&#31526;&#24207;&#21015;&#20013;&#35825;&#23548;&#26377;&#24847;&#20041;&#30340;&#21333;&#20803;
&lt;/p&gt;
&lt;p&gt;
Inducing Meaningful Units from Character Sequences with Dynamic Capacity Slot Attention. (arXiv:2102.01223v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.01223
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#23481;&#37327;&#27133;&#27880;&#24847;&#21147;&#27169;&#22411;&#20174;&#23383;&#31526;&#24207;&#21015;&#20013;&#23398;&#20064;&#25277;&#35937;&#26377;&#24847;&#20041;&#21333;&#20803;&#65292;&#25104;&#21151;&#21457;&#29616;&#20102;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#21333;&#20803;&#30456;&#20284;&#30340;&#12289;&#36866;&#29992;&#20110;&#26356;&#39640;&#32423;&#21035;&#25277;&#35937;&#30340;&#21487;&#25429;&#25417;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#31526;&#26412;&#36523;&#24182;&#19981;&#20256;&#36798;&#24847;&#20041;&#65292;&#20294;&#23383;&#31526;&#24207;&#21015;&#21364;&#21487;&#20197;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#23383;&#31526;&#24207;&#21015;&#20013;&#30340;&#25277;&#35937;&#26377;&#24847;&#20041;&#21333;&#20803;&#12290;&#25105;&#20204;&#30340;&#21160;&#24577;&#23481;&#37327;&#27133;&#27880;&#24847;&#21147;&#27169;&#22411;&#19981;&#26159;&#23545;&#24207;&#21015;&#36827;&#34892;&#20998;&#21106;&#65292;&#32780;&#26159;&#21457;&#29616;&#24207;&#21015;&#20013;&#23545;&#35937;&#30340;&#36830;&#32493;&#34920;&#31034;&#65292;&#25193;&#23637;&#20102;&#22270;&#20687;&#20013;&#23545;&#35937;&#21457;&#29616;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#35821;&#35328;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#21644;&#21453;&#21521;&#25506;&#27979;&#20998;&#31867;&#22120;&#35780;&#20272;&#25152;&#24471;&#21040;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#21457;&#29616;&#20102;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#21333;&#20803;&#22312;&#24418;&#24335;&#12289;&#20869;&#23481;&#21644;&#25277;&#35937;&#32423;&#21035;&#19978;&#30456;&#20284;&#30340;&#21333;&#20803;&#65292;&#24182;&#26174;&#31034;&#20102;&#22312;&#26356;&#39640;&#32423;&#21035;&#30340;&#25277;&#35937;&#20013;&#25429;&#25417;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Characters do not convey meaning, but sequences of characters do. We propose an unsupervised distributional method to learn the abstract meaningful units in a sequence of characters. Rather than segmenting the sequence, our Dynamic Capacity Slot Attention model discovers continuous representations of the objects in the sequence, extending an architecture for object discovery in images. We train our model on different languages and evaluate the quality of the obtained representations with forward and reverse probing classifiers. These experiments show that our model succeeds in discovering units which are similar to those proposed previously in form, content and level of abstraction, and which show promise for capturing meaningful information at a higher level of abstraction.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DA3D&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#22522;&#20110;&#27491;&#24120;&#25968;&#25454;&#30340;&#24322;&#24120;&#21453;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#27809;&#26377;&#39046;&#22495;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2101.04645</link><description>&lt;p&gt;
&#21452;&#37325;&#23545;&#25239;&#28608;&#27963;&#24322;&#24120;&#26816;&#27979;&#65306;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#26159;&#24322;&#24120;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Double-Adversarial Activation Anomaly Detection: Adversarial Autoencoders are Anomaly Generators. (arXiv:2101.04645v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.04645
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DA3D&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#22522;&#20110;&#27491;&#24120;&#25968;&#25454;&#30340;&#24322;&#24120;&#21453;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#27809;&#26377;&#39046;&#22495;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#36825;&#26159;&#30001;&#20110;&#22266;&#26377;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#24615;&#25152;&#23548;&#33268;&#30340;&#12290;&#30001;&#20110;&#25163;&#21160;&#20998;&#26512;&#35266;&#23519;&#25968;&#25454;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#65292;&#22240;&#27492;&#36890;&#24120;&#21482;&#26377;&#19968;&#20123;&#24050;&#30693;&#30340;&#24322;&#24120;&#65292;&#22914;&#26524;&#26377;&#30340;&#35805;&#12290;&#21463;&#21040;&#29983;&#25104;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#38544;&#34255;&#28608;&#27963;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;DA3D&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#20165;&#22522;&#20110;&#27491;&#24120;&#25968;&#25454;&#29983;&#25104;&#24322;&#24120;&#30340;&#21453;&#20363;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#36825;&#20123;&#20154;&#24037;&#24322;&#24120;&#21487;&#20197;&#26816;&#27979;&#21040;&#30495;&#23454;&#20294;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#22411;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#36716;&#21270;&#20026;&#19968;&#31181;&#30417;&#30563;&#20219;&#21153;&#65292;&#36825;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26356;&#26131;&#22788;&#29702;&#12290;DA3D&#22312;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#19979;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is a challenging task for machine learning algorithms due to the inherent class imbalance. It is costly and time-demanding to manually analyse the observed data, thus usually only few known anomalies if any are available. Inspired by generative models and the analysis of the hidden activations of neural networks, we introduce a novel unsupervised anomaly detection method called DA3D. Here, we use adversarial autoencoders to generate anomalous counterexamples based on the normal data only. These artificial anomalies used during training allow the detection of real, yet unseen anomalies. With our novel generative approach, we transform the unsupervised task of anomaly detection to a supervised one, which is more tractable by machine learning and especially deep learning methods. DA3D surpasses the performance of state-of-the-art anomaly detection methods in a purely data-driven way, where no domain knowledge is required.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25932;&#23545;&#26694;&#26550;&#65292;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#31354;&#38388;&#26469;&#20272;&#35745;Riesz Representer&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#38750;&#28176;&#36817;&#22343;&#26041;&#36895;&#29575;&#20197;&#21450;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26465;&#20214;&#12290;&#36825;&#20010;&#26465;&#20214;&#20351;&#24471;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36827;&#34892;&#25512;&#26029;&#26102;&#26080;&#38656;&#26679;&#26412;&#20998;&#21106;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#39640;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2101.00009</link><description>&lt;p&gt;
&#23545;Riesz Representer&#30340;&#25932;&#23545;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adversarial Estimation of Riesz Representers. (arXiv:2101.00009v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.00009
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25932;&#23545;&#26694;&#26550;&#65292;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#31354;&#38388;&#26469;&#20272;&#35745;Riesz Representer&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#38750;&#28176;&#36817;&#22343;&#26041;&#36895;&#29575;&#20197;&#21450;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26465;&#20214;&#12290;&#36825;&#20010;&#26465;&#20214;&#20351;&#24471;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36827;&#34892;&#25512;&#26029;&#26102;&#26080;&#38656;&#26679;&#26412;&#20998;&#21106;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#39640;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22240;&#26524;&#21644;&#32467;&#26500;&#21442;&#25968;&#26159;&#22522;&#20110;&#24213;&#23618;&#22238;&#24402;&#30340;&#32447;&#24615;&#27867;&#20989;&#12290;Riesz Representer&#26159;&#21322;&#21442;&#25968;&#32447;&#24615;&#27867;&#20989;&#28176;&#36817;&#26041;&#24046;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25932;&#23545;&#26694;&#26550;&#65292;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#31354;&#38388;&#26469;&#20272;&#35745;Riesz Representer&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#38750;&#28176;&#36817;&#22343;&#26041;&#36895;&#29575;&#65292;&#20854;&#20013;&#28041;&#21450;&#19968;&#20010;&#31216;&#20026;&#20020;&#30028;&#21322;&#24452;&#30340;&#25277;&#35937;&#37327;&#65292;&#28982;&#21518;&#23558;&#20854;&#19987;&#38376;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20316;&#20026;&#20027;&#35201;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20020;&#30028;&#21322;&#24452;&#29702;&#35770;&#26469;&#35777;&#26126;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#26679;&#26412;&#20998;&#21106;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#8220;&#22797;&#26434;&#24230;-&#36895;&#29575;&#40065;&#26834;&#24615;&#8221;&#26465;&#20214;&#12290;&#36825;&#20010;&#26465;&#20214;&#20855;&#26377;&#23454;&#38469;&#21518;&#26524;&#65306;&#22312;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#38656;&#26679;&#26412;&#20998;&#21106;&#30340;&#25512;&#26029;&#65292;&#36825;&#21487;&#33021;&#20250;&#25552;&#39640;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#27169;&#25311;&#20013;&#23454;&#29616;&#20102;&#21517;&#20041;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many causal and structural parameters are linear functionals of an underlying regression. The Riesz representer is a key component in the asymptotic variance of a semiparametrically estimated linear functional. We propose an adversarial framework to estimate the Riesz representer using general function spaces. We prove a nonasymptotic mean square rate in terms of an abstract quantity called the critical radius, then specialize it for neural networks, random forests, and reproducing kernel Hilbert spaces as leading cases. Furthermore, we use critical radius theory -- in place of Donsker theory -- to prove asymptotic normality without sample splitting, uncovering a ``complexity-rate robustness'' condition. This condition has practical consequences: inference without sample splitting is possible in several machine learning settings, which may improve finite sample performance compared to sample splitting. Our estimators achieve nominal coverage in highly nonlinear simulations where previo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27491;&#20132;&#22810;&#26679;&#24615;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#20013;&#23884;&#20837;&#22810;&#20010;&#36335;&#24452;&#24182;&#26045;&#21152;&#27491;&#20132;&#32422;&#26463;&#65292;&#35753;&#27169;&#22411;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#36755;&#20837;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2010.12190</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#20132;&#22810;&#26679;&#24615;&#23454;&#29616;&#31283;&#20581;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Neural Networks via Orthogonal Diversity. (arXiv:2010.12190v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.12190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27491;&#20132;&#22810;&#26679;&#24615;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#20013;&#23884;&#20837;&#22810;&#20010;&#36335;&#24452;&#24182;&#26045;&#21152;&#27491;&#20132;&#32422;&#26463;&#65292;&#35753;&#27169;&#22411;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#36755;&#20837;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23545;&#20110;&#30001;&#23545;&#25239;&#25915;&#20987;&#29983;&#25104;&#30340;&#22270;&#20687;&#19978;&#30340;&#30475;&#19981;&#35265;&#30340;&#25200;&#21160;&#26159;&#33030;&#24369;&#30340;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;DNN&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#12290;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#21450;&#20854;&#21464;&#31181;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#22686;&#24378;DNN&#40065;&#26834;&#24615;&#26368;&#26377;&#25928;&#30340;&#25216;&#26415;&#20043;&#19968;&#12290;&#36890;&#24120;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#20391;&#37325;&#20110;&#36890;&#36807;&#24341;&#20837;&#25200;&#21160;&#25968;&#25454;&#20016;&#23500;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20123;&#25200;&#21160;&#25968;&#25454;&#30340;&#25968;&#25454;&#22686;&#24378;&#25928;&#26524;&#24182;&#19981;&#33021;&#36129;&#29486;&#21040;DNN&#26412;&#36523;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;&#20026;&#20102;&#25552;&#39640;DNN&#26412;&#36523;&#30340;&#40065;&#26834;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#24378;&#27169;&#22411;&#26469;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#36755;&#20837;&#65288;&#21253;&#25324;&#23545;&#25239;&#24615;&#26679;&#26412;&#65289;&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#32593;&#32476;&#20013;&#23884;&#20837;&#20102;&#22810;&#20010;&#36335;&#24452;&#65292;&#24182;&#23545;&#36825;&#20123;&#36335;&#24452;&#26045;&#21152;&#27491;&#20132;&#32422;&#26463;&#65292;&#20197;&#20445;&#35777;&#23427;&#20204;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are vulnerable to invisible perturbations on the images generated by adversarial attacks, which raises researches on the adversarial robustness of DNNs. A series of methods represented by the adversarial training and its variants have proven as one of the most effective techniques in enhancing the DNN robustness. Generally, adversarial training focuses on enriching the training data by involving perturbed data. Such data augmentation effect of the involved perturbed data in adversarial training does not contribute to the robustness of DNN itself and usually suffers from clean accuracy drop. Towards the robustness of DNN itself, we in this paper propose a novel defense that aims at augmenting the model in order to learn features that are adaptive to diverse inputs, including adversarial examples. More specifically, to augment the model, multiple paths are embedded into the network, and an orthogonality constraint is imposed on these paths to guarantee the div
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#22411;&#38543;&#26426;&#20613;&#31435;&#21494;&#29305;&#24449;&#36827;&#34892;&#31471;&#21040;&#31471;&#20869;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#20869;&#26680;&#23398;&#20064;&#21644;&#32447;&#24615;&#23398;&#20064;&#22120;&#34701;&#21512;&#20026;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#32593;&#32476;&#21644;&#32447;&#24615;&#20998;&#31867;&#22120;&#32852;&#21512;&#35757;&#32451;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2009.04614</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#22411;&#38543;&#26426;&#20613;&#31435;&#21494;&#29305;&#24449;&#36827;&#34892;&#31471;&#21040;&#31471;&#20869;&#26680;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-end Kernel Learning via Generative Random Fourier Features. (arXiv:2009.04614v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.04614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#22411;&#38543;&#26426;&#20613;&#31435;&#21494;&#29305;&#24449;&#36827;&#34892;&#31471;&#21040;&#31471;&#20869;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#20869;&#26680;&#23398;&#20064;&#21644;&#32447;&#24615;&#23398;&#20064;&#22120;&#34701;&#21512;&#20026;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#32593;&#32476;&#21644;&#32447;&#24615;&#20998;&#31867;&#22120;&#32852;&#21512;&#35757;&#32451;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20613;&#31435;&#21494;&#29305;&#24449;&#65288;RFFs&#65289;&#20026;&#35889;&#20869;&#26680;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#30446;&#21069;&#22522;&#20110;RFFs&#30340;&#20869;&#26680;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20197;&#20004;&#38454;&#27573;&#26041;&#24335;&#24037;&#20316;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#26368;&#20248;&#29305;&#24449;&#26144;&#23556;&#36890;&#24120;&#34987;&#34920;&#36848;&#20026;&#30446;&#26631;&#23545;&#40784;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#23558;&#23398;&#20064;&#21040;&#30340;&#20869;&#26680;&#19982;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#20869;&#26680;&#65288;&#36890;&#24120;&#26159;&#29702;&#24819;&#20869;&#26680;&#65289;&#23545;&#40784;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#36807;&#31243;&#20013;&#65292;&#32447;&#24615;&#23398;&#20064;&#22120;&#38024;&#23545;&#26144;&#23556;&#30340;&#38543;&#26426;&#29305;&#24449;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#23545;&#40784;&#20013;&#30340;&#39044;&#23450;&#20041;&#20869;&#26680;&#19981;&#19968;&#23450;&#23545;&#20110;&#32447;&#24615;&#23398;&#20064;&#22120;&#30340;&#27867;&#21270;&#26159;&#26368;&#20248;&#30340;&#12290;&#30456;&#21453;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#20869;&#26680;&#23398;&#20064;&#21644;&#32447;&#24615;&#23398;&#20064;&#22120;&#34701;&#21512;&#20026;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#30340;&#19968;&#38454;&#27573;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#36807;RFFs&#38544;&#24335;&#23398;&#20064;&#20869;&#26680;&#30340;&#29983;&#25104;&#32593;&#32476;&#65292;&#25509;&#30528;&#20351;&#29992;&#19968;&#20010;&#20840;&#36830;&#25509;&#23618;&#21442;&#25968;&#21270;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#27714;&#35299;&#20248;&#21270;&#38382;&#39064;&#65292;&#32852;&#21512;&#35757;&#32451;&#29983;&#25104;&#32593;&#32476;&#21644;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random Fourier features (RFFs) provide a promising way for kernel learning in a spectral case. Current RFFs-based kernel learning methods usually work in a two-stage way. In the first-stage process, learning the optimal feature map is often formulated as a target alignment problem, which aims to align the learned kernel with the pre-defined target kernel (usually the ideal kernel). In the second-stage process, a linear learner is conducted with respect to the mapped random features. Nevertheless, the pre-defined kernel in target alignment is not necessarily optimal for the generalization of the linear learner. Instead, in this paper, we consider a one-stage process that incorporates the kernel learning and linear learner into a unifying framework. To be specific, a generative network via RFFs is devised to implicitly learn the kernel, followed by a linear classifier parameterized as a full-connected layer. Then the generative network and the classifier are jointly trained by solving th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#30417;&#30563;&#23398;&#20064;&#21644;VAEs&#32479;&#19968;&#20110;&#22522;&#20110;&#27491;&#24577;&#27969;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#23545;&#22825;&#25991;&#31890;&#23376;&#37325;&#24314;&#36827;&#34892;&#20102;&#35206;&#30422;&#12289;&#31995;&#32479;&#24615;&#21644;&#25311;&#21512;&#22909;&#22351;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;KL&#25955;&#24230;&#30446;&#26631;&#23454;&#29616;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;VAEs&#30340;&#32479;&#19968;&#12290;&#21033;&#29992;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#30340;&#26041;&#27861;&#21487;&#20197;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25311;&#21512;&#20248;&#24230;p&#20540;&#12290;</title><link>http://arxiv.org/abs/2008.05825</link><description>&lt;p&gt;
&#23558;&#30417;&#30563;&#23398;&#20064;&#21644;VAEs&#32479;&#19968;&#22312;&#22522;&#20110;&#27491;&#24577;&#27969;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#23545;&#22825;&#25991;&#31890;&#23376;&#37325;&#24314;&#36827;&#34892;&#35206;&#30422;&#12289;&#31995;&#32479;&#24615;&#21644;&#25311;&#21512;&#22909;&#22351;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unifying supervised learning and VAEs -- coverage, systematics and goodness-of-fit in normalizing-flow based neural network models for astro-particle reconstructions. (arXiv:2008.05825v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.05825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30417;&#30563;&#23398;&#20064;&#21644;VAEs&#32479;&#19968;&#20110;&#22522;&#20110;&#27491;&#24577;&#27969;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#23545;&#22825;&#25991;&#31890;&#23376;&#37325;&#24314;&#36827;&#34892;&#20102;&#35206;&#30422;&#12289;&#31995;&#32479;&#24615;&#21644;&#25311;&#21512;&#22909;&#22351;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;KL&#25955;&#24230;&#30446;&#26631;&#23454;&#29616;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;VAEs&#30340;&#32479;&#19968;&#12290;&#21033;&#29992;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#30340;&#26041;&#27861;&#21487;&#20197;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25311;&#21512;&#20248;&#24230;p&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22825;&#25991;&#31890;&#23376;&#29289;&#29702;&#23398;&#20013;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#20214;&#23646;&#24615;&#39044;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#32467;&#26524;&#21482;&#34987;&#29992;&#20316;&#28857;&#39044;&#27979;&#12290;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#21644;&#35206;&#30422;&#29575;(1)&#65292;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;(2)&#25110;&#25311;&#21512;&#20248;&#24230;&#24230;&#37327;(3)&#32463;&#24120;&#27809;&#26377;&#34987;&#35745;&#31639;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#35757;&#32451;&#21644;&#32593;&#32476;&#26550;&#26500;&#36873;&#25321;&#65292;&#21487;&#20197;&#23558;&#25152;&#26377;&#36825;&#20123;&#23646;&#24615;&#34701;&#20837;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#32593;&#32476;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#21644;&#26631;&#31614;&#32852;&#21512;&#20998;&#24067;&#30340;KL&#25955;&#24230;&#30446;&#26631;&#20351;&#24471;&#22312;&#38543;&#26426;&#21464;&#20998;&#25512;&#29702;&#30340;&#19968;&#31181;&#32479;&#19968;&#19979;&#23558;&#30417;&#30563;&#23398;&#20064;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAEs)&#32479;&#19968;&#36215;&#26469;&#12290;&#36825;&#31181;&#32479;&#19968;&#24615;&#28608;&#21457;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#65292;&#21487;&#20197;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25311;&#21512;&#20248;&#24230;p&#20540;&#12290;&#22312;&#36825;&#31181;&#24314;&#35774;&#20013;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#20204;&#22914;&#20309;&#20026;&#24050;&#23450;&#20041;&#30340;&#21518;&#39564;&#20998;&#24067;&#20005;&#26684;&#23450;&#20041;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-network based predictions of event properties in astro-particle physics are getting more and more common. However, in many cases the result is just utilized as a point prediction. Statistical uncertainties and coverage (1), systematic uncertainties (2) or a goodness-of-fit measure (3) are often not calculated. Here we describe a certain choice of training and network architecture that allows to incorporate all these properties into a single network model. We show that a KL-divergence objective of the joint distribution of data and labels allows to unify supervised learning and variational autoencoders (VAEs) under one umbrella of stochastic variational inference. The unification motivates an extended supervised learning scheme which allows to calculate a goodness-of-fit p-value for the neural network model. Conditional normalizing flows amortized with a neural network are crucial in this construction. We discuss how they allow to rigorously define coverage for posteriors defined
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20559;&#21387;&#21387;&#32553;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#20559;&#21387;&#21387;&#32553;&#22120;&#21487;&#20197;&#22312;&#21333;&#33410;&#28857;&#21644;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2002.12410</link><description>&lt;p&gt;
&#20851;&#20110;&#20559;&#21387;&#21387;&#32553;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Biased Compression for Distributed Learning. (arXiv:2002.12410v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.12410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20559;&#21387;&#21387;&#32553;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#20559;&#21387;&#21387;&#32553;&#22120;&#21487;&#20197;&#22312;&#21333;&#33410;&#28857;&#21644;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#36890;&#20449;&#21387;&#32553;&#25216;&#26415;&#20316;&#20026;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#32531;&#35299;&#36890;&#20449;&#29942;&#39048;&#30340;&#24517;&#19981;&#21487;&#23569;&#30340;&#24037;&#20855;&#32780;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20559;&#21387;&#21387;&#32553;&#22120;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#34920;&#29616;&#20986;&#27604;&#34987;&#24191;&#27867;&#30740;&#31350;&#21644;&#29702;&#35299;&#30340;&#26080;&#20559;&#21387;&#21387;&#32553;&#22120;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#20102;&#35299;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31867;&#20559;&#21387;&#21387;&#32553;&#31639;&#23376;&#65292;&#20854;&#20013;&#20004;&#31867;&#26159;&#26032;&#30340;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#21644;&#20998;&#24067;&#24335;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#20559;&#21387;&#21387;&#32553;&#22120;&#21487;&#20197;&#22312;&#21333;&#33410;&#28857;&#21644;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32463;&#36807;&#38169;&#35823;&#21453;&#39304;&#26426;&#21046;&#22788;&#29702;&#30340;&#20998;&#24067;&#24335;&#21387;&#32553;&#30340;SGD&#26041;&#27861;&#20855;&#26377;&#36951;&#20256;&#36895;&#29575;$O\left( \delta L \exp \left[-\frac{\mu K}{\delta L}\right] + \frac{(C + \delta D)}{K\mu}\right)$&#65292;&#20854;&#20013;$\delta\ge 1$&#26159;&#19968;&#20010;&#36880;&#28176;&#22686;&#38271;&#30340;&#21387;&#32553;&#21442;&#25968;&#65292;m&#26410;&#23436;&#24453;&#32493;
&lt;/p&gt;
&lt;p&gt;
In the last few years, various communication compression techniques have emerged as an indispensable tool helping to alleviate the communication bottleneck in distributed learning. However, despite the fact biased compressors often show superior performance in practice when compared to the much more studied and understood unbiased compressors, very little is known about them. In this work we study three classes of biased compression operators, two of which are new, and their performance when applied to (stochastic) gradient descent and distributed (stochastic) gradient descent. We show for the first time that biased compressors can lead to linear convergence rates both in the single node and distributed settings. We prove that distributed compressed SGD method, employed with error feedback mechanism, enjoys the ergodic rate $O\left( \delta L \exp \left[-\frac{\mu K}{\delta L}\right] + \frac{(C + \delta D)}{K\mu}\right)$, where $\delta\ge 1$ is a compression parameter which grows when m
&lt;/p&gt;</description></item><item><title>PVNet&#26159;&#19968;&#31181;&#21033;&#29992;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#25968;&#25454;&#30340;LRCN&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#30340;&#20809;&#20239;&#21457;&#30005;&#21151;&#29575;&#12290;&#35813;&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22825;&#27668;&#25968;&#25454;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/1902.01453</link><description>&lt;p&gt;
PVNet: &#22522;&#20110;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#30340;&#26102;&#31354;&#20809;&#20239;&#21457;&#30005;&#21151;&#29575;&#39044;&#27979;&#30340; LRCN &#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
PVNet: A LRCN Architecture for Spatio-Temporal Photovoltaic PowerForecasting from Numerical Weather Prediction. (arXiv:1902.01453v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.01453
&lt;/p&gt;
&lt;p&gt;
PVNet&#26159;&#19968;&#31181;&#21033;&#29992;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#25968;&#25454;&#30340;LRCN&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#30340;&#20809;&#20239;&#21457;&#30005;&#21151;&#29575;&#12290;&#35813;&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22825;&#27668;&#25968;&#25454;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#20239;&#21457;&#30005;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#20043;&#19968;&#65292;&#28982;&#32780;&#20854;&#20135;&#37327;&#21463;&#22825;&#27668;&#26465;&#20214;&#65288;&#22914;&#22826;&#38451;&#36752;&#23556;&#21644;&#28201;&#24230;&#65289;&#30340;&#24433;&#21709;&#32780;&#20855;&#26377;&#39640;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#21363;&#20351;&#26159;&#22312;24&#23567;&#26102;&#39044;&#27979;&#20013;&#65292;&#20809;&#20239;&#21457;&#30005;&#30340;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#23548;&#33268;&#33021;&#28304;&#20379;&#24212;&#21830;&#38656;&#35201;&#21551;&#21160;&#65288;&#24448;&#24448;&#36824;&#20250;&#25490;&#25918;&#30899;&#65289;&#30340;&#21457;&#30005;&#21378;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#30340;&#38271;&#26399;&#24490;&#29615;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#30340;&#20809;&#20239;&#21457;&#30005;&#39044;&#27979;&#12290;&#35813;&#32593;&#32476;&#26550;&#26500;&#20805;&#20998;&#21033;&#29992;&#20102;&#25972;&#20010;&#24863;&#20852;&#36259;&#22320;&#21306;&#19978;&#37319;&#26679;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22825;&#27668;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#32654;&#22269;&#22269;&#23478;&#28023;&#27915;&#21644;&#22823;&#27668;&#31649;&#29702;&#23616;&#65288;NOAA&#65289;&#30340;NWP&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#24503;&#22269;&#30340;&#31354;&#38388;&#32858;&#21512;&#20809;&#20239;&#21457;&#30005;&#37327;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#25345;&#32493;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photovoltaic (PV) power generation has emerged as one of the lead renewable energy sources. Yet, its production is characterized by high uncertainty, being dependent on weather conditions like solar irradiance and temperature. Predicting PV production, even in the 24-hour forecast, remains a challenge and leads energy providers to left idling - often carbon emitting - plants. In this paper, we introduce a Long-Term Recurrent Convolutional Network using Numerical Weather Predictions (NWP) to predict, in turn, PV production in the 24-hour and 48-hour forecast horizons. This network architecture fully leverages both temporal and spatial weather data, sampled over the whole geographical area of interest. We train our model on an NWP dataset from the National Oceanic and Atmospheric Administration (NOAA) to predict spatially aggregated PV production in Germany. We compare its performance to the persistence model and state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#24418;&#24335;&#21644;&#21160;&#37327;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22810;&#20010;&#21608;&#26399;&#20869;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#20445;&#35777;&#27867;&#21270;&#24615;&#33021;&#30340;&#20462;&#25913;&#21518;&#30340;&#21160;&#37327;&#26356;&#26032;&#35268;&#21017;&#12290;&#23545;&#20110;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26631;&#20934;&#30340;&#24102;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20063;&#33021;&#22815;&#20855;&#26377;&#27867;&#21270;&#24615;&#33021;&#12290;&#35813;&#35770;&#25991;&#36824;&#32473;&#20986;&#20102;&#23545;&#20110;&#26399;&#26395;&#30495;&#23454;&#39118;&#38505;&#30340;&#19978;&#30028;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/1809.04564</link><description>&lt;p&gt;
&#20851;&#20110;&#24102;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization of Stochastic Gradient Descent with Momentum. (arXiv:1809.04564v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1809.04564
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#24418;&#24335;&#21644;&#21160;&#37327;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22810;&#20010;&#21608;&#26399;&#20869;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#20445;&#35777;&#27867;&#21270;&#24615;&#33021;&#30340;&#20462;&#25913;&#21518;&#30340;&#21160;&#37327;&#26356;&#26032;&#35268;&#21017;&#12290;&#23545;&#20110;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26631;&#20934;&#30340;&#24102;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20063;&#33021;&#22815;&#20855;&#26377;&#27867;&#21270;&#24615;&#33021;&#12290;&#35813;&#35770;&#25991;&#36824;&#32473;&#20986;&#20102;&#23545;&#20110;&#26399;&#26395;&#30495;&#23454;&#39118;&#38505;&#30340;&#19978;&#30028;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#22522;&#20110;&#21160;&#37327;&#30340;&#21152;&#36895;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21464;&#31181;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#20960;&#20046;&#27809;&#26377;&#29702;&#35770;&#19978;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#23384;&#22312;&#19968;&#31181;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#20110;&#22810;&#20010;&#21608;&#26399;&#30340;&#26631;&#20934;&#37325;&#29699;&#21160;&#37327;&#65288;SGDM&#65289;SGD&#65292;&#20854;&#31283;&#23450;&#38388;&#38553;&#21464;&#24471;&#26080;&#38480;&#22823;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#24179;&#28369;Lipschitz&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#21363;SGD&#25552;&#21069;&#21160;&#37327;&#65288;SGDEM&#65289;&#65292;&#22312;&#24191;&#27867;&#30340;&#27493;&#38271;&#33539;&#22260;&#20869;&#65292;&#23427;&#21487;&#20197;&#22312;&#22810;&#20010;&#21608;&#26399;&#20869;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#20445;&#35777;&#27867;&#21270;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#21160;&#37327;&#33539;&#22260;&#65292;&#20351;&#24471;&#22810;&#20010;&#21608;&#26399;&#30340;&#26631;&#20934;SGDM&#65292;&#20316;&#20026;SGDEM&#30340;&#29305;&#27530;&#24418;&#24335;&#65292;&#20063;&#20855;&#26377;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#27867;&#21270;&#24615;&#33021;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#23545;&#26399;&#26395;&#30495;&#23454;&#39118;&#38505;&#36827;&#34892;&#20102;&#19968;&#20010;&#19978;&#30028;&#65292;&#19982;&#35757;&#32451;&#27493;&#39588;&#25968;&#37327;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
While momentum-based accelerated variants of stochastic gradient descent (SGD) are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In this work, we first show that there exists a convex loss function for which the stability gap for multiple epochs of SGD with standard heavy-ball momentum (SGDM) becomes unbounded. Then, for smooth Lipschitz loss functions, we analyze a modified momentum-based update rule, i.e., SGD with early momentum (SGDEM) under a broad range of step-sizes, and show that it can train machine learning models for multiple epochs with a guarantee for generalization. Finally, for the special case of strongly convex loss functions, we find a range of momentum such that multiple epochs of standard SGDM, as a special form of SGDEM, also generalizes. Extending our results on generalization, we also develop an upper bound on the expected true risk, in terms of the number of training step
&lt;/p&gt;</description></item></channel></rss>