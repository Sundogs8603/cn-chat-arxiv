<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2311.01017</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#25945;&#20250;&#26234;&#33021;&#20307;&#19990;&#30028;&#30340;&#36816;&#20316;&#26041;&#24335;&#12290;&#23613;&#31649;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20294;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#19982;&#20351;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#25193;&#23637;&#19990;&#30028;&#27169;&#22411;&#30340;&#36827;&#23637;&#30456;&#23545;&#36739;&#24930;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#65306;&#22788;&#29702;&#22797;&#26434;&#21644;&#26080;&#32467;&#26500;&#30340;&#35266;&#23519;&#31354;&#38388;&#20197;&#21450;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;&#39318;&#20808;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#28982;&#21518;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#24182;&#34892;&#35299;&#30721;&#21644;&#21435;&#22122;&#26631;&#35760;&#65292;&#25105;&#20204;&#23558;&#36974;&#34109;&#29983;&#25104;&#22270;&#20687;&#36716;&#25442;&#22120;&#36716;&#25442;&#20026;&#31163;&#25955;&#25193;&#25955;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#31616;&#21333;&#30340;&#25913;&#36827;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;&#24403;&#24212;&#29992;&#20110;&#28857;&#20113;&#35266;&#23519;&#30340;&#19990;&#30028;&#27169;&#22411;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#21160;&#24494;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#65292;&#23558;&#25152;&#38656;&#23548;&#25968;&#30340;&#22797;&#26434;&#24230;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00860</link><description>&lt;p&gt;
&#38646;&#22352;&#26631;&#31227;&#21160;&#65306;&#38024;&#23545;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#20248;&#21270;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero Coordinate Shift: Whetted Automatic Differentiation for Physics-informed Operator Learning. (arXiv:2311.00860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#21160;&#24494;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#65292;&#23558;&#25152;&#38656;&#23548;&#25968;&#30340;&#22797;&#26434;&#24230;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#65288;AD&#65289;&#26159;&#29289;&#29702;&#32422;&#26463;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#29992;&#20110;&#35745;&#31639;&#32593;&#32476;&#36755;&#20986;&#30456;&#23545;&#20110;&#22352;&#26631;&#30340;&#39640;&#38454;&#23548;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36731;&#37327;&#32423;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#38024;&#23545;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#33258;&#21160;&#24494;&#20998;&#65292;&#31216;&#20026;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#12290;ZCS&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#37327;&#20540;&#30340;&#21494;&#21464;&#37327;&#65292;&#29992;&#20110;&#27599;&#20010;&#31354;&#38388;&#25110;&#26102;&#38388;&#32500;&#24230;&#65292;&#36890;&#36807;&#23558;&#25152;&#38656;&#23548;&#25968;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#24040;&#22823;&#25552;&#21319;&#12290;ZCS&#24456;&#23481;&#26131;&#22312;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#23454;&#29616;&#65307;&#25105;&#20204;&#20351;&#29992;DeepXDE&#36719;&#20214;&#21253;&#36827;&#34892;&#20102;&#33258;&#24049;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#20998;&#26512;&#21644;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#35757;&#32451;&#29289;&#29702;&#32422;&#26463;&#30340;DeepONets&#26469;&#35299;&#20915;&#26080;&#25968;&#25454;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ZCS&#19968;&#30452;&#36890;&#36807;&#38477;&#20302;GPU&#20869;&#23384;&#28040;&#32791;&#25552;&#20379;&#20102;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic differentiation (AD) is a critical step in physics-informed machine learning, required for computing the high-order derivatives of network output w.r.t. coordinates. In this paper, we present a novel and lightweight algorithm to conduct such AD for physics-informed operator learning, as we call the trick of Zero Coordinate Shift (ZCS). Instead of making all sampled coordinates leaf variables, ZCS introduces only one scalar-valued leaf variable for each spatial or temporal dimension, leading to a game-changing performance leap by simplifying the wanted derivatives from "many-roots-many-leaves" to "one-root-many-leaves". ZCS is easy to implement with current deep learning libraries; our own implementation is by extending the DeepXDE package. We carry out a comprehensive benchmark analysis and several case studies, training physics-informed DeepONets to solve partial differential equations (PDEs) without data. The results show that ZCS has persistently brought down GPU memory co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;VAE&#20013;&#32534;&#30721;&#22120;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.19653</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#30340;&#26080;&#38480;&#25968;&#25454;&#35745;&#21010;&#21319;&#32423;VAE&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models. (arXiv:2310.19653v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19653
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;VAE&#20013;&#32534;&#30721;&#22120;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#20854;&#32534;&#30721;&#22120;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#65288;&#36830;&#32493;&#65289;&#25968;&#25454;&#20998;&#24067;$p_{\mathrm{data}}(\mathbf{x})$&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#22266;&#23450;&#32534;&#30721;&#22120;&#36991;&#20813;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#34920;&#31034;&#19981;&#22826;&#21487;&#35299;&#37322;&#65292;&#20294;&#31616;&#21270;&#20102;&#35757;&#32451;&#65292;&#21487;&#20197;&#31934;&#30830;&#21644;&#36830;&#32493;&#22320;&#36924;&#36817;$p_{\mathrm{data}}(\mathbf{x})$&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#35757;&#32451;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;VAE&#20013;&#32534;&#30721;&#22120;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#20123;&#20986;&#20154;&#24847;&#26009;&#65292;&#22240;&#20026;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;&#21478;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#65292;&#29983;&#25104;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#30340;VAE&#30340;&#27867;&#21270;&#24615;&#33021;&#12289;&#20998;&#25674;&#24046;&#36317;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders (VAEs) are popular models for representation learning but their encoders are susceptible to overfitting (Cremer et al., 2018) because they are trained on a finite training set instead of the true (continuous) data distribution $p_{\mathrm{data}}(\mathbf{x})$. Diffusion models, on the other hand, avoid this issue by keeping the encoder fixed. This makes their representations less interpretable, but it simplifies training, enabling accurate and continuous approximations of $p_{\mathrm{data}}(\mathbf{x})$. In this paper, we show that overfitting encoders in VAEs can be effectively mitigated by training on samples from a pre-trained diffusion model. These results are somewhat unexpected as recent findings (Alemohammad et al., 2023; Shumailov et al., 2023) observe a decay in generative performance when models are trained on data generated by another generative model. We analyze generalization performance, amortization gap, and robustness of VAEs trained with our pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20462;&#27491;&#20102;Koopman&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21608;&#26399;&#37325;&#26032;&#32534;&#30721;&#8221;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.15386</link><description>&lt;p&gt;
&#20462;&#27491;Koopman&#34920;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Course Correcting Koopman Representations. (arXiv:2310.15386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20462;&#27491;&#20102;Koopman&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21608;&#26399;&#37325;&#26032;&#32534;&#30721;&#8221;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#34920;&#31034;&#26088;&#22312;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#23548;&#33268;&#28508;&#22312;&#31354;&#38388;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#31616;&#21270;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#24314;&#27169;&#21644;&#25511;&#21046;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27492;&#38382;&#39064;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#24314;&#27169;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#19981;&#21516;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21608;&#26399;&#37325;&#26032;&#32534;&#30721;&#8221;&#30340;&#25512;&#29702;&#26102;&#38388;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#38271;&#26399;&#21160;&#24577;&#30340;&#20934;&#30830;&#25429;&#25417;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Koopman representations aim to learn features of nonlinear dynamical systems (NLDS) which lead to linear dynamics in the latent space. Theoretically, such features can be used to simplify many problems in modeling and control of NLDS. In this work we study autoencoder formulations of this problem, and different ways they can be used to model dynamics, specifically for future state prediction over long horizons. We discover several limitations of predicting future states in the latent space and propose an inference-time mechanism, which we refer to as Periodic Reencoding, for faithfully capturing long term dynamics. We justify this method both analytically and empirically via experiments in low and high dimensional NLDS.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29289;&#29702;&#20449;&#24687;&#22270;&#21367;&#31215;&#32593;&#32476;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#20960;&#20309;&#20307;&#20013;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#24182;&#32467;&#21512;&#32463;&#20856;&#25968;&#20540;&#27714;&#35299;&#22120;&#20174;&#32780;&#35299;&#20915;&#20102;&#29289;&#29702;&#20449;&#24687;&#26694;&#26550;&#22312;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.14948</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#22270;&#21367;&#31215;&#32593;&#32476;&#65306;&#38754;&#21521;&#22797;&#26434;&#20960;&#20309;&#30340;&#24191;&#20041;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Graph Convolutional Networks: Towards a generalized framework for complex geometries. (arXiv:2310.14948v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29289;&#29702;&#20449;&#24687;&#22270;&#21367;&#31215;&#32593;&#32476;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#20960;&#20309;&#20307;&#20013;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#24182;&#32467;&#21512;&#32463;&#20856;&#25968;&#20540;&#27714;&#35299;&#22120;&#20174;&#32780;&#35299;&#20915;&#20102;&#29289;&#29702;&#20449;&#24687;&#26694;&#26550;&#22312;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;[9]&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20197;&#21450;&#20182;&#20204;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20043;&#21518;&#65292;&#24050;&#32463;&#26377;&#24456;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#22797;&#26434;&#30340;&#19977;&#32500;&#20960;&#20309;&#20307;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982;&#32463;&#20856;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20256;&#32479;&#25968;&#20540;&#25216;&#26415;&#20013;&#29992;&#20110;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32593;&#26684;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#35777;&#26126;&#20102;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;&#21512;&#29702;&#24615;&#12290;&#22312;&#35777;&#26126;&#20102;&#29289;&#29702;&#20449;&#24687;&#26694;&#26550;&#22312;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#35745;&#31639;PDE&#27531;&#24046;&#26102;&#23384;&#22312;&#38382;&#39064;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#23558;&#32463;&#20856;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#19982;&#29289;&#29702;&#20449;&#24687;&#26694;&#26550;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#35813;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#22312;&#19968;&#20010;&#19981;&#35268;&#21017;&#20960;&#20309;&#20307;&#19978;&#30340;&#19977;&#32500;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the seminal work of [9] and their Physics-Informed neural networks (PINNs), many efforts have been conducted towards solving partial differential equations (PDEs) with Deep Learning models. However, some challenges remain, for instance the extension of such models to complex three-dimensional geometries, and a study on how such approaches could be combined to classical numerical solvers. In this work, we justify the use of graph neural networks for these problems, based on the similarity between these architectures and the meshes used in traditional numerical techniques for solving partial differential equations. After proving an issue with the Physics-Informed framework for complex geometries, during the computation of PDE residuals, an alternative procedure is proposed, by combining classical numerical solvers and the Physics-Informed framework. Finally, we propose an implementation of this approach, that we test on a three-dimensional problem on an irregular geometry.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31890;&#23376;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#36234;&#29420;&#31435;&#26679;&#26412;&#30340;&#24120;&#35265;&#20551;&#35774;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13102</link><description>&lt;p&gt;
&#31890;&#23376;&#24341;&#23548;&#65306;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#22810;&#26679;&#24615;&#37319;&#26679;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models. (arXiv:2310.13102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31890;&#23376;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#36234;&#29420;&#31435;&#26679;&#26412;&#30340;&#24120;&#35265;&#20551;&#35774;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#24191;&#27867;&#25104;&#21151;&#65292;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#21152;&#24555;&#20854;&#37319;&#26679;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33719;&#21462;&#22810;&#26679;&#24615;&#26679;&#26412;&#65292;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#22810;&#27425;&#37319;&#26679;&#65292;&#36825;&#20250;&#36896;&#25104;&#19982;&#37319;&#26679;&#26102;&#38388;&#26080;&#20851;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#22788;&#29702;&#20102;&#22914;&#20309;&#36890;&#36807;&#36229;&#36234;&#29420;&#31435;&#26679;&#26412;&#30340;&#24120;&#35265;&#20551;&#35774;&#26469;&#25552;&#39640;&#22810;&#26679;&#24615;&#21644;&#37319;&#26679;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31890;&#23376;&#24341;&#23548;&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#37319;&#26679;&#30340;&#25193;&#23637;&#65292;&#20854;&#20013;&#30340;&#32852;&#21512;&#31890;&#23376;&#26102;&#21464;&#20301;&#21183;&#24378;&#21046;&#23454;&#29616;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#31890;&#23376;&#24341;&#23548;&#20135;&#29983;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#20197;&#21450;&#23427;&#23545;&#20301;&#21183;&#36873;&#25321;&#30340;&#24433;&#21709;&#21644;&#19982;&#20854;&#20182;&#23398;&#31185;&#26041;&#27861;&#30340;&#32852;&#31995;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#25105;&#20204;&#33021;&#22815;&#22686;&#21152;&#22810;&#26679;&#24615;&#32780;&#19981;&#24433;&#21709;&#36136;&#37327;&#65292;&#24182;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#20013;&#38477;&#20302;&#20102;&#24179;&#22343;13%&#30340;&#20808;&#36827;&#25216;&#26415;&#20013;&#20540;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In light of the widespread success of generative models, a significant amount of research has gone into speeding up their sampling time. However, generative models are often sampled multiple times to obtain a diverse set incurring a cost that is orthogonal to sampling time. We tackle the question of how to improve diversity and sample efficiency by moving beyond the common assumption of independent samples. We propose particle guidance, an extension of diffusion-based generative sampling where a joint-particle time-evolving potential enforces diversity. We analyze theoretically the joint distribution that particle guidance generates, its implications on the choice of potential, and the connections with methods in other disciplines. Empirically, we test the framework both in the setting of conditional image generation, where we are able to increase diversity without affecting quality, and molecular conformer generation, where we reduce the state-of-the-art median error by 13% on average
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#25200;&#21160;&#21644;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.11122</link><description>&lt;p&gt;
&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Amortized Bayesian Inference. (arXiv:2310.11122v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#25200;&#21160;&#21644;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#26159;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#21644;&#20915;&#31574;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#29616;&#20195;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#22522;&#26412;&#36873;&#25321;&#28041;&#21450;&#20284;&#28982;&#20989;&#25968;&#21644;&#20808;&#39564;&#20998;&#24067;&#30340;&#35268;&#33539;&#12289;&#21518;&#39564;&#36924;&#36817;&#22120;&#21644;&#25968;&#25454;&#12290;&#27599;&#20010;&#36873;&#25321;&#37117;&#21487;&#20197;&#26174;&#30528;&#24433;&#21709;&#22522;&#20110;&#27169;&#22411;&#30340;&#25512;&#26029;&#21644;&#21518;&#32493;&#20915;&#31574;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#23558;&#25935;&#24863;&#24615;&#20998;&#26512;&#25972;&#21512;&#21040;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#65288;ABI&#65292;&#21363;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#25311;&#25512;&#26029;&#65289;&#20013;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#26435;&#37325;&#20849;&#20139;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32534;&#30721;&#26367;&#20195;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#25512;&#26029;&#26469;&#35780;&#20272;&#23545;&#21508;&#31181;&#25968;&#25454;&#25200;&#21160;&#25110;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#36125;&#21494;&#26031;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20004;&#20010;&#27493;&#39588;&#37117;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference is a powerful framework for making probabilistic inferences and decisions under uncertainty. Fundamental choices in modern Bayesian workflows concern the specification of the likelihood function and prior distributions, the posterior approximator, and the data. Each choice can significantly influence model-based inference and subsequent decisions, thereby necessitating sensitivity analysis. In this work, we propose a multifaceted approach to integrate sensitivity analyses into amortized Bayesian inference (ABI, i.e., simulation-based inference with neural networks). First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to various data perturbations or pre-processing procedures. In contrast to most other Bayesian approaches, both steps circumvent the costly
&lt;/p&gt;</description></item><item><title>SD-PINN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#24674;&#22797;&#31354;&#38388;&#30456;&#20851;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#31995;&#25968;&#65292;&#26080;&#38656;&#39046;&#22495;&#29305;&#23450;&#30340;&#29289;&#29702;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#23545;&#22122;&#22768;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#31354;&#38388;&#21464;&#21270;&#20302;&#31209;&#20551;&#35774;&#24674;&#22797;&#27809;&#26377;&#21487;&#29992;&#27979;&#37327;&#25968;&#25454;&#30340;&#20301;&#32622;&#30340;&#31995;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.10970</link><description>&lt;p&gt;
SD-PINN: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31354;&#38388;&#30456;&#20851;&#20559;&#24494;&#20998;&#26041;&#31243;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
SD-PINN: Deep Learning based Spatially Dependent PDEs Recovery. (arXiv:2310.10970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10970
&lt;/p&gt;
&lt;p&gt;
SD-PINN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#24674;&#22797;&#31354;&#38388;&#30456;&#20851;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#31995;&#25968;&#65292;&#26080;&#38656;&#39046;&#22495;&#29305;&#23450;&#30340;&#29289;&#29702;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#23545;&#22122;&#22768;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#31354;&#38388;&#21464;&#21270;&#20302;&#31209;&#20551;&#35774;&#24674;&#22797;&#27809;&#26377;&#21487;&#29992;&#27979;&#37327;&#25968;&#25454;&#30340;&#20301;&#32622;&#30340;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#33021;&#22815;&#30452;&#25509;&#20174;&#29289;&#29702;&#27979;&#37327;&#20013;&#24674;&#22797;&#22312;&#25972;&#20010;&#31354;&#38388;&#22495;&#20013;&#20445;&#25345;&#19981;&#21464;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#31995;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;&#30456;&#20851;&#30340;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#31070;&#32463;&#32593;&#32476;&#65288;SD-PINN&#65289;&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#24674;&#22797;&#31354;&#38388;&#30456;&#20851;&#30340;PDE&#31995;&#25968;&#65292;&#28040;&#38500;&#20102;&#23545;&#39046;&#22495;&#29305;&#23450;&#30340;&#29289;&#29702;&#19987;&#19994;&#30693;&#35782;&#30340;&#35201;&#27714;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30001;&#20110;&#21152;&#20837;&#20102;&#29289;&#29702;&#32422;&#26463;&#32780;&#23545;&#22122;&#22768;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#23427;&#36824;&#33021;&#22815;&#23558;PDE&#31995;&#25968;&#30340;&#31354;&#38388;&#21464;&#21270;&#20302;&#31209;&#20551;&#35774;&#32435;&#20837;&#32771;&#34385;&#65292;&#20174;&#32780;&#24674;&#22797;&#27809;&#26377;&#21487;&#29992;&#27979;&#37327;&#25968;&#25454;&#30340;&#20301;&#32622;&#30340;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The physics-informed neural network (PINN) is capable of recovering partial differential equation (PDE) coefficients that remain constant throughout the spatial domain directly from physical measurements. In this work, we propose a spatially dependent physics-informed neural network (SD-PINN), which enables the recovery of coefficients in spatially-dependent PDEs using a single neural network, eliminating the requirement for domain-specific physical expertise. The proposed method exhibits robustness to noise owing to the incorporation of physical constraints. It can also incorporate the low-rank assumption of the spatial variation for the PDE coefficients to recover the coefficients at locations without available measurements.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#27934;&#23519;&#29616;&#35937;&#21487;&#33021;&#26159;&#30001;&#31070;&#32463;&#32593;&#32476;&#20174;&#25042;&#24816;&#35757;&#32451;&#21160;&#24577;&#36807;&#28193;&#21040;&#20016;&#23500;&#30340;&#29305;&#24449;&#23398;&#20064;&#27169;&#24335;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#36319;&#36394;&#36275;&#22815;&#30340;&#32479;&#35745;&#37327;&#65292;&#21457;&#29616;&#27934;&#23519;&#26159;&#22312;&#32593;&#32476;&#39318;&#20808;&#23581;&#35797;&#25311;&#21512;&#26680;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#21518;&#65292;&#36827;&#34892;&#21518;&#26399;&#29305;&#24449;&#23398;&#20064;&#25214;&#21040;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#20043;&#21518;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06110</link><description>&lt;p&gt;
&#20174;&#25042;&#24816;&#21040;&#20016;&#23500;&#35757;&#32451;&#21160;&#24577;&#30340;&#27934;&#23519;&#21147;
&lt;/p&gt;
&lt;p&gt;
Grokking as the Transition from Lazy to Rich Training Dynamics. (arXiv:2310.06110v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06110
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#27934;&#23519;&#29616;&#35937;&#21487;&#33021;&#26159;&#30001;&#31070;&#32463;&#32593;&#32476;&#20174;&#25042;&#24816;&#35757;&#32451;&#21160;&#24577;&#36807;&#28193;&#21040;&#20016;&#23500;&#30340;&#29305;&#24449;&#23398;&#20064;&#27169;&#24335;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#36319;&#36394;&#36275;&#22815;&#30340;&#32479;&#35745;&#37327;&#65292;&#21457;&#29616;&#27934;&#23519;&#26159;&#22312;&#32593;&#32476;&#39318;&#20808;&#23581;&#35797;&#25311;&#21512;&#26680;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#21518;&#65292;&#36827;&#34892;&#21518;&#26399;&#29305;&#24449;&#23398;&#20064;&#25214;&#21040;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#20043;&#21518;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#27934;&#23519;&#29616;&#35937;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25439;&#22833;&#22312;&#27979;&#35797;&#25439;&#22833;&#20043;&#21069;&#22823;&#24133;&#19979;&#38477;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#20174;&#25042;&#24816;&#30340;&#35757;&#32451;&#21160;&#24577;&#36716;&#21464;&#20026;&#20016;&#23500;&#30340;&#29305;&#24449;&#23398;&#20064;&#27169;&#24335;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#19968;&#26426;&#21046;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;Vanilla&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#22810;&#39033;&#24335;&#22238;&#24402;&#38382;&#39064;&#19978;&#36827;&#34892;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#35813;&#35757;&#32451;&#23637;&#29616;&#20102;&#26080;&#27861;&#29992;&#29616;&#26377;&#29702;&#35770;&#35299;&#37322;&#30340;&#27934;&#23519;&#29616;&#35937;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#35813;&#32593;&#32476;&#27979;&#35797;&#25439;&#22833;&#30340;&#36275;&#22815;&#32479;&#35745;&#37327;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#36319;&#36394;&#36825;&#20123;&#32479;&#35745;&#37327;&#25581;&#31034;&#20102;&#27934;&#23519;&#29616;&#35937;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#39318;&#20808;&#23581;&#35797;&#20351;&#29992;&#21021;&#22987;&#29305;&#24449;&#25311;&#21512;&#26680;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#65292;&#25509;&#30528;&#22312;&#35757;&#32451;&#25439;&#22833;&#24050;&#32463;&#24456;&#20302;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21518;&#26399;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#25214;&#21040;&#20102;&#19968;&#20010;&#33021;&#22815;&#27867;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27934;&#23519;&#20135;&#29983;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#29305;&#24449;&#23398;&#20064;&#30340;&#36895;&#29575;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#32553;&#25918;&#32593;&#32476;&#21442;&#25968;&#26469;&#31934;&#30830;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose that the grokking phenomenon, where the train loss of a neural network decreases much earlier than its test loss, can arise due to a neural network transitioning from lazy training dynamics to a rich, feature learning regime. To illustrate this mechanism, we study the simple setting of vanilla gradient descent on a polynomial regression problem with a two layer neural network which exhibits grokking without regularization in a way that cannot be explained by existing theories. We identify sufficient statistics for the test loss of such a network, and tracking these over training reveals that grokking arises in this setting when the network first attempts to fit a kernel regression solution with its initial features, followed by late-time feature learning where a generalizing solution is identified after train loss is already low. We find that the key determinants of grokking are the rate of feature learning -- which can be controlled precisely by parameters that scale the ne
&lt;/p&gt;</description></item><item><title>Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2310.05898</link><description>&lt;p&gt;
&#29422;&#23376;&#31192;&#23494;&#22320;&#35299;&#20915;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#65306;&#27491;&#22914;&#26446;&#38597;&#26222;&#35834;&#22827;&#25152;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05898
&lt;/p&gt;
&lt;p&gt;
Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;Lion&#65288;&#36827;&#21270;&#30340;&#31526;&#21495;&#21160;&#37327;&#65289;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#23427;&#22312;&#35757;&#32451;&#25928;&#26524;&#19978;&#19982;AdamW&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#27491;&#22914;&#25105;&#20204;&#21487;&#20197;&#20174;&#38543;&#26426;&#25628;&#32034;&#31243;&#24207;&#30340;&#32467;&#26524;&#20013;&#26399;&#24453;&#30340;&#65292;Lion&#38598;&#25104;&#20102;&#20960;&#20010;&#29616;&#26377;&#31639;&#27861;&#30340;&#20803;&#32032;&#65292;&#21253;&#25324;&#31526;&#21495;&#21160;&#37327;&#12289;&#29420;&#31435;&#30340;&#26435;&#37325;&#34928;&#20943;&#12289;Polak&#21644;Nesterov&#21160;&#37327;&#65292;&#20294;&#21448;&#19981;&#23646;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#29702;&#35770;&#22522;&#30784;&#20248;&#21270;&#22120;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;Lion&#20316;&#20026;&#24191;&#27867;&#20219;&#21153;&#30340;&#36890;&#29992;&#20248;&#21270;&#22120;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#36825;&#31181;&#32570;&#20047;&#29702;&#35770;&#30340;&#26126;&#30830;&#24615;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#21644;&#25193;&#23637;Lion&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#24320;Lion&#30340;&#31070;&#31192;&#38754;&#32433;&#12290;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;$f(x)$&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#19981;&#21516;&#26465;&#20214;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30446;&#26631;&#30005;&#27744;&#21644;&#21442;&#32771;&#30005;&#27744;&#20043;&#38388;&#30340;&#30005;&#20449;&#21495;&#24046;&#24322;&#65292;&#26080;&#35770;&#26448;&#26009;&#21644;&#32769;&#21270;&#26465;&#20214;&#22914;&#20309;&#65292;&#22312;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#30340;&#21516;&#26102;&#20026;&#36890;&#29992;&#30340;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2310.05052</link><description>&lt;p&gt;
&#36328;&#19981;&#21516;&#26465;&#20214;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#30340;&#23398;&#20064;&#32454;&#32990;&#20869;&#21644;&#32454;&#32990;&#38388;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Learning Intra- and Inter-Cell Differences for Accurate Battery Lifespan Prediction across Diverse Conditions. (arXiv:2310.05052v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#19981;&#21516;&#26465;&#20214;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30446;&#26631;&#30005;&#27744;&#21644;&#21442;&#32771;&#30005;&#27744;&#20043;&#38388;&#30340;&#30005;&#20449;&#21495;&#24046;&#24322;&#65292;&#26080;&#35770;&#26448;&#26009;&#21644;&#32769;&#21270;&#26465;&#20214;&#22914;&#20309;&#65292;&#22312;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#30340;&#21516;&#26102;&#20026;&#36890;&#29992;&#30340;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#23545;&#30005;&#27744;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;&#30446;&#21069;&#65292;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20381;&#36182;&#20110;&#29305;&#23450;&#30446;&#26631;&#30005;&#27744;&#30340;&#26089;&#26399;&#30005;&#20449;&#21495;&#26469;&#39044;&#27979;&#23427;&#20204;&#30340;&#23551;&#21629;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#19981;&#36275;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#29305;&#23450;&#32769;&#21270;&#26465;&#20214;&#24320;&#21457;&#30340;&#65292;&#36825;&#19981;&#20165;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#27169;&#22411;&#33021;&#21147;&#65292;&#32780;&#19988;&#38477;&#20302;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#36864;&#21270;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#20854;&#20182;&#26465;&#20214;&#19979;&#21487;&#29992;&#30340;&#20016;&#23500;&#21382;&#21490;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26126;&#30830;&#25429;&#25417;&#30446;&#26631;&#30005;&#27744;&#21644;&#21442;&#32771;&#30005;&#27744;&#20043;&#38388;&#30340;&#30005;&#20449;&#21495;&#24046;&#24322;&#65292;&#26080;&#35770;&#23427;&#20204;&#30340;&#26448;&#26009;&#21644;&#32769;&#21270;&#26465;&#20214;&#22914;&#20309;&#65292;&#26469;&#39044;&#27979;&#30446;&#26631;&#30005;&#27744;&#30340;&#23551;&#21629;&#12290;&#36890;&#36807;&#36825;&#31181;&#32454;&#32990;&#38388;&#24046;&#24322;&#65292;&#25105;&#20204;&#19981;&#20165;&#25193;&#23637;&#20102;&#29305;&#24449;&#31354;&#38388;&#65292;&#36824;&#20026;&#36890;&#29992;&#30340;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26174;&#33879;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;
Battery life prediction holds significant practical value for battery research and development. Currently, many data-driven models rely on early electrical signals from specific target batteries to predict their lifespan. A common shortfall is that most existing methods are developed based on specific aging conditions, which not only limits their model's capability but also diminishes their effectiveness in predicting degradation under varied conditions. As a result, these models often miss out on fully benefiting from the rich historical data available under other conditions. Here, to address above, we introduce an approach that explicitly captures differences between electrical signals of a target battery and a reference battery, irrespective of their materials and aging conditions, to forecast the target battery life. Through this inter-cell difference, we not only enhance the feature space but also pave the way for a universal battery life prediction framework. Remarkably, our mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#20301;&#32622;-&#26041;&#21521;&#31354;&#38388;&#20013;&#20849;&#20139;&#26435;&#37325;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#34920;&#36798;&#21147;&#24378;&#30340;SE$(n)$&#31561;&#21464;&#32593;&#32476;&#12290;&#20182;&#20204;&#22522;&#20110;&#21516;&#24577;&#31354;&#38388;&#29702;&#35770;&#65292;&#25512;&#23548;&#20986;&#20960;&#20309;&#20248;&#21270;&#30340;&#36793;&#23646;&#24615;&#65292;&#24182;&#23558;&#26435;&#37325;&#20849;&#20139;&#24418;&#24335;&#21270;&#20026;&#23545;&#31561;&#22788;&#29702;&#30456;&#21516;&#28857;&#23545;&#30340;&#28040;&#24687;&#20989;&#25968;&#12290;&#20182;&#20204;&#22312;&#22788;&#29702;3D&#28857;&#20113;&#26102;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31561;&#21464;&#32676;&#21367;&#31215;&#32593;&#32476;&#65292;&#24182;&#36873;&#25321;&#20102;$\mathbb{R}^3 {\times} S^2$&#20316;&#20026;&#26368;&#20339;&#30340;&#22788;&#29702;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.02970</link><description>&lt;p&gt;
&#24555;&#36895;&#12289;&#34920;&#36798;&#21147;&#24378;&#30340;SE$(n)$&#31561;&#21464;&#32593;&#32476;&#36890;&#36807;&#22312;&#20301;&#32622;-&#26041;&#21521;&#31354;&#38388;&#20013;&#20849;&#20139;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space. (arXiv:2310.02970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#20301;&#32622;-&#26041;&#21521;&#31354;&#38388;&#20013;&#20849;&#20139;&#26435;&#37325;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#34920;&#36798;&#21147;&#24378;&#30340;SE$(n)$&#31561;&#21464;&#32593;&#32476;&#12290;&#20182;&#20204;&#22522;&#20110;&#21516;&#24577;&#31354;&#38388;&#29702;&#35770;&#65292;&#25512;&#23548;&#20986;&#20960;&#20309;&#20248;&#21270;&#30340;&#36793;&#23646;&#24615;&#65292;&#24182;&#23558;&#26435;&#37325;&#20849;&#20139;&#24418;&#24335;&#21270;&#20026;&#23545;&#31561;&#22788;&#29702;&#30456;&#21516;&#28857;&#23545;&#30340;&#28040;&#24687;&#20989;&#25968;&#12290;&#20182;&#20204;&#22312;&#22788;&#29702;3D&#28857;&#20113;&#26102;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31561;&#21464;&#32676;&#21367;&#31215;&#32593;&#32476;&#65292;&#24182;&#36873;&#25321;&#20102;$\mathbb{R}^3 {\times} S^2$&#20316;&#20026;&#26368;&#20339;&#30340;&#22788;&#29702;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22522;&#20110;&#21516;&#24577;&#31354;&#38388;&#29702;&#35770;&#25512;&#23548;&#20986;&#29992;&#20110;&#28789;&#27963;&#30340;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#8220;&#20960;&#20309;&#20248;&#21270;&#36793;&#23646;&#24615;&#8221;&#12290;&#25105;&#20204;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#20849;&#20139;&#24418;&#24335;&#21270;&#20026;&#23545;&#31561;&#22320;&#22788;&#29702;&#24182;&#19988;&#24212;&#35813;&#34987;&#24179;&#31561;&#23545;&#24453;&#30340;&#28857;&#23545;&#30340;&#28040;&#24687;&#20989;&#25968;&#20849;&#20139;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#31561;&#20215;&#31867;&#65292;&#36825;&#20123;&#31561;&#20215;&#31867;&#22312;&#32676;&#20013;&#36827;&#34892;&#21464;&#25442;&#26102;&#26159;&#30456;&#21516;&#30340;&#65292;&#24182;&#19988;&#25512;&#23548;&#20986;&#21807;&#19968;&#26631;&#35782;&#36825;&#20123;&#31867;&#21035;&#30340;&#23646;&#24615;&#12290;&#36890;&#36807;&#22312;&#36825;&#20123;&#23646;&#24615;&#19978;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#21487;&#20197;&#23454;&#29616;&#26435;&#37325;&#20849;&#20139;&#12290;&#20316;&#20026;&#35813;&#29702;&#35770;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31561;&#21464;&#32676;&#21367;&#31215;&#32593;&#32476;&#26469;&#22788;&#29702;3D&#28857;&#20113;&#12290;&#21516;&#24577;&#31354;&#38388;&#29702;&#35770;&#21578;&#35785;&#25105;&#20204;&#22914;&#20309;&#22312;&#20301;&#32622;$\mathbb{R}^3$&#12289;&#20301;&#32622;&#21644;&#26041;&#21521;$\mathbb{R}^3 {\times} S^2$&#30340;&#21516;&#24577;&#31354;&#38388;&#20197;&#21450;&#32676;SE$(3)$&#19978;&#30340;&#29305;&#24449;&#22270;&#19978;&#36827;&#34892;&#32676;&#21367;&#31215;&#12290;&#22312;&#36825;&#20123;&#36873;&#25321;&#20013;&#65292;$\mathbb{R}^3 {\times} S^2$&#26159;&#19968;&#20010;&#26368;&#20339;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#22788;&#29702;&#26041;&#21521;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the theory of homogeneous spaces we derive \textit{geometrically optimal edge attributes} to be used within the flexible message passing framework. We formalize the notion of weight sharing in convolutional networks as the sharing of message functions over point-pairs that should be treated equally. We define equivalence classes of point-pairs that are identical up to a transformation in the group and derive attributes that uniquely identify these classes. Weight sharing is then obtained by conditioning message functions on these attributes. As an application of the theory, we develop an efficient equivariant group convolutional network for processing 3D point clouds. The theory of homogeneous spaces tells us how to do group convolutions with feature maps over the homogeneous space of positions $\mathbb{R}^3$, position and orientations $\mathbb{R}^3 {\times} S^2$, and the group SE$(3)$ itself. Among these, $\mathbb{R}^3 {\times} S^2$ is an optimal choice due to the ability to 
&lt;/p&gt;</description></item><item><title>EGraFFBench&#23545;&#20845;&#31181;EGraFF&#31639;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.02428</link><description>&lt;p&gt;
EGraFFBench: &#29992;&#20110;&#21407;&#23376;&#27169;&#25311;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#21147;&#22330;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations. (arXiv:2310.02428v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02428
&lt;/p&gt;
&lt;p&gt;
EGraFFBench&#23545;&#20845;&#31181;EGraFF&#31639;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#22266;&#26377;&#23545;&#31216;&#24615;&#65292;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#21147;&#22330;(EGraFF)&#22312;&#24314;&#27169;&#21407;&#23376;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#26032;&#22411;&#26550;&#26500;&#30340;&#24320;&#21457;&#28526;&#65292;&#36825;&#20123;&#26550;&#26500;&#23558;&#31561;&#21464;&#24615;&#30340;&#24402;&#32435;&#20559;&#35265;&#19982;&#22270;&#21464;&#25442;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#31561;&#26550;&#26500;&#21019;&#26032;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24314;&#27169;&#21407;&#23376;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30446;&#21069;&#23545;&#36825;&#20123;&#20351;&#29992;EGraFF&#36827;&#34892;&#23454;&#38469;&#21407;&#23376;&#27169;&#25311;&#20219;&#21153;&#30340;&#24443;&#24213;&#35780;&#20272;&#36824;&#32570;&#20047;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#23545;6&#31181;EGraFF&#31639;&#27861;(NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet)&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#23454;&#38469;&#21407;&#23376;&#27169;&#25311;&#20013;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;&#38500;&#20102;&#23545;&#22522;&#20110;&#22522;&#20934;&#27979;&#35797;&#25991;&#29486;&#30340;&#20843;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#21644;&#20998;&#26512;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#22235;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivariant graph neural networks force fields (EGraFFs) have shown great promise in modelling complex interactions in atomic systems by exploiting the graphs' inherent symmetries. Recent works have led to a surge in the development of novel architectures that incorporate equivariance-based inductive biases alongside architectural innovations like graph transformers and message passing to model atomic interactions. However, thorough evaluations of these deploying EGraFFs for the downstream task of real-world atomistic simulations, is lacking. To this end, here we perform a systematic benchmarking of 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet), with the aim of understanding their capabilities and limitations for realistic atomistic simulations. In addition to our thorough evaluation and analysis on eight existing datasets based on the benchmarking literature, we release two new benchmark datasets, propose four new metrics, and three new challenging tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#22312;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#19979;&#32473;&#20986;&#20102;&#19981;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.01769</link><description>&lt;p&gt;
&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#20943;&#32531;&#30697;&#38453;&#24863;&#30693;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#65306;&#23545;&#31216;&#24615;&#21644;&#21021;&#22987;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization. (arXiv:2310.01769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#22312;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#19979;&#32473;&#20986;&#20102;&#19981;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#38416;&#36848;&#20102;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#25913;&#21464;&#26799;&#24230;&#19979;&#38477;&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#22312;&#23545;&#31216;&#35774;&#32622;&#20013;&#65292;&#36890;&#36807;&#23545;&#31216;&#21442;&#25968;&#21270;&#23398;&#20064;&#26410;&#30693;&#30340;&#21322;&#27491;&#23450;&#30697;&#38453;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65288;$k&gt;r$&#65289;&#38543;&#26426;&#21021;&#22987;&#21270;&#26799;&#24230;&#19979;&#38477;&#30340;&#26032;&#22411;$\Omega (1/T^2)$&#19979;&#30028;&#65292;&#19982;&#31934;&#30830;&#21442;&#25968;&#21270;&#24773;&#20917;&#65288;$k=r$&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;$\exp (-\Omega (T))$&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#23545;&#31216;&#35774;&#32622;&#65292;&#20854;&#20013;$M^* \in \mathbb{R}^{n_1 \times n_2}$&#26159;&#26410;&#30693;&#30697;&#38453;&#65292;&#37319;&#29992;&#38750;&#23545;&#31216;&#21442;&#25968;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper rigorously shows how over-parameterization changes the convergence behaviors of gradient descent (GD) for the matrix sensing problem, where the goal is to recover an unknown low-rank ground-truth matrix from near-isotropic linear measurements. First, we consider the symmetric setting with the symmetric parameterization where $M^* \in \mathbb{R}^{n \times n}$ is a positive semi-definite unknown matrix of rank $r \ll n$, and one uses a symmetric parameterization $XX^\top$ to learn $M^*$. Here $X \in \mathbb{R}^{n \times k}$ with $k &gt; r$ is the factor matrix. We give a novel $\Omega (1/T^2)$ lower bound of randomly initialized GD for the over-parameterized case ($k &gt;r$) where $T$ is the number of iterations. This is in stark contrast to the exact-parameterization scenario ($k=r$) where the convergence rate is $\exp (-\Omega (T))$. Next, we study asymmetric setting where $M^* \in \mathbb{R}^{n_1 \times n_2}$ is the unknown matrix of rank $r \ll \min\{n_1,n_2\}$, and one uses an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#24674;&#22797;&#25110;&#36229;&#36234;&#20102;&#24050;&#30693;&#30340;&#36335;&#24452;&#33539;&#25968;&#30028;&#38480;&#65292;&#24182;&#25361;&#25112;&#20102;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#19968;&#20123;&#20855;&#20307;&#25215;&#35834;&#12290;</title><link>http://arxiv.org/abs/2310.01225</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#20195;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65306;&#24433;&#21709;&#12289;&#21069;&#26223;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A path-norm toolkit for modern networks: consequences, promises and challenges. (arXiv:2310.01225v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#24674;&#22797;&#25110;&#36229;&#36234;&#20102;&#24050;&#30693;&#30340;&#36335;&#24452;&#33539;&#25968;&#30028;&#38480;&#65292;&#24182;&#25361;&#25112;&#20102;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#19968;&#20123;&#20855;&#20307;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#33021;&#22815;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#36866;&#29992;&#20110;&#26368;&#24191;&#27867;&#30340;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#21487;&#20197;&#24674;&#22797;&#25110;&#36229;&#36234;&#24050;&#30693;&#30340;&#27492;&#31867;&#33539;&#25968;&#30340;&#26368;&#23574;&#38160;&#30028;&#38480;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#36335;&#24452;&#33539;&#25968;&#36824;&#20139;&#26377;&#36335;&#24452;&#33539;&#25968;&#30340;&#24120;&#35268;&#20248;&#28857;&#65306;&#35745;&#31639;&#31616;&#20415;&#12289;&#23545;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#22312;&#21069;&#39304;&#32593;&#32476;&#19978;&#27604;&#25805;&#20316;&#31526;&#33539;&#25968;&#30340;&#20056;&#31215;&#65288;&#21478;&#19968;&#31181;&#24120;&#29992;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#38160;&#24230;&#12290;&#24037;&#20855;&#21253;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#26131;&#20110;&#23454;&#26045;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#25968;&#20540;&#35780;&#20272;&#22312;ImageNet&#19978;&#23545;ResNet&#30340;&#26368;&#23574;&#38160;&#30028;&#38480;&#26469;&#25361;&#25112;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#20855;&#20307;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the first toolkit around path-norms that is fully able to encompass general DAG ReLU networks with biases, skip connections and max pooling. This toolkit notably allows us to establish generalization bounds for real modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms further enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on feedforward networks compared to the product of operators' norms, another complexity measure most commonly used.  The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#30340;&#22122;&#22768;&#20960;&#20309;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#21457;&#29616;&#22122;&#22768;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#20960;&#20309;&#29305;&#24449;&#26377;&#21033;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SGD&#22312;&#36867;&#33073;&#23574;&#38160;&#26497;&#23567;&#20540;&#26102;&#19982;GD&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#36867;&#33073;&#26041;&#21521;&#22312;&#24179;&#22374;&#26041;&#21521;&#19978;&#26377;&#26174;&#33879;&#20998;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.00692</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#22122;&#22768;&#20960;&#20309;&#65306;&#23450;&#37327;&#21644;&#20998;&#26512;&#29305;&#24449;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization. (arXiv:2310.00692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#30340;&#22122;&#22768;&#20960;&#20309;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#21457;&#29616;&#22122;&#22768;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#20960;&#20309;&#29305;&#24449;&#26377;&#21033;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SGD&#22312;&#36867;&#33073;&#23574;&#38160;&#26497;&#23567;&#20540;&#26102;&#19982;GD&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#36867;&#33073;&#26041;&#21521;&#22312;&#24179;&#22374;&#26041;&#21521;&#19978;&#26377;&#26174;&#33879;&#20998;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#30340;&#22122;&#22768;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#20960;&#20309;&#29305;&#24449;&#26377;&#21033;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#29616;&#35937;&#30340;&#29702;&#35770;&#21644;&#23450;&#37327;&#35299;&#37322;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#23545;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#19978;&#36848;&#8220;&#22122;&#22768;&#20960;&#20309;&#8221;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#32454;&#33268;&#22320;&#30740;&#31350;&#20102;&#24179;&#22343;&#21644;&#26041;&#21521;&#30340;&#19968;&#33268;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#26679;&#26412;&#22823;&#23567;&#21644;&#36755;&#20837;&#25968;&#25454;&#36864;&#21270;&#23545;&#19968;&#33268;&#24615;&#24378;&#24230;&#30340;&#24433;&#21709;&#12290;&#20316;&#20026;&#29305;&#23450;&#24212;&#29992;&#65292;&#25105;&#20204;&#21033;&#29992;&#22122;&#22768;&#20960;&#20309;&#29305;&#24449;&#30740;&#31350;&#20102;SGD&#22914;&#20309;&#20174;&#23574;&#38160;&#26497;&#23567;&#20540;&#20013;&#36867;&#33073;&#65292;&#21457;&#29616;&#36867;&#33073;&#26041;&#21521;&#22312;&#24179;&#22374;&#26041;&#21521;&#19978;&#26377;&#26174;&#33879;&#20998;&#37327;&#65292;&#36825;&#19982;&#21482;&#22312;&#26368;&#23574;&#38160;&#26041;&#21521;&#36867;&#33073;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;GD&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical studies have demonstrated that the noise in stochastic gradient descent (SGD) aligns favorably with the local geometry of loss landscape. However, theoretical and quantitative explanations for this phenomenon remain sparse. In this paper, we offer a comprehensive theoretical investigation into the aforementioned {\em noise geometry} for over-parameterized linear (OLMs) models and two-layer neural networks. We scrutinize both average and directional alignments, paying special attention to how factors like sample size and input data degeneracy affect the alignment strength. As a specific application, we leverage our noise geometry characterizations to study how SGD escapes from sharp minima, revealing that the escape direction has significant components along flat directions. This is in stark contrast to GD, which escapes only along the sharpest directions. To substantiate our theoretical findings, both synthetic and real-world experiments are provided.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20849;&#20248;&#21270;&#25968;&#25454;&#27969;&#21644;SIMD&#23454;&#29616;&#26469;&#39640;&#25928;&#22320;&#22312;CPU&#19978;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#22823;&#24133;&#25552;&#21319;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.00574</link><description>&lt;p&gt;
SIMD&#25968;&#25454;&#27969;&#20849;&#20248;&#21270;&#29992;&#20110;CPU&#19978;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SIMD Dataflow Co-optimization for Efficient Neural Networks Inferences on CPUs. (arXiv:2310.00574v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00574
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20849;&#20248;&#21270;&#25968;&#25454;&#27969;&#21644;SIMD&#23454;&#29616;&#26469;&#39640;&#25928;&#22320;&#22312;CPU&#19978;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#22823;&#24133;&#25552;&#21319;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#22312;CPU&#19978;&#37096;&#32626;&#31070;&#32463;&#32593;&#32476;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#20851;&#27880;&#30340;&#26159;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#25512;&#29702;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#26041;&#27861;&#26159;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#27969;&#65288;&#21363;&#35745;&#31639;&#39034;&#24207;&#65289;&#65292;&#36890;&#36807;&#21551;&#21457;&#24335;&#24341;&#23548;&#20998;&#26512;&#21644;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#26469;&#25506;&#32034;&#25968;&#25454;&#37325;&#29992;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#21508;&#31181;&#21333;&#25351;&#20196;&#22810;&#25968;&#25454;&#65288;SIMD&#65289;&#23454;&#29616;&#20197;&#23454;&#29616;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#36755;&#20986;&#20445;&#25345;&#22312;SIMD&#23492;&#23384;&#22120;&#20013;&#30340;&#25968;&#25454;&#27969;&#21516;&#26102;&#26368;&#22823;&#21270;&#36755;&#20837;&#21644;&#26435;&#37325;&#37325;&#29992;&#65292;&#22312;&#21508;&#31181;&#25512;&#29702;&#24037;&#20316;&#36127;&#36733;&#19979;&#22987;&#32456;&#33021;&#22815;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#65292;&#30456;&#27604;&#20170;&#22825;&#30340;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#23454;&#29616;&#65292;8&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#21152;&#36895;&#27604;&#21487;&#36798;3&#20493;&#65292;&#32780;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#21152;&#36895;&#27604;&#21487;&#36798;4.8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the challenges associated with deploying neural networks on CPUs, with a particular focus on minimizing inference time while maintaining accuracy. Our novel approach is to use the dataflow (i.e., computation order) of a neural network to explore data reuse opportunities using heuristic-guided analysis and a code generation framework, which enables exploration of various Single Instruction, Multiple Data (SIMD) implementations to achieve optimized neural network execution. Our results demonstrate that the dataflow that keeps outputs in SIMD registers while also maximizing both input and weight reuse consistently yields the best performance for a wide variety of inference workloads, achieving up to 3x speedup for 8-bit neural networks, and up to 4.8x speedup for binary neural networks, respectively, over the optimized implementations of neural networks today.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21644;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00116</link><description>&lt;p&gt;
&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization. (arXiv:2310.00116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21644;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20363;&#22914;&#35774;&#35745;&#20855;&#26377;&#26356;&#22909;&#40065;&#26834;&#24615;&#24615;&#36136;&#30340;&#26032;&#26550;&#26500;&#65288;&#20363;&#22914;&#65292;Lipschitz-capped&#32593;&#32476;&#65289;&#25110;&#20462;&#25913;&#35757;&#32451;&#36807;&#31243;&#26412;&#36523;&#65288;&#20363;&#22914;&#65292;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#65292;&#32422;&#26463;&#23398;&#20064;&#25110;&#27491;&#21017;&#21270;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#22686;&#21152;&#36755;&#20837;&#65288;&#29305;&#24449;&#65289;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21487;&#33021;&#24182;&#19981;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#23545;&#24320;&#21457;&#33021;&#22815;&#30452;&#25509;&#25805;&#32437;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#30340;&#35757;&#32451;&#36807;&#31243;&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#35813;&#31867;&#21035;&#30340;&#26368;&#26032;&#21457;&#23637;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#40065;&#26834;&#35757;&#32451;&#31639;&#27861;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#36755;&#20986;&#65288;logit&#65289;&#31354;&#38388;&#20013;&#22686;&#21152;&#36793;&#30028;&#65292;&#24182;&#27839;&#30528;&#33030;&#24369;&#26041;&#21521;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20004;&#20010;&#30446;&#26631;&#21487;&#20197;&#30452;&#25509;&#20419;&#36827;&#36755;&#20837;&#31354;&#38388;&#20013;&#26356;&#22823;&#30340;&#36793;&#30028;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
To improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., Lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). These approaches, however, might not be effective at increasing the margin in the input (feature) space. As a result, there has been an increasing interest in developing training procedures that can directly manipulate the decision boundary in the input space. In this paper, we build upon recent developments in this category by developing a robust training algorithm whose objective is to increase the margin in the output (logit) space while regularizing the Lipschitz constant of the model along vulnerable directions. We show that these two objectives can directly promote larger margins in the input space. To this end, we develop a scalable method for calcula
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;E(3)&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;EQGAT-diff&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#30340;&#21407;&#23376;&#20301;&#32622;&#21644;&#20998;&#31867;&#30340;&#21270;&#23398;&#20803;&#32032;&#19982;&#38190;&#31867;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#20013;&#25913;&#36827;&#65292;&#20854;&#22312;&#20174;&#22836;&#35774;&#35745;3D&#20998;&#23376;&#26041;&#38754;&#30340;&#24615;&#33021;&#26174;&#33879;&#36229;&#36807;&#24050;&#26377;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.17296</link><description>&lt;p&gt;
&#23548;&#33322;&#31561;&#21464;&#25193;&#25955;&#22522;&#29983;&#25104;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#29992;&#20110;&#20174;&#22836;&#29983;&#25104;3D&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation. (arXiv:2309.17296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;E(3)&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;EQGAT-diff&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#30340;&#21407;&#23376;&#20301;&#32622;&#21644;&#20998;&#31867;&#30340;&#21270;&#23398;&#20803;&#32032;&#19982;&#38190;&#31867;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#20013;&#25913;&#36827;&#65292;&#20854;&#22312;&#20174;&#22836;&#35774;&#35745;3D&#20998;&#23376;&#26041;&#38754;&#30340;&#24615;&#33021;&#26174;&#33879;&#36229;&#36807;&#24050;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#26159;&#26448;&#26009;&#31185;&#23398;&#21644;&#33647;&#29289;&#21457;&#29616;&#20013;&#20174;&#22836;&#35774;&#35745;3D&#20998;&#23376;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22823;&#20998;&#23376;&#32467;&#26500;&#21644;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#24615;&#33021;&#20173;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;E(3)&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#37325;&#28857;&#20851;&#27880;&#20197;&#21069;&#30340;&#31354;&#30333;&#28857;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#36830;&#32493;&#21644;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#20010;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;EQGAT-diff&#27169;&#22411;&#65292;&#20854;&#22312;QM9&#21644;GEOM-Drugs&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#22987;&#32456;&#22823;&#22823;&#36229;&#36807;&#24050;&#24314;&#31435;&#27169;&#22411;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;EQGAT-diff&#37319;&#29992;&#36830;&#32493;&#30340;&#21407;&#23376;&#20301;&#32622;&#65292;&#32780;&#21270;&#23398;&#20803;&#32032;&#21644;&#38190;&#31867;&#22411;&#26159;&#20998;&#31867;&#30340;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#30456;&#20851;&#30340;&#25439;&#22833;&#21152;&#26435;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#35757;&#32451;&#25910;&#25947;&#21644;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#23545;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#36866;&#29992;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deep generative diffusion models are a promising avenue for de novo 3D molecular design in material science and drug discovery. However, their utility is still constrained by suboptimal performance with large molecular structures and limited training data. Addressing this gap, we explore the design space of E(3) equivariant diffusion models, focusing on previously blank spots. Our extensive comparative analysis evaluates the interplay between continuous and discrete state spaces. Out of this investigation, we introduce the EQGAT-diff model, which consistently surpasses the performance of established models on the QM9 and GEOM-Drugs datasets by a large margin. Distinctively, EQGAT-diff takes continuous atomic positions while chemical elements and bond types are categorical and employ a time-dependent loss weighting that significantly increases training convergence and the quality of generated samples. To further strengthen the applicability of diffusion models to limited training data, 
&lt;/p&gt;</description></item><item><title>&#35282;&#36793;&#36317;&#25439;&#22833;&#19982;&#36741;&#21161;&#20219;&#21153;&#32467;&#21512;&#22312;&#21322;&#30417;&#30563;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#35282;&#36793;&#36317;&#25439;&#22833;&#21516;&#26102;&#36798;&#21040;&#26368;&#23567;&#21270;&#32039;&#20945;&#24615;&#25439;&#22833;&#21644;&#38450;&#27490;&#23398;&#20064;&#24179;&#20961;&#35299;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15643</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35282;&#36793;&#36317;&#25439;&#22833;&#22312;&#21322;&#30417;&#30563;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why do Angular Margin Losses work well for Semi-Supervised Anomalous Sound Detection?. (arXiv:2309.15643v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15643
&lt;/p&gt;
&lt;p&gt;
&#35282;&#36793;&#36317;&#25439;&#22833;&#19982;&#36741;&#21161;&#20219;&#21153;&#32467;&#21512;&#22312;&#21322;&#30417;&#30563;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#35282;&#36793;&#36317;&#25439;&#22833;&#21516;&#26102;&#36798;&#21040;&#26368;&#23567;&#21270;&#32039;&#20945;&#24615;&#25439;&#22833;&#21644;&#38450;&#27490;&#23398;&#20064;&#24179;&#20961;&#35299;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#31995;&#32479;&#36890;&#24120;&#21033;&#29992;&#35282;&#36793;&#36317;&#25439;&#22833;&#26469;&#36890;&#36807;&#19968;&#20010;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#21512;&#36866;&#30340;&#22768;&#23398;&#25968;&#25454;&#34920;&#31034;&#65292;&#35813;&#20219;&#21153;&#36890;&#24120;&#26159;&#19968;&#20010;&#30417;&#30563;&#25110;&#33258;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#12290;&#20854;&#22522;&#26412;&#24605;&#24819;&#26159;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#36741;&#21161;&#20219;&#21153;&#65292;&#38656;&#35201;&#22312;&#23398;&#20064;&#30340;&#34920;&#31034;&#20013;&#25429;&#25417;&#21040;&#20851;&#20110;&#27491;&#24120;&#25968;&#25454;&#30340;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#19988;&#36825;&#20123;&#20449;&#24687;&#20063;&#36275;&#20197;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#26679;&#26412;&#12290;&#29305;&#21035;&#26159;&#22312;&#22122;&#22768;&#26465;&#20214;&#19979;&#65292;&#22522;&#20110;&#35282;&#36793;&#36317;&#25439;&#22833;&#30340;&#21028;&#21035;&#27169;&#22411;&#24448;&#24448;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#25110;&#21333;&#31867;&#27169;&#22411;&#30340;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;&#20026;&#20160;&#20040;&#22312;&#36741;&#21161;&#20219;&#21153;&#20013;&#20351;&#29992;&#35282;&#36793;&#36317;&#25439;&#22833;&#23545;&#20110;&#26816;&#27979;&#24322;&#24120;&#22768;&#38899;&#25928;&#26524;&#33391;&#22909;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#26368;&#23567;&#21270;&#35282;&#36793;&#36317;&#25439;&#22833;&#20063;&#26368;&#23567;&#21270;&#20102;&#32039;&#20945;&#24615;&#25439;&#22833;&#65292;&#21516;&#26102;&#22266;&#26377;&#22320;&#38450;&#27490;&#23398;&#20064;&#24179;&#20961;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;m
&lt;/p&gt;
&lt;p&gt;
State-of-the-art anomalous sound detection systems often utilize angular margin losses to learn suitable representations of acoustic data using an auxiliary task, which usually is a supervised or self-supervised classification task. The underlying idea is that, in order to solve this auxiliary task, specific information about normal data needs to be captured in the learned representations and that this information is also sufficient to differentiate between normal and anomalous samples. Especially in noisy conditions, discriminative models based on angular margin losses tend to significantly outperform systems based on generative or one-class models. The goal of this work is to investigate why using angular margin losses with auxiliary tasks works well for detecting anomalous sounds. To this end, it is shown, both theoretically and experimentally, that minimizing angular margin losses also minimizes compactness loss while inherently preventing learning trivial solutions. Furthermore, m
&lt;/p&gt;</description></item><item><title>&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#26159;&#19968;&#31181;&#37319;&#29992;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#27169;&#22359;&#29305;&#21270;&#21644;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24418;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.12862</link><description>&lt;p&gt;
&#20851;&#32852;&#21464;&#25442;&#22120;&#26159;&#19968;&#31181;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Associative Transformer Is A Sparse Representation Learner. (arXiv:2309.12862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12862
&lt;/p&gt;
&lt;p&gt;
&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#26159;&#19968;&#31181;&#37319;&#29992;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#27169;&#22359;&#29305;&#21270;&#21644;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#20013;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22522;&#20110;&#31232;&#30095;&#20132;&#20114;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#29983;&#29289;&#21407;&#29702;&#26356;&#20026;&#25509;&#36817;&#12290;&#21253;&#25324;Set Transformer&#21644;Perceiver&#22312;&#20869;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19982;&#26377;&#38480;&#33021;&#21147;&#30340;&#28508;&#22312;&#31354;&#38388;&#30456;&#32467;&#21512;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22522;&#20110;&#26368;&#36817;&#23545;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#29702;&#35770;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#12290;AiT&#24341;&#20837;&#20102;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#65292;&#26082;&#21487;&#20197;&#20316;&#20026;&#20808;&#39564;&#26469;&#25351;&#23548;&#20849;&#20139;&#24037;&#20316;&#31354;&#38388;&#30340;&#29942;&#39048;&#27880;&#24847;&#21147;&#65292;&#21448;&#21487;&#20197;&#20316;&#20026;&#20851;&#32852;&#35760;&#24518;&#30340;&#21560;&#24341;&#23376;&#12290;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36825;&#20123;&#20808;&#39564;&#33258;&#28982;&#22320;&#21457;&#23637;&#20986;&#27169;&#22359;&#30340;&#29305;&#21270;&#65292;&#27599;&#20010;&#27169;&#22359;&#23545;&#24418;&#25104;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24402;&#32435;&#20559;&#22909;&#26377;&#25152;&#36129;&#29486;&#12290;&#29942;&#39048;&#21487;&#20197;&#20419;&#36827;&#36755;&#20837;&#20043;&#38388;&#20026;&#23558;&#20449;&#24687;&#20889;&#20837;&#20869;&#23384;&#32780;&#36827;&#34892;&#31454;&#20105;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AiT&#26159;&#19968;&#31181;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging from the monolithic pairwise attention mechanism in conventional Transformer models, there is a growing interest in leveraging sparse interactions that align more closely with biological principles. Approaches including the Set Transformer and the Perceiver employ cross-attention consolidated with a latent space that forms an attention bottleneck with limited capacity. Building upon recent neuroscience studies of Global Workspace Theory and associative memory, we propose the Associative Transformer (AiT). AiT induces low-rank explicit memory that serves as both priors to guide bottleneck attention in the shared workspace and attractors within associative memory of a Hopfield network. Through joint end-to-end training, these priors naturally develop module specialization, each contributing a distinct inductive bias to form attention bottlenecks. A bottleneck can foster competition among inputs for writing information into the memory. We show that AiT is a sparse representation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Forman-Ricci&#26354;&#29575;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#35266;&#23519;&#31163;&#25955;&#26354;&#29575;&#65292;&#21487;&#20197;&#28155;&#21152;&#25110;&#21024;&#38500;&#36793;&#20197;&#20943;&#36731;&#36825;&#20004;&#31181;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.09384</link><description>&lt;p&gt;
&#20351;&#29992;Forman-Ricci&#26354;&#29575;&#30340;&#25193;&#23637;&#26469;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Over-Smoothing and Over-Squashing using Augmentations of Forman-Ricci Curvature. (arXiv:2309.09384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Forman-Ricci&#26354;&#29575;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#35266;&#23519;&#31163;&#25955;&#26354;&#29575;&#65292;&#21487;&#20197;&#28155;&#21152;&#25110;&#21024;&#38500;&#36793;&#20197;&#20943;&#36731;&#36825;&#20004;&#31181;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26368;&#36817;&#25551;&#36848;&#20102;&#20960;&#20010;&#28508;&#22312;&#30340;&#38519;&#38449;&#12290;&#36825;&#20123;&#21253;&#25324;&#26080;&#27861;&#20934;&#30830;&#21033;&#29992;&#32534;&#30721;&#22312;&#38271;&#36317;&#31163;&#36830;&#25509;&#20013;&#30340;&#20449;&#24687;&#65288;&#36807;&#24230;&#21387;&#32553;&#65289;&#65292;&#20197;&#21450;&#22312;&#32593;&#32476;&#28145;&#24230;&#22686;&#21152;&#26102;&#38590;&#20197;&#21306;&#20998;&#38468;&#36817;&#33410;&#28857;&#30340;&#23398;&#20064;&#34920;&#31034;&#65288;&#36807;&#24230;&#24179;&#28369;&#65289;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#34920;&#24449;&#36825;&#20004;&#31181;&#25928;&#24212;&#30340;&#26041;&#27861;&#26159;&#31163;&#25955;&#26354;&#29575;&#65306;&#23548;&#33268;&#36807;&#24230;&#21387;&#32553;&#25928;&#24212;&#30340;&#38271;&#36317;&#31163;&#36830;&#25509;&#20855;&#26377;&#20302;&#26354;&#29575;&#65292;&#32780;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#30340;&#36793;&#20855;&#26377;&#39640;&#26354;&#29575;&#12290;&#36825;&#20010;&#35266;&#23519;&#24341;&#21457;&#20102;&#19968;&#20123;&#37325;&#36830;&#25216;&#26415;&#65292;&#36890;&#36807;&#22686;&#21152;&#25110;&#21024;&#38500;&#36793;&#26469;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21033;&#29992;&#22270;&#29305;&#24449;&#65288;&#22914;&#26354;&#29575;&#25110;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#35889;&#65289;&#30340;&#37325;&#36830;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#26354;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#23376;&#22270;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Graph Neural Networks (GNNs) have been successfully leveraged for learning on graph-structured data across domains, several potential pitfalls have been described recently. Those include the inability to accurately leverage information encoded in long-range connections (over-squashing), as well as difficulties distinguishing the learned representations of nearby nodes with growing network depth (over-smoothing). An effective way to characterize both effects is discrete curvature: Long-range connections that underlie over-squashing effects have low curvature, whereas edges that contribute to over-smoothing have high curvature. This observation has given rise to rewiring techniques, which add or remove edges to mitigate over-smoothing and over-squashing. Several rewiring approaches utilizing graph characteristics, such as curvature or the spectrum of the graph Laplacian, have been proposed. However, existing methods, especially those based on curvature, often require expensive subr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SAH-GNN&#65292;&#19968;&#31181;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24212;&#29992;&#32467;&#26500;&#24863;&#30693;&#30340;&#36763;&#31995;&#32479;&#21704;&#23494;&#39039;&#23884;&#20837;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;SAH-GNN&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#23398;&#20064;&#36763;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#20934;&#36763;&#32467;&#26500;&#24418;&#24335;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#29289;&#29702;&#24847;&#20041;&#19978;&#30340;&#33021;&#37327;&#23432;&#24658;&#12290;</title><link>http://arxiv.org/abs/2309.04885</link><description>&lt;p&gt;
&#32467;&#26500;&#24863;&#30693;&#30340;&#36763;&#31995;&#32479;&#21704;&#23494;&#39039;&#65288;&#22270;&#65289;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Symplectic Structure-Aware Hamiltonian (Graph) Embeddings. (arXiv:2309.04885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SAH-GNN&#65292;&#19968;&#31181;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24212;&#29992;&#32467;&#26500;&#24863;&#30693;&#30340;&#36763;&#31995;&#32479;&#21704;&#23494;&#39039;&#23884;&#20837;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;SAH-GNN&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#23398;&#20064;&#36763;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#20934;&#36763;&#32467;&#26500;&#24418;&#24335;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#29289;&#29702;&#24847;&#20041;&#19978;&#30340;&#33021;&#37327;&#23432;&#24658;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#65292;&#22266;&#23450;&#23884;&#20837;&#27969;&#24418;&#30340;&#20551;&#35774;&#24120;&#24120;&#38480;&#21046;&#20102;&#20854;&#23545;&#19981;&#21516;&#22270;&#20960;&#20309;&#32467;&#26500;&#30340;&#36866;&#24212;&#24615;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;GNNs&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#23450;&#24459;&#32435;&#20837;&#33410;&#28857;&#29305;&#24449;&#26356;&#26032;&#20013;&#65292;&#26469;&#35299;&#20915;&#36825;&#31867;&#23884;&#20837;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAH-GNN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#25512;&#24191;&#21040;&#26356;&#28789;&#27963;&#30340;&#33410;&#28857;&#29305;&#24449;&#26356;&#26032;&#20013;&#12290;&#19982;&#29616;&#26377;&#30340;&#21463;&#21704;&#23494;&#39039;&#21551;&#21457;&#30340;GNNs&#19981;&#21516;&#65292;SAH-GNN&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#36763;&#26031;&#33922;&#36153;&#23572;&#27969;&#24418;&#19978;&#30340;&#40654;&#26364;&#20248;&#21270;&#65292;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#28508;&#22312;&#30340;&#36763;&#32467;&#26500;&#65292;&#20174;&#32780;&#35268;&#36991;&#20102;&#29616;&#26377;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#20934;&#36763;&#32467;&#26500;&#24418;&#24335;&#30340;&#21704;&#23494;&#39039;GNNs&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#20351;&#24471;SAH-GNN&#33021;&#22815;&#22312;&#27809;&#26377;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#36866;&#24212;&#21508;&#31181;&#22270;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#33021;&#37327;&#23432;&#24658;&#65292;&#20351;&#24471;&#38544;&#24335;&#21704;&#23494;&#39039;&#31995;&#32479;&#20855;&#26377;&#29289;&#29702;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional Graph Neural Networks (GNNs), the assumption of a fixed embedding manifold often limits their adaptability to diverse graph geometries. Recently, Hamiltonian system-inspired GNNs are proposed to address the dynamic nature of such embeddings by incorporating physical laws into node feature updates. In this work, we present SAH-GNN, a novel approach that generalizes Hamiltonian dynamics for more flexible node feature updates. Unlike existing Hamiltonian-inspired GNNs, SAH-GNN employs Riemannian optimization on the symplectic Stiefel manifold to adaptively learn the underlying symplectic structure during training, circumventing the limitations of existing Hamiltonian GNNs that rely on a pre-defined form of standard symplectic structure. This innovation allows SAH-GNN to automatically adapt to various graph datasets without extensive hyperparameter tuning. Moreover, it conserves energy during training such that the implicit Hamiltonian system is physically meaningful. To thi
&lt;/p&gt;</description></item><item><title>ECHO-VICODE&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#26469;&#20998;&#31867;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;31&#20010;&#35270;&#22270;&#31867;&#21035;&#65292;&#24182;&#20855;&#26377;&#38598;&#25104;&#30340;&#31163;&#32676;&#26816;&#27979;&#21151;&#33021;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#30340;&#38169;&#35823;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16483</link><description>&lt;p&gt;
&#20855;&#26377;&#38598;&#25104;&#31163;&#32676;&#26816;&#27979;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#22270;&#20998;&#31867;&#65292;&#20197;&#22686;&#24378;&#33258;&#21160;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Echocardiographic View Classification with Integrated Out-of-Distribution Detection for Enhanced Automatic Echocardiographic Analysis. (arXiv:2308.16483v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16483
&lt;/p&gt;
&lt;p&gt;
ECHO-VICODE&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#26469;&#20998;&#31867;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;31&#20010;&#35270;&#22270;&#31867;&#21035;&#65292;&#24182;&#20855;&#26377;&#38598;&#25104;&#30340;&#31163;&#32676;&#26816;&#27979;&#21151;&#33021;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#30340;&#38169;&#35823;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#33258;&#21160;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#26512;&#21644;&#35299;&#37322;&#39046;&#22495;&#20013;&#65292;&#33258;&#21160;&#35270;&#22270;&#20998;&#31867;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#30001;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#21487;&#21464;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ECHO-VICODE&#65288;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#22270;&#20998;&#31867;&#19982;&#31163;&#32676;&#26816;&#27979;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#26469;&#20998;&#31867;31&#20010;&#31867;&#21035;&#65292;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22788;&#29702;&#22810;&#31181;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#22270;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;ECHO-VICODE&#36824;&#21152;&#20837;&#20102;&#19968;&#20010;&#38598;&#25104;&#30340;&#31163;&#32676;&#26816;&#27979;&#21151;&#33021;&#65292;&#21033;&#29992;&#30456;&#23545;&#39532;&#27663;&#36317;&#31163;&#26377;&#25928;&#35782;&#21035;&#24120;&#35265;&#30340;&#8220;&#25509;&#36817;&#31163;&#32676;&#8221;&#23454;&#20363;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ECHO-VICODE&#22312;&#35270;&#22270;&#20998;&#31867;&#21644;&#31163;&#32676;&#26816;&#27979;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#28508;&#22312;&#38169;&#35823;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of automatic echocardiographic analysis and interpretation, automatic view classification is a critical yet challenging task, owing to the inherent complexity and variability of echocardiographic data. This study presents ECHOcardiography VIew Classification with Out-of-Distribution dEtection (ECHO-VICODE), a novel deep learning-based framework that effectively addresses this challenge by training to classify 31 classes, surpassing previous studies and demonstrating its capacity to handle a wide range of echocardiographic views. Furthermore, ECHO-VICODE incorporates an integrated out-of-distribution (OOD) detection function, leveraging the relative Mahalanobis distance to effectively identify 'near-OOD' instances commonly encountered in echocardiographic data. Through extensive experimentation, we demonstrated the outstanding performance of ECHO-VICODE in terms of view classification and OOD detection, significantly reducing the potential for errors in ech
&lt;/p&gt;</description></item><item><title>Karasu&#26159;&#19968;&#31181;&#36890;&#36807;&#20419;&#36827;&#31867;&#20284;&#22522;&#30784;&#35774;&#26045;&#12289;&#26694;&#26550;&#12289;&#31639;&#27861;&#25110;&#25968;&#25454;&#38598;&#19978;&#24037;&#20316;&#30340;&#29992;&#25143;&#20043;&#38388;&#30340;&#25968;&#25454;&#20849;&#20139;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#36164;&#28304;&#37197;&#32622;&#37197;&#32622;&#25991;&#20214;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11792</link><description>&lt;p&gt;
Karasu:&#19968;&#31181;&#29992;&#20110;&#22823;&#25968;&#25454;&#20998;&#26512;&#30340;&#39640;&#25928;&#38598;&#32676;&#37197;&#32622;&#30340;&#21327;&#20316;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics. (arXiv:2308.11792v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11792
&lt;/p&gt;
&lt;p&gt;
Karasu&#26159;&#19968;&#31181;&#36890;&#36807;&#20419;&#36827;&#31867;&#20284;&#22522;&#30784;&#35774;&#26045;&#12289;&#26694;&#26550;&#12289;&#31639;&#27861;&#25110;&#25968;&#25454;&#38598;&#19978;&#24037;&#20316;&#30340;&#29992;&#25143;&#20043;&#38388;&#30340;&#25968;&#25454;&#20849;&#20139;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#36164;&#28304;&#37197;&#32622;&#37197;&#32622;&#25991;&#20214;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#31867;&#22411;&#21644;&#38598;&#32676;&#35268;&#27169;&#31561;&#37197;&#32622;&#36873;&#39033;&#30340;&#24191;&#27867;&#22810;&#26679;&#24615;&#65292;&#36873;&#25321;&#36866;&#21512;&#22823;&#25968;&#25454;&#20998;&#26512;&#20316;&#19994;&#30340;&#27491;&#30830;&#36164;&#28304;&#26159;&#22256;&#38590;&#30340;&#12290;&#30001;&#20110;&#31967;&#31957;&#30340;&#36873;&#25321;&#21487;&#33021;&#23545;&#36164;&#28304;&#25928;&#29575;&#12289;&#25104;&#26412;&#21644;&#33021;&#28304;&#20351;&#29992;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#37325;&#22797;&#24037;&#20316;&#36127;&#36733;&#36827;&#34892;&#37197;&#32622;&#25991;&#20214;&#20998;&#26512;&#65292;&#20197;&#23547;&#25214;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30001;&#20110;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#36153;&#26102;&#19988;&#26114;&#36149;&#30340;&#37197;&#32622;&#25991;&#20214;&#20998;&#26512;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#29992;&#25143;&#30340;&#22823;&#25968;&#25454;&#20998;&#26512;&#20316;&#19994;&#21487;&#20197;&#20849;&#20139;&#35768;&#22810;&#20849;&#21516;&#29305;&#24615;&#65306;&#23427;&#20204;&#36890;&#24120;&#22312;&#31867;&#20284;&#30340;&#22522;&#30784;&#35774;&#26045;&#19978;&#25805;&#20316;&#65292;&#20351;&#29992;&#31867;&#20284;&#30340;&#31639;&#27861;&#22312;&#31867;&#20284;&#30340;&#26694;&#26550;&#20013;&#23454;&#29616;&#12290;&#20849;&#20139;&#32858;&#21512;&#37197;&#32622;&#25991;&#20214;&#20998;&#26512;&#36816;&#34892;&#20197;&#21327;&#20316;&#35299;&#20915;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#28508;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Karasu&#65292;&#19968;&#31181;&#20419;&#36827;&#22312;&#31867;&#20284;&#22522;&#30784;&#35774;&#26045;&#12289;&#26694;&#26550;&#12289;&#31639;&#27861;&#25110;&#25968;&#25454;&#38598;&#19978;&#24037;&#20316;&#30340;&#29992;&#25143;&#20043;&#38388;&#20849;&#20139;&#25968;&#25454;&#30340;&#26356;&#39640;&#25928;&#30340;&#36164;&#28304;&#37197;&#32622;&#37197;&#32622;&#25991;&#20214;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting the right resources for big data analytics jobs is hard because of the wide variety of configuration options like machine type and cluster size. As poor choices can have a significant impact on resource efficiency, cost, and energy usage, automated approaches are gaining popularity. Most existing methods rely on profiling recurring workloads to find near-optimal solutions over time. Due to the cold-start problem, this often leads to lengthy and costly profiling phases. However, big data analytics jobs across users can share many common properties: they often operate on similar infrastructure, using similar algorithms implemented in similar frameworks. The potential in sharing aggregated profiling runs to collaboratively address the cold start problem is largely unexplored.  We present Karasu, an approach to more efficient resource configuration profiling that promotes data sharing among users working with similar infrastructures, frameworks, algorithms, or datasets. Karasu tr
&lt;/p&gt;</description></item><item><title>&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;</title><link>http://arxiv.org/abs/2308.09687</link><description>&lt;p&gt;
&#24819;&#27861;&#22270;&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09687
&lt;/p&gt;
&lt;p&gt;
&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24819;&#27861;&#22270;&#65288;Graph of Thoughts&#65292;GoT&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25552;&#31034;&#33021;&#21147;&#19978;&#36229;&#36234;&#20102;Chain-of-Thought&#25110;Tree of Thoughts&#65288;ToT&#65289;&#31561;&#33539;&#24335;&#12290;GoT&#30340;&#20851;&#38190;&#24605;&#24819;&#21644;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#23558;LLM&#29983;&#25104;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#20854;&#20013;&#20449;&#24687;&#21333;&#20803;&#65288;"LLM&#24819;&#27861;"&#65289;&#26159;&#39030;&#28857;&#65292;&#36793;&#34920;&#31034;&#36825;&#20123;&#39030;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#23558;&#20219;&#24847;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#12289;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#20248;&#21183;&#65292;&#20363;&#22914;&#22312;&#25490;&#24207;&#20219;&#21153;&#19978;&#36136;&#37327;&#25552;&#39640;&#20102;62%&#65292;&#21516;&#26102;&#25104;&#26412;&#38477;&#20302;&#20102;&#36229;&#36807;31%&#12290;&#25105;&#20204;&#30830;&#20445;GoT&#33021;&#22815;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#24320;&#21019;&#26032;&#30340;&#25552;&#31034;&#26041;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#20351;&#24471;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinki
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#26080;&#20284;&#28982;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23545;&#24050;&#30693;&#23646;&#20110;&#20004;&#20010;&#31867;&#21035;&#30340;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#22312;&#26080;&#20284;&#28982;&#25512;&#26029;&#39046;&#22495;&#65292;&#36890;&#36807;&#23558;&#26631;&#35760;&#26679;&#26412;&#36890;&#36807;&#27491;&#21521;&#27169;&#25311;&#33719;&#24471;&#65292;&#26410;&#26631;&#35760;&#26679;&#26412;&#36890;&#36807;&#23454;&#39564;&#25910;&#38598;&#65292;&#32473;&#20986;&#20102;&#19968;&#20010;&#26435;&#34913;m&#21644;n&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.09043</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#30340;&#26080;&#20284;&#28982;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kernel-Based Tests for Likelihood-Free Hypothesis Testing. (arXiv:2308.09043v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#26080;&#20284;&#28982;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23545;&#24050;&#30693;&#23646;&#20110;&#20004;&#20010;&#31867;&#21035;&#30340;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#22312;&#26080;&#20284;&#28982;&#25512;&#26029;&#39046;&#22495;&#65292;&#36890;&#36807;&#23558;&#26631;&#35760;&#26679;&#26412;&#36890;&#36807;&#27491;&#21521;&#27169;&#25311;&#33719;&#24471;&#65292;&#26410;&#26631;&#35760;&#26679;&#26412;&#36890;&#36807;&#23454;&#39564;&#25910;&#38598;&#65292;&#32473;&#20986;&#20102;&#19968;&#20010;&#26435;&#34913;m&#21644;n&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20004;&#20010;&#24179;&#34913;&#31867;&#21035;&#30340;n&#20010;&#35266;&#27979;&#20013;&#65292;&#32771;&#34385;&#23545;&#39069;&#22806;m&#20010;&#24050;&#30693;&#23646;&#20110;&#20854;&#20013;&#19968;&#20010;&#31867;&#21035;&#30340;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#30340;&#20219;&#21153;&#12290;&#35813;&#38382;&#39064;&#30340;&#29305;&#27530;&#24773;&#20917;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65306;&#24403;&#23436;&#20840;&#20102;&#35299;&#31867;&#21035;&#20998;&#24067;&#26102;&#65288;n=&#8734;&#65289;&#65292;&#26368;&#20248;&#35299;&#26159;&#20351;&#29992;&#20284;&#28982;&#27604;&#26816;&#39564;&#65307;&#24403;m=1&#26102;&#65292;&#23545;&#24212;&#20108;&#20998;&#31867;&#38382;&#39064;&#65307;&#24403;m&#8776;n&#26102;&#65292;&#31561;&#21516;&#20110;&#20004;&#26679;&#26412;&#26816;&#39564;&#12290;&#20013;&#38388;&#30340;&#24773;&#20917;&#20986;&#29616;&#22312;&#26080;&#20284;&#28982;&#25512;&#26029;&#39046;&#22495;&#65292;&#20854;&#20013;&#26631;&#35760;&#26679;&#26412;&#36890;&#36807;&#36816;&#34892;&#27491;&#21521;&#27169;&#25311;&#33719;&#24471;&#65292;&#32780;&#26410;&#26631;&#35760;&#26679;&#26412;&#36890;&#36807;&#23454;&#39564;&#25910;&#38598;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;m&#21644;n&#20043;&#38388;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#26435;&#34913;&#65306;&#22686;&#21152;&#25968;&#25454;&#26679;&#26412;m&#20250;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;/&#27169;&#25311;&#25968;&#25454;&#37327;n&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#65288;a&#65289;&#24341;&#20837;&#20102;&#19968;&#20010;&#24120;&#24120;&#36935;&#21040;&#30340;&#24773;&#20917;&#65292;&#21363;&#26410;&#26631;&#35760;&#26679;&#26412;&#26469;&#33258;&#20004;&#20010;&#31867;&#21035;&#30340;&#28151;&#21512;&#29289;&#65307;&#65288;b&#65289;&#30740;&#31350;&#20102;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#39118;&#38505;&#23450;&#20041;&#20026;&#35823;&#20998;&#31867;&#27010;&#29575;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given $n$ observations from two balanced classes, consider the task of labeling an additional $m$ inputs that are known to all belong to \emph{one} of the two classes. Special cases of this problem are well-known: with complete knowledge of class distributions ($n=\infty$) the problem is solved optimally by the likelihood-ratio test; when $m=1$ it corresponds to binary classification; and when $m\approx n$ it is equivalent to two-sample testing. The intermediate settings occur in the field of likelihood-free inference, where labeled samples are obtained by running forward simulations and the unlabeled sample is collected experimentally. In recent work it was discovered that there is a fundamental trade-off between $m$ and $n$: increasing the data sample $m$ reduces the amount $n$ of training/simulation data needed. In this work we (a) introduce a generalization where unlabeled samples come from a mixture of the two classes -- a case often encountered in practice; (b) study the minimax 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#21033;&#29992;&#37327;&#23376;&#26680;&#65292;&#25193;&#23637;&#20102;&#31070;&#32463;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08467</link><description>&lt;p&gt;
&#35770;&#31070;&#32463;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
On Neural Quantum Support Vector Machines. (arXiv:2308.08467v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#21033;&#29992;&#37327;&#23376;&#26680;&#65292;&#25193;&#23637;&#20102;&#31070;&#32463;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312; \cite{simon2023algorithms} &#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22235;&#31181;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;NSVMs&#65289;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#21363;&#20855;&#26377;&#37327;&#23376;&#26680;&#30340;NSVMs&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#36825;&#20010;&#24773;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In \cite{simon2023algorithms} we introduced four algorithms for the training of neural support vector machines (NSVMs) and demonstrated their feasibility. In this note we introduce neural quantum support vector machines, that is, NSVMs with a quantum kernel, and extend our results to this setting.
&lt;/p&gt;</description></item><item><title>Real Robot Challenge 2022&#20026;RL&#21644;&#26426;&#22120;&#20154;&#23398;&#30028;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#25311;&#20013;&#24471;&#21040;&#30340;&#35265;&#35299;&#19981;&#33021;&#36716;&#21270;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07741</link><description>&lt;p&gt;
Real Robot Challenge 2022: &#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#28789;&#24039;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World. (arXiv:2308.07741v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07741
&lt;/p&gt;
&lt;p&gt;
Real Robot Challenge 2022&#20026;RL&#21644;&#26426;&#22120;&#20154;&#23398;&#30028;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#25311;&#20013;&#24471;&#21040;&#30340;&#35265;&#35299;&#19981;&#33021;&#36716;&#21270;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#23454;&#39564;&#22312;&#26102;&#38388;&#21644;&#25104;&#26412;&#19978;&#35201;&#27714;&#24456;&#39640;&#12290;&#22240;&#27492;&#65292;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#31038;&#21306;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#20351;&#29992;&#27169;&#25311;&#22120;&#26469;&#24320;&#21457;&#21644;&#35780;&#20272;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20174;&#27169;&#25311;&#20013;&#24471;&#21040;&#30340;&#35265;&#35299;&#19981;&#19968;&#23450;&#33021;&#22815;&#36716;&#21270;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#28041;&#21450;&#22797;&#26434;&#29615;&#22659;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;Real Robot Challenge 2022&#20316;&#20026;RL&#21644;&#26426;&#22120;&#20154;&#23398;&#30028;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#35753;&#21442;&#19982;&#32773;&#33021;&#22815;&#20687;&#22312;&#27169;&#25311;&#20013;&#19968;&#26679;&#36731;&#26494;&#22320;&#36828;&#31243;&#23454;&#39564;&#30495;&#23454;&#26426;&#22120;&#20154;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#24050;&#32463;&#25104;&#29087;&#20026;&#19968;&#31181;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#24335;&#65292;&#20943;&#36731;&#20102;&#23545;&#26114;&#36149;&#22312;&#32447;&#20132;&#20114;&#30340;&#20381;&#36182;.&#22240;&#27492;&#65292;&#25105;&#20204;&#35201;&#27714;&#21442;&#19982;&#32773;&#20174;&#25552;&#20379;&#30340;&#30495;&#23454;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#20004;&#20010;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#65292;&#21253;&#25324;&#25512;&#21160;&#12289;&#25235;&#21462;&#21644;&#25163;&#20869;&#23450;&#20301;&#12290;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#36719;&#20214;&#25991;&#26723;&#21270;&#65292;&#24182;&#22312;&#22522;&#20110;&#20223;&#30495;&#30340;&#21021;&#27493;&#38454;&#27573;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimentation on real robots is demanding in terms of time and costs. For this reason, a large part of the reinforcement learning (RL) community uses simulators to develop and benchmark algorithms. However, insights gained in simulation do not necessarily translate to real robots, in particular for tasks involving complex interactions with the environment. The Real Robot Challenge 2022 therefore served as a bridge between the RL and robotics communities by allowing participants to experiment remotely with a real robot - as easily as in simulation.  In the last years, offline reinforcement learning has matured into a promising paradigm for learning from pre-collected datasets, alleviating the reliance on expensive online interactions. We therefore asked the participants to learn two dexterous manipulation tasks involving pushing, grasping, and in-hand orientation from provided real-robot datasets. An extensive software documentation and an initial stage based on a simulation of the re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;AI&#30740;&#31350;&#24212;&#35813;&#21463;&#21457;&#23637;&#24515;&#29702;&#23398;&#21551;&#21457;&#65292;&#24182;&#30740;&#31350;&#20351;&#20195;&#29702;&#33021;&#22815;&#36827;&#20837;&#25991;&#21270;&#30340;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#31038;&#20250;AI&#23398;&#26657;&#24037;&#20855;&#20197;&#20415;&#20110;&#36827;&#34892;&#30456;&#20851;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.07871</link><description>&lt;p&gt;
&#31038;&#20250;AI&#23398;&#26657;&#65306;&#20174;&#21457;&#23637;&#24515;&#29702;&#23398;&#21040;&#20154;&#24037;&#31038;&#20250;&#25991;&#21270;&#20195;&#29702;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents. (arXiv:2307.07871v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07871
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;AI&#30740;&#31350;&#24212;&#35813;&#21463;&#21457;&#23637;&#24515;&#29702;&#23398;&#21551;&#21457;&#65292;&#24182;&#30740;&#31350;&#20351;&#20195;&#29702;&#33021;&#22815;&#36827;&#20837;&#25991;&#21270;&#30340;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#31038;&#20250;AI&#23398;&#26657;&#24037;&#20855;&#20197;&#20415;&#20110;&#36827;&#34892;&#30456;&#20851;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#24515;&#29702;&#23398;&#23478;&#38271;&#26399;&#20197;&#26469;&#24050;&#32463;&#30830;&#31435;&#20102;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#22312;&#20154;&#31867;&#26234;&#21147;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#20837;&#12289;&#21442;&#19982;&#21644;&#20174;&#20154;&#31867;&#25991;&#21270;&#20013;&#21463;&#30410;&#12290;&#31038;&#20250;&#20132;&#20114;&#20195;&#29702;&#30340;AI&#30740;&#31350;&#22823;&#22810;&#20851;&#27880;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#25991;&#21270;&#30340;&#20986;&#29616;&#65288;&#36890;&#24120;&#27809;&#26377;&#24378;&#28872;&#30340;&#21457;&#23637;&#24515;&#29702;&#23398;&#22522;&#30784;&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;AI&#30740;&#31350;&#24212;&#35813;&#21463;&#24515;&#29702;&#23398;&#21551;&#21457;&#65292;&#24182;&#30740;&#31350;&#33021;&#22815;&#36827;&#20837;&#25991;&#21270;&#30340;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;Michael Tomasello&#21644;Jerome Bruner&#30340;&#29702;&#35770;&#65292;&#20171;&#32461;&#20102;&#20182;&#20204;&#30340;&#19968;&#20123;&#27010;&#24565;&#65292;&#24182;&#27010;&#36848;&#20102;&#20851;&#38190;&#27010;&#24565;&#21644;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31038;&#20250;AI&#23398;&#26657;&#8212;&#8212;&#19968;&#20010;&#21253;&#25324;&#23450;&#21046;&#21442;&#25968;&#21270;&#29615;&#22659;&#30340;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#20851;&#20110;&#36825;&#20123;&#27010;&#24565;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;RL&#20195;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27492;&#31867;&#23454;&#39564;&#30340;&#31034;&#20363;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#21560;&#24341;AI&#31038;&#21306;&#22260;&#32469;&#36825;&#20123;&#27010;&#24565;&#36827;&#34892;&#35752;&#35770;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developmental psychologists have long-established the importance of socio-cognitive abilities in human intelligence. These abilities enable us to enter, participate and benefit from human culture. AI research on social interactive agents mostly concerns the emergence of culture in a multi-agent setting (often without a strong grounding in developmental psychology). We argue that AI research should be informed by psychology and study socio-cognitive abilities enabling to enter a culture too. We discuss the theories of Michael Tomasello and Jerome Bruner to introduce some of their concepts to AI and outline key concepts and socio-cognitive abilities. We present The SocialAI school - a tool including a customizable parameterized uite of procedurally generated environments, which simplifies conducting experiments regarding those concepts. We show examples of such experiments with RL agents and Large Language Models. The main motivation of this work is to engage the AI community around the 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#33258;&#21160;&#28040;&#38500;&#24207;&#21015;&#20887;&#20313;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#21387;&#32553;&#27604;&#21644;&#35299;&#21387;&#32553;&#24615;&#33021;&#30340;LeCo&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#23427;&#33021;&#22815;&#22312;&#21387;&#32553;&#27604;&#21644;&#38543;&#26426;&#35775;&#38382;&#36895;&#24230;&#19978;&#23454;&#29616;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.15374</link><description>&lt;p&gt;
LeCo&#65306;&#36890;&#36807;&#23398;&#20064;&#24207;&#21015;&#30456;&#20851;&#24615;&#23454;&#29616;&#36731;&#37327;&#32423;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
LeCo: Lightweight Compression via Learning Serial Correlations. (arXiv:2306.15374v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15374
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#33258;&#21160;&#28040;&#38500;&#24207;&#21015;&#20887;&#20313;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#21387;&#32553;&#27604;&#21644;&#35299;&#21387;&#32553;&#24615;&#33021;&#30340;LeCo&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#23427;&#33021;&#22815;&#22312;&#21387;&#32553;&#27604;&#21644;&#38543;&#26426;&#35775;&#38382;&#36895;&#24230;&#19978;&#23454;&#29616;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36731;&#37327;&#32423;&#25968;&#25454;&#21387;&#32553;&#26159;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#65292;&#23427;&#20351;&#24471;&#21015;&#24335;&#23384;&#20648;&#22312;&#20998;&#26512;&#26597;&#35810;&#26041;&#38754;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#20043;&#21069;&#26377;&#20851;&#22522;&#20110;&#23383;&#20856;&#32534;&#30721;&#26469;&#36924;&#36817;Shannon&#29109;&#30340;&#30740;&#31350;&#24050;&#32463;&#24456;&#20840;&#38754;&#65292;&#20294;&#40092;&#26377;&#20043;&#21069;&#30340;&#24037;&#20316;&#31995;&#32479;&#22320;&#21033;&#29992;&#21015;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#26469;&#36827;&#34892;&#21387;&#32553;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LeCo&#65288;&#21363;&#23398;&#20064;&#21387;&#32553;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#28040;&#38500;&#24207;&#21015;&#20887;&#20313;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#21387;&#32553;&#27604;&#21644;&#35299;&#21387;&#32553;&#24615;&#33021;&#30340;&#26694;&#26550;&#12290;LeCo&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#29616;&#26377;&#30340;&#65288;&#20020;&#26102;&#30340;&#65289;&#31639;&#27861;&#65292;&#22914;&#21442;&#32771;&#24103;&#65288;Frame-of-Reference&#65289;&#65292;Delta&#32534;&#30721;&#21644;&#28216;&#31243;&#32534;&#30721;&#65288;Run-Length Encoding&#65289;&#37117;&#26159;&#29305;&#20363;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24494;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;LeCo&#21407;&#22411;&#22312;&#21387;&#32553;&#27604;&#21644;&#38543;&#26426;&#35775;&#38382;&#36895;&#24230;&#19978;&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21462;&#24471;&#20102;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;&#24403;&#23558;LeCo&#38598;&#25104;&#26102;
&lt;/p&gt;
&lt;p&gt;
Lightweight data compression is a key technique that allows column stores to exhibit superior performance for analytical queries. Despite a comprehensive study on dictionary-based encodings to approach Shannon's entropy, few prior works have systematically exploited the serial correlation in a column for compression. In this paper, we propose LeCo (i.e., Learned Compression), a framework that uses machine learning to remove the serial redundancy in a value sequence automatically to achieve an outstanding compression ratio and decompression performance simultaneously. LeCo presents a general approach to this end, making existing (ad-hoc) algorithms such as Frame-of-Reference (FOR), Delta Encoding, and Run-Length Encoding (RLE) special cases under our framework. Our microbenchmark with three synthetic and six real-world data sets shows that a prototype of LeCo achieves a Pareto improvement on both compression ratio and random access speed over the existing solutions. When integrating LeC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#27969;&#21305;&#37197;&#30340;&#31561;&#21464;CNF&#35757;&#32451;&#30446;&#26631;&#65292;&#21487;&#20197;&#25552;&#39640;&#31561;&#21464;CNF&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.15030</link><description>&lt;p&gt;
&#31561;&#21464;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Equivariant flow matching. (arXiv:2306.15030v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#27969;&#21305;&#37197;&#30340;&#31561;&#21464;CNF&#35757;&#32451;&#30446;&#26631;&#65292;&#21487;&#20197;&#25552;&#39640;&#31561;&#21464;CNF&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#21270;&#27969;&#26159;&#19968;&#31867;&#29305;&#21035;&#36866;&#29992;&#20110;&#29289;&#29702;&#23398;&#20013;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#27969;&#30340;&#20934;&#30830;&#20284;&#28982;&#24615;&#36136;&#21487;&#20197;&#23454;&#29616;&#23545;&#24050;&#30693;&#30446;&#26631;&#33021;&#37327;&#20989;&#25968;&#30340;&#21152;&#26435;&#37325;&#37325;&#21644;&#26080;&#20559;&#35266;&#27979;&#37327;&#30340;&#35745;&#31639;&#12290;&#20363;&#22914;&#65292;Boltzmann&#29983;&#25104;&#22120;&#36890;&#36807;&#35757;&#32451;&#27969;&#29983;&#25104;&#22788;&#20110;&#24179;&#34913;&#29366;&#24577;&#30340;&#22810;&#20307;&#31995;&#32479;&#65288;&#22914;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#65289;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#37319;&#26679;&#38382;&#39064;&#12290;&#20026;&#20102;&#26500;&#24314;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#20063;&#24456;&#20851;&#38190;&#23558;&#30446;&#26631;&#33021;&#37327;&#30340;&#23545;&#31216;&#24615;&#32435;&#20837;&#27169;&#22411;&#20013;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#31561;&#21464;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#65288;CNF&#65289;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;CNF&#30340;&#35757;&#32451;&#21644;&#26679;&#26412;&#29983;&#25104;&#30340;&#35745;&#31639;&#24320;&#38144;&#36739;&#22823;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31561;&#21464;&#27969;&#21305;&#37197;&#65292;&#19968;&#31181;&#26032;&#30340;&#31561;&#21464;CNF&#35757;&#32451;&#30446;&#26631;&#65292;&#20854;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#26368;&#20248;&#36755;&#36816;&#27969;&#21305;&#37197;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows are a class of deep generative models that are especially interesting for modeling probability distributions in physics, where the exact likelihood of flows allows reweighting to known target energy functions and computing unbiased observables. For instance, Boltzmann generators tackle the long-standing sampling problem in statistical physics by training flows to produce equilibrium samples of many-body systems such as small molecules and proteins. To build effective models for such systems, it is crucial to incorporate the symmetries of the target energy into the model, which can be achieved by equivariant continuous normalizing flows (CNFs). However, CNFs can be computationally expensive to train and generate samples from, which has hampered their scalability and practical application. In this paper, we introduce equivariant flow matching, a new training objective for equivariant CNFs that is based on the recently proposed optimal transport flow matching. Equivarian
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#32508;&#36848;&#20102;&#21487;&#35757;&#32451;&#30340;&#12289;&#21387;&#32553;&#30340;&#23884;&#20837;&#23618;&#22312;&#21387;&#32553;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.13724</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#21387;&#32553;&#23884;&#20837;&#23618;&#21450;&#20854;&#22312;&#25512;&#33616;&#31995;&#32479;&#19978;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of compressed embedding layers and their applications for recommender systems. (arXiv:2306.13724v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13724
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#32508;&#36848;&#20102;&#21487;&#35757;&#32451;&#30340;&#12289;&#21387;&#32553;&#30340;&#23884;&#20837;&#23618;&#22312;&#21387;&#32553;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22238;&#39038;&#20102;&#21487;&#35757;&#32451;&#30340;&#12289;&#21387;&#32553;&#30340;&#23884;&#20837;&#23618;&#30340;&#25991;&#29486;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#21387;&#32553;&#24040;&#22411;&#31070;&#32463;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#21387;&#32553;&#23884;&#20837;&#23618;&#25152;&#27979;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We review the literature on trainable, compressed embedding layers and discuss their applicability for compressing gigantic neural recommender systems. We also report the results we measured with our compressed embedding layers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SCAIS&#26694;&#26550;&#65292;&#21253;&#21547;&#19968;&#32452;19&#20010;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;67&#20010;&#25351;&#26631;&#65292;&#26088;&#22312;&#20419;&#36827;&#21644;&#32467;&#26500;&#21270;&#20851;&#20110;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35752;&#35770;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#26041;&#27861;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2306.13686</link><description>&lt;p&gt;
&#25299;&#23637;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35270;&#35282;&#65306; &#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#32508;&#21512;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Broadening the perspective for sustainable AI: Comprehensive sustainability criteria and indicators for AI systems. (arXiv:2306.13686v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SCAIS&#26694;&#26550;&#65292;&#21253;&#21547;&#19968;&#32452;19&#20010;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;67&#20010;&#25351;&#26631;&#65292;&#26088;&#22312;&#20419;&#36827;&#21644;&#32467;&#26500;&#21270;&#20851;&#20110;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35752;&#35770;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#26041;&#27861;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22686;&#21152;&#20351;&#29992;&#23548;&#33268;&#20102;&#22810;&#26041;&#38754;&#30340;&#31038;&#20250;&#12289;&#29615;&#22659;&#21644;&#32463;&#27982;&#21518;&#26524;&#65292;&#21253;&#25324;&#38750;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12289;&#27495;&#35270;&#12289;&#19981;&#24179;&#31561;&#21152;&#21095;&#12289;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#65292;&#20197;&#21450;&#32463;&#27982;&#23454;&#21147;&#30340;&#38598;&#20013;&#12290;&#26412;&#25991;&#36890;&#36807;&#32771;&#34385;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#22810;&#26041;&#38754;&#24615;&#65292;&#20026;&#8220;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#8221;&#30340;&#29702;&#24565;&#25552;&#20379;&#20102;&#23454;&#36136;&#24615;&#30340;&#25903;&#25345;&#12290;&#25552;&#20986;&#20102;SCAIS&#26694;&#26550;&#65288;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;&#25351;&#26631;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#32452;19&#20010;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;67&#20010;&#25351;&#26631;&#65292;&#36825;&#20123;&#26631;&#20934;&#21644;&#25351;&#26631;&#22522;&#20110;&#25209;&#21028;&#24615;&#23457;&#26597;&#21644;&#19987;&#23478;&#30740;&#35752;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#26041;&#27861;&#20026;&#20419;&#36827;&#21644;&#32467;&#26500;&#21270;&#20851;&#20110;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35752;&#35770;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#25972;&#20307;&#24615;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#26694;&#26550;&#65292;&#20026;AI&#31995;&#32479;&#30340;&#21518;&#32493;&#21457;&#23637;&#21644;&#35780;&#20272;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increased use of AI systems is associated with multi-faceted societal, environmental, and economic consequences. These include non-transparent decision-making processes, discrimination, increasing inequalities, rising energy consumption and greenhouse gas emissions in AI model development and application, and an increasing concentration of economic power. By considering the multi-dimensionality of sustainability, this paper takes steps towards substantiating the call for an overarching perspective on "sustainable AI". It presents the SCAIS Framework (Sustainability Criteria and Indicators for Artificial Intelligence Systems) which contains a set 19 sustainability criteria for sustainable AI and 67 indicators that is based on the results of a critical review and expert workshops. This interdisciplinary approach contributes a unique holistic perspective to facilitate and structure the discourse on sustainable AI. Further, it provides a concrete framework that lays the foundation for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#40065;&#26834;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#19978;&#28216;&#20219;&#21153;&#20998;&#25104;&#20960;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#24182;&#24212;&#29992;&#26497;&#23567;&#26497;&#22823;&#25439;&#22833;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#20445;&#35777;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#22343;&#21248;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12070</link><description>&lt;p&gt;
&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#28216;&#20219;&#21153;&#36866;&#24212;&#24615;&#30340;&#20219;&#21153;&#40065;&#26834;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Task-Robust Pre-Training for Worst-Case Downstream Adaptation. (arXiv:2306.12070v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#40065;&#26834;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#19978;&#28216;&#20219;&#21153;&#20998;&#25104;&#20960;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#24182;&#24212;&#29992;&#26497;&#23567;&#26497;&#22823;&#25439;&#22833;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#20445;&#35777;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#22343;&#21248;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#20851;&#24515;&#27169;&#22411;&#19981;&#20165;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;&#21512;&#29702;&#30340;&#26465;&#20214;&#21464;&#21270;&#19979;&#30340;&#34892;&#20026;&#12290;&#24403;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#26102;&#65292;&#21516;&#26679;&#30340;&#21746;&#23398;&#20063;&#36866;&#29992;&#12290;&#28982;&#32780;&#65292;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#24182;&#19981;&#20250;&#22312;&#19968;&#31995;&#21015;&#30456;&#20851;&#19979;&#28216;&#20219;&#21153;&#20013;&#22343;&#21248;&#22320;&#34920;&#29616;&#33391;&#22909;&#12290;&#26412;&#25991;&#32771;&#34385;&#39044;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#20445;&#35777;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#22343;&#21248;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#31216;&#27492;&#30446;&#26631;&#20026;&#19979;&#28216;&#20219;&#21153;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#23558;&#19978;&#28216;&#20219;&#21153;&#20998;&#25104;&#20960;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#24182;&#24212;&#29992;&#31616;&#21333;&#30340;minimax loss &#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training has achieved remarkable success when transferred to downstream tasks. In machine learning, we care about not only the good performance of a model but also its behavior under reasonable shifts of condition. The same philosophy holds when pre-training a foundation model. However, the foundation model may not uniformly behave well for a series of related downstream tasks. This happens, for example, when conducting mask recovery regression where the recovery ability or the training instances diverge like pattern features are extracted dominantly on pre-training, but semantic features are also required on a downstream task. This paper considers pre-training a model that guarantees a uniformly good performance over the downstream tasks. We call this goal as $\textit{downstream-task robustness}$. Our method first separates the upstream task into several representative ones and applies a simple minimax loss for pre-training. We then design an efficient algorithm to solve the minim
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#37319;&#26679;&#32593;&#32476;&#32467;&#26500;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#22312;&#24674;&#22797;&#32593;&#32476;&#30340;&#22270;&#24418;&#32467;&#26500;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21518;&#39564;&#27010;&#29575;&#30340;&#20934;&#30830;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2306.11380</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Take on Gaussian Process Networks. (arXiv:2306.11380v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#37319;&#26679;&#32593;&#32476;&#32467;&#26500;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#22312;&#24674;&#22797;&#32593;&#32476;&#30340;&#22270;&#24418;&#32467;&#26500;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21518;&#39564;&#27010;&#29575;&#30340;&#20934;&#30830;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#32593;&#32476;&#65288;GPNs&#65289;&#26159;&#19968;&#31867;&#26377;&#21521;&#22270;&#27169;&#22411;&#65292;&#20854;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#20316;&#20026;&#32593;&#32476;&#20013;&#27599;&#20010;&#21464;&#37327;&#32473;&#23450;&#20854;&#29238;&#21464;&#37327;&#30340;&#26465;&#20214;&#26399;&#26395;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#35813;&#27169;&#22411;&#20801;&#35768;&#20197;&#32039;&#20945;&#20294;&#28789;&#27963;&#30340;&#26041;&#24335;&#25551;&#36848;&#36830;&#32493;&#32852;&#21512;&#20998;&#24067;&#65292;&#23545;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#20165;&#20570;&#26368;&#23569;&#30340;&#21442;&#25968;&#20551;&#35774;&#12290;GPNs&#30340;&#36125;&#21494;&#26031;&#32467;&#26500;&#23398;&#20064;&#38656;&#35201;&#35745;&#31639;&#32593;&#32476;&#32467;&#26500;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#21363;&#20351;&#22312;&#20302;&#32500;&#24773;&#20917;&#19979;&#65292;&#36825;&#20063;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#33945;&#29305;&#21345;&#32599;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#26469;&#20174;&#32593;&#32476;&#32467;&#26500;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#36981;&#24490;&#36125;&#21494;&#26031;&#33539;&#24335;&#65292;&#36890;&#36807;&#36793;&#32536;&#20284;&#28982;&#27604;&#36739;&#27169;&#22411;&#65292;&#24182;&#35745;&#31639;GPN&#29305;&#24449;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#27169;&#25311;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24674;&#22797;&#32593;&#32476;&#30340;&#22270;&#24418;&#32467;&#26500;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20854;&#21518;&#39564;&#30340;&#20934;&#30830;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Networks (GPNs) are a class of directed graphical models which employ Gaussian processes as priors for the conditional expectation of each variable given its parents in the network. The model allows describing continuous joint distributions in a compact but flexible manner with minimal parametric assumptions on the dependencies between variables. Bayesian structure learning of GPNs requires computing the posterior over graphs of the network and is computationally infeasible even in low dimensions. This work implements Monte Carlo and Markov Chain Monte Carlo methods to sample from the posterior distribution of network structures. As such, the approach follows the Bayesian paradigm, comparing models via their marginal likelihood and computing the posterior probability of the GPN features. Simulation studies show that our method outperforms state-of-the-art algorithms in recovering the graphical structure of the network and provides an accurate approximation of its poste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#21442;&#32771;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#12289;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#29992;&#25143;&#25511;&#21046;&#30340;&#36523;&#20221;&#12290;&#35813;&#26550;&#26500;&#20351;&#29992;&#21435;&#20013;&#24515;&#21270;&#26631;&#35782;&#31526;&#36827;&#34892;&#36523;&#20221;&#39564;&#35777;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#23454;&#29616;&#24378;&#22823;&#30340;&#23433;&#20840;&#24615;&#21644;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#65292;&#24182;&#33021;&#26681;&#25454;&#38656;&#27714;&#38598;&#25104;&#21508;&#31181;&#39069;&#22806;&#30340;&#20803;&#32032;&#65292;&#26159;&#19968;&#20010;&#36866;&#29992;&#33539;&#22260;&#24191;&#27867;&#30340; BCFL &#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.10841</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#25903;&#25345;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#21442;&#32771;&#26550;&#26500;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Blockchain-Enabled Federated Learning: A Reference Architecture Design, Implementation, and Verification. (arXiv:2306.10841v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#21442;&#32771;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#12289;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#29992;&#25143;&#25511;&#21046;&#30340;&#36523;&#20221;&#12290;&#35813;&#26550;&#26500;&#20351;&#29992;&#21435;&#20013;&#24515;&#21270;&#26631;&#35782;&#31526;&#36827;&#34892;&#36523;&#20221;&#39564;&#35777;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#23454;&#29616;&#24378;&#22823;&#30340;&#23433;&#20840;&#24615;&#21644;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#65292;&#24182;&#33021;&#26681;&#25454;&#38656;&#27714;&#38598;&#25104;&#21508;&#31181;&#39069;&#22806;&#30340;&#20803;&#32032;&#65292;&#26159;&#19968;&#20010;&#36866;&#29992;&#33539;&#22260;&#24191;&#27867;&#30340; BCFL &#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;BCFL&#65289;&#21442;&#32771;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#23558;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#30340;&#12289;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#23562;&#37325;&#25968;&#25454;&#38544;&#31169;&#21644;&#29992;&#25143;&#25511;&#21046;&#30340;&#36523;&#20221;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#25112;&#30053;&#24615;&#22320;&#37319;&#29992;&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#26631;&#35782;&#31526;&#65288;DID&#65289;&#30340;&#36523;&#20221;&#39564;&#35777;&#31995;&#32479;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#20351;&#29992;&#20854;&#33258;&#20027; DID &#23433;&#20840;&#22320;&#35748;&#35777;&#24182;&#33719;&#24471;&#23545;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#36825;&#20123;&#20449;&#24687;&#34987;&#35760;&#24405;&#22312;&#21306;&#22359;&#38142;&#19978;&#12290;&#36890;&#36807;&#25191;&#34892;&#26234;&#33021;&#21512;&#32422;&#26469;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#24615;&#21644;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#26159;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340; BCFL &#21442;&#32771;&#26550;&#26500;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#38656;&#27714;&#21644;&#29992;&#20363;&#38598;&#25104;&#21508;&#31181;&#39069;&#22806;&#30340;&#20803;&#32032;&#65292;&#20351;&#20854;&#25104;&#20026;&#24191;&#27867;&#36866;&#29992;&#30340; BCFL &#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an innovative reference architecture for blockchain-enabled federated learning (BCFL), a state-of-the-art approach that amalgamates the strengths of federated learning and blockchain technology. This results in a decentralized, collaborative machine learning system that respects data privacy and user-controlled identity. Our architecture strategically employs a decentralized identifier (DID)-based authentication system, allowing participants to authenticate and then gain access to the federated learning platform securely using their self-sovereign DIDs, which are recorded on the blockchain. Ensuring robust security and efficient decentralization through the execution of smart contracts is a key aspect of our approach. Moreover, our BCFL reference architecture provides significant extensibility, accommodating the integration of various additional elements, as per specific requirements and use cases, thereby rendering it an adaptable solution for a wide range of BCFL 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;MARBLE&#65292;&#19968;&#20010;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#20026;&#38899;&#20048;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.10548</link><description>&lt;p&gt;
MARBLE&#65306;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MARBLE: Music Audio Representation Benchmark for Universal Evaluation. (arXiv:2306.10548v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;MARBLE&#65292;&#19968;&#20010;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#20026;&#38899;&#20048;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33402;&#26415;&#19982;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20043;&#38388;&#20132;&#21449;&#30340;&#24191;&#27867;&#26102;&#20195;&#20013;&#65292;&#20363;&#22914;&#22270;&#20687;&#29983;&#25104;&#21644;&#34394;&#26500;&#20849;&#21019;&#65292;&#38899;&#20048;&#30340;AI&#20173;&#28982;&#30456;&#23545;&#21021;&#27493;&#65292;&#29305;&#21035;&#26159;&#22312;&#38899;&#20048;&#29702;&#35299;&#26041;&#38754;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#35780;&#20272;&#22522;&#20934;MARBLE&#65292;&#26088;&#22312;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#23450;&#20041;&#21253;&#25324;&#22768;&#23398;&#65292;&#28436;&#22863;&#65292;&#20048;&#35889;&#21644;&#39640;&#32423;&#25551;&#36848;&#22312;&#20869;&#30340;&#22235;&#20010;&#23618;&#27425;&#30340;&#32508;&#21512;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;8&#20010;&#20844;&#20849;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;14&#39033;&#20219;&#21153;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21327;&#35758;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#22522;&#20110;&#38899;&#20048;&#24405;&#38899;&#24320;&#21457;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34920;&#24449;&#30340;&#20844;&#24179;&#21644;&#26631;&#20934;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;MARBLE&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#37325;&#29992;&#30340;&#24037;&#20855;&#24211;&#65292;&#20197;&#25903;&#25345;&#31038;&#21306;&#39537;&#21160;&#30340;&#23458;&#35266;&#22522;&#20934;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of extensive intersection between art and Artificial Intelligence (AI), such as image generation and fiction co-creation, AI for music remains relatively nascent, particularly in music understanding. This is evident in the limited work on deep music representations, the scarcity of large-scale datasets, and the absence of a universal and community-driven benchmark. To address this issue, we introduce the Music Audio Representation Benchmark for universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy with four hierarchy levels, including acoustic, performance, score, and high-level description. We then establish a unified protocol based on 14 tasks on 8 public-available datasets, providing a fair and standard assessment of representations of all open-sourced pre-trained models developed on music recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and re
&lt;/p&gt;</description></item><item><title>PEAR&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26041;&#27861;&#65292;&#29992;&#20110;Boosting&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#12290;&#23427;&#36890;&#36807;&#23545;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26469;&#29983;&#25104;&#39640;&#25928;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#26469;&#35757;&#32451;&#20998;&#23618;&#20195;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PEAR&#33021;&#22815;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06394</link><description>&lt;p&gt;
PEAR: &#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#29992;&#20110;Boosting&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning. (arXiv:2306.06394v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06394
&lt;/p&gt;
&lt;p&gt;
PEAR&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26041;&#27861;&#65292;&#29992;&#20110;Boosting&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#12290;&#23427;&#36890;&#36807;&#23545;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26469;&#29983;&#25104;&#39640;&#25928;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#26469;&#35757;&#32451;&#20998;&#23618;&#20195;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PEAR&#33021;&#22815;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#21033;&#29992;&#26102;&#38388;&#25277;&#35937;&#21644;&#22686;&#21152;&#30340;&#25506;&#32034;&#24615;&#33021;&#35299;&#20915;&#22797;&#26434;&#30340;&#38271;&#26399;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#38750;&#38745;&#24577;&#24615;&#65292;&#20998;&#23618;&#20195;&#29702;&#38590;&#20197;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#65288;PEAR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#23569;&#37327;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#65292;&#20135;&#29983;&#39640;&#25928;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;&#65292;&#28982;&#21518;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#32852;&#21512;&#20248;&#21270;HRL&#20195;&#29702;&#12290;&#25105;&#20204;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#26469;$(i)$&#38480;&#21046;&#25105;&#20204;&#26041;&#27861;&#30340;&#27425;&#20248;&#24615;&#65292;&#21644;$(ii)$&#25512;&#23548;&#20986;&#20351;&#29992;RL&#21644;IL&#30340;&#24191;&#20041;&#21363;&#25554;&#21363;&#29992;&#30340;&#26694;&#26550;&#36827;&#34892;&#32852;&#21512;&#20248;&#21270;&#12290;PEAR&#20351;&#29992;&#19968;&#20123;&#19987;&#23478;&#28436;&#31034;&#65292;&#24182;&#23545;&#20219;&#21153;&#32467;&#26500;&#36827;&#34892;&#26368;&#23567;&#30340;&#38480;&#21046;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#19982;&#20856;&#22411;&#30340;&#27169;&#22411;&#33258;&#30001;RL&#31639;&#27861;&#38598;&#25104;&#65292;&#20135;&#29983;&#19968;&#20010;&#23454;&#29992;&#30340;HRL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning (HRL) has the potential to solve complex long horizon tasks using temporal abstraction and increased exploration. However, hierarchical agents are difficult to train due to inherent non-stationarity. We present primitive enabled adaptive relabeling (PEAR), a two-phase approach where we first perform adaptive relabeling on a few expert demonstrations to generate efficient subgoal supervision, and then jointly optimize HRL agents by employing reinforcement learning (RL) and imitation learning (IL). We perform theoretical analysis to $(i)$ bound the sub-optimality of our approach, and $(ii)$ derive a generalized plug-and-play framework for joint optimization using RL and IL. PEAR uses a handful of expert demonstrations and makes minimal limiting assumptions on the task structure. Additionally, it can be easily integrated with typical model free RL algorithms to produce a practical HRL algorithm. We perform experiments on challenging robotic environments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#22312;TSP&#23454;&#20363;&#35757;&#32451;&#20043;&#21069;&#65292;&#23558;&#31070;&#32463;&#27169;&#22411;&#29992;&#30456;&#20851;&#31639;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.06064</link><description>&lt;p&gt;
&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural Algorithmic Reasoning for Combinatorial Optimisation. (arXiv:2306.06064v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#22312;TSP&#23454;&#20363;&#35757;&#32451;&#20043;&#21069;&#65292;&#23558;&#31070;&#32463;&#27169;&#22411;&#29992;&#30456;&#20851;&#31639;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;NP&#38590;/&#23436;&#20840;&#32452;&#21512;&#38382;&#39064;&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#36229;&#36234;&#20256;&#32479;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;&#20854;&#38271;&#26399;&#30446;&#26631;&#26159;&#36890;&#36807;&#23398;&#20064;&#20165;&#20174;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#26356;&#20248;&#35299;&#26469;&#36229;&#36234;&#25163;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#32780;&#26053;&#34892;&#21830;&#38382;&#39064;(TSP)&#26159;&#32463;&#24120;&#34987;&#36825;&#20123;&#26041;&#27861;&#30596;&#20934;&#30340;&#19968;&#20010;&#37325;&#35201;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#35299;&#20915;TSP&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24120;&#24120;&#24573;&#30053;&#20102;&#38382;&#39064;&#22266;&#26377;&#30340;&#8220;&#31639;&#27861;&#8221;&#26412;&#36136;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#35774;&#35745;&#29992;&#20110;TSP&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#24120;&#24120;&#21033;&#29992;&#35832;&#22914;&#26597;&#25214;&#26368;&#23567;&#29983;&#25104;&#26641;&#20043;&#31867;&#30340;&#25104;&#29087;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#25913;&#36827;TSP&#38382;&#39064;&#30340;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#23545;TSP&#23454;&#20363;&#36827;&#34892;&#35757;&#32451;&#20043;&#21069;&#65292;&#22312;&#30456;&#20851;&#31639;&#27861;&#19978;&#23545;&#25105;&#20204;&#30340;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving NP-hard/complete combinatorial problems with neural networks is a challenging research area that aims to surpass classical approximate algorithms. The long-term objective is to outperform hand-designed heuristics for NP-hard/complete problems by learning to generate superior solutions solely from training data. The Travelling Salesman Problem (TSP) is a prominent combinatorial optimisation problem often targeted by such approaches. However, current neural-based methods for solving TSP often overlook the inherent "algorithmic" nature of the problem. In contrast, heuristics designed for TSP frequently leverage well-established algorithms, such as those for finding the minimum spanning tree. In this paper, we propose leveraging recent advancements in neural algorithmic reasoning to improve the learning of TSP problems. Specifically, we suggest pre-training our neural model on relevant algorithms before training it on TSP instances. Our results demonstrate that, using this learning
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#65292;&#36827;&#19968;&#27493;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.19190</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#36870;&#36817;&#20284;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Inverse Approximation Theory for Nonlinear Recurrent Neural Networks. (arXiv:2305.19190v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#65292;&#36827;&#19968;&#27493;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#26469;&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#12290;&#36825;&#26159;&#36817;&#20284;&#29702;&#35770;&#20013;&#30340;&#19968;&#31181;&#31216;&#20026;Bernstein&#22411;&#32467;&#26524;&#30340;&#32467;&#26524;&#65292;&#23427;&#22312;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#20551;&#35774;&#31354;&#38388;&#26377;&#25928;&#36924;&#36817;&#30340;&#26465;&#20214;&#19979;&#25512;&#23548;&#20986;&#30446;&#26631;&#20989;&#25968;&#30340;&#23646;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#21487;&#20197;&#34987;&#20855;&#26377;hardtanh/tanh&#28608;&#27963;&#20989;&#25968;&#30340;RNNs&#31283;&#23450;&#36924;&#36817;&#30340;&#26102;&#20505;&#65292;&#24517;&#39035;&#20855;&#26377;&#19968;&#20010;&#25351;&#25968;&#34928;&#20943;&#30340;&#35760;&#24518;&#32467;&#26500;--&#36825;&#20010;&#27010;&#24565;&#21487;&#20197;&#34987;&#26126;&#30830;&#23450;&#20041;&#12290;&#36825;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#37327;&#21270;&#20102;RNN&#26550;&#26500;&#22312;&#23398;&#20064;&#20855;&#26377;&#38271;&#26399;&#35760;&#24518;&#30340;&#24207;&#21015;&#20851;&#31995;&#26102;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove an inverse approximation theorem for the approximation of nonlinear sequence-to-sequence relationships using RNNs. This is a so-called Bernstein-type result in approximation theory, which deduces properties of a target function under the assumption that it can be effectively approximated by a hypothesis space. In particular, we show that nonlinear sequence relationships, viewed as functional sequences, that can be stably approximated by RNNs with hardtanh/tanh activations must have an exponential decaying memory structure -- a notion that can be made precise. This extends the previously identified curse of memory in linear RNNs into the general nonlinear setting, and quantifies the essential limitations of the RNN architecture for learning sequential relationships with long-term memory. Based on the analysis, we propose a principled reparameterization method to overcome the limitations. Our theoretical results are confirmed by numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#25968;&#20540;&#32534;&#30721;&#21644;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#28151;&#21512;&#25968;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22810;&#20998;&#31867;&#27169;&#22411;&#21644;SRG&#31639;&#27861;&#26469;&#29983;&#25104;&#35299;&#37322;&#24615;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.18437</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#22312;&#31867;&#21035;&#21644;&#28151;&#21512;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65306;&#26080;&#25439;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Explainable Machine Learning for Categorical and Mixed Data with Lossless Visualization. (arXiv:2305.18437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#25968;&#20540;&#32534;&#30721;&#21644;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#28151;&#21512;&#25968;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22810;&#20998;&#31867;&#27169;&#22411;&#21644;SRG&#31639;&#27861;&#26469;&#29983;&#25104;&#35299;&#37322;&#24615;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#28151;&#21512;&#25968;&#25454;&#26500;&#24314;&#20934;&#30830;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19968;&#30452;&#26159;&#31639;&#27861;&#38754;&#23545;&#38750;&#25968;&#20540;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#20540;&#32534;&#30721;&#26041;&#26696;&#21644;&#26080;&#25439;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25903;&#25345;&#20934;&#30830;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22810;&#20998;&#31867;&#27169;&#22411;&#21644;&#28436;&#31034;&#20854;&#37325;&#35201;&#20316;&#29992;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#28151;&#21512;&#25968;&#25454;&#31867;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#21253;&#65292;&#20197;&#23545;&#28151;&#21512;&#25968;&#25454;&#30340;&#25152;&#26377;&#20869;&#37096;&#25805;&#20316;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#12290;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#39034;&#24207;&#35268;&#21017;&#29983;&#25104;&#65288;SRG&#65289;&#8221;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#35745;&#31639;&#23454;&#39564;&#20013;&#25104;&#21151;&#35780;&#20272;&#35813;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building accurate and interpretable Machine Learning (ML) models for heterogeneous/mixed data is a long-standing challenge for algorithms designed for numeric data. This work focuses on developing numeric coding schemes for non-numeric attributes for ML algorithms to support accurate and explainable ML models, methods for lossless visualization of n-D non-numeric categorical data with visual rule discovery in these visualizations, and accurate and explainable ML models for categorical data. This study proposes a classification of mixed data types and analyzes their important role in Machine Learning. It presents a toolkit for enforcing interpretability of all internal operations of ML algorithms on mixed data with a visual data exploration on mixed data. A new Sequential Rule Generation (SRG) algorithm for explainable rule generation with categorical data is proposed and successfully evaluated in multiple computational experiments. This work is one of the steps to the full scope ML alg
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FAVAS&#31639;&#27861;&#65292;&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#29615;&#22659;&#19979;&#35757;&#32451;DNNs&#30340;&#26032;&#22411;&#20013;&#24515;&#21270;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;FAVAS&#31639;&#27861;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.16099</link><description>&lt;p&gt;
FAVAS: &#24102;&#26377;&#24322;&#27493;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#24179;&#22343;&#30340;&#26032;&#22411;&#20013;&#24515;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FAVAS: Federated AVeraging with ASynchronous clients. (arXiv:2305.16099v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FAVAS&#31639;&#27861;&#65292;&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#29615;&#22659;&#19979;&#35757;&#32451;DNNs&#30340;&#26032;&#22411;&#20013;&#24515;&#21270;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;FAVAS&#31639;&#27861;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20013;&#24515;&#21270;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FAVAS&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#22823;&#22411;&#26080;&#32447;&#32593;&#32476;&#19978;&#20280;&#32553;&#21516;&#27493;&#36890;&#20449;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23458;&#25143;&#31471;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#35745;&#31639;&#36895;&#24230;&#65292;&#24322;&#27493;&#26356;&#26032;&#21487;&#33021;&#20250;&#23548;&#33268;&#26174;&#30528;&#30340;&#20559;&#24046;&#65288;&#23545;&#8220;&#24555;&#36895;&#8221;&#23458;&#25143;&#31471;&#26356;&#26377;&#21033;&#65289;&#12290;&#22240;&#27492;&#65292;FL&#30340;&#23454;&#38469;&#37096;&#32626;&#38656;&#35201;&#22788;&#29702;&#22312;&#36890;&#20449;/&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#20855;&#26377;&#24378;&#28872;&#21464;&#21270;&#30340;&#35745;&#31639;&#36895;&#24230;&#30340;&#29992;&#25143;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;FAVAS&#22312;&#24179;&#28369;&#30340;&#38750;&#20984;&#29615;&#22659;&#20013;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#20180;&#32454;&#27604;&#36739;&#20102;&#33719;&#24471;&#30340;&#25910;&#25947;&#20445;&#35777;&#19982;&#29616;&#26377;&#36793;&#30028;&#65288;&#22914;&#26524;&#26377;&#65289;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FAVAS&#31639;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel centralized Asynchronous Federated Learning (FL) framework, FAVAS, for training Deep Neural Networks (DNNs) in resource-constrained environments. Despite its popularity, ``classical'' federated learning faces the increasingly difficult task of scaling synchronous communication over large wireless networks. Moreover, clients typically have different computing resources and therefore computing speed, which can lead to a significant bias (in favor of ``fast'' clients) when the updates are asynchronous. Therefore, practical deployment of FL requires to handle users with strongly varying computing speed in communication/resource constrained setting. We provide convergence guarantees for FAVAS in a smooth, non-convex environment and carefully compare the obtained convergence guarantees with existing bounds, when they are available. Experimental results show that the FAVAS algorithm outperforms current methods on standard benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#24322;&#36136;&#24615;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#38598;&#20307;&#20851;&#31995;&#25512;&#26029;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#38598;&#20307;&#22320;&#25512;&#26029;&#19981;&#21516;&#36793;&#30340;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65292;&#20351;&#29992;&#29289;&#29702;&#24863;&#24212;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;&#25512;&#26029;&#20934;&#30830;&#24615;&#21644;&#20445;&#25345;&#29289;&#29702;&#20445;&#30495;&#24230;&#26041;&#38754;&#19968;&#33268;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00557</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#24322;&#36136;&#24615;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#38598;&#20307;&#20851;&#31995;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Collective Relational Inference for learning physics-consistent heterogeneous particle interactions. (arXiv:2305.00557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#24322;&#36136;&#24615;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#38598;&#20307;&#20851;&#31995;&#25512;&#26029;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#38598;&#20307;&#22320;&#25512;&#26029;&#19981;&#21516;&#36793;&#30340;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65292;&#20351;&#29992;&#29289;&#29702;&#24863;&#24212;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;&#25512;&#26029;&#20934;&#30830;&#24615;&#21644;&#20445;&#25345;&#29289;&#29702;&#20445;&#30495;&#24230;&#26041;&#38754;&#19968;&#33268;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#22312;&#33258;&#28982;&#30028;&#21644;&#24037;&#31243;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#25581;&#31034;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#23450;&#24459;&#20855;&#26377;&#22522;&#26412;&#37325;&#35201;&#24615;&#65292;&#20294;&#30001;&#20110;&#24213;&#23618;&#37197;&#32622;&#22797;&#26434;&#24615;&#32780;&#20855;&#26377;&#26497;&#22823;&#30340;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#21457;&#29616;&#21516;&#36136;&#31995;&#32479;&#31890;&#23376;&#36712;&#36857;&#20013;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26080;&#27861;&#25581;&#31034;&#24322;&#36136;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32780;&#36825;&#31181;&#31995;&#32479;&#22312;&#29616;&#23454;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20854;&#20013;&#22810;&#20010;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#21516;&#26102;&#23384;&#22312;&#65292;&#24182;&#19988;&#38656;&#35201;&#20851;&#31995;&#25512;&#26029;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26041;&#27861;&#29992;&#20110;&#20851;&#31995;&#25512;&#26029;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#29305;&#24449;&#65306;&#39318;&#20808;&#65292;&#23427;&#38598;&#20307;&#22320;&#25512;&#26029;&#19981;&#21516;&#36793;&#30340;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65307;&#20854;&#27425;&#65292;&#23427;&#20351;&#29992;&#29289;&#29702;&#24863;&#24212;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#25512;&#26029;&#30340;&#30456;&#20114;&#20316;&#29992;&#20934;&#30830;&#24615;&#21644;&#20445;&#25345;&#29289;&#29702;&#20445;&#30495;&#24230;&#26041;&#38754;&#19968;&#33268;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#23450;&#20102;&#20855;&#26377;&#37325;&#35201;&#29289;&#29702;&#24847;&#20041;&#30340;&#26032;&#22411;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65292;&#25581;&#31034;&#20102;&#32479;&#27835;&#31995;&#32479;&#30340;&#38544;&#34255;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#22312;&#25552;&#39640;&#29289;&#29702;&#24615;&#36136;&#30340;&#39044;&#27979;&#26041;&#38754;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interacting particle systems are ubiquitous in nature and engineering. Revealing particle interaction laws is of fundamental importance but also particularly challenging due to underlying configurational complexities. Recently developed machine learning methods show great potential in discovering pairwise interactions from particle trajectories in homogeneous systems. However, they fail to reveal interactions in heterogeneous systems that are prevalent in reality, where multiple interaction types coexist simultaneously and relational inference is required. Here, we propose a novel probabilistic method for relational inference, which possesses two distinctive characteristics compared to existing methods. First, it infers the interaction types of different edges collectively, and second, it uses a physics-induced graph neural network to learn physics-consistent pairwise interactions. We evaluate the proposed methodology across several benchmark datasets and demonstrate that it is consist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#24212;&#29992;&#28145;&#24230;&#38598;&#21512;&#37327;&#21270;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#26694;&#26550;&#65292;&#20854;&#22312;&#22238;&#24402;&#20934;&#30830;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#38752;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16210</link><description>&lt;p&gt;
&#28145;&#24230;&#38598;&#21512;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#37327;&#21270;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Quantifying Calibrated Uncertainty via Deep Ensembles in Multi-output Regression Task. (arXiv:2303.16210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#24212;&#29992;&#28145;&#24230;&#38598;&#21512;&#37327;&#21270;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#26694;&#26550;&#65292;&#20854;&#22312;&#22238;&#24402;&#20934;&#30830;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#38752;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#21512;&#26159;&#36924;&#36817;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#19968;&#31181;&#31616;&#21333;&#30452;&#25509;&#30340;&#26041;&#27861;&#65292;&#24050;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#20998;&#31867;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#25506;&#31350;&#35813;&#26041;&#27861;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#39044;&#27979;&#23548;&#24377;&#32467;&#26500;&#30340;&#31354;&#27668;&#21160;&#21147;&#24615;&#33021;&#12290;&#36890;&#36807;&#20180;&#32454;&#30740;&#31350;&#38598;&#21512;&#20013;&#31070;&#32463;&#32593;&#32476;&#25968;&#37327;&#30340;&#24433;&#21709;&#65292;&#35266;&#23519;&#21040;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#26222;&#36941;&#23384;&#22312;&#20302;&#20272;&#30340;&#36235;&#21183;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20107;&#21518;&#26657;&#20934;&#30340;&#28145;&#24230;&#38598;&#21512;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20854;&#25913;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24615;&#33021;&#12290;&#30452;&#35266;&#22320;&#23558;&#20854;&#19982;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#36827;&#34892;&#27604;&#36739;&#65292;&#36825;&#26159;&#24037;&#31243;&#20013;&#26368;&#24120;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#22238;&#24402;&#20934;&#30830;&#24615;&#12289;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#38752;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#20063;&#30740;&#31350;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensemble is a simple and straightforward approach for approximating Bayesian inference and has been successfully applied to many classification tasks. This study aims to comprehensively investigate this approach in the multi-output regression task to predict the aerodynamic performance of a missile configuration. By scrutinizing the effect of the number of neural networks used in the ensemble, an obvious trend toward underconfidence in estimated uncertainty is observed. In this context, we propose the deep ensemble framework that applies the post-hoc calibration method, and its improved uncertainty quantification performance is demonstrated. It is compared with Gaussian process regression, the most prevalent model for uncertainty quantification in engineering, and is proven to have superior performance in terms of regression accuracy, reliability of estimated uncertainty, and training efficiency. Finally, the impact of the suggested framework on the results of Bayesian optimizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#38754;&#21521;&#22478;&#24066;&#35745;&#31639;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#23398;&#20064;&#39046;&#22495;&#30340;&#21457;&#23637;&#29616;&#29366;&#65292;&#21253;&#25324;&#20854;&#26694;&#26550;&#12289;&#23454;&#29616;&#26041;&#27861;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#20197;&#21450;&#24403;&#21069;&#30340;&#30740;&#31350;&#28909;&#28857;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.14483</link><description>&lt;p&gt;
&#38754;&#21521;&#22478;&#24066;&#35745;&#31639;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey. (arXiv:2303.14483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#38754;&#21521;&#22478;&#24066;&#35745;&#31639;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#23398;&#20064;&#39046;&#22495;&#30340;&#21457;&#23637;&#29616;&#29366;&#65292;&#21253;&#25324;&#20854;&#26694;&#26550;&#12289;&#23454;&#29616;&#26041;&#27861;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#20197;&#21450;&#24403;&#21069;&#30340;&#30740;&#31350;&#28909;&#28857;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20808;&#36827;&#20256;&#24863;&#22120;&#21644;&#22823;&#22411;&#25968;&#25454;&#24211;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#22478;&#24066;&#31995;&#32479;&#26102;&#31354;&#25968;&#25454;&#34987;&#35760;&#24405;&#21644;&#23384;&#20648;&#12290;&#36825;&#20123;&#25968;&#25454;&#30340;&#28436;&#21270;&#27169;&#24335;&#30340;&#39044;&#27979;&#23398;&#20064;&#26159;&#22478;&#24066;&#35745;&#31639;&#20013;&#22522;&#26412;&#20294;&#37325;&#35201;&#30340;&#24490;&#29615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#22478;&#24066;&#26234;&#33021;&#31649;&#29702;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#22312;&#20132;&#36890;&#12289;&#29615;&#22659;&#12289;&#23433;&#20840;&#12289;&#20844;&#20849;&#21355;&#29983;&#31561;&#39046;&#22495;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#32479;&#35745;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#25429;&#25417;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#30340;&#22797;&#26434;&#30456;&#20851;&#24615;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNN&#65289;&#30340;&#26694;&#26550;&#12290;STGNN&#36890;&#36807;&#38598;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#21508;&#31181;&#26102;&#38388;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#22797;&#26434;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#25552;&#21462;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#23398;&#20064;&#20219;&#21153;&#65292;&#26377;&#25928;&#35774;&#35745;&#31354;&#38388;&#20381;&#36182;&#23398;&#20064;&#27169;&#22359;&#12289;&#26102;&#38388;&#20381;&#36182;&#23398;&#20064;&#27169;&#22359;&#12289;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#27861;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of sophisticated sensors and large database technologies, more and more spatio-temporal data in urban systems are recorded and stored. Predictive learning for the evolution patterns of these spatio-temporal data is a basic but important loop in urban computing, which can better support urban intelligent management decisions, especially in the fields of transportation, environment, security, public health, etc. Since traditional statistical learning and deep learning methods can hardly capture the complex correlations in the urban spatio-temporal data, the framework of spatio-temporal graph neural network (STGNN) has been proposed in recent years. STGNNs enable the extraction of complex spatio-temporal dependencies by integrating graph neural networks (GNNs) and various temporal learning methods. However, for different predictive learning tasks, it is a challenging problem to effectively design the spatial dependencies learning modules, temporal dependencies learnin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28151;&#27788;&#39044;&#27979;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#21457;&#29616;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#35268;&#27169;&#12289;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#20998;&#23618;&#31070;&#32463;&#22522;&#30784;&#20989;&#25968;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.08011</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#26377;&#25928;&#22320;&#39044;&#27979;&#21508;&#31181;&#28151;&#27788;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Large statistical learning models effectively forecast diverse chaotic systems. (arXiv:2303.08011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08011
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28151;&#27788;&#39044;&#27979;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#21457;&#29616;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#35268;&#27169;&#12289;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#20998;&#23618;&#31070;&#32463;&#22522;&#30784;&#20989;&#25968;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#28151;&#27788;&#21644;&#19981;&#21487;&#39044;&#27979;&#26159;&#21516;&#20041;&#35789;&#65292;&#20294;&#26368;&#36817;&#32479;&#35745;&#39044;&#27979;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20174;&#22797;&#26434;&#31995;&#32479;&#30340;&#38271;&#26102;&#38388;&#35266;&#27979;&#20013;&#33719;&#24471;&#24847;&#24819;&#19981;&#21040;&#30340;&#35265;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#35268;&#27169;&#19978;&#30340;&#28151;&#27788;&#39044;&#27979;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545; 135 &#31181;&#19981;&#21516;&#20302;&#32500;&#28151;&#27788;&#31995;&#32479;&#30340;&#20247;&#21253;&#25968;&#25454;&#24211;&#36827;&#34892; 24 &#31181;&#20195;&#34920;&#24615;&#26368;&#39640;&#30340;&#22810;&#20803;&#39044;&#27979;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#35268;&#27169;&#30340;&#39046;&#22495;&#19981;&#21487;&#30693;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22987;&#32456;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#20135;&#29983;&#25345;&#32493;&#25968;&#21313;&#20010;&#26446;&#38597;&#26222;&#35834;&#22827;&#26102;&#38388;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#26368;&#20339;&#30340;&#28151;&#27788;&#39044;&#27979;&#32467;&#26524;&#30001;&#26368;&#36817;&#24341;&#20837;&#30340;&#20998;&#23618;&#31070;&#32463;&#22522;&#30784;&#20989;&#25968;&#27169;&#22411;&#23454;&#29616;&#65292;&#20294;&#21363;&#20351;&#26159;&#36890;&#29992;&#30340;&#21464;&#21387;&#22120;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20063;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29289;&#29702;&#21551;&#21457;&#24335;&#28151;&#21512;&#26041;&#27861;&#22914;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#20648;&#23618;&#35745;&#31639;&#26426;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#23588;&#20854;&#26159;&#22312;&#26356;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chaos and unpredictability are traditionally synonymous, yet recent advances in statistical forecasting suggest that large machine learning models can derive unexpected insight from extended observation of complex systems. Here, we study the forecasting of chaos at scale, by performing a large-scale comparison of 24 representative state-of-the-art multivariate forecasting methods on a crowdsourced database of 135 distinct low-dimensional chaotic systems. We find that large, domain-agnostic time series forecasting methods based on artificial neural networks consistently exhibit strong forecasting performance, in some cases producing accurate predictions lasting for dozens of Lyapunov times. Best-in-class results for forecasting chaos are achieved by recently-introduced hierarchical neural basis function models, though even generic transformers and recurrent neural networks perform strongly. However, physics-inspired hybrid methods like neural ordinary equations and reservoir computers c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#36229;&#22768;&#24433;&#20687;&#39044;&#27979;&#20799;&#31185;&#30097;&#20284;&#38417;&#23614;&#28814;&#30340;&#35786;&#26029;&#12289;&#31649;&#29702;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#36229;&#22768;&#24433;&#20687;&#21644;&#20020;&#24202;&#12289;&#23454;&#39564;&#23460;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#25512;&#24191;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#21040;&#22810;&#35270;&#22270;&#21644;&#19981;&#23436;&#25972;&#27010;&#24565;&#38598;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.14460</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#21644;&#33021;&#22815;&#24178;&#39044;&#30340;&#36229;&#22768;&#25104;&#20687;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#20799;&#31185;&#38417;&#23614;&#28814;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Intervenable Ultrasonography-based Machine Learning Models for Pediatric Appendicitis. (arXiv:2302.14460v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#36229;&#22768;&#24433;&#20687;&#39044;&#27979;&#20799;&#31185;&#30097;&#20284;&#38417;&#23614;&#28814;&#30340;&#35786;&#26029;&#12289;&#31649;&#29702;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#36229;&#22768;&#24433;&#20687;&#21644;&#20020;&#24202;&#12289;&#23454;&#39564;&#23460;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#25512;&#24191;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#21040;&#22810;&#35270;&#22270;&#21644;&#19981;&#23436;&#25972;&#27010;&#24565;&#38598;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38417;&#23614;&#28814;&#26159;&#20799;&#31185;&#33145;&#37096;&#25163;&#26415;&#30340;&#26368;&#24120;&#35265;&#21407;&#22240;&#20043;&#19968;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#25903;&#25345;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#35786;&#26029;&#21644;&#31649;&#29702;&#24739;&#32773;&#65292;&#21516;&#26102;&#20943;&#23569;&#38750;&#20851;&#38190;&#25163;&#26415;&#30340;&#25968;&#37327;&#12290;&#20197;&#24448;&#30340;&#38417;&#23614;&#28814;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20027;&#35201;&#20851;&#27880;&#20020;&#24202;&#12289;&#23454;&#39564;&#23460;&#12289;&#35780;&#20998;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#65292;&#20027;&#35201;&#24573;&#35270;&#20102;&#33145;&#37096;&#36229;&#22768;&#25104;&#20687;&#65292;&#36825;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#21644;&#26041;&#20415;&#33719;&#24471;&#30340;&#35786;&#26029;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#39564;&#35777;&#20102;&#21033;&#29992;&#36229;&#22768;&#24433;&#20687;&#39044;&#27979;&#30097;&#20284;&#38417;&#23614;&#28814;&#35786;&#26029;&#12289;&#31649;&#29702;&#21644;&#20005;&#37325;&#31243;&#24230;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#30001;579&#21517;&#20799;&#31185;&#24739;&#32773;&#30340;1709&#24133;&#36229;&#22768;&#24433;&#20687;&#65292;&#20197;&#21450;&#20020;&#24202;&#21644;&#23454;&#39564;&#23460;&#25968;&#25454;&#26500;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#36129;&#29486;&#22312;&#20110;&#23558;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#35270;&#22270;&#21644;&#19981;&#23436;&#25972;&#27010;&#24565;&#38598;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#36866;&#29992;&#20110;&#24178;&#39044;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Appendicitis is among the most frequent reasons for pediatric abdominal surgeries. With recent advances in machine learning, data-driven decision support could help clinicians diagnose and manage patients while reducing the number of non-critical surgeries. Previous decision support systems for appendicitis focused on clinical, laboratory, scoring and computed tomography data, mainly ignoring abdominal ultrasound, a noninvasive and readily available diagnostic modality. To this end, we developed and validated interpretable machine learning models for predicting the diagnosis, management and severity of suspected appendicitis using ultrasound images. Our models were trained on a dataset comprising 579 pediatric patients with 1709 ultrasound images accompanied by clinical and laboratory data. Our methodological contribution is the generalization of concept bottleneck models to prediction problems with multiple views and incomplete concept sets. Notably, such models lend themselves to int
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#36125;&#21494;&#26031;&#23618;&#27425;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25903;&#25345;&#20998;&#25674;&#25512;&#26029;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;&#21644;&#24615;&#33021;&#39564;&#35777;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23545;&#22235;&#20010;&#23618;&#27425;&#35777;&#25454;&#31215;&#32047;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2301.11873</link><description>&lt;p&gt;
&#27604;&#36739;&#36125;&#21494;&#26031;&#23618;&#27425;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Method for Comparing Bayesian Hierarchical Models. (arXiv:2301.11873v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11873
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#36125;&#21494;&#26031;&#23618;&#27425;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25903;&#25345;&#20998;&#25674;&#25512;&#26029;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;&#21644;&#24615;&#33021;&#39564;&#35777;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23545;&#22235;&#20010;&#23618;&#27425;&#35777;&#25454;&#31215;&#32047;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#65288;BMC&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#31454;&#20105;&#35745;&#31639;&#27169;&#22411;&#30340;&#30456;&#23545;&#20248;&#21183;&#65292;&#24182;&#23558;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#21040;&#27169;&#22411;&#36873;&#25321;&#20915;&#31574;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#32500;&#23884;&#22871;&#21442;&#25968;&#32467;&#26500;&#65292;BMC&#22312;&#24120;&#35265;&#30340;&#23618;&#27425;&#27169;&#22411;&#20013;&#24120;&#24120;&#38590;&#20197;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#20219;&#20309;&#21487;&#23454;&#20363;&#21270;&#20026;&#27010;&#29575;&#31243;&#24207;&#30340;&#23618;&#27425;&#27169;&#22411;&#38598;&#36827;&#34892;BMC&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#25903;&#25345;&#20998;&#25674;&#25512;&#26029;&#65292;&#23427;&#21487;&#20197;&#22312;&#20219;&#20309;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#20043;&#21069;&#65292;&#23545;&#21518;&#39564;&#27169;&#22411;&#27010;&#29575;&#36827;&#34892;&#39640;&#25928;&#30340;&#37325;&#26032;&#20272;&#35745;&#21644;&#24555;&#36895;&#24615;&#33021;&#39564;&#35777;&#12290;&#22312;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#39564;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#27604;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26725;&#24335;&#25277;&#26679;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25152;&#26377;BMC&#35774;&#32622;&#20013;&#20986;&#33394;&#30340;&#20998;&#25674;&#25512;&#26029;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#20808;&#21069;&#34987;&#35748;&#20026;&#26159;&#22235;&#20010;&#23618;&#27425;&#35777;&#25454;&#31215;&#32047;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian model comparison (BMC) offers a principled approach for assessing the relative merits of competing computational models and propagating uncertainty into model selection decisions. However, BMC is often intractable for the popular class of hierarchical models due to their high-dimensional nested parameter structure. To address this intractability, we propose a deep learning method for performing BMC on any set of hierarchical models which can be instantiated as probabilistic programs. Since our method enables amortized inference, it allows efficient re-estimation of posterior model probabilities and fast performance validation prior to any real-data application. In a series of extensive validation studies, we benchmark the performance of our method against the state-of-the-art bridge sampling method and demonstrate excellent amortized inference across all BMC settings. We then showcase our method by comparing four hierarchical evidence accumulation models that have previously b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#26694;&#30340;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#19979;&#39640;&#25928;&#22320;&#35782;&#21035;&#39118;&#38505;&#34913;&#37327;&#23450;&#20041;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#39640;&#27010;&#29575;&#36793;&#30028;&#26694;&#21644;&#36873;&#25321;&#19979;&#19968;&#20010;&#35780;&#20272;&#28857;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11588</link><description>&lt;p&gt;
&#22522;&#20110;&#36793;&#30028;&#26694;&#30340;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#39118;&#38505;&#34913;&#37327;
&lt;/p&gt;
&lt;p&gt;
Bounding Box-based Multi-objective Bayesian Optimization of Risk Measures under Input Uncertainty. (arXiv:2301.11588v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#26694;&#30340;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#19979;&#39640;&#25928;&#22320;&#35782;&#21035;&#39118;&#38505;&#34913;&#37327;&#23450;&#20041;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#39640;&#27010;&#29575;&#36793;&#30028;&#26694;&#21644;&#36873;&#25321;&#19979;&#19968;&#20010;&#35780;&#20272;&#28857;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;MOBO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#65288;IU&#65289;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#35782;&#21035;&#30001;&#39118;&#38505;&#34913;&#37327;&#23450;&#20041;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65288;PF&#65289;&#12290;&#29616;&#26377;&#30340;IU&#19979;&#24085;&#32047;&#25176;&#20248;&#21270;&#30340;BO&#26041;&#27861;&#26159;&#29305;&#23450;&#39118;&#38505;&#25110;&#32773;&#27809;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#19968;&#33324;&#39118;&#38505;&#34913;&#37327;&#24182;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;&#25152;&#25552;&#26041;&#27861;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20551;&#35774;&#40657;&#31665;&#20989;&#25968;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;GP&#27169;&#22411;&#26500;&#24314;&#39118;&#38505;&#34913;&#37327;&#30340;&#39640;&#27010;&#29575;&#36793;&#30028;&#26694;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#38750;&#25903;&#37197;&#36793;&#30028;&#26694;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#36793;&#30028;&#26694;&#30340;&#25311;&#36317;&#31163;&#30340;&#26368;&#22823;&#20540;&#23450;&#20041;&#30340;&#26368;&#22823;&#26368;&#23567;&#36317;&#31163;&#36873;&#25321;&#19979;&#19968;&#20010;&#35780;&#20272;&#28857;&#30340;&#26041;&#27861;&#12290;&#20316;&#20026;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#26377;&#38480;&#27425;&#36845;&#20195;&#20013;&#36820;&#22238;&#20219;&#24847;&#31934;&#30830;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a novel multi-objective Bayesian optimization (MOBO) method to efficiently identify the Pareto front (PF) defined by risk measures for black-box functions under the presence of input uncertainty (IU). Existing BO methods for Pareto optimization in the presence of IU are risk-specific or without theoretical guarantees, whereas our proposed method addresses general risk measures and has theoretical guarantees. The basic idea of the proposed method is to assume a Gaussian process (GP) model for the black-box function and to construct high-probability bounding boxes for the risk measures using the GP model. Furthermore, in order to reduce the uncertainty of non-dominated bounding boxes, we propose a method of selecting the next evaluation point using a maximin distance defined by the maximum value of a quasi distance based on bounding boxes. As theoretical analysis, we prove that the algorithm can return an arbitrary-accurate solution in a finite number of iterati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#30456;&#26426;&#30340;&#35835;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#29289;&#20307;&#37325;&#26032;&#23450;&#21521;&#25511;&#21046;&#22120;&#65292;&#21487;&#20197;&#23454;&#26102;&#12289;&#21160;&#24577;&#22320;&#37325;&#26032;&#23450;&#21521;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#20013;&#20301;&#25968;&#37325;&#26032;&#23450;&#21521;&#26102;&#38388;&#25509;&#36817;&#20110;&#19971;&#31186;&#12290;&#35813;&#25511;&#21046;&#22120;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#23545;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#26032;&#29289;&#20307;&#24418;&#29366;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2211.11744</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24863;&#30693;&#23454;&#29616;&#25163;&#25345;&#28789;&#24039;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Visual Dexterity: In-hand Dexterous Manipulation from Depth. (arXiv:2211.11744v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11744
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#30456;&#26426;&#30340;&#35835;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#29289;&#20307;&#37325;&#26032;&#23450;&#21521;&#25511;&#21046;&#22120;&#65292;&#21487;&#20197;&#23454;&#26102;&#12289;&#21160;&#24577;&#22320;&#37325;&#26032;&#23450;&#21521;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#20013;&#20301;&#25968;&#37325;&#26032;&#23450;&#21521;&#26102;&#38388;&#25509;&#36817;&#20110;&#19971;&#31186;&#12290;&#35813;&#25511;&#21046;&#22120;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#23545;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#26032;&#29289;&#20307;&#24418;&#29366;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#25345;&#29289;&#20307;&#30340;&#37325;&#26032;&#23450;&#21521;&#23545;&#20110;&#25191;&#34892;&#35768;&#22810;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#38750;&#24120;&#24517;&#35201;&#65292;&#20363;&#22914;&#22312;&#24403;&#21069;&#26426;&#22120;&#20154;&#26080;&#27861;&#35302;&#21450;&#30340;&#32467;&#26500;&#19981;&#22826;&#23436;&#21892;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#24037;&#20855;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#37325;&#26032;&#23450;&#21521;&#31995;&#32479;&#65292;&#20551;&#35774;&#20197;&#19979;&#24773;&#20917;&#20043;&#19968;&#25110;&#22810;&#31181;&#24773;&#20917;&#21516;&#26102;&#23384;&#22312;&#65306;&#20165;&#37325;&#26032;&#23450;&#21521;&#20855;&#26377;&#31616;&#21333;&#24418;&#29366;&#30340;&#29305;&#23450;&#29289;&#20307;&#12289;&#37325;&#26032;&#23450;&#21521;&#33539;&#22260;&#26377;&#38480;&#12289;&#24930;&#36895;&#25110;&#20934;&#38745;&#24577;&#25805;&#20316;&#12289;&#20165;&#27169;&#25311;&#32467;&#26524;&#12289;&#38656;&#35201;&#19987;&#29992;&#19988;&#26114;&#36149;&#30340;&#20256;&#24863;&#22120;&#22871;&#20214;&#20197;&#21450;&#20854;&#20182;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#37096;&#32626;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20570;&#36825;&#20123;&#20551;&#35774;&#30340;&#36890;&#29992;&#29289;&#20307;&#37325;&#26032;&#23450;&#21521;&#25511;&#21046;&#22120;&#12290;&#23427;&#20351;&#29992;&#26469;&#33258;&#21333;&#20010;&#26222;&#36890;&#28145;&#24230;&#25668;&#20687;&#26426;&#30340;&#35835;&#25968;&#65292;&#20197;&#23454;&#26102;&#26041;&#24335;&#36890;&#36807;&#20219;&#24847;&#26059;&#36716;&#21160;&#24577;&#37325;&#26032;&#23450;&#21521;&#22797;&#26434;&#19988;&#26032;&#39062;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#20013;&#20301;&#25968;&#37325;&#26032;&#23450;&#21521;&#26102;&#38388;&#25509;&#36817;&#20110;&#19971;&#31186;&#12290;&#35813;&#25511;&#21046;&#22120;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#26032;&#29289;&#20307;&#24418;&#29366;&#19978;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324; ...
&lt;/p&gt;
&lt;p&gt;
In-hand object reorientation is necessary for performing many dexterous manipulation tasks, such as tool use in less structured environments that remain beyond the reach of current robots. Prior works built reorientation systems assuming one or many of the following: reorienting only specific objects with simple shapes, limited range of reorientation, slow or quasistatic manipulation, simulation-only results, the need for specialized and costly sensor suites, and other constraints which make the system infeasible for real-world deployment. We present a general object reorientation controller that does not make these assumptions. It uses readings from a single commodity depth camera to dynamically reorient complex and new object shapes by any rotation in real-time, with the median reorientation time being close to seven seconds. The controller is trained using reinforcement learning in simulation and evaluated in the real world on new object shapes not used for training, including the m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#30417;&#30563;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#21033;&#29992;&#27492;&#26041;&#27861;&#21487;&#20197;&#26500;&#24314;&#20986;&#31867;&#20284;&#20110;&#40657;&#30418;&#27169;&#22411;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#35813;&#20915;&#31574;&#26641;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#21644;&#32039;&#20945;&#24615;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.09894</link><description>&lt;p&gt;
&#22522;&#20110;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#30417;&#30563;&#29305;&#24449;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Supervised Feature Compression based on Counterfactual Analysis. (arXiv:2211.09894v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#30417;&#30563;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#21033;&#29992;&#27492;&#26041;&#27861;&#21487;&#20197;&#26500;&#24314;&#20986;&#31867;&#20284;&#20110;&#40657;&#30418;&#27169;&#22411;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#35813;&#20915;&#31574;&#26641;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#21644;&#32039;&#20945;&#24615;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#24050;&#25104;&#20026;&#20107;&#21518;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#35813;&#24037;&#20316;&#26088;&#22312;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#35782;&#21035;&#39044;&#35757;&#32451;&#40657;&#30418;&#27169;&#22411;&#30340;&#37325;&#35201;&#20915;&#31574;&#36793;&#30028;&#12290;&#35813;&#20449;&#24687;&#29992;&#20110;&#22312;&#25968;&#25454;&#38598;&#20013;&#26500;&#24314;&#19968;&#31181;&#21487;&#35843;&#25972;&#32454;&#24230;&#30340;&#29305;&#24449;&#30340;&#30417;&#30563;&#31163;&#25955;&#21270;&#12290;&#20351;&#29992;&#31163;&#25955;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#31867;&#20284;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#20294;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#32039;&#20945;&#24615;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#31232;&#30095;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations are becoming a de-facto standard in post-hoc interpretable machine learning. For a given classifier and an instance classified in an undesired class, its counterfactual explanation corresponds to small perturbations of that instance that allows changing the classification outcome. This work aims to leverage Counterfactual Explanations to detect the important decision boundaries of a pre-trained black-box model. This information is used to build a supervised discretization of the features in the dataset with a tunable granularity. Using the discretized dataset, an optimal Decision Tree can be trained that resembles the black-box model, but that is interpretable and compact. Numerical results on real-world datasets show the effectiveness of the approach in terms of accuracy and sparsity.
&lt;/p&gt;</description></item><item><title>&#35813;&#31639;&#27861;&#36890;&#36807;&#36873;&#25321;&#24615;&#36801;&#31227;&#23398;&#20064;&#20174;&#22810;&#20010;&#23567;&#36741;&#21161;&#38598;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#20174;&#20855;&#26377;&#8220;&#30456;&#20284;&#8221;&#29305;&#24449;&#30340;&#32452;&#32455;&#22270;&#20687;&#20013;&#23398;&#20064;&#26579;&#33394;&#27169;&#24335;&#65292;&#20197;&#23454;&#29616;&#20687;&#30149;&#29702;&#23398;&#23478;&#19968;&#26679;&#33258;&#21160;&#35780;&#20998;&#32452;&#32455;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2209.05954</link><description>&lt;p&gt;
&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#20687;&#30149;&#29702;&#23398;&#23478;&#19968;&#26679;&#33258;&#21160;&#35780;&#20998;&#32452;&#32455;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Automatically Score Tissue Images Like a Pathologist by Transfer Learning. (arXiv:2209.05954v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31639;&#27861;&#36890;&#36807;&#36873;&#25321;&#24615;&#36801;&#31227;&#23398;&#20064;&#20174;&#22810;&#20010;&#23567;&#36741;&#21161;&#38598;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#20174;&#20855;&#26377;&#8220;&#30456;&#20284;&#8221;&#29305;&#24449;&#30340;&#32452;&#32455;&#22270;&#20687;&#20013;&#23398;&#20064;&#26579;&#33394;&#27169;&#24335;&#65292;&#20197;&#23454;&#29616;&#20687;&#30149;&#29702;&#23398;&#23478;&#19968;&#26679;&#33258;&#21160;&#35780;&#20998;&#32452;&#32455;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#26159;&#20840;&#29699;&#31532;&#20108;&#22823;&#27515;&#20129;&#21407;&#22240;&#12290;&#26089;&#26399;&#35786;&#26029;&#30284;&#30151;&#21487;&#20197;&#25405;&#25937;&#24456;&#22810;&#29983;&#21629;&#12290;&#30149;&#29702;&#23398;&#23478;&#24517;&#39035;&#25163;&#21160;&#26597;&#30475;&#32452;&#32455;&#24494;&#38453;&#21015; (TMA) &#22270;&#20687;&#20197;&#35782;&#21035;&#32959;&#30244;&#65292;&#36825;&#21487;&#33021;&#20250;&#32791;&#36153;&#26102;&#38388;&#12289;&#19981;&#19968;&#33268;&#19988;&#20027;&#35266;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#26816;&#27979;&#32959;&#30244;&#30340;&#31639;&#27861;&#35201;&#20040;&#27809;&#26377;&#36798;&#21040;&#30149;&#29702;&#23398;&#23478;&#30340;&#20934;&#30830;&#27700;&#24179;&#65292;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#21442;&#19982;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#20855;&#26377;&#19981;&#21516;&#24418;&#29366;&#12289;&#22823;&#23567;&#21644;&#20301;&#32622;&#30340; TMA &#22270;&#20687;&#21487;&#33021;&#20855;&#26377;&#30456;&#21516;&#30340;&#24471;&#20998;&#12290;&#30001;&#20110;&#21307;&#30103;&#32452;&#32455;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#38480;&#21046;&#65292;&#23398;&#20064; TMA &#22270;&#20687;&#30340;&#26579;&#33394;&#27169;&#24335;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#26469;&#33258;&#19981;&#21516;&#30284;&#30151;&#31867;&#22411;&#30340; TMA &#22270;&#20687;&#21487;&#33021;&#20855;&#26377;&#20849;&#21516;&#30340;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20294;&#30452;&#25509;&#20351;&#29992;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#36873;&#25321;&#24615;&#36801;&#31227;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#23567;&#36741;&#21161;&#38598;&#30340;&#30693;&#35782;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#25552;&#21462;&#26174;&#31034;&#8220;&#31867;&#20284;&#8221;&#30340;&#32452;&#32455;&#22270;&#20687;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;TMA&#22270;&#20687;&#35780;&#20998;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer is the second leading cause of death in the world. Diagnosing cancer early on can save many lives. Pathologists have to look at tissue microarray (TMA) images manually to identify tumors, which can be time-consuming, inconsistent and subjective. Existing algorithms that automatically detect tumors have either not achieved the accuracy level of a pathologist or require substantial human involvements. A major challenge is that TMA images with different shapes, sizes, and locations can have the same score. Learning staining patterns in TMA images requires a huge number of images, which are severely limited due to privacy concerns and regulations in medical organizations. TMA images from different cancer types may have common characteristics that could provide valuable information, but using them directly harms the accuracy. By selective transfer learning from multiple small auxiliary sets, the proposed algorithm is able to extract knowledge from tissue images showing a ``similar" s
&lt;/p&gt;</description></item><item><title>FAIR4Cov&#26159;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;COVID-19&#24739;&#32773;&#65292;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.10581</link><description>&lt;p&gt;
FAIR4Cov&#65306;&#29992;&#20110; COVID-19 &#26816;&#27979;&#30340;&#34701;&#21512;&#38899;&#39057;&#23454;&#20363;&#21644;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
FAIR4Cov: Fused Audio Instance and Representation for COVID-19 Detection. (arXiv:2204.10581v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10581
&lt;/p&gt;
&lt;p&gt;
FAIR4Cov&#26159;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;COVID-19&#24739;&#32773;&#65292;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36523;&#20307;&#22768;&#38899;&#30340;&#20998;&#31867;&#25216;&#26415;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#34987;&#30740;&#31350;&#29992;&#20110;&#25903;&#25345;&#35786;&#26029;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#22312;&#32954;&#37096;&#30142;&#30149;&#26041;&#38754;&#12290;&#38024;&#23545; COVID-19 &#30123;&#24773;&#30340;&#32039;&#36843;&#24615;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#27169;&#22411;&#34987;&#24320;&#21457;&#26469;&#22522;&#20110;&#22768;&#23398;&#36755;&#20837;&#35782;&#21035; COVID-19 &#24739;&#32773;&#12290;&#22823;&#22810;&#25968;&#27169;&#22411;&#20391;&#37325;&#20110;&#21683;&#22013;&#65292;&#22240;&#20026;&#24178;&#21683;&#26159; COVID-19 &#26368;&#20026;&#20154;&#25152;&#30693;&#30340;&#30151;&#29366;&#12290;&#28982;&#32780;&#65292;&#21628;&#21560;&#21644;&#35328;&#35821;&#31561;&#20854;&#20182;&#36523;&#20307;&#22768;&#38899;&#20063;&#34987;&#21457;&#29616;&#19982; COVID-19 &#30456;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; FAIR4Cov&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#36523;&#20307;&#22768;&#38899;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#12290;FAIR4Cov &#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#19968;&#20010;&#33258;&#27880;&#24847;&#34701;&#21512;&#21333;&#20803;&#65292;&#23427;&#30340;&#35757;&#32451;&#30446;&#30340;&#26159;&#24314;&#31435;&#22810;&#20010;&#36523;&#20307;&#22768;&#38899;&#21644;&#38899;&#39057;&#34920;&#31034;&#30340;&#20851;&#31995;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35774;&#32622;&#20102;&#23454;&#39564;&#65292;&#24182;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#26041;&#27861;&#65292;&#21253;&#25324;&#36328;&#25968;&#25454;&#38598;&#35780;&#20272;&#21644;&#26089;&#26399;&#26816;&#27979;&#35774;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FAIR4Cov &#32988;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#21033;&#29992;&#21508;&#31181;&#36523;&#20307;&#22768;&#38899;&#26816;&#27979; COVID-19 &#24739;&#32773;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio-based classification techniques on body sounds have long been studied to support diagnostic decisions, particularly in pulmonary diseases. In response to the urgency of the COVID-19 pandemic, a growing number of models are developed to identify COVID-19 patients based on acoustic input. Most models focus on cough because the dry cough is the best-known symptom of COVID-19. However, other body sounds, such as breath and speech, have also been revealed to correlate with COVID-19 as well. In this work, rather than relying on a specific body sound, we propose Fused Audio Instance and Representation for COVID-19 Detection (FAIR4Cov). It relies on constructing a joint feature vector obtained from a plurality of body sounds in waveform and spectrogram representation. The core component of FAIR4Cov is a self-attention fusion unit that is trained to establish the relation of multiple body sounds and audio representations and integrate it into a compact feature vector. We set up our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#65288;NETS&#65289;&#39044;&#27979;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;NETS-ImpGAN&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#35757;&#32451;&#19981;&#23436;&#25972;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#22270;&#26102;&#24207;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.02271</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Networked Time Series Prediction with Incomplete Data. (arXiv:2110.02271v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.02271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#65288;NETS&#65289;&#39044;&#27979;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;NETS-ImpGAN&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#35757;&#32451;&#19981;&#23436;&#25972;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#22270;&#26102;&#24207;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#65288;NETS&#65289;&#26159;&#32473;&#23450;&#22270;&#19978;&#30340;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#65292;&#27599;&#20010;&#33410;&#28857;&#23545;&#24212;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#23427;&#22312;&#26234;&#33021;&#20132;&#36890;&#12289;&#29615;&#22659;&#30417;&#27979;&#21644;&#31227;&#21160;&#32593;&#32476;&#31649;&#29702;&#31561;&#39046;&#22495;&#37117;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#26159;&#22522;&#20110;&#21382;&#21490;&#20540;&#21644;&#24213;&#23618;&#22270;&#26469;&#39044;&#27979;NETS&#30340;&#26410;&#26469;&#20540;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#38656;&#35201;&#23436;&#25972;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#20256;&#24863;&#22120;&#25925;&#38556;&#12289;&#19981;&#23436;&#20840;&#24863;&#30693;&#35206;&#30422;&#31561;&#21407;&#22240;&#65292;&#25968;&#25454;&#32570;&#22833;&#26159;&#24120;&#35265;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;NETS&#39044;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NETS-ImpGAN&#65292;&#19968;&#31181;&#21487;&#20197;&#22312;&#21382;&#21490;&#21644;&#26410;&#26469;&#30340;&#32570;&#22833;&#20540;&#19978;&#35757;&#32451;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#22270;&#26102;&#24207;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
A networked time series (NETS) is a family of time series on a given graph, one for each node. It has found a wide range of applications from intelligent transportation, environment monitoring to mobile network management. An important task in such applications is to predict the future values of a NETS based on its historical values and the underlying graph. Most existing methods require complete data for training. However, in real-world scenarios, it is not uncommon to have missing data due to sensor malfunction, incomplete sensing coverage, etc. In this paper, we study the problem of NETS prediction with incomplete data. We propose NETS-ImpGAN, a novel deep learning framework that can be trained on incomplete data with missing values in both history and future. Furthermore, we propose novel Graph Temporal Attention Networks by incorporating the attention mechanism to capture both inter-time series correlations and temporal correlations. We conduct extensive experiments on three real-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25913;&#36827;Moment Accountant&#26041;&#27861;&#65292;DP-SGD&#20855;&#26377;&#21487;&#20851;&#38381;&#24418;&#24335;&#30340;$(\epsilon&#65292;\delta)$-DP&#20445;&#35777;&#65292;&#24182;&#19988;&#20854;&#20445;&#35777;&#25509;&#36817;&#26159;&#32039;&#23494;&#30340;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2102.09030</link><description>&lt;p&gt;
&#35770;DP-SGD&#30340;Moment Accountant&#26041;&#27861;&#30340;&#32039;&#23494;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Tightness of the Moment Accountant for DP-SGD. (arXiv:2102.09030v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.09030
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25913;&#36827;Moment Accountant&#26041;&#27861;&#65292;DP-SGD&#20855;&#26377;&#21487;&#20851;&#38381;&#24418;&#24335;&#30340;$(\epsilon&#65292;\delta)$-DP&#20445;&#35777;&#65292;&#24182;&#19988;&#20854;&#20445;&#35777;&#25509;&#36817;&#26159;&#32039;&#23494;&#30340;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#20379;&#24046;&#20998;&#38544;&#31169;&#65292;&#22312;&#24046;&#20998;&#38544;&#31169;SGD&#65288;DP-SGD&#65289;&#20013;&#65292;&#22312;&#25191;&#34892;&#21098;&#20999;&#25805;&#20316;&#21518;&#65292;&#21521;&#26412;&#22320;SGD&#26356;&#26032;&#28155;&#21152;&#26631;&#20934;&#24046;&#20026;$ \sigma $&#30340;&#39640;&#26031;&#22122;&#22768;&#12290;&#36890;&#36807;&#38750;&#24179;&#20961;&#22320;&#25913;&#36827;Moment Accountant&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#23553;&#38381;&#24418;&#24335;&#30340;$(\epsilon&#65292;\delta)$-DP&#20445;&#35777;&#65306;&#22914;&#26524;$ \sigma=\sqrt{ 2(\epsilon+\ln(1/\delta))/\epsilon} $&#65292;&#21017;DP-SGD&#26159;$ (\epsilon \leq 1/2&#65292;\delta = 1 / N) $-DP&#65292;&#20854;&#20013;$T$&#33267;&#23569;&#20026;$ \approx 2k^2/\epsilon$&#65292; $(2/e)^2k^2-1/2\geq \ln(N)$&#65292;&#20854;&#20013;$T$&#26159;&#22238;&#21512;&#30340;&#24635;&#25968;&#65292;$ K = kN $&#26159;&#26799;&#24230;&#35745;&#31639;&#30340;&#24635;&#25968;&#65292;&#20854;&#20013;$ k $&#29992;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;$N$&#30340;&#26102;&#20195;&#25968;&#37327;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#34920;&#36798;&#24335;&#25509;&#36817;&#32039;&#65292;&#22312;$T$&#23567;&#20110;&#32422;&#20026;$ 8 $&#20493;&#20110;&#19979;&#30028;$ \approx 2k^2/\epsilon$&#30340;&#24120;&#25968;&#22240;&#23376;&#26102;&#65292;$(\epsilon&#65292;\delta)$-DP&#20445;&#35777;&#23558;&#34987;&#36829;&#21453;&#12290;&#36873;&#25321;&#26368;&#23567;&#21487;&#33021;&#20540;&#30340;$T \approx 2k^2/\epsilon$&#19981;&#20165;&#20250;&#23548;&#33268;&#25509;&#36817;&#23494;&#38598;&#30340;DP&#20445;&#35777;&#65292;&#32780;&#19988;&#36824;&#20250;&#26368;&#23567;&#21270;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to provide differential privacy, Gaussian noise with standard deviation $\sigma$ is added to local SGD updates after performing a clipping operation in Differential Private SGD (DP-SGD). By non-trivially improving the moment account method we prove a closed form $(\epsilon,\delta)$-DP guarantee: DP-SGD is $(\epsilon\leq 1/2,\delta=1/N)$-DP if $\sigma=\sqrt{2(\epsilon +\ln(1/\delta))/\epsilon}$ with $T$ at least $\approx 2k^2/\epsilon$ and $(2/e)^2k^2-1/2\geq \ln(N)$, where $T$ is the total number of rounds, and $K=kN$ is the total number of gradient computations where $k$ measures $K$ in number of epochs of size $N$ of the local data set. We prove that our expression is close to tight in that if $T$ is more than a constant factor $\approx 8$ smaller than the lower bound $\approx 2k^2/\epsilon$, then the $(\epsilon,\delta)$-DP guarantee is violated. Choosing the smallest possible value $T\approx 2k^2/\epsilon$ not only leads to a close to tight DP guarantee, but also minimizes 
&lt;/p&gt;</description></item></channel></rss>