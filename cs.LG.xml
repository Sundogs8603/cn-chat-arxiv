<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>DT-SNN&#26159;&#19968;&#31181;&#36755;&#20837;&#24863;&#30693;&#30340;&#21160;&#24577;&#26102;&#38388;&#27493;&#38271;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#65292; &#21487;&#20197;&#26681;&#25454;&#25968;&#25454;&#29305;&#24615;&#21160;&#24577;&#30830;&#23450;&#26102;&#38388;&#27493;&#38271;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#23558;SNN&#30340;&#25928;&#29575;&#26368;&#22823;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.17346</link><description>&lt;p&gt;
&#36755;&#20837;&#24863;&#30693;&#30340;&#21160;&#24577;&#26102;&#38388;&#27493;&#38271;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39640;&#25928;&#30340;&#20869;&#23384;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Input-Aware Dynamic Timestep Spiking Neural Networks for Efficient In-Memory Computing. (arXiv:2305.17346v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17346
&lt;/p&gt;
&lt;p&gt;
DT-SNN&#26159;&#19968;&#31181;&#36755;&#20837;&#24863;&#30693;&#30340;&#21160;&#24577;&#26102;&#38388;&#27493;&#38271;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#65292; &#21487;&#20197;&#26681;&#25454;&#25968;&#25454;&#29305;&#24615;&#21160;&#24577;&#30830;&#23450;&#26102;&#38388;&#27493;&#38271;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#23558;SNN&#30340;&#25928;&#29575;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#33021;&#22815;&#22788;&#29702;&#31232;&#30095;&#21644;&#20108;&#36827;&#21046;&#33033;&#20914;&#20449;&#24687;&#65292;&#24182;&#36991;&#20813;&#26114;&#36149;&#30340;&#20056;&#27861;&#25805;&#20316;&#65292;&#22240;&#27492;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#34429;&#28982;SNN&#22312;&#20869;&#23384;&#35745;&#31639;&#65288;IMC&#65289;&#26550;&#26500;&#19978;&#30340;&#25928;&#29575;&#21487;&#20197;&#23454;&#29616;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;SNN&#30340;&#33021;&#37327;&#25104;&#26412;&#21644;&#24310;&#36831;&#38543;&#30528;&#22312;IMC&#30828;&#20214;&#19978;&#20351;&#29992;&#30340;&#26102;&#38388;&#27493;&#38271;&#25968;&#37327;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26368;&#22823;&#21270;SNN&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36755;&#20837;&#24863;&#30693;&#30340;&#21160;&#24577;&#26102;&#38388;&#27493;&#38271;SNN&#65288;DT-SNN&#65289;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21160;&#24577;&#30830;&#23450;&#26102;&#38388;&#27493;&#38271;&#30340;&#25968;&#37327;&#65292;&#32780;&#19988;&#36824;&#26681;&#25454;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#24615;&#36827;&#34892;&#35843;&#25972;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#21518;&#35745;&#31639;&#36755;&#20986;&#30340;&#29109;&#65292;&#24182;&#23558;&#20854;&#19982;&#39044;&#23450;&#20041;&#30340;&#38408;&#20540;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#20915;&#23450;&#24403;&#21069;&#26102;&#38388;&#27493;&#38271;&#22788;&#29702;&#30340;&#20449;&#24687;&#26159;&#21542;&#36275;&#20197;&#36827;&#34892;&#26377;&#20449;&#24515;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;DT-SNN&#37096;&#32626;&#22312;IMC&#26550;&#26500;&#19978;&#65292;&#24182;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have recently attracted widespread research interest as an efficient alternative to traditional Artificial Neural Networks (ANNs) because of their capability to process sparse and binary spike information and avoid expensive multiplication operations. Although the efficiency of SNNs can be realized on the In-Memory Computing (IMC) architecture, we show that the energy cost and latency of SNNs scale linearly with the number of timesteps used on IMC hardware. Therefore, in order to maximize the efficiency of SNNs, we propose input-aware Dynamic Timestep SNN (DT-SNN), a novel algorithmic solution to dynamically determine the number of timesteps during inference on an input-dependent basis. By calculating the entropy of the accumulated output after each timestep, we can compare it to a predefined threshold and decide if the information processed at the current timestep is sufficient for a confident prediction. We deploy DT-SNN on an IMC architecture and show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;&#24120;&#35265;&#12289;&#29616;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#25915;&#20987;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#25915;&#20987;&#32773;&#23545;&#20195;&#29702;$\alpha$&#25511;&#21046;&#30340;&#26356;&#19968;&#33324;&#21270;&#25915;&#20987;&#24418;&#24335;&#12290;&#24182;&#35299;&#20915;&#20102;&#20808;&#21069;&#25915;&#20987;&#27169;&#22411;&#20013;&#32570;&#20047;&#21487;&#35777;&#26126;&#38450;&#24481;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17342</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#31574;&#30053;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24191;&#20041;&#25915;&#20987;&#24418;&#24335;&#21644;&#21487;&#35777;&#26126;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL. (arXiv:2305.17342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;&#24120;&#35265;&#12289;&#29616;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#25915;&#20987;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#25915;&#20987;&#32773;&#23545;&#20195;&#29702;$\alpha$&#25511;&#21046;&#30340;&#26356;&#19968;&#33324;&#21270;&#25915;&#20987;&#24418;&#24335;&#12290;&#24182;&#35299;&#20915;&#20102;&#20808;&#21069;&#25915;&#20987;&#27169;&#22411;&#20013;&#32570;&#20047;&#21487;&#35777;&#26126;&#38450;&#24481;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#30740;&#31350;&#30452;&#25509;&#25200;&#21160;&#21463;&#23475;&#32773;&#30340;&#29366;&#24577;/&#21160;&#20316;&#25110;&#22522;&#30784;&#36716;&#31227;&#21160;&#24577;&#20197;&#23637;&#31034;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#33030;&#24369;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#30452;&#25509;&#25805;&#32437;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21478;&#19968;&#31181;&#24120;&#35265;&#19988;&#29616;&#23454;&#30340;&#25915;&#20987;&#35774;&#32622;&#65306;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#30340;&#35774;&#32622;&#20013;&#65292;&#22312;&#37096;&#32626;&#26399;&#38388;&#65292;&#21463;&#23475;&#20195;&#29702;$\nu$&#34987;&#25915;&#20987;&#32773;&#25511;&#21046;&#21478;&#19968;&#20010;&#20195;&#29702;$\alpha$&#20197;&#25932;&#23545;&#26041;&#24335;&#34892;&#21160;&#65292;&#20351;&#29992;&#8220;&#23545;&#25239;&#31574;&#30053;&#8221;&#23545;&#21463;&#23475;&#20195;&#29702;&#36827;&#34892;&#25915;&#20987;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#25915;&#20987;&#27169;&#22411;&#32771;&#34385;&#20102;&#36825;&#31181;&#35774;&#32622;&#65292;&#20294;&#20182;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#25915;&#20987;&#32773;&#21487;&#20197;&#36935;&#21040;&#25269;&#25239;&#65292;&#22240;&#27492;&#21482;&#33021;&#37096;&#20998;&#25511;&#21046;&#20195;&#29702;$\alpha$&#65292;&#21516;&#26102;&#24341;&#20837;&#21487;&#23519;&#35273;&#30340;&#8220;&#24322;&#24120;&#8221;&#34892;&#20026;&#65292;&#36825;&#20123;&#34892;&#20026;&#24456;&#23481;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#24182;&#19988;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#23545;&#25239;&#31574;&#30053;&#30340;&#21487;&#35777;&#26126;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#19968;&#33324;&#21270;&#30340;&#25915;&#20987;&#24418;&#24335;&#65292;&#27169;&#25311;&#20102;&#25915;&#20987;&#32773;&#22312;&#20309;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#25511;&#21046;&#20195;&#29702;$\alpha$&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing works consider direct perturbations of victim's state/action or the underlying transition dynamics to show vulnerability of reinforcement learning agents under adversarial attacks. However, such direct manipulation may not always be feasible in practice. In this paper, we consider another common and realistic attack setup: in a multi-agent RL setting with well-trained agents, during deployment time, the victim agent $\nu$ is exploited by an attacker who controls another agent $\alpha$ to act adversarially against the victim using an \textit{adversarial policy}. Prior attack models under such setup do not consider that the attacker can confront resistance and thus can only take partial control of the agent $\alpha$, as well as introducing perceivable ``abnormal'' behaviors that are easily detectable. A provable defense against these adversarial policies is also lacking. To resolve these issues, we introduce a more general attack formulation that models to what extent the a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#25968;&#20540;&#31639;&#26415;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;PCA&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#23545;&#20197;&#24448;&#26041;&#27861;&#65292;&#20854;&#22312;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#19978;&#22343;&#26377;&#25552;&#21319;&#65292;&#23454;&#29616;&#20102;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#21644;&#39640;&#25928;&#21516;&#24577;&#30005;&#36335;&#65292;&#35745;&#31639;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17341</link><description>&lt;p&gt;
&#20351;&#29992;&#20248;&#21270;&#31354;&#38388;&#30340;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#26469;&#25913;&#36827;&#38544;&#31169;&#20445;&#25252;PCA
&lt;/p&gt;
&lt;p&gt;
Improved Privacy-Preserving PCA Using Space-optimized Homomorphic Matrix Multiplication. (arXiv:2305.17341v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#25968;&#20540;&#31639;&#26415;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;PCA&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#23545;&#20197;&#24448;&#26041;&#27861;&#65292;&#20854;&#22312;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#19978;&#22343;&#26377;&#25552;&#21319;&#65292;&#23454;&#29616;&#20102;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#21644;&#39640;&#25928;&#21516;&#24577;&#30005;&#36335;&#65292;&#35745;&#31639;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36817;&#20284;&#25968;&#20540;&#31639;&#26415;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;PCA&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#31181;&#34987;&#31216;&#20026;PowerMethod&#30340;PCA&#24120;&#35268;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20197;&#21327;&#26041;&#24046;&#30697;&#38453;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#19982;&#25968;&#25454;&#38598;&#30340;&#31532;&#19968;&#20027;&#25104;&#20998;&#23545;&#24212;&#30340;&#36817;&#20284;&#29305;&#24449;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65288;&#22914;Pandas CSCML 21&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20197;&#19979;&#20248;&#21270;&#65306;&#65288;i&#65289;&#20248;&#21270;&#20102;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#25216;&#26415;&#65288;Jiang&#31561;&#20154;SIGSAC 2018&#65289;&#65292;&#35813;&#25216;&#26415;&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35745;&#31639;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65307;&#65288;ii&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#21516;&#24577;&#30005;&#36335;&#26469;&#21516;&#24577;&#35745;&#31639;&#21327;&#26041;&#24046;&#30697;&#38453;&#65307;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#29992;&#20110;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal Component Analysis (PCA) is a pivotal technique in the fields of machine learning and data analysis. In this study, we present a novel approach for privacy-preserving PCA using an approximate numerical arithmetic homomorphic encryption scheme. We build our method upon a proposed PCA routine known as the PowerMethod, which takes the covariance matrix as input and produces an approximate eigenvector corresponding to the first principal component of the dataset. Our method surpasses previous approaches (e.g., Pandas CSCML 21) in terms of efficiency, accuracy, and scalability.  To achieve such efficiency and accuracy, we have implemented the following optimizations: (i) We optimized a homomorphic matrix multiplication technique (Jiang et al. SIGSAC 2018) that will play a crucial role in the computation of the covariance matrix. (ii) We devised an efficient homomorphic circuit for computing the covariance matrix homomorphically. (iii) We designed a novel and efficient homomorphic 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19979;&#26356;&#24555;&#22320;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17333</link><description>&lt;p&gt;
&#21482;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Just Forward Passes. (arXiv:2305.17333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19979;&#26356;&#24555;&#22320;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#22823;&#65292;&#21453;&#21521;&#20256;&#25773;&#38656;&#35201;&#30340;&#23384;&#20648;&#31354;&#38388;&#25968;&#37327;&#21464;&#24471;&#36807;&#39640;&#12290;&#38646;&#38454;&#65288;ZO&#65289;&#26041;&#27861;&#29702;&#35770;&#19978;&#20165;&#20351;&#29992;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#23601;&#21487;&#20197;&#20272;&#35745;&#26799;&#24230;&#65292;&#20294;&#36890;&#24120;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#30340;&#36895;&#24230;&#38750;&#24120;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65288;MeZO&#65289;&#65292;&#23558;&#32463;&#20856;&#30340;ZO-SGD&#26041;&#27861;&#36866;&#24212;&#20110;&#21407;&#22320;&#25805;&#20316;&#65292;&#20174;&#32780;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#21482;&#20351;&#29992;&#19968;&#24352;A100 80GB GPU&#65292;MeZO&#23601;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;300&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#32780;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21487;&#20197;&#22312;&#30456;&#21516;&#30340;&#39044;&#31639;&#19979;&#20165;&#35757;&#32451;&#19968;&#20010;27&#20159;&#20010;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#22411;&#31867;&#22411;&#65288;&#25513;&#30721;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65289;&#12289;&#27169;&#22411;&#35268;&#27169;&#65288;&#39640;&#36798;66B&#65289;&#21644;&#19979;&#28216;&#20219;&#21153;&#65288;&#20998;&#31867;&#12289;&#22810;&#39033;&#36873;&#25321;&#21644;&#29983;&#25104;&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#65288;1&#65289;MeZO&#26126;&#26174;&#20248;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#32447;&#24615;PR&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear pr
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#33021;&#21147;&#26159;&#19968;&#31181;&#24230;&#37327;&#27169;&#22411;&#26377;&#25928;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#21028;&#26029;&#26159;&#21542;&#38656;&#35201;&#33719;&#21462;&#26356;&#22810;&#25968;&#25454;&#25110;&#32773;&#23547;&#25214;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17332</link><description>&lt;p&gt;
&#23398;&#20064;&#33021;&#21147;&#65306;&#27169;&#22411;&#26377;&#25928;&#32500;&#24230;&#30340;&#24230;&#37327;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learning Capacity: A Measure of the Effective Dimensionality of a Model. (arXiv:2305.17332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17332
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#33021;&#21147;&#26159;&#19968;&#31181;&#24230;&#37327;&#27169;&#22411;&#26377;&#25928;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#21028;&#26029;&#26159;&#21542;&#38656;&#35201;&#33719;&#21462;&#26356;&#22810;&#25968;&#25454;&#25110;&#32773;&#23547;&#25214;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#28909;&#21147;&#23398;&#21644;&#25512;&#29702;&#20043;&#38388;&#30340;&#27491;&#24335;&#23545;&#24212;&#20851;&#31995;&#65292;&#23558;&#26679;&#26412;&#25968;&#37327;&#35270;&#20026;&#21453;&#28201;&#24230;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#8220;&#23398;&#20064;&#33021;&#21147;&#8221;&#65292;&#36825;&#26159;&#27169;&#22411;&#26377;&#25928;&#32500;&#24230;&#30340;&#24230;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#35768;&#22810;&#22312;&#20856;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#23398;&#20064;&#33021;&#21147;&#20165;&#21344;&#21442;&#25968;&#25968;&#37327;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#21462;&#20915;&#20110;&#29992;&#20110;&#35757;&#32451;&#30340;&#26679;&#26412;&#25968;&#37327;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#19978;&#19982;&#20174;PAC-Bayesian&#26694;&#26550;&#33719;&#24471;&#30340;&#33021;&#21147;&#27010;&#24565;&#19968;&#33268;&#12290;&#23398;&#20064;&#33021;&#21147;&#20316;&#20026;&#27979;&#35797;&#35823;&#24046;&#30340;&#20989;&#25968;&#19981;&#20250;&#20986;&#29616;&#21452;&#23792;&#19979;&#38477;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#22312;&#38750;&#24120;&#23567;&#21644;&#38750;&#24120;&#22823;&#30340;&#26679;&#26412;&#22823;&#23567;&#22788;&#39281;&#21644;&#65292;&#36825;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#35828;&#26126;&#26159;&#21542;&#24212;&#35813;&#33719;&#21462;&#26356;&#22810;&#25968;&#25454;&#25110;&#32773;&#23547;&#25214;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23398;&#20064;&#33021;&#21147;&#26469;&#29702;&#35299;&#26377;&#25928;&#32500;&#25968;&#65292;&#21363;&#20351;&#26159;&#38750;&#21442;&#25968;&#27169;&#22411;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We exploit a formal correspondence between thermodynamics and inference, where the number of samples can be thought of as the inverse temperature, to define a "learning capacity'' which is a measure of the effective dimensionality of a model. We show that the learning capacity is a tiny fraction of the number of parameters for many deep networks trained on typical datasets, depends upon the number of samples used for training, and is numerically consistent with notions of capacity obtained from the PAC-Bayesian framework. The test error as a function of the learning capacity does not exhibit double descent. We show that the learning capacity of a model saturates at very small and very large sample sizes; this provides guidelines, as to whether one should procure more data or whether one should search for new architectures, to improve performance. We show how the learning capacity can be used to understand the effective dimensionality, even for non-parametric models such as random fores
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#36866;&#24212;&#26816;&#32034;&#22120;(AAR)&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#20174;&#24050;&#30693;&#30340;&#28304;LM&#20013;&#23398;&#20064;LM&#30340;&#20559;&#22909;&#65292;&#33021;&#22815;&#20197;&#36890;&#29992;&#25554;&#20214;&#30340;&#24418;&#24335;&#24110;&#21161;&#30446;&#26631;LM&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.17331</link><description>&lt;p&gt;
&#22522;&#20110;&#22686;&#24378;&#30340;&#36866;&#24212;&#24615;&#26816;&#32034;&#22120;&#20197;&#36890;&#29992;&#25554;&#20214;&#30340;&#24418;&#24335;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In. (arXiv:2305.17331v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#36866;&#24212;&#26816;&#32034;&#22120;(AAR)&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#20174;&#24050;&#30693;&#30340;&#28304;LM&#20013;&#23398;&#20064;LM&#30340;&#20559;&#22909;&#65292;&#33021;&#22815;&#20197;&#36890;&#29992;&#25554;&#20214;&#30340;&#24418;&#24335;&#24110;&#21161;&#30446;&#26631;LM&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#22806;&#37096;&#20449;&#24687;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;(LMs)&#25191;&#34892;&#30693;&#35782;&#23494;&#38598;&#30340;&#20219;&#21153;&#12290;&#26816;&#32034;&#22686;&#24378;&#30340;&#20808;&#21069;&#24037;&#20316;&#36890;&#24120;&#32852;&#21512;&#24494;&#35843;&#26816;&#32034;&#22120;&#21644;LM&#65292;&#20351;&#23427;&#20204;&#32039;&#23494;&#32806;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36890;&#29992;&#26816;&#32034;&#25554;&#20214;&#30340;&#26041;&#26696;&#65306;&#26816;&#32034;&#22120;&#26159;&#35201;&#24110;&#21161;&#30446;&#26631;LM&#30340;&#65292;&#36825;&#20123;LM&#21487;&#33021;&#20107;&#20808;&#19981;&#30693;&#36947;&#25110;&#26080;&#27861;&#19968;&#36215;&#24494;&#35843;&#12290;&#20026;&#20102;&#26816;&#32034;&#20986;&#23545;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;LM&#26377;&#29992;&#30340;&#25991;&#26723;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#36866;&#24212;&#26816;&#32034;&#22120;(AAR)&#65292;&#23427;&#20174;&#24050;&#30693;&#30340;&#28304;LM&#20013;&#23398;&#20064;LM&#30340;&#20559;&#22909;&#12290;&#22312;MMLU&#21644;PopQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#29992;&#23567;&#22411;&#28304;LM&#35757;&#32451;&#30340;AAR&#33021;&#22815;&#26174;&#30528;&#25552;&#39640;&#20174;250M Flan-T5&#21040;175B InstructGPT&#33539;&#22260;&#20869;&#30340;&#26356;&#22823;&#30446;&#26631;LM&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#19981;&#21516;LM&#30340;&#20559;&#22909;&#37325;&#21472;&#65292;&#20351;&#24471;&#20197;&#21333;&#20010;&#28304;LM&#35757;&#32451;&#30340;AAR&#33021;&#22815;&#20316;&#20026;&#21508;&#31181;&#30446;&#26631;LM&#30340;&#36890;&#29992;&#25554;&#20214;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;...
&lt;/p&gt;
&lt;p&gt;
Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM's preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.17330</link><description>&lt;p&gt;
MADiff&#65306;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MADiff: Offline Multi-agent Learning with Diffusion Models. (arXiv:2305.17330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#36817;&#22312;&#21253;&#25324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#20869;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20854;&#20013;&#31574;&#30053;&#36890;&#36807;&#22312;&#22312;&#32447;&#35780;&#20272;&#20013;&#20135;&#29983;&#36712;&#36857;&#26469;&#36827;&#34892;&#35268;&#21010;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#26174;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;DM&#22914;&#20309;&#22312;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#20013;&#25805;&#20316;&#65292;&#20854;&#20013;&#20195;&#29702;&#21830;&#24456;&#38590;&#22312;&#29420;&#31435;&#24314;&#27169;&#27599;&#20010;&#20195;&#29702;&#21830;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#22242;&#38431;&#21512;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;MADiff&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MADiff&#26159;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#23545;&#22810;&#20010;&#25193;&#25955;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#22797;&#26434;&#21327;&#35843;&#24314;&#27169;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MADiff&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#26694;&#26550;&#65292;&#23427;&#26082;&#21487;&#20197;&#34892;&#20026;&#20026;&#20998;&#25955;&#30340;&#25919;&#31574;&#65292;&#21448;&#21487;&#20197;&#20026;&#38598;&#20013;&#25511;&#21046;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#25163;&#24314;&#27169;&#65292;&#24182;&#21487;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model (DM), as a powerful generative model, recently achieved huge success in various scenarios including offline reinforcement learning, where the policy learns to conduct planning by generating trajectory in the online evaluation. However, despite the effectiveness shown for single-agent learning, it remains unclear how DMs can operate in multi-agent problems, where agents can hardly complete teamwork without good coordination by independently modeling each agent's trajectories. In this paper, we propose MADiff, a novel generative multi-agent learning framework to tackle this problem. MADiff is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple diffusion agents. To the best of our knowledge, MADiff is the first diffusion-based multi-agent offline RL framework, which behaves as both a decentralized policy and a centralized controller, which includes opponent modeling and can be used for multi-agent trajectory predic
&lt;/p&gt;</description></item><item><title>&#12298;Zero-TPrune&#12299;&#26159;&#19968;&#20010;&#32771;&#34385;&#21040;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20284;&#24615;&#30340;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#36827;&#34892;&#20196;&#29260;&#21098;&#26525;&#65292;&#20197;&#27714;&#35299;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;Transformer&#27169;&#22411;&#21363;&#25554;&#21363;&#29992;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17328</link><description>&lt;p&gt;
&#12298;Zero-TPrune: &#22522;&#20110;&#39044;&#35757;&#32451;Transformers&#20851;&#27880;&#22270;&#30340;&#38646;&#23556;&#20987;&#20196;&#29260;&#21098;&#26525;&#26041;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers. (arXiv:2305.17328v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17328
&lt;/p&gt;
&lt;p&gt;
&#12298;Zero-TPrune&#12299;&#26159;&#19968;&#20010;&#32771;&#34385;&#21040;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20284;&#24615;&#30340;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#36827;&#34892;&#20196;&#29260;&#21098;&#26525;&#65292;&#20197;&#27714;&#35299;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;Transformer&#27169;&#22411;&#21363;&#25554;&#21363;&#29992;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;Transformer&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#27169;&#22411;&#30340;&#20307;&#31215;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#32780;&#25512;&#29702;&#25104;&#26412;&#21017;&#38543;&#36755;&#20837;&#24207;&#21015;&#20013;&#20196;&#29260;&#25968;&#37327;&#30340;&#24179;&#26041;&#25552;&#39640;&#12290;&#20196;&#29260;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26032;&#20852;&#35299;&#20915;&#26041;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#26131;&#20110;&#22312;&#21508;&#31181;Transformer&#25903;&#25345;&#30340;&#27169;&#22411;&#19978;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20196;&#29260;&#21098;&#26525;&#26041;&#27861;&#38656;&#35201;&#22312;&#21098;&#26525;&#21518;&#25110;&#26399;&#38388;&#36827;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#36825;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#25506;&#35752;&#20102;&#27809;&#26377;&#24494;&#35843;&#30340;&#21363;&#25554;&#21363;&#29992;&#30340;&#39044;&#35757;&#32451;Transformer&#30340;&#21098;&#26525;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#20102;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Zero-TPrune&#65292;&#36825;&#26159;&#19968;&#31181;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#26082;&#32771;&#34385;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21448;&#32771;&#34385;&#30456;&#20284;&#24615;&#26469;&#25191;&#34892;&#20196;&#29260;&#21098;&#26525;&#12290;Zero-TPrune&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#20026;&#20196;&#29260;&#29983;&#25104;&#19968;&#20010;&#37325;&#35201;&#24615;&#25490;&#21517;&#24182;&#31227;&#38500;&#20449;&#24687;&#36739;&#23569;&#30340;&#20196;&#29260;&#12290;&#27880;&#24847;&#30697;&#38453;&#21487;&#29992;&#20110;&#25512;&#26029;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployment of Transformer models on the edge is increasingly challenging due to the exponentially growing model size and inference cost that scales quadratically with the number of tokens in the input sequence. Token pruning is an emerging solution to address this challenge due to its ease of deployment on various Transformer backbones. However, most token pruning methods require a computationally-expensive fine-tuning process after or during pruning, which is not desirable in many cases. Some recent works explore pruning of off-the-shelf pre-trained Transformers without fine-tuning. However, they only take the importance of tokens into consideration. In this work, we propose Zero-TPrune, the first zero-shot method that considers both the importance and similarity of tokens in performing token pruning. Zero-TPrune leverages the attention graph of pre-trained Transformer models to produce an importance rank for tokens and removes the less informative tokens. The attention matrix can be 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;</title><link>http://arxiv.org/abs/2305.17326</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;KL&#25955;&#24230;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Kernel-SSL
&lt;/p&gt;
&lt;p&gt;
Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#36890;&#24120;&#23558;&#19968;&#20010;&#27491;&#38170;&#28857;&#26679;&#26412;&#19982;&#35768;&#22810;&#36127;&#26679;&#26412;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#23436;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#30456;&#21453;&#65292;&#38750;&#23545;&#27604;&#23398;&#20064;&#65292;&#20363;&#22914;BYOL&#12289;SimSiam&#21644;Barlow Twins&#31561;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26174;&#24335;&#20351;&#29992;&#36127;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;SSL&#12290;&#21463;&#23545;&#27604;&#23398;&#20064;&#29616;&#26377;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Kernel-SSL&#65292;&#30452;&#25509;&#20248;&#21270;RKHS&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#12290;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;Kernel-SSL&#22312;&#32447;&#24615;&#35780;&#20272;&#35774;&#32622;&#19979;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#22823;&#24133;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#36827;&#34892;100&#20010;epoch&#30340;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;SimCLR&#34920;&#29616;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;&#65292;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#27493;&#38271;&#30340;&#36873;&#25321;&#21644;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17323</link><description>&lt;p&gt;
&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#30340;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Some Primal-Dual Theory for Subgradient Methods for Strongly Convex Optimization. (arXiv:2305.17323v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;&#65292;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#27493;&#38271;&#30340;&#36873;&#25321;&#21644;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#24378;&#20984;&#20294;&#28508;&#22312;&#38750;&#20809;&#28369;&#38750;Lipschitz&#20248;&#21270;&#30340;&#65288;&#38543;&#26426;&#65289;&#27425;&#26799;&#24230;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#31561;&#20215;&#23545;&#20598;&#25551;&#36848;&#65288;&#31867;&#20284;&#20110;&#23545;&#20598;&#24179;&#22343;&#65289;&#26469;&#25551;&#36848;&#32463;&#20856;&#30340;&#27425;&#26799;&#24230;&#27861;&#65292;&#36817;&#31471;&#27425;&#26799;&#24230;&#27861;&#21644;&#20999;&#25442;&#27425;&#26799;&#24230;&#27861;&#12290;&#36825;&#20123;&#31561;&#20215;&#24615;&#33021;&#22815;&#20197; $O(1/T)$ &#30340;&#36895;&#24230;&#25910;&#25947;&#65292;&#21516;&#26102;&#33021;&#22815;&#22312;&#24378;&#20984;&#20248;&#21270;&#38382;&#39064;&#19978;&#20998;&#21035;&#36824;&#25552;&#20379;&#20102;&#32463;&#20856;&#21407;&#22987;&#38388;&#38553;&#21644;&#21069;&#20154;&#26410;&#26366;&#20998;&#26512;&#30340;&#23545;&#20598;&#38388;&#38553;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20026;&#36825;&#20123;&#32463;&#20856;&#26041;&#27861;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#36817;&#20046;&#25152;&#26377;&#30340;&#27493;&#38271;&#36873;&#25321;&#21644;&#19968;&#31995;&#21015;&#30340;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#23545;&#20110;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#27425;&#26799;&#24230;&#27861;&#30340;&#26089;&#26399;&#36845;&#20195;&#21487;&#33021;&#20250;&#20986;&#29616;&#25351;&#25968;&#32423;&#30340;&#21457;&#25955;&#65292;&#32780;&#20043;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#22788;&#29702;&#36807;&#36825;&#31181;&#38382;&#39064;&#12290;&#21363;&#20351;&#22312;&#36825;&#31181;&#19981;&#33391;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20173;&#28982;&#30830;&#20445;&#21644; bounds &#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider (stochastic) subgradient methods for strongly convex but potentially nonsmooth non-Lipschitz optimization. We provide new equivalent dual descriptions (in the style of dual averaging) for the classic subgradient method, the proximal subgradient method, and the switching subgradient method. These equivalences enable $O(1/T)$ convergence guarantees in terms of both their classic primal gap and a not previously analyzed dual gap for strongly convex optimization. Consequently, our theory provides these classic methods with simple, optimal stopping criteria and optimality certificates at no added computational cost. Our results apply under nearly any stepsize selection and for a range of non-Lipschitz ill-conditioned problems where the early iterations of the subgradient method may diverge exponentially quickly (a phenomenon which, to the best of our knowledge, no prior works address). Even in the presence of such undesirable behaviors, our theory still ensures and bounds eventu
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#20302;&#33021;&#35265;&#24230;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24863;&#30693;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;3D&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#8220;REDFormer&#8221;&#65292;&#36890;&#36807;&#40479;&#30640;&#30456;&#26426;-&#38647;&#36798;&#34701;&#21512;&#36827;&#34892;&#23454;&#29616;&#12290;&#35813;&#27169;&#22411;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#20998;&#31867;&#21644;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#19988;&#30456;&#36739;&#20110;&#29616;&#26377;&#27169;&#22411;&#26356;&#32463;&#27982;&#23454;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.17318</link><description>&lt;p&gt;
&#38647;&#36798;&#29031;&#20142;&#40657;&#26263;&#65306;&#36890;&#36807;&#30456;&#26426;-&#38647;&#36798;&#34701;&#21512;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20302;&#33021;&#35265;&#24230;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Radar Enlighten the Dark: Enhancing Low-Visibility Perception for Automated Vehicles with Camera-Radar Fusion. (arXiv:2305.17318v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17318
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20302;&#33021;&#35265;&#24230;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24863;&#30693;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;3D&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#8220;REDFormer&#8221;&#65292;&#36890;&#36807;&#40479;&#30640;&#30456;&#26426;-&#38647;&#36798;&#34701;&#21512;&#36827;&#34892;&#23454;&#29616;&#12290;&#35813;&#27169;&#22411;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#20998;&#31867;&#21644;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#19988;&#30456;&#36739;&#20110;&#29616;&#26377;&#27169;&#22411;&#26356;&#32463;&#27982;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#24863;&#22120;&#34701;&#21512;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#30340;&#39550;&#39542;&#26465;&#20214;&#19979;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24863;&#30693;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#24694;&#21155;&#30340;&#22825;&#27668;&#21644;&#20302;&#20809;&#29031;&#26465;&#20214;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#20256;&#24863;&#22120;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#65292;&#20174;&#32780;&#20351;&#36710;&#36742;&#23433;&#20840;&#38754;&#20020;&#28508;&#22312;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;3D&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#8220;REDFormer&#8221;&#65292;&#21033;&#29992;&#40479;&#30640;&#30456;&#26426;-&#38647;&#36798;&#34701;&#21512;&#30340;&#20415;&#21033;&#21644;&#32463;&#27982;&#23454;&#29992;&#24615;&#26469;&#35299;&#20915;&#20302;&#33021;&#35265;&#24230;&#38382;&#39064;&#12290;&#22312;&#20351;&#29992;&#22810;&#38647;&#36798;&#28857;&#20113;&#12289;&#22825;&#27668;&#20449;&#24687;&#21644;&#26102;&#38388;&#25968;&#25454;&#30340;nuScenes&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20998;&#31867;&#21644;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#27169;&#22411;&#32452;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#23545;&#24212;&#23545;&#19978;&#36848;&#38382;&#39064;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sensor fusion is a crucial augmentation technique for improving the accuracy and reliability of perception systems for automated vehicles under diverse driving conditions. However, adverse weather and low-light conditions remain challenging, where sensor performance degrades significantly, exposing vehicle safety to potential risks. Advanced sensors such as LiDARs can help mitigate the issue but with extremely high marginal costs. In this paper, we propose a novel transformer-based 3D object detection model "REDFormer" to tackle low visibility conditions, exploiting the power of a more practical and cost-effective solution by leveraging bird's-eye-view camera-radar fusion. Using the nuScenes dataset with multi-radar point clouds, weather information, and time-of-day data, our model outperforms state-of-the-art (SOTA) models on classification and detection accuracy. Finally, we provide extensive ablation studies of each model component on their contributions to address the above-mention
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#23627;&#39030;&#31867;&#22411;&#30340;&#20998;&#31867;&#26694;&#26550;&#65292;&#20174;&#21355;&#26143;&#22270;&#20687;&#19978;&#25552;&#21462;&#24314;&#31569;&#32423;&#21035;&#30340;&#39640;&#20998;&#36776;&#29575;&#23627;&#39030;&#31867;&#22411;&#25968;&#25454;&#65292;&#22635;&#34917;&#20102;&#20844;&#24320;&#25968;&#25454;&#24211;&#32570;&#22833;&#30340;&#25968;&#25454;&#65292;&#20026;&#21306;&#22495;&#39118;&#38505;&#35780;&#20272;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.17315</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39118;&#38505;&#35780;&#20272;&#33258;&#21160;&#23627;&#39030;&#31867;&#22411;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Automatic Roof Type Classification Through Machine Learning for Regional Wind Risk Assessment. (arXiv:2305.17315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#23627;&#39030;&#31867;&#22411;&#30340;&#20998;&#31867;&#26694;&#26550;&#65292;&#20174;&#21355;&#26143;&#22270;&#20687;&#19978;&#25552;&#21462;&#24314;&#31569;&#32423;&#21035;&#30340;&#39640;&#20998;&#36776;&#29575;&#23627;&#39030;&#31867;&#22411;&#25968;&#25454;&#65292;&#22635;&#34917;&#20102;&#20844;&#24320;&#25968;&#25454;&#24211;&#32570;&#22833;&#30340;&#25968;&#25454;&#65292;&#20026;&#21306;&#22495;&#39118;&#38505;&#35780;&#20272;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23627;&#39030;&#31867;&#22411;&#26159;&#39118;&#38505;&#27169;&#22411;&#20013;&#26368;&#20851;&#38190;&#30340;&#24314;&#31569;&#29305;&#24449;&#20043;&#19968;&#65292;&#28982;&#32780;&#20844;&#24320;&#25968;&#25454;&#24211;&#20013;&#26368;&#24120;&#32570;&#22833;&#36825;&#19968;&#29305;&#24449;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#23627;&#39030;&#20998;&#31867;&#26694;&#26550;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#30340;&#23627;&#39030;&#31867;&#22411;&#25968;&#25454;&#12290;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#24314;&#31569;&#32423;&#21035;&#21355;&#26143;&#22270;&#20687;&#19978;&#23545;&#23627;&#39030;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#27169;&#22411;&#22312;1000&#20010;&#27979;&#35797;&#24314;&#31569;&#19978;&#30340;F1&#20998;&#25968;&#36798;&#21040;0.96&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;CNN&#27169;&#22411;&#39044;&#27979;&#32654;&#22269;&#21271;&#21345;&#32599;&#26469;&#32435;&#21644;&#20315;&#32599;&#37324;&#36798;&#24030;&#30340;161,772&#20010;&#21333;&#25143;&#20303;&#23429;&#30340;&#23627;&#39030;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22478;&#24066;&#21644;&#20154;&#21475;&#26222;&#26597;&#21306;&#23610;&#24230;&#30340;&#23627;&#39030;&#31867;&#22411;&#20998;&#24067;&#12290;&#26222;&#26597;&#21306;&#20043;&#38388;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#23627;&#39030;&#31867;&#22411;&#23384;&#22312;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#25913;&#21892;&#32570;&#22833;&#23627;&#39030;&#25968;&#25454;&#30340;&#23436;&#25972;&#24615;&#65292;&#24320;&#21457;&#20102;&#22635;&#34917;&#31639;&#27861;&#65292;&#20351;&#29992;&#20851;&#38190;&#24314;&#31569;&#23646;&#24615;&#21644;&#37051;&#23621;&#25968;&#25454;&#22635;&#20805;&#20302;&#36136;&#37327;&#22270;&#20687;&#20013;&#32570;&#23569;&#30340;&#23627;&#39030;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Roof type is one of the most critical building characteristics for wind vulnerability modeling. It is also the most frequently missing building feature from publicly available databases. An automatic roof classification framework is developed herein to generate high-resolution roof-type data using machine learning. A Convolutional Neural Network (CNN) was trained to classify roof types using building-level satellite images. The model achieved an F1 score of 0.96 on predicting roof types for 1,000 test buildings. The CNN model was then used to predict roof types for 161,772 single-family houses in New Hanover County, NC, and Miami-Dade County, FL. The distribution of roof type in city and census tract scales was presented. A high variance was observed in the dominant roof type among census tracts. To improve the completeness of the roof-type data, imputation algorithms were developed to populate missing roof data due to low-quality images, using critical building attributes and neighbor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#21542;&#23450;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;NeQA&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#21453;&#21521;&#32553;&#25918;&#12289;U&#22411;&#32553;&#25918;&#25110;&#27491;&#21521;&#32553;&#25918;&#65292;&#35299;&#20915;NeQA&#20381;&#36182;&#20110;&#38382;&#31572;&#21644;&#21542;&#23450;&#29702;&#35299;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#20854;&#32553;&#25918;&#36235;&#21183;&#30001;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#32553;&#25918;&#36235;&#21183;&#32452;&#21512;&#24418;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.17311</link><description>&lt;p&gt;
&#36229;&#36234;&#27491;&#21521;&#32553;&#25918;&#65306;&#21542;&#23450;&#35821;&#23545;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#36235;&#21183;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models. (arXiv:2305.17311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#21542;&#23450;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;NeQA&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#21453;&#21521;&#32553;&#25918;&#12289;U&#22411;&#32553;&#25918;&#25110;&#27491;&#21521;&#32553;&#25918;&#65292;&#35299;&#20915;NeQA&#20381;&#36182;&#20110;&#38382;&#31572;&#21644;&#21542;&#23450;&#29702;&#35299;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#20854;&#32553;&#25918;&#36235;&#21183;&#30001;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#32553;&#25918;&#36235;&#21183;&#32452;&#21512;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#27491;&#21521;&#32553;&#25918;&#65292;&#22312;&#22823;&#23567;&#12289;&#35745;&#31639;&#25110;&#25968;&#25454;&#26041;&#38754;&#25193;&#23637;&#27169;&#22411;&#20250;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#21542;&#23450;&#38382;&#21477;&#30340;&#25968;&#25454;&#38598;NeQA&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#19981;&#20250;&#34920;&#29616;&#20986;&#31616;&#21333;&#30340;&#27491;&#21521;&#32553;&#25918;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#20219;&#21153;&#21487;&#20197;&#34920;&#29616;&#20986;&#21453;&#21521;&#32553;&#25918;&#12289;U&#24418;&#32553;&#25918;&#25110;&#27491;&#21521;&#32553;&#25918;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#25552;&#31034;&#26041;&#27861;&#25110;&#27169;&#22411;&#26063;&#32676;&#26102;&#65292;&#36825;&#19977;&#31181;&#32553;&#25918;&#36235;&#21183;&#20250;&#25353;&#29031;&#36825;&#20010;&#39034;&#24207;&#21457;&#29983;&#36716;&#21464;&#12290;&#25105;&#20204;&#20551;&#35774;&#35299;&#20915;NeQA&#20381;&#36182;&#20110;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#38382;&#31572;&#65288;&#20219;&#21153;1&#65289;&#21644;&#21542;&#23450;&#29702;&#35299;&#65288;&#20219;&#21153;2&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#20219;&#21153;1&#20855;&#26377;&#32447;&#24615;&#32553;&#25918;&#65292;&#32780;&#20219;&#21153;2&#20855;&#26377;S&#24418;&#32553;&#25918;&#65292;&#24182;&#20855;&#26377;&#19968;&#20010;&#32039;&#24613;&#30340;&#36716;&#25240;&#28857;&#65292;&#23558;&#36825;&#20004;&#20010;&#32553;&#25918;&#36235;&#21183;&#32452;&#21512;&#36215;&#26469;&#21363;&#21487;&#24471;&#20986;&#26368;&#32456;&#30340;NeQA&#32553;&#25918;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22797;&#26434;&#32553;&#25918;&#36235;&#21183;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to exhibit positive scaling, where performance improves as models are scaled up in terms of size, compute, or data. In this work, we introduce NeQA, a dataset consisting of questions with negation in which language models do not exhibit straightforward positive scaling. We show that this task can exhibit inverse scaling, U-shaped scaling, or positive scaling, and the three scaling trends shift in this order as we use more powerful prompting methods or model families. We hypothesize that solving NeQA depends on two subtasks: question answering (task 1) and negation understanding (task 2). We find that task 1 has linear scaling, while task 2 has sigmoid-shaped scaling with an emergent transition point, and composing these two scaling trends yields the final scaling trend of NeQA. Our work reveals and provides a way to analyze the complex scaling trends of language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Chain-of-Thought Hub &#30340;&#24320;&#28304;&#35780;&#20272;&#22871;&#20214;&#65292;&#30446;&#30340;&#26159;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#26159;&#20026;&#20102;&#36861;&#36394;LLMs&#36827;&#23637;&#32780;&#32534;&#21046;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#22522;&#20934;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#19982;&#25512;&#29702;&#33021;&#21147;&#30456;&#20851;&#65292;&#32780; Claude-v1.3 &#26159;&#36804;&#20170;&#20026;&#27490;&#25512;&#29702;&#33021;&#21147;&#26368;&#24378;&#30340;LLM&#12290;</title><link>http://arxiv.org/abs/2305.17306</link><description>&lt;p&gt;
&#8220;Chain-of-Thought Hub: &#36830;&#32493;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#34920;&#29616;&#30340;&#21162;&#21147;&#8221;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance. (arXiv:2305.17306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Chain-of-Thought Hub &#30340;&#24320;&#28304;&#35780;&#20272;&#22871;&#20214;&#65292;&#30446;&#30340;&#26159;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#26159;&#20026;&#20102;&#36861;&#36394;LLMs&#36827;&#23637;&#32780;&#32534;&#21046;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#22522;&#20934;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#19982;&#25512;&#29702;&#33021;&#21147;&#30456;&#20851;&#65292;&#32780; Claude-v1.3 &#26159;&#36804;&#20170;&#20026;&#27490;&#25512;&#29702;&#33021;&#21147;&#26368;&#24378;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#20294;&#20063;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; Chain-of-Thought Hub&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20043;&#25152;&#20197;&#23545;&#36825;&#20010;&#35774;&#32622;&#24863;&#20852;&#36259;&#65292;&#26159;&#22240;&#20026; (1) &#20174; GPT &#21644; PaLM &#27169;&#22411;&#23478;&#26063;&#30340;&#34892;&#20026;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22797;&#26434;&#30340;&#25512;&#29702;&#24456;&#21487;&#33021;&#26159;&#19968;&#20010;&#26356;&#24369;&#21644;&#26356;&#24378;&#30340;LLMs&#20043;&#38388;&#30340;&#20851;&#38190;&#21306;&#21035;&#65307; (2) &#25105;&#20204;&#39044;&#35265;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#25104;&#20026;&#19979;&#19968;&#20195;&#35745;&#31639;&#24179;&#21488;&#65292;&#24182;&#20419;&#36827;&#22522;&#20110;LLM&#30340;&#26032;&#24212;&#29992;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#36825;&#33258;&#28982;&#38656;&#35201;&#22522;&#30784;&#27169;&#22411;&#25191;&#34892;&#24120;&#24120;&#28041;&#21450;&#35821;&#35328;&#21644;&#36923;&#36753;&#25805;&#20316;&#32452;&#21512;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#32534;&#21046;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#22522;&#20934;&#65292;&#20197;&#36319;&#36394;LLMs&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30446;&#21069;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;(1) &#27169;&#22411;&#35268;&#27169;&#26174;&#28982;&#19982;&#25512;&#29702;&#33021;&#21147;&#30456;&#20851;&#65307;(2) &#25130;&#33267;2023&#24180;5&#26376;&#65292;Claude-v1.3 &#26159;&#36804;&#20170;&#20026;&#27490;&#25512;&#29702;&#33021;&#21147;&#26368;&#24378;&#30340;LLM &#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications, this naturally requires the foundation models to perform complex tasks that often involve the composition of linguistic and logical operations. Our approach is to compile a suite of challenging reasoning benchmarks to track the progress of LLMs. Our current results show that: (1) model scale clearly correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#40657;&#30418;&#27169;&#22411;&#36716;&#21270;&#25104;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.17303</link><description>&lt;p&gt;
&#20174;&#40657;&#30418;&#27169;&#22411;&#21040;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#36716;&#21270;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#40657;&#30418;&#27169;&#22411;&#36716;&#21270;&#25104;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;AI&#27169;&#22411;&#26159;&#21307;&#30103;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#21363;&#20351;&#36755;&#20837;&#20998;&#24067;&#36731;&#24494;&#31227;&#20301;&#65288;&#20363;&#22914;&#25195;&#25551;&#20202;&#31867;&#22411;&#65289;&#65292;&#20063;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#32780;&#25918;&#23556;&#31185;&#21307;&#29983;&#21017;&#20381;&#36182;&#20110;&#24322;&#24120;&#24615;&#30340;&#36890;&#29992;&#25551;&#36848;&#24615;&#35268;&#21017;&#12290;&#24494;&#35843;&#27169;&#22411;&#20197;&#23558;&#30693;&#35782;&#20174;&#19968;&#20010;&#39046;&#22495;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#35745;&#31639;&#25104;&#26412;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#38024;&#23545;&#26410;&#30693;&#30340;&#30446;&#26631;&#22495;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35748;&#20026;NN&#30340;&#21487;&#35299;&#37322;&#32452;&#20214;&#22823;&#33268;&#26159;&#22495;&#19981;&#21464;&#30340;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#19981;&#21450;&#23427;&#20204;&#30340;BB&#21464;&#20307;&#12290;&#22312;&#28304;&#22495;&#20013;&#25105;&#20204;&#20808;&#20351;&#29992;&#20154;&#31867;&#29702;&#35299;&#30340;&#27010;&#24565;&#20174;BB&#24320;&#22987;&#65292;&#23558;&#20854;&#25552;&#28860;&#25104;&#19968;&#32452;&#27973;&#26174;&#26131;&#25026;&#30340;interpretable&#27169;&#22411;&#12290;&#30001;&#20110;&#27599;&#20010;interpretable&#27169;&#22411;&#37117;&#35206;&#30422;&#20102;&#25968;&#25454;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20855;&#26377;&#19968;&#32452;interpretable&#27169;&#22411;&#30340;&#28151;&#21512;&#21487;&#20197;&#23454;&#29616;&#19982;BB&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (\eg scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#31283;&#23450;&#24615;&#24809;&#32602;&#33258;&#36866;&#24212;&#65288;SPA&#65289;&#23398;&#20064;&#29575;&#65292;&#35813;&#23398;&#20064;&#29575;&#20351;FTRL&#20855;&#26377;&#31232;&#30095;&#24615;&#12289;&#28216;&#25103;&#20381;&#36182;&#24615;&#21644;&#26368;&#20339;&#19990;&#30028;&#65288;BOBW&#65289;&#19977;&#31181;&#36866;&#24212;&#24615;&#31867;&#22411;&#65292;&#20854;&#20013;SPA-sparse&#31639;&#27861;&#21487;&#36866;&#24212;&#20110;&#26410;&#30693;&#30340;&#31232;&#30095;&#32423;&#21035;&#65292;SPA-game-dependency&#31639;&#27861;&#21487;&#26681;&#25454;&#25152;&#29609;&#30340;&#28216;&#25103;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#20854;&#34892;&#20026;&#65292;BOBW&#31639;&#27861;&#21017;&#26159;&#26082;&#20855;&#26377;&#31232;&#30095;&#24615;&#21448;&#20855;&#26377;&#28216;&#25103;&#20381;&#36182;&#24615;&#30340;&#36866;&#24212;&#24615;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.17301</link><description>&lt;p&gt;
&#31283;&#23450;&#24615;&#24809;&#32602;&#33258;&#36866;&#24212;&#36319;&#38543;&#27491;&#21017;&#21270;&#39046;&#34966;&#65306;&#31232;&#30095;&#24615;&#12289;&#28216;&#25103;&#20381;&#36182;&#24615;&#21644;&#26368;&#20339;&#19990;&#30028;&#30340;&#24182;&#23384;
&lt;/p&gt;
&lt;p&gt;
Stability-penalty-adaptive Follow-the-regularized-leader: Sparsity, Game-dependency, and Best-of-both-worlds. (arXiv:2305.17301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#31283;&#23450;&#24615;&#24809;&#32602;&#33258;&#36866;&#24212;&#65288;SPA&#65289;&#23398;&#20064;&#29575;&#65292;&#35813;&#23398;&#20064;&#29575;&#20351;FTRL&#20855;&#26377;&#31232;&#30095;&#24615;&#12289;&#28216;&#25103;&#20381;&#36182;&#24615;&#21644;&#26368;&#20339;&#19990;&#30028;&#65288;BOBW&#65289;&#19977;&#31181;&#36866;&#24212;&#24615;&#31867;&#22411;&#65292;&#20854;&#20013;SPA-sparse&#31639;&#27861;&#21487;&#36866;&#24212;&#20110;&#26410;&#30693;&#30340;&#31232;&#30095;&#32423;&#21035;&#65292;SPA-game-dependency&#31639;&#27861;&#21487;&#26681;&#25454;&#25152;&#29609;&#30340;&#28216;&#25103;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#20854;&#34892;&#20026;&#65292;BOBW&#31639;&#27861;&#21017;&#26159;&#26082;&#20855;&#26377;&#31232;&#30095;&#24615;&#21448;&#20855;&#26377;&#28216;&#25103;&#20381;&#36182;&#24615;&#30340;&#36866;&#24212;&#24615;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#36866;&#24212;&#38382;&#39064;&#30340;&#22256;&#38590;&#31243;&#24230;&#26159;&#25193;&#23637;&#31639;&#27861;&#36866;&#29992;&#24615;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;&#36319;&#38543;&#27491;&#21017;&#21270;&#39046;&#34966;&#36817;&#24180;&#26469;&#25104;&#20026;&#33719;&#21462;&#28120;&#27760;&#27861;&#20013;&#21508;&#31181;&#31867;&#22411;&#36866;&#24212;&#24615;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#24191;&#36825;&#31181;&#36866;&#24212;&#24615;&#65292;&#25105;&#20204;&#20026;FTRL&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#31216;&#20026;&#31283;&#23450;&#24615;&#24809;&#32602;&#33258;&#36866;&#24212;&#65288;SPA&#65289;&#23398;&#20064;&#29575;&#12290;&#35813;&#23398;&#20064;&#29575;&#20135;&#29983;&#30340;&#36951;&#25022;&#30028;&#20849;&#21516;&#21462;&#20915;&#20110;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#24809;&#32602;&#65292;&#20854;&#20013;FTRL&#30340;&#36951;&#25022;&#36890;&#24120;&#34987;&#20998;&#35299;&#12290;&#20973;&#20511;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20960;&#20010;&#20855;&#26377;&#19977;&#31181;&#36866;&#24212;&#24615;&#31867;&#22411;&#30340;&#31639;&#27861;&#65306;&#31232;&#30095;&#24615;&#12289;&#28216;&#25103;&#20381;&#36182;&#24615;&#21644;&#26368;&#20339;&#19990;&#30028;&#65288;BOBW&#65289;&#12290;&#31232;&#30095;&#24615;&#32463;&#24120;&#20986;&#29616;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#20013;&#65292;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#31232;&#30095;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;$k$-arms&#20551;&#23450;&#20107;&#20808;&#24050;&#30693;&#31232;&#30095;&#32423;&#21035;$s \leq k$&#65292;&#32780;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#36890;&#24120;&#19981;&#26159;&#24773;&#20917;&#12290;&#20026;&#20102;&#36866;&#24212;&#26410;&#30693;&#30340;&#31232;&#30095;&#32423;&#21035;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;SPA-sparse&#65292;&#35813;&#31639;&#27861;&#26174;&#31034;&#27604;&#29616;&#26377;&#31232;&#30095;&#31639;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#12290;&#28216;&#25103;&#20381;&#36182;&#24615;&#26159;&#21478;&#19968;&#31181;&#36866;&#24212;&#24615;&#31867;&#22411;&#65292;&#24403;&#29992;&#20110;&#29983;&#25104;&#25968;&#25454;&#30340;&#28216;&#25103;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#21363;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;SPA-game-dependency&#65292;&#35813;&#31639;&#27861;&#26681;&#25454;&#25152;&#29609;&#30340;&#28216;&#25103;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#20854;&#34892;&#20026;&#65292;&#24182;&#34920;&#26126;&#23427;&#27604;&#38750;&#33258;&#36866;&#24212;&#31639;&#27861;&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26082;&#20855;&#26377;&#31232;&#30095;&#24615;&#21448;&#20855;&#26377;&#28216;&#25103;&#20381;&#36182;&#24615;&#36866;&#24212;&#24615;&#30340;BOBW&#31639;&#27861;&#65292;&#24182;&#26174;&#31034;&#23427;&#27604;&#20165;&#38598;&#20013;&#20110;&#19968;&#31181;&#36866;&#24212;&#24615;&#31867;&#22411;&#30340;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptivity to the difficulties of a problem is a key property in sequential decision-making problems to broaden the applicability of algorithms. Follow-the-Regularized-Leader (FTRL) has recently emerged as one of the most promising approaches for obtaining various types of adaptivity in bandit problems. Aiming to further generalize this adaptivity, we develop a generic adaptive learning rate, called Stability-Penalty-Adaptive (SPA) learning rate for FTRL. This learning rate yields a regret bound jointly depending on stability and penalty of the algorithm, into which the regret of FTRL is typically decomposed. With this result, we establish several algorithms with three types of adaptivity: sparsity, game-dependency, and Best-of-Both-Worlds (BOBW). Sparsity frequently appears in real-world problems. However, existing sparse multi-armed bandit algorithms with $k$-arms assume that the sparsity level $s \leq k$ is known in advance, which is often not the case in real-world scenarios. To ad
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21019;&#36896;&#26356;&#21152;&#31283;&#20581;&#12289;&#39640;&#25928;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#36890;&#36807;&#21457;&#29616;&#37325;&#22797;&#23376;&#30005;&#36335;&#21644;&#20998;&#26512;&#26524;&#34631;&#30340;&#33322;&#21521;&#26041;&#21521;&#30005;&#36335;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#36830;&#25509;&#27169;&#24335;&#21644;&#27169;&#22411;&#65292;&#20197;&#25506;&#32034;&#22914;&#20309;&#36827;&#19968;&#27493;&#25193;&#23637;&#29616;&#26377;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.17300</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#21019;&#36896;&#36830;&#25509;&#32452;&#38480;&#21046;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#24378;&#22823;&#12289;&#26356;&#39640;&#25928;&#12289;&#26356;&#36866;&#24212;&#24615;&#24378;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence. (arXiv:2305.17300v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17300
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21019;&#36896;&#26356;&#21152;&#31283;&#20581;&#12289;&#39640;&#25928;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#36890;&#36807;&#21457;&#29616;&#37325;&#22797;&#23376;&#30005;&#36335;&#21644;&#20998;&#26512;&#26524;&#34631;&#30340;&#33322;&#21521;&#26041;&#21521;&#30005;&#36335;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#36830;&#25509;&#27169;&#24335;&#21644;&#27169;&#22411;&#65292;&#20197;&#25506;&#32034;&#22914;&#20309;&#36827;&#19968;&#27493;&#25193;&#23637;&#29616;&#26377;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36793;&#32536;&#19978;&#30340;&#39640;&#25928;&#23398;&#20064;&#65288;&#23454;&#29616;&#36866;&#24212;&#24615;&#24378;&#12289;&#20302;&#22797;&#26434;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65289;&#20173;&#28982;&#26159;&#22269;&#38450;&#21644;&#21830;&#19994;&#24212;&#29992;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#25105;&#20204;&#26500;&#24819;&#20102;&#20351;&#29992;&#22823;&#22411;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25429;&#33719;&#31070;&#32463;&#20803;&#21644;&#31361;&#35302;&#36830;&#25509;&#30340;&#22823;&#33041;&#22320;&#22270;&#65292;&#26469;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#27969;&#31243;&#12290;&#25105;&#20204;&#22312;&#35813;&#27969;&#31243;&#32467;&#26500;&#20869;&#36861;&#27714;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the progress in deep learning networks, efficient learning at the edge (enabling adaptable, low-complexity machine learning solutions) remains a critical need for defense and commercial applications. We envision a pipeline to utilize large neuroimaging datasets, including maps of the brain which capture neuron and synapse connectivity, to improve machine learning approaches. We have pursued different approaches within this pipeline structure. First, as a demonstration of data-driven discovery, the team has developed a technique for discovery of repeated subcircuits, or motifs. These were incorporated into a neural architecture search approach to evolve network architectures. Second, we have conducted analysis of the heading direction circuit in the fruit fly, which performs fusion of visual and angular velocity features, to explore augmenting existing computational models with new insight. Our team discovered a novel pattern of connectivity, implemented a new model, and demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21307;&#30103;&#24212;&#29992;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20915;&#31574;&#26641;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#29992;&#23427;&#26469;&#30830;&#23450;&#26641;&#30340;&#31283;&#23450;&#27700;&#24179;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22521;&#35757;&#31283;&#23450;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#31350;&#31283;&#23450;&#24615;&#12289;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#19981;&#21487;&#36991;&#20813;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.17299</link><description>&lt;p&gt;
&#25552;&#39640;&#20915;&#31574;&#26641;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Stability in Decision Tree Models. (arXiv:2305.17299v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21307;&#30103;&#24212;&#29992;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20915;&#31574;&#26641;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#29992;&#23427;&#26469;&#30830;&#23450;&#26641;&#30340;&#31283;&#23450;&#27700;&#24179;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22521;&#35757;&#31283;&#23450;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#31350;&#31283;&#23450;&#24615;&#12289;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#19981;&#21487;&#36991;&#20813;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#32467;&#26500;&#26131;&#20110;&#29702;&#35299;&#65292;&#20915;&#31574;&#26641;&#36890;&#24120;&#22312;&#38656;&#35201;&#21487;&#35299;&#37322;&#24615;&#30340;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#36817;&#26399;&#30340;&#24037;&#20316;&#38598;&#20013;&#20110;&#25913;&#36827;&#20915;&#31574;&#26641;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#39044;&#27979;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65307;&#28982;&#32780;&#65292;&#20854;&#19981;&#31283;&#23450;&#24615;&#34429;&#28982;&#26377;&#20805;&#20998;&#30340;&#35760;&#24405;&#65292;&#20294;&#21364;&#24471;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#38469;&#30340;&#21307;&#30103;&#24212;&#29992;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#21270;&#20915;&#31574;&#26641;&#27169;&#22411;&#30340;&#19968;&#23567;&#27493;&#12290;&#30001;&#20110;&#31283;&#23450;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#22312;&#21307;&#30103;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20915;&#31574;&#26641;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#30830;&#23450;&#26641;&#30340;&#31283;&#23450;&#27700;&#24179;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22521;&#35757;&#31283;&#23450;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#24182;&#35843;&#26597;&#20102;&#20915;&#31574;&#26641;&#27169;&#22411;&#20043;&#38388;&#19981;&#21487;&#36991;&#20813;&#30340;&#26435;&#34913;&#65292;&#21253;&#25324;&#22312;&#31283;&#23450;&#24615;&#12289;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20845;&#20010;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#23637;&#31034;&#20102;&#25152;&#25552;&#35758;&#26041;&#27861;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to their inherently interpretable structure, decision trees are commonly used in applications where interpretability is essential. Recent work has focused on improving various aspects of decision trees, including their predictive power and robustness; however, their instability, albeit well-documented, has been addressed to a lesser extent. In this paper, we take a step towards the stabilization of decision tree models through the lens of real-world health care applications due to the relevance of stability and interpretability in this space. We introduce a new distance metric for decision trees and use it to determine a tree's level of stability. We propose a novel methodology to train stable decision trees and investigate the existence of trade-offs that are inherent to decision tree models - including between stability, predictive power, and interpretability. We demonstrate the value of the proposed methodology through an extensive quantitative and qualitative analysis of six 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#20294;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#22312;&#20998;&#31163;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17297</link><description>&lt;p&gt;
&#26080;&#29420;&#31435;&#24615;&#30340;&#27867;&#21270;&#35823;&#24046;&#65306;&#21435;&#22122;&#12289;&#32447;&#24615;&#22238;&#24402;&#21644;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalization Error without Independence: Denoising, Linear Regression, and Transfer Learning. (arXiv:2305.17297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#20294;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#22312;&#20998;&#31163;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32447;&#24615;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#19968;&#20123;&#37325;&#35201;&#24037;&#20316;&#39564;&#35777;&#20102;&#29702;&#35770;&#24037;&#20316;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#36825;&#20123;&#24037;&#20316;&#30001;&#20110;&#25216;&#26415;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#65292;&#36825;&#20123;&#20551;&#35774;&#21253;&#25324;&#20855;&#26377;&#33391;&#22909;&#26465;&#20214;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#20197;&#21450;&#20855;&#26377;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65292;&#36825;&#20123;&#20551;&#35774;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#19968;&#20123;&#20851;&#20110;&#20998;&#24067;&#20559;&#31227;&#30340;&#24037;&#20316;&#36890;&#24120;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#25216;&#26415;&#20551;&#35774;&#65292;&#24182;&#19988;&#19981;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#26356;&#22909;&#22320;&#23545;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#20294;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#36890;&#36807;&#20998;&#31163;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#20551;&#35774;&#26469;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#22312;&#36825;&#20123;&#26494;&#24347;&#30340;&#20551;&#35774;&#19979;&#65292;&#30740;&#31350;&#20102;&#21435;&#22122;&#38382;&#39064;&#12289;&#32447;&#24615;&#22238;&#24402;&#21644;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying the generalization abilities of linear models with real data is a central question in statistical learning. While there exist a limited number of prior important works (Loureiro et al. (2021A, 2021B), Wei et al. 2022) that do validate theoretical work with real data, these works have limitations due to technical assumptions. These assumptions include having a well-conditioned covariance matrix and having independent and identically distributed data. These assumptions are not necessarily valid for real data. Additionally, prior works that do address distributional shifts usually make technical assumptions on the joint distribution of the train and test data (Tripuraneni et al. 2021, Wu and Xu 2020), and do not test on real data.  In an attempt to address these issues and better model real data, we look at data that is not I.I.D. but has a low-rank structure. Further, we address distributional shift by decoupling assumptions on the training and test distribution. We provide anal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Fourier-DeepONet&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#23436;&#20840;&#27874;&#24418;&#21453;&#28436;&#65292;&#20855;&#26377;&#23545;&#22320;&#38663;&#28304;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#21487;&#21464;&#28304;&#30340;FWI&#12290;</title><link>http://arxiv.org/abs/2305.17289</link><description>&lt;p&gt;
Fourier-DeepONet: &#22686;&#24378;&#20613;&#37324;&#21494;&#31639;&#23376;&#28145;&#24230;&#32593;&#32476;&#30340;&#23436;&#20840;&#27874;&#24418;&#21453;&#28436;&#65292;&#25552;&#39640;&#20102;&#31934;&#24230;&#65292;&#36890;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fourier-DeepONet: Fourier-enhanced deep operator networks for full waveform inversion with improved accuracy, generalizability, and robustness. (arXiv:2305.17289v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17289
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Fourier-DeepONet&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#23436;&#20840;&#27874;&#24418;&#21453;&#28436;&#65292;&#20855;&#26377;&#23545;&#22320;&#38663;&#28304;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#21487;&#21464;&#28304;&#30340;FWI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#20840;&#27874;&#24418;&#21453;&#28436;&#65288;FWI&#65289;&#26159;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20174;&#22320;&#38663;&#27874;&#24418;&#25968;&#25454;&#20013;&#25512;&#26029;&#22320;&#19979;&#32467;&#26500;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;FWI&#24050;&#32463;&#36234;&#26469;&#36234;&#34987;&#30740;&#31350;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#28508;&#22312;&#30340;&#35843;&#26597;&#28304;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#28304;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#20005;&#37325;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20613;&#37324;&#21494;&#22686;&#24378;&#30340;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#65288;Fourier-DeepONet&#65289;&#29992;&#20110;&#20840;&#27874;&#24418;&#21453;&#28436;&#65292;&#24182;&#19988;&#20855;&#26377;&#22320;&#38663;&#28304;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21253;&#25324;&#22320;&#38663;&#28304;&#30340;&#39057;&#29575;&#21644;&#20301;&#32622;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#20316;&#20026;DeepONet&#30340;&#35299;&#30721;&#22120;&#65292;&#24182;&#21033;&#29992;&#28304;&#21442;&#25968;&#20316;&#20026;Fourier-DeepONet&#30340;&#20854;&#20013;&#19968;&#20010;&#36755;&#20837;&#65292;&#20197;&#23454;&#29616;&#23545;&#20855;&#26377;&#21487;&#21464;&#28304;&#30340;FWI&#30340;&#20998;&#36776;&#29575;&#12290;&#20026;&#20102;&#27979;&#35797;Fourier-DeepONet&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#36924;&#30495;&#30340;FWI&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;FWI-F&#21644;FWI-L&#65289;&#65292;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#30340;&#28304;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Full waveform inversion (FWI) infers the subsurface structure information from seismic waveform data by solving a non-convex optimization problem. Data-driven FWI has been increasingly studied with various neural network architectures to improve accuracy and computational efficiency. Nevertheless, the applicability of pre-trained neural networks is severely restricted by potential discrepancies between the source function used in the field survey and the one utilized during training. Here, we develop a Fourier-enhanced deep operator network (Fourier-DeepONet) for FWI with the generalization of seismic sources, including the frequencies and locations of sources. Specifically, we employ the Fourier neural operator as the decoder of DeepONet, and we utilize source parameters as one input of Fourier-DeepONet, facilitating the resolution of FWI with variable sources. To test Fourier-DeepONet, we develop two new and realistic FWI benchmark datasets (FWI-F and FWI-L) with varying source frequ
&lt;/p&gt;</description></item><item><title>GC-Flow&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#24314;&#27169;&#31867;&#21035;&#26465;&#20214;&#27010;&#29575;&#21644;&#31867;&#21035;&#20808;&#39564;&#65292;&#36890;&#36807;&#37197;&#22791;&#39640;&#26031;&#28151;&#21512;&#34920;&#31034;&#31354;&#38388;&#65292;&#20445;&#25345;&#39044;&#27979;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#33391;&#22909;&#20998;&#31163;&#30340;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.17284</link><description>&lt;p&gt;
GC-Flow: &#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#27969;&#32593;&#32476;&#29992;&#20110;&#26377;&#25928;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
GC-Flow: A Graph-Based Flow Network for Effective Clustering. (arXiv:2305.17284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17284
&lt;/p&gt;
&lt;p&gt;
GC-Flow&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#24314;&#27169;&#31867;&#21035;&#26465;&#20214;&#27010;&#29575;&#21644;&#31867;&#21035;&#20808;&#39564;&#65292;&#36890;&#36807;&#37197;&#22791;&#39640;&#26031;&#28151;&#21512;&#34920;&#31034;&#31354;&#38388;&#65292;&#20445;&#25345;&#39044;&#27979;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#33391;&#22909;&#20998;&#31163;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26159;&#30452;&#25509;&#24314;&#27169;&#21322;&#30417;&#30563;&#20998;&#31867;&#22270;&#25968;&#25454;&#31867;&#21518;&#39564;&#27010;&#29575;$p&#65288;y|\mathbf{x}&#65289;$&#30340;&#21028;&#21035;&#27169;&#22411;&#12290;&#34429;&#28982;&#20316;&#20026;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26159;&#20174;GCN&#20013;&#25552;&#21462;&#30340;&#33410;&#28857;&#34920;&#24449;&#24120;&#32570;&#23569;&#26377;&#25928;&#32858;&#31867;&#25152;&#38656;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#30446;&#26631;&#19981;&#21516;&#12290;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#24402;&#19968;&#21270;&#27969;&#65292;&#29992;&#20110;&#26367;&#25442;GCN&#23618;&#65292;&#26500;&#24314;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21516;&#26102;&#24314;&#27169;&#31867;&#21035;&#26465;&#20214;&#27010;&#29575;$p(\mathbf{x}|y)$&#21644;&#31867;&#21035;&#20808;&#39564;$p(y)$&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31070;&#32463;&#32593;&#32476;GC-Flow&#20445;&#30041;&#20102;&#22270;&#21367;&#31215;&#25805;&#20316;&#65292;&#21516;&#26102;&#37197;&#22791;&#20102;&#39640;&#26031;&#28151;&#21512;&#34920;&#31034;&#31354;&#38388;&#12290;&#36825;&#26377;&#20004;&#20010;&#22909;&#22788;&#65306;&#23427;&#19981;&#20165;&#20445;&#25345;&#20102;GCN&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#36824;&#30001;&#20110;&#34920;&#31034;&#31354;&#38388;&#30340;&#32467;&#26500;&#32780;&#20135;&#29983;&#20102;&#33391;&#22909;&#20998;&#31163;&#30340;&#32858;&#31867;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#36825;&#20123;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#39069;&#22806;&#30340;&#21442;&#25968;&#21270;&#27491;&#21017;&#21270;&#20248;&#21183;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) are \emph{discriminative models} that directly model the class posterior $p(y|\mathbf{x})$ for semi-supervised classification of graph data. While being effective, as a representation learning approach, the node representations extracted from a GCN often miss useful information for effective clustering, because the objectives are different. In this work, we design normalizing flows that replace GCN layers, leading to a \emph{generative model} that models both the class conditional likelihood $p(\mathbf{x}|y)$ and the class prior $p(y)$. The resulting neural network, GC-Flow, retains the graph convolution operations while being equipped with a Gaussian mixture representation space. It enjoys two benefits: it not only maintains the predictive power of GCN, but also produces well-separated clusters, due to the structuring of the representation space. We demonstrate these benefits on a variety of benchmark data sets. Moreover, we show that additional par
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212; Sharpened Lazy Incremental Quasi-Newton (SLIQN) &#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#26174;&#24335;&#30340;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#21644;$O(d^2)$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17283</link><description>&lt;p&gt;
&#20248;&#21270;&#36864;&#28779;&#31639;&#27861;&#30340;&#35823;&#24046;&#30028;&#21644;&#23616;&#37096;&#25628;&#32034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Sharpened Lazy Incremental Quasi-Newton Method. (arXiv:2305.17283v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212; Sharpened Lazy Incremental Quasi-Newton (SLIQN) &#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#26174;&#24335;&#30340;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#21644;$O(d^2)$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20855;&#26377;$Lipschitz$&#36830;&#32493;Hessian&#30697;&#38453;&#22312;$d$&#32500;&#31354;&#38388;&#20013;&#65292;$n$&#20010;&#24378;&#20984;&#20809;&#28369;&#20989;&#25968;&#30340;&#26377;&#38480;&#21644;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;$n$&#30340;&#25968;&#37327;&#24456;&#22823;&#65292;&#22240;&#27492;&#24517;&#39035;&#20351;&#29992;&#27599;&#36845;&#20195;&#19968;&#27425;&#19982;$n$&#26080;&#20851;&#30340;&#22686;&#37327;&#24335;&#25110;&#38543;&#26426;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212; Sharpened Lazy Incremental Quasi-Newton (SLIQN) &#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#26174;&#24335;&#30340;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#21644;$O(d^2)$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the finite sum minimization of $n$ strongly convex and smooth functions with Lipschitz continuous Hessians in $d$ dimensions. In many applications where such problems arise, including maximum likelihood estimation, empirical risk minimization, and unsupervised learning, the number of observations $n$ is large, and it becomes necessary to use incremental or stochastic algorithms whose per-iteration complexity is independent of $n$. Of these, the incremental/stochastic variants of the Newton method exhibit superlinear convergence, but incur a per-iteration complexity of $O(d^3)$, which may be prohibitive in large-scale settings. On the other hand, the incremental Quasi-Newton method incurs a per-iteration complexity of $O(d^2)$ but its superlinear convergence rate has only been characterized asymptotically. This work puts forth the Sharpened Lazy Incremental Quasi-Newton (SLIQN) method that achieves the best of both worlds: an explicit superlinear convergence rate with a per-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#20013;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#35813;&#35268;&#21017;&#22312;Nagata&#32500;&#24230;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#27492;&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#21644;Heisenberg&#32676;&#20013;&#20063;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.17282</link><description>&lt;p&gt;
&#24230;&#37327;&#31354;&#38388;&#21644;Nagata&#32500;&#24230;&#20013;k-NN&#35268;&#21017;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;(II)
&lt;/p&gt;
&lt;p&gt;
Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. II. (arXiv:2305.17282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#20013;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#35813;&#35268;&#21017;&#22312;Nagata&#32500;&#24230;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#27492;&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#21644;Heisenberg&#32676;&#20013;&#20063;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32487;&#32493;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#30740;&#31350;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#12290;&#30001;&#20110;C\'erou&#21644;Guyader(2006)&#20197;&#21450;Preiss(1983)&#30340;&#32467;&#26524;&#65292;&#24050;&#30693;&#35813;&#35268;&#21017;&#22312;&#27599;&#20010;Nagata&#24847;&#20041;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#24230;&#37327;&#31354;&#38388;X&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26080;&#24179;&#23616;&#24773;&#20917;&#19979;&#27492;&#35268;&#21017;&#22312;&#36825;&#26679;&#30340;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#22312;Devroye&#65292;Gy\"{o}rfi&#65292;Krzy\.{z}ak&#21644;Lugosi&#65288;1994&#65289;&#22312;&#27431;&#20960;&#37324;&#24471;&#35774;&#32622;&#20013;&#24212;&#29992;&#30340;&#25171;&#30772;&#24179;&#23616;&#31574;&#30053;&#19979;&#65292;&#25105;&#20204;&#35774;&#27861;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#65288;&#21363;Nagata&#32500;&#24230;&#20026;&#38646;&#30340;&#31354;&#38388;&#65289;&#20013;&#23637;&#31034;&#20102;&#24378;&#26222;&#36941;&#19968;&#33268;&#24615;&#12290;&#32467;&#21512;C\'erou&#21644;Guyader&#30340;&#23450;&#29702;&#21644;Assouad&#21644;Quentin de Gromard (2006)&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#25512;&#20986;$k$-NN&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;$k$-NN&#35268;&#21017;&#22312;Heisenberg&#32676;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#32780;&#35813;&#32676;&#24182;&#38750;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We continue to investigate the $k$ nearest neighbour learning rule in separable metric spaces. Thanks to the results of C\'erou and Guyader (2006) and Preiss (1983), this rule is known to be universally consistent in every metric space $X$ that is sigma-finite dimensional in the sense of Nagata. Here we show that the rule is strongly universally consistent in such spaces in the absence of ties. Under the tie-breaking strategy applied by Devroye, Gy\"{o}rfi, Krzy\.{z}ak, and Lugosi (1994) in the Euclidean setting, we manage to show the strong universal consistency in non-Archimedian metric spaces (that is, those of Nagata dimension zero). Combining the theorem of C\'erou and Guyader with results of Assouad and Quentin de Gromard (2006), one deduces that the $k$-NN rule is universally consistent in metric spaces having finite dimension in the sense of de Groot. In particular, the $k$-NN rule is universally consistent in the Heisenberg group which is not sigma-finite dimensional in the se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#31639;&#27861;&#26469;&#35299;&#20915;&#23398;&#20064;DAGs&#20013;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#22806;&#23618;&#21033;&#29992;&#25299;&#25169;&#20132;&#25442;&#20248;&#21270;&#25299;&#25169;&#39034;&#24207;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#20505;&#36873;&#20132;&#25442;&#23545;&#30340;&#26041;&#27861;&#65292;&#31639;&#27861;&#22312;&#23398;&#20064;&#39640;&#36136;&#37327;DAGs&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.17277</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#20132;&#25442;&#20248;&#21270;NOTEARS&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Optimizing NOTEARS Objectives via Topological Swaps. (arXiv:2305.17277v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#31639;&#27861;&#26469;&#35299;&#20915;&#23398;&#20064;DAGs&#20013;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#22806;&#23618;&#21033;&#29992;&#25299;&#25169;&#20132;&#25442;&#20248;&#21270;&#25299;&#25169;&#39034;&#24207;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#20505;&#36873;&#20132;&#25442;&#23545;&#30340;&#26041;&#27861;&#65292;&#31639;&#27861;&#22312;&#23398;&#20064;&#39640;&#36136;&#37327;DAGs&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#20986;&#29616;&#20102;&#19968;&#31867;&#26377;&#36259;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#28041;&#21450;&#21040;&#22312;&#32473;&#23450;&#25439;&#22833;&#25110;&#24471;&#20998;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#23567;&#21270;&#19968;&#20010;&#24809;&#32602;&#22270;&#20013;&#23384;&#22312;&#24490;&#29615;&#30340;&#38750;&#20984;&#36830;&#32493;&#32422;&#26463;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19982;&#36825;&#31867;&#38750;&#20984;&#31243;&#24207;&#30456;&#20851;&#30340;&#20248;&#21270;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#31639;&#27861;&#65292;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#21033;&#29992;&#38750;&#20984;&#32422;&#26463;&#12290;&#31639;&#27861;&#30340;&#22806;&#23618;&#36890;&#36807;&#36845;&#20195;&#22320;&#20132;&#25442;DAG&#30340;&#25299;&#25169;&#39034;&#24207;&#20013;&#30340;&#33410;&#28857;&#23545;&#26469;&#20248;&#21270;&#25299;&#25169;&#39034;&#24207;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#26159;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20026;&#27599;&#27425;&#36845;&#20195;&#29983;&#25104;&#19968;&#32452;&#20505;&#36873;&#20132;&#25442;&#23545;&#12290;&#22312;&#20869;&#23618;&#20013;&#65292;&#32473;&#23450;&#25299;&#25169;&#39034;&#24207;&#65292;&#25105;&#20204;&#21033;&#29992;&#33021;&#22815;&#22788;&#29702;&#32447;&#24615;&#32422;&#26463;&#30340;&#29616;&#25104;&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#65292;&#23427;&#20445;&#35777;&#25910;&#25947;&#21040;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#20010;&#31283;&#23450;&#28857;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#21487;&#33021;&#20250;&#38519;&#20837;&#20122;&#26368;&#20248;&#35299;&#20013;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#22312;&#23398;&#20064;&#39640;&#36136;&#37327;DAGs&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, an intriguing class of non-convex optimization problems has emerged in the context of learning directed acyclic graphs (DAGs). These problems involve minimizing a given loss or score function, subject to a non-convex continuous constraint that penalizes the presence of cycles in a graph. In this work, we delve into the optimization challenges associated with this class of non-convex programs. To address these challenges, we propose a bi-level algorithm that leverages the non-convex constraint in a novel way. The outer level of the algorithm optimizes over topological orders by iteratively swapping pairs of nodes within the topological order of a DAG. A key innovation of our approach is the development of an effective method for generating a set of candidate swapping pairs for each iteration. At the inner level, given a topological order, we utilize off-the-shelf solvers that can handle linear constraints. The key advantage of our proposed algorithm is that it is guaranteed to
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21338;&#24328;&#29702;&#35770;&#20013;&#26799;&#24230;&#26041;&#27861;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#65292;&#22312;&#37096;&#20998;&#26354;&#29575;&#26465;&#20214;&#19979;&#20445;&#35777;&#20102;&#23616;&#37096;&#25910;&#25947;&#65292;&#21516;&#26102;&#24179;&#22343;&#29305;&#24449;&#20540;&#27604;&#26368;&#23567;&#29305;&#24449;&#20540;&#26356;&#33021;&#20307;&#29616;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17275</link><description>&lt;p&gt;
&#27839;&#30528;&#37096;&#20998;&#26354;&#29575;&#65292;&#26799;&#24230;&#26041;&#27861;&#22312;&#38646;&#21644;&#21338;&#24328;&#20013;&#23616;&#37096;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Local Convergence of Gradient Methods for Min-Max Games under Partial Curvature. (arXiv:2305.17275v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21338;&#24328;&#29702;&#35770;&#20013;&#26799;&#24230;&#26041;&#27861;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#65292;&#22312;&#37096;&#20998;&#26354;&#29575;&#26465;&#20214;&#19979;&#20445;&#35777;&#20102;&#23616;&#37096;&#25910;&#25947;&#65292;&#21516;&#26102;&#24179;&#22343;&#29305;&#24449;&#20540;&#27604;&#26368;&#23567;&#29305;&#24449;&#20540;&#26356;&#33021;&#20307;&#29616;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38646;&#21644;&#21487;&#24494;&#20998;&#21338;&#24328;&#30340;&#26799;&#24230;&#26041;&#27861;&#23545;&#23616;&#37096;&#32435;&#20160;&#22343;&#34913;&#30340;&#25910;&#25947;&#24615;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#24403; $S \succ 0$ &#26102;&#65292;&#36825;&#31181;&#21160;&#24577;&#20250;&#22312;&#23616;&#37096;&#25910;&#25947;&#65292;&#32780;&#24403; $S = 0$ &#26102;&#21487;&#33021;&#20250;&#21457;&#25955;&#65292;&#20854;&#20013; $S \succeq 0$ &#26159;&#22343;&#34913;&#26102;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#23545;&#31216;&#37096;&#20998;&#65292;&#23427;&#21344;&#25454;&#20102;&#28216;&#25103;&#30340; &#8220;&#21183;&#33021;&#8221; &#32452;&#20214;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21482;&#35201; $S$ &#19981;&#20026;&#38646;&#65288;&#37096;&#20998;&#26354;&#29575;&#65289;&#65292;&#24182;&#19988;&#21453;&#23545;&#31216;&#37096;&#20998; $A$ &#30340;&#29305;&#24449;&#21521;&#37327;&#19982; $S$ &#30340;&#26680;&#30456;&#23545;&#20301;&#32622;&#33391;&#22909;&#65292;&#36825;&#20123;&#21160;&#24577;&#20063;&#20250;&#25910;&#25947;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403; $S \ll A$ &#26102;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#36890;&#24120;&#21462;&#20915;&#20110; $S$ &#30340;&#29305;&#24449;&#20540;&#30340;&#24179;&#22343;&#20540;&#65292;&#32780;&#19981;&#26159;&#26368;&#23567;&#20540;&#65292;&#36825;&#19982;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#31867;&#27604;&#25152;&#24314;&#35758;&#30340;&#30456;&#21453;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#32771;&#34385;&#35745;&#31639;&#36830;&#32493;&#21338;&#24328;&#30340;&#28151;&#21512;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#30001;&#20110;&#37096;&#20998;&#26354;&#29575;&#65292;&#38181;&#24418;&#31890;&#23376;&#26041;&#27861;&#8212;&#8212;&#23427;&#22312;&#28151;&#21512;&#31574;&#30053;&#30340;&#26435;&#37325;&#21644;&#25903;&#25345;&#19978;&#36827;&#34892;&#20248;&#21270;&#8212;&#8212;&#21576;&#29616;&#20986;&#19968;&#33268;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#65292;&#32780;&#26631;&#20934;&#30340;&#23545;&#20598;&#26799;&#24230;&#26041;&#27861;&#21017;&#21487;&#33021;&#20250;&#21457;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the convergence to local Nash equilibria of gradient methods for two-player zero-sum differentiable games. It is well-known that such dynamics converge locally when $S \succ 0$ and may diverge when $S=0$, where $S\succeq 0$ is the symmetric part of the Jacobian at equilibrium that accounts for the "potential" component of the game. We show that these dynamics also converge as soon as $S$ is nonzero (partial curvature) and the eigenvectors of the antisymmetric part $A$ are in general position with respect to the kernel of $S$. We then study the convergence rates when $S \ll A$ and prove that they typically depend on the average of the eigenvalues of $S$, instead of the minimum as an analogy with minimization problems would suggest. To illustrate our results, we consider the problem of computing mixed Nash equilibria of continuous games. We show that, thanks to partial curvature, conic particle methods -- which optimize over both weights and supports of the mixed strategies -- g
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#36710;&#36947;&#26816;&#27979;&#27969;&#27700;&#32447;&#65292;&#35813;&#27969;&#27700;&#32447;&#21253;&#25324;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#34987;&#37319;&#29992;&#20197;&#36890;&#36807;&#37325;&#26500;&#38543;&#26426;&#25513;&#33180;&#22270;&#20687;&#20013;&#30340;&#20002;&#22833;&#20687;&#32032;&#20026;&#30446;&#26631;&#26469;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#36710;&#36947;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17271</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#26041;&#27861;&#23454;&#29616;&#40065;&#26834;&#36710;&#36947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Lane Detection through Self Pre-training with Masked Sequential Autoencoders and Fine-tuning with Customized PolyLoss. (arXiv:2305.17271v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#36710;&#36947;&#26816;&#27979;&#27969;&#27700;&#32447;&#65292;&#35813;&#27969;&#27700;&#32447;&#21253;&#25324;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#34987;&#37319;&#29992;&#20197;&#36890;&#36807;&#37325;&#26500;&#38543;&#26426;&#25513;&#33180;&#22270;&#20687;&#20013;&#30340;&#20002;&#22833;&#20687;&#32032;&#20026;&#30446;&#26631;&#26469;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#36710;&#36947;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36947;&#26816;&#27979;&#26159;&#36710;&#36742;&#23450;&#20301;&#30340;&#20851;&#38190;&#65292;&#26159;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#21644;&#35768;&#22810;&#26234;&#33021;&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#36710;&#36947;&#26816;&#27979;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#21644;&#32858;&#21512;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#36710;&#36947;&#32447;&#21644;&#22270;&#20687;&#20013;&#20854;&#20182;&#21306;&#22495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#24182;&#25552;&#21319;&#36710;&#36947;&#26816;&#27979;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#34987;&#37319;&#29992;&#20197;&#36890;&#36807;&#37325;&#26500;&#38543;&#26426;&#25513;&#27169;&#22270;&#20687;&#20013;&#30340;&#20002;&#22833;&#20687;&#32032;&#20026;&#30446;&#26631;&#26469;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#22312;&#32454;&#35843;&#20998;&#21106;&#38454;&#27573;&#20013;&#65292;&#36830;&#32493;&#30340;&#22270;&#20687;&#24103;&#34987;&#29992;&#20316;&#36755;&#20837;&#65292;
&lt;/p&gt;
&lt;p&gt;
Lane detection is crucial for vehicle localization which makes it the foundation for automated driving and many intelligent and advanced driving assistant systems. Available vision-based lane detection methods do not make full use of the valuable features and aggregate contextual information, especially the interrelationships between lane lines and other regions of the images in continuous frames. To fill this research gap and upgrade lane detection performance, this paper proposes a pipeline consisting of self pre-training with masked sequential autoencoders and fine-tuning with customized PolyLoss for the end-to-end neural network models using multi-continuous image frames. The masked sequential autoencoders are adopted to pre-train the neural network models with reconstructing the missing pixels from a random masked image as the objective. Then, in the fine-tuning segmentation phase where lane detection segmentation is performed, the continuous image frames are served as the inputs,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#39640;&#21361;&#23381;&#20135;&#22919;&#35745;&#21010;&#65292;&#25552;&#20986;&#20102;&#26089;&#26399;&#22922;&#23072;&#26816;&#27979;&#12289;&#20934;&#30830;&#35782;&#21035;&#39640;&#39118;&#38505;&#20250;&#21592;&#21644;&#25552;&#20379;&#21487;&#35299;&#37322;&#25351;&#26631;&#31561;&#19977;&#20010;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#23381;&#26399;&#39118;&#38505;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.17261</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20851;&#38381;&#39640;&#21361;&#23381;&#20135;&#22919;&#25252;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Closing the Gap in High-Risk Pregnancy Care Using Machine Learning and Human-AI Collaboration. (arXiv:2305.17261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#39640;&#21361;&#23381;&#20135;&#22919;&#35745;&#21010;&#65292;&#25552;&#20986;&#20102;&#26089;&#26399;&#22922;&#23072;&#26816;&#27979;&#12289;&#20934;&#30830;&#35782;&#21035;&#39640;&#39118;&#38505;&#20250;&#21592;&#21644;&#25552;&#20379;&#21487;&#35299;&#37322;&#25351;&#26631;&#31561;&#19977;&#20010;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#23381;&#26399;&#39118;&#38505;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#20445;&#38505;&#20844;&#21496;&#36890;&#24120;&#20351;&#29992;&#31639;&#27861;&#26469;&#35782;&#21035;&#20250;&#21463;&#30410;&#20110;&#25252;&#29702;&#21644;&#29366;&#20917;&#31649;&#29702;&#35745;&#21010;&#30340;&#20250;&#21592;&#65292;&#35813;&#35745;&#21010;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#39640;&#31471;&#20020;&#24202;&#25903;&#25345;&#12290;&#31639;&#27861;&#35782;&#21035;&#19982;&#20020;&#24202;&#24178;&#39044;&#20043;&#38388;&#30340;&#21450;&#26102;&#12289;&#20934;&#30830;&#21644;&#26080;&#32541;&#38598;&#25104;&#21462;&#20915;&#20110;&#31995;&#32479;&#35774;&#35745;&#24072;&#21644;&#25252;&#29702;&#31649;&#29702;&#21592;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;&#12290;&#25105;&#20204;&#20851;&#27880;&#20102;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#23381;&#20135;&#22919;&#19981;&#33391;&#20135;&#21069;&#12289;&#20135;&#26399;&#21644;&#20135;&#21518;&#20107;&#20214;&#30340;&#39640;&#21361;&#23381;&#20135;&#22919;&#35745;&#21010;&#65292;&#24182;&#25551;&#36848;&#20102;&#25105;&#20204;&#22914;&#20309;&#20811;&#26381;&#25252;&#29702;&#31649;&#29702;&#21592;&#25152;&#36848;&#30340;&#19977;&#20010;HRP&#35745;&#21010;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#26089;&#26399;&#26816;&#27979;&#22922;&#23072;&#65292;&#65288;2&#65289;&#20934;&#30830;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#39640;&#39118;&#38505;&#20250;&#21592;&#65292;&#20197;&#21450;&#65288;3&#65289;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#25351;&#26631;&#26469;&#34917;&#20805;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23381;&#26399;&#35782;&#21035;&#31639;&#27861;&#65292;&#22312;&#22238;&#39038;&#24615;&#30740;&#31350;&#20013;&#27604;&#20043;&#21069;&#22522;&#20110;&#20195;&#30721;&#30340;&#27169;&#22411;&#25552;&#21069;&#20102;57&#22825;&#35782;&#21035;&#22922;&#23072;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#20250;&#24433;&#21709;&#23381;&#26399;&#30340;&#24182;&#21457;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Health insurers often use algorithms to identify members who would benefit from care and condition management programs, which provide personalized, high-touch clinical support. Timely, accurate, and seamless integration between algorithmic identification and clinical intervention depends on effective collaboration between the system designers and nurse care managers. We focus on a high-risk pregnancy (HRP) program designed to reduce the likelihood of adverse prenatal, perinatal, and postnatal events and describe how we overcome three challenges of HRP programs as articulated by nurse care managers; (1) early detection of pregnancy, (2) accurate identification of impactable high-risk members, and (3) provision of explainable indicators to supplement predictions. We propose a novel algorithm for pregnancy identification that identifies pregnancies 57 days earlier than previous code-based models in a retrospective study. We then build a model to predict impactable pregnancy complications 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#30340;&#20381;&#36182;&#24615;&#65292;&#21457;&#29616;&#22823;&#22411;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#65292;&#36825;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.17256</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26159;&#25042;&#24816;&#30340;&#23398;&#20064;&#32773;&#65306;&#20998;&#26512;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#25463;&#24452;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning. (arXiv:2305.17256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#30340;&#20381;&#36182;&#24615;&#65292;&#21457;&#29616;&#22823;&#22411;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#65292;&#36825;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20854;&#20013;LLM&#36890;&#36807;&#20960;&#20010;&#36755;&#20837;-&#26631;&#31614;&#23545;&#65288;&#25552;&#31034;&#65289;&#30340;&#26465;&#20214;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#28508;&#21147;&#24040;&#22823;&#65292;&#20294;&#25105;&#20204;&#23545;&#24433;&#21709;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31283;&#20581;&#24615;&#30340;&#22240;&#32032;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;LLM&#23545;&#25552;&#31034;&#20869;&#25463;&#24452;&#25110;&#20551;&#30456;&#20851;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#24357;&#34917;&#36825;&#19968;&#30693;&#35782;&#24046;&#36317;&#12290;&#36890;&#36807;&#20998;&#31867;&#21644;&#25277;&#21462;&#20219;&#21153;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLM&#26159;&#8220;&#25042;&#24816;&#23398;&#20064;&#32773;&#8221;&#30340;&#20107;&#23454;&#65292;&#23427;&#24448;&#24448;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#26469;&#33719;&#21462;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#65292;&#21363;&#36739;&#22823;&#30340;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are "lazy learners" that tend to exploit shortcuts in prompts for downstream tasks. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts.
&lt;/p&gt;</description></item><item><title>FineMorphs&#26159;&#19968;&#31181;&#22810;&#20803;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24418;&#29366;&#20998;&#26512;&#20013;&#30340;&#24494;&#20998;&#21516;&#32986;&#27010;&#24565;&#23545;&#27169;&#22411;&#29366;&#24577;&#36827;&#34892;&#20248;&#21270;&#65292;&#33021;&#22815;&#33258;&#28982;&#22320;&#20943;&#23569;&#65288;&#25110;&#22686;&#21152;&#65289;&#32500;&#24230;&#24182;&#36866;&#24212;&#22823;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.17255</link><description>&lt;p&gt;
FineMorphs:&#29992;&#20110;&#22238;&#24402;&#30340;&#20223;&#23556;-&#24494;&#20998;&#21516;&#32986;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FineMorphs: Affine-diffeomorphic sequences for regression. (arXiv:2305.17255v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17255
&lt;/p&gt;
&lt;p&gt;
FineMorphs&#26159;&#19968;&#31181;&#22810;&#20803;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24418;&#29366;&#20998;&#26512;&#20013;&#30340;&#24494;&#20998;&#21516;&#32986;&#27010;&#24565;&#23545;&#27169;&#22411;&#29366;&#24577;&#36827;&#34892;&#20248;&#21270;&#65292;&#33021;&#22815;&#33258;&#28982;&#22320;&#20943;&#23569;&#65288;&#25110;&#22686;&#21152;&#65289;&#32500;&#24230;&#24182;&#36866;&#24212;&#22823;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20223;&#23556;&#21644;&#24494;&#20998;&#21516;&#32986;&#21464;&#25442;&#24207;&#21015;&#30340;&#22810;&#20803;&#22238;&#24402;&#27169;&#22411;FineMorphs&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#24418;&#29366;&#20998;&#26512;&#30340;&#27010;&#24565;&#65292;&#22312;&#23398;&#20064;&#26399;&#38388;&#36890;&#36807;&#30001;&#20809;&#28369;&#21521;&#37327;&#22330;&#29983;&#25104;&#30340;&#24494;&#20998;&#21516;&#32986;&#20248;&#21270;&#22320;&#8220;&#37325;&#22609;&#8221;&#27169;&#22411;&#29366;&#24577;&#12290;&#20223;&#23556;&#21464;&#25442;&#21644;&#21521;&#37327;&#22330;&#22312;&#26368;&#20248;&#25511;&#21046;&#29615;&#22659;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#27425;&#20248;&#21521;&#37327;&#22330;&#33258;&#28982;&#22320;&#20943;&#23569;&#65288;&#25110;&#22686;&#21152;&#65289;&#32500;&#24230;&#24182;&#36866;&#24212;&#22823;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#35813;&#27169;&#22411;&#30340;&#35299;&#23384;&#22312;&#24615;&#35777;&#26126;&#21644;&#26368;&#20248;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FineMorphs&#22312;&#19982;&#25991;&#29486;&#20013;&#26368;&#20808;&#36827;&#21644;&#22522;&#20110;TensorFlow&#30340;&#31264;&#23494;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#27604;&#36739;&#20013;&#65292;&#21462;&#24471;&#20102;&#26377;&#21033;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A multivariate regression model of affine and diffeomorphic transformation sequences - FineMorphs - is presented. Leveraging concepts from shape analysis, model states are optimally "reshaped" by diffeomorphisms generated by smooth vector fields during learning. Affine transformations and vector fields are optimized within an optimal control setting, and the model can naturally reduce (or increase) dimensionality and adapt to large datasets via suboptimal vector fields. An existence proof of solution and necessary conditions for optimality for the model are derived. Experimental results on real datasets from the UCI repository are presented, with favorable results in comparison with state-of-the-art in the literature and densely-connected neural networks in TensorFlow.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#21463;&#38480;&#21046;&#20869;&#26680;&#26426;&#22120;&#26041;&#27861;&#32467;&#21512;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#21407;&#22987;-&#23545;&#20598;&#22810;&#35282;&#24230;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#21407;&#22987;&#21644;&#23545;&#20598;&#20844;&#24335;&#30340;&#23436;&#20840;&#31561;&#20215;&#24615;&#65292;&#24182;&#26368;&#32456;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#31561;&#20215;&#24615;&#21644;&#25552;&#20379;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2305.17251</link><description>&lt;p&gt;
&#22810;&#35282;&#24230;&#21463;&#38480;&#21046;&#20869;&#26680;&#26426;&#22120;&#30340;&#23545;&#20598;&#24615;
&lt;/p&gt;
&lt;p&gt;
Duality in Multi-View Restricted Kernel Machines. (arXiv:2305.17251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17251
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#21463;&#38480;&#21046;&#20869;&#26680;&#26426;&#22120;&#26041;&#27861;&#32467;&#21512;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#21407;&#22987;-&#23545;&#20598;&#22810;&#35282;&#24230;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#21407;&#22987;&#21644;&#23545;&#20598;&#20844;&#24335;&#30340;&#23436;&#20840;&#31561;&#20215;&#24615;&#65292;&#24182;&#26368;&#32456;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#31561;&#20215;&#24615;&#21644;&#25552;&#20379;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#21463;&#38480;&#21046;&#20869;&#26680;&#26426;&#22120;&#26041;&#27861;&#32467;&#21512;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#21407;&#22987;-&#23545;&#20598;&#22810;&#35282;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#21487;&#20197;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#35813;&#26694;&#26550;&#30340;&#21407;&#22987;&#21644;&#23545;&#20598;&#34920;&#31034;&#65292;&#24182;&#20174;&#29702;&#35770;&#35282;&#24230;&#20851;&#32852;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#21407;&#22987;&#21464;&#37327;&#26469;&#23454;&#29616;&#21407;&#22987;&#21644;&#23545;&#20598;&#20844;&#24335;&#30340;&#23436;&#20840;&#31561;&#20215;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#36882;&#24402;&#39044;&#27979;&#26410;&#30475;&#21040;&#27979;&#35797;&#25968;&#25454;&#24182;&#21487;&#35270;&#21270;&#25152;&#23398;&#29305;&#24449;&#30340;&#26041;&#24335;&#65292;&#22312;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#39564;&#35777;&#20102;&#31561;&#20215;&#24615;&#24182;&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#20851;&#31995;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a unifying setting that combines existing restricted kernel machine methods into a single primal-dual multi-view framework for kernel principal component analysis in both supervised and unsupervised settings. We derive the primal and dual representations of the framework and relate different training and inference algorithms from a theoretical perspective. We show how to achieve full equivalence in primal and dual formulations by rescaling primal variables. Finally, we experimentally validate the equivalence and provide insight into the relationships between different methods on a number of time series data sets by recursively forecasting unseen test data and visualizing the learned features.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#22870;&#21169;&#30340;&#20219;&#21153;&#38388;&#36827;&#34892;&#34892;&#20026;&#36801;&#31227;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#19968;&#20123;&#38543;&#26426;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#33021;&#22815;&#26263;&#21547;&#38271;&#26399;&#29615;&#22659;&#21160;&#24577;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#38544;&#24335;&#27169;&#22411;&#30340;&#35268;&#21010;&#25216;&#26415;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36866;&#24212;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17250</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#29305;&#24449;&#30340;&#33258;&#30417;&#30563;&#22686;&#24378;&#23398;&#20064;&#23454;&#29616;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Reinforcement Learning that Transfers using Random Features. (arXiv:2305.17250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17250
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#22870;&#21169;&#30340;&#20219;&#21153;&#38388;&#36827;&#34892;&#34892;&#20026;&#36801;&#31227;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#19968;&#20123;&#38543;&#26426;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#33021;&#22815;&#26263;&#21547;&#38271;&#26399;&#29615;&#22659;&#21160;&#24577;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#38544;&#24335;&#27169;&#22411;&#30340;&#35268;&#21010;&#25216;&#26415;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36866;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35299;&#20915;&#20855;&#26377;&#39640;&#32500;&#35266;&#27979;&#21644;&#38271;&#26399;&#20915;&#31574;&#26041;&#26696;&#30340;&#21333;&#20219;&#21153;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#38590;&#20197;&#27178;&#36328;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#21017;&#23398;&#20064;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#33258;&#28982;&#22320;&#23454;&#29616;&#20102;&#19981;&#21516;&#22870;&#21169;&#20989;&#25968;&#30340;&#36801;&#31227;&#65292;&#20294;&#30001;&#20110;&#35823;&#24046;&#30340;&#32047;&#31215;&#32780;&#38590;&#20197;&#36866;&#24212;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;&#20026;&#20102;&#23454;&#29616;&#20004;&#32773;&#20860;&#39038;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#22312;&#20855;&#26377;&#19981;&#21516;&#22870;&#21169;&#30340;&#20219;&#21153;&#38388;&#36827;&#34892;&#34892;&#20026;&#36801;&#31227;&#65292;&#21516;&#26102;&#36991;&#24320;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#33258;&#30001;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#29992;&#19968;&#20123;&#38543;&#26426;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#33021;&#22815;&#26263;&#21547;&#38271;&#26399;&#29615;&#22659;&#21160;&#24577;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#36825;&#20123;&#38544;&#24335;&#27169;&#22411;&#30340;&#35268;&#21010;&#25216;&#26415;&#65288;&#22914;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65289;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36866;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-free reinforcement learning algorithms have exhibited great potential in solving single-task sequential decision-making problems with high-dimensional observations and long horizons, but are known to be hard to generalize across tasks. Model-based RL, on the other hand, learns task-agnostic models of the world that naturally enables transfer across different reward functions, but struggles to scale to complex environments due to the compounding error. To get the best of both worlds, we propose a self-supervised reinforcement learning method that enables the transfer of behaviors across tasks with different rewards, while circumventing the challenges of model-based RL. In particular, we show self-supervised pre-training of model-free reinforcement learning with a number of random features as rewards allows implicit modeling of long-horizon environment dynamics. Then, planning techniques like model-predictive control using these implicit models enable fast adaptation to problems wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NASimEmu&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#26234;&#33021;&#20307;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#27169;&#25311;&#22120;&#21644;&#20223;&#30495;&#22120;&#30340;&#32467;&#21512;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20223;&#30495;&#22120;&#20013;&#37096;&#32626;&#65292;&#20174;&#32780;&#39564;&#35777;&#25152;&#20351;&#29992;&#30340;&#25277;&#35937;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#26694;&#26550;&#30340;&#35774;&#35745;&#26088;&#22312;&#22521;&#35757;&#36890;&#29992;&#30340;&#26234;&#33021;&#20307;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#22330;&#26223;&#20013;&#36827;&#34892;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.17246</link><description>&lt;p&gt;
NASimEmu: &#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#22330;&#26223;&#30340;&#26234;&#33021;&#20307;&#30340;&#32593;&#32476;&#25915;&#20987;&#27169;&#25311;&#22120;&#19982;&#20223;&#30495;&#22120;
&lt;/p&gt;
&lt;p&gt;
NASimEmu: Network Attack Simulator &amp; Emulator for Training Agents Generalizing to Novel Scenarios. (arXiv:2305.17246v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NASimEmu&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#26234;&#33021;&#20307;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#27169;&#25311;&#22120;&#21644;&#20223;&#30495;&#22120;&#30340;&#32467;&#21512;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20223;&#30495;&#22120;&#20013;&#37096;&#32626;&#65292;&#20174;&#32780;&#39564;&#35777;&#25152;&#20351;&#29992;&#30340;&#25277;&#35937;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#26694;&#26550;&#30340;&#35774;&#35745;&#26088;&#22312;&#22521;&#35757;&#36890;&#29992;&#30340;&#26234;&#33021;&#20307;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#22330;&#26223;&#20013;&#36827;&#34892;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29992;&#20110;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22521;&#35757;&#25915;&#20987;&#22411;&#28183;&#36879;&#27979;&#35797;&#26234;&#33021;&#20307;&#30340;&#26694;&#26550;&#24448;&#24448;&#38590;&#20197;&#20351;&#26234;&#33021;&#20307;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21407;&#22240;&#22312;&#20110;&#22522;&#20110;&#27169;&#25311;&#30340;&#26694;&#26550;&#20013;&#23384;&#22312;&#29616;&#23454;&#24046;&#36317;&#65292;&#32780;&#22522;&#20110;&#20223;&#30495;&#30340;&#26694;&#26550;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#26694;&#26550;&#36890;&#24120;&#20351;&#29992;&#19981;&#29616;&#23454;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;NASimEmu&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#20855;&#26377;&#20849;&#20139;&#25509;&#21475;&#30340;&#27169;&#25311;&#22120;&#21644;&#20223;&#30495;&#22120;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#26234;&#33021;&#20307;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20223;&#30495;&#22120;&#20013;&#37096;&#32626;&#65292;&#20174;&#32780;&#39564;&#35777;&#25152;&#20351;&#29992;&#30340;&#25277;&#35937;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20419;&#36827;&#20102;&#33021;&#22815;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#22330;&#26223;&#20013;&#36827;&#34892;&#36716;&#31227;&#30340;&#36890;&#29992;&#26234;&#33021;&#20307;&#30340;&#24320;&#21457;&#12290;&#23545;&#20110;&#27169;&#25311;&#37096;&#20998;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#27169;&#25311;&#22120;NASim&#24182;&#22686;&#24378;&#20102;&#20854;&#23454;&#29616;&#12290;&#20223;&#30495;&#22120;&#20351;&#29992;&#34892;&#19994;&#32423;&#24037;&#20855;&#23454;&#26045;&#65292;&#22914;Vagrant&#12289;VirtualBox&#21644;Metasp&#12290;
&lt;/p&gt;
&lt;p&gt;
Current frameworks for training offensive penetration testing agents with deep reinforcement learning struggle to produce agents that perform well in real-world scenarios, due to the reality gap in simulation-based frameworks and the lack of scalability in emulation-based frameworks. Additionally, existing frameworks often use an unrealistic metric that measures the agents' performance on the training data. NASimEmu, a new framework introduced in this paper, addresses these issues by providing both a simulator and an emulator with a shared interface. This approach allows agents to be trained in simulation and deployed in the emulator, thus verifying the realism of the used abstraction. Our framework promotes the development of general agents that can transfer to novel scenarios unseen during their training. For the simulation part, we adopt an existing simulator NASim and enhance its realism. The emulator is implemented with industry-level tools, such as Vagrant, VirtualBox, and Metasp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65306;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#65292;&#23427;&#26159;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#21644;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#27867;&#21270;&#21644;&#29305;&#20363;&#65292;&#20854;&#30446;&#26631;&#26159;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#65292;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.17225</link><description>&lt;p&gt;
&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causal Component Analysis. (arXiv:2305.17225v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65306;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#65292;&#23427;&#26159;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#21644;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#27867;&#21270;&#21644;&#29305;&#20363;&#65292;&#20854;&#30446;&#26631;&#26159;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#65292;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#30340;&#30446;&#26631;&#26159;&#20174;&#28151;&#21512;&#35266;&#27979;&#21040;&#30340;&#21464;&#37327;&#20013;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#32780;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#30446;&#26631;&#26159;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#24378;&#30456;&#20851;&#24615;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#20197;&#21450;&#32534;&#30721;&#23427;&#20204;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26410;&#30693;&#22270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65292;&#31216;&#20026;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#12290;CauCA&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;ICA&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#23545;&#28508;&#22312;&#25104;&#20998;&#20043;&#38388;&#30340;&#22240;&#26524;&#20381;&#36182;&#24314;&#27169;&#65292;&#20063;&#26159;CRL&#30340;&#19968;&#20010;&#29305;&#20363;&#12290;&#19982;CRL&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#65292;&#20165;&#20851;&#27880;&#20110;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#12290;&#25152;&#26377;&#20851;&#20110;CauCA&#22238;&#25910;&#22522;&#30784;&#30495;&#30456;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;CRL&#65292;&#32780;&#21487;&#33021;&#24615;&#32467;&#26524;&#21487;&#20197;&#20316;&#20026;&#25193;&#23637;CRL&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#23558;&#20174;&#23545;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#23454;&#26045;&#19981;&#21516;&#31867;&#22411;&#24178;&#39044;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#24449;CauCA&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Independent Component Analysis (ICA) aims to recover independent latent variables from observed mixtures thereof. Causal Representation Learning (CRL) aims instead to infer causally related (thus often statistically dependent) latent variables, together with the unknown graph encoding their causal relationships. We introduce an intermediate problem termed Causal Component Analysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the causal dependence among the latent components, and as a special case of CRL. In contrast to CRL, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. Any impossibility results regarding the recovery of the ground truth in CauCA also apply for CRL, while possibility results may serve as a stepping stone for extensions to CRL. We characterize CauCA identifiability from multiple datasets generated through different types of interventions on the latent causal variables. As a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#35777;&#26497;&#23567;&#26497;&#20540;&#20248;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17224</link><description>&lt;p&gt;
&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#27861;&#24555;&#36895;&#26497;&#23567;&#21270;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fast and Minimax Optimal Estimation of Low-Rank Matrices via Non-Convex Gradient Descent. (arXiv:2305.17224v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#35777;&#26497;&#23567;&#26497;&#20540;&#20248;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22122;&#22768;&#27979;&#37327;&#20013;&#20272;&#35745;&#20302;&#31209;&#30697;&#38453;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#26088;&#22312;&#23454;&#29616;&#26497;&#23567;&#26497;&#20540;&#35823;&#24046;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#30340;&#33021;&#21147;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#20351;&#29992;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#26469;&#35299;&#20915;&#12290;&#29702;&#35770;&#19978;&#65292;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#26497;&#23567;&#26497;&#20540;&#35823;&#24046;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#32463;&#24120;&#25910;&#25947;&#24471;&#38750;&#24120;&#32531;&#24930;&#65292;&#20197;&#33267;&#20110;&#29978;&#33267;&#26080;&#27861;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#25552;&#20379;&#36866;&#24230;&#20934;&#30830;&#30340;&#20272;&#35745;&#20540;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#37325;&#26032;&#32553;&#25918;&#25110;&#39044;&#22788;&#29702;&#25913;&#36827;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#26041;&#27861;&#20063;&#20250;&#22823;&#22823;&#25918;&#22823;&#27979;&#37327;&#35823;&#24046;&#65292;&#23548;&#33268;&#24471;&#21040;&#30340;&#20272;&#35745;&#27604;&#29702;&#35770;&#19978;&#21487;&#23454;&#29616;&#30340;&#26497;&#23567;&#26497;&#20540;&#35823;&#24046;&#23569;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36890;&#24120;&#30340;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21487;&#35777;&#26126;&#20445;&#30041;&#20854;&#26497;&#23567;&#26497;&#20540;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of estimating a low-rank matrix from noisy measurements, with the specific goal of achieving minimax optimal error. In practice, the problem is commonly solved using non-convex gradient descent, due to its ability to scale to large-scale real-world datasets. In theory, non-convex gradient descent is capable of achieving minimax error. But in practice, it often converges extremely slowly, such that it cannot even deliver estimations of modest accuracy within reasonable time. On the other hand, methods that improve the convergence of non-convex gradient descent, through rescaling or preconditioning, also greatly amplify the measurement noise, resulting in estimations that are orders of magnitude less accurate than what is theoretically achievable with minimax optimal error. In this paper, we propose a slight modification to the usual non-convex gradient descent method that remedies the issue of slow convergence, while provably preserving its minimax optimality. Our p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#35821;&#20041;&#35299;&#26512;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#35774;&#32622;&#21644;&#26032;&#31639;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#26032;&#31639;&#27861;FedSQL&#21644;Lorar&#20248;&#20110;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#35774;&#32622;&#30340;&#24378;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.17221</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#35821;&#20041;&#35299;&#26512;&#20219;&#21153;&#65306;&#20219;&#21153;&#24418;&#24335;&#65292;&#35780;&#20272;&#35774;&#32622;&#21450;&#26032;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for Semantic Parsing: Task Formulation, Evaluation Setup, New Algorithms. (arXiv:2305.17221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#35821;&#20041;&#35299;&#26512;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#35774;&#32622;&#21644;&#26032;&#31639;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#26032;&#31639;&#27861;FedSQL&#21644;Lorar&#20248;&#20110;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#35774;&#32622;&#30340;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#65292;&#21363;&#38024;&#23545;&#35821;&#20041;&#35299;&#26512;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#20854;&#35821;&#20041;&#20998;&#26512;&#25968;&#25454;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#32852;&#37030;&#23398;&#20064;&#27169;&#24335;&#23545;&#20110;&#37027;&#20123;&#27809;&#26377;&#36275;&#22815;&#35757;&#32451;&#25968;&#25454;&#26469;&#24320;&#21457;&#19968;&#20010;&#25968;&#25454;&#39269;&#39295;&#30340;&#31070;&#32463;&#35821;&#20041;&#20998;&#26512;&#22120;&#30340;&#23458;&#25143;&#31471;&#23588;&#20854;&#26377;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#35774;&#32622;&#26469;&#30740;&#31350;&#36825;&#20010;&#20219;&#21153;&#65292;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;&#21333;&#22495;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#20316;&#20026;&#23458;&#25143;&#31471;&#26469;&#24418;&#25104;&#19968;&#20010;&#29616;&#23454;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#29616;&#23454;&#35774;&#32622;&#20013;&#23458;&#25143;&#32676;&#30340;&#24322;&#36136;&#24615;&#24456;&#39640;&#65292;&#26631;&#20934;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#25152;&#20197;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;LOss Reduction Adjusted Re-weighting (Lorar)&#26469;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;&#65292;&#35813;&#26426;&#21046;&#22522;&#20110;&#23458;&#25143;&#31471;&#27599;&#36718;&#35757;&#32451;&#25439;&#22833;&#30340;&#20943;&#23569;&#24773;&#20917;&#26469;&#35843;&#33410;&#27599;&#20010;&#23458;&#25143;&#31471;&#23545;&#20110;&#20840;&#23616;&#27169;&#22411;&#26356;&#26032;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30452;&#35273;&#26159;&#65292;&#25439;&#22833;&#20943;&#23569;&#30340;&#36234;&#22810;&#65292;&#23458;&#25143;&#31471;&#31163;&#20840;&#23616;&#26368;&#20248;&#35299;&#23601;&#36234;&#36828;&#65292;&#20854;&#23545;&#27169;&#22411;&#26356;&#26032;&#30340;&#36129;&#29486;&#23601;&#24212;&#35813;&#36234;&#39640;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#24322;&#26500;&#25991;&#26412;&#21040;SQL FL&#35774;&#32622;&#30340;&#26032;&#30340;FL&#31639;&#27861;FedSQL&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FedSQL&#21644;Lorar&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;FL&#35774;&#32622;&#20013;&#30340;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a new task of federated learning (FL) for semantic parsing, where multiple clients collaboratively train one global model without sharing their semantic parsing data. By leveraging data from multiple clients, the FL paradigm can be especially beneficial for clients that have little training data to develop a data-hungry neural semantic parser on their own. We propose an evaluation setup to study this task, where we re-purpose widely-used single-domain text-to-SQL datasets as clients to form a realistic heterogeneous FL setting and collaboratively train a global model. As standard FL algorithms suffer from the high client heterogeneity in our realistic setup, we further propose a novel LOss Reduction Adjusted Re-weighting (Lorar) mechanism to mitigate the performance degradation, which adjusts each client's contribution to the global model update based on its training loss reduction during each round. Our intuition is that the larger the loss reduction, the further aw
&lt;/p&gt;</description></item><item><title>GVdoc &#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25991;&#26723;&#20998;&#31867;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#25991;&#26723;&#22270;&#24182;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#33410;&#28857;&#21644;&#22270;&#23884;&#20837;&#65292;&#26377;&#25928;&#35299;&#20915;&#35270;&#35273;&#25991;&#26723;&#20998;&#31867;&#22120;&#22312;&#39046;&#22495;&#20869;&#22806;&#26679;&#26412;&#20998;&#31867;&#21644;&#21306;&#20998;&#20013;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.17219</link><description>&lt;p&gt;
GVdoc: &#22522;&#20110;&#22270;&#30340;&#35270;&#35273;&#25991;&#26723;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
GVdoc: Graph-based Visual Document Classification. (arXiv:2305.17219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17219
&lt;/p&gt;
&lt;p&gt;
GVdoc &#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25991;&#26723;&#20998;&#31867;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#25991;&#26723;&#22270;&#24182;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#33410;&#28857;&#21644;&#22270;&#23884;&#20837;&#65292;&#26377;&#25928;&#35299;&#20915;&#35270;&#35273;&#25991;&#26723;&#20998;&#31867;&#22120;&#22312;&#39046;&#22495;&#20869;&#22806;&#26679;&#26412;&#20998;&#31867;&#21644;&#21306;&#20998;&#20013;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#40065;&#26834;&#24615;&#21462;&#20915;&#20110;&#20854;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#21644;&#23545;&#39046;&#22495;&#20869;&#22806;&#26679;&#26412;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#35270;&#35273;&#25991;&#26723;&#20998;&#31867;&#22120;&#22312;&#20998;&#24067;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27491;&#30830;&#20998;&#31867;&#21644;&#21306;&#20998;&#39046;&#22495;&#22806;&#20363;&#23376;&#26041;&#38754;&#24448;&#24448;&#36935;&#21040;&#22256;&#38590;&#12290;&#22522;&#20110;&#22270;&#30340;&#25991;&#26723;&#20998;&#31867;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;&#24067;&#23616;&#30340;&#25991;&#26723;&#22270;&#65292;&#28982;&#21518;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#33410;&#28857;&#21644;&#22270;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21363;&#20351;&#21442;&#25968;&#26356;&#23569;&#65292;&#22312;&#39046;&#22495;&#22806;&#30340;&#26679;&#26412;&#19978;&#20063;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#22312;&#39046;&#22495;&#20869;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of a model for real-world deployment is decided by how well it performs on unseen data and distinguishes between in-domain and out-of-domain samples. Visual document classifiers have shown impressive performance on in-distribution test sets. However, they tend to have a hard time correctly classifying and differentiating out-of-distribution examples. Image-based classifiers lack the text component, whereas multi-modality transformer-based models face the token serialization problem in visual documents due to their diverse layouts. They also require a lot of computing power during inference, making them impractical for many real-world applications. We propose, GVdoc, a graph-based document classification model that addresses both of these challenges. Our approach generates a document graph based on its layout, and then trains a graph neural network to learn node and graph embeddings. Through experiments, we show that our model, even with fewer parameters, outperforms stat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#33021;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#22270;&#20687;&#36755;&#20986;&#65292;&#21516;&#26102;&#20063;&#33021;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2305.17216</link><description>&lt;p&gt;
&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
Generating Images with Multimodal Language Models. (arXiv:2305.17216v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#33021;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#22270;&#20687;&#36755;&#20986;&#65292;&#21516;&#26102;&#20063;&#33021;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#20165;&#21253;&#21547;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#36890;&#36807;&#26144;&#23556;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65306;&#22270;&#20687;&#26816;&#32034;&#12289;&#26032;&#39062;&#22270;&#20687;&#29983;&#25104;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#33021;&#22815;&#22312;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#36827;&#34892;&#26465;&#20214;&#35843;&#33410;&#65292;&#29983;&#25104;&#36830;&#36143;&#22270;&#20687;&#65288;&#21644;&#25991;&#26412;&#65289;&#36755;&#20986;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26144;&#23556;&#32593;&#32476;&#65292;&#23558;LLM&#22522;&#20110;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#30340;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#20026;&#35270;&#35273;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#21033;&#29992;LLM&#24378;&#22823;&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#35270;&#35273;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#19988;&#22797;&#26434;&#35821;&#35328;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#22522;&#20934;&#29983;&#25104;&#27169;&#22411;&#12290;&#38500;&#20102;&#26032;&#39062;&#22270;&#20687;&#29983;&#25104;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#26816;&#32034;&#22270;&#20687;&#65292;&#24182;&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26059;&#36716;&#20248;&#21270;&#22120;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#21487;&#20197;&#31616;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#65292;&#29978;&#33267;&#22312;&#20960;&#20046;&#19981;&#38656;&#35843;&#25972;&#22522;&#32447;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2305.17212</link><description>&lt;p&gt;
&#26059;&#36716;&#20248;&#21270;&#22120;&#65306;&#31616;&#21333;&#32780;&#24378;&#20581;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotational Optimizers: Simple &amp; Robust DNN Training. (arXiv:2305.17212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26059;&#36716;&#20248;&#21270;&#22120;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#21487;&#20197;&#31616;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#65292;&#29978;&#33267;&#22312;&#20960;&#20046;&#19981;&#38656;&#35843;&#25972;&#22522;&#32447;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#21462;&#20915;&#20110;&#23398;&#20064;&#29575;&#12289;&#26435;&#37325;&#34928;&#20943;&#12289;&#21021;&#22987;&#21270;&#31561;&#36229;&#21442;&#25968;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#20132;&#20114;&#20316;&#29992;&#21487;&#20197;&#22312;&#23610;&#24230;&#19981;&#21464;&#23618;&#65288;&#22914;&#24402;&#19968;&#21270;&#23618;&#65289;&#20013;&#20135;&#29983;&#29699;&#38754;&#36816;&#21160;&#21160;&#24577;&#65292;&#36825;&#20123;&#21160;&#24577;&#25910;&#25947;&#21040;&#24179;&#34913;&#29366;&#24577;&#65292;&#20854;&#20013;&#26435;&#37325;&#33539;&#25968;&#21644;&#39044;&#26399;&#26059;&#36716;&#26356;&#26032;&#22823;&#23567;&#26159;&#22266;&#23450;&#30340;&#12290;&#25105;&#20204;&#23545;AdamW&#12289;&#24102;&#21160;&#37327;&#30340;SGD&#21644;Lion&#20013;&#30340;&#36825;&#20010;&#24179;&#34913;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#21516;&#36229;&#21442;&#25968;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20123;&#20248;&#21270;&#22120;&#30340;&#26059;&#36716;&#21464;&#20307;&#65288;RVs&#65289;&#65292;&#24378;&#21046;&#39044;&#26399;&#35282;&#24230;&#26356;&#26032;&#22823;&#23567;&#19982;&#25972;&#20010;&#35757;&#32451;&#26399;&#38388;&#30340;&#24179;&#34913;&#20540;&#30456;&#21305;&#37197;&#12290;&#36825;&#31616;&#21270;&#20102;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#28040;&#38500;&#25910;&#25947;&#21040;&#24179;&#34913;&#29366;&#24577;&#30340;&#30636;&#24577;&#30456;&#24212;&#12290;&#25105;&#20204;&#30340;&#26059;&#36716;&#20248;&#21270;&#22120;&#21487;&#20197;&#21305;&#37197;&#21407;&#22987;&#21464;&#20307;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#38656;&#35201;&#23545;&#22522;&#32447;&#36229;&#21442;&#25968;&#36827;&#34892;&#26368;&#23569;&#25110;&#19981;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training dynamics of modern deep neural networks depend on complex interactions between the learning rate, weight decay, initialization, and other hyperparameters. These interactions can give rise to Spherical Motion Dynamics in scale-invariant layers (e.g., normalized layers), which converge to an equilibrium state, where the weight norm and the expected rotational update size are fixed. Our analysis of this equilibrium in AdamW, SGD with momentum, and Lion provides new insights into the effects of different hyperparameters and their interactions on the training process. We propose rotational variants (RVs) of these optimizers that force the expected angular update size to match the equilibrium value throughout training. This simplifies the training dynamics by removing the transient phase corresponding to the convergence to an equilibrium. Our rotational optimizers can match the performance of the original variants, often with minimal or no tuning of the baseline hyperparameters,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21151;&#33021;&#27969;&#21305;&#37197;&#65288;FFM&#65289;&#30340;&#20989;&#25968;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#27010;&#29575;&#27979;&#24230;&#25554;&#20540;&#21644;&#23398;&#20064;&#24213;&#23618;&#20989;&#25968;&#31354;&#38388;&#19978;&#29983;&#25104;&#27979;&#24230;&#30340;&#21521;&#37327;&#22330;&#26469;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#12290;&#36825;&#31181;&#26080;&#38656;&#20284;&#28982;&#25110;&#27169;&#25311;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#20960;&#31181;&#20989;&#25968;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.17209</link><description>&lt;p&gt;
&#21151;&#33021;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Functional Flow Matching. (arXiv:2305.17209v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21151;&#33021;&#27969;&#21305;&#37197;&#65288;FFM&#65289;&#30340;&#20989;&#25968;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#27010;&#29575;&#27979;&#24230;&#25554;&#20540;&#21644;&#23398;&#20064;&#24213;&#23618;&#20989;&#25968;&#31354;&#38388;&#19978;&#29983;&#25104;&#27979;&#24230;&#30340;&#21521;&#37327;&#22330;&#26469;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#12290;&#36825;&#31181;&#26080;&#38656;&#20284;&#28982;&#25110;&#27169;&#25311;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#20960;&#31181;&#20989;&#25968;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21151;&#33021;&#27969;&#21305;&#37197;&#65288;Functional Flow Matching, FFM&#65289;&#30340;&#20989;&#25968;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#26368;&#36817;&#24341;&#20837;&#30340;&#27969;&#21305;&#37197;&#65288;Flow Matching&#65289;&#30452;&#25509;&#25512;&#24191;&#21040;&#26080;&#38480;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#23450;&#20041;&#20102;&#19968;&#32452;&#27010;&#29575;&#27979;&#24230;&#36335;&#24452;&#65292;&#22312;&#22266;&#23450;&#30340;&#39640;&#26031;&#27979;&#24230;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65292;&#28982;&#21518;&#23398;&#20064;&#20989;&#25968;&#30340;&#24213;&#23618;&#31354;&#38388;&#19978;&#29983;&#25104;&#27492;&#27979;&#24230;&#36335;&#24452;&#30340;&#21521;&#37327;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20284;&#28982;&#25110;&#27169;&#25311;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#20989;&#25968;&#31354;&#38388;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#19981;&#20165;&#25552;&#20379;&#26500;&#24314;&#36825;&#31181;&#27169;&#22411;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36824;&#23545;&#25105;&#20204;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;FFM&#26041;&#27861;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#20960;&#31181;&#20989;&#25968;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose Functional Flow Matching (FFM), a function-space generative model that generalizes the recently-introduced Flow Matching model to operate directly in infinite-dimensional spaces. Our approach works by first defining a path of probability measures that interpolates between a fixed Gaussian measure and the data distribution, followed by learning a vector field on the underlying space of functions that generates this path of measures. Our method does not rely on likelihoods or simulations, making it well-suited to the function space setting. We provide both a theoretical framework for building such models and an empirical evaluation of our techniques. We demonstrate through experiments on synthetic and real-world benchmarks that our proposed FFM method outperforms several recently proposed function-space generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24189;&#28789;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;GBN&#65289;&#20013;&#30340;&#8220;&#24189;&#28789;&#22122;&#22768;&#8221;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;Ghost Noise Injection (GNI)&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36991;&#20813;&#23567;&#25209;&#37327;&#35757;&#32451;&#24102;&#26469;&#30340;&#35757;&#32451;-&#27979;&#35797;&#24046;&#24322;&#25928;&#24212;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#20379;&#26356;&#22909;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17205</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24189;&#28789;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Ghost Noise for Regularizing Deep Neural Networks. (arXiv:2305.17205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24189;&#28789;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;GBN&#65289;&#20013;&#30340;&#8220;&#24189;&#28789;&#22122;&#22768;&#8221;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;Ghost Noise Injection (GNI)&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36991;&#20813;&#23567;&#25209;&#37327;&#35757;&#32451;&#24102;&#26469;&#30340;&#35757;&#32451;-&#27979;&#35797;&#24046;&#24322;&#25928;&#24212;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#20379;&#26356;&#22909;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#31283;&#23450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#36807;&#31243;&#24182;&#25552;&#39640;&#27979;&#35797;&#24615;&#33021;&#12290;BN&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#21462;&#20915;&#20110;&#25209;&#37327;&#22823;&#23567;&#65292;&#26174;&#24335;&#20351;&#29992;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#20250;&#36890;&#36807;&#25209;&#37327;&#24402;&#19968;&#21270;&#25552;&#39640;&#27867;&#21270;&#24615;&#65292;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#37117;&#20855;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#24341;&#20837;&#30340;&#8220;&#24189;&#28789;&#22122;&#22768;&#8221;&#19982;&#24402;&#19968;&#21270;&#36827;&#34892;&#20998;&#31163;&#65292;&#24182;&#23450;&#37327;&#20998;&#26512;&#22122;&#22768;&#30340;&#20998;&#24067;&#21450;&#20854;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26469;&#30740;&#31350;GBN&#30340;&#26377;&#25928;&#24615;&#12290;&#21463;&#25105;&#20204;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#31216;&#20026;Ghost Noise Injection (GNI)&#65292;&#27169;&#20223;&#20102;GBN&#20013;&#30340;&#22122;&#22768;&#65292;&#32780;&#36991;&#20813;&#20102;&#23567;&#25209;&#37327;&#35757;&#32451;&#24102;&#26469;&#30340;&#26377;&#23475;&#30340;&#35757;&#32451;-&#27979;&#35797;&#24046;&#24322;&#25928;&#24212;&#12290;&#23454;&#39564;&#35777;&#26126;GNI&#21487;&#20197;&#27604;GBN&#25552;&#20379;&#26356;&#22909;&#30340;&#27867;&#21270;&#25928;&#30410;&#12290;Ghost Noise Injection&#22312;&#20854;&#20182;&#38750;&#22122;&#22768;&#35774;&#32622;&#20013;&#65292;&#20363;&#22914;&#23618;&#24402;&#19968;&#21270;&#32593;&#32476;&#20063;&#33021;&#22815;&#20135;&#29983;&#31215;&#26497;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Normalization (BN) is widely used to stabilize the optimization process and improve the test performance of deep neural networks. The regularization effect of BN depends on the batch size and explicitly using smaller batch sizes with Batch Normalization, a method known as Ghost Batch Normalization (GBN), has been found to improve generalization in many settings. We investigate the effectiveness of GBN by disentangling the induced "Ghost Noise" from normalization and quantitatively analyzing the distribution of noise as well as its impact on model performance. Inspired by our analysis, we propose a new regularization technique called Ghost Noise Injection (GNI) that imitates the noise in GBN without incurring the detrimental train-test discrepancy effects of small batch training. We experimentally show that GNI can provide a greater generalization benefit than GBN. Ghost Noise Injection can also be beneficial in otherwise non-noisy settings such as layer-normalized networks, provi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#20998;&#37327;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#29420;&#29305;&#24433;&#21709;&#25351;&#26631;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#32452;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992; LightGBM &#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#22312;&#27779;&#23572;&#29595;&#38144;&#21806;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17201</link><description>&lt;p&gt;
&#22522;&#20110; Trend &#21644; Seasonality &#20998;&#35299;&#21644; LightGBM &#30340;&#38144;&#21806;&#39044;&#27979;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Sales Forecasting using Trend and Seasonality Decomposition with LightGBM. (arXiv:2305.17201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#20998;&#37327;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#29420;&#29305;&#24433;&#21709;&#25351;&#26631;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#32452;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992; LightGBM &#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#22312;&#27779;&#23572;&#29595;&#38144;&#21806;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#27779;&#23572;&#29595;&#21644;&#20122;&#39532;&#36874;&#31561;&#22823;&#22411;&#38646;&#21806;&#21830;&#38144;&#21806;&#39044;&#27979;&#30340;&#38590;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#26681;&#25454;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#20998;&#37327;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#29420;&#29305;&#24433;&#21709;&#25351;&#26631;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#32452;&#65292;&#24182;&#37319;&#29992; LightGBM &#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#20998;&#32452;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;MAPE&#65288;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#65289;&#22312;&#27979;&#35797;&#38598;&#19978;&#21487;&#36798; 4.49%&#12290;
&lt;/p&gt;
&lt;p&gt;
Retail sales forecasting presents a significant challenge for large retailers such as Walmart and Amazon, due to the vast assortment of products, geographical location heterogeneity, seasonality, and external factors including weather, local economic conditions, and geopolitical events. Various methods have been employed to tackle this challenge, including traditional time series models, machine learning models, and neural network mechanisms, but the difficulty persists. Categorizing data into relevant groups has been shown to improve sales forecast accuracy as time series from different categories may exhibit distinct patterns. In this paper, we propose a new measure to indicate the unique impacts of the trend and seasonality components on a time series and suggest grouping time series based on this measure. We apply this approach to Walmart sales data from 01/29/2011 to 05/22/2016 and generate sales forecasts from 05/23/2016 to 06/19/2016. Our experiments show that the proposed strat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;MOMA-PPO&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#20132;&#20114;&#25968;&#25454;&#24182;&#20248;&#21270;&#26234;&#33021;&#20307;&#30340;&#25919;&#31574;&#65292;&#35299;&#20915;&#20102;&#31574;&#30053;&#19968;&#33268;&#24615;&#21644;&#31574;&#30053;&#24494;&#35843;&#20004;&#20010;&#21327;&#35843;&#38382;&#39064;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#32447;MARL&#22330;&#26223;&#20013;&#32988;&#36807;&#20027;&#27969;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.17198</link><description>&lt;p&gt;
&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21327;&#35843;&#38382;&#39064;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem. (arXiv:2305.17198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17198
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;MOMA-PPO&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#20132;&#20114;&#25968;&#25454;&#24182;&#20248;&#21270;&#26234;&#33021;&#20307;&#30340;&#25919;&#31574;&#65292;&#35299;&#20915;&#20102;&#31574;&#30053;&#19968;&#33268;&#24615;&#21644;&#31574;&#30053;&#24494;&#35843;&#20004;&#20010;&#21327;&#35843;&#38382;&#39064;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#32447;MARL&#22330;&#26223;&#20013;&#32988;&#36807;&#20027;&#27969;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22810;&#20010;&#26234;&#33021;&#20307;&#36827;&#34892;&#21327;&#35843;&#26159;&#19968;&#39033;&#37325;&#35201;&#38382;&#39064;&#65292;&#20855;&#26377;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#21338;&#24328;&#35770;&#12289;&#32463;&#27982;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26159;&#22312;&#32447;&#30340;&#65292;&#22240;&#27492;&#22312;&#25910;&#38598;&#26032;&#30340;&#20132;&#20114;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#25110;&#21361;&#38505;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#12290;&#34429;&#28982;&#36825;&#20123;&#31639;&#27861;&#24212;&#35813;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#65292;&#20294;&#36825;&#26679;&#20570;&#20250;&#24341;&#36215;&#31163;&#32447;&#21327;&#35843;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#24418;&#24335;&#21270;&#20102;&#31574;&#30053;&#19968;&#33268;&#24615;&#65288;SA&#65289;&#21644;&#31574;&#30053;&#24494;&#35843;&#65288;SFT&#65289;&#20004;&#20010;&#21327;&#35843;&#38382;&#39064;&#65292;&#36825;&#26159;&#24403;&#21069;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#21512;&#25104;&#20132;&#20114;&#25968;&#25454;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#24494;&#35843;&#31574;&#30053;&#30340;&#21516;&#26102;&#25910;&#25947;&#20110;&#19968;&#20010;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;Model-based Offline Multi-Agent Proximal Policy Optimization&#65288;MOMA-PPO&#65289;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#32447;MARL&#22330;&#26223;&#20013;&#32988;&#36807;&#20027;&#27969;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training multiple agents to coordinate is an important problem with applications in robotics, game theory, economics, and social sciences. However, most existing Multi-Agent Reinforcement Learning (MARL) methods are online and thus impractical for real-world applications in which collecting new interactions is costly or dangerous. While these algorithms should leverage offline data when available, doing so gives rise to the offline coordination problem. Specifically, we identify and formalize the strategy agreement (SA) and the strategy fine-tuning (SFT) challenges, two coordination issues at which current offline MARL algorithms fail. To address this setback, we propose a simple model-based approach that generates synthetic interaction data and enables agents to converge on a strategy while fine-tuning their policies accordingly. Our resulting method, Model-based Offline Multi-Agent Proximal Policy Optimization (MOMA-PPO), outperforms the prevalent learning methods in challenging offl
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#23545;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#36827;&#34892;&#20998;&#26512;&#20855;&#26377;&#21457;&#29616;&#26032;&#29983;&#29289;&#23398;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#21152;&#36895;&#25506;&#32034;&#20122;&#32454;&#32990;&#22823;&#20998;&#23376;&#21644;&#32454;&#32990;&#22120;&#20998;&#23376;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.17193</link><description>&lt;p&gt;
&#22522;&#20110;AI&#30340;&#36229;&#20998;&#36776;&#26174;&#24494;&#38236;&#20998;&#26512;&#65306;&#22312;&#27809;&#26377;&#22522;&#20934;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#29983;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
AI-based analysis of super-resolution microscopy: Biological discovery in the absence of ground truth. (arXiv:2305.17193v1 [q-bio.SC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17193
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#23545;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#36827;&#34892;&#20998;&#26512;&#20855;&#26377;&#21457;&#29616;&#26032;&#29983;&#29289;&#23398;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#21152;&#36895;&#25506;&#32034;&#20122;&#32454;&#32990;&#22823;&#20998;&#23376;&#21644;&#32454;&#32990;&#22120;&#20998;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20998;&#36776;&#26174;&#24494;&#38236;&#30340;&#32435;&#31859;&#32423;&#20998;&#36776;&#29575;&#29616;&#24050;&#20351;&#33639;&#20809;&#20998;&#23376;&#23450;&#20301;&#24037;&#20855;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#25972;&#20010;&#32454;&#32990;&#32467;&#26500;&#29983;&#29289;&#23398;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#25968;&#25454;&#20998;&#26512;&#20855;&#26377;&#21457;&#29616;&#26032;&#29983;&#29289;&#23398;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#32780;&#26032;&#29983;&#29289;&#23398;&#26412;&#36523;&#27809;&#26377;&#34987;&#21457;&#29616;&#36807;&#65292;&#20063;&#27809;&#26377;&#22522;&#20934;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#22312;&#36229;&#20998;&#36776;&#26174;&#24494;&#38236;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#20854;&#28508;&#21147;&#65292;&#20197;&#23454;&#29616;&#23545;&#20122;&#32454;&#32990;&#22823;&#20998;&#23376;&#21644;&#32454;&#32990;&#22120;&#20998;&#23376;&#32467;&#26500;&#30340;&#21152;&#36895;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The nanoscale resolution of super-resolution microscopy has now enabled the use of fluorescent based molecular localization tools to study whole cell structural biology. Machine learning based analysis of super-resolution data offers tremendous potential for discovery of new biology, that by definition is not known and lacks ground truth. Herein, we describe the application of weakly supervised learning paradigms to super-resolution microscopy and its potential to enable the accelerated exploration of the molecular architecture of subcellular macromolecules and organelles.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MT-SLVR &#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#19981;&#21464;&#24615;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17191</link><description>&lt;p&gt;
MT-SLVR: &#22810;&#20219;&#21153;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#21464;&#25442;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations. (arXiv:2305.17191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17191
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MT-SLVR &#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#19981;&#21464;&#24615;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22240;&#20854;&#33021;&#20174;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21019;&#24314;&#39640;&#36136;&#37327;&#34920;&#31034;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#24378;&#22823;&#30340;&#29305;&#24449;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#25552;&#20379;&#20102;&#25193;&#20805;&#19981;&#21464;&#24615;&#65292;&#36825;&#36890;&#24120;&#26159;&#19968;&#31181;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#20174;&#20808;&#39564;&#19978;&#19981;&#30693;&#36947;&#25152;&#38656;&#30340;&#19981;&#21464;&#24615;&#25968;&#37327;&#21644;&#31867;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#33258;&#30417;&#30563;&#26694;&#26550;(MT-SLVR)&#65292;&#20197;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#21464;&#20307;&#21644;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#25552;&#20379;&#20102;&#24378;&#22823;&#21644;&#28789;&#27963;&#30340;&#29305;&#24449;&#65292;&#21487;&#20351;&#22810;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#21463;&#30410;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#21508;&#31181;&#38899;&#39057;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#22343;&#26377;&#25913;&#21892;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive self-supervised learning has gained attention for its ability to create high-quality representations from large unlabelled data sets. A key reason that these powerful features enable data-efficient learning of downstream tasks is that they provide augmentation invariance, which is often a useful inductive bias. However, the amount and type of invariances preferred is not known apriori, and varies across different downstream tasks. We therefore propose a multi-task self-supervised framework (MT-SLVR) that learns both variant and invariant features in a parameter-efficient manner. Our multi-task representation provides a strong and flexible feature that benefits diverse downstream tasks. We evaluate our approach on few-shot classification tasks drawn from a variety of audio domains and demonstrate improved classification performance on all of them
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#20998;&#27573;&#20223;&#23556;&#25805;&#20316;&#20195;&#26367;&#20256;&#32479;&#20056;&#27861;&#30340;&#39640;&#25928;Transformer&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26356;&#25913;&#35757;&#32451;&#36229;&#21442;&#25968;&#21363;&#21487;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#25104;&#21151;&#23454;&#29616;&#35757;&#32451;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25152;&#26377;&#20056;&#27861;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.17190</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27573;&#20223;&#23556;&#25805;&#20316;&#23454;&#29616;&#39640;&#25928;Transformer&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Hardware-Efficient Transformer Training via Piecewise Affine Operations. (arXiv:2305.17190v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#20998;&#27573;&#20223;&#23556;&#25805;&#20316;&#20195;&#26367;&#20256;&#32479;&#20056;&#27861;&#30340;&#39640;&#25928;Transformer&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26356;&#25913;&#35757;&#32451;&#36229;&#21442;&#25968;&#21363;&#21487;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#25104;&#21151;&#23454;&#29616;&#35757;&#32451;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25152;&#26377;&#20056;&#27861;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#65292;&#22823;&#22810;&#25968;&#35745;&#31639;&#25104;&#26412;&#37117;&#26159;&#30001;&#20056;&#27861;&#25152;&#36129;&#29486;&#30340;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#20943;&#23569;&#30001;&#27492;&#24102;&#26469;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#21463;Mogami&#65288;2020&#65289;&#21551;&#21457;&#65292;&#29992;&#19968;&#31181;&#24265;&#20215;&#30340;&#20998;&#27573;&#20223;&#23556;&#36924;&#36817;&#26367;&#25442;&#20056;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#28014;&#28857;&#25968;&#30340;&#20301;&#34920;&#31034;&#20316;&#20026;&#25972;&#25968;&#30456;&#21152;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#20351;&#29992;&#25152;&#24471;&#21040;&#30340;&#20462;&#25913;&#21518;&#30340;&#30697;&#38453;&#20056;&#27861;&#35757;&#32451;Transformer&#65292;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#24433;&#21709;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26356;&#25913;&#35757;&#32451;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#29992;&#20998;&#27573;&#20223;&#23556;&#26367;&#25442;&#20102;&#32593;&#32476;&#20013;&#30340;&#25152;&#26377;&#38750;&#32447;&#24615;&#65292;&#20351;&#23427;&#20204;&#22312;&#36755;&#20837;&#21644;&#26435;&#37325;&#26041;&#38754;&#37117;&#25104;&#20026;&#23436;&#20840;&#32852;&#21512;&#30340;&#20998;&#27573;&#20223;&#23556;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#28040;&#38500;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25152;&#26377;&#20056;&#27861;&#25805;&#20316;&#65292;&#21253;&#25324;&#21069;&#21521;&#20256;&#25773;&#12289;&#21453;&#21521;&#20256;&#25773;&#21644;&#20248;&#21270;&#22120;&#26356;&#26032;&#30340;&#25805;&#20316;&#65292;&#23637;&#31034;&#20102;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#39318;&#27425;&#25104;&#21151;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiplications are responsible for most of the computational cost involved in neural network training and inference. Recent research has thus looked for ways to reduce the cost associated with them. Inspired by Mogami (2020), we replace multiplication with a cheap piecewise affine approximation that is achieved by adding the bit representation of the floating point numbers together as integers. We show that transformers can be trained with the resulting modified matrix multiplications on both vision and language tasks with little to no performance impact, and without changes to the training hyperparameters. We further replace all non-linearities in the networks making them fully and jointly piecewise affine in both inputs and weights. Finally, we show that we can eliminate all multiplications in the entire training process, including operations in the forward pass, backward pass and optimizer update, demonstrating the first successful training of modern neural network architectures in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#21521;&#37327;&#20540;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#30340;&#23436;&#25972;&#35823;&#24046;&#20998;&#26512;&#65292;&#21253;&#25324;&#22312;&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#19979;&#21521;&#37327;&#20540;RF&#20272;&#35745;&#22120;&#30340;&#24378;&#19968;&#33268;&#24615;&#21644;&#22312;&#33391;&#22909;&#35268;&#23450;&#30340;&#24773;&#20917;&#19979;&#26497;&#23567;&#21270;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.17170</link><description>&lt;p&gt;
&#21521;&#37327;&#20540;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#30340;&#35823;&#24046;&#30028;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Error Bounds for Learning with Vector-Valued Random Features. (arXiv:2305.17170v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#21521;&#37327;&#20540;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#30340;&#23436;&#25972;&#35823;&#24046;&#20998;&#26512;&#65292;&#21253;&#25324;&#22312;&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#19979;&#21521;&#37327;&#20540;RF&#20272;&#35745;&#22120;&#30340;&#24378;&#19968;&#33268;&#24615;&#21644;&#22312;&#33391;&#22909;&#35268;&#23450;&#30340;&#24773;&#20917;&#19979;&#26497;&#23567;&#21270;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#21521;&#37327;&#20540;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#30340;&#23436;&#25972;&#35823;&#24046;&#20998;&#26512;&#12290;&#35813;&#29702;&#35770;&#26159;&#38024;&#23545;&#23436;&#20840;&#36890;&#29992;&#30340;&#26080;&#38480;&#32500;&#24230;&#36755;&#20837;-&#36755;&#20986;&#35774;&#23450;&#20013;&#30340;RF Ridge&#22238;&#24402;&#32780;&#24320;&#21457;&#30340;&#65292;&#20294;&#20173;&#36866;&#29992;&#20110;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#26377;&#38480;&#32500;&#24230;&#20998;&#26512;&#12290;&#19982;&#25991;&#29486;&#20013;&#20854;&#20182;&#31867;&#20284;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#24213;&#23618;&#39118;&#38505;&#20989;&#25968;&#30340;&#30452;&#25509;&#20998;&#26512;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#22522;&#20110;&#38543;&#26426;&#30697;&#38453;&#30340;&#26174;&#24335;RF Ridge&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#20844;&#24335;&#30340;&#20351;&#29992;&#12290;&#36825;&#28040;&#38500;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#20013;&#30340;&#27987;&#24230;&#32467;&#26524;&#25110;&#20854;&#23545;&#38543;&#26426;&#31639;&#23376;&#30340;&#25512;&#24191;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#24314;&#31435;&#30340;&#20027;&#35201;&#32467;&#26524;&#21253;&#25324;&#22312;&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#19979;&#21521;&#37327;&#20540;RF&#20272;&#35745;&#22120;&#30340;&#24378;&#19968;&#33268;&#24615;&#21644;&#22312;&#33391;&#22909;&#35268;&#23450;&#30340;&#24773;&#20917;&#19979;&#26497;&#23567;&#21270;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#23454;&#29616;&#36825;&#20123;&#25910;&#25947;&#36895;&#29575;&#25152;&#38656;&#30340;&#21442;&#25968;&#22797;&#26434;&#24230;(&#38543;&#26426;&#29305;&#24449;&#25968;&#37327;)&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;(&#26631;&#35760;&#25968;&#25454;&#25968;&#37327;)&#19982;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive error analysis of learning with vector-valued random features (RF). The theory is developed for RF ridge regression in a fully general infinite-dimensional input-output setting, but nonetheless applies to and improves existing finite-dimensional analyses. In contrast to comparable work in the literature, the approach proposed here relies on a direct analysis of the underlying risk functional and completely avoids the explicit RF ridge regression solution formula in terms of random matrices. This removes the need for concentration results in random matrix theory or their generalizations to random operators. The main results established in this paper include strong consistency of vector-valued RF estimators under model misspecification and minimax optimal convergence rates in the well-specified setting. The parameter complexity (number of random features) and sample complexity (number of labeled data) required to achieve such rates are comparable with 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#21305;&#37197;&#25216;&#26415;&#30340;&#21518;&#39564;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#25512;&#29702;&#65292;&#36890;&#36807;&#25552;&#20379;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#24341;&#21147;&#27874;&#25512;&#26029;&#19978;&#21462;&#24471;&#20102;&#21487;&#27604;&#31163;&#25955;&#27969;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17161</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#27169;&#25311;&#25512;&#29702;&#30340;&#27969;&#21305;&#37197;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flow Matching for Scalable Simulation-Based Inference. (arXiv:2305.17161v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17161
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#21305;&#37197;&#25216;&#26415;&#30340;&#21518;&#39564;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#25512;&#29702;&#65292;&#36890;&#36807;&#25552;&#20379;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#24341;&#21147;&#27874;&#25512;&#26029;&#19978;&#21462;&#24471;&#20102;&#21487;&#27604;&#31163;&#25955;&#27969;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31163;&#25955;&#35268;&#33539;&#21270;&#27969;&#30340;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#26041;&#27861;&#24050;&#25104;&#20026;&#27169;&#25311;&#25512;&#29702;&#65288;SBI&#65289;&#30340;&#25104;&#29087;&#24037;&#20855;&#65292;&#20294;&#23558;&#20854;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24314;&#31435;&#22312;&#26368;&#36817;&#29983;&#25104;&#24314;&#27169;&#26041;&#38754;&#30340;&#36827;&#23637;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#27969;&#21305;&#37197;&#30340;&#21518;&#39564;&#20272;&#35745;&#65288;FMPE&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#35268;&#33539;&#21270;&#27969;&#36827;&#34892;SBI&#30340;&#25216;&#26415;&#12290;&#19982;&#31163;&#25955;&#27969;&#19981;&#21516;&#65292;&#20687;&#25193;&#25955;&#27169;&#22411;&#19968;&#26679;&#65292;&#27969;&#21305;&#37197;&#20801;&#35768;&#26080;&#32422;&#26463;&#30340;&#26550;&#26500;&#65292;&#25552;&#20379;&#20102;&#29992;&#20110;&#22797;&#26434;&#25968;&#25454;&#27169;&#24577;&#30340;&#22686;&#24378;&#28789;&#27963;&#24615;&#12290;&#22240;&#27492;&#65292;&#27969;&#21305;&#37197;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#23494;&#24230;&#35780;&#20272;&#12289;&#24555;&#36895;&#30340;&#35757;&#32451;&#21644;&#26080;&#32541;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;SBI&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FMPE&#22312;SBI&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#28982;&#21518;&#22312;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31185;&#23398;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#25913;&#36827;&#30340;&#21487;&#25193;&#23637;&#24615;&#65306;&#23545;&#20110;&#24341;&#21147;&#27874;&#25512;&#26029;&#65292;FMPE&#32988;&#36807;&#22522;&#20110;&#30456;&#20284;&#31163;&#25955;&#27969;&#30340;&#26041;&#27861;&#65292;&#22312;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;30%&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#26377;&#35828;&#26381;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures--making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30% with substanti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ETSE&#30340;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#32974;&#20799;&#20581;&#24247;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#37319;&#29992;&#22810;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;7&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17156</link><description>&lt;p&gt;
&#19968;&#31181;&#25913;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36229;&#21442;&#25968;&#35843;&#25972;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#32974;&#20799;&#20581;&#24247;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Improved Model Ensembled of Different Hyper-parameter Tuned Machine Learning Algorithms for Fetal Health Prediction. (arXiv:2305.17156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ETSE&#30340;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#32974;&#20799;&#20581;&#24247;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#37319;&#29992;&#22810;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;7&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32974;&#20799;&#20581;&#24247;&#23545;&#20110;&#23381;&#26399;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20250;&#24433;&#21709;&#21040;&#27597;&#20146;&#21644;&#32974;&#20799;&#30340;&#20581;&#24247;&#12290;&#30417;&#27979;&#32974;&#20799;&#20581;&#24247;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#36895;&#24230;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ETSE&#30340;&#26426;&#22120;&#23398;&#20064;&#23450;&#21046;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;ExtraTrees&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#32974;&#20799;&#20581;&#24247;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#31163;&#32676;&#20540;&#21076;&#38500;&#12289;&#32570;&#22833;&#20540;&#34917;&#20840;&#12289;&#25968;&#25454;&#26631;&#20934;&#21270;&#21644;&#25968;&#25454;&#25277;&#26679;&#31561;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#23454;&#29616;&#20102;7&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;XGBoost&#12289;LGBM&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;ExtraTrees&#21644;K&#36817;&#37051;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#36807;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#37319;&#29992;&#36229;&#21442;&#25968;&#35843;&#25972;&#36827;&#34892;&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fetal health is a critical concern during pregnancy as it can impact the well-being of both the mother and the baby. Regular monitoring and timely interventions are necessary to ensure the best possible outcomes. While there are various methods to monitor fetal health in the mother's womb, the use of artificial intelligence (AI) can improve the accuracy, efficiency, and speed of diagnosis. In this study, we propose a robust ensemble model called ensemble of tuned Support Vector Machine and ExtraTrees (ETSE) for predicting fetal health. Initially, we employed various data preprocessing techniques such as outlier rejection, missing value imputation, data standardization, and data sampling. Then, seven machine learning (ML) classifiers including Support Vector Machine (SVM), XGBoost (XGB), Light Gradient Boosting Machine (LGBM), Decision Tree (DT), Random Forest (RF), ExtraTrees (ET), and K-Neighbors were implemented. These models were evaluated and then optimized by hyperparameter tuning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#25968;&#20540;&#26041;&#26696;&#31283;&#23450;&#24615;&#29305;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21152;&#20837;&#20102;&#30828;&#24615;&#32422;&#26463;&#26469;&#20445;&#35777;&#20854;&#26435;&#37325;&#31283;&#23450;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#38271;&#26399;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17155</link><description>&lt;p&gt;
&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#38271;&#26399;&#39044;&#27979;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stability of implicit neural networks for long-term forecasting in dynamical systems. (arXiv:2305.17155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#25968;&#20540;&#26041;&#26696;&#31283;&#23450;&#24615;&#29305;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21152;&#20837;&#20102;&#30828;&#24615;&#32422;&#26463;&#26469;&#20445;&#35777;&#20854;&#26435;&#37325;&#31283;&#23450;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#38271;&#26399;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#29289;&#29702;&#20449;&#21495;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#30740;&#31350;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#20026;&#20102;&#35268;&#36991;&#20256;&#32479;&#27714;&#35299;&#22120;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#20204;&#37117;&#22522;&#20110;&#33258;&#22238;&#24402;&#26041;&#27861;&#24182;&#23637;&#31034;&#20986;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#21463;&#38544;&#24335;&#25968;&#20540;&#26041;&#26696;&#30340;&#31283;&#23450;&#24615;&#29305;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31283;&#23450;&#30340;&#33258;&#22238;&#24402;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#26681;&#25454;&#25968;&#20540;&#26041;&#26696;&#30340;&#31283;&#23450;&#24615;&#23450;&#20041;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#29702;&#35770;&#26469;&#20445;&#35777;&#32593;&#32476;&#39044;&#27979;&#30340;&#31283;&#23450;&#24615;&#12290;&#23427;&#23548;&#33268;&#25105;&#20204;&#23545;&#20854;&#26435;&#37325;&#28155;&#21152;&#20102;&#30828;&#24615;&#32422;&#26463;&#65292;&#24182;&#22312;&#28508;&#31354;&#38388;&#20013;&#20256;&#25773;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#31283;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#36755;&#36816;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38271;&#26399;&#39044;&#27979;&#19978;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting physical signals in long time range is among the most challenging tasks in Partial Differential Equations (PDEs) research. To circumvent limitations of traditional solvers, many different Deep Learning methods have been proposed. They are all based on auto-regressive methods and exhibit stability issues. Drawing inspiration from the stability property of implicit numerical schemes, we introduce a stable auto-regressive implicit neural network. We develop a theory based on the stability definition of schemes to ensure the stability in forecasting of this network. It leads us to introduce hard constraints on its weights and propagate the dynamics in the latent space. Our experimental results validate our stability property, and show improved results at long-term forecasting for two transports PDEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#32593;&#32476;&#34920;&#31034;&#20013;&#27010;&#24565;&#31354;&#38388;&#30340;&#20984;&#24615;&#23545;&#27867;&#21270;&#33021;&#21147;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#35266;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36817;&#20284;&#20984;&#24615;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2305.17154</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#32593;&#32476;&#34920;&#31034;&#20013;&#27010;&#24565;&#31354;&#38388;&#30340;&#20984;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On convex conceptual regions in deep network representations. (arXiv:2305.17154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#32593;&#32476;&#34920;&#31034;&#20013;&#27010;&#24565;&#31354;&#38388;&#30340;&#20984;&#24615;&#23545;&#27867;&#21270;&#33021;&#21147;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#35266;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36817;&#20284;&#20984;&#24615;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#23545;&#40784;&#30340;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;Gardenfors&#30340;&#27010;&#24565;&#31354;&#38388;&#26159;&#29702;&#35299;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#20010;&#37325;&#35201;&#26694;&#26550;&#12290;&#22312;&#27010;&#24565;&#31354;&#38388;&#20013;&#65292;&#23545;&#35937;&#21306;&#22495;&#30340;&#20984;&#24615;&#34987;&#35748;&#20026;&#26159;&#20419;&#36827;&#27867;&#21270;&#33021;&#21147;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#35266;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#26426;&#21046;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#35265;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#23398;&#20064;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#27010;&#24565;&#21306;&#22495;&#30340;&#20984;&#24615;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#32452;&#29992;&#20110;&#27979;&#37327;&#37319;&#26679;&#25968;&#25454;&#20013;&#20984;&#24615;&#30340;&#24037;&#20855;&#65292;&#24182;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#23618;&#34920;&#31034;&#20013;&#30340;&#20984;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20984;&#24615;&#23545;&#20110;&#22522;&#26412;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26159;&#31283;&#20581;&#30340;&#65292;&#22240;&#27492;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#36136;&#37327;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#36817;&#20284;&#20984;&#24615;&#22312;&#31070;&#32463;&#34920;&#31034;&#20013;&#24191;&#27867;&#23384;&#22312;&#20110;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#65292;&#21253;&#25324;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#20154;&#31867;&#27963;&#21160;&#12289;&#25991;&#26412;&#21644;&#33041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current study of human-machine alignment aims at understanding the geometry of latent spaces and the correspondence to human representations. G\"ardenfors' conceptual spaces is a prominent framework for understanding human representations. Convexity of object regions in conceptual spaces is argued to promote generalizability, few-shot learning, and intersubject alignment. Based on these insights, we investigate the notion of convexity of concept regions in machine-learned latent spaces. We develop a set of tools for measuring convexity in sampled data and evaluate emergent convexity in layered representations of state-of-the-art deep networks. We show that convexity is robust to basic re-parametrization, hence, meaningful as a quality of machine-learned latent spaces. We find that approximate convexity is pervasive in neural representations in multiple application domains, including models of images, audio, human activity, text, and brain data. We measure convexity separately for l
&lt;/p&gt;</description></item><item><title>mldr.resampling&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;11&#31181;&#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#26041;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#65292;&#26088;&#22312;&#24212;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17152</link><description>&lt;p&gt;
mldr.resampling: &#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#31639;&#27861;&#26377;&#25928;&#30340;&#21442;&#32771;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
mldr.resampling: Efficient Reference Implementations of Multilabel Resampling Algorithms. (arXiv:2305.17152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17152
&lt;/p&gt;
&lt;p&gt;
mldr.resampling&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;11&#31181;&#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#26041;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#65292;&#26088;&#22312;&#24212;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#37319;&#26679;&#31639;&#27861;&#26159;&#24212;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#24773;&#20917;&#30340;&#26377;&#29992;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#24517;&#39035;&#22788;&#29702;&#22810;&#26631;&#31614;&#25968;&#25454;&#20013;&#30340;&#22855;&#24322;&#24615;&#65292;&#20363;&#22914;&#21516;&#19968;&#23454;&#20363;&#20013;&#39057;&#32321;&#21644;&#19981;&#39057;&#32321;&#26631;&#31614;&#30340;&#20986;&#29616;&#12290;&#36825;&#31687;&#21407;&#21019;&#36719;&#20214;&#21457;&#34920;&#20171;&#32461;&#20102; mldr.resampling&#65292;&#36825;&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;&#20102;11&#31181;&#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#26041;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#65292;&#24378;&#35843;&#25928;&#29575;&#65292;&#22240;&#20026;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#32791;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resampling algorithms are a useful approach to deal with imbalanced learning in multilabel scenarios. These methods have to deal with singularities in the multilabel data, such as the occurrence of frequent and infrequent labels in the same instance. Implementations of these methods are sometimes limited to the pseudocode provided by their authors in a paper. This Original Software Publication presents mldr.resampling, a software package that provides reference implementations for eleven multilabel resampling methods, with an emphasis on efficiency since these algorithms are usually time-consuming.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35786;&#26029;&#26102;&#31354;&#21464;&#25442;&#22120;&#65288;DFStrans&#65289;&#65292;&#20854;&#21033;&#29992;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#26102;&#31354;&#20381;&#36182;&#24615;&#21457;&#29616;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#22797;&#26434;&#26102;&#31354;&#20381;&#36182;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#21462;&#21487;&#25805;&#20316;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.17149</link><description>&lt;p&gt;
&#20855;&#26377;&#31934;&#30830;&#32534;&#30721;&#30340;&#35786;&#26029;&#26102;&#31354;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diagnostic Spatio-temporal Transformer with Faithful Encoding. (arXiv:2305.17149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35786;&#26029;&#26102;&#31354;&#21464;&#25442;&#22120;&#65288;DFStrans&#65289;&#65292;&#20854;&#21033;&#29992;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#26102;&#31354;&#20381;&#36182;&#24615;&#21457;&#29616;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#22797;&#26434;&#26102;&#31354;&#20381;&#36182;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#21462;&#21487;&#25805;&#20316;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#24403;&#22522;&#26412;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20855;&#26377;&#22797;&#26434;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#26102;&#30340;&#24322;&#24120;&#35786;&#26029;&#20219;&#21153;&#12290;&#20854;&#20013;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#26159;&#20174;&#25551;&#36848;&#26102;&#38388;&#21644;&#31354;&#38388;&#25351;&#25968;&#20043;&#38388;&#39640;&#38454;&#20132;&#20114;&#30340;&#20381;&#36182;&#24352;&#37327;&#20013;&#25552;&#21462;&#21487;&#25805;&#20316;&#35265;&#35299;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#30417;&#30563;&#20381;&#36182;&#21457;&#29616;&#65292;&#20854;&#20013;&#26102;&#31354;&#20381;&#36182;&#24615;&#34987;&#20316;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#21103;&#20135;&#21697;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;ST&#21464;&#25442;&#22120;&#20013;&#20351;&#29992;&#30340;&#26102;&#38388;&#20301;&#32622;&#32534;&#30721;&#22312;&#25429;&#25417;&#26356;&#39640;&#39057;&#29575;&#65288;&#30701;&#26102;&#38388;&#23610;&#24230;&#65289;&#26041;&#38754;&#23384;&#22312;&#20005;&#37325;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#29702;&#35770;&#20445;&#35777;&#30340;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#21457;&#29616;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#26041;&#21521;&#19978;&#25552;&#20379;&#26131;&#20110;&#28040;&#21270;&#30340;&#35786;&#26029;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;DFStrans&#65288;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#35786;&#26029;&#26102;&#31354;&#21464;&#25442;&#22120;&#65289;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the task of anomaly diagnosis when the underlying data generation process has a complex spatio-temporal (ST) dependency. The key technical challenge is to extract actionable insights from the dependency tensor characterizing high-order interactions among temporal and spatial indices. We formalize the problem as supervised dependency discovery, where the ST dependency is learned as a side product of multivariate time-series classification. We show that temporal positional encoding used in existing ST transformer works has a serious limitation in capturing higher frequencies (short time scales). We propose a new positional encoding with a theoretical guarantee, based on discrete Fourier transform. We also propose a new ST dependency discovery framework, which can provide readily consumable diagnostic information in both spatial and temporal directions. Finally, we demonstrate the utility of the proposed model, DFStrans (Diagnostic Fourier-based Spatio-temporal Transf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20445;&#25252;&#20010;&#20154;&#25935;&#24863;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#39640;&#25928;&#20302;&#32500;&#21512;&#25104;&#25968;&#25454;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;Wasserstein&#36317;&#31163;&#26041;&#38754;&#20855;&#26377;&#25928;&#29992;&#20445;&#35777;&#65307;&#19982;&#26631;&#20934;&#25200;&#21160;&#20998;&#26512;&#19981;&#21516;&#65292;&#20351;&#29992;&#31169;&#26377;&#20027;&#25104;&#20998;&#20998;&#26512;&#36807;&#31243;&#36991;&#20813;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.17148</link><description>&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#20302;&#32500;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Differentially private low-dimensional representation of high-dimensional data. (arXiv:2305.17148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20445;&#25252;&#20010;&#20154;&#25935;&#24863;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#39640;&#25928;&#20302;&#32500;&#21512;&#25104;&#25968;&#25454;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;Wasserstein&#36317;&#31163;&#26041;&#38754;&#20855;&#26377;&#25928;&#29992;&#20445;&#35777;&#65307;&#19982;&#26631;&#20934;&#25200;&#21160;&#20998;&#26512;&#19981;&#21516;&#65292;&#20351;&#29992;&#31169;&#26377;&#20027;&#25104;&#20998;&#20998;&#26512;&#36807;&#31243;&#36991;&#20813;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#20010;&#20154;&#25935;&#24863;&#20449;&#24687;&#30340;&#21516;&#26102;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#22788;&#20110;&#39640;&#32500;&#31354;&#38388;&#20013;&#26102;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#20250;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#39640;&#25928;&#22320;&#29983;&#25104;&#20302;&#32500;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#22312;Wasserstein&#36317;&#31163;&#26041;&#38754;&#20855;&#26377;&#25928;&#29992;&#20445;&#35777;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#26159;&#20351;&#29992;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#31934;&#24230;&#30028;&#38480;&#30340;&#31169;&#26377;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#36807;&#31243;&#65292;&#20174;&#32780;&#35268;&#36991;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#19982;&#20351;&#29992;Davis-Kahan&#23450;&#29702;&#36827;&#34892;&#26631;&#20934;&#25200;&#21160;&#20998;&#26512;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31169;&#26377;PCA&#20998;&#26512;&#19981;&#38656;&#35201;&#20551;&#35774;&#26679;&#26412;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35889;&#38388;&#38553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private synthetic data provide a powerful mechanism to enable data analysis while protecting sensitive information about individuals. However, when the data lie in a high-dimensional space, the accuracy of the synthetic data suffers from the curse of dimensionality. In this paper, we propose a differentially private algorithm to generate low-dimensional synthetic data efficiently from a high-dimensional dataset with a utility guarantee with respect to the Wasserstein distance. A key step of our algorithm is a private principal component analysis (PCA) procedure with a near-optimal accuracy bound that circumvents the curse of dimensionality. Different from the standard perturbation analysis using the Davis-Kahan theorem, our analysis of private PCA works without assuming the spectral gap for the sample covariance matrix.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.17147</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24322;&#36136;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20351;&#24471;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23581;&#35797;&#23558;&#20854;&#19982;&#19968;&#31181;&#21516;&#36136;&#30340;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#24182;&#38656;&#35201;&#20154;&#31867;&#39564;&#35777;&#65292;&#20294;&#32570;&#20047;&#23545;&#23545;&#40784;&#25152;&#38656;&#26041;&#38754;&#21644;&#28145;&#24230;&#30340;&#20849;&#35782;&#20197;&#21450;&#36896;&#25104;&#30340;&#20154;&#31867;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#65288;1&#65289;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#21333;&#20010;&#20154;&#31867;&#20559;&#35265;&#65292;&#24182;&#19988;&#65288;2&#65289;&#20801;&#35768;&#35780;&#20272;&#38024;&#23545;&#21508;&#31181;&#30446;&#26631;&#20540;&#30340;&#24322;&#36136;&#20195;&#29702;&#20154;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#23427;&#20195;&#34920;&#20102;&#20195;&#29702;&#20154;&#25191;&#34892;&#26368;&#33021;&#28385;&#36275;&#30446;&#26631;&#20215;&#20540;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#37327;&#21270;&#26159;&#36890;&#36807;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#30340;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#36827;&#34892;&#30340;&#65292;&#35813;&#26694;&#26550;&#23558;&#20215;&#20540;&#31354;&#38388;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65292;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#20010;&#27169;&#22411;&#30340;&#20215;&#20540;&#21512;&#29702;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;A2EHV&#26041;&#27861;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25628;&#32034;&#26041;&#27861;OpenTau&#65292;&#37319;&#29992;&#26641;&#24418;&#31243;&#24207;&#20998;&#35299;&#25216;&#26415;&#21644;&#31867;&#22411;&#22635;&#20805;&#24494;&#35843;&#26041;&#27861;&#35299;&#20915;&#31867;&#22411;&#39044;&#27979;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;&#22312;TypeScript&#31867;&#22411;&#39044;&#27979;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#22312;&#31867;&#22411;&#26816;&#26597;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24179;&#22343;&#31934;&#24230;&#20026;0.707&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.17145</link><description>&lt;p&gt;
&#31243;&#24207;&#20998;&#35299;&#19982;&#31867;&#22411;&#22635;&#20805;&#35757;&#32451;&#30340;&#31867;&#22411;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Type Prediction With Program Decomposition and Fill-in-the-Type Training. (arXiv:2305.17145v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17145
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25628;&#32034;&#26041;&#27861;OpenTau&#65292;&#37319;&#29992;&#26641;&#24418;&#31243;&#24207;&#20998;&#35299;&#25216;&#26415;&#21644;&#31867;&#22411;&#22635;&#20805;&#24494;&#35843;&#26041;&#27861;&#35299;&#20915;&#31867;&#22411;&#39044;&#27979;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;&#22312;TypeScript&#31867;&#22411;&#39044;&#27979;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#22312;&#31867;&#22411;&#26816;&#26597;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24179;&#22343;&#31934;&#24230;&#20026;0.707&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TypeScript&#21644;Python&#26159;&#25903;&#25345;&#21487;&#36873;&#31867;&#22411;&#27880;&#37322;&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#24456;&#26377;&#29992;&#20294;&#24341;&#20837;&#21644;&#32500;&#25252;&#36215;&#26469;&#24456;&#40635;&#28902;&#12290;&#36825;&#28608;&#21457;&#20102;&#33258;&#21160;&#31867;&#22411;&#39044;&#27979;&#65306;&#32473;&#23450;&#19968;&#20010;&#26410;&#26631;&#35760;&#31867;&#22411;&#30340;&#31243;&#24207;&#65292;&#29983;&#25104;&#19968;&#20010;&#31867;&#22411;&#27491;&#30830;&#30340;&#36755;&#20986;&#31243;&#24207;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20110;&#31867;&#22411;&#39044;&#27979;&#38750;&#24120;&#26377;&#24076;&#26395;&#65292;&#20294;&#23384;&#22312;&#25361;&#25112;&#65306;&#20013;&#38388;&#22635;&#20805;&#30340;&#34920;&#29616;&#24046;&#65292;&#31243;&#24207;&#21487;&#33021;&#19981;&#36866;&#21512;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#29983;&#25104;&#30340;&#31867;&#22411;&#21487;&#33021;&#26080;&#27861;&#36890;&#36807;&#31867;&#22411;&#26816;&#26597;&#65292;&#32780;&#19988;&#24456;&#38590;&#27979;&#37327;&#36755;&#20986;&#31243;&#24207;&#30340;&#31867;&#22411;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;OpenTau&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#31867;&#22411;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#22411;&#39044;&#27979;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26641;&#24418;&#31243;&#24207;&#20998;&#35299;&#25216;&#26415;&#26469;&#25628;&#32034;&#29983;&#25104;&#30340;&#31867;&#22411;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;LLMs&#30340;&#31867;&#22411;&#22635;&#20805;&#24494;&#35843;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26032;&#30340;TypeScript&#31867;&#22411;&#39044;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;47.4&#65285;&#30340;&#25991;&#20214;&#36890;&#36807;&#31867;&#22411;&#26816;&#26597;&#65288;14.5&#65285;&#30340;&#32477;&#23545;&#25913;&#36827;&#65289;&#65292;&#24179;&#22343;&#31934;&#24230;&#36798;&#21040;0.707&#65292;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
TypeScript and Python are two programming languages that support optional type annotations, which are useful but tedious to introduce and maintain. This has motivated automated type prediction: given an untyped program, produce a well-typed output program. Large language models (LLMs) are promising for type prediction, but there are challenges: fill-in-the-middle performs poorly, programs may not fit into the context window, generated types may not type check, and it is difficult to measure how well-typed the output program is. We address these challenges by building OpenTau, a search-based approach for type prediction that leverages large language models. We propose a new metric for type prediction quality, give a tree-based program decomposition that searches a space of generated types, and present fill-in-the-type fine-tuning for LLMs. We evaluate our work with a new dataset for TypeScript type prediction, and show that 47.4% of files type check (14.5% absolute improvement) with an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Ghost in the Minecraft (GITM)&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#22312;Minecraft&#20013;&#20855;&#22791;&#36890;&#29992;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#65292;&#21487;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#29087;&#32451;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2305.17144</link><description>&lt;p&gt;
Minecraft&#20013;&#30340;&#24189;&#28789;&#65306;&#21033;&#29992;&#22522;&#20110;&#25991;&#26412;&#30693;&#35782;&#21644;&#35760;&#24518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#36890;&#29992;&#33021;&#21147;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory. (arXiv:2305.17144v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Ghost in the Minecraft (GITM)&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#22312;Minecraft&#20013;&#20855;&#22791;&#36890;&#29992;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#65292;&#21487;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#29087;&#32451;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Minecraft&#29609;&#27861;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#25104;&#20026;&#24320;&#21457;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#26234;&#33021;&#20307;&#30340;&#20016;&#23500;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#30446;&#26631;&#19978;&#65292;&#20363;&#22914;&#27969;&#34892;&#30340;&#8220;ObtainDiamond&#8221;&#20219;&#21153;&#65292;&#24182;&#19988;&#36824;&#27809;&#26377;&#26174;&#31034;&#20986;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#8220;ObtainDiamond&#8221;&#20219;&#21153;&#30340;&#30446;&#21069;&#26368;&#39640;&#25104;&#21151;&#29575;&#21482;&#26377;&#32422;20&#65285;&#65292;&#20984;&#26174;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25511;&#21046;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Ghost in the Minecraft (GITM)&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21019;&#24314;Minecraft&#20013;&#30340;&#36890;&#29992;&#33021;&#21147;&#26234;&#33021;&#20307;&#12290;&#36825;&#20123;&#20855;&#22791;LLM&#20013;&#30340;&#36923;&#36753;&#21644;&#24120;&#35782;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#29087;&#32451;&#22320;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular "ObtainDiamond" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the "ObtainDiamond" task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;CTDE&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;MAPPO&#31639;&#27861;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20915;&#31574;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26435;&#37325;&#35843;&#24230;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#32531;&#35299;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#21327;&#20316;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.17141</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#19982;&#21327;&#20316;&#20915;&#31574;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Research on Multi-Agent Communication and Collaborative Decision-Making Based on Deep Reinforcement Learning. (arXiv:2305.17141v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;CTDE&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;MAPPO&#31639;&#27861;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20915;&#31574;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26435;&#37325;&#35843;&#24230;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#32531;&#35299;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#21327;&#20316;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#65292;&#20026;&#20102;&#20811;&#26381;&#21644;&#32531;&#35299;&#29615;&#22659;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#20027;&#27969;&#26041;&#27861;&#26159;&#37319;&#29992;&#38598;&#20013;&#24335;&#35757;&#32451;&#20998;&#25955;&#24335;&#25191;&#34892;&#65288;CTDE&#65289;&#26694;&#26550;&#12290;&#26412;&#25991;&#22522;&#20110;CTDE&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;MAPPO&#65289;&#31639;&#27861;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20915;&#31574;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#26435;&#37325;&#35843;&#24230;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26426;&#21046;&#12290;&#19981;&#21516;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#26469;&#32531;&#35299;&#30001;&#26412;&#22320;&#35266;&#27979;&#24341;&#36215;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#21327;&#21161;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#20915;&#31574;&#12290;&#20855;&#20307;&#26041;&#27861;&#26159;&#22312;&#31574;&#30053;&#32593;&#32476;&#37096;&#20998;&#24341;&#20837;&#19968;&#20010;&#36890;&#20449;&#27169;&#22359;&#12290;&#36890;&#20449;&#27169;&#22359;&#30001;&#26435;&#37325;&#29983;&#25104;&#22120;&#12289;&#26435;&#37325;&#35843;&#24230;&#22120;&#12289;&#20449;&#24687;&#32534;&#30721;&#22120;&#12289;&#20449;&#24687;&#35299;&#30721;&#22120;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#32452;&#25104;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#21327;&#20316;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a multi-agent environment, In order to overcome and alleviate the non-stationarity of the multi-agent environment, the mainstream method is to adopt the framework of Centralized Training Decentralized Execution (CTDE). This thesis is based on the framework of CTDE, and studies the cooperative decision-making of multi-agent based on the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm for multi-agent proximal policy optimization. In order to alleviate the non-stationarity of the multi-agent environment, a multi-agent communication mechanism based on weight scheduling and attention module is introduced. Different agents can alleviate the non-stationarity caused by local observations through information exchange between agents, assisting in the collaborative decision-making of agents. The specific method is to introduce a communication module in the policy network part. The communication module is composed of a weight generator, a weight scheduler, a message encoder, a messag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#26234;&#33021;&#36710;&#36742;&#31995;&#32479;&#20013;&#38598;&#25104;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#38754;&#25351;&#21335;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#20854;&#23545;&#35821;&#38899;&#12289;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#39046;&#22495;&#21644;&#19982;&#20262;&#29702;&#36947;&#24503;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.17137</link><description>&lt;p&gt;
&#26234;&#33021;&#36710;&#36742;&#31995;&#32479;&#20013;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Integrating Generative Artificial Intelligence in Intelligent Vehicle Systems. (arXiv:2305.17137v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#26234;&#33021;&#36710;&#36742;&#31995;&#32479;&#20013;&#38598;&#25104;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#38754;&#25351;&#21335;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#20854;&#23545;&#35821;&#38899;&#12289;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#39046;&#22495;&#21644;&#19982;&#20262;&#29702;&#36947;&#24503;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#20840;&#38754;&#25351;&#21335;&#65292;&#20026;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#21644;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#36710;&#36742;&#29615;&#22659;&#20013;&#30340;&#24403;&#21069;&#29366;&#24577;&#12289;&#28508;&#22312;&#24212;&#29992;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#27934;&#35265;&#12290;&#38543;&#30528;&#27773;&#36710;&#34892;&#19994;&#36880;&#28176;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26377;&#28508;&#21147;&#22312;&#29992;&#25143;&#20132;&#20114;&#26041;&#38754;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#65292;&#25552;&#20379;&#26356;&#27785;&#28024;&#12289;&#30452;&#35266;&#12289;&#20010;&#24615;&#21270;&#30340;&#36710;&#20869;&#20307;&#39564;&#12290;&#25105;&#20204;&#25552;&#20379;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#22312;&#27773;&#36710;&#39046;&#22495;&#20013;&#30340;&#24403;&#21069;&#24212;&#29992;&#27010;&#36848;&#65292;&#37325;&#28857;&#24378;&#35843;&#35821;&#38899;&#12289;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;&#38543;&#21518;&#25105;&#20204;&#27010;&#36848;&#20102;&#20851;&#38190;&#26410;&#26469;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#25324;&#39046;&#22495;&#36866;&#24212;&#24615;&#12289;&#23545;&#40784;&#12289;&#22810;&#27169;&#24577;&#38598;&#25104;&#31561;&#65292;&#20197;&#21450;&#35299;&#20915;&#19982;&#20262;&#29702;&#36947;&#24503;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;&#36890;&#36807;&#20419;&#36827;&#21327;&#20316;&#21644;&#35299;&#20915;&#36825;&#20123;&#30740;&#31350;&#39046;&#22495;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#25104;&#20026;&#26234;&#33021;&#36710;&#36742;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22686;&#24378;&#39550;&#39542;&#21592;&#21644;&#20056;&#23458;&#30340;&#23433;&#20840;&#12289;&#33298;&#36866;&#21644;&#20415;&#21033;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to serve as a comprehensive guide for researchers and practitioners, offering insights into the current state, potential applications, and future research directions for generative artificial intelligence and foundation models within the context of intelligent vehicles. As the automotive industry progressively integrates AI, generative artificial intelligence technologies hold the potential to revolutionize user interactions, delivering more immersive, intuitive, and personalised in-car experiences. We provide an overview of current applications of generative artificial intelligence in the automotive domain, emphasizing speech, audio, vision, and multimodal interactions. We subsequently outline critical future research areas, including domain adaptability, alignment, multimodal integration and others, as well as, address the challenges and risks associated with ethics. By fostering collaboration and addressing these research areas, generative artificial intelligence can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; 3T &#26041;&#27861;&#65292;&#21363;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;3T &#21516;&#26102;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#23545;&#27604;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#23545;&#26816;&#32034;&#20219;&#21153;&#21644;&#20998;&#31867;&#38382;&#39064;&#22343;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16999</link><description>&lt;p&gt;
&#19977;&#22612;&#65306;&#21033;&#29992;&#39044;&#35757;&#32451;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#28789;&#27963;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Three Towers: Flexible Contrastive Learning with Pretrained Image Models. (arXiv:2305.16999v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; 3T &#26041;&#27861;&#65292;&#21363;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;3T &#21516;&#26102;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#23545;&#27604;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#23545;&#26816;&#32034;&#20219;&#21153;&#21644;&#20998;&#31867;&#38382;&#39064;&#22343;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19977;&#22612;&#65288;3T&#65289;&#8221;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#32435;&#20837;&#23545;&#27604;&#23398;&#20064;&#65292;&#25913;&#36827;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;&#19982;&#36890;&#24120;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23545;&#27604;&#27169;&#22411;&#19981;&#21516;&#65292;&#26368;&#36817;&#30340; LiT&#65288;Zhai &#31561;&#20154;&#65292;2022&#65289;&#34920;&#26126;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#23884;&#20837;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20294;&#26159;&#65292;LiT &#30452;&#25509;&#29992;&#20923;&#32467;&#30340;&#23884;&#20837;&#26367;&#25442;&#22270;&#20687;&#22612;&#65292;&#25490;&#38500;&#20102;&#23545;&#22270;&#20687;&#22612;&#36827;&#34892;&#23545;&#27604;&#35757;&#32451;&#30340;&#20219;&#20309;&#28508;&#22312;&#22909;&#22788;&#12290;&#36890;&#36807; 3T&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#30340;&#31574;&#30053;&#65292;&#20801;&#35768;&#22270;&#20687;&#22612;&#21516;&#26102;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#23545;&#27604;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19977;&#20010;&#22612;&#65292;&#20854;&#20013;&#21253;&#21547;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#65292;&#24182;&#40723;&#21169;&#35813;&#31532;&#19977;&#20010;&#22612;&#19982;&#20027;&#35201;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22612;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;3T &#22312;&#26816;&#32034;&#20219;&#21153;&#19978;&#22987;&#32456;&#20248;&#20110; LiT &#21644; CLIP &#39118;&#26684;&#30340;&#20174;&#22836;&#24320;&#22987;&#23545;&#27604;&#23398;&#20064;&#22522;&#32447;&#12290;&#23545;&#20110;&#20998;&#31867;&#38382;&#39064;&#65292;3T &#22312;&#20174;&#22836;&#24320;&#22987;&#22522;&#32447;&#30340;&#22522;&#30784;&#19978;&#21487;&#38752;&#22320;&#25913;&#21892;&#65292;&#34429;&#28982;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#21450; LiT&#65292;&#20294;&#20173;&#28982;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#26041;&#27861;&#20984;&#26174;&#20102;&#23558;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#27880;&#20837;&#21040;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#30340;&#21033;&#29992;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Three Towers (3T), a flexible method to improve the contrastive learning of vision-language models by incorporating pretrained image classifiers. While contrastive models are usually trained from scratch, LiT (Zhai et al., 2022) has recently shown performance gains from using pretrained classifier embeddings. However, LiT directly replaces the image tower with the frozen embeddings, excluding any potential benefits of contrastively training the image tower. With 3T, we propose a more flexible strategy that allows the image tower to benefit from both pretrained embeddings and contrastive training. To achieve this, we introduce a third tower that contains the frozen pretrained embeddings, and we encourage alignment between this third tower and the main image-text towers. Empirically, 3T consistently improves over LiT and the CLIP-style from-scratch baseline for retrieval tasks. For classification, 3T reliably improves over the from-scratch baseline, and while it underperform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#23485;&#24230;&#21644;&#20219;&#24847;&#28145;&#24230;&#30340;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;&#65292;&#21457;&#29616;&#24403;&#19988;&#20165;&#24403;&#28608;&#27963;&#20989;&#25968;&#26082;&#19981;&#26159;&#20840;&#32431;&#30340;&#65292;&#20063;&#19981;&#26159;&#21453;&#20840;&#32431;&#30340;&#65292;&#20063;&#19981;&#26159; $\mathbb{R}$-&#20223;&#23556;&#30340;&#26102;&#65292;&#28145;&#31364;&#30340;&#22797;&#20540;&#32593;&#32476;&#20855;&#26377;&#26222;&#36866;&#36924;&#36817;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#36275;&#22815;&#30340;&#23485;&#24230;&#20381;&#36182;&#20110;&#32771;&#34385;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#23545;&#20110;&#19968;&#31867;&#21487;&#20801;&#35768;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#23485;&#24230;&#20026; $n+m+4$ &#26159;&#36275;&#22815;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.16910</link><description>&lt;p&gt;
&#24102;&#26377;&#22797;&#20540;&#30340;&#28145;&#31364;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Universal approximation with complex-valued deep narrow neural networks. (arXiv:2305.16910v1 [math.FA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#23485;&#24230;&#21644;&#20219;&#24847;&#28145;&#24230;&#30340;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;&#65292;&#21457;&#29616;&#24403;&#19988;&#20165;&#24403;&#28608;&#27963;&#20989;&#25968;&#26082;&#19981;&#26159;&#20840;&#32431;&#30340;&#65292;&#20063;&#19981;&#26159;&#21453;&#20840;&#32431;&#30340;&#65292;&#20063;&#19981;&#26159; $\mathbb{R}$-&#20223;&#23556;&#30340;&#26102;&#65292;&#28145;&#31364;&#30340;&#22797;&#20540;&#32593;&#32476;&#20855;&#26377;&#26222;&#36866;&#36924;&#36817;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#36275;&#22815;&#30340;&#23485;&#24230;&#20381;&#36182;&#20110;&#32771;&#34385;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#23545;&#20110;&#19968;&#31867;&#21487;&#20801;&#35768;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#23485;&#24230;&#20026; $n+m+4$ &#26159;&#36275;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#23485;&#24230;&#21644;&#20219;&#24847;&#28145;&#24230;&#30340;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;&#12290;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#37027;&#20123;&#28608;&#27963;&#20989;&#25968; $\varrho:\mathbb{CC}\to \mathbb{C}$ &#30340;&#23436;&#25972;&#25551;&#36848;&#65292;&#36825;&#20123;&#20989;&#25968;&#20855;&#26377;&#36825;&#26679;&#19968;&#20010;&#23646;&#24615;&#65306;&#23427;&#20204;&#20851;&#32852;&#30340;&#32593;&#32476;&#26159;&#26222;&#36866;&#30340;&#65292;&#21363;&#33021;&#22815;&#22312;&#32039;&#33268;&#22495;&#19978;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#33267;&#20219;&#24847;&#31934;&#24230;&#12290;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#34920;&#26126;&#20102;&#24403;&#19988;&#20165;&#24403;&#23427;&#20204;&#30340;&#28608;&#27963;&#20989;&#25968;&#26082;&#19981;&#26159;&#20840;&#32431;&#30340;&#65292;&#20063;&#19981;&#26159;&#21453;&#20840;&#32431;&#30340;&#65292;&#20063;&#19981;&#26159; $\mathbb{R}$-&#20223;&#23556;&#30340;&#65292;&#28145;&#31364;&#30340;&#22797;&#20540;&#32593;&#32476;&#26159;&#26222;&#36866;&#30340;&#12290;&#36825;&#26159;&#19968;&#20010;&#27604;&#23485;&#24230;&#20219;&#24847;&#12289;&#28145;&#24230;&#22266;&#23450;&#30340;&#23545;&#20598;&#35774;&#32622;&#20013;&#26356;&#22823;&#30340;&#20989;&#25968;&#31867;&#12290;&#19982;&#23454;&#20540;&#24773;&#20917;&#19981;&#21516;&#30340;&#26159;&#65292;&#36275;&#22815;&#30340;&#23485;&#24230;&#20381;&#36182;&#20110;&#32771;&#34385;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23485;&#24230;&#20026; $2n+2m+5$ &#24635;&#26159;&#36275;&#22815;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120; $\max\{2n,2m\}$ &#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#31867;&#21487;&#20801;&#35768;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#23485;&#24230;&#20026; $n+m+4$ &#26159;&#36275;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the universality of complex-valued neural networks with bounded widths and arbitrary depths. Under mild assumptions, we give a full description of those activation functions $\varrho:\mathbb{CC}\to \mathbb{C}$ that have the property that their associated networks are universal, i.e., are capable of approximating continuous functions to arbitrary accuracy on compact domains. Precisely, we show that deep narrow complex-valued networks are universal if and only if their activation function is neither holomorphic, nor antiholomorphic, nor $\mathbb{R}$-affine. This is a much larger class of functions than in the dual setting of arbitrary width and fixed depth. Unlike in the real case, the sufficient width differs significantly depending on the considered activation function. We show that a width of $2n+2m+5$ is always sufficient and that in general a width of $\max\{2n,2m\}$ is necessary. We prove, however, that a width of $n+m+4$ suffices for a rich subclass of the admissible acti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#26368;&#36817;&#23383;&#31526;&#20018;&#38382;&#39064;&#26159;&#21542;&#23384;&#22312;&#27604;&#24179;&#20961;&#30340;&#31351;&#20030;&#25628;&#32034;&#31639;&#27861;&#26356;&#24555;&#30340;&#31639;&#27861;&#65292;&#38024;&#23545;&#19981;&#21516;&#29256;&#26412;&#30340;&#38382;&#39064;&#30340;&#33258;&#28982;&#29256;&#26412;&#65292;&#24471;&#20986;&#20197;&#19979;&#32467;&#26524;&#65306;&#23545;&#20110;&#36830;&#32493;&#30340;&#38382;&#39064;&#29256;&#26412;&#65292;&#19981;&#23384;&#22312;&#27604;&#24179;&#20961;&#30340;&#31351;&#20030;&#25628;&#32034;&#31639;&#27861;&#26356;&#24555;&#30340;&#35299;&#27861;&#65292;&#23545;&#20110;&#31163;&#25955;&#29256;&#26412;&#65292;&#22914;&#26524;$d$&#26159;&#24120;&#25968;&#65292;&#21017;&#20197;$2^{O(\sqrt{n})}$&#27425;&#27604;&#36739;&#21487;&#20197;&#35299;&#20915;&#23427;&#12290;</title><link>http://arxiv.org/abs/2305.16878</link><description>&lt;p&gt;
&#20320;&#33021;&#27604;&#31351;&#20030;&#25628;&#32034;&#26356;&#24555;&#22320;&#35299;&#20915;&#26368;&#36817;&#23383;&#31526;&#20018;&#38382;&#39064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can You Solve Closest String Faster than Exhaustive Search?. (arXiv:2305.16878v2 [cs.CC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26368;&#36817;&#23383;&#31526;&#20018;&#38382;&#39064;&#26159;&#21542;&#23384;&#22312;&#27604;&#24179;&#20961;&#30340;&#31351;&#20030;&#25628;&#32034;&#31639;&#27861;&#26356;&#24555;&#30340;&#31639;&#27861;&#65292;&#38024;&#23545;&#19981;&#21516;&#29256;&#26412;&#30340;&#38382;&#39064;&#30340;&#33258;&#28982;&#29256;&#26412;&#65292;&#24471;&#20986;&#20197;&#19979;&#32467;&#26524;&#65306;&#23545;&#20110;&#36830;&#32493;&#30340;&#38382;&#39064;&#29256;&#26412;&#65292;&#19981;&#23384;&#22312;&#27604;&#24179;&#20961;&#30340;&#31351;&#20030;&#25628;&#32034;&#31639;&#27861;&#26356;&#24555;&#30340;&#35299;&#27861;&#65292;&#23545;&#20110;&#31163;&#25955;&#29256;&#26412;&#65292;&#22914;&#26524;$d$&#26159;&#24120;&#25968;&#65292;&#21017;&#20197;$2^{O(\sqrt{n})}$&#27425;&#27604;&#36739;&#21487;&#20197;&#35299;&#20915;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23547;&#25214;&#29992;&#26368;&#20339;&#23383;&#31526;&#20018;&#34920;&#31034;&#32473;&#23450;&#38598;&#21512;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#36817;&#23383;&#31526;&#20018;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#22823;&#23567;&#20026;$n$&#65292;&#38271;&#24230;&#20026;$d$&#65292;&#30001;&#23383;&#31526;&#20018;$X \subseteq \Sigma^d$&#32452;&#25104;&#30340;&#38598;&#21512;&#65292;&#25214;&#21040;&#23383;&#31526;&#20018;$x^*$&#65292;&#20351;&#24471;&#26368;&#23567;&#30340;Hamming&#29699;&#30340;&#21322;&#24452;&#22218;&#25324;&#20102;$X$&#20013;&#30340;&#25152;&#26377;&#23383;&#31526;&#20018;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#36817;&#23383;&#31526;&#20018;&#38382;&#39064;&#26159;&#21542;&#23384;&#22312;&#27604;&#24179;&#20961;&#30340;&#31351;&#20030;&#25628;&#32034;&#31639;&#27861;&#26356;&#24555;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#38382;&#39064;&#30340;&#20004;&#20010;&#33258;&#28982;&#29256;&#26412;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20197;&#19979;&#32467;&#26524;&#65306;&#22312;&#36830;&#32493;&#30340;&#26368;&#36817;&#23383;&#31526;&#20018;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#22312;$\Sigma^d$&#20013;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#23383;&#31526;&#20018;$x^*$&#12290;&#23545;&#20110;&#20108;&#36827;&#21046;&#23383;&#31526;&#20018;&#65292;&#31351;&#20030;&#25628;&#32034;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#20026;$O(2^d poly(nd))$&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#19981;&#33021;&#25913;&#36827;&#20026;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(2^{(1-\epsilon) d} poly(nd))$&#65292;&#20219;&#20309;$\epsilon &gt; 0$&#37117;&#19981;&#25104;&#31435;&#65292;&#38500;&#38750;&#24378;&#25351;&#25968;&#26102;&#38388;&#20551;&#35774;&#22833;&#36133;&#12290;&#22312;&#31163;&#25955;&#30340;&#26368;&#36817;&#23383;&#31526;&#20018;&#38382;&#39064;&#20013;&#65292;&#35201;&#27714;$x^*$&#22312;&#36755;&#20837;&#38598;&#21512;$X$&#20013;&#12290;&#34429;&#28982;&#36825;&#23558;&#38382;&#39064;&#36716;&#25442;&#20026;NP-&#23436;&#20840;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;$d$&#26159;&#24120;&#25968;&#65292;&#21017;&#20197;$2^{O(\sqrt{n})}$&#27425;&#27604;&#36739;&#30340;&#24418;&#24335;&#21487;&#20197;&#35299;&#20915;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the fundamental problem of finding the best string to represent a given set, in the form of the Closest String problem: Given a set $X \subseteq \Sigma^d$ of $n$ strings, find the string $x^*$ minimizing the radius of the smallest Hamming ball around $x^*$ that encloses all the strings in $X$. In this paper, we investigate whether the Closest String problem admits algorithms that are faster than the trivial exhaustive search algorithm. We obtain the following results for the two natural versions of the problem:  $\bullet$ In the continuous Closest String problem, the goal is to find the solution string $x^*$ anywhere in $\Sigma^d$. For binary strings, the exhaustive search algorithm runs in time $O(2^d poly(nd))$ and we prove that it cannot be improved to time $O(2^{(1-\epsilon) d} poly(nd))$, for any $\epsilon &gt; 0$, unless the Strong Exponential Time Hypothesis fails.  $\bullet$ In the discrete Closest String problem, $x^*$ is required to be in the input set $X$. While this p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#31163;&#25955;&#21270;&#20559;&#24046;&#21644;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#65292;&#24471;&#21040;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#30340;&#27867;&#21270;&#24046;&#36317;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.16791</link><description>&lt;p&gt;
&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#27867;&#21270;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization Capacities of Neural Controlled Differential Equations. (arXiv:2305.16791v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#31163;&#25955;&#21270;&#20559;&#24046;&#21644;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#65292;&#24471;&#21040;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#30340;&#27867;&#21270;&#24046;&#36317;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;Kidger&#65292;Morrill&#31561;&#65292;2020&#65289;&#20174;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#20013;&#39044;&#27979;&#32467;&#26524;&#30340;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#26159;&#19968;&#20010;&#26410;&#35266;&#23519;&#21040;&#30340;&#36830;&#32493;&#36335;&#24452;&#30340;&#31163;&#25955;&#21270;&#65292;&#32467;&#26524;&#36890;&#36807;&#19968;&#20010;&#20855;&#26377;&#26410;&#30693;&#21521;&#37327;&#22330;&#30340;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#20381;&#36182;&#20110;&#36825;&#20010;&#36335;&#24452;&#12290;&#20351;&#29992;&#31163;&#25955;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#20250;&#24341;&#20837;&#31163;&#25955;&#20559;&#24046;&#65292;&#25105;&#20204;&#31934;&#30830;&#22320;&#37327;&#21270;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;&#36890;&#36807;&#20351;&#29992;&#20851;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#27969;&#30340;&#36830;&#32493;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36924;&#36817;&#20559;&#24046;&#30452;&#25509;&#19982;&#30001;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#29983;&#25104;&#27169;&#22411;&#30340;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#30340;&#36924;&#36817;&#35823;&#24046;&#30456;&#20851;&#12290;&#36890;&#36807;&#32467;&#21512;&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#19978;&#30028;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#36798;&#21040;&#30340;&#26399;&#26395;&#25439;&#22833;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a supervised learning setup in which the goal is to predicts an outcome from a sample of irregularly sampled time series using Neural Controlled Differential Equations (Kidger, Morrill, et al. 2020). In our framework, the time series is a discretization of an unobserved continuous path, and the outcome depends on this path through a controlled differential equation with unknown vector field. Learning with discrete data thus induces a discretization bias, which we precisely quantify. Using theoretical results on the continuity of the flow of controlled differential equations, we show that the approximation bias is directly related to the approximation error of a Lipschitz function defining the generative model by a shallow neural network. By combining these result with recent work linking the Lipschitz constant of neural networks to their generalization capacities, we upper bound the generalization gap between the expected loss attained by the empirical risk minimizer and th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#36947;&#32622;&#20449;&#24230;&#19982;&#20266;&#32622;&#20449;&#24230;&#30340;&#29305;&#24449;&#25554;&#20540;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39640;&#32570;&#22833;&#29305;&#24449;&#29575;&#30340;&#22270;&#20687;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16618</link><description>&lt;p&gt;
&#24102;&#37096;&#20998;&#30693;&#35782;&#29305;&#24449;&#30340;&#22270;&#20687;&#32570;&#22833;&#29305;&#24449;&#34917;&#20840;&#20013;&#30340;&#32622;&#20449;&#24230;&#29305;&#24615;&#36741;&#21161;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Confidence-Based Feature Imputation for Graphs with Partially Known Features. (arXiv:2305.16618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#36947;&#32622;&#20449;&#24230;&#19982;&#20266;&#32622;&#20449;&#24230;&#30340;&#29305;&#24449;&#25554;&#20540;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39640;&#32570;&#22833;&#29305;&#24449;&#29575;&#30340;&#22270;&#20687;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#32570;&#22833;&#29305;&#24449;&#34917;&#20840;&#38382;&#39064;&#12290;&#36807;&#21435;&#26377;&#20960;&#31181;&#26041;&#27861;&#24050;&#32463;&#35299;&#20915;&#20102;&#20855;&#26377;&#32570;&#22833;&#29305;&#24449;&#30340;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#32570;&#22833;&#29305;&#24449;&#29575;&#30340;&#24773;&#20917;&#65292;&#23427;&#20204;&#26080;&#27861;&#36991;&#20813;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;&#21363;&#33410;&#28857;&#29305;&#24449;&#20013;&#30340;&#20449;&#36947;&#32622;&#20449;&#24230;&#65292;&#23427;&#34987;&#25351;&#23450;&#32473;&#27599;&#20010;&#33410;&#28857;&#30340;&#22635;&#20805;&#20449;&#36947;&#29305;&#24449;&#65292;&#20197;&#21453;&#26144;&#22635;&#20805;&#30340;&#30830;&#23450;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20266;&#32622;&#20449;&#24230;&#65292;&#20351;&#29992;&#32570;&#22833;&#29305;&#24449;&#33410;&#28857;&#19982;&#20854;&#26368;&#36817;&#30340;&#24050;&#30693;&#29305;&#24449;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#36947;&#26368;&#30701;&#36335;&#24452;&#36317;&#31163;&#26469;&#26367;&#25442;&#23454;&#38469;&#23398;&#20064;&#36807;&#31243;&#20013;&#32570;&#22833;&#30340;&#30495;&#23454;&#32622;&#20449;&#24230;&#12290;&#22522;&#20110;&#20266;&#32622;&#20449;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#25554;&#20540;&#26041;&#26696;&#65292;&#23427;&#25191;&#34892;&#20449;&#36947;&#20869;&#33410;&#28857;&#25193;&#25955;&#21644;&#33410;&#28857;&#20869;&#20449;&#36947;&#20256;&#25773;&#12290;&#35813;&#26041;&#26696;&#21487;&#20197;&#22312;&#38750;&#24120;&#39640;&#30340;&#32570;&#22833;&#29575;&#19979;&#65288;&#20363;&#22914;&#65292;99.5&#65285;&#65289;&#25345;&#20037;&#23384;&#22312;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates a missing feature imputation problem for graph learning tasks. Several methods have previously addressed learning tasks on graphs with missing features. However, in cases of high rates of missing features, they were unable to avoid significant performance degradation. To overcome this limitation, we introduce a novel concept of channel-wise confidence in a node feature, which is assigned to each imputed channel feature of a node for reflecting certainty of the imputation. We then design pseudo-confidence using the channel-wise shortest path distance between a missing-feature node and its nearest known-feature node to replace unavailable true confidence in an actual learning process. Based on the pseudo-confidence, we propose a novel feature imputation scheme that performs channel-wise inter-node diffusion and node-wise inter-channel propagation. The scheme can endure even at an exceedingly high missing rate (e.g., 99.5\%) and it achieves state-of-the-art accurac
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20998;&#26512;&#20102;&#26032;&#25552;&#20986;&#30340;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#32531;&#35299;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16573</link><description>&lt;p&gt;
&#25506;&#32034;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#26435;&#37325;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16573
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20998;&#26512;&#20102;&#26032;&#25552;&#20986;&#30340;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#32531;&#35299;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25968;&#25454;&#20013;&#30340;&#35782;&#21035;&#38382;&#39064;&#26368;&#36817;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#20998;&#24067;&#36890;&#24120;&#26159;&#25351;&#25968;&#20998;&#24067;&#65292;&#38500;&#38750;&#26377;&#24847;&#22320;&#35843;&#25972;&#26679;&#26412;&#25968;&#37327;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33879;&#21517;&#30340;&#32463;&#20856;&#27491;&#21017;&#21270;&#25216;&#26415;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#20294;&#24050;&#30693;&#20854;&#23545;&#29616;&#26377;&#21508;&#31181;&#19981;&#21516;&#26041;&#27861;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#20026;&#20160;&#20040;&#36825;&#31181;&#26041;&#27861;&#23545;&#38271;&#23614;&#25968;&#25454;&#26377;&#25928;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#20851;&#27880;&#20102;&#31070;&#32463;&#23849;&#28291;&#21644;&#27599;&#20010;&#35757;&#32451;&#38454;&#27573;&#30340;&#22278;&#38181;&#25928;&#24212;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#20998;&#35299;&#20026;&#30001;&#26435;&#20540;&#34928;&#20943;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#24341;&#36215;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#20013;Fisher&#21028;&#21035;&#27604;&#30340;&#22686;&#21152;&#20197;&#21450;&#30001;&#26435;&#37325;&#34928;&#20943;&#21644;&#31867;&#24179;&#34913;&#27491;&#21017;&#21270;&#24341;&#36215;&#30340;&#38544;&#24335;&#36923;&#36753;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#25104;&#21151;&#32531;&#35299;&#20102;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38271;&#23614;&#25968;&#25454;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognition problems in long-tailed data, where the sample size per class is heavily skewed, have recently gained importance because the distribution of the sample size per class in a dataset is generally exponential unless the sample size is intentionally adjusted. Various approaches have been devised to address these problems. Recently, weight balancing, which combines well-known classical regularization techniques with two-stage training, has been proposed. Despite its simplicity, it is known for its high performance against existing methods devised in various ways. However, there is a lack of understanding as to why this approach is effective for long-tailed data. In this study, we analyze the method focusing on neural collapse and cone effect at each training stage and find that it can be decomposed into the increase in Fisher's discriminant ratio of the feature extractor caused by weight decay and cross entropy loss and implicit logit adjustment caused by weight decay and class-b
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#23545;&#20110;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#26131;&#20110;&#20135;&#29983;&#31867;&#22349;&#22604;&#65292;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#26131;&#20110;&#25233;&#21046;&#31867;&#21035;&#30456;&#20851;&#30340;&#22797;&#26434;&#29305;&#24449;&#65307;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20559;&#21521;&#20110;&#23547;&#25214;&#26356;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23548;&#33268;&#36825;&#31181;&#29616;&#35937;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.16536</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#20102;&#21738;&#20123;&#29305;&#24449;&#65311;&#20851;&#20110;&#31616;&#26131;&#20559;&#24046;&#22312;&#31867;&#22349;&#22604;&#21644;&#29305;&#24449;&#25233;&#21046;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression. (arXiv:2305.16536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16536
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#23545;&#20110;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#26131;&#20110;&#20135;&#29983;&#31867;&#22349;&#22604;&#65292;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#26131;&#20110;&#25233;&#21046;&#31867;&#21035;&#30456;&#20851;&#30340;&#22797;&#26434;&#29305;&#24449;&#65307;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20559;&#21521;&#20110;&#23547;&#25214;&#26356;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23548;&#33268;&#36825;&#31181;&#29616;&#35937;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#20855;&#22791;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#26377;&#30417;&#30563;&#22330;&#26223;&#19979;&#26131;&#20110;&#22349;&#22604;&#21516;&#19968;&#31867;&#21035;&#20869;&#30340;&#23376;&#31867;&#34920;&#31034;&#65292;&#20002;&#22833;&#19968;&#37096;&#20998;&#29305;&#24449;&#20449;&#24687;&#65307;&#32780;&#26080;&#30417;&#30563;&#23398;&#20064;&#21017;&#21487;&#33021;&#36890;&#36807;&#23398;&#20064;&#26131;&#20110;&#22788;&#29702;&#30340;&#31867;&#21035;&#26080;&#20851;&#29305;&#24449;&#32780;&#26080;&#35270;&#19968;&#20123;&#31867;&#21035;&#30456;&#20851;&#30340;&#22797;&#26434;&#29305;&#24449;&#20449;&#24687;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20250;&#26174;&#33879;&#22320;&#38477;&#20302;&#34920;&#24449;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32479;&#19968;&#20005;&#35880;&#30340;&#26694;&#26550;&#26469;&#29702;&#35299;&#27979;&#35797;&#26102;&#30340;&#31867;&#22349;&#22604;&#21644;&#29305;&#24449;&#25233;&#21046;&#20135;&#29983;&#30340;&#21407;&#22240;&#65292;&#30456;&#20851;&#20998;&#26512;&#34920;&#26126;&#65292;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20559;&#21521;&#20110;&#23547;&#25214;&#26356;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23548;&#33268;&#23376;&#31867;&#34920;&#31034;&#22349;&#22604;&#21644;&#31867;&#21035;&#30456;&#20851;&#30340;&#22797;&#26434;&#29305;&#24449;&#34987;&#25233;&#21046;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25552;&#39640;&#23884;&#20837;&#32500;&#24230;&#21644;&#25913;&#36827;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25552;&#20379;&#26377;&#25928;&#30340;&#39044;&#38450;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning (CL) has emerged as a powerful technique for representation learning, with or without label supervision. However, supervised CL is prone to collapsing representations of subclasses within a class by not capturing all their features, and unsupervised CL may suppress harder class-relevant features by focusing on learning easy class-irrelevant features; both significantly compromise representation quality. Yet, there is no theoretical understanding of \textit{class collapse} or \textit{feature suppression} at \textit{test} time. We provide the first unified theoretically rigorous framework to determine \textit{which} features are learnt by CL. Our analysis indicate that, perhaps surprisingly, bias of (stochastic) gradient descent towards finding simpler solutions is a key factor in collapsing subclass representations and suppressing harder class-relevant features. Moreover, we present increasing embedding dimensionality and improving the quality of data augmentations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36229;&#22823;&#35268;&#27169;&#22270;&#30340;&#22312;&#32447;&#33410;&#28857;&#20998;&#31867;&#31639;&#27861;FastONL&#65292;&#23427;&#22522;&#20110;&#24191;&#20041;&#23616;&#37096;&#25512;&#36865;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#36817;&#20284;&#36870;&#30697;&#38453;&#21015;&#24182;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#27969;&#34892;&#30340;&#22270;&#26680;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#36951;&#25022;&#20540;&#21644;&#27599;&#20010;&#39044;&#27979;&#30340;&#36739;&#20302;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.16257</link><description>&lt;p&gt;
&#38754;&#21521;&#36229;&#22823;&#35268;&#27169;&#22270;&#30340;&#24555;&#36895;&#22312;&#32447;&#33410;&#28857;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast Online Node Labeling for Very Large Graphs. (arXiv:2305.16257v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36229;&#22823;&#35268;&#27169;&#22270;&#30340;&#22312;&#32447;&#33410;&#28857;&#20998;&#31867;&#31639;&#27861;FastONL&#65292;&#23427;&#22522;&#20110;&#24191;&#20041;&#23616;&#37096;&#25512;&#36865;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#36817;&#20284;&#36870;&#30697;&#38453;&#21015;&#24182;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#27969;&#34892;&#30340;&#22270;&#26680;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#36951;&#25022;&#20540;&#21644;&#27599;&#20010;&#39044;&#27979;&#30340;&#36739;&#20302;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36716;&#23548;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#22312;&#32447;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#24403;&#21069;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22312;$\mathcal{O}(n^3)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;$\mathcal{O}(n^2)$&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#20869;&#27714;&#35299;&#22270;&#26680;&#30697;&#38453;&#30340;&#36870;&#65292;&#35201;&#20040;&#38656;&#35201;&#37319;&#26679;&#22823;&#37327;&#30340;&#38543;&#26426;&#29983;&#25104;&#26641;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#26494;&#24347;&#25216;&#26415;&#30340;&#25913;&#36827;&#31639;&#27861;&#12290;&#24403;&#36866;&#24403;&#36873;&#25321;&#21442;&#25968;&#21270;&#30340;&#22270;&#26680;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#26377;&#25928;&#30340;&#36951;&#25022;&#20540;&#20026;$\mathcal{O}(\sqrt{n^{1+\gamma}})$&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#35813;&#26494;&#24347;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#31639;&#27861;FastONL&#65292;&#20854;&#36951;&#25022;&#20540;&#20026;$\mathcal{O}(k\sqrt{n^{1+\gamma}})$&#12290;FastONL&#30340;&#20851;&#38190;&#26159;&#19968;&#31181;&#24191;&#20041;&#23616;&#37096;&#25512;&#36865;&#26041;&#27861;&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#36817;&#20284;&#36870;&#30697;&#38453;&#21015;&#24182;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#27969;&#34892;&#30340;&#22270;&#26680;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#39044;&#27979;&#30340;&#25104;&#26412;&#20026;$\mathcal{O}(\text{vol}({\mathcal{S}})\log 1/\epsilon)$
&lt;/p&gt;
&lt;p&gt;
This paper studies the online node classification problem under a transductive learning setting. Current methods either invert a graph kernel matrix with $\mathcal{O}(n^3)$ runtime and $\mathcal{O}(n^2)$ space complexity or sample a large volume of random spanning trees, thus are difficult to scale to large graphs. In this work, we propose an improvement based on the \textit{online relaxation} technique introduced by a series of works (Rakhlin et al.,2012; Rakhlin and Sridharan, 2015; 2017). We first prove an effective regret $\mathcal{O}(\sqrt{n^{1+\gamma}})$ when suitable parameterized graph kernels are chosen, then propose an approximate algorithm FastONL enjoying $\mathcal{O}(k\sqrt{n^{1+\gamma}})$ regret based on this relaxation. The key of FastONL is a \textit{generalized local push} method that effectively approximates inverse matrix columns and applies to a series of popular kernels. Furthermore, the per-prediction cost is $\mathcal{O}(\text{vol}({\mathcal{S}})\log 1/\epsilon)$
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.16044</link><description>&lt;p&gt;
&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#23558;&#22122;&#22768;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#26159;&#22823;&#33041;&#38750;&#20961;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#30340;&#22522;&#30784;&#65292;&#24182;&#24050;&#25104;&#20026;&#31070;&#32463;&#24418;&#24577;&#26234;&#33021;&#30340;&#25903;&#26609;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#37319;&#29992;&#24102;&#26377;&#22122;&#22768;&#31070;&#32463;&#20803;&#21160;&#21147;&#23398;&#30340;&#33033;&#20914;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#26174;&#31034;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#29702;&#35770;&#19978;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;NDL&#20026;&#20195;&#29702;&#26799;&#24230;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#12290;&#36890;&#36807;&#23558;&#21508;&#31181;SNN&#26550;&#26500;&#21644;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#30830;&#23450;&#24615;SNNs&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#34920;&#29616;&#21147;&#30340;&#21516;&#26102;&#65292;&#21160;&#24577;&#20943;&#23569;&#26080;&#25928;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15805</link><description>&lt;p&gt;
&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#29992;&#20110;&#39640;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. (arXiv:2305.15805v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#34920;&#29616;&#21147;&#30340;&#21516;&#26102;&#65292;&#21160;&#24577;&#20943;&#23569;&#26080;&#25928;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#29992;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#38590;&#20197;&#25193;&#23637;&#21040;&#38271;&#24207;&#21015;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#24037;&#20316;&#35797;&#22270;&#20943;&#23569;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#22823;&#22810;&#25968;LLM&#20173;&#28982;&#22312;&#25152;&#26377;&#26631;&#35760;&#23545;&#20043;&#38388;&#37319;&#29992;&#27880;&#24847;&#23618;&#65292;&#20174;&#32780;&#20135;&#29983;&#20108;&#27425;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#26469;&#21160;&#24577;&#20462;&#21098;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21487;&#23398;&#20064;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30830;&#23450;&#21738;&#20123;&#26080;&#20851;&#30340;&#26631;&#35760;&#21487;&#20197;&#20174;&#19978;&#19979;&#25991;&#20013;&#21024;&#38500;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#35299;&#20915;&#20102;&#24615;&#33021;&#38382;&#39064;&#65292;&#32780;&#19988;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20026;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21098;&#26525;&#24378;&#24230;&#21487;&#20197;&#30001;&#31232;&#30095;&#24230;&#21442;&#25968;&#25351;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity para
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;&#26032;&#26041;&#27861; - &#32622;&#20449;&#26368;&#20248;&#36755;&#36816;(COT)&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#22522;&#20110;&#32463;&#39564;&#30340;&#21464;&#20307; - &#24102;&#38376;&#38480;&#30340;&#32622;&#20449;&#26368;&#20248;&#36755;&#36816;(COTT)&#65292;&#23427;&#20204;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#20272;&#35745;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#20266;&#26631;&#31614;&#36716;&#31227;&#35823;&#24046;&#26102;&#12290;</title><link>http://arxiv.org/abs/2305.15640</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#34920;&#24449;&#21306;&#20998;&#20110;&#20998;&#24067;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Characterizing Out-of-Distribution Error via Optimal Transport. (arXiv:2305.15640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;&#26032;&#26041;&#27861; - &#32622;&#20449;&#26368;&#20248;&#36755;&#36816;(COT)&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#22522;&#20110;&#32463;&#39564;&#30340;&#21464;&#20307; - &#24102;&#38376;&#38480;&#30340;&#32622;&#20449;&#26368;&#20248;&#36755;&#36816;(COTT)&#65292;&#23427;&#20204;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#20272;&#35745;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#20266;&#26631;&#31614;&#36716;&#31227;&#35823;&#24046;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#37096;&#32626;&#20013;&#65292;&#27809;&#22312;&#20998;&#24067;(out-of-distribution)&#30340;&#25968;&#25454;&#23545;&#27169;&#22411;&#25552;&#20986;&#20102;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#22240;&#27492;&#39044;&#27979;&#27169;&#22411;&#22312;&#27809;&#26631;&#31614;&#30340;o
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) data poses serious challenges in deployed machine learning models, so methods of predicting a model's performance on OOD data without labels are important for machine learning safety. While a number of methods have been proposed by prior work, they often underestimate the actual error, sometimes by a large margin, which greatly impacts their applicability to real tasks. In this work, we identify pseudo-label shift, or the difference between the predicted and true OOD label distributions, as a key indicator to this underestimation. Based on this observation, we introduce a novel method for estimating model performance by leveraging optimal transport theory, Confidence Optimal Transport (COT), and show that it provably provides more robust error estimates in the presence of pseudo-label shift. Additionally, we introduce an empirically-motivated variant of COT, Confidence Optimal Transport with Thresholding (COTT), which applies thresholding to the individual tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27714;&#35299;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#35299;&#20915;&#25193;&#25955;ODE&#38382;&#39064;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#31283;&#23450;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20013;&#37319;&#26679;&#39640;&#36136;&#37327;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2305.15357</link><description>&lt;p&gt;
&#36890;&#36807;&#27714;&#35299;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#35299;&#20915;&#25193;&#25955;ODE&#38382;&#39064;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution. (arXiv:2305.15357v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27714;&#35299;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#35299;&#20915;&#25193;&#25955;ODE&#38382;&#39064;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#31283;&#23450;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20013;&#37319;&#26679;&#39640;&#36136;&#37327;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25193;&#25955;&#27169;&#22411;&#21453;&#21521;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#38543;&#26426;&#24615;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#22312;&#27599;&#27425;&#37319;&#26679;&#26102;&#24615;&#33021;&#27874;&#21160;&#24456;&#22823;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#23569;&#37327;&#37325;&#26032;&#37319;&#26679;&#27493;&#39588;&#30340;&#37319;&#26679;&#22120;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#36825;&#31181;&#22266;&#26377;&#38543;&#26426;&#24615;&#23548;&#33268;&#20854;&#26080;&#25928;&#21644;&#19981;&#31283;&#23450;&#65292;&#20351;&#29992;&#25143;&#38590;&#20197;&#20445;&#35777;&#36229;&#20998;&#36776;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36825;&#31181;&#38543;&#26426;&#24615;&#35270;&#20026;&#19968;&#31181;&#26426;&#36935;&#65306;&#20840;&#38754;&#20998;&#26512;&#21644;&#21033;&#29992;&#23427;&#23548;&#33268;&#20102;&#26500;&#24314;&#19968;&#31181;&#26377;&#25928;&#30340;&#21363;&#25554;&#21363;&#29992;&#37319;&#26679;&#26041;&#27861;&#65292;&#20855;&#26377;&#28508;&#21147;&#20351;&#19968;&#31995;&#21015;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#21463;&#30410;&#12290;&#26356;&#35814;&#32454;&#22320;&#35828;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#27714;&#35299;&#25193;&#25955;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;&#25193;&#25955;ODE&#65289;&#21644;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#65288;BC&#65289;&#65292;&#31283;&#23450;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20013;&#37319;&#26679;&#39640;&#36136;&#37327;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#24182;&#20998;&#26512;&#20854;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models, as a kind of powerful generative model, have given impressive results on image super-resolution (SR) tasks. However, due to the randomness introduced in the reverse process of diffusion models, the performances of diffusion-based SR models are fluctuating at every time of sampling, especially for samplers with few resampled steps. This inherent randomness of diffusion models results in ineffectiveness and instability, making it challenging for users to guarantee the quality of SR results. However, our work takes this randomness as an opportunity: fully analyzing and leveraging it leads to the construction of an effective plug-and-play sampling method that owns the potential to benefit a series of diffusion-based SR methods. More in detail, we propose to steadily sample high-quality SR images from pretrained diffusion-based SR models by solving diffusion ordinary differential equations (diffusion ODEs) with optimal boundary conditions (BCs) and analyze the characterist
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#21487;&#22797;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26159;&#25511;&#21046;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#27491;&#24335;&#30340;&#21487;&#22797;&#21046;&#24615;&#32467;&#26524;</title><link>http://arxiv.org/abs/2305.15284</link><description>&lt;p&gt;
&#21487;&#22797;&#29616;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Replicable Reinforcement Learning. (arXiv:2305.15284v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#21487;&#22797;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26159;&#25511;&#21046;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#27491;&#24335;&#30340;&#21487;&#22797;&#21046;&#24615;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#12289;&#34892;&#20026;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#65292;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#23548;&#33268;&#20102;&#31639;&#27861;&#26694;&#26550;&#30340;&#24418;&#25104;&#65292;&#21363;&#35201;&#27714;&#31639;&#27861;&#22312;&#20174;&#30456;&#21516;&#30340;&#24213;&#23618;&#20998;&#24067;&#25552;&#21462;&#30340;&#20004;&#20010;&#19981;&#21516;&#26679;&#26412;&#19978;&#36816;&#34892;&#26102;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65288;&#27010;&#29575;&#39640;&#65289;&#12290;&#34429;&#28982;&#20173;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#30340;&#35768;&#22810;&#22522;&#26412;&#20219;&#21153;&#65292;&#21253;&#25324;&#32479;&#35745;&#26597;&#35810;&#23398;&#20064;&#12289;&#37325;&#35201;&#39033;&#38382;&#39064;&#21644;&#20998;&#24067;&#27979;&#35797;&#65292;&#37117;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#21487;&#35777;&#26126;&#21487;&#22797;&#29616;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#21487;&#22797;&#29616;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#24182;&#34892;&#20540;&#36845;&#20195;&#30340;&#21487;&#35777;&#22797;&#21046;&#31639;&#27861;&#20197;&#21450;&#19968;&#20010;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#21487;&#35777;&#22797;&#21046;&#30340;R-max&#12290;&#36825;&#26159;&#25511;&#21046;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#27491;&#24335;&#21487;&#22797;&#21046;&#24615;&#32467;&#26524;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#25209;&#37327;&#23398;&#20064;&#29615;&#22659;&#20013;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#22797;&#21046;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The replicability crisis in the social, behavioral, and data sciences has led to the formulation of algorithm frameworks for replicability -- i.e., a requirement that an algorithm produce identical outputs (with high probability) when run on two different samples from the same underlying distribution. While still in its infancy, provably replicable algorithms have been developed for many fundamental tasks in machine learning and statistics, including statistical query learning, the heavy hitters problem, and distribution testing. In this work we initiate the study of replicable reinforcement learning, providing a provably replicable algorithm for parallel value iteration, and a provably replicable version of R-max in the episodic setting. These are the first formal replicability results for control problems, which present different challenges for replication than batch learning settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#20351;&#29992;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#21442;&#25968;&#25968;&#25454;&#23545;&#36710;&#36742;&#36827;&#34892;&#35780;&#20998;&#39044;&#27979;&#65292;&#22686;&#21152;&#20102;&#25968;&#25454;&#30340;&#23436;&#25972;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15218</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#22312;&#36710;&#36742;&#35780;&#20998;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#22522;&#20110;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#21442;&#25968;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Machine Learning for Vehicle Rating Predictions Using Image, Text, and Parametric Data. (arXiv:2305.15218v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#20351;&#29992;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#21442;&#25968;&#25968;&#25454;&#23545;&#36710;&#36742;&#36827;&#34892;&#35780;&#20998;&#39044;&#27979;&#65292;&#22686;&#21152;&#20102;&#25968;&#25454;&#30340;&#23436;&#25972;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#36710;&#36742;&#35780;&#20998;&#39044;&#27979;&#21487;&#20197;&#24110;&#21161;&#35774;&#35745;&#21644;&#37197;&#32622;&#22909;&#30340;&#36710;&#36742;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24335;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#20174;&#36710;&#36742;&#21442;&#25968;&#12289;&#25991;&#26412;&#25551;&#36848;&#21644;&#22270;&#20687;&#20013;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#39044;&#27979;&#20116;&#31181;&#36710;&#36742;&#35780;&#20998;&#65292;&#21253;&#25324;&#24635;&#20998;&#21644;&#35780;&#20215;&#20998;&#25968;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate vehicle rating prediction can facilitate designing and configuring good vehicles. This prediction allows vehicle designers and manufacturers to optimize and improve their designs in a timely manner, enhance their product performance, and effectively attract consumers. However, most of the existing data-driven methods rely on data from a single mode, e.g., text, image, or parametric data, which results in a limited and incomplete exploration of the available information. These methods lack comprehensive analyses and exploration of data from multiple modes, which probably leads to inaccurate conclusions and hinders progress in this field. To overcome this limitation, we propose a multi-modal learning model for more comprehensive and accurate vehicle rating predictions. Specifically, the model simultaneously learns features from the parametric specifications, text descriptions, and images of vehicles to predict five vehicle rating scores, including the total score, critics score,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24341;&#20837;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#33410;&#30465;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26159;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#27425;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2305.15151</link><description>&lt;p&gt;
&#30693;&#35782;&#35774;&#35745;&#65306;&#36890;&#36807;&#30693;&#35782;&#25552;&#28860;&#25512;&#21160;&#34507;&#30333;&#36136;&#35774;&#35745;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement. (arXiv:2305.15151v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24341;&#20837;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#33410;&#30465;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26159;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#27425;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#34507;&#30333;&#36136;&#35774;&#35745;&#20013;&#65292;&#23547;&#25214;&#25240;&#21472;&#20026;&#25152;&#26399;&#26395;&#32467;&#26500;&#30340;&#27688;&#22522;&#37240;&#24207;&#21015;&#24050;&#32463;&#21462;&#24471;&#20102;&#31454;&#20105;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#30053;&#20102;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#37325;&#35201;&#24615;&#65292;&#26410;&#33021;&#35206;&#30422;&#24191;&#27867;&#30340;&#34507;&#30333;&#36136;&#31354;&#38388;&#65292;&#24182;&#19988;&#27809;&#26377;&#34701;&#20837;&#24120;&#35265;&#30340;&#34507;&#30333;&#36136;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#26469;&#33410;&#30465;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;CATH&#12289;TS50&#21644;TS500&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#30693;&#35782;&#35774;&#35745;&#26041;&#27861;&#22312;CATH&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;PiFold&#26041;&#27861;&#32422;9&#65285;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30693;&#35782;&#35774;&#35745;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown competitive performance in protein design that aims to find the amino acid sequence folding into the desired structure. However, most of them disregard the importance of predictive confidence, fail to cover the vast protein space, and do not incorporate common protein knowledge. After witnessing the great success of pretrained models on diverse protein-related tasks and the fact that recovery is highly correlated with confidence, we wonder whether this knowledge can push the limits of protein design further. As a solution, we propose a knowledge-aware module that refines low-quality residues. We also introduce a memory-retrieval mechanism to save more than 50\% of the training time. We extensively evaluate our proposed method on the CATH, TS50, and TS500 datasets and our results show that our Knowledge-Design method outperforms the previous PiFold method by approximately 9\% on the CATH dataset. Specifically, Knowledge-Design is the first method that achieves 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#12290;</title><link>http://arxiv.org/abs/2305.14749</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-State RNA Design with Geometric Multi-Graph Neural Networks. (arXiv:2305.14749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;RNA&#35774;&#35745;&#22312;&#21512;&#25104;&#29983;&#29289;&#23398;&#21644;&#27835;&#30103;&#24320;&#21457;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;RNA&#22810;&#26679;&#30340;&#29983;&#29289;&#23398;&#21151;&#33021;&#30340;&#22522;&#30784;&#26159;&#23427;&#30340;&#26500;&#35937;&#28789;&#27963;&#24615;&#65292;&#20351;&#21333;&#19968;&#24207;&#21015;&#33021;&#22815;&#37319;&#29992;&#22810;&#31181;&#19981;&#21516;&#30340;&#19977;&#32500;&#32467;&#26500;&#29366;&#24577;&#12290;&#30446;&#21069;&#65292;&#35745;&#31639;&#29983;&#29289;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#32463;&#24120;&#34987;&#25552;&#20986;&#20026;&#36870;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#37319;&#29992;&#21333;&#19968;&#39044;&#26399;&#32467;&#26500;&#26500;&#35937;&#26469;&#35774;&#35745;&#24207;&#21015;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;gRNAde&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#19968;&#32452;&#19977;&#32500;RNA&#39592;&#26550;&#32467;&#26500;&#25805;&#20316;&#30340;&#20960;&#20309;RNA&#35774;&#35745;&#27969;&#31243;&#65292;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#19977;&#32500;RNA&#35774;&#35745;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;gRNAde&#30340;&#25928;&#29992;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/chaitjo/geometric-rna-design&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational RNA design has broad applications across synthetic biology and therapeutic development. Fundamental to the diverse biological functions of RNA is its conformational flexibility, enabling single sequences to adopt a variety of distinct 3D states. Currently, computational biomolecule design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired structural conformation. In this work, we propose gRNAde, a geometric RNA design pipeline that operates on sets of 3D RNA backbone structures to explicitly account for and reflect RNA conformational diversity in its designs. We demonstrate the utility of gRNAde for improving native sequence recovery over single-state approaches on a new large-scale 3D RNA design dataset, especially for multi-state and structurally diverse RNAs. Our code is available at https://github.com/chaitjo/geometric-rna-design
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#23558;&#37327;&#23376;&#32858;&#31867;&#24212;&#29992;&#20110;&#22270;&#32467;&#26500;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;GPU&#30340;&#24182;&#34892;&#31639;&#27861;&#26469;&#35745;&#31639;&#28508;&#22312;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14641</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;GPU&#30340;&#24182;&#34892;&#31639;&#27861;&#36827;&#34892;&#22270;&#20998;&#26512;&#65306;&#37327;&#23376;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Graphy Analysis Using a GPU-based Parallel Algorithm: Quantum Clustering. (arXiv:2305.14641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#23558;&#37327;&#23376;&#32858;&#31867;&#24212;&#29992;&#20110;&#22270;&#32467;&#26500;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;GPU&#30340;&#24182;&#34892;&#31639;&#27861;&#26469;&#35745;&#31639;&#28508;&#22312;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#37327;&#23376;&#32858;&#31867;&#24212;&#29992;&#20110;&#22270;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;&#12290;&#37327;&#23376;&#32858;&#31867;&#65288;QC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#28508;&#22312;&#20989;&#25968;&#26469;&#30830;&#23450;&#32858;&#31867;&#20013;&#24515;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#25214;&#21040;&#32858;&#31867;&#20013;&#24515;&#12290;GPU&#24182;&#34892;&#21270;&#29992;&#20110;&#35745;&#31639;&#28508;&#22312;&#20540;&#12290;&#25105;&#20204;&#36824;&#23545;&#20116;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;&#22235;&#20010;&#25351;&#26631;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;$\sigma$&#23545;&#23454;&#39564;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The article introduces a new method for applying Quantum Clustering to graph structures. Quantum Clustering (QC) is a novel density-based unsupervised learning method that determines cluster centers by constructing a potential function. In this method, we use the Graph Gradient Descent algorithm to find the centers of clusters. GPU parallelization is utilized for computing potential values. We also conducted experiments on five widely used datasets and evaluated using four indicators. The results show superior performance of the method. Finally, we discuss the influence of $\sigma$ on the experimental results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#22312;&#38899;&#39057;&#25968;&#25454;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14032</link><description>&lt;p&gt;
&#24102;&#26377;&#38899;&#39057;&#20809;&#35889;&#21464;&#25442;&#22120;&#30340; Patch-Mix &#23545;&#27604;&#23398;&#20064;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification. (arXiv:2305.14032v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#22312;&#38899;&#39057;&#25968;&#25454;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21628;&#21560;&#22768;&#21253;&#21547;&#26089;&#26399;&#35786;&#26029;&#33268;&#21629;&#32954;&#37096;&#30142;&#30149;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#33258; COVID-19 &#30123;&#24773;&#20197;&#26469;&#65292;&#22522;&#20110;&#30005;&#23376;&#21548;&#35786;&#22120;&#30340;&#26080;&#25509;&#35302;&#21307;&#30103;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#20026;&#27492;&#65292;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35786;&#26029;&#32954;&#37096;&#30142;&#30149;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;&#31232;&#32570;&#65292;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#21628;&#21560;&#38899;&#20998;&#31867;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340; Patch-Mix &#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#28151;&#21512;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#30340;&#34917;&#19969;&#65292;&#19982; Audio Spectrogram Transformer (AST) &#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340; Patch-Mix &#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21306;&#20998;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#28151;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; ICBHI &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#39640;&#24471;&#20998; 4.08%&#12290;
&lt;/p&gt;
&lt;p&gt;
Respiratory sound contains crucial information for the early diagnosis of fatal lung diseases. Since the COVID-19 pandemic, there has been a growing interest in contact-free medical care based on electronic stethoscopes. To this end, cutting-edge deep learning models have been developed to diagnose lung diseases; however, it is still challenging due to the scarcity of medical data. In this study, we demonstrate that the pretrained model on large-scale visual and audio datasets can be generalized to the respiratory sound classification task. In addition, we introduce a straightforward Patch-Mix augmentation, which randomly mixes patches between different samples, with Audio Spectrogram Transformer (AST). We further propose a novel and effective Patch-Mix Contrastive Learning to distinguish the mixed representations in the latent space. Our method achieves state-of-the-art performance on the ICBHI dataset, outperforming the prior leading score by an improvement of 4.08%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DistroFair&#30340;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#26816;&#27979;&#21040;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;</title><link>http://arxiv.org/abs/2305.13935</link><description>&lt;p&gt;
&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Distribution-aware Fairness Test Generation. (arXiv:2305.13935v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DistroFair&#30340;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#26816;&#27979;&#21040;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#39564;&#35777;&#22270;&#20687;&#35782;&#21035;&#36719;&#20214;&#20013;&#30340;&#32452;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65288;&#31216;&#20026;DistroFair&#65289;&#65292;&#36890;&#36807;&#23558;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23545;&#35937;&#24341;&#20837;&#21040;&#22270;&#20687;&#35782;&#21035;&#22120;&#20013;&#65292;&#36890;&#36807;&#19977;&#31181;&#35821;&#20041;&#20445;&#30041;&#22270;&#20687;&#21464;&#25442; - &#23545;&#35937;&#21024;&#38500;&#65292;&#23545;&#35937;&#25554;&#20837;&#21644;&#23545;&#35937;&#26059;&#36716;&#26469;&#31995;&#32479;&#24615;&#22320;&#26292;&#38706;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#30340;&#31867;&#32423;&#21035;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#30693;&#21517;&#25968;&#25454;&#38598;&#65288;CityScapes&#21644;MS-COCO&#65289;&#21644;&#19977;&#20010;&#20027;&#35201;&#30340;&#21830;&#19994;&#22270;&#20687;&#35782;&#21035;&#36719;&#20214;&#65288;&#21363;Amazon Rekognition&#65292;Google Cloud Vision&#21644;Azure&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#23545;DistroFair&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;DistroFair&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#65292;&#32422;&#26377;21&#65285;&#36890;&#36807;&#30495;&#23454;&#26631;&#20934;&#25110;&#20803;&#27979;&#35797;&#26631;&#20934;&#26174;&#38706;&#20986;&#20102;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses how to validate group fairness in image recognition software. We propose a distribution-aware fairness testing approach (called DistroFair) that systematically exposes class-level fairness violations in image classifiers via a synergistic combination of out-of-distribution (OOD) testing and semantic-preserving image mutation. DistroFair automatically learns the distribution (e.g., number/orientation) of objects in a set of images. Then it systematically mutates objects in the images to become OOD using three semantic-preserving image mutations -- object deletion, object insertion and object rotation. We evaluate DistroFair using two well-known datasets (CityScapes and MS-COCO) and three major, commercial image recognition software (namely, Amazon Rekognition, Google Cloud Vision and Azure Computer Vision). Results show that about 21% of images generated by DistroFair reveal class-level fairness violations using either ground truth or metamorphic oracles. DistroFair 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30005;&#24359;&#28809;&#29076;&#28195;&#30005;&#23548;&#29575;&#30340;&#26041;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#20102;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#21644;&#26631;&#20934;&#20559;&#24046;&#35745;&#31639;&#21450;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.13519</link><description>&lt;p&gt;
&#21457;&#23637;&#29992;&#20110;&#39044;&#27979;&#30789;&#37240;&#30416;&#30005;&#23548;&#29575;&#30340;&#38750;&#32447;&#24615;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Development of Non-Linear Equations for Predicting Electrical Conductivity in Silicates. (arXiv:2305.13519v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30005;&#24359;&#28809;&#29076;&#28195;&#30005;&#23548;&#29575;&#30340;&#26041;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#20102;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#21644;&#26631;&#20934;&#20559;&#24046;&#35745;&#31639;&#21450;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23548;&#29575;&#22312;&#30005;&#24359;&#28809;(EAF)&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#23427;&#19982;&#29076;&#28195;&#30340;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#33021;&#37327;&#25439;&#22833;&#21644;&#20302;&#25928;&#29575;&#12290;&#25968;&#23398;&#24314;&#27169;&#26377;&#21161;&#20110;&#29702;&#35299;&#36825;&#31181;&#29616;&#35937;&#30340;&#34892;&#20026;&#65292;&#30740;&#31350;&#32773;&#20204;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;EAF&#29076;&#28195;&#30340;&#30005;&#23548;&#29575;&#12290;&#26368;&#20339;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#38544;&#34255;&#23618;&#26377;100&#20010;&#31070;&#32463;&#20803;&#65292;&#20351;&#29992;6&#20010;&#39044;&#27979;&#21464;&#37327;&#21644;&#19968;&#20010;&#39044;&#27979;&#21464;&#37327;&#30005;&#23548;&#29575;&#12290;&#35745;&#31639;&#20102;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#21644;&#32477;&#23545;&#35823;&#24046;&#26631;&#20934;&#20559;&#24046;&#65292;&#24182;&#36827;&#34892;&#20102;&#25935;&#24863;&#24615;&#20998;&#26512;&#26469;&#23545;&#27599;&#20010;&#39044;&#27979;&#21464;&#37327;&#30340;&#24433;&#21709;&#19982;&#39044;&#27979;&#21464;&#37327;&#36827;&#34892;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical conductivity is of fundamental importance in electric arc furnaces (EAF) and the interaction of this phenomenon with the process slag results in energy losses and low optimization. As mathematical modeling helps in understanding the behavior of phenomena and it was used to predict the electrical conductivity of EAF slags through artificial neural networks. The best artificial neural network had 100 neurons in the hidden layer, with 6 predictor variables and the predicted variable, electrical conductivity. Mean absolute error and standard deviation of absolute error were calculated, and sensitivity analysis was performed to correlate the effect of each predictor variable with the predicted variable.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20174;&#21477;&#32534;&#36753;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;SQL&#30340;&#35821;&#35328;&#27169;&#22411;&#32416;&#38169;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;SQL&#26597;&#35810;&#34920;&#31034;&#25913;&#36827;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#30830;&#21305;&#37197;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#20102;2.4-6.5&#65292;&#26368;&#22810;&#25552;&#39640;4.3&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.13073</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#30340;&#35821;&#35328;&#27169;&#22411;&#32416;&#38169;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL Error Correction with Language Models of Code. (arXiv:2305.13073v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20174;&#21477;&#32534;&#36753;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;SQL&#30340;&#35821;&#35328;&#27169;&#22411;&#32416;&#38169;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;SQL&#26597;&#35810;&#34920;&#31034;&#25913;&#36827;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#30830;&#21305;&#37197;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#20102;2.4-6.5&#65292;&#26368;&#22810;&#25552;&#39640;4.3&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#20173;&#19981;&#22815;&#20934;&#30830;&#20197;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#26500;&#24314;&#33258;&#21160;&#25991;&#26412;&#21040;SQL&#32416;&#38169;&#27169;&#22411;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#21333;&#35789;&#23618;&#38754;&#30340;&#32534;&#36753;&#32570;&#20047;&#19978;&#19979;&#25991;&#24182;&#19988;&#26377;&#26102;&#19981;&#26126;&#30830;&#65292;&#22240;&#27492;&#25552;&#20986;&#26500;&#24314;&#20174;&#21477;&#32534;&#36753;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#22823;&#22810;&#25968;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#19987;&#38376;&#39044;&#35757;&#32451;SQL&#65292;&#20294;&#23427;&#20204;&#29087;&#24713;Python&#31561;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#24120;&#35265;&#25968;&#25454;&#32467;&#26500;&#21644;&#20854;&#25805;&#20316;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SQL&#26597;&#35810;&#34920;&#31034;&#21450;&#20854;&#32534;&#36753;&#26041;&#27861;&#65292;&#26356;&#31526;&#21512;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#30340;&#38169;&#35823;&#32416;&#38169;&#27169;&#22411;&#25552;&#39640;&#20102;&#19981;&#21516;&#35299;&#26512;&#22120;&#30340;&#31934;&#30830;&#21305;&#37197;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#20102;2.4-6.5&#65292;&#24182;&#33719;&#24471;&#20102;&#20004;&#20010;&#24378;&#22522;&#32447;&#30340;&#32477;&#23545;&#25913;&#36827;&#26368;&#22810;4.3&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/OSU-NLP-Group/Auto-SQL-Correction &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in text-to-SQL parsing, current semantic parsers are still not accurate enough for practical use. In this paper, we investigate how to build automatic text-to-SQL error correction models. Noticing that token-level edits are out of context and sometimes ambiguous, we propose building clause-level edit models instead. Besides, while most language models of code are not specifically pre-trained for SQL, they know common data structures and their operations in programming languages such as Python. Thus, we propose a novel representation for SQL queries and their edits that adheres more closely to the pre-training corpora of language models of code. Our error correction model improves the exact set match accuracy of different parsers by 2.4-6.5 and obtains up to 4.3 point absolute improvement over two strong baselines. Our code and data are available at https://github.com/OSU-NLP-Group/Auto-SQL-Correction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#24067;&#23616;&#36139;&#27665;&#31391;&#36947;&#36335;&#12290;&#36890;&#36807;&#25513;&#30721;&#31574;&#30053;&#20248;&#21270;&#65292;&#21487;&#20351;&#21487;&#36798;&#24615;&#25552;&#39640;14.3&#65285;&#65292;&#23545;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.13060</link><description>&lt;p&gt;
&#20511;&#21161;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36139;&#27665;&#31391;&#36947;&#36335;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Road Planning for Slums via Deep Reinforcement Learning. (arXiv:2305.13060v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#24067;&#23616;&#36139;&#27665;&#31391;&#36947;&#36335;&#12290;&#36890;&#36807;&#25513;&#30721;&#31574;&#30053;&#20248;&#21270;&#65292;&#21487;&#20351;&#21487;&#36798;&#24615;&#25552;&#39640;14.3&#65285;&#65292;&#23545;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#30334;&#19975;&#36139;&#27665;&#31391;&#23621;&#27665;&#30001;&#20110;&#36139;&#27665;&#31391;&#20869;&#19981;&#36275;&#30340;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#32780;&#36973;&#21463;&#22478;&#24066;&#26381;&#21153;&#26080;&#27861;&#35775;&#38382;&#30340;&#22256;&#22659;&#65292;&#32780;&#36139;&#27665;&#31391;&#36947;&#36335;&#35268;&#21010;&#23545;&#22478;&#24066;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#37325;&#32452;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#35201;&#20040;&#32791;&#26102;&#65292;&#19981;&#33021;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#36139;&#27665;&#31391;&#65292;&#35201;&#20040;&#22312;&#21487;&#36798;&#24615;&#21644;&#24314;&#35774;&#25104;&#26412;&#26041;&#38754;&#20135;&#29983;&#27425;&#20248;&#30340;&#36947;&#36335;&#35268;&#21010;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#24067;&#23616;&#36139;&#27665;&#31391;&#36947;&#36335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#33719;&#36139;&#27665;&#31391;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#36873;&#25321;&#35745;&#21010;&#36947;&#36335;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#25513;&#30721;&#31574;&#30053;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36830;&#25509;&#36139;&#27665;&#31391;&#22320;&#28857;&#30340;&#36947;&#36335;&#35268;&#21010;&#65292;&#20197;&#26368;&#23567;&#30340;&#24314;&#35774;&#25104;&#26412;&#12290;&#23545;&#19981;&#21516;&#22269;&#23478;&#30340;&#30495;&#23454;&#36139;&#27665;&#31391;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20351;&#21487;&#36798;&#24615;&#25552;&#39640;14.3&#65285;&#65292;&#23545;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Millions of slum dwellers suffer from poor accessibility to urban services due to inadequate road infrastructure within slums, and road planning for slums is critical to the sustainable development of cities. Existing re-blocking or heuristic methods are either time-consuming which cannot generalize to different slums, or yield sub-optimal road plans in terms of accessibility and construction costs. In this paper, we present a deep reinforcement learning based approach to automatically layout roads for slums. We propose a generic graph model to capture the topological structure of a slum, and devise a novel graph neural network to select locations for the planned roads. Through masked policy optimization, our model can generate road plans that connect places in a slum at minimal construction costs. Extensive experiments on real-world slums in different countries verify the effectiveness of our model, which can significantly improve accessibility by 14.3% against existing baseline metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13030</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
Adaptive action supervision in reinforcement learning from real-world multi-agent demonstrations. (arXiv:2305.13030v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#23545;&#30495;&#23454;&#19990;&#30028;&#29983;&#29289;&#22810;&#26234;&#33021;&#20307;&#36827;&#34892;&#24314;&#27169;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#22312;&#32593;&#32476;&#31354;&#38388;&#20013;&#29983;&#25104;&#28789;&#27963;&#21644;&#22810;&#26679;&#21270;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#65307;&#28982;&#32780;&#65292;&#22312;&#24314;&#27169;&#30495;&#23454;&#19990;&#30028;&#29983;&#29289;&#22810;&#26234;&#33021;&#20307;&#26102;&#65292;&#22312;&#28304;&#65288;&#21363;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65289;&#21644;&#30446;&#26631;&#65288;&#21363; RL &#30340;&#32593;&#32476;&#31354;&#38388;&#65289;&#20043;&#38388;&#23384;&#22312;&#22495;&#24046;&#24322;&#65292;&#24182;&#19988;&#28304;&#29615;&#22659;&#21442;&#25968;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30340;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892; RL &#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#32467;&#21512; RL &#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#30340;&#28436;&#31034;&#21160;&#20316;&#26469;&#22312; RL &#20013;&#21033;&#29992;&#26410;&#30693;&#28304;&#21160;&#24577;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#20026;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling of real-world biological multi-agents is a fundamental problem in various scientific and engineering fields. Reinforcement learning (RL) is a powerful framework to generate flexible and diverse behaviors in cyberspace; however, when modeling real-world biological multi-agents, there is a domain gap between behaviors in the source (i.e., real-world data) and the target (i.e., cyberspace for RL), and the source environment parameters are usually unknown. In this paper, we propose a method for adaptive action supervision in RL from real-world demonstrations in multi-agent scenarios. We adopt an approach that combines RL and supervised learning by selecting actions of demonstrations in RL based on the minimum distance of dynamic time warping for utilizing the information of the unknown source dynamics. This approach can be easily applied to many existing neural network architectures and provide us with an RL model balanced between reproducibility as imitation and generalization ab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;ReLU&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20004;&#23618;&#27169;&#22411;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.12467</link><description>&lt;p&gt;
&#29702;&#35299;ReLU&#32593;&#32476;&#30340;&#22810;&#38454;&#27573;&#20248;&#21270;&#21160;&#24577;&#21644;&#20016;&#23500;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks. (arXiv:2305.12467v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;ReLU&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20004;&#23618;&#27169;&#22411;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#32463;&#24120;&#34920;&#29616;&#20986;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#12290;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#21644;&#25439;&#22833;&#30340;&#38750;&#20984;&#24615;&#20026;&#29702;&#35770;&#20998;&#26512;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20108;&#23618;ReLU&#32593;&#32476;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#25551;&#36848;&#12290;&#22312;&#36825;&#31181;&#29305;&#23450;&#30340;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25429;&#33719;&#20102;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#21040;&#26368;&#32456;&#25910;&#25947;&#30340;&#25972;&#20010;&#20248;&#21270;&#36807;&#31243;&#12290;&#23613;&#31649;&#25105;&#20204;&#30740;&#31350;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#30456;&#23545;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;&#29305;&#23450;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;&#20063;&#21487;&#20197;&#34987;&#31934;&#30830;&#22320;&#35782;&#21035;&#21644;&#29702;&#35770;&#19978;&#25429;&#33719;&#65292;&#20363;&#22914;...
&lt;/p&gt;
&lt;p&gt;
The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this specific setting, our analysis captures the whole optimization process starting from random initialization to final convergence. Despite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend. Specific nonlinear behaviors can also be precisely identified and captured theoretically, such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Segment Training&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#27835;&#27861;&#20801;&#35768;&#20351;&#29992;&#24658;&#23450;&#30340;&#20869;&#23384;&#28040;&#32791;&#26469;&#23398;&#20064;&#22823;&#22411;&#22270;&#24418;&#23646;&#24615;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#34987;&#35780;&#20272;&#22312;&#20960;&#39033;&#22823;&#22411;&#22270;&#24418;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#20960;&#20010;&#26368;&#20808;&#36827;&#22522;&#20934;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12322</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#24418;&#27573;&#35757;&#32451;&#23398;&#20064;&#22823;&#22411;&#22270;&#24418;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Large Graph Property Prediction via Graph Segment Training. (arXiv:2305.12322v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Segment Training&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#27835;&#27861;&#20801;&#35768;&#20351;&#29992;&#24658;&#23450;&#30340;&#20869;&#23384;&#28040;&#32791;&#26469;&#23398;&#20064;&#22823;&#22411;&#22270;&#24418;&#23646;&#24615;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#34987;&#35780;&#20272;&#22312;&#20960;&#39033;&#22823;&#22411;&#22270;&#24418;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#20960;&#20010;&#26368;&#20808;&#36827;&#22522;&#20934;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39044;&#27979;&#22823;&#22411;&#22270;&#30340;&#23646;&#24615;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#39044;&#27979;&#37117;&#38656;&#35201;&#25972;&#20010;&#22270;&#30340;&#30693;&#35782;&#65292;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#29992;&#30340;&#20869;&#23384;&#37327;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Segment Training&#65288;GST&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#27835;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#24658;&#23450;&#30340;&#20869;&#23384;&#21344;&#29992;&#37327;&#26469;&#23398;&#20064;&#22823;&#22411;&#22270;&#24418;&#23646;&#24615;&#39044;&#27979;&#12290; GST&#39318;&#20808;&#23558;&#22823;&#22411;&#22270;&#24418;&#21010;&#20998;&#20026;&#27573;&#65292;&#28982;&#21518;&#36890;&#36807;&#35757;&#32451;&#36845;&#20195;&#20013;&#20165;&#23545;&#20960;&#20010;&#27573;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21382;&#21490;&#23884;&#20837;&#34920;&#26469;&#25913;&#36827;GST&#33539;&#20363;&#65292;&#20197;&#26377;&#25928;&#22320;&#33719;&#21462;&#26410;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#30340;&#27573;&#30340;&#23884;&#20837;&#12290;&#20026;&#20102;&#20943;&#36731;&#21382;&#21490;&#23884;&#20837;&#30340;&#36807;&#26102;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#27979;&#22836;&#20197;&#20462;&#22797;&#36755;&#20837;&#20998;&#24067;&#31227;&#20301;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#8220;Stale Embedding Dropout&#8221;&#26469;&#22312;&#35757;&#32451;&#26399;&#38388;&#38477;&#20302;&#20559;&#24046;&#65292;&#20174;&#32780;&#20002;&#24323;&#19968;&#20123;&#36807;&#26102;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#23545;&#22823;&#22411;&#22270;&#24418;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#21270;&#21512;&#29289;&#20998;&#31867;&#21644;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;GST-EFD&#65288;&#21253;&#21547;&#25152;&#26377;&#25216;&#26415;&#65289;&#20248;&#20110;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#65292;&#23384;&#20648;&#22120;&#28040;&#32791;&#21644;&#36816;&#34892;&#26102;&#25928;&#29575;&#26041;&#38754;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to predict properties of large graphs is challenging because each prediction requires the knowledge of an entire graph, while the amount of memory available during training is bounded. Here we propose Graph Segment Training (GST), a general framework that utilizes a divide-and-conquer approach to allow learning large graph property prediction with a constant memory footprint. GST first divides a large graph into segments and then backpropagates through only a few segments sampled per training iteration. We refine the GST paradigm by introducing a historical embedding table to efficiently obtain embeddings for segments not sampled for backpropagation. To mitigate the staleness of historical embeddings, we design two novel techniques. First, we finetune the prediction head to fix the input distribution shift. Second, we introduce Stale Embedding Dropout to drop some stale embeddings during training to reduce bias. We evaluate our complete method GST-EFD (with all the techniques 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;Transformer&#32467;&#26500;&#21644;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#20837;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.12095</link><description>&lt;p&gt;
&#20351;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20877;&#27425;&#21331;&#36234;&#65306;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer
&lt;/p&gt;
&lt;p&gt;
Make Transformer Great Again for Time Series Forecasting: Channel Aligned Robust Dual Transformer. (arXiv:2305.12095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;Transformer&#32467;&#26500;&#21644;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#20837;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;Transformer&#21644;MLP&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#20248;&#21183;&#12290;&#23613;&#31649;&#22312;NLP&#21644;CV&#26041;&#38754;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;MLP&#30456;&#27604;&#65292;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#30340;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;Transformer&#65292;&#21363;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#65288;CARD&#65289;&#65292;&#20197;&#35299;&#20915;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;CARD&#24341;&#20837;&#20102;&#21452;Transformer&#32467;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#25417;&#20449;&#21495;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#21644;&#22810;&#20010;&#21464;&#37327;&#22312;&#26102;&#38388;&#19978;&#30340;&#21160;&#24577;&#20381;&#36182;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#20943;&#36731;&#28508;&#22312;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;&#36825;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#22522;&#20110;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#39044;&#27979;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#23545;&#22810;&#20010;&#38271;&#26399;&#21644;&#30701;&#26399;&#39044;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;CARD&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated the great power of deep learning methods, particularly Transformer and MLP, for time series forecasting. Despite its success in NLP and CV, many studies found that Transformer is less effective than MLP for time series forecasting. In this work, we design a special Transformer, i.e., channel-aligned robust dual Transformer (CARD for short), that addresses key shortcomings of Transformer in time series forecasting. First, CARD introduces a dual Transformer structure that allows it to capture both temporal correlations among signals and dynamical dependence among multiple variables over time. Second, we introduce a robust loss function for time series forecasting to alleviate the potential overfitting issue. This new loss function weights the importance of forecasting over a finite horizon based on prediction uncertainties. Our evaluation of multiple long-term and short-term forecasting datasets demonstrates that CARD significantly outperforms state-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33258;&#21160;&#25915;&#20987;&#26694;&#26550;SneakyPrompt&#65292;&#20197;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#25628;&#32034;&#22791;&#36873;&#20196;&#29260;&#26469;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.12082</link><description>&lt;p&gt;
SneakyPrompt&#65306;&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters. (arXiv:2305.12082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33258;&#21160;&#25915;&#20987;&#26694;&#26550;SneakyPrompt&#65292;&#20197;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#25628;&#32034;&#22791;&#36873;&#20196;&#29260;&#26469;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#65292;&#22914;Stable Diffusion&#21644;DALL$\cdot$E 2&#31561;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#38382;&#39064;&#26159;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#65292;&#20363;&#22914;&#19982;&#26292;&#21147;&#21644;&#25104;&#20154;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#24120;&#35265;&#20570;&#27861;&#26159;&#37096;&#32626;&#25152;&#35859;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#22522;&#20110;&#25991;&#26412;&#25110;&#22270;&#20687;&#29305;&#24449;&#38459;&#27490;&#19981;&#23433;&#20840;&#20869;&#23481;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#27492;&#31867;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#21487;&#33021;&#32469;&#36807;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#25163;&#21160;&#23436;&#25104;&#24182;&#19987;&#38376;&#38024;&#23545;Stable Diffusion&#23448;&#26041;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;Stable Diffusion&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#32469;&#36807;&#27604;&#29575;&#20165;&#20026;23.51&#65285;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33258;&#21160;&#25915;&#20987;&#26694;&#26550;SneakyPrompt&#65292;&#20197;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#29616;&#23454;&#19990;&#30028;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#25628;&#32034;&#22791;&#36873;&#20196;&#29260;&#26469;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models such as Stable Diffusion and DALL$\cdot$E 2 have attracted much attention since their publication due to their wide application in the real world. One challenging problem of text-to-image generative models is the generation of Not-Safe-for-Work (NSFW) content, e.g., those related to violence and adult. Therefore, a common practice is to deploy a so-called safety filter, which blocks NSFW content based on either text or image features. Prior works have studied the possible bypass of such safety filters. However, existing works are largely manual and specific to Stable Diffusion's official safety filter. Moreover, the bypass ratio of Stable Diffusion's safety filter is as low as 23.51% based on our evaluation.  In this paper, we propose the first automated attack framework, called SneakyPrompt, to evaluate the robustness of real-world safety filters in state-of-the-art text-to-image generative models. Our key insight is to search for alternative tokens in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25216;&#26415;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#20013;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#19981;&#38169;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11586</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#34701;&#20837;&#19981;&#30830;&#23450;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Bayesian approach to Gaussian process regression with uncertain inputs. (arXiv:2305.11586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25216;&#26415;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#20013;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#19981;&#38169;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20165;&#20551;&#35774;&#27169;&#22411;&#35266;&#27979;&#25968;&#25454;&#30340;&#36755;&#20986;&#20855;&#26377;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#24314;&#27169;&#20551;&#35774;&#12289;&#27979;&#37327;&#35823;&#24046;&#31561;&#22240;&#32032;&#65292;&#35266;&#27979;&#25968;&#25454;&#30340;&#36755;&#20837;&#20301;&#32622;&#21487;&#33021;&#20063;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#21487;&#21464;&#24615;&#34701;&#20837;&#21040;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#12290;&#32771;&#34385;&#20004;&#31181;&#21487;&#35266;&#27979;&#37327;&#8212;&#8212;&#20855;&#26377;&#22266;&#23450;&#36755;&#20837;&#30340;&#22122;&#22768;&#27745;&#26579;&#36755;&#20986;&#21644;&#20855;&#26377;&#20808;&#39564;&#20998;&#24067;&#23450;&#20041;&#30340;&#19981;&#30830;&#23450;&#36755;&#20837;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26694;&#26550;&#20272;&#35745;&#21518;&#39564;&#20998;&#24067;&#20197;&#25512;&#26029;&#19981;&#30830;&#23450;&#30340;&#25968;&#25454;&#20301;&#32622;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36793;&#38469;&#21270;&#26041;&#27861;&#23558;&#36825;&#20123;&#36755;&#20837;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#39640;&#26031;&#36807;&#31243;&#39044;&#27979;&#20013;&#12290;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#36825;&#31181;&#26032;&#22238;&#24402;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#20854;&#20013;&#35266;&#23519;&#21040;&#19981;&#21516;&#27700;&#24179;&#36755;&#20837;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#26222;&#36866;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional Gaussian process regression exclusively assumes the existence of noise in the output data of model observations. In many scientific and engineering applications, however, the input locations of observational data may also be compromised with uncertainties owing to modeling assumptions, measurement errors, etc. In this work, we propose a Bayesian method that integrates the variability of input data into Gaussian process regression. Considering two types of observables -- noise-corrupted outputs with fixed inputs and those with prior-distribution-defined uncertain inputs, a posterior distribution is estimated via a Bayesian framework to infer the uncertain data locations. Thereafter, such quantified uncertainties of inputs are incorporated into Gaussian process predictions by means of marginalization. The effectiveness of this new regression technique is demonstrated through several numerical examples, in which a consistently good performance of generalization is observed, w
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22270;&#19978;&#38271;&#23614;&#20998;&#31867;&#30340;&#31532;&#19968;&#20010;&#27867;&#21270;&#36793;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34920;&#24449;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#26032;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.09938</link><description>&lt;p&gt;
&#22270;&#20013;&#38271;&#23614;&#31867;&#21035;&#30340;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Characterizing Long-Tail Categories on Graphs. (arXiv:2305.09938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22270;&#19978;&#38271;&#23614;&#20998;&#31867;&#30340;&#31532;&#19968;&#20010;&#27867;&#21270;&#36793;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34920;&#24449;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#26032;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#21253;&#25324;&#37329;&#34701;&#20132;&#26131;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#21644;&#21512;&#20316;&#32593;&#32476;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#20316;&#21697;&#20027;&#35201;&#38598;&#20013;&#20110;&#36890;&#36807;&#22270;&#22686;&#24378;&#25110;&#30446;&#26631;&#37325;&#26032;&#21152;&#26435;&#28040;&#38500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26377;&#38480;&#30340;&#25991;&#29486;&#25552;&#20379;&#29702;&#35770;&#24037;&#20855;&#26469;&#34920;&#24449;&#22270;&#19978;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#65292;&#24182;&#29702;&#35299;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#21363;&#27599;&#20010;&#20219;&#21153;&#23545;&#24212;&#20110;&#39044;&#27979;&#19968;&#20010;&#29305;&#23450;&#30340;&#31867;&#21035;&#65292;&#25552;&#20986;&#20102;&#22270;&#19978;&#38271;&#23614;&#20998;&#31867;&#30340;&#31532;&#19968;&#20010;&#27867;&#21270;&#36793;&#30028;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#38271;&#23614;&#20998;&#31867;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#25152;&#26377;&#20219;&#21153;&#20013;&#30340;&#25439;&#22833;&#33539;&#22260;&#21644;&#20219;&#21153;&#24635;&#25968;&#30340;&#25903;&#37197;&#12290;&#22312;&#29702;&#35770;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#34920;&#24449;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-tail data distributions are prevalent in many real-world networks, including financial transaction networks, e-commerce networks, and collaboration networks. Despite the success of recent developments, the existing works mainly focus on debiasing the machine learning models via graph augmentation or objective reweighting. However, there is limited literature that provides a theoretical tool to characterize the behaviors of long-tail categories on graphs and understand the generalization performance in real scenarios. To bridge this gap, we propose the first generalization bound for long-tail classification on graphs by formulating the problem in the fashion of multi-task learning, i.e., each task corresponds to the prediction of one particular category. Our theoretical results show that the generalization performance of long-tail classification is dominated by the range of losses across all tasks and the total number of tasks. Building upon the theoretical findings, we propose a n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#33041;&#30005;&#20449;&#21495;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20351;&#29992;&#22810;&#29305;&#24449;&#30456;&#20851;&#20998;&#26512;&#26041;&#27861;&#26469;&#33258;&#21160;&#20998;&#35299;&#21644;&#25552;&#21462;&#22810;&#31181;&#31867;&#22411;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#12290;&#20854;&#20013;&#65292;PCA&#38477;&#32500;&#25216;&#26415;&#29992;&#20110;&#25214;&#21040;&#21344;&#20027;&#23548;&#30340;&#20381;&#36182;&#20851;&#31995;&#26041;&#21521;&#65292;&#20174;&#32780;&#25552;&#21462;&#33041;&#30005;&#20449;&#21495;&#20013;&#24494;&#23567;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09478</link><description>&lt;p&gt;
&#26102;&#38388;&#24310;&#36831;&#22810;&#29305;&#24449;&#30456;&#20851;&#20998;&#26512;&#20197;&#25552;&#21462;&#33041;&#30005;&#20449;&#21495;&#20013;&#24494;&#23567;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
Time delay multi-feature correlation analysis to extract subtle dependencies from EEG signals. (arXiv:2305.09478v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#33041;&#30005;&#20449;&#21495;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20351;&#29992;&#22810;&#29305;&#24449;&#30456;&#20851;&#20998;&#26512;&#26041;&#27861;&#26469;&#33258;&#21160;&#20998;&#35299;&#21644;&#25552;&#21462;&#22810;&#31181;&#31867;&#22411;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#12290;&#20854;&#20013;&#65292;PCA&#38477;&#32500;&#25216;&#26415;&#29992;&#20110;&#25214;&#21040;&#21344;&#20027;&#23548;&#30340;&#20381;&#36182;&#20851;&#31995;&#26041;&#21521;&#65292;&#20174;&#32780;&#25552;&#21462;&#33041;&#30005;&#20449;&#21495;&#20013;&#24494;&#23567;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#26159;&#26497;&#20854;&#22797;&#26434;&#30340;&#33041;&#27963;&#21160;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20363;&#22914;&#19968;&#23545;&#30005;&#26497;&#20449;&#21495;&#22312;&#19981;&#21516;&#26102;&#38388;&#24310;&#36831;&#65288;&#28382;&#21518;$\Delta$t&#65289;&#19979;&#30340;&#32852;&#21512;&#20998;&#24067;&#65288;$\rho_{\Delta t}$&#65289;&#21487;&#20197;&#35775;&#38382;&#36825;&#31181;&#38544;&#34255;&#21160;&#24577;&#30340;&#26576;&#20123;&#32454;&#33410;&#12290;&#26631;&#20934;&#26041;&#27861;&#26159;&#30417;&#35270;&#36825;&#26679;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#21333;&#19968;&#35780;&#20272;&#65292;&#20363;&#22914;Pearson&#30456;&#20851;&#65288;&#25110;&#20114;&#20449;&#24687;&#65289;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#27861;&#32467;&#26524;&#36890;&#24120;&#30456;&#23545;&#19981;&#22826;&#26377;&#36259;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36825;&#26679;&#30340;&#22797;&#26434;&#20449;&#21495;&#21487;&#33021;&#30001;&#22810;&#31181;&#31867;&#22411;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#26500;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20998;&#35299;&#21644;&#25552;&#21462;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#32852;&#21512;&#20998;&#24067;&#24314;&#27169;&#20026;&#25152;&#26377;&#32771;&#34385;&#30340;&#28382;&#21518;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#39033;&#24335;&#20272;&#35745;&#65292;&#28982;&#21518;&#36890;&#36807;PCA&#38477;&#32500;&#25214;&#21040;&#21344;&#20027;&#23548;&#30340;&#20381;&#36182;&#20851;&#31995;&#26041;&#21521;$f_v$&#12290;&#36825;&#26679;&#25105;&#20204;&#24471;&#21040;&#19968;&#20123;&#28382;&#21518;&#20381;&#36182;&#29305;&#24449;$a_i(\Delta t)$&#65292;&#29992;&#20110;&#25551;&#36848;&#21508;&#20010;&#28382;&#21518;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#21450;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) signals are resultants of extremely complex brain activity. Some details of this hidden dynamics might be accessible through e.g. joint distributions $\rho_{\Delta t}$ of signals of pairs of electrodes shifted by various time delays (lag $\Delta t$). A standard approach is monitoring a single evaluation of such joint distributions, like Pearson correlation (or mutual information), which turns out relatively uninteresting as expected, there is usually a small peak for zero delay and nearly symmetric drop with delay. In contrast, such a complex signal might be composed of multiple types of statistical dependencies - this article proposes approach to automatically decompose and extract them. Specifically, we model such joint distributions as polynomials estimated for all considered lag dependencies, then with PCA dimensionality reduction find dominant dependency directions $f_v$. This way we get a few lag dependent features $a_i(\Delta t)$ describing separat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#21152;&#26435;&#30340;&#26368;&#23567;&#26497;&#22823;&#39118;&#38505;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#36991;&#20813;&#21327;&#21464;&#37327;&#28418;&#31227;&#23545;&#30417;&#30563;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.08637</link><description>&lt;p&gt;
&#20026;&#21327;&#21464;&#37327;&#28418;&#31227;&#33258;&#36866;&#24212;&#24341;&#20837;&#21452;&#37325;&#21152;&#26435;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Double-Weighting for Covariate Shift Adaptation. (arXiv:2305.08637v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#21152;&#26435;&#30340;&#26368;&#23567;&#26497;&#22823;&#39118;&#38505;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#36991;&#20813;&#21327;&#21464;&#37327;&#28418;&#31227;&#23545;&#30417;&#30563;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#20013;&#24120;&#24120;&#21463;&#21040;&#21327;&#21464;&#37327;&#28418;&#31227;&#24433;&#21709;&#65292;&#21363;&#35757;&#32451;&#26679;&#26412;&#21644;&#27979;&#35797;&#26679;&#26412;&#30340;&#23454;&#20363;&#36793;&#32536;&#20998;&#24067;&#19981;&#21516;&#20294;&#26631;&#31614;&#26465;&#20214;&#30456;&#21516;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#27604;&#29575;p_te&#65288;x&#65289;/p_tr&#65288;x&#65289;&#23545;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#21152;&#26435;&#65288;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65289;&#65292;&#25110;&#32773;&#20351;&#29992;&#27604;&#29575;p_tr&#65288;x&#65289;/p_te&#65288;x&#65289;&#23545;&#27979;&#35797;&#26679;&#26412;&#36827;&#34892;&#21152;&#26435;&#65288;&#40065;&#26834;&#26041;&#27861;&#65289;&#26469;&#35299;&#20915;&#36825;&#31181;&#21327;&#21464;&#37327;&#28418;&#31227;&#12290;&#28982;&#32780;&#65292;&#22312;&#25903;&#25345;&#19981;&#21305;&#37197;&#25110;&#19978;&#36848;&#27604;&#29575;&#21462;&#22823;&#20540;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#33021;&#24456;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#26497;&#22823;&#39118;&#38505;&#20998;&#31867;(MRC)&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#26679;&#26412;&#21644;&#27979;&#35797;&#26679;&#26412;&#36827;&#34892;&#21152;&#26435;&#26469;&#36991;&#20813;&#36825;&#31181;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#33719;&#24471;&#20004;&#32452;&#21152;&#26435;&#65292;&#24182;&#25512;&#24191;&#20102;&#20256;&#32479;&#30340;&#26680;&#22343;&#20540;&#21305;&#37197;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning is often affected by a covariate shift in which the marginal distributions of instances (covariates $x$) of training and testing samples $\mathrm{p}_\text{tr}(x)$ and $\mathrm{p}_\text{te}(x)$ are different but the label conditionals coincide. Existing approaches address such covariate shift by either using the ratio $\mathrm{p}_\text{te}(x)/\mathrm{p}_\text{tr}(x)$ to weight training samples (reweighting methods) or using the ratio $\mathrm{p}_\text{tr}(x)/\mathrm{p}_\text{te}(x)$ to weight testing samples (robust methods). However, the performance of such approaches can be poor under support mismatch or when the above ratios take large values. We propose a minimax risk classification (MRC) approach for covariate shift adaptation that avoids such limitations by weighting both training and testing samples. In addition, we develop effective techniques that obtain both sets of weights and generalize the conventional kernel mean matching method. We provide novel genera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#30340;&#27169;&#20272;&#35745;&#24207;&#21015;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#28085;&#30422;&#35299;&#26512;&#26680;&#21644;Epanechnikov&#26680;&#30340;&#21457;&#29616;&#65292;&#24847;&#20041;&#22312;&#20110;&#28085;&#30422;&#20102;&#22312;&#22522;&#20110;KDE&#30340;&#27169;&#20272;&#35745;&#30340;&#28176;&#36817;&#32479;&#35745;&#25928;&#29575;&#26041;&#38754;&#26368;&#20248;&#30340;&#38750;&#36127;&#26680;&#8212;&#8212;&#21452;&#37325;&#26680;&#12290;</title><link>http://arxiv.org/abs/2305.08463</link><description>&lt;p&gt;
&#22343;&#20540;&#28418;&#31227;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Mean Shift. (arXiv:2305.08463v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#30340;&#27169;&#20272;&#35745;&#24207;&#21015;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#28085;&#30422;&#35299;&#26512;&#26680;&#21644;Epanechnikov&#26680;&#30340;&#21457;&#29616;&#65292;&#24847;&#20041;&#22312;&#20110;&#28085;&#30422;&#20102;&#22312;&#22522;&#20110;KDE&#30340;&#27169;&#20272;&#35745;&#30340;&#28176;&#36817;&#32479;&#35745;&#25928;&#29575;&#26041;&#38754;&#26368;&#20248;&#30340;&#38750;&#36127;&#26680;&#8212;&#8212;&#21452;&#37325;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#20540;&#28418;&#31227;&#65288;MS&#65289;&#31639;&#27861;&#23547;&#25214;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#30340;&#27169;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;MS&#31639;&#27861;&#20135;&#29983;&#30340;&#27169;&#20272;&#35745;&#24207;&#21015;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#22312;&#30456;&#24403;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#20511;&#21161;&#20110;&#20851;&#20110;{\L}ojasiewicz&#19981;&#31561;&#24335;&#30340;&#35770;&#35777;&#65292;&#35780;&#20272;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#28085;&#30422;&#35299;&#26512;&#26680;&#21644;Epanechnikov&#26680;&#30340;&#21457;&#29616;&#65292;&#24847;&#20041;&#22312;&#20110;&#28085;&#30422;&#20102;&#22312;&#22522;&#20110;KDE&#30340;&#27169;&#20272;&#35745;&#30340;&#28176;&#36817;&#32479;&#35745;&#25928;&#29575;&#26041;&#38754;&#26368;&#20248;&#30340;&#38750;&#36127;&#26680;&#8212;&#8212;&#21452;&#37325;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mean shift (MS) algorithm seeks a mode of the kernel density estimate (KDE). This study presents a convergence guarantee of the mode estimate sequence generated by the MS algorithm and an evaluation of the convergence rate, under fairly mild conditions, with the help of the argument concerning the {\L}ojasiewicz inequality. Our findings, which extend existing ones covering analytic kernels and the Epanechnikov kernel, are significant in that they cover the biweight kernel that is optimal among non-negative kernels in terms of the asymptotic statistical efficiency for the KDE-based mode estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#37492;&#21035;&#22120;&#35757;&#32451;GAN&#30340;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#65292;&#25581;&#31034;&#20102;&#23398;&#20064;&#29575;&#12289;&#27491;&#21017;&#21270;&#21644;&#24102;&#23485;&#23545;&#20854;&#24433;&#21709;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#25910;&#25947;&#12289;&#25391;&#33633;&#25110;&#21457;&#25955;&#30340;&#30456;&#21464;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.08277</link><description>&lt;p&gt;
&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Local Convergence of Gradient Descent-Ascent for Training Generative Adversarial Networks. (arXiv:2305.08277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#37492;&#21035;&#22120;&#35757;&#32451;GAN&#30340;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#65292;&#25581;&#31034;&#20102;&#23398;&#20064;&#29575;&#12289;&#27491;&#21017;&#21270;&#21644;&#24102;&#23485;&#23545;&#20854;&#24433;&#21709;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#25910;&#25947;&#12289;&#25391;&#33633;&#25110;&#21457;&#25955;&#30340;&#30456;&#21464;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22797;&#26434;&#39640;&#32500;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#35757;&#32451;GAN&#30340;&#26631;&#20934;&#26041;&#27861;&#28041;&#21450;&#23545;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#65288;GDA&#65289;&#36807;&#31243;&#12290;&#30001;&#20110;&#21160;&#24577;&#30340;&#38750;&#32447;&#24615;&#24615;&#36136;&#65292;&#35813;&#36807;&#31243;&#36890;&#24120;&#24456;&#38590;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#37492;&#21035;&#22120;&#35757;&#32451;GAN&#26102;&#30340;GDA&#23616;&#37096;&#21160;&#24577;&#12290;&#35813;&#25910;&#25947;&#24615;&#20998;&#26512;&#26159;&#22312;[Becker et al. 2022]&#30340;&#8220;&#23396;&#31435;&#28857;&#27169;&#22411;&#8221;&#20551;&#35774;&#19979;&#65292;&#23545;&#25551;&#36848;GDA&#36845;&#20195;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#36827;&#34892;&#32447;&#24615;&#21270;&#24471;&#21040;&#30340;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#23398;&#20064;&#29575;&#12289;&#27491;&#21017;&#21270;&#21644;&#26680;&#21028;&#21035;&#22120;&#30340;&#24102;&#23485;&#23545;GDA&#23616;&#37096;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#21464;&#29616;&#35937;&#65292;&#34920;&#26126;&#31995;&#32479;&#20309;&#26102;&#25910;&#25947;&#12289;&#25391;&#33633;&#25110;&#21457;&#25955;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#39564;&#35777;&#25105;&#20204;&#32467;&#35770;&#30340;&#25968;&#20540;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) are a popular formulation to train generative models for complex high dimensional data. The standard method for training GANs involves a gradient descent-ascent (GDA) procedure on a minimax optimization problem. This procedure is hard to analyze in general due to the nonlinear nature of the dynamics. We study the local dynamics of GDA for training a GAN with a kernel-based discriminator. This convergence analysis is based on a linearization of a non-linear dynamical system that describes the GDA iterations, under an \textit{isolated points model} assumption from [Becker et al. 2022]. Our analysis brings out the effect of the learning rates, regularization, and the bandwidth of the kernel discriminator, on the local convergence rate of GDA. Importantly, we show phase transitions that indicate when the system converges, oscillates, or diverges. We also provide numerical simulations that verify our claims.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08040</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#27744;&#21270;&#30340;&#21487;&#35777;&#26126;&#22810;&#23454;&#20363;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-instance Deep AUC Maximization with Stochastic Pooling. (arXiv:2305.08040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26032;&#22411;&#24212;&#29992;&#65292;&#29992;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#65292;&#20854;&#20013;&#23558;&#21333;&#20010;&#31867;&#26631;&#31614;&#20998;&#37197;&#32473;&#19968;&#32452;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#24739;&#32773;&#30340;&#22810;&#20010;CT&#25195;&#25551;&#30340;&#22810;&#20010;2D&#20999;&#29255;&#65289;&#12290;&#25105;&#20204;&#22312;DAM&#30340;&#32972;&#26223;&#19979;&#35299;&#20915;&#20102;MIL&#20013;&#34987;&#24573;&#30053;&#20294;&#38750;&#24120;&#37325;&#35201;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#21363;&#21253;&#22823;&#23567;&#36807;&#22823;&#65292;&#26080;&#27861;&#22312;&#21453;&#21521;&#20256;&#25773;&#26102;&#21152;&#36733;&#21040;GPU&#20869;&#23384;&#20013;&#65292;&#36825;&#26159;MIL&#26631;&#20934;&#27744;&#21270;&#26041;&#27861;&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#20851;&#20110;&#27719;&#32858;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#26500;&#36896;&#20026;&#22810;&#32423;&#32452;&#21512;&#20989;&#25968;&#12290;&#36890;&#36807;&#32508;&#21512;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#21644;&#38750;&#20984;&#26497;&#23567;&#26368;&#22823;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#19988;&#21487;&#35777;&#26126;&#30340;&#22810;&#23454;&#20363;DAM&#65288;MIDAM&#65289;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#26368;&#22823;&#27744;&#21270;&#25110;&#38543;&#26426;&#27880;&#24847;&#21147;&#27744;&#21270;&#65292;&#20165;&#23545;&#27599;&#20010;&#21253;&#23545;&#24212;&#30340;&#23454;&#20363;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#26469;&#35745;&#31639; sto&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a novel application of deep AUC maximization (DAM) for multi-instance learning (MIL), in which a single class label is assigned to a bag of instances (e.g., multiple 2D slices of a CT scan for a patient). We address a neglected yet non-negligible computational challenge of MIL in the context of DAM, i.e., bag size is too large to be loaded into {GPU} memory for backpropagation, which is required by the standard pooling methods of MIL. To tackle this challenge, we propose variance-reduced stochastic pooling methods in the spirit of stochastic optimization by formulating the loss function over the pooled prediction as a multi-level compositional function. By synthesizing techniques from stochastic compositional optimization and non-convex min-max optimization, we propose a unified and provable muli-instance DAM (MIDAM) algorithm with stochastic smoothed-max pooling or stochastic attention-based pooling, which only samples a few instances for each bag to compute a sto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23384;&#22312;&#30340;&#21435;&#21367;&#31215;&#23450;&#20041;&#30340;&#33539;&#30068;&#35770;&#20803;&#20998;&#26512;&#65292;&#23558;&#31515;&#21345;&#20799;&#31215;&#21644;&#24186;&#27169;&#31215;&#30340;&#27010;&#24565;&#24212;&#35813;&#26500;&#25104;&#21435;&#21367;&#31215;&#30340;&#26680;&#24515;&#65292;&#24182;&#23637;&#29616;&#20102;&#22788;&#29702;&#20989;&#25968;&#12289;&#31561;&#21464;&#26144;&#23556;&#12289;&#20851;&#31995;&#21644;&#38543;&#26426;&#26144;&#23556;&#30340;&#30456;&#20284;&#24615;&#21644;&#20851;&#38190;&#21306;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.06886</link><description>&lt;p&gt;
&#23545;&#8220;&#21435;&#21367;&#31215;&#8221;&#23450;&#20041;&#30340;&#33539;&#30068;&#35770;&#20803;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Category-theoretical Meta-analysis of Definitions of Disentanglement. (arXiv:2305.06886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23384;&#22312;&#30340;&#21435;&#21367;&#31215;&#23450;&#20041;&#30340;&#33539;&#30068;&#35770;&#20803;&#20998;&#26512;&#65292;&#23558;&#31515;&#21345;&#20799;&#31215;&#21644;&#24186;&#27169;&#31215;&#30340;&#27010;&#24565;&#24212;&#35813;&#26500;&#25104;&#21435;&#21367;&#31215;&#30340;&#26680;&#24515;&#65292;&#24182;&#23637;&#29616;&#20102;&#22788;&#29702;&#20989;&#25968;&#12289;&#31561;&#21464;&#26144;&#23556;&#12289;&#20851;&#31995;&#21644;&#38543;&#26426;&#26144;&#23556;&#30340;&#30456;&#20284;&#24615;&#21644;&#20851;&#38190;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23558;&#25968;&#25454;&#30340;&#21464;&#21270;&#22240;&#32032;&#20998;&#31163;&#26159;&#19968;&#20010;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#30740;&#31350;&#20154;&#21592;&#20197;&#21508;&#31181;&#26041;&#24335;&#30740;&#31350;&#23427;&#65292;&#23548;&#33268;&#20102;&#20247;&#22810;&#30340;&#23450;&#20041;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#32463;&#39564;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#20173;&#38656;&#35201;&#26356;&#22810;&#30340;&#29702;&#35770;&#30740;&#31350;&#26469;&#20805;&#20998;&#29702;&#35299;&#21435;&#21367;&#31215;&#30340;&#23450;&#20041;&#23646;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#23450;&#20041;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23384;&#22312;&#30340;&#21435;&#21367;&#31215;&#23450;&#20041;&#30340;&#33539;&#30068;&#35770;&#20803;&#20998;&#26512;&#65292;&#23558;&#33539;&#30068;&#35770;&#20316;&#20026;&#19968;&#20010;&#32479;&#19968;&#32780;&#20005;&#35880;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#31515;&#21345;&#20799;&#31215;&#21644;&#24186;&#27169;&#31215;&#30340;&#27010;&#24565;&#24212;&#35813;&#26500;&#25104;&#21435;&#21367;&#31215;&#30340;&#26680;&#24515;&#12290;&#26377;&#20102;&#36825;&#20123;&#26680;&#24515;&#27010;&#24565;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22788;&#29702;&#65288;i&#65289;&#20989;&#25968;&#65292;&#65288;ii&#65289;&#31561;&#21464;&#26144;&#23556;&#65292;&#65288;iii&#65289;&#20851;&#31995;&#21644;&#65288;iv&#65289;&#38543;&#26426;&#26144;&#23556;&#30340;&#30456;&#20284;&#24615;&#21644;&#20851;&#38190;&#21306;&#21035;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#33539;&#30068;&#35770;&#20803;&#20998;&#26512;&#28145;&#21270;&#20102;&#25105;&#20204;&#23545;&#21435;&#21367;&#31215;&#21450;&#20854;&#19981;&#21516;&#21046;&#23450;&#30340;&#29702;&#35299;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#19981;&#21516;&#30340;&#23450;&#20041;&#20043;&#38388;&#36827;&#34892;&#23548;&#33322;&#65292;&#24182;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangling the factors of variation in data is a fundamental concept in machine learning and has been studied in various ways by different researchers, leading to a multitude of definitions. Despite the numerous empirical studies, more theoretical research is needed to fully understand the defining properties of disentanglement and how different definitions relate to each other. This paper presents a meta-analysis of existing definitions of disentanglement, using category theory as a unifying and rigorous framework. We propose that the concepts of the cartesian and monoidal products should serve as the core of disentanglement. With these core concepts, we show the similarities and crucial differences in dealing with (i) functions, (ii) equivariant maps, (iii) relations, and (iv) stochastic maps. Overall, our meta-analysis deepens our understanding of disentanglement and its various formulations and can help researchers navigate different definitions and choose the most appropriate o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#40065;&#26834;&#26816;&#27979;&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32858;&#31867;&#25216;&#26415;&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#39046;&#20808;&#28382;&#21518;&#20272;&#35745;&#30340;&#32858;&#21512;&#65292;&#20174;&#32780;&#24378;&#21270;&#20102;&#23545;&#21407;&#22987;&#23431;&#23449;&#20013;&#30340;&#19968;&#33268;&#20851;&#31995;&#30340;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.06704</link><description>&lt;p&gt;
&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#30340;&#40065;&#26834;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models. (arXiv:2305.06704v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#40065;&#26834;&#26816;&#27979;&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32858;&#31867;&#25216;&#26415;&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#39046;&#20808;&#28382;&#21518;&#20272;&#35745;&#30340;&#32858;&#21512;&#65292;&#20174;&#32780;&#24378;&#21270;&#20102;&#23545;&#21407;&#22987;&#23431;&#23449;&#20013;&#30340;&#19968;&#33268;&#20851;&#31995;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#21457;&#29616;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#65292;&#21487;&#20197;&#33719;&#24471;&#20851;&#38190;&#20449;&#24687;&#65292;&#36825;&#25351;&#30340;&#26159;&#20004;&#20010;&#30456;&#23545;&#26102;&#38388;&#20114;&#31227;&#30340;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#29992;&#20110;&#25511;&#21046;&#12289;&#39044;&#27979;&#25110;&#32858;&#31867;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#26816;&#27979;&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25152;&#35774;&#24819;&#30340;&#31649;&#36947;&#25509;&#25910;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#20174;&#27599;&#20010;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#19968;&#32452;&#23376;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#21508;&#31181;&#32858;&#31867;&#25216;&#26415;&#65288;&#20363;&#22914;K-means++&#21644;&#35889;&#32858;&#31867;&#65289;&#65292;&#37319;&#29992;&#21508;&#31181;&#25104;&#23545;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#12290;&#19968;&#26086;&#32858;&#31867;&#34987;&#25552;&#21462;&#20986;&#26469;&#65292;&#36328;&#32858;&#31867;&#30340;&#39046;&#20808;&#28382;&#21518;&#20272;&#35745;&#34987;&#32858;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#23545;&#21407;&#22987;&#23431;&#23449;&#20013;&#19968;&#33268;&#20851;&#31995;&#30340;&#35782;&#21035;&#12290;&#30001;&#20110;&#22810;
&lt;/p&gt;
&lt;p&gt;
In multivariate time series systems, key insights can be obtained by discovering lead-lag relationships inherent in the data, which refer to the dependence between two time series shifted in time relative to one another, and which can be leveraged for the purposes of control, forecasting or clustering. We develop a clustering-driven methodology for the robust detection of lead-lag relationships in lagged multi-factor models. Within our framework, the envisioned pipeline takes as input a set of time series, and creates an enlarged universe of extracted subsequence time series from each input time series, by using a sliding window approach. We then apply various clustering techniques (e.g, K-means++ and spectral clustering), employing a variety of pairwise similarity measures, including nonlinear ones. Once the clusters have been extracted, lead-lag estimates across clusters are aggregated to enhance the identification of the consistent relationships in the original universe. Since multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#20854;&#36890;&#36807;&#27010;&#29575;&#27979;&#24230;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#35299;&#20915;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#38382;&#39064;&#24471;&#21040;&#23450;&#20041;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.06348</link><description>&lt;p&gt;
&#24102;&#27010;&#29575;&#24577;&#23556;&#21644;&#26680;&#24179;&#22343;&#23884;&#20837;&#30340;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised learning with probabilistic morphisms and kernel mean embeddings. (arXiv:2305.06348v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#20854;&#36890;&#36807;&#27010;&#29575;&#27979;&#24230;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#35299;&#20915;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#38382;&#39064;&#24471;&#21040;&#23450;&#20041;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;X&#21644;&#26631;&#31614;&#31354;&#38388;Y&#12290; &#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#24517;&#39035;&#27491;&#30830;&#22320;&#24230;&#37327;&#21487;&#33021;&#39044;&#27979;&#22120;&#30340;&#20551;&#35774;&#31354;&#38388;H&#20013;&#30340;&#20803;&#32032;&#19982;&#30417;&#31649;&#36816;&#31639;&#31526;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#30417;&#31649;&#36816;&#31639;&#31526;&#21487;&#33021;&#19981;&#23646;&#20110;H&#12290; &#20026;&#20102;&#23450;&#20041;&#27491;&#30830;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#22312;&#25237;&#24433;&#928;X&#65306;X&#215;Y&#8594;X&#30456;&#23545;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#119883;&#215;&#119884;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#956;Y| X&#30340;&#29305;&#27530;&#24615;&#36136;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#20316;&#20026;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; &#22914;&#26524;Y&#26159;&#19968;&#20010;&#20855;&#26377;Borel &#963;-&#20195;&#25968; BY&#30340;&#21487;&#20998;&#30340;&#21487;&#24230;&#37327;&#21270;&#25299;&#25169;&#31354;&#38388;&#65292;&#21017;&#25552;&#20986;&#20102;&#20851;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#30456;&#23545;&#20110;&#25237;&#24433;&#928;X&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#956;Y| X&#30340;&#21478;&#19968;&#31181;&#29305;&#27530;&#24615;&#36136;&#30340;&#34920;&#24449;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper I propose a concept of a correct loss function in a generative model of supervised learning for an input space $\mathcal{X}$ and a label space $\mathcal{Y}$, which are measurable spaces. A correct loss function in a generative model of supervised learning must correctly measure the discrepancy between elements of a hypothesis space $\mathcal{H}$ of possible predictors and the supervisor operator, which may not belong to $\mathcal{H}$. To define correct loss functions, I propose a characterization of a regular conditional probability measure $\mu_{\mathcal{Y}|\mathcal{X}}$ for a probability measure $\mu$ on $\mathcal{X} \times \mathcal{Y}$ relative to the projection $\Pi_{\mathcal{X}}: \mathcal{X}\times\mathcal{Y}\to \mathcal{X}$ as a solution of a linear operator equation. If $\mathcal{Y}$ is a separable metrizable topological space with the Borel $\sigma$-algebra $ \mathcal{B} (\mathcal{Y})$, I propose another characterization of a regular conditional probability measure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#29305;&#24449;&#23376;&#31354;&#38388;&#23637;&#24320;&#21644;&#32467;&#26500;&#20027;&#25104;&#20998;&#20004;&#31181;&#26041;&#27861;&#26469;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06142</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Feature Expansion for Graph Neural Networks. (arXiv:2305.06142v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#29305;&#24449;&#23376;&#31354;&#38388;&#23637;&#24320;&#21644;&#32467;&#26500;&#20027;&#25104;&#20998;&#20004;&#31181;&#26041;&#27861;&#26469;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26088;&#22312;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#34920;&#31034;&#65292;&#24182;&#23637;&#29616;&#20986;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#25903;&#37197;&#34920;&#31034;&#23398;&#20064;&#30340;&#29305;&#24449;&#31354;&#38388;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23578;&#26410;&#34987;&#31995;&#32479;&#22320;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20998;&#26512;&#31354;&#38388;&#27169;&#22411;&#21644;&#35889;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#35299;&#20026;&#30830;&#23450;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;&#21487;&#35757;&#32451;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#36890;&#36807;&#30697;&#38453;&#31354;&#38388;&#20998;&#26512;&#26126;&#30830;&#22320;&#30740;&#31350;&#29305;&#24449;&#31354;&#38388;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21457;&#29616;&#65292;&#30001;&#20110;&#37325;&#22797;&#32858;&#21512;&#65292;&#29305;&#24449;&#31354;&#38388;&#20542;&#21521;&#20110;&#32447;&#24615;&#30456;&#20851;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;1&#65289;&#29305;&#24449;&#23376;&#31354;&#38388;&#23637;&#24320;&#21644;2&#65289;&#32467;&#26500;&#20027;&#25104;&#20998;&#26469;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks aim to learn representations for graph-structured data and show impressive performance, particularly in node classification. Recently, many methods have studied the representations of GNNs from the perspective of optimization goals and spectral graph theory. However, the feature space that dominates representation learning has not been systematically studied in graph neural networks. In this paper, we propose to fill this gap by analyzing the feature space of both spatial and spectral models. We decompose graph neural networks into determined feature spaces and trainable weights, providing the convenience of studying the feature space explicitly using matrix space analysis. In particular, we theoretically find that the feature space tends to be linearly correlated due to repeated aggregations. Motivated by these findings, we propose 1) feature subspaces flattening and 2) structural principal components to expand the feature space. Extensive experiments verify the 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; VCC&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#26368;&#37325;&#35201;&#30340;VIP&#26631;&#35760;&#65292;&#19968;&#23450;&#31243;&#24230;&#19978;&#21387;&#32553;&#24207;&#21015;&#65292;&#20174;&#32780;&#20351;Transformer&#27169;&#22411;&#21487;&#22788;&#29702;&#38271;&#24230;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.04241</link><description>&lt;p&gt;
Vcc: &#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#37325;&#35201;&#26631;&#35760;&#23558;Transformer&#25193;&#23637;&#21040;128K&#20196;&#29260;&#25110;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens. (arXiv:2305.04241v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04241
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; VCC&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#26368;&#37325;&#35201;&#30340;VIP&#26631;&#35760;&#65292;&#19968;&#23450;&#31243;&#24230;&#19978;&#21387;&#32553;&#24207;&#21015;&#65292;&#20174;&#32780;&#20351;Transformer&#27169;&#22411;&#21487;&#22788;&#29702;&#38271;&#24230;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#26377;&#20154;&#33268;&#21147;&#20110;&#38477;&#20302;&#36825;&#20123;&#27169;&#22411;&#30340;&#20108;&#27425;&#25104;&#26412;&#65288;&#20316;&#20026;&#24207;&#21015;&#38271;&#24230;&#30340;&#20989;&#25968;&#65289;&#65292;&#20294;&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#65288;&#22914;&#36229;&#36807;16K&#26631;&#35760;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;Ultra long sequences&#30340;Transformer&#27169;&#22411;&#30340;&#25928;&#29575;&#26174;&#30528;&#25552;&#39640;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#27599;&#23618;&#23558;&#24207;&#21015;&#21387;&#32553;&#20026;&#26356;&#23567;&#30340;&#34920;&#31034;&#65292;&#36890;&#36807;&#21033;&#29992;&#35768;&#22810;&#20219;&#21153;&#20013;&#20165;&#26377;&#30340;&#23569;&#25968;&#30340;&#29305;&#27530;&#26631;&#35760;&#65288;&#25105;&#20204;&#31216;&#20854;&#20026;VIP&#26631;&#35760;&#65289;&#19982;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#26368;&#30456;&#20851;&#30340;&#36825;&#20010;&#20107;&#23454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;VIP&#26631;&#35760;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#21363;VIP&#26631;&#35760;&#20013;&#24515;&#21387;&#32553;&#65288;VCC&#65289;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#26681;&#25454;&#20854;&#23545;&#36817;&#20284;VIP&#26631;&#35760;&#34920;&#31034;&#30340;&#24433;&#21709;&#26377;&#36873;&#25321;&#22320;&#21387;&#32553;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are central in modern natural language processing and computer vision applications. Despite recent works devoted to reducing the quadratic cost of such models (as a function of the sequence length), dealing with ultra long sequences (e.g., with more than 16K tokens) remains challenging. Applications such as answering questions based on a book or summarizing a scientific article are inefficient or infeasible. Here, we propose to significantly improve the efficiency of Transformers for ultra long sequences, by compressing the sequence into a much smaller representation at each layer. Specifically, by exploiting the fact that in many tasks, only a small subset of special tokens (we call VIP-tokens) are most relevant to the final prediction, we propose a VIP-token centric compression (VCC) scheme which selectively compresses the sequence based on their impact on approximating the representation of the VIP-tokens. Compared with competitive baselines, our algorithm is not only e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#24182;&#21487;&#33021;&#25918;&#22823;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.03355</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#32508;&#21512;&#30740;&#31350;&#65306;&#24615;&#33021;&#12289;&#38544;&#31169;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness. (arXiv:2305.03355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#24182;&#21487;&#33021;&#25918;&#22823;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#26088;&#22312;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#29305;&#24449;&#32534;&#30721;&#25104;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#26159;&#19968;&#31181;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#30456;&#20851;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#21387;&#32553;&#22270;&#20687;&#30340;&#20449;&#24687;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20174;&#23433;&#20840;&#24615;&#35282;&#24230;&#20840;&#38754;&#20998;&#26512;&#36825;&#19968;&#25216;&#26415;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#23545;&#28508;&#22312;&#39118;&#38505;&#32570;&#20047;&#31995;&#32479;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;&#25105;&#20204;&#25104;&#21151;&#20351;&#29992;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26469;&#26174;&#31034;&#20173;&#28982;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#12290;&#26412;&#25991;&#36824;&#34920;&#26126;&#65292;&#25968;&#25454;&#38598;&#21387;&#32553;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#21487;&#33021;&#20250;&#20135;&#29983;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#25918;&#22823;&#31867;&#21035;&#38388;&#30340;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#12290;&#26412;&#30740;&#31350;&#20026;&#25968;&#25454;&#38598;&#21387;&#32553;&#35780;&#20272;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of dataset distillation is to encode the rich features of an original dataset into a tiny dataset. It is a promising approach to accelerate neural network training and related studies. Different approaches have been proposed to improve the informativeness and generalization performance of distilled images. However, no work has comprehensively analyzed this technique from a security perspective and there is a lack of systematic understanding of potential risks. In this work, we conduct extensive experiments to evaluate current state-of-the-art dataset distillation methods. We successfully use membership inference attacks to show that privacy risks still remain. Our work also demonstrates that dataset distillation can cause varying degrees of impact on model robustness and amplify model unfairness across classes when making predictions. This work offers a large-scale benchmarking framework for dataset distillation evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;DynaVol&#65292;&#21487;&#20197;&#22312;&#22810;&#23454;&#20307;&#21160;&#24577;&#22330;&#26223;&#20013;&#23398;&#20064;&#26102;&#38388;&#21464;&#21270;&#30340;&#20307;&#31215;&#34920;&#31034;&#65292;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;3D&#26684;&#28857;&#21644;&#32852;&#21512;&#23398;&#20064;&#26684;&#28857;&#32423;&#23616;&#37096;&#21160;&#24577;&#12289;&#29289;&#20307;&#32423;&#20840;&#23616;&#21160;&#24577;&#21644;&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#22686;&#24378;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#20307;&#32032;&#21270;&#30340;&#26102;&#31354;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00393</link><description>&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#23545;&#21160;&#24577;&#22330;&#26223;&#36827;&#34892;&#29289;&#20307;&#20013;&#24515;&#20307;&#32032;&#21270;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Voxelization of Dynamic Scenes via Inverse Neural Rendering. (arXiv:2305.00393v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;DynaVol&#65292;&#21487;&#20197;&#22312;&#22810;&#23454;&#20307;&#21160;&#24577;&#22330;&#26223;&#20013;&#23398;&#20064;&#26102;&#38388;&#21464;&#21270;&#30340;&#20307;&#31215;&#34920;&#31034;&#65292;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;3D&#26684;&#28857;&#21644;&#32852;&#21512;&#23398;&#20064;&#26684;&#28857;&#32423;&#23616;&#37096;&#21160;&#24577;&#12289;&#29289;&#20307;&#32423;&#20840;&#23616;&#21160;&#24577;&#21644;&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#22686;&#24378;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#20307;&#32032;&#21270;&#30340;&#26102;&#31354;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#30340;3D&#22330;&#26223;&#20013;&#29702;&#35299;&#19990;&#30028;&#30340;&#32452;&#25104;&#21160;&#24577;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#26102;&#38388;&#32447;&#32034;&#65292;&#35201;&#20040;&#24573;&#30053;&#20102;&#22330;&#26223;&#20998;&#35299;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DynaVol&#65292;&#19968;&#31181;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#20026;&#22810;&#23454;&#20307;&#65288;&#22914;&#29289;&#20307;&#65289;&#30340;&#21160;&#24577;&#22330;&#26223;&#23398;&#20064;&#26102;&#38388;&#21464;&#21270;&#30340;&#20307;&#31215;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#20010;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#23427;&#32500;&#25252;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;3D&#26684;&#28857;&#65292;&#21160;&#24577;&#32780;&#28789;&#27963;&#22320;&#23558;&#31354;&#38388;&#20301;&#32622;&#32465;&#23450;&#21040;&#19981;&#21516;&#30340;&#23454;&#20307;&#65292;&#20174;&#32780;&#22312;&#20195;&#34920;&#24615;&#27700;&#24179;&#19978;&#40723;&#21169;&#20449;&#24687;&#30340;&#20998;&#31163;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#32852;&#21512;&#23398;&#20064;&#26684;&#28857;&#32423;&#23616;&#37096;&#21160;&#24577;&#12289;&#29289;&#20307;&#32423;&#20840;&#23616;&#21160;&#24577;&#21644;&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#20307;&#32032;&#21270;&#30340;&#26102;&#31354;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;DynaVol&#35757;&#32451;&#26041;&#26696;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the compositional dynamics of the world in unsupervised 3D scenarios is challenging. Existing approaches either fail to make effective use of time cues or ignore the multi-view consistency of scene decomposition. In this paper, we propose DynaVol, an inverse neural rendering framework that provides a pilot study for learning time-varying volumetric representations for dynamic scenes with multiple entities (like objects). It has two main contributions. First, it maintains a time-dependent 3D grid, which dynamically and flexibly binds the spatial locations to different entities, thus encouraging the separation of information at a representational level. Second, our approach jointly learns grid-level local dynamics, object-level global dynamics, and the compositional neural radiance fields in an end-to-end architecture, thereby enhancing the spatiotemporal consistency of object-centric scene voxelization. We present a two-stage training scheme for DynaVol and validate its ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#31867;&#20998;&#31867;&#20013;&#25932;&#23545;&#35757;&#32451;&#30340;&#40065;&#26834;&#35299;&#23384;&#22312;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#27599;&#20010;&#27169;&#22411;&#20013;&#23384;&#22312; Borel &#21487;&#27979;&#30340;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#24182;&#19982;&#26368;&#20248;&#20256;&#36755;&#21644;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#22312;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#23545;&#19981;&#21487;&#30693;&#20998;&#31867;&#22120;&#30340;&#25932;&#23545;&#35757;&#32451;&#38382;&#39064;&#23384;&#22312; Borel &#21487;&#27979;&#30340;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.00075</link><description>&lt;p&gt;
&#22810;&#31867;&#20998;&#31867;&#20013;&#25932;&#23545;&#35757;&#32451;&#35299;&#30340;&#23384;&#22312;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the existence of solutions to adversarial training in multiclass classification. (arXiv:2305.00075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#31867;&#20998;&#31867;&#20013;&#25932;&#23545;&#35757;&#32451;&#30340;&#40065;&#26834;&#35299;&#23384;&#22312;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#27599;&#20010;&#27169;&#22411;&#20013;&#23384;&#22312; Borel &#21487;&#27979;&#30340;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#24182;&#19982;&#26368;&#20248;&#20256;&#36755;&#21644;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#22312;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#23545;&#19981;&#21487;&#30693;&#20998;&#31867;&#22120;&#30340;&#25932;&#23545;&#35757;&#32451;&#38382;&#39064;&#23384;&#22312; Borel &#21487;&#27979;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25932;&#23545;&#35757;&#32451;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#19977;&#31181;&#27169;&#22411;&#65292;&#26088;&#22312;&#26500;&#24314;&#23545;&#25239;&#25200;&#21160;&#19979;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#20010;&#27169;&#22411;&#20013;&#23384;&#22312; Borel &#21487;&#27979;&#30340;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#25932;&#23545;&#35757;&#32451;&#38382;&#39064;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#25299;&#23637;&#20102;&#20316;&#32773;&#20043;&#21069;&#30340;&#26368;&#20248;&#20256;&#36755;&#32852;&#31995;&#65292;&#24182;&#22312;&#22810;&#31867;&#24773;&#20917;&#19979;&#25932;&#23545;&#35757;&#32451;&#21644;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#20043;&#38388;&#24314;&#31435;&#20102;&#26032;&#30340;&#32852;&#31995;&#12290;&#20316;&#20026;&#25105;&#20204;&#32467;&#26524;&#30340;&#25512;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20108;&#20803;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;&#23545;&#19981;&#21487;&#30693;&#20998;&#31867;&#22120;&#30340;&#25932;&#23545;&#35757;&#32451;&#38382;&#39064;&#23384;&#22312; Borel &#21487;&#27979;&#30340;&#35299;&#65292;&#36825;&#19968;&#32467;&#26524;&#25913;&#36827;&#20102;&#20851;&#20110;&#25932;&#23545;&#35757;&#32451;&#30340;&#25991;&#29486;&#65292;&#25991;&#29486;&#20013;&#20165;&#24050;&#30693;&#21482;&#26377;&#22312;&#29305;&#24449;&#31354;&#38388;&#30340;&#25193;&#22823;&#36890;&#29992; $&#963;$-&#20195;&#25968;&#20869;&#23384;&#22312;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study three models of the problem of adversarial training in multiclass classification designed to construct robust classifiers against adversarial perturbations of data in the agnostic-classifier setting. We prove the existence of Borel measurable robust classifiers in each model and provide a unified perspective of the adversarial training problem, expanding the connections with optimal transport initiated by the authors in previous work and developing new connections between adversarial training in the multiclass setting and total variation regularization. As a corollary of our results, we prove the existence of Borel measurable solutions to the agnostic adversarial training problem in the binary classification setting, a result that improves results in the literature of adversarial training, where robust classifiers were only known to exist within the enlarged universal $\sigma$-algebra of the feature space.
&lt;/p&gt;</description></item><item><title>&#21516;&#28304;&#24615;&#21407;&#21017;&#19981;&#19968;&#23450;&#26159;&#24433;&#21709;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#36234;&#24615;&#30340;&#21807;&#19968;&#21407;&#22240;&#65307;&#26412;&#25991;&#25552;&#20986;Contextual Stochastic Block Model for Homophily (CSBM-H)&#20197;&#28145;&#20837;&#30740;&#31350;&#21516;&#28304;&#24615;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.14274</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#23545;&#33410;&#28857;&#20998;&#31867;&#26377;&#24110;&#21161;&#65306;&#30740;&#31350;&#21516;&#28304;&#24615;&#21407;&#21017;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability. (arXiv:2304.14274v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14274
&lt;/p&gt;
&lt;p&gt;
&#21516;&#28304;&#24615;&#21407;&#21017;&#19981;&#19968;&#23450;&#26159;&#24433;&#21709;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#36234;&#24615;&#30340;&#21807;&#19968;&#21407;&#22240;&#65307;&#26412;&#25991;&#25552;&#20986;Contextual Stochastic Block Model for Homophily (CSBM-H)&#20197;&#28145;&#20837;&#30740;&#31350;&#21516;&#28304;&#24615;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#28304;&#24615;&#21407;&#21017;&#25351;&#30456;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#26356;&#26377;&#21487;&#33021;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#65288;NC&#65289;&#20219;&#21153;&#19978;&#24615;&#33021;&#20248;&#36234;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#25552;&#20986;&#29702;&#35770;&#32467;&#26524;&#35748;&#20026;&#65292;&#21363;&#20351;&#21516;&#28304;&#24615;&#21407;&#21017;&#34987;&#25171;&#30772;&#65292;&#21482;&#35201;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#33410;&#28857;&#20998;&#20139;&#30456;&#20284;&#30340;&#37051;&#23621;&#27169;&#24335;&#65292;GNN&#30340;&#20248;&#21183;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#23545;&#21516;&#28304;&#24615;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#35770;&#28857;&#20165;&#32771;&#34385;&#20102;&#21516;&#31867;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#24573;&#30053;&#20102;&#36328;&#31867;&#21035;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#36825;&#26159;&#30740;&#31350;&#21516;&#28304;&#24615;&#25928;&#24212;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20363;&#23376;&#35777;&#26126;&#20102;&#19978;&#36848;&#19981;&#36275;&#65292;&#24182;&#35748;&#20026;&#21487;&#21306;&#20998;&#24615;&#30340;&#29702;&#24819;&#24773;&#20917;&#26159;&#21516;&#31867;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#23567;&#20110;&#36328;&#31867;&#21035;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#36825;&#20010;&#24819;&#27861;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#21516;&#28304;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Contextual Stochastic Block Model for Homophily (CSBM-H)&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Homophily principle, i.e. nodes with the same labels are more likely to be connected, was believed to be the main reason for the performance superiority of Graph Neural Networks (GNNs) over Neural Networks (NNs) on Node Classification (NC) tasks. Recently, people have developed theoretical results arguing that, even though the homophily principle is broken, the advantage of GNNs can still hold as long as nodes from the same class share similar neighborhood patterns, which questions the validity of homophily. However, this argument only considers intra-class Node Distinguishability (ND) and ignores inter-class ND, which is insufficient to study the effect of homophily. In this paper, we first demonstrate the aforementioned insufficiency with examples and argue that an ideal situation for ND is to have smaller intra-class ND than inter-class ND. To formulate this idea and have a better understanding of homophily, we propose Contextual Stochastic Block Model for Homophily (CSBM-H) and def
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;FedVS&#65292;&#19968;&#31181;&#21516;&#26102;&#35299;&#20915;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#28382;&#21518;&#23458;&#25143;&#31471;&#21644;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26412;&#22320;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#31192;&#23494;&#20849;&#20139;&#26041;&#26696;&#65292;&#20197;&#20445;&#35777;&#20449;&#24687;&#29702;&#35770;&#38544;&#31169;&#65292;&#24182;&#36890;&#36807;&#35299;&#23494;&#35745;&#31639;&#32929;&#20221;&#65292;&#26080;&#25439;&#37325;&#26500;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#23884;&#20837;&#30340;&#27719;&#24635;&#12290;</title><link>http://arxiv.org/abs/2304.13407</link><description>&lt;p&gt;
FedVS: &#38754;&#21521;&#20998;&#21106;&#27169;&#22411;&#30340;&#23481;&#38169;&#21644;&#38544;&#31169;&#20445;&#25252;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedVS: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models. (arXiv:2304.13407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;FedVS&#65292;&#19968;&#31181;&#21516;&#26102;&#35299;&#20915;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#28382;&#21518;&#23458;&#25143;&#31471;&#21644;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26412;&#22320;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#31192;&#23494;&#20849;&#20139;&#26041;&#26696;&#65292;&#20197;&#20445;&#35777;&#20449;&#24687;&#29702;&#35770;&#38544;&#31169;&#65292;&#24182;&#36890;&#36807;&#35299;&#23494;&#35745;&#31639;&#32929;&#20221;&#65292;&#26080;&#25439;&#37325;&#26500;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#23884;&#20837;&#30340;&#27719;&#24635;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#30001;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#35768;&#22810;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#32452;&#25104;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#34987;&#22402;&#30452;&#20998;&#21106;&#65292;&#19981;&#21516;&#30340;&#29305;&#24449;&#23384;&#20648;&#22312;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#19978;&#12290;&#20998;&#21106;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#38382;&#39064;&#26159;&#35757;&#32451;&#19968;&#20010;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#21010;&#20998;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#20998;&#21106;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#36831;&#28382;&#30340;&#23458;&#25143;&#31471;&#36896;&#25104;&#30340;&#24615;&#33021;&#19979;&#38477;&#65307;2&#65289;&#23458;&#25143;&#31471;&#19978;&#20256;&#25968;&#25454;&#23884;&#20837;&#23548;&#33268;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FedVS&#26469;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;FedVS&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#35774;&#35745;&#26412;&#22320;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#31192;&#23494;&#20849;&#20139;&#26041;&#26696;&#65292;&#20174;&#32780;&#20445;&#35777;&#38024;&#23545;&#21246;&#32467;&#23458;&#25143;&#21644;&#22909;&#22855;&#26381;&#21153;&#22120;&#30340;&#20449;&#24687;&#29702;&#35770;&#38544;&#31169;&#65292;&#24182;&#19988;&#36890;&#36807;&#35299;&#23494;&#35745;&#31639;&#32929;&#20221;&#65292;&#26080;&#25439;&#37325;&#26500;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#23884;&#20837;&#30340;&#27719;&#24635;&#12290;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;VFL&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#34920;&#26684;&#65292;CV&#65292;&#22270;&#20687;&#65292;NLP&#65289;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;FedVS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a vertical federated learning (VFL) system consisting of a central server and many distributed clients, the training data are vertically partitioned such that different features are privately stored on different clients. The problem of split VFL is to train a model split between the server and the clients. This paper aims to address two major challenges in split VFL: 1) performance degradation due to straggling clients during training; and 2) data and model privacy leakage from clients' uploaded data embeddings. We propose FedVS to simultaneously address these two challenges. The key idea of FedVS is to design secret sharing schemes for the local data and models, such that information-theoretical privacy against colluding clients and curious server is guaranteed, and the aggregation of all clients' embeddings is reconstructed losslessly, via decrypting computation shares from the non-straggling clients. Extensive experiments on various types of VFL datasets (including tabular, CV, a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;TnALE&#31639;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#23616;&#37096;&#26522;&#20030;&#26356;&#26032;&#27599;&#20010;&#19982;&#32467;&#26500;&#30456;&#20851;&#30340;&#21464;&#37327;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35780;&#20272;&#27425;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#38382;&#39064;&#12290;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22914;&#26524;&#22312;&#27599;&#20010;&#37051;&#22495;&#20013;&#36798;&#21040;&#20102;&#36275;&#22815;&#30340;&#30446;&#26631;&#20989;&#25968;&#38477;&#20302;&#65292;TnALE&#21644;TNLS&#37117;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#24230;&#65292;&#30452;&#21040;&#19968;&#20010;&#24120;&#25968;&#12290;&#21516;&#26102;&#65292;&#19982;TNLS&#30456;&#27604;&#65292; TnALE&#38656;&#35201;&#26356;&#23569;&#30340;&#35780;&#20272;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.12875</link><description>&lt;p&gt;
&#20132;&#26367;&#23616;&#37096;&#26522;&#20030;(TnALE): &#29992;&#36739;&#23569;&#30340;&#35780;&#20272;&#35299;&#20915;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Alternating Local Enumeration (TnALE): Solving Tensor Network Structure Search with Fewer Evaluations. (arXiv:2304.12875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TnALE&#31639;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#23616;&#37096;&#26522;&#20030;&#26356;&#26032;&#27599;&#20010;&#19982;&#32467;&#26500;&#30456;&#20851;&#30340;&#21464;&#37327;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35780;&#20272;&#27425;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#38382;&#39064;&#12290;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22914;&#26524;&#22312;&#27599;&#20010;&#37051;&#22495;&#20013;&#36798;&#21040;&#20102;&#36275;&#22815;&#30340;&#30446;&#26631;&#20989;&#25968;&#38477;&#20302;&#65292;TnALE&#21644;TNLS&#37117;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#24230;&#65292;&#30452;&#21040;&#19968;&#20010;&#24120;&#25968;&#12290;&#21516;&#26102;&#65292;&#19982;TNLS&#30456;&#27604;&#65292; TnALE&#38656;&#35201;&#26356;&#23569;&#30340;&#35780;&#20272;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;(TN)&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#20294;&#36873;&#25321;&#19968;&#20010;&#22909;&#30340;TN&#27169;&#22411;&#65292;&#21363;TN&#32467;&#26500;&#25628;&#32034;(TN-SS)&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;TNLS ~ \cite {li2022permutation} &#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#30340;&#35745;&#31639;&#25928;&#29575;&#20173;&#28982;&#26159;&#26080;&#27861;&#25215;&#21463;&#30340;&#65292;&#38656;&#35201;&#22826;&#22810;&#35780;&#20272;&#30446;&#26631;&#20989;&#25968;&#30340;&#27425;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TnALE&#65292;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#26522;&#20030;&#20132;&#26367;&#26356;&#26032;&#27599;&#20010;&#19982;&#32467;&#26500;&#30456;&#20851;&#30340;&#21464;&#37327;&#65292;&#19982;TNLS&#30456;&#27604;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35780;&#20272;&#27425;&#25968;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;TNLS&#21644;TnALE&#30340;&#19979;&#38477;&#27493;&#39588;&#65292;&#35777;&#26126;&#22914;&#26524;&#22312;&#27599;&#20010;&#37051;&#22495;&#20013;&#36798;&#21040;&#20102;&#36275;&#22815;&#30340;&#30446;&#26631;&#20989;&#25968;&#38477;&#20302;&#65292;&#37027;&#20040;&#20004;&#31181;&#31639;&#27861;&#37117;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#24230;&#65292;&#30452;&#21040;&#19968;&#20010;&#24120;&#25968;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;TNLS&#21644;TnALE&#30340;&#35780;&#20272;&#25928;&#29575;&#65292;&#25581;&#31034;&#20102;&#22312;TNLS&#20013;&#36890;&#24120;&#38656;&#35201;&#937;(2 ^ N)&#20010;&#35780;&#20272;&#25165;&#33021;&#22312;&#37051;&#22495;&#20869;&#36798;&#21040;&#30446;&#26631;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor network (TN) is a powerful framework in machine learning, but selecting a good TN model, known as TN structure search (TN-SS), is a challenging and computationally intensive task. The recent approach TNLS~\cite{li2022permutation} showed promising results for this task, however, its computational efficiency is still unaffordable, requiring too many evaluations of the objective function. We propose TnALE, a new algorithm that updates each structure-related variable alternately by local enumeration, \emph{greatly} reducing the number of evaluations compared to TNLS. We theoretically investigate the descent steps for TNLS and TnALE, proving that both algorithms can achieve linear convergence up to a constant if a sufficient reduction of the objective is \emph{reached} in each neighborhood. We also compare the evaluation efficiency of TNLS and TnALE, revealing that $\Omega(2^N)$ evaluations are typically required in TNLS for \emph{reaching} the objective reduction in the neighborhood
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35745;&#31639;&#20869;&#23384;&#21152;&#36895;&#22120;&#21151;&#32791;&#21103;&#20449;&#36947;&#25915;&#20987;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22823;&#22122;&#22768;&#21644;&#23545;&#25239;&#25514;&#26045;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#37325;&#26500;&#29992;&#25143;&#30340;&#31169;&#26377;&#36755;&#20837;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.11056</link><description>&lt;p&gt;
PowerGAN: &#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35745;&#31639;&#20869;&#23384;&#21152;&#36895;&#22120;&#21151;&#32791;&#21103;&#20449;&#36947;&#25915;&#20987;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PowerGAN: A Machine Learning Approach for Power Side-Channel Attack on Compute-in-Memory Accelerators. (arXiv:2304.11056v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35745;&#31639;&#20869;&#23384;&#21152;&#36895;&#22120;&#21151;&#32791;&#21103;&#20449;&#36947;&#25915;&#20987;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22823;&#22122;&#22768;&#21644;&#23545;&#25239;&#25514;&#26045;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#37325;&#26500;&#29992;&#25143;&#30340;&#31169;&#26377;&#36755;&#20837;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#20351;&#29992;&#27169;&#25311;&#35745;&#31639;&#20869;&#23384;&#65288;CIM&#65289;&#21152;&#36895;&#22120;&#36827;&#34892;DNN&#25512;&#29702;&#30340;&#33021;&#28304;&#25928;&#29575;&#21644;&#21306;&#22359;&#21521;&#37327;&#20056;&#27861;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20445;&#25252;&#29992;&#25143;&#36755;&#20837;&#38544;&#31169;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#23433;&#20840;&#28431;&#27934;&#65292;&#21363;&#22312;&#19968;&#20010;&#36866;&#24403;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#39044;&#22788;&#29702;&#19979;&#65292;&#21363;&#20351;&#27809;&#26377;DNN&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#25915;&#20987;&#32773;&#20063;&#21487;&#20197;&#20174;&#21151;&#32791;&#20391;&#20449;&#36947;&#25915;&#20987;&#37325;&#26500;&#29992;&#25143;&#30340;&#31169;&#26377;&#36755;&#20837;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#22686;&#24378;&#37325;&#26500;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#22823;&#22122;&#22768;&#27700;&#24179;&#21644;&#23545;&#25239;&#25514;&#26045;&#34987;&#24212;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#22312;&#20174;&#27169;&#25311;CIM&#21152;&#36895;&#22120;&#21151;&#32791;&#27844;&#28431;&#37325;&#26500;&#29992;&#25143;&#36755;&#20837;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25968;&#25454;&#20013;&#29992;&#20110;&#33041;&#32959;&#30244;&#26816;&#27979;&#30340;U-Net&#19978;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analog compute-in-memory (CIM) accelerators are becoming increasingly popular for deep neural network (DNN) inference due to their energy efficiency and in-situ vector-matrix multiplication (VMM) capabilities. However, as the use of DNNs expands, protecting user input privacy has become increasingly important. In this paper, we identify a security vulnerability wherein an adversary can reconstruct the user's private input data from a power side-channel attack, under proper data acquisition and pre-processing, even without knowledge of the DNN model. We further demonstrate a machine learning-based attack approach using a generative adversarial network (GAN) to enhance the reconstruction. Our results show that the attack methodology is effective in reconstructing user inputs from analog CIM accelerator power leakage, even when at large noise levels and countermeasures are applied. Specifically, we demonstrate the efficacy of our approach on the U-Net for brain tumor detection in magnetic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#20272;&#20540;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#20272;&#20540;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.10701</link><description>&lt;p&gt;
&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Matching-based Data Valuation for Generative Model. (arXiv:2304.10701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#20272;&#20540;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#20272;&#20540;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20272;&#20540;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#22686;&#24378;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#24182;&#20445;&#25252;&#25968;&#25454;&#29305;&#24615;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21028;&#21035;&#27169;&#22411;&#19978;&#65292;&#24573;&#30053;&#20102;&#26368;&#36817;&#21560;&#24341;&#20102;&#22823;&#37327;&#20851;&#27880;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#21028;&#21035;&#27169;&#22411;&#31867;&#20284;&#65292;&#38656;&#35201;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#25968;&#25454;&#36129;&#29486;&#30340;&#32039;&#36843;&#38656;&#27714;&#20063;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21028;&#21035;&#27169;&#22411;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#22312;&#23454;&#38469;&#20013;&#30452;&#25509;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#36817;&#26399;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20174;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#35282;&#24230;&#23545;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#20272;&#20540;&#38382;&#39064;&#36827;&#34892;&#20102;&#26500;&#24314;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;Generative Model Valuator&#8221;&#65288;GMValuator&#65289;&#8212;&#8212;&#31532;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#25968;&#25454;&#20272;&#20540;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#23454;&#20363;&#21450;&#30001;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#24212;&#21512;&#25104;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#20272;&#35745;&#21407;&#22987;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20026;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#21253;&#25324;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#35780;&#20272;&#25968;&#25454;&#23454;&#20363;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is critical in machine learning, as it helps enhance model transparency and protect data properties. Existing data valuation methods have primarily focused on discriminative models, neglecting deep generative models that have recently gained considerable attention. Similar to discriminative models, there is an urgent need to assess data contributions in deep generative models as well. However, previous data valuation approaches mainly relied on discriminative model performance metrics and required model retraining. Consequently, they cannot be applied directly and efficiently to recent deep generative models, such as generative adversarial networks and diffusion models, in practice. To bridge this gap, we formulate the data valuation problem in generative models from a similarity-matching perspective. Specifically, we introduce Generative Model Valuator (GMValuator), the first model-agnostic approach for any generative models, designed to provide data valuation for gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#35821;&#20041;&#32500;&#24230;&#20316;&#20026;&#25351;&#32441;&#30340;&#36861;&#28335;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#26512;&#35774;&#35745;&#21464;&#37327;&#23545;&#20110;&#20934;&#30830;&#24615;-&#36136;&#37327;&#26435;&#34913;&#30340;&#24433;&#21709;&#65292;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#35745;&#31639;&#37327;&#65292;&#26356;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.09752</link><description>&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;&#25351;&#32441;&#36861;&#28335;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Attributing Image Generative Models using Latent Fingerprints. (arXiv:2304.09752v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#35821;&#20041;&#32500;&#24230;&#20316;&#20026;&#25351;&#32441;&#30340;&#36861;&#28335;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#26512;&#35774;&#35745;&#21464;&#37327;&#23545;&#20110;&#20934;&#30830;&#24615;-&#36136;&#37327;&#26435;&#34913;&#30340;&#24433;&#21709;&#65292;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#35745;&#31639;&#37327;&#65292;&#26356;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20351;&#24471;&#20135;&#29983;&#30340;&#20869;&#23481;&#38590;&#20197;&#21306;&#20998;&#26159;&#21542;&#28304;&#20110;&#33258;&#28982;&#29615;&#22659;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#24320;&#28304;&#24320;&#21457;&#24341;&#36215;&#20102;&#23545;&#20110;&#20854;&#34987;&#24694;&#24847;&#21033;&#29992;&#30340;&#25285;&#24551;&#12290;&#20854;&#20013;&#19968;&#20010;&#28508;&#22312;&#30340;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#26159;&#36890;&#36807;&#25351;&#32441;&#36861;&#28335;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#32441;&#36861;&#28335;&#26041;&#27861;&#22312;&#36861;&#28335;&#20934;&#30830;&#24615;&#19982;&#29983;&#25104;&#36136;&#37327;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#26435;&#34913;&#65292;&#24182;&#19988;&#32570;&#20047;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28508;&#22312;&#35821;&#20041;&#32500;&#24230;&#20316;&#20026;&#25351;&#32441;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#20998;&#26512;&#35774;&#35745;&#21464;&#37327;&#23545;&#20110;&#20934;&#30830;&#24615;-&#36136;&#37327;&#26435;&#34913;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#25351;&#32441;&#32500;&#24230;&#30340;&#36873;&#25321;&#12289;&#24378;&#24230;&#21644;&#23481;&#37327;&#12290;&#30456;&#27604;&#20043;&#21069;&#30340; SOTA&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#26368;&#23569;&#30340;&#35745;&#31639;&#65292;&#24182;&#19988;&#26356;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992; StyleGAN2 &#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have enabled the creation of contents that are indistinguishable from those taken from the nature. Open-source development of such models raised concerns about the risks in their misuse for malicious purposes. One potential risk mitigation strategy is to attribute generative models via fingerprinting. Current fingerprinting methods exhibit significant tradeoff between robust attribution accuracy and generation quality, and also lack designing principles to improve this tradeoff. This paper investigates the use of latent semantic dimensions as fingerprints, from where we can analyze the effects of design variables, including the choice of fingerprinting dimensions, strength, and capacity, on the accuracy-quality tradeoff. Compared with previous SOTA, our method requires minimum computation and is more applicable to large-scale models. We use StyleGAN2 and the latent diffusion model to demonstrate the efficacy of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#24102;&#25805;&#20316;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22810;&#31181;&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#20844;&#24320;GitHub&#20195;&#30721;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2304.08743</link><description>&lt;p&gt;
&#24102;&#25805;&#20316;&#32422;&#26463;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#19979;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Actor-Critic Deep Reinforcement Learning Algorithms for Robotics Control with Action Constraints. (arXiv:2304.08743v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#24102;&#25805;&#20316;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22810;&#31181;&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#20844;&#24320;GitHub&#20195;&#30721;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25805;&#20316;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25805;&#20316;&#21463;&#38480;&#30340;RL&#20013;&#65292;&#23398;&#20064;&#31995;&#32479;&#37319;&#21462;&#30340;&#27599;&#20010;&#25805;&#20316;&#37117;&#24517;&#39035;&#31526;&#21512;&#26576;&#20123;&#32422;&#26463;&#26465;&#20214;&#12290;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#23545;&#20110;&#30830;&#20445;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#25805;&#20316;&#30340;&#21487;&#34892;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#29616;&#26377;&#31639;&#27861;&#21450;&#20854;&#26032;&#39062;&#30340;&#21464;&#20307;&#65292;&#28085;&#30422;&#22810;&#31181;&#25805;&#20316;&#38480;&#21046;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;&#28145;&#20837;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#20986;&#20154;&#24847;&#26009;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;&#22522;&#32447;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#20351;&#29992;&#30340;&#22522;&#20934;&#38382;&#39064;&#21644;&#30456;&#20851;&#20195;&#30721;&#21487;&#22312;github.com/omron-sinicx/action-constrained-RL-benchmark&#19978;&#22312;&#32447;&#33719;&#21462;&#65292;&#20197;&#36827;&#19968;&#27493;&#24320;&#23637;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a benchmark for evaluating action-constrained reinforcement learning (RL) algorithms. In action-constrained RL, each action taken by the learning system must comply with certain constraints. These constraints are crucial for ensuring the feasibility and safety of actions in real-world systems. We evaluate existing algorithms and their novel variants across multiple robotics control environments, encompassing multiple action constraint types. Our evaluation provides the first in-depth perspective of the field, revealing surprising insights, including the effectiveness of a straightforward baseline approach. The benchmark problems and associated code utilized in our experiments are made available online at github.com/omron-sinicx/action-constrained-RL-benchmark for further research and development.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;INDEED&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#19981;&#21516;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#30340;&#25805;&#20316;&#31526;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#19988;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.07993</link><description>&lt;p&gt;
&#20869;&#22312;&#19978;&#19979;&#25991;&#31639;&#23376;&#23398;&#20064;&#29992;&#20110;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
In-Context Operator Learning for Differential Equation Problems. (arXiv:2304.07993v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;INDEED&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#19981;&#21516;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#30340;&#25805;&#20316;&#31526;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#19988;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#8212;&#8212;IN-context Differential Equation Encoder-Decoder&#65288;INDEED&#65289;&#65292;&#29992;&#20110;&#20174;&#25968;&#25454;&#20013;&#21516;&#26102;&#23398;&#20064;&#25805;&#20316;&#31526;&#24182;&#22312;&#25512;&#29702;&#38454;&#27573;&#23558;&#20854;&#24212;&#29992;&#20110;&#26032;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#26435;&#37325;&#26356;&#26032;&#12290;&#29616;&#26377;&#26041;&#27861;&#23616;&#38480;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#29305;&#23450;&#30340;&#26041;&#31243;&#35299;&#25110;&#29305;&#23450;&#30340;&#25805;&#20316;&#31526;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#26041;&#31243;&#30340;&#26032;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#25805;&#20316;&#31526;&#23398;&#20064;&#22120;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#25670;&#33073;&#20026;&#26032;&#38382;&#39064;&#37325;&#26032;&#35757;&#32451;&#65288;&#29978;&#33267;&#24494;&#35843;&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#22256;&#25200;&#65292;&#36824;&#21487;&#20197;&#21033;&#29992;&#25805;&#20316;&#31526;&#20043;&#38388;&#20849;&#20139;&#30340;&#20849;&#21516;&#28857;&#65292;&#36825;&#26679;&#22312;&#23398;&#20064;&#26032;&#30340;&#25805;&#20316;&#31526;&#26102;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#28436;&#31034;&#21363;&#21487;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#65292;&#21253;&#25324;ODE&#21644;PDE&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#65292;&#21516;&#26102;&#26174;&#31034;&#23427;&#21487;&#20197;&#25512;&#24191;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new neural-network-based approach, namely IN-context Differential Equation Encoder-Decoder (INDEED), to simultaneously learn operators from data and apply it to new questions during the inference stage, without any weight update. Existing methods are limited to using a neural network to approximate a specific equation solution or a specific operator, requiring retraining when switching to a new problem with different equations. By training a single neural network as an operator learner, we can not only get rid of retraining (even fine-tuning) the neural network for new problems, but also leverage the commonalities shared across operators so that only a few demos are needed when learning a new operator. Our numerical results show the neural network's capability as a few-shot operator learner for a diversified type of differential equation problems, including forward and inverse problems of ODEs and PDEs, and also show that it can generalize its learning capabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23558;&#31070;&#32463;&#25805;&#20316;&#31526;&#23398;&#20064;&#24212;&#29992;&#20110;&#36229;&#22768;&#26029;&#23618;&#25104;&#20687;&#21453;&#28436;&#65292;&#36890;&#36807;&#23398;&#20064;&#26102;&#38388;&#39134;&#34892;&#25968;&#25454;&#21644;&#24322;&#36136;&#22768;&#36895;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#21487;&#36991;&#20813;&#35745;&#31639;&#23494;&#38598;&#22411;&#21453;&#28436;&#36807;&#31243;&#30340;&#39044;&#27979;&#24322;&#36136;&#22768;&#22330;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#28508;&#22312;&#30340;&#22312;&#20083;&#33146;&#25104;&#20687;&#20013;&#36827;&#34892;&#36719;&#32452;&#32455;&#20998;&#24067;&#39044;&#27979;&#21644;&#32959;&#30244;&#35782;&#21035;&#30340;&#23454;&#26102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.03297</link><description>&lt;p&gt;
&#36229;&#22768;&#26029;&#23618;&#25104;&#20687;&#21453;&#28436;&#30340;&#31070;&#32463;&#25805;&#20316;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neural Operator Learning for Ultrasound Tomography Inversion. (arXiv:2304.03297v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23558;&#31070;&#32463;&#25805;&#20316;&#31526;&#23398;&#20064;&#24212;&#29992;&#20110;&#36229;&#22768;&#26029;&#23618;&#25104;&#20687;&#21453;&#28436;&#65292;&#36890;&#36807;&#23398;&#20064;&#26102;&#38388;&#39134;&#34892;&#25968;&#25454;&#21644;&#24322;&#36136;&#22768;&#36895;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#21487;&#36991;&#20813;&#35745;&#31639;&#23494;&#38598;&#22411;&#21453;&#28436;&#36807;&#31243;&#30340;&#39044;&#27979;&#24322;&#36136;&#22768;&#22330;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#28508;&#22312;&#30340;&#22312;&#20083;&#33146;&#25104;&#20687;&#20013;&#36827;&#34892;&#36719;&#32452;&#32455;&#20998;&#24067;&#39044;&#27979;&#21644;&#32959;&#30244;&#35782;&#21035;&#30340;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25805;&#20316;&#31526;&#23398;&#20064;&#20316;&#20026;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#36827;&#34892;&#22797;&#26434;&#20989;&#25968;&#31354;&#38388;&#26144;&#23556;&#30340;&#19968;&#31181;&#25163;&#27573;&#24050;&#32463;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#31070;&#32463;&#25805;&#20316;&#31526;&#23398;&#20064;&#24212;&#29992;&#20110;&#39134;&#34892;&#26102;&#38388;&#36229;&#22768;&#35745;&#31639;&#23618;&#26512;&#25104;&#20687;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20840;&#27874;&#27714;&#35299;&#22120;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#38388;&#39134;&#34892;&#65288;TOF&#65289;&#25968;&#25454;&#21644;&#24322;&#36136;&#22768;&#36895;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#35813;&#25805;&#20316;&#31526;&#23398;&#20064;&#30340;&#26032;&#39062;&#24212;&#29992;&#35268;&#36991;&#20102;&#38656;&#35201;&#35299;&#20915;&#35745;&#31639;&#23494;&#38598;&#22411;&#36845;&#20195;&#21453;&#38382;&#39064;&#30340;&#38656;&#27714;&#12290;&#35813;&#25805;&#20316;&#31526;&#23398;&#20064;&#22312;&#31163;&#32447;&#27169;&#24335;&#19979;&#23398;&#20064;&#38750;&#32447;&#24615;&#26144;&#23556;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#30340;&#21333;&#27425;&#21069;&#21521;&#36890;&#36807;&#26469;&#39044;&#27979;&#24322;&#36136;&#22768;&#22330;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23558;&#25805;&#20316;&#31526;&#23398;&#20064;&#29992;&#20110;&#36229;&#22768;&#26029;&#23618;&#25104;&#20687;&#65292;&#20063;&#26159;&#28508;&#22312;&#30340;&#23454;&#26102;&#39044;&#27979;&#20083;&#33146;&#25104;&#20687;&#20013;&#30340;&#36719;&#32452;&#32455;&#20998;&#24067;&#20197;&#36827;&#34892;&#32959;&#30244;&#35782;&#21035;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operator learning as a means of mapping between complex function spaces has garnered significant attention in the field of computational science and engineering (CS&amp;E). In this paper, we apply Neural operator learning to the time-of-flight ultrasound computed tomography (USCT) problem. We learn the mapping between time-of-flight (TOF) data and the heterogeneous sound speed field using a full-wave solver to generate the training data. This novel application of operator learning circumnavigates the need to solve the computationally intensive iterative inverse problem. The operator learns the non-linear mapping offline and predicts the heterogeneous sound field with a single forward pass through the model. This is the first time operator learning has been used for ultrasound tomography and is the first step in potential real-time predictions of soft tissue distribution for tumor identification in beast imaging.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Re-IQA &#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#35757;&#32451;&#20004;&#20010;&#32534;&#30721;&#22120;&#65292;&#23398;&#20064;&#22270;&#20687;&#30340;&#39640;&#32423;&#20869;&#23481;&#21644;&#20302;&#32423;&#36136;&#37327;&#29305;&#24449;&#65292;&#20197;&#29983;&#25104;&#20114;&#34917;&#30340;&#20302;&#21644;&#39640;&#32423;&#22270;&#20687;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#37326;&#22806;&#33258;&#21160;&#21270;&#30340;&#24863;&#30693;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#65292;&#19988;&#22312;&#22810;&#20010;&#22823;&#22411;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#24211;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00451</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#37326;&#22806;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#30340; Re-IQA &#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild. (arXiv:2304.00451v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00451
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Re-IQA &#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#35757;&#32451;&#20004;&#20010;&#32534;&#30721;&#22120;&#65292;&#23398;&#20064;&#22270;&#20687;&#30340;&#39640;&#32423;&#20869;&#23481;&#21644;&#20302;&#32423;&#36136;&#37327;&#29305;&#24449;&#65292;&#20197;&#29983;&#25104;&#20114;&#34917;&#30340;&#20302;&#21644;&#39640;&#32423;&#22270;&#20687;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#37326;&#22806;&#33258;&#21160;&#21270;&#30340;&#24863;&#30693;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#65292;&#19988;&#22312;&#22810;&#20010;&#22823;&#22411;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#24211;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#24863;&#30693;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#27599;&#22825;&#24433;&#21709;&#30528;&#25968;&#21313;&#20159;&#20114;&#32852;&#32593;&#21644;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#12290;&#20026;&#20102;&#25512;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#65292;&#35757;&#32451;&#20004;&#20010;&#29420;&#31435;&#30340;&#32534;&#30721;&#22120;&#22312;&#26080;&#30417;&#30563;&#30340;&#29615;&#22659;&#19979;&#23398;&#20064;&#39640;&#23618;&#27425;&#30340;&#20869;&#23481;&#21644;&#20302;&#23618;&#27425;&#30340;&#22270;&#20687;&#36136;&#37327;&#29305;&#24449;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#23427;&#33021;&#22815;&#29983;&#25104;&#19982;&#34920;&#31034;&#22270;&#20687;&#20869;&#23481;&#30340;&#39640;&#32423;&#29305;&#24449;&#20114;&#34917;&#30340;&#22270;&#20687;&#36136;&#37327;&#30340;&#20302;&#32423;&#34920;&#31034;&#12290;&#25105;&#20204;&#31216;&#29992;&#20110;&#35757;&#32451;&#36825;&#20004;&#20010;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#20026; Re-IQA&#12290;&#20026;&#20102;&#22312;&#37326;&#22806;&#35780;&#20272;&#22270;&#20687;&#36136;&#37327;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174; Re-IQA &#26694;&#26550;&#33719;&#24471;&#30340;&#20114;&#34917;&#30340;&#20302;&#21644;&#39640;&#32423;&#22270;&#20687;&#34920;&#31034;&#26469;&#35757;&#32451;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#22270;&#20687;&#34920;&#31034;&#26144;&#23556;&#21040;&#36136;&#37327;&#24471;&#20998;&#65292;&#20855;&#20307;&#32454;&#33410;&#35265;&#22270;1&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#21547;&#30495;&#23454;&#21644;&#21512;&#25104;&#22270;&#20687;&#30340;&#22810;&#20010;&#22823;&#22411;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#24211;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Perceptual Image Quality Assessment is a challenging problem that impacts billions of internet, and social media users daily. To advance research in this field, we propose a Mixture of Experts approach to train two separate encoders to learn high-level content and low-level image quality features in an unsupervised setting. The unique novelty of our approach is its ability to generate low-level representations of image quality that are complementary to high-level features representing image content. We refer to the framework used to train the two encoders as Re-IQA. For Image Quality Assessment in the Wild, we deploy the complementary low and high-level image representations obtained from the Re-IQA framework to train a linear regression model, which is used to map the image representations to the ground truth quality scores, refer Figure 1. Our method achieves state-of-the-art performance on multiple large-scale image quality assessment databases containing both real and syn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#27979;&#24615;&#19978;&#19979;&#25991;&#24863;&#30693;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#21644;&#25509;&#25910;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#65292;&#23454;&#29616;&#38754;&#21521;&#20840;&#27785;&#28024;&#22810;&#29992;&#25143;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#39640;&#25928;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2303.17907</link><description>&lt;p&gt;
&#38754;&#21521;&#20840;&#27785;&#28024;&#22810;&#29992;&#25143;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#39044;&#27979;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#37325;&#23450;&#21521;&#27493;&#34892;
&lt;/p&gt;
&lt;p&gt;
Predictive Context-Awareness for Full-Immersive Multiuser Virtual Reality with Redirected Walking. (arXiv:2303.17907v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#27979;&#24615;&#19978;&#19979;&#25991;&#24863;&#30693;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#21644;&#25509;&#25910;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#65292;&#23454;&#29616;&#38754;&#21521;&#20840;&#27785;&#28024;&#22810;&#29992;&#25143;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#39640;&#25928;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#27491;&#26397;&#30528;&#22686;&#24378;&#27785;&#28024;&#24863;&#12289;&#25903;&#25345;&#22810;&#29992;&#25143;&#20307;&#39564;&#21644;&#22312;&#34394;&#25311;&#20307;&#39564;&#20013;&#25903;&#25345;&#26080;&#38480;&#21046;&#30340;&#31227;&#21160;&#65292;&#32780;&#36890;&#36807;&#37325;&#23450;&#21521;&#27493;&#34892;&#23558;&#29992;&#25143;&#38480;&#21046;&#22312;&#19987;&#38376;&#30340;VR&#35774;&#32622;&#20869;&#12290;&#20026;&#20102;&#28385;&#36275;&#26410;&#26469;VR&#31995;&#32479;&#30340;&#26497;&#31471;&#25968;&#25454;&#36895;&#29575;&#21644;&#24310;&#36831;&#35201;&#27714;&#65292;&#25903;&#25345;&#26080;&#32447;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#23558;&#22312;&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#39057;&#29575;&#19978;&#36816;&#34892;&#65292;&#24182;&#36890;&#36807;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#23454;&#29616;&#39640;&#24230;&#23450;&#21521;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#27979;&#24615;&#19978;&#19979;&#25991;&#24863;&#30693;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#21644;&#25509;&#25910;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#30701;&#26399;&#39044;&#27979;&#22810;&#29992;&#25143;VR&#35774;&#32622;&#20013;&#29992;&#25143;&#30340;&#27178;&#21521;&#31227;&#21160;&#65292;&#21487;&#20197;&#21033;&#29992;&#29992;&#25143;&#26041;&#21521;&#19978;&#30340;&#30452;&#32447;&#35270;&#36317;&#65288;LoS&#65289;&#8220;&#36319;&#36394;&#8221;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual Reality (VR) technology is being advanced along the lines of enhancing its immersiveness, enabling multiuser Virtual Experiences (VEs), and supporting unconstrained mobility of the users in their VEs, while constraining them within specialized VR setups through Redirected Walking (RDW). For meeting the extreme data-rate and latency requirements of future VR systems, supporting wireless networking infrastructures will operate in millimeter Wave (mmWave) frequencies and leverage highly directional communication in both transmission and reception through beamforming and beamsteering. We propose to leverage predictive context-awareness for optimizing transmitter and receiver-side beamforming and beamsteering. In particular, we argue that short-term prediction of users' lateral movements in multiuser VR setups with RDW can be utilized for optimizing transmitter-side beamforming and beamsteering through Line-of-Sight (LoS) "tracking" in the users' directions. At the same time, short-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;HARFLOW3D&#65292;&#23427;&#20197;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;FPGA&#30340;&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;HARFLOW3D&#30456;&#27604;&#20854;&#20182;&#26041;&#26696;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2303.17218</link><description>&lt;p&gt;
HARFLOW3D&#65306;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;
&lt;/p&gt;
&lt;p&gt;
HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices. (arXiv:2303.17218v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;HARFLOW3D&#65292;&#23427;&#20197;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;FPGA&#30340;&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;HARFLOW3D&#30456;&#27604;&#20854;&#20182;&#26041;&#26696;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#22312;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27969;&#24335;&#26550;&#26500;&#30340;&#24037;&#20855;&#38142;&#65292;&#23558;&#27492;&#31867;&#27169;&#22411;&#26144;&#23556;&#21040;FPGA&#19978;&#65292;&#32771;&#34385;&#27169;&#22411;&#22266;&#26377;&#29305;&#24615;&#21644;&#30446;&#26631;FPGA&#35774;&#22791;&#30340;&#29305;&#24449;&#12290;HARFLOW3D&#24037;&#20855;&#38142;&#20197;ONNX&#26684;&#24335;&#30340;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;FPGA&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#35813;&#24037;&#20855;&#38142;&#30001;&#22810;&#20010;&#37096;&#20998;&#32452;&#25104;&#65292;&#21253;&#25324;i) 3D CNN&#35299;&#26512;&#22120;&#65292;ii) &#24615;&#33021;&#21644;&#36164;&#28304;&#27169;&#22411;&#65292;iii) &#29992;&#20110;&#22312;&#29983;&#25104;&#30340;&#30828;&#20214;&#19978;&#25191;&#34892;3D&#27169;&#22411;&#30340;&#35843;&#24230;&#31639;&#27861;&#65292;iv) &#38024;&#23545;3D&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#36164;&#28304;&#24863;&#30693;&#20248;&#21270;&#24341;&#25806;&#65292;v) &#33258;&#21160;&#26144;&#23556;&#21040;&#21487;&#21512;&#25104;&#30340;FPGA&#20195;&#30721;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;3D CNN&#21644;FPGA&#31995;&#32479;&#37197;&#23545;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#24037;&#20855;&#38142;&#25903;&#25345;&#24191;&#27867;&#27169;&#22411;&#21644;&#35774;&#22791;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;3D CNN&#21152;&#36895;&#22120;&#35774;&#35745;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#24037;&#20855;&#38142;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks have proven to be highly effective, achieving state-of-the-art results. This study introduces a novel streaming architecture based toolflow for mapping such models onto FPGAs considering the model's inherent characteristics and the features of the targeted FPGA device. The HARFLOW3D toolflow takes as input a 3D CNN in ONNX format and a description of the FPGA characteristics, generating a design that minimizes the latency of the computation. The toolflow is comprised of a number of parts, including i) a 3D CNN parser, ii) a performance and resource model, iii) a scheduling algorithm for executing 3D models on the generated hardware, iv) a resource-aware optimization engine tailored for 3D models, v) an automated mapping to synthesizable code for FPGAs. The ability of the toolflow to support a broad range of models and devices is shown through a number of experiments on various 3D CNN and FPGA system pairs. Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; Iterative Markovian Fitting&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#24230; Schr\"odinger&#26725;&#65288;SBs&#65289;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#29616;&#20986;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.16852</link><description>&lt;p&gt;
&#25193;&#25955;Schr\"odinger&#26725;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Diffusion Schr\"odinger Bridge Matching. (arXiv:2303.16852v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; Iterative Markovian Fitting&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#24230; Schr\"odinger&#26725;&#65288;SBs&#65289;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#29616;&#20986;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#36816;&#36755;&#38382;&#39064;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#30528;&#35768;&#22810;&#24212;&#29992;&#65292;&#20363;&#22914;&#26032;&#22411;&#30340;&#36136;&#37327;&#20256;&#36755;&#26041;&#27861;&#65292;&#22914;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDMs&#65289;&#21644;&#27969;&#21305;&#37197;&#27169;&#22411;&#65288;FMMs&#65289;&#65292;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#25110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#23454;&#29616;&#36825;&#26679;&#30340;&#20256;&#36755;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#36817;&#20284;&#30830;&#23450;&#24615;&#21160;&#24577;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#26144;&#23556;&#26159;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#24615;&#36136;&#65292;&#20294; DDMs &#21644; FMMs &#24182;&#19981;&#33021;&#20445;&#35777;&#25552;&#20379;&#25509;&#36817; OT &#26144;&#23556;&#30340;&#20256;&#36755;&#12290;&#30456;&#21453;&#65292;Schr\"odinger&#26725;&#65288;SBs&#65289;&#35745;&#31639;&#38543;&#26426;&#21160;&#24577;&#26144;&#23556;&#65292;&#21487;&#20197;&#24674;&#22797;&#27491;&#21017;&#29109;&#29256;&#26412;&#30340; OT&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#25968;&#20540;&#26041;&#27861;&#36817;&#20284; SBs &#30340;&#32500;&#24230;&#32553;&#25918;&#24046;&#25110;&#22312;&#36845;&#20195;&#20013;&#31215;&#32047;&#35823;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36845;&#20195;&#39532;&#23572;&#31185;&#22827;&#25311;&#21512;&#65292;&#19968;&#31181;&#35299;&#20915;&#39640;&#32500;&#24230; SB &#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26041;&#27861;&#35774;&#35745;&#20026;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#23558;&#32622;&#20449;&#20256;&#25773;&#25193;&#23637;&#21040; KL &#25955;&#24230;&#65292;&#21033;&#29992;&#26465;&#20214;&#29420;&#31435;&#24615;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#30830;&#20445;&#19968;&#33268;&#24615;&#21644;&#25910;&#25947;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#29616;&#26377;&#25104;&#26524;&#26041;&#27861;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving transport problems, i.e. finding a map transporting one given distribution to another, has numerous applications in machine learning. Novel mass transport methods motivated by generative modeling have recently been proposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models (FMMs) implement such a transport through a Stochastic Differential Equation (SDE) or an Ordinary Differential Equation (ODE). However, while it is desirable in many applications to approximate the deterministic dynamic Optimal Transport (OT) map which admits attractive properties, DDMs and FMMs are not guaranteed to provide transports close to the OT map. In contrast, Schr\"odinger bridges (SBs) compute stochastic dynamic mappings which recover entropy-regularized versions of OT. Unfortunately, existing numerical methods approximating SBs either scale poorly with dimension or accumulate errors across iterations. In this work, we introduce Iterative Markovian Fitting, a new methodology for solv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#35823;&#24046;&#39044;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#25506;&#35752;&#20102;&#32622;&#20449;&#24230;&#12289;&#23616;&#37096;&#27969;&#24418;&#24179;&#28369;&#24230;&#21644;&#27169;&#22411;&#19968;&#33268;&#24615;&#35780;&#20998;&#20989;&#25968;&#30340;&#20248;&#32570;&#28857;&#65292;&#21457;&#29616;&#22312;&#22797;&#26434;&#26426;&#21046;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20808;&#36827;&#30340;&#35780;&#20998;&#26080;&#27861;&#22312;&#20998;&#24067;&#36716;&#31227;&#21644;&#25439;&#22351;&#19979;&#36229;&#36234;&#31616;&#21333;&#30340;&#27169;&#22411;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#22312;&#21463;&#25439;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#19968;&#33268;&#24615;&#25171;&#20998;&#20173;&#28982;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#38598;&#25104;&#22810;&#26679;&#24615;&#26377;&#21161;&#20110;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13589</link><description>&lt;p&gt;
Scoring Functions &#21644; Generalization Prediction &#30340;&#35814;&#32454;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Scoring Functions and Generalization Prediction. (arXiv:2303.13589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#35823;&#24046;&#39044;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#25506;&#35752;&#20102;&#32622;&#20449;&#24230;&#12289;&#23616;&#37096;&#27969;&#24418;&#24179;&#28369;&#24230;&#21644;&#27169;&#22411;&#19968;&#33268;&#24615;&#35780;&#20998;&#20989;&#25968;&#30340;&#20248;&#32570;&#28857;&#65292;&#21457;&#29616;&#22312;&#22797;&#26434;&#26426;&#21046;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20808;&#36827;&#30340;&#35780;&#20998;&#26080;&#27861;&#22312;&#20998;&#24067;&#36716;&#31227;&#21644;&#25439;&#22351;&#19979;&#36229;&#36234;&#31616;&#21333;&#30340;&#27169;&#22411;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#22312;&#21463;&#25439;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#19968;&#33268;&#24615;&#25171;&#20998;&#20173;&#28982;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#38598;&#25104;&#22810;&#26679;&#24615;&#26377;&#21161;&#20110;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#35823;&#24046;&#39044;&#27979;&#22120;&#65288;GEPs&#65289;&#30340;&#25928;&#26524;&#65292;&#36825;&#20123; GEPs &#26088;&#22312;&#36890;&#36807;&#20174;&#26679;&#26412;&#32423;&#20998;&#25968;&#20013;&#25512;&#23548;&#20986;&#25968;&#25454;&#38598;&#32423;&#35823;&#24046;&#20272;&#35745;&#20540;&#65292;&#20174;&#32780;&#39044;&#27979;&#27169;&#22411;&#22312;&#26410;&#35265;&#20998;&#24067;&#19978;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;GEPs &#24120;&#24120;&#21033;&#29992;&#19981;&#21516;&#30340;&#26426;&#21046;&#65288;&#20363;&#22914;&#65292;&#22238;&#24402;&#22120;&#12289;&#38408;&#20540;&#20989;&#25968;&#12289;&#26657;&#20934;&#25968;&#25454;&#38598;&#31561;&#65289;&#65292;&#26469;&#25512;&#23548;&#36825;&#31181;&#35823;&#24046;&#20272;&#35745;&#20540;&#65292;&#36825;&#20250;&#28151;&#28102;&#29305;&#23450;&#35780;&#20998;&#20989;&#25968;&#30340;&#20248;&#28857;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#22312;&#26426;&#21046;&#36873;&#25321;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#35780;&#20998;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;&#65288;&#32622;&#20449;&#24230;&#12289;&#23616;&#37096;&#27969;&#24418;&#24179;&#28369;&#24230;&#12289;&#27169;&#22411;&#19968;&#33268;&#24615;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22797;&#26434;&#26426;&#21046;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#20272;&#35745;&#20998;&#24067;&#36716;&#31227;&#21644;&#25439;&#22351;&#19979;&#30340;&#35823;&#24046;&#26102;&#65292;&#26368;&#20808;&#36827;&#30340;&#32622;&#20449;&#24230;&#21644;&#24179;&#28369;&#24230;&#22522;&#30784;&#35780;&#20998;&#26080;&#27861;&#36229;&#36234;&#31616;&#21333;&#30340;&#27169;&#22411;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#21463;&#21040;&#25439;&#23475;&#26102;&#65288;&#20363;&#22914;&#26631;&#31614;&#22122;&#22768;&#12289;&#27979;&#37327;&#22122;&#22768;&#12289;&#27424;&#37319;&#26679;&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#19968;&#33268;&#24615;&#25171;&#20998;&#20173;&#28982;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#38598;&#25104;&#22810;&#26679;&#24615;&#26377;&#21161;&#20110;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization error predictors (GEPs) aim to predict model performance on unseen distributions by deriving dataset-level error estimates from sample-level scores. However, GEPs often utilize disparate mechanisms (e.g., regressors, thresholding functions, calibration datasets, etc), to derive such error estimates, which can obfuscate the benefits of a particular scoring function. Therefore, in this work, we rigorously study the effectiveness of popular scoring functions (confidence, local manifold smoothness, model agreement), independent of mechanism choice. We find, absent complex mechanisms, that state-of-the-art confidence- and smoothness- based scores fail to outperform simple model-agreement scores when estimating error under distribution shifts and corruptions. Furthermore, on realistic settings where the training data has been compromised (e.g., label noise, measurement noise, undersampling), we find that model-agreement scores continue to perform well and that ensemble diversi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; PIQ&#65292;&#36890;&#36807;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#21521;&#37327;&#37327;&#21270;&#65292;&#23558;&#20854;&#34920;&#31034;&#36716;&#25442;&#20026;&#31163;&#25955;&#31867;&#29305;&#23450;&#30340;&#28508;&#31354;&#38388;&#65292;&#20174;&#32780;&#35299;&#37322;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#35813;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#23481;&#26131;&#35753;&#20154;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.12659</link><description>&lt;p&gt;
&#36890;&#36807;&#37327;&#21270;&#36827;&#34892;&#30340;&#20107;&#21518;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Posthoc Interpretation via Quantization. (arXiv:2303.12659v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; PIQ&#65292;&#36890;&#36807;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#21521;&#37327;&#37327;&#21270;&#65292;&#23558;&#20854;&#34920;&#31034;&#36716;&#25442;&#20026;&#31163;&#25955;&#31867;&#29305;&#23450;&#30340;&#28508;&#31354;&#38388;&#65292;&#20174;&#32780;&#35299;&#37322;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#35813;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#23481;&#26131;&#35753;&#20154;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#36890;&#36807;&#37327;&#21270;&#23454;&#29616;&#30340;&#20107;&#21518;&#35299;&#37322;&#65288;PIQ&#65289;&#8221;&#65292;&#29992;&#20110;&#35299;&#37322;&#35757;&#32451;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21521;&#37327;&#37327;&#21270;&#23558;&#20998;&#31867;&#22120;&#30340;&#34920;&#31034;&#36716;&#25442;&#20026;&#31163;&#25955;&#65292;&#31867;&#29305;&#23450;&#30340;&#28508;&#31354;&#38388;&#12290;&#31867;&#29305;&#23450;&#30340;&#30721;&#26412;&#20316;&#20026;&#29942;&#39048;&#65292;&#36843;&#20351;&#35299;&#37322;&#32773;&#19987;&#27880;&#20110;&#20998;&#31867;&#22120;&#35748;&#20026;&#29992;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;&#36755;&#20837;&#25968;&#25454;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#19982;&#25991;&#29486;&#20013;&#30340;&#20960;&#31181;&#20854;&#20182;&#35299;&#37322;&#26041;&#27861;&#30456;&#27604;&#65292;PIQ&#29983;&#25104;&#30340;&#35299;&#37322;&#26356;&#23481;&#26131;&#34987;&#21442;&#19982;&#25105;&#20204;&#29992;&#25143;&#30740;&#31350;&#30340;&#20154;&#25152;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new approach, called "Posthoc Interpretation via Quantization (PIQ)", for interpreting decisions made by trained classifiers. Our method utilizes vector quantization to transform the representations of a classifier into a discrete, class-specific latent space. The class-specific codebooks act as a bottleneck that forces the interpreter to focus on the parts of the input data deemed relevant by the classifier for making a prediction. We evaluated our method through quantitative and qualitative studies and found that PIQ generates interpretations that are more easily understood by participants to our user studies when compared to several other interpretation methods in the literature.
&lt;/p&gt;</description></item><item><title>CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08127</link><description>&lt;p&gt;
CB2&#65306;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08127
&lt;/p&gt;
&lt;p&gt;
CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CB2 &#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#24773;&#22659;&#19979;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010; 3D &#28216;&#25103;&#29615;&#22659;&#12289;&#19968;&#20010;&#21518;&#31471;&#26381;&#21153;&#22120;&#65292;&#21487;&#20026;&#20154;&#31867;&#26234;&#33021;&#20307;&#25552;&#20379;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#65292;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312; https://cb2.ai &#19978;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#31995;&#32479;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25935;&#24863;&#24230;&#26377;&#25928;&#29983;&#25104;MPC&#31574;&#30053;&#36924;&#36817;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#34987;&#25552;&#20986;&#65292;&#26412;&#25991;&#22312;&#27492;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#22522;&#20110;&#39044;&#27979;&#26657;&#27491;&#27493;&#39588;&#30340;&#25913;&#36827;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05607</link><description>&lt;p&gt;
MPC&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31574;&#30053;&#36924;&#36817;&#30340;&#25913;&#36827;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Improved Data Augmentation Scheme for Model Predictive Control Policy Approximation. (arXiv:2303.05607v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05607
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25935;&#24863;&#24230;&#26377;&#25928;&#29983;&#25104;MPC&#31574;&#30053;&#36924;&#36817;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#34987;&#25552;&#20986;&#65292;&#26412;&#25991;&#22312;&#27492;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#22522;&#20110;&#39044;&#27979;&#26657;&#27491;&#27493;&#39588;&#30340;&#25913;&#36827;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;MPC&#31574;&#30053;&#36924;&#36817;&#30340;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#12290;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#19968;&#20010;&#36817;&#20284;&#30340;MPC&#31574;&#30053;&#38656;&#35201;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36328;&#21487;&#34892;&#29366;&#24577;&#31354;&#38388;&#37319;&#26679;&#30340;&#26368;&#20248;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#35757;&#32451;&#26679;&#26412;&#30340;&#20851;&#38190;&#25361;&#25112;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#30340;MPC&#31574;&#30053;&#36924;&#36817;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#21033;&#29992;&#21442;&#25968;&#25935;&#24863;&#24230;&#20174;&#21333;&#20010;&#31163;&#32447;MPC&#35745;&#31639;&#20013;&#24265;&#20215;&#22320;&#29983;&#25104;&#22810;&#20010;&#39069;&#22806;&#26679;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#31934;&#30830;&#26679;&#26412;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#35823;&#24046;&#34987;&#35777;&#26126;&#38543;&#30528;&#25968;&#25454;&#22686;&#24378;&#30340;&#37051;&#22495;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#26657;&#27491;&#27493;&#39588;&#30340;&#25913;&#36827;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#65292;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#29992;&#25143;&#23450;&#20041;&#30340;&#31934;&#24230;&#32423;&#21035;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#22686;&#24378;&#26679;&#26412;&#30340;&#35823;&#24046;&#30028;&#29420;&#31435;&#20110;&#37051;&#22495;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of data generation for MPC policy approximation. Learning an approximate MPC policy from expert demonstrations requires a large data set consisting of optimal state-action pairs, sampled across the feasible state space. Yet, the key challenge of efficiently generating the training samples has not been studied widely. Recently, a sensitivity-based data augmentation framework for MPC policy approximation was proposed, where the parametric sensitivities are exploited to cheaply generate several additional samples from a single offline MPC computation. The error due to augmenting the training data set with inexact samples was shown to increase with the size of the neighborhood around each sample used for data augmentation. Building upon this work, this letter paper presents an improved data augmentation scheme based on predictor-corrector steps that enforces a user-defined level of accuracy, and shows that the error bound of the augmented samples are indepe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#31471;&#21040;&#31471;&#31574;&#30053;&#20989;&#25968;&#21644;&#20048;&#35266;&#24179;&#28369;&#34394;&#25311;&#21338;&#24328;&#31639;&#27861;&#24212;&#29992;&#20110;&#26356;&#21152;&#22797;&#26434;&#30340;&#21830;&#19994;&#28216;&#25103;&#29200;&#30707;&#25136;&#35352;&#65292;&#25552;&#20986;&#25913;&#36827;&#25216;&#26415;&#24182;&#22312;&#20154;&#26426;&#23545;&#25112;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.05197</link><description>&lt;p&gt;
&#25913;&#36827;&#25216;&#26415;&#25484;&#25569;&#31574;&#30053;&#21345;&#29260;&#28216;&#25103;&#65288;&#29200;&#30707;&#25136;&#35352;&#65289;
&lt;/p&gt;
&lt;p&gt;
Mastering Strategy Card Game (Hearthstone) with Improved Techniques. (arXiv:2303.05197v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#31471;&#21040;&#31471;&#31574;&#30053;&#20989;&#25968;&#21644;&#20048;&#35266;&#24179;&#28369;&#34394;&#25311;&#21338;&#24328;&#31639;&#27861;&#24212;&#29992;&#20110;&#26356;&#21152;&#22797;&#26434;&#30340;&#21830;&#19994;&#28216;&#25103;&#29200;&#30707;&#25136;&#35352;&#65292;&#25552;&#20986;&#25913;&#36827;&#25216;&#26415;&#24182;&#22312;&#20154;&#26426;&#23545;&#25112;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#21345;&#29260;&#28216;&#25103;&#26159;&#19968;&#31181;&#33879;&#21517;&#30340;&#28216;&#25103;&#31867;&#22411;&#65292;&#35201;&#27714;&#26234;&#33021;&#28216;&#25103;&#29609;&#27861;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#30340;&#29702;&#24819;&#27979;&#35797;&#24179;&#21488;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21512;&#24182;&#20102;&#31471;&#21040;&#31471;&#31574;&#30053;&#20989;&#25968;&#21644;&#20048;&#35266;&#24179;&#28369;&#34394;&#25311;&#21338;&#24328;&#65292;&#22312;&#12298;&#39764;&#27861;&#19982;&#39749;&#21147;&#65306;&#20195;&#30721;&#20256;&#35828;&#12299;&#31561;&#31574;&#30053;&#21345;&#29260;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#21040;&#29200;&#30707;&#25136;&#35352;&#19978;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20960;&#31181;&#25913;&#36827;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#36827;&#34892;&#26426;&#22120;&#23545;&#20154;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#36992;&#35831;&#20102;&#19968;&#20301;&#22312;&#20013;&#22269;&#23448;&#26041;&#32852;&#36187;&#20013;&#25490;&#21517;&#21069;&#21313;&#30340;&#29200;&#30707;&#25136;&#35352;&#25773;&#20027;&#65292;&#35813;&#22320;&#21306;&#30340;&#29609;&#23478;&#25968;&#37327;&#20272;&#35745;&#22312;&#25968;&#30334;&#19975;&#20154;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#23436;&#25972;&#28216;&#25103;&#65288;&#21253;&#25324;&#26500;&#24314;&#21345;&#32452;&#21644;&#23545;&#25112;&#65289;&#30340;&#20116;&#23616;&#27604;&#36187;&#20013;&#20987;&#36133;&#20102;&#20154;&#31867;&#29609;&#23478;&#65292;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Strategy card game is a well-known genre that is demanding on the intelligent game-play and can be an ideal test-bench for AI. Previous work combines an end-to-end policy function and an optimistic smooth fictitious play, which shows promising performances on the strategy card game Legend of Code and Magic. In this work, we apply such algorithms to Hearthstone, a famous commercial game that is more complicated in game rules and mechanisms. We further propose several improved techniques and consequently achieve significant progress. For a machine-vs-human test we invite a Hearthstone streamer whose best rank was top 10 of the official league in China region that is estimated to be of millions of players. Our models defeat the human player in all Best-of-5 tournaments of full games (including both deck building and battle), showing a strong capability of decision making.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25104;&#21151;&#35774;&#32622;&#21518;&#38376;&#65292;&#22312;&#29616;&#26377;&#38450;&#24481;&#25514;&#26045;&#19979;&#20063;&#20855;&#26377;&#24378;&#22823;&#30340;&#25915;&#20987;&#24615;&#33021;&#21644;&#32784;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.03320</link><description>&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35774;&#32622;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Learning to Backdoor Federated Learning. (arXiv:2303.03320v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25104;&#21151;&#35774;&#32622;&#21518;&#38376;&#65292;&#22312;&#29616;&#26377;&#38450;&#24481;&#25514;&#26045;&#19979;&#20063;&#20855;&#26377;&#24378;&#22823;&#30340;&#25915;&#20987;&#24615;&#33021;&#21644;&#32784;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#20013;&#65292;&#24694;&#24847;&#21442;&#19982;&#32773;&#21487;&#20197;&#36731;&#26131;&#22320;&#23558;&#21518;&#38376;&#23884;&#20837;&#21040;&#32858;&#21512;&#27169;&#22411;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#22312;&#20027;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#36817;&#26399;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#25514;&#26045;&#65292;&#21253;&#25324;&#35757;&#32451;&#38454;&#27573;&#30340;&#22522;&#20110;&#32858;&#21512;&#30340;&#38450;&#24481;&#21644;&#21518;&#26399;&#38450;&#24481;&#25514;&#26045;&#12290;&#34429;&#28982;&#36825;&#20123;&#38450;&#24481;&#25514;&#26045;&#22312;&#29616;&#26377;&#30340;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#21518;&#38376;&#25915;&#20987;&#20013;&#21487;&#20197;&#33719;&#24471;&#21512;&#29702;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#38754;&#23545;&#26356;&#39640;&#32423;&#30340;&#25915;&#20987;&#26102;&#19981;&#36275;&#20197;&#24212;&#23545;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65292;&#25915;&#20987;&#32773;&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;&#20854;&#26412;&#22320;&#25968;&#25454;&#21644;FL&#31995;&#32479;&#30340;&#20849;&#21516;&#30693;&#35782;&#24314;&#31435;&#30340;&#20223;&#30495;&#22120;&#35757;&#32451;&#19968;&#20010;&#65288;&#38750;&#36817;&#35270;&#65289;&#25915;&#20987;&#31574;&#30053;&#65292;&#28982;&#21518;&#22312;&#23454;&#38469;&#30340;FL&#35757;&#32451;&#20013;&#24212;&#29992;&#23427;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#26694;&#26550;&#26082;&#26159;&#36866;&#24212;&#24615;&#21448;&#26159;&#28789;&#27963;&#30340;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25514;&#26045;&#19979;&#20063;&#33021;&#23454;&#29616;&#24378;&#22823;&#30340;&#25915;&#20987;&#24615;&#33021;&#21644;&#32784;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a federated learning (FL) system, malicious participants can easily embed backdoors into the aggregated model while maintaining the model's performance on the main task. To this end, various defenses, including training stage aggregation-based defenses and post-training mitigation defenses, have been proposed recently. While these defenses obtain reasonable performance against existing backdoor attacks, which are mainly heuristics based, we show that they are insufficient in the face of more advanced attacks. In particular, we propose a general reinforcement learning-based backdoor attack framework where the attacker first trains a (non-myopic) attack policy using a simulator built upon its local data and common knowledge on the FL system, which is then applied during actual FL training. Our attack framework is both adaptive and flexible and achieves strong attack performance and durability even under state-of-the-art defenses.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#25910;&#38598;&#39640;&#19981;&#35268;&#21017;&#28508;&#33021;&#33021;&#37327;&#38754;&#19978;&#30340;&#29289;&#29702;&#26377;&#36259;&#30340;&#31283;&#24577;&#65292; &#24182;&#22312;Pd / Fe / Ir&#65288;111&#65289;&#31995;&#32479;&#20013;&#24212;&#29992;&#20110;&#35782;&#21035;&#33258;&#26059;&#32467;&#26500;&#65292;&#35266;&#23519;&#20102;&#20854;&#20013;&#19968;&#20123;&#32467;&#26500;&#30340;&#26377;&#38480;&#28201;&#24230;&#33258;&#26059;&#21160;&#21147;&#23398;&#29305;&#24615;&#21644;&#25299;&#25169;&#30005;&#33655;&#19982;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.02876</link><description>&lt;p&gt;
&#29992;&#20803;&#21551;&#21457;&#24335;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#25910;&#38598;&#22825;&#28982;&#25233;&#21046;&#24577;&#30340;&#22825;&#31354;rmion&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Metaheuristic conditional neural network for harvesting skyrmionic metastable states. (arXiv:2303.02876v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02876
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#25910;&#38598;&#39640;&#19981;&#35268;&#21017;&#28508;&#33021;&#33021;&#37327;&#38754;&#19978;&#30340;&#29289;&#29702;&#26377;&#36259;&#30340;&#31283;&#24577;&#65292; &#24182;&#22312;Pd / Fe / Ir&#65288;111&#65289;&#31995;&#32479;&#20013;&#24212;&#29992;&#20110;&#35782;&#21035;&#33258;&#26059;&#32467;&#26500;&#65292;&#35266;&#23519;&#20102;&#20854;&#20013;&#19968;&#20123;&#32467;&#26500;&#30340;&#26377;&#38480;&#28201;&#24230;&#33258;&#26059;&#21160;&#21147;&#23398;&#29305;&#24615;&#21644;&#25299;&#25169;&#30005;&#33655;&#19982;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35782;&#21035;&#39640;&#19981;&#35268;&#21017;&#28508;&#33021;&#33021;&#37327;&#38754;&#19978;&#30340;&#29289;&#29702;&#26377;&#36259;&#30340;&#31283;&#24577;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#35745;&#31639;&#30340;&#21442;&#25968;&#22522;&#30784;&#19978;&#24314;&#31435;&#30340;&#21476;&#20856;&#24494;&#35266;&#26059;&#36716;&#21704;&#23494;&#39039;&#37327;&#23545;Pd / Fe / Ir&#65288;111&#65289;&#31995;&#32479;&#20013;&#30340;&#25299;&#25169;&#30005;&#33655;$Q$&#20540;&#20174;1&#21040;$-13$&#30340;&#33258;&#26059;&#32467;&#26500;&#36827;&#34892;&#20102;&#20998;&#26512; &#12290;&#20026;&#20102;&#20419;&#36827;&#30456;&#20851;&#33258;&#26059;&#32467;&#26500;&#30340;&#25910;&#38598;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#26032;&#24320;&#21457;&#30340;&#8220;&#20219;&#24847;&#20998;&#27573;&#27169;&#22411;&#8221;&#65288;SAM&#65289;&#12290;&#24182;&#20351;&#29992;&#26377;&#38480;&#28201;&#24230;&#33258;&#26059;&#21160;&#21147;&#23398;&#27169;&#25311;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;Q&#20540;&#33539;&#22260;&#20174;$-3$&#21040;$-6$&#30340;&#33258;&#26059;&#32467;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#39640;&#36798;20K&#30340;&#28201;&#24230;&#65292;&#39044;&#27979;&#23551;&#21629;&#38271;&#36798;200ps&#20197;&#19978;&#65292;&#24403;&#36825;&#20123;&#32467;&#26500;&#34928;&#20943;&#26102;&#65292;&#26032;&#30340;&#25299;&#25169;&#26059;&#36716;&#32467;&#26500;&#23601;&#20250;&#24418;&#25104;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#33258;&#26059;&#32467;&#26500;&#30340;&#30456;&#23545;&#31283;&#23450;&#24615;&#19982;&#20854;&#25299;&#25169;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a metaheuristic conditional neural-network-based method aimed at identifying physically interesting metastable states in a potential energy surface of high rugosity. To demonstrate how this method works, we identify and analyze spin textures with topological charge $Q$ ranging from 1 to $-13$ (where antiskyrmions have $Q&lt;0$) in the Pd/Fe/Ir(111) system, which we model using a classical atomistic spin Hamiltonian based on parameters computed from density functional theory. To facilitate the harvest of relevant spin textures, we make use of the newly developed Segment Anything Model (SAM). Spin textures with $Q$ ranging from $-3$ to $-6$ are further analyzed using finite-temperature spin-dynamics simulations. We observe that for temperatures up to around 20\,K, lifetimes longer than 200\,ps are predicted, and that when these textures decay, new topological spin textures are formed. We also find that the relative stability of the spin textures depend linearly on the topological
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#65292;Soft Actor-Critic&#31639;&#27861;&#21644;Soft Q-learning&#31639;&#27861;&#22312;&#26368;&#22823;&#29109;&#26694;&#26550;&#19979;&#25910;&#25947;&#20110;&#21516;&#19968;&#35299;&#65292;&#36825;&#19968;&#32467;&#35770;&#23545;&#20248;&#21270;&#31639;&#27861;&#20855;&#26377;&#36739;&#22823;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.01240</link><description>&lt;p&gt;
Soft Actor-Critic&#31639;&#27861;&#30340;&#25910;&#25947;&#28857;
&lt;/p&gt;
&lt;p&gt;
The Point to Which Soft Actor-Critic Converges. (arXiv:2303.01240v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#65292;Soft Actor-Critic&#31639;&#27861;&#21644;Soft Q-learning&#31639;&#27861;&#22312;&#26368;&#22823;&#29109;&#26694;&#26550;&#19979;&#25910;&#25947;&#20110;&#21516;&#19968;&#35299;&#65292;&#36825;&#19968;&#32467;&#35770;&#23545;&#20248;&#21270;&#31639;&#27861;&#20855;&#26377;&#36739;&#22823;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Soft Actor-Critic&#26159;Soft Q-learning&#30340;&#25104;&#21151;&#21518;&#32487;&#32773;&#65292;&#23613;&#31649;&#23427;&#20204;&#37117;&#22788;&#20110;&#26368;&#22823;&#29109;&#26694;&#26550;&#19979;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#25910;&#25947;&#20110;&#30456;&#21516;&#30340;&#35299;&#65292;&#36825;&#19968;&#32467;&#26524;&#38750;&#24120;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#23558;&#20248;&#21270;&#20174;&#22256;&#38590;&#30340;&#26041;&#24335;&#36716;&#21270;&#20026;&#20102;&#31616;&#21333;&#30340;&#26041;&#24335;&#12290;&#21516;&#26679;&#30340;&#35777;&#26126;&#20063;&#36866;&#29992;&#20110;&#20854;&#20182;&#27491;&#21017;&#39033;&#65292;&#20363;&#22914;KL&#25955;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft actor-critic is a successful successor over soft Q-learning. While lived under maximum entropy framework, their relationship is still unclear. In this paper, we prove that in the limit they converge to the same solution. This is appealing since it translates the optimization from an arduous to an easier way. The same justification can also be applied to other regularizers such as KL divergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Inseq&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#25512;&#24191;&#21487;&#35299;&#37322;&#24615;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#23427;&#20026;&#24120;&#35265;&#30340;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#25552;&#20379;&#20102;&#25552;&#21462;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#30452;&#35266;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;GPT-2&#20013;&#23637;&#31034;&#20102;Inseq&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#20854;&#26377;&#21161;&#20110;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.13942</link><description>&lt;p&gt;
Inseq&#65306;&#19968;&#20010;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Inseq: An Interpretability Toolkit for Sequence Generation Models. (arXiv:2302.13942v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Inseq&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#25512;&#24191;&#21487;&#35299;&#37322;&#24615;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#23427;&#20026;&#24120;&#35265;&#30340;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#25552;&#20379;&#20102;&#25552;&#21462;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#30452;&#35266;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;GPT-2&#20013;&#23637;&#31034;&#20102;Inseq&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#20854;&#26377;&#21161;&#20110;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36807;&#21435;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27969;&#34892;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#32780;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#19987;&#38376;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Inseq&#65292;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#20351;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#26222;&#21450;&#21270;&#12290;Inseq&#33021;&#22815;&#30452;&#35266;&#19988;&#20248;&#21270;&#22320;&#25552;&#21462;&#27969;&#34892;&#30340;&#20165;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#30340;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#23427;&#26469;&#31361;&#20986;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24182;&#22312;GPT-2&#20013;&#23450;&#20301;&#20107;&#23454;&#30693;&#35782;&#12290;&#30001;&#20110;&#20854;&#25903;&#25345;&#23545;&#27604;&#29305;&#24449;&#24402;&#22240;&#31561;&#21069;&#27839;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#25509;&#21475;&#65292;&#22240;&#27492;Inseq&#21487;&#20197;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#65292;&#38598;&#20013;&#20248;&#33391;&#23454;&#36341;&#65292;&#24182;&#23454;&#29616;&#20844;&#27491;&#21644;&#21487;&#37325;&#22797;&#30340;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models' internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32858;&#31867;&#26041;&#27861;(SLAC-Time)&#65292;&#37319;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20316;&#20026;&#20195;&#29702;&#20219;&#21153;&#65292;&#19981;&#38656;&#35201;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#20855;&#26377;&#26356;&#20581;&#22766;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.13457</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#32858;&#31867;&#21253;&#21547;&#32570;&#22833;&#20540;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454; (SLAC-Time): &#24212;&#29992;&#20110;TBI&#34920;&#22411;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Self-Supervised Learning-based Approach to Clustering Multivariate Time-Series Data with Missing Values (SLAC-Time): An Application to TBI Phenotyping. (arXiv:2302.13457v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32858;&#31867;&#26041;&#27861;(SLAC-Time)&#65292;&#37319;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20316;&#20026;&#20195;&#29702;&#20219;&#21153;&#65292;&#19981;&#38656;&#35201;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#20855;&#26377;&#26356;&#20581;&#22766;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20026;&#32858;&#31867;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#24456;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#32570;&#22833;&#20540;&#65292;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#27714;&#22312;&#32858;&#31867;&#20043;&#21069;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#22823;&#37327;&#35745;&#31639;&#21644;&#22122;&#22768;&#65292;&#20174;&#32780;&#23548;&#33268;&#26080;&#25928;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#29992;&#20110;&#32858;&#31867;&#21253;&#21547;&#32570;&#22833;&#20540;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26041;&#27861;(SLAC-Time)&#12290;SLAC-Time&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20316;&#20026;&#20195;&#29702;&#20219;&#21153;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#23398;&#20064;&#26356;&#20581;&#22766;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21516;&#26102;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21644;&#25152;&#23398;&#34920;&#31034;&#30340;&#32858;&#31867;&#20998;&#37197;&#12290;&#23427;&#20351;&#29992;K-means&#26041;&#27861;&#36845;&#20195;&#22320;&#23545;&#23398;&#20064;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#28982;&#21518;&#21033;&#29992;&#38543;&#21518;&#30340;&#32858;&#31867;&#20998;&#37197;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning approaches provide a promising direction for clustering multivariate time-series data. However, real-world time-series data often include missing values, and the existing approaches require imputing missing values before clustering, which may cause extensive computations and noise and result in invalid interpretations. To address these challenges, we present a Self-supervised Learning-based Approach to Clustering multivariate Time-series data with missing values (SLAC-Time). SLAC-Time is a Transformer-based clustering method that uses time-series forecasting as a proxy task for leveraging unlabeled data and learning more robust time-series representations. This method jointly learns the neural network parameters and the cluster assignments of the learned representations. It iteratively clusters the learned representations with the K-means method and then utilizes the subsequent cluster assignments as pseudo-labels to update the model parameters. To evaluate our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;MDF-Net&#65292;&#23558;&#20020;&#24202;&#25968;&#25454;&#19982;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#30142;&#30149;&#23450;&#20301;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.13390</link><description>&lt;p&gt;
MDF-Net&#65306;&#32467;&#21512;X&#23556;&#32447;&#19982;&#20020;&#24202;&#25968;&#25454;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MDF-Net for Abnormality Detection by Fusing X-Rays with Clinical Data. (arXiv:2302.13390v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;MDF-Net&#65292;&#23558;&#20020;&#24202;&#25968;&#25454;&#19982;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#30142;&#30149;&#23450;&#20301;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#23558;&#24739;&#32773;&#30340;&#20020;&#24202;&#20449;&#24687;&#21152;&#20837;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#20197;&#25552;&#39640;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30142;&#30149;&#23450;&#20301;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#24403;&#21069;&#30340;&#20998;&#31867;&#22120;&#22312;&#20351;&#29992;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#26102;&#20855;&#26377;&#39640;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#23545;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#35775;&#35848;&#34920;&#26126;&#65292;&#20020;&#24202;&#25968;&#25454;&#23545;&#20110;&#35299;&#37322;&#22270;&#20687;&#21644;&#20570;&#20986;&#27491;&#30830;&#30340;&#35786;&#26029;&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#65292;&#24182;&#20855;&#26377;&#24456;&#39640;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#30001;&#20004;&#31181;&#34701;&#21512;&#26041;&#27861;&#32452;&#25104;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#24739;&#32773;&#30340;&#20020;&#24202;&#25968;&#25454;&#65288;&#32467;&#26500;&#21270;&#25968;&#25454;&#65289;&#21644;&#33016;&#37096;X&#23556;&#32447;&#65288;&#22270;&#20687;&#25968;&#25454;&#65289;&#12290;&#30001;&#20110;&#36825;&#20004;&#31181;&#25968;&#25454;&#27169;&#24577;&#22312;&#19981;&#21516;&#30340;&#32500;&#24230;&#31354;&#38388;&#20013;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;&#25490;&#21015;&#31574;&#30053;&#31354;&#38388;&#21270;&#65292;&#20197;&#20415;&#22312;Mask R-CNN&#27169;&#22411;&#20013;&#20419;&#36827;&#22810;&#27169;&#24577;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#22810;&#31181;&#25968;&#25454;&#27169;&#24577;&#30340;MIMIC-Eye&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65306;MIMIC-CXR&#65288;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#65289;&#12289;MIMIC IV-ED&#65288;&#24739;&#32773;&#30340;&#20020;&#24202;&#25968;&#25454;&#65289;&#21644;REFLACX&#65288;&#35780;&#20272;&#27880;&#37322;&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;MDF-Net&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the effects of including patients' clinical information on the performance of deep learning (DL) classifiers for disease location in chest X-ray images. Although current classifiers achieve high performance using chest X-ray images alone, our interviews with radiologists indicate that clinical data is highly informative and essential for interpreting images and making proper diagnoses.  In this work, we propose a novel architecture consisting of two fusion methods that enable the model to simultaneously process patients' clinical data (structured data) and chest X-rays (image data). Since these data modalities are in different dimensional spaces, we propose a spatial arrangement strategy, spatialization, to facilitate the multimodal learning process in a Mask R-CNN model. We performed an extensive experimental evaluation using MIMIC-Eye, a dataset comprising modalities: MIMIC-CXR (chest X-ray images), MIMIC IV-ED (patients' clinical data), and REFLACX (annotatio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;ProbConserv&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23432;&#24658;&#32422;&#26463;&#19982;&#36125;&#21494;&#26031;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#23558;&#23432;&#24658;&#32422;&#26463;&#32435;&#20837;&#36890;&#29992;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#20013;&#65292;&#20197;&#20415;&#22312;&#23398;&#20064;&#39640;&#38590;&#24230;&#30340;PDE&#36816;&#31639;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.11002</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#20197;&#36981;&#23432;&#23432;&#24658;&#23450;&#24459;&#30340;&#29289;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Physical Models that Can Respect Conservation Laws. (arXiv:2302.11002v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11002
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;ProbConserv&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23432;&#24658;&#32422;&#26463;&#19982;&#36125;&#21494;&#26031;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#23558;&#23432;&#24658;&#32422;&#26463;&#32435;&#20837;&#36890;&#29992;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#20013;&#65292;&#20197;&#20415;&#22312;&#23398;&#20064;&#39640;&#38590;&#24230;&#30340;PDE&#36816;&#31639;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#36817;&#19968;&#20123;&#24037;&#20316;&#38598;&#20013;&#22312;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#20449;&#24687;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#20854;&#20013;&#35768;&#22810;&#24037;&#20316;&#38598;&#20013;&#22312;&#30456;&#23545;&#8220;&#23481;&#26131;&#8221;&#30340;PDE&#31639;&#23376;&#65288;&#20363;&#22914;&#26925;&#22278;&#21644;&#25243;&#29289;&#32447;&#65289;&#19978;&#65292;&#32780;&#30456;&#23545;&#8220;&#22256;&#38590;&#8221;&#30340;PDE&#31639;&#23376;&#65288;&#20363;&#22914;&#21452;&#26354;&#32447;&#65289;&#21017;&#36739;&#23569;&#12290;&#22312;&#25968;&#20540;PDE&#26041;&#38754;&#65292;&#21518;&#19968;&#31181;&#38382;&#39064;&#31867;&#38656;&#35201;&#25511;&#21046;&#19968;&#31181;&#20307;&#31215;&#20803;&#32032;&#25110;&#23432;&#24658;&#32422;&#26463;&#31867;&#22411;&#65292;&#36825;&#34987;&#35270;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#25215;&#35834;&#65292;&#38656;&#35201;&#26080;&#32541;&#22320;&#23558;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in scientific machine learning (SciML) has focused on incorporating partial differential equation (PDE) information into the learning process. Much of this work has focused on relatively ``easy'' PDE operators (e.g., elliptic and parabolic), with less emphasis on relatively ``hard'' PDE operators (e.g., hyperbolic). Within numerical PDEs, the latter problem class requires control of a type of volume element or conservation constraint, which is known to be challenging. Delivering on the promise of SciML requires seamlessly incorporating both types of problems into the learning process. To address this issue, we propose ProbConserv, a framework for incorporating conservation constraints into a generic SciML architecture. To do so, ProbConserv combines the integral form of a conservation law with a Bayesian update. We provide a detailed analysis of ProbConserv on learning with the Generalized Porous Medium Equation (GPME), a widely-applicable parameterized family of PDEs that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#27880;&#20837;&#35757;&#32451;&#26041;&#26696;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#29305;&#27530;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#21644;&#31616;&#21270;&#20256;&#32479;&#22122;&#22768;&#27880;&#20837;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2302.10802</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#27880;&#20837;&#35757;&#32451;&#26041;&#26696;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#25913;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Noise Injection-based Training Scheme for Better Model Robustness. (arXiv:2302.10802v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#27880;&#20837;&#35757;&#32451;&#26041;&#26696;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#29305;&#27530;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#21644;&#31616;&#21270;&#20256;&#32479;&#22122;&#22768;&#27880;&#20837;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22122;&#38899;&#27880;&#20837;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#27880;&#20837;&#35757;&#32451;&#26041;&#26696;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#25913;&#36827;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#20284;&#28982;&#27604;&#26041;&#27861;&#26469;&#20272;&#35745;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#19979;&#30340;&#31361;&#35302;&#26435;&#37325;&#21644;&#22122;&#22768;&#27700;&#24179;&#30340;&#26799;&#24230;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36924;&#36817;&#30340;&#20256;&#32479;&#22122;&#22768;&#27880;&#20837;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#20869;&#23384;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#26696;&#24212;&#29992;&#20110;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#22312;&#21407;&#22987;&#20934;&#30830;&#24615;&#19978;&#30053;&#24494;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noise injection-based method has been shown to be able to improve the robustness of artificial neural networks in previous work. In this work, we propose a novel noise injection-based training scheme for better model robustness. Specifically, we first develop a likelihood ratio method to estimate the gradient with respect to both synaptic weights and noise levels for stochastic gradient descent training. Then, we design an approximation for the vanilla noise injection-based training method to reduce memory and improve computational efficiency. Next, we apply our proposed scheme to spiking neural networks and evaluate the performance of classification accuracy and robustness on MNIST and Fashion-MNIST datasets. Experiment results show that our proposed method achieves a much better performance on adversarial robustness and slightly better performance on original accuracy, compared with the conventional gradient-based training method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21517;&#20026;NeuralStagger&#30340;&#36890;&#29992;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#32422;&#26463;&#19979;&#30340;&#31070;&#32463;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#27861;&#22120;&#36827;&#34892;&#26102;&#31354;&#20998;&#35299;&#24471;&#21040;&#22810;&#20010;&#31895;&#20998;&#36776;&#29575;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#29289;&#29702;&#32422;&#26463;&#30340;&#26631;&#20934;&#25439;&#22833;&#32852;&#21512;&#35757;&#32451;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.10255</link><description>&lt;p&gt;
NeuralStagger: &#21033;&#29992;&#26102;&#31354;&#20998;&#35299;&#21152;&#36895;&#29289;&#29702;&#32422;&#26463;&#19979;&#30340;&#31070;&#32463;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#27861;&#22120;
&lt;/p&gt;
&lt;p&gt;
NeuralStagger: Accelerating Physics-constrained Neural PDE Solver with Spatial-temporal Decomposition. (arXiv:2302.10255v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21517;&#20026;NeuralStagger&#30340;&#36890;&#29992;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#32422;&#26463;&#19979;&#30340;&#31070;&#32463;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#27861;&#22120;&#36827;&#34892;&#26102;&#31354;&#20998;&#35299;&#24471;&#21040;&#22810;&#20010;&#31895;&#20998;&#36776;&#29575;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#29289;&#29702;&#32422;&#26463;&#30340;&#26631;&#20934;&#25439;&#22833;&#32852;&#21512;&#35757;&#32451;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#21152;&#36895;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#27714;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#22312;&#35757;&#32451;&#31070;&#32463;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#27861;&#22120;&#20013;&#24341;&#20837;&#29289;&#29702;&#32422;&#26463;&#26469;&#20943;&#23569;&#25968;&#25454;&#20351;&#29992;&#21644;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#26377;&#38480;&#32500;&#24230;&#36817;&#20284;&#30340;&#29289;&#29702;&#32422;&#26463;&#20026;&#20102;&#20445;&#35777;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#24517;&#39035;&#35299;&#20915;&#26368;&#23567;&#23610;&#24230;&#30340;&#29289;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#23548;&#33268;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#21253;&#25324;&#36755;&#20837;&#12289;&#36755;&#20986;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuralStagger&#30340;&#36890;&#29992;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#30340;&#23398;&#20064;&#20219;&#21153;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#36827;&#34892;&#20998;&#35299;&#65292;&#24418;&#25104;&#20960;&#20010;&#36739;&#31895;&#20998;&#36776;&#29575;&#30340;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#23376;&#20219;&#21153;&#23450;&#20041;&#20102;&#19968;&#20010;&#31895;&#20998;&#36776;&#29575;&#30340;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#20854;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#26356;&#23569;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;&#29289;&#29702;&#32422;&#26463;&#30340;&#26631;&#20934;&#25439;&#22833;&#32852;&#21512;&#35757;&#32451;&#23427;&#20204;&#65292;&#21482;&#38656;&#31616;&#21333;&#22320;&#25490;&#21015;&#20854;&#36755;&#20986;&#20197;&#37325;&#26500;&#23436;&#25972;&#30340;&#27169;&#25311;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have shown great potential in accelerating the solution of partial differential equations (PDEs). Recently, there has been a growing interest in introducing physics constraints into training neural PDE solvers to reduce the use of costly data and improve the generalization ability. However, these physics constraints, based on certain finite dimensional approximations over the function space, must resolve the smallest scaled physics to ensure the accuracy and stability of the simulation, resulting in high computational costs from large input, output, and neural networks. This paper proposes a general acceleration methodology called NeuralStagger by spatially and temporally decomposing the original learning tasks into several coarser-resolution subtasks. We define a coarse-resolution neural solver for each subtask, which requires fewer computational resources, and jointly train them with the vanilla physics-constrained loss by simply arranging their outputs to reconstruct
&lt;/p&gt;</description></item><item><title>HyFL&#26159;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#25216;&#26415;&#21644;&#20998;&#23618;&#32852;&#21512;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#20445;&#35777;&#25968;&#25454;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#38544;&#31169;&#23433;&#20840;&#65292;&#26377;&#21161;&#20110;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2302.09904</link><description>&lt;p&gt;
HyFL:&#19968;&#31181;&#29992;&#20110;&#31169;&#26377;&#32852;&#21512;&#23398;&#20064;&#30340;&#28151;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyFL: A Hybrid Framework For Private Federated Learning. (arXiv:2302.09904v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09904
&lt;/p&gt;
&lt;p&gt;
HyFL&#26159;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#25216;&#26415;&#21644;&#20998;&#23618;&#32852;&#21512;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#20445;&#35777;&#25968;&#25454;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#38544;&#31169;&#23433;&#20840;&#65292;&#26377;&#21161;&#20110;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#35774;&#22791;&#19978;&#20445;&#30041;&#35757;&#32451;&#25968;&#25454;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;FL&#20013;&#30340;&#28431;&#27934;&#65292;&#21253;&#25324;&#36890;&#36807;&#21333;&#20010;&#27169;&#22411;&#26356;&#26032;&#29978;&#33267;&#25972;&#20010;&#20840;&#23616;&#27169;&#22411;&#27844;&#28431;&#25935;&#24863;&#20449;&#24687;&#12290;&#34429;&#28982;&#20851;&#27880;&#28857;&#24050;&#25918;&#22312;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#19978;&#65292;&#20294;&#26377;&#38480;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;&#20840;&#23616;&#27169;&#22411;&#38544;&#31169;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23458;&#25143;&#31471;&#26412;&#22320;&#35757;&#32451;&#20026;&#24694;&#24847;&#23458;&#25143;&#31471;&#21551;&#21160;&#24378;&#22823;&#30340;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#24320;&#36767;&#20102;&#36884;&#24452;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#24037;&#20316;&#25552;&#20379;&#20840;&#38754;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HyFL&#65292;&#36825;&#26159;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#21487;&#23454;&#29616;&#25968;&#25454;&#21644;&#20840;&#23616;&#27169;&#22411;&#38544;&#31169;&#65292;&#24182;&#20419;&#36827;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;HyFL&#30340;&#22522;&#30784;&#26159;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#25216;&#26415;&#21644;&#20998;&#23618;&#32852;&#21512;&#23398;&#20064;&#30340;&#29420;&#29305;&#32452;&#21512;&#12290;&#22312;HyFL&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#22312;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#19978;&#36827;&#34892;&#23433;&#20840;&#32858;&#21512;&#65292;&#20197;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HyFL&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#65292;&#21516;&#26102;&#30830;&#20445;&#23458;&#25143;&#31471;&#25968;&#25454;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#24378;&#22823;&#38544;&#31169;&#21644;&#23433;&#20840;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as an efficient approach for large-scale distributed machine learning, ensuring data privacy by keeping training data on client devices. However, recent research has highlighted vulnerabilities in FL, including the potential disclosure of sensitive information through individual model updates and even the aggregated global model. While much attention has been given to clients' data privacy, limited research has addressed the issue of global model privacy. Furthermore, local training at the client's side has opened avenues for malicious clients to launch powerful model poisoning attacks. Unfortunately, no existing work has provided a comprehensive solution that tackles all these issues. Therefore, we introduce HyFL, a hybrid framework that enables data and global model privacy while facilitating large-scale deployments. The foundation of HyFL is a unique combination of secure multi-party computation (MPC) techniques with hierarchical federated learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#36864;&#21270;&#29616;&#35937;&#65292;&#22312;&#20840;&#36830;&#25509;ReLU&#32593;&#32476;&#21021;&#22987;&#21270;&#26102;&#65292;&#20004;&#20010;&#36755;&#20837;&#20043;&#38388;&#30340;&#35282;&#24230;&#20250;&#36235;&#36817;&#20110;0&#12290;&#36890;&#36807;&#20351;&#29992;&#32452;&#21512;&#23637;&#24320;&#65292;&#24471;&#21040;&#20102;&#20854;&#36235;&#21521;&#20110;0&#30340;&#36895;&#24230;&#30340;&#31934;&#30830;&#20844;&#24335;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.09712</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#36864;&#21270;&#65306;&#20840;&#36830;&#25509;ReLU&#32593;&#32476;&#21021;&#22987;&#21270;&#26102;&#65292;&#28040;&#22833;&#35282;&#24230;&#30340;&#29616;&#35937; (arXiv:2302.09712v2 [stat.ML] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Depth Degeneracy in Neural Networks: Vanishing Angles in Fully Connected ReLU Networks on Initialization. (arXiv:2302.09712v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#36864;&#21270;&#29616;&#35937;&#65292;&#22312;&#20840;&#36830;&#25509;ReLU&#32593;&#32476;&#21021;&#22987;&#21270;&#26102;&#65292;&#20004;&#20010;&#36755;&#20837;&#20043;&#38388;&#30340;&#35282;&#24230;&#20250;&#36235;&#36817;&#20110;0&#12290;&#36890;&#36807;&#20351;&#29992;&#32452;&#21512;&#23637;&#24320;&#65292;&#24471;&#21040;&#20102;&#20854;&#36235;&#21521;&#20110;0&#30340;&#36895;&#24230;&#30340;&#31934;&#30830;&#20844;&#24335;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#35768;&#22810;&#20854;&#24615;&#36136;&#20173;&#26410;&#34987;&#29702;&#35770;&#19978;&#29702;&#35299;&#65292;&#20854;&#20013;&#19968;&#20010;&#35868;&#22242;&#26159;&#28145;&#24230;&#36864;&#21270;&#29616;&#35937;&#65306;&#32593;&#32476;&#23618;&#25968;&#36234;&#28145;&#65292;&#21021;&#22987;&#21270;&#26102;&#32593;&#32476;&#36234;&#25509;&#36817;&#20110;&#24120;&#25968;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#20004;&#20010;&#36755;&#20837;&#20043;&#38388;&#38543;&#30528;&#23618;&#25968;&#21464;&#21270;&#30340;&#35282;&#24230;&#28436;&#21464;&#24773;&#20917;&#12290;&#36890;&#36807;&#20351;&#29992;&#32452;&#21512;&#23637;&#24320;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#23427;&#38543;&#28145;&#24230;&#22686;&#21152;&#36235;&#21521;&#20110;0&#30340;&#36895;&#24230;&#30340;&#31934;&#30830;&#20844;&#24335;&#65292;&#36825;&#20123;&#20844;&#24335;&#25429;&#25417;&#20102;&#24494;&#35266;&#27874;&#21160;&#12290;&#25105;&#20204;&#29992;Monte Carlo&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#32467;&#26524;&#20934;&#30830;&#22320;&#36817;&#20284;&#20102;&#26377;&#38480;&#32593;&#32476;&#30340;&#34892;&#20026;&#12290;&#36825;&#20123;&#20844;&#24335;&#20197;&#36890;&#36807;ReLU&#20989;&#25968;&#30340;&#30456;&#20851;&#39640;&#26031;&#21464;&#37327;&#30340;&#28151;&#21512;&#30697;&#24418;&#24335;&#32473;&#20986;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32452;&#21512;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable performance on a variety of tasks, many properties of deep neural networks are not yet theoretically understood. One such mystery is the depth degeneracy phenomenon: the deeper you make your network, the closer your network is to a constant function on initialization. In this paper, we examine the evolution of the angle between two inputs to a ReLU neural network as a function of the number of layers. By using combinatorial expansions, we find precise formulas for how fast this angle goes to zero as depth increases. These formulas capture microscopic fluctuations that are not visible in the popular framework of infinite width limits, and leads to qualitatively different predictions. We validate our theoretical results with Monte Carlo experiments and show that our results accurately approximate finite network behaviour. The formulas are given in terms of the mixed moments of correlated Gaussians passed through the ReLU function. We also find a surprising combinatoria
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#21452;&#23618;&#20248;&#21270;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#22823;&#20284;&#28982;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#26469;&#20272;&#35745;&#19987;&#23478;&#30340;&#20445;&#23432;&#27169;&#22411;&#20197;&#21450;&#19987;&#23478;&#30340;&#29615;&#22659;&#21160;&#24577;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25512;&#26029;&#19987;&#19994;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07457</link><description>&lt;p&gt;
&#36890;&#36807;&#28436;&#31034;&#26469;&#29702;&#35299;&#19987;&#19994;&#25216;&#33021;&#65306;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#22823;&#20284;&#28982;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning. (arXiv:2302.07457v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07457
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#21452;&#23618;&#20248;&#21270;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#22823;&#20284;&#28982;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#26469;&#20272;&#35745;&#19987;&#23478;&#30340;&#20445;&#23432;&#27169;&#22411;&#20197;&#21450;&#19987;&#23478;&#30340;&#29615;&#22659;&#21160;&#24577;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25512;&#26029;&#19987;&#19994;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65288;Offline IRL&#65289;&#26088;&#22312;&#20174;&#19987;&#23478;&#20195;&#29702;&#30340;&#22266;&#23450;&#26377;&#38480;&#28436;&#31034;&#20013;&#24674;&#22797;&#25903;&#25745;&#35266;&#23519;&#21040;&#30340;&#25805;&#20316;&#30340;&#22870;&#21169;&#21644;&#29615;&#22659;&#21160;&#24577;&#30340;&#32467;&#26500;&#12290;&#20934;&#30830;&#30340;&#19987;&#19994;&#25191;&#34892;&#20219;&#21153;&#30340;&#27169;&#22411;&#22312;&#23433;&#20840;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#65292;&#20363;&#22914;&#20020;&#24202;&#20915;&#31574;&#21644;&#33258;&#21160;&#39550;&#39542;&#12290;&#28982;&#32780;&#65292;&#19987;&#23478;&#21916;&#22909;&#38544;&#21547;&#22312;&#35266;&#23519;&#21040;&#30340;&#25805;&#20316;&#20013;&#30340;&#32467;&#26500;&#19982;&#19987;&#23478;&#23545;&#29615;&#22659;&#21160;&#24577;&#30340;&#27169;&#22411;&#65288;&#21363;&#8220;&#19990;&#30028;&#8221;&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#20174;&#20855;&#26377;&#26377;&#38480;&#35206;&#30422;&#33539;&#22260;&#30340;&#26377;&#38480;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#19981;&#20934;&#30830;&#19990;&#30028;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#20272;&#35745;&#30340;&#22870;&#21169;&#30340;&#19981;&#20934;&#30830;&#24615;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#20844;&#24335;&#30340;&#20272;&#35745;&#20219;&#21153;&#65292;&#20854;&#20013;&#19978;&#23618;&#26159;&#22522;&#20110;&#19987;&#23478;&#31574;&#30053;&#30340;&#20445;&#23432;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;&#19979;&#23618;&#65289;&#12290;&#31574;&#30053;&#27169;&#22411;&#26159;&#20445;&#23432;&#30340;&#65292;&#22240;&#20026;&#23427;&#22312;&#24809;&#32602;&#65288;&#24809;&#32602;&#20250;&#38543;&#30528;&#19987;&#23478;&#23545;&#19990;&#30028;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#22686;&#21152;&#65289;&#19979;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#31163;&#32447;IRL&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline inverse reinforcement learning (Offline IRL) aims to recover the structure of rewards and environment dynamics that underlie observed actions in a fixed, finite set of demonstrations from an expert agent. Accurate models of expertise in executing a task has applications in safety-sensitive applications such as clinical decision making and autonomous driving. However, the structure of an expert's preferences implicit in observed actions is closely linked to the expert's model of the environment dynamics (i.e. the ``world''). Thus, inaccurate models of the world obtained from finite data with limited coverage could compound inaccuracy in estimated rewards. To address this issue, we propose a bi-level optimization formulation of the estimation task wherein the upper level is likelihood maximization based upon a conservative model of the expert's policy (lower level). The policy model is conservative in that it maximizes reward subject to a penalty that is increasing in the uncerta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20960;&#20309;&#20195;&#25968;&#30340;&#20960;&#20309; Clifford &#20195;&#25968;&#32593;&#32476;&#65288;GCANs&#65289;&#65292;&#37319;&#29992;&#23545;&#31216;&#32676;&#21464;&#25442;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#65292;&#36890;&#36807;&#32676;&#20316;&#29992;&#23618;&#12289;&#28608;&#27963;&#21644;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#21487;&#20197;&#20248;&#21270;&#20960;&#20309;&#27169;&#26495;&#65292;&#25552;&#39640;&#19977;&#32500;&#21018;&#20307;&#21464;&#25442;&#21644;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06594</link><description>&lt;p&gt;
&#20960;&#20309; Clifford &#20195;&#25968;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Geometric Clifford Algebra Networks. (arXiv:2302.06594v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20960;&#20309;&#20195;&#25968;&#30340;&#20960;&#20309; Clifford &#20195;&#25968;&#32593;&#32476;&#65288;GCANs&#65289;&#65292;&#37319;&#29992;&#23545;&#31216;&#32676;&#21464;&#25442;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#65292;&#36890;&#36807;&#32676;&#20316;&#29992;&#23618;&#12289;&#28608;&#27963;&#21644;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#21487;&#20197;&#20248;&#21270;&#20960;&#20309;&#27169;&#26495;&#65292;&#25552;&#39640;&#19977;&#32500;&#21018;&#20307;&#21464;&#25442;&#21644;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20960;&#20309;&#65288;Clifford&#65289;&#20195;&#25968;&#30340;&#23545;&#31216;&#32676;&#21464;&#25442;&#30340;&#20960;&#20309; Clifford &#20195;&#25968;&#32593;&#32476;&#65288;GCANs&#65289;&#29992;&#20110;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#29616;&#20195;&#65288;&#22522;&#20110;&#24179;&#38754;&#30340;&#65289;&#20960;&#20309;&#20195;&#25968;&#30340;&#31934;&#39635;&#65292;&#35813;&#20195;&#25968;&#24314;&#31435;&#22312;&#20316;&#20026; $\mathrm{Pin}(p,q,r)$ &#32676;&#20803;&#32032;&#30340;&#31561;&#36317;&#26144;&#23556;&#20043;&#19978;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#32676;&#20316;&#29992;&#23618;&#30340;&#27010;&#24565;&#65292;&#23427;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#32676;&#20316;&#29992;&#32447;&#24615;&#32452;&#21512;&#29289;&#20307;&#21464;&#25442;&#65292;&#37197;&#21512;&#26032;&#30340;&#28608;&#27963;&#21644;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#36825;&#20123;&#23618;&#20316;&#20026;&#21487;&#35843;&#25972;&#30340;&#8220;&#20960;&#20309;&#27169;&#26495;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26469;&#20248;&#21270;&#12290;&#29702;&#35770;&#19978;&#30340;&#20248;&#21183;&#22312;&#19977;&#32500;&#21018;&#20307;&#21464;&#25442;&#21644;&#22823;&#35268;&#27169;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#24314;&#27169;&#20013;&#24471;&#21040;&#20102;&#24378;&#28872;&#20307;&#29616;&#65292;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26041;&#27861;&#26174;&#30528;&#25552;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Geometric Clifford Algebra Networks (GCANs) for modeling dynamical systems. GCANs are based on symmetry group transformations using geometric (Clifford) algebras. We first review the quintessence of modern (plane-based) geometric algebra, which builds on isometries encoded as elements of the $\mathrm{Pin}(p,q,r)$ group. We then propose the concept of group action layers, which linearly combine object transformations using pre-specified group actions. Together with a new activation and normalization scheme, these layers serve as adjustable $\textit{geometric templates}$ that can be refined via gradient descent. Theoretical advantages are strongly reflected in the modeling of three-dimensional rigid body transformations as well as large-scale fluid dynamics simulations, showing significantly improved performance over traditional methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#38544;&#31169;&#12289;&#40065;&#26834;&#24615;&#21644;&#23454;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#26681;&#26412;&#30340;&#24179;&#34913;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#32500;&#24230;&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.04787</link><description>&lt;p&gt;
&#20851;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#38544;&#31169;&#12289;&#40065;&#26834;&#24615;&#21644;&#23454;&#29992;&#24615;&#19977;&#38590;&#39064;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the Privacy-Robustness-Utility Trilemma in Distributed Learning. (arXiv:2302.04787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#38544;&#31169;&#12289;&#40065;&#26834;&#24615;&#21644;&#23454;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#26681;&#26412;&#30340;&#24179;&#34913;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#32500;&#24230;&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#22312;&#25935;&#24863;&#20844;&#20849;&#39046;&#22495;&#24212;&#29992;&#20013;&#30340;&#26222;&#21450;&#38656;&#35201;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#23545;&#25925;&#38556;&#21644;&#25932;&#23545;&#34892;&#20026;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#34429;&#28982;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#37117;&#26377;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#30340;&#32508;&#21512;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32039;&#23494;&#20998;&#26512;&#20102;&#20219;&#20309;&#31639;&#27861;&#22312;&#30830;&#20445;&#23545;&#25239;&#24615;&#26426;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#35802;&#23454;&#26426;&#22120;&#30340;&#24046;&#20998;&#38544;&#31169;&#26041;&#38754;&#20250;&#20135;&#29983;&#35823;&#24046;&#30340;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#38544;&#31169;&#12289;&#40065;&#26834;&#24615;&#21644;&#23454;&#29992;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#25105;&#20204;&#36890;&#36807;&#24179;&#22343;&#20272;&#35745;&#30340;&#26696;&#20363;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#19979;&#38480;&#65292;&#24182;&#23558;&#20854;&#21463;&#21040;&#20998;&#24067;&#24335;&#24046;&#20998;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#32422;&#26463;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20102;&#19968;&#20123;&#20943;&#23569;&#21333;&#21521;&#36793;&#38469;&#20013;&#24515;&#21270;&#20272;&#35745;&#30340;&#38477;&#20302;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#39640;&#32500;&#24230;&#30340;&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#19978;&#38480;&#21305;&#37197;&#12290;&#21518;&#38754;&#25152;&#36848;&#30340;&#26041;&#27861;&#23558;&#35745;&#31639;&#25104;&#26412;&#20998;&#25674;&#21270;&#65292;&#24182;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#38416;&#26126;&#20102;&#22312;&#38544;&#31169;-&#40065;&#26834;&#24615;-&#25928;&#29992;&#19977;&#38590;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#20986;&#29616;&#30340;&#20132;&#21449;&#23398;&#31185;&#25361;&#25112;&#65292;&#24182;&#21487;&#20197;&#25351;&#23548;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#23454;&#38469;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ubiquity of distributed machine learning (ML) in sensitive public domain applications calls for algorithms that protect data privacy, while being robust to faults and adversarial behaviors. Although privacy and robustness have been extensively studied independently in distributed ML, their synthesis remains poorly understood. We present the first tight analysis of the error incurred by any algorithm ensuring robustness against a fraction of adversarial machines, as well as differential privacy (DP) for honest machines' data against any other curious entity. Our analysis exhibits a fundamental trade-off between privacy, robustness, and utility. To prove our lower bound, we consider the case of mean estimation, subject to distributed DP and robustness constraints, and devise reductions to centralized estimation of one-way marginals. We prove our matching upper bound by presenting a new distributed ML algorithm using a high-dimensional robust aggregation rule. The latter amortizes the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40657;&#30418;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#20197;&#22312;&#22522;&#30784;&#22270;&#20687;&#21644;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20013;&#24341;&#20837;&#29305;&#23450;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2302.04237</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#40657;&#30418;&#23545;&#25239;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Black Box Adversarial Prompting for Foundation Models. (arXiv:2302.04237v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40657;&#30418;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#20197;&#22312;&#22522;&#30784;&#22270;&#20687;&#21644;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20013;&#24341;&#20837;&#29305;&#23450;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#30028;&#38754;&#20801;&#35768;&#29992;&#25143;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#26041;&#38754;&#24555;&#36895;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290; &#28982;&#32780;&#65292;&#25552;&#31034;&#20013;&#30340;&#23567;&#25913;&#21464;&#21644;&#35774;&#35745;&#36873;&#25321;&#21487;&#33021;&#23548;&#33268;&#36755;&#20986;&#20013;&#30340;&#26174;&#30528;&#24046;&#24322;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38024;&#23545;&#38750;&#32467;&#26500;&#21270;&#22270;&#20687;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#40657;&#30418;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290; &#36825;&#20123;&#25552;&#31034;&#21487;&#20197;&#29420;&#31435;&#23384;&#22312;&#65292;&#20063;&#21487;&#20197;&#38468;&#21152;&#21040;&#33391;&#24615;&#25552;&#31034;&#20043;&#21069;&#65292;&#20174;&#32780;&#23548;&#33268;&#29305;&#23450;&#30340;&#29983;&#25104;&#36807;&#31243;&#34892;&#20026;&#65292;&#20363;&#22914;&#29983;&#25104;&#29305;&#23450;&#23545;&#35937;&#30340;&#22270;&#20687;&#25110;&#29983;&#25104;&#39640;&#22256;&#24785;&#24230;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. However, small changes and design choices in the prompt can lead to significant differences in the output. In this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. These prompts, which can be standalone or prepended to benign prompts, induce specific behaviors into the generative process, such as generating images of a particular object or generating high perplexity text.
&lt;/p&gt;</description></item><item><title>Rover &#26159;&#19968;&#31181;&#22312;&#32447; Spark SQL &#35843;&#20248;&#26381;&#21153;&#65292;&#36890;&#36807;&#24212;&#29992;&#24191;&#20041;&#36716;&#31227;&#23398;&#20064;&#65292;&#32467;&#21512;&#19987;&#23478;&#30693;&#35782;&#21644;&#21382;&#21490;&#20219;&#21153;&#65292;&#26469;&#21152;&#36895;&#35843;&#20248;&#36807;&#31243;&#65292;&#25552;&#39640;&#24615;&#33021;&#24182;&#36991;&#20813;&#19981;&#33391;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.04046</link><description>&lt;p&gt;
Rover&#65306;&#19968;&#31181;&#36890;&#36807;&#24191;&#20041;&#36716;&#31227;&#23398;&#20064;&#25552;&#20379;&#22312;&#32447; Spark SQL &#35843;&#20248;&#26381;&#21153;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rover: An online Spark SQL tuning service via generalized transfer learning. (arXiv:2302.04046v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04046
&lt;/p&gt;
&lt;p&gt;
Rover &#26159;&#19968;&#31181;&#22312;&#32447; Spark SQL &#35843;&#20248;&#26381;&#21153;&#65292;&#36890;&#36807;&#24212;&#29992;&#24191;&#20041;&#36716;&#31227;&#23398;&#20064;&#65292;&#32467;&#21512;&#19987;&#23478;&#30693;&#35782;&#21644;&#21382;&#21490;&#20219;&#21153;&#65292;&#26469;&#21152;&#36895;&#35843;&#20248;&#36807;&#31243;&#65292;&#25552;&#39640;&#24615;&#33021;&#24182;&#36991;&#20813;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#25968;&#25454;&#20998;&#26512;&#24341;&#25806;&#65288;&#22914;Spark&#65289;&#26159;&#24037;&#19994;&#30028;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#30340;&#24120;&#29992;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;Spark SQL &#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#37197;&#32622;&#30340;&#36873;&#25321;&#65292;&#20854;&#20013;&#26368;&#20248;&#37197;&#32622;&#38543;&#25191;&#34892;&#30340;&#24037;&#20316;&#36127;&#36733;&#32780;&#21464;&#21270;&#12290;&#22312;&#20247;&#22810;&#30340; Spark SQL &#35843;&#20248;&#26041;&#26696;&#20013;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#36275;&#22815;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#37197;&#32622;&#65292;&#20294;&#23427;&#21463;&#21040;&#37325;&#26032;&#20248;&#21270;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#22312;&#23454;&#38469;&#29983;&#20135;&#20013;&#19981;&#23454;&#29992;&#12290;&#24403;&#24212;&#29992;&#36716;&#31227;&#23398;&#20064;&#26469;&#21152;&#36895;&#35843;&#20248;&#36807;&#31243;&#26102;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#20004;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#25361;&#25112;&#65306;1&#65289;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#20256;&#36882;&#35843;&#20248;&#21382;&#21490;&#35760;&#24405;&#26041;&#38754;&#65292;&#32780;&#26469;&#33258; Spark &#24037;&#31243;&#24072;&#30340;&#19987;&#19994;&#30693;&#35782;&#20855;&#26377;&#26497;&#22823;&#30340;&#28508;&#21147;&#26469;&#25913;&#21892;&#35843;&#20248;&#24615;&#33021;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65307;2&#65289;&#24212;&#35813;&#20180;&#32454;&#21033;&#29992;&#21382;&#21490;&#20219;&#21153;&#65292;&#20854;&#20013;&#20351;&#29992;&#19981;&#30456;&#20284;&#30340;&#20219;&#21153;&#20250;&#23548;&#33268;&#29983;&#20135;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; Rover&#65292;&#19968;&#31181;&#24050;&#37096;&#32626;&#30340;&#22312;&#32447; Spark SQL &#35843;&#20248;&#26381;&#21153;&#65292;
&lt;/p&gt;
&lt;p&gt;
Distributed data analytic engines like Spark are common choices to process massive data in industry. However, the performance of Spark SQL highly depends on the choice of configurations, where the optimal ones vary with the executed workloads. Among various alternatives for Spark SQL tuning, Bayesian optimization (BO) is a popular framework that finds near-optimal configurations given sufficient budget, but it suffers from the re-optimization issue and is not practical in real production. When applying transfer learning to accelerate the tuning process, we notice two domain-specific challenges: 1) most previous work focus on transferring tuning history, while expert knowledge from Spark engineers is of great potential to improve the tuning performance but is not well studied so far; 2) history tasks should be carefully utilized, where using dissimilar ones lead to a deteriorated performance in production. In this paper, we present Rover, a deployed online Spark SQL tuning service for e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#27969;&#37327;&#20998;&#21106;&#20998;&#24067;&#65292;&#25317;&#22622;&#25511;&#21046;&#21644;&#35843;&#24230;&#20248;&#21270;O-RAN&#24212;&#29992;&#20013;&#30340;&#26234;&#33021;&#27969;&#37327;&#36716;&#21457;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#24310;&#36831;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20840;&#38754;&#30340;&#24615;&#33021;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2302.02711</link><description>&lt;p&gt;
6G O-RAN&#20013;&#30340;&#32593;&#32476;&#36741;&#21161;&#26234;&#33021;&#27969;&#37327;&#36716;&#21457;: &#19968;&#20010;&#22810;&#23618;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Network-Aided Intelligent Traffic Steering in 6G O-RAN: A Multi-Layer Optimization Framework. (arXiv:2302.02711v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#27969;&#37327;&#20998;&#21106;&#20998;&#24067;&#65292;&#25317;&#22622;&#25511;&#21046;&#21644;&#35843;&#24230;&#20248;&#21270;O-RAN&#24212;&#29992;&#20013;&#30340;&#26234;&#33021;&#27969;&#37327;&#36716;&#21457;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#24310;&#36831;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20840;&#38754;&#30340;&#24615;&#33021;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20026;6G&#32593;&#32476;&#23454;&#29616;&#26234;&#33021;&#12289;&#21487;&#32534;&#31243;&#21644;&#22810;&#21378;&#21830;&#30340;&#26080;&#32447;&#30005;&#25509;&#20837;&#32593;&#32476;&#65288;RAN&#65289;&#65292;&#22312;&#26631;&#20934;&#21270;&#21644;&#24320;&#25918;&#24335;RAN&#65288;O-RAN&#65289;&#30340;&#21457;&#23637;&#20013;&#20570;&#20986;&#20102;&#30456;&#24403;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;O-RAN&#22312;&#25511;&#21046;&#21644;&#20248;&#21270;RAN&#21151;&#33021;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#21516;&#26102;&#20248;&#21270;&#27969;&#37327;&#20998;&#21106;&#20998;&#24067;&#12289;&#25317;&#22622;&#25511;&#21046;&#21644;&#35843;&#24230;&#65288;JFCS&#65289;&#65292;&#20197;&#23454;&#29616;O-RAN&#20013;&#30340;&#26234;&#33021;&#27969;&#37327;&#36716;&#21457;&#24212;&#29992;&#12290;&#32467;&#21512;&#32593;&#32476;&#25928;&#29992;&#26368;&#22823;&#21270;&#21644;&#38543;&#26426;&#20248;&#21270;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#23618;&#20248;&#21270;&#26694;&#26550;&#65292;&#25552;&#20379;&#24555;&#36895;&#25910;&#25947;&#12289;&#38271;&#26399;&#25928;&#29992;&#26368;&#20248;&#21270;&#21644;&#26174;&#33879;&#30340;&#24310;&#36831;&#38477;&#20302;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#21644;&#22522;&#20934;RAN&#26041;&#27861;&#30456;&#27604;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#19977;&#20010;: i) &#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;JFCS&#26694;&#26550;&#65292;&#21487;&#39640;&#25928;&#19988;&#33258;&#36866;&#24212;&#22320;&#23558;&#27969;&#37327;&#24341;&#23548;&#21040;&#36866;&#24403;&#30340;&#26080;&#32447;&#30005;&#21333;&#20803;; ii) &#25105;&#20204;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#20302;&#22797;&#26434;&#24230;&#31639;&#27861;&#65292;inne
&lt;/p&gt;
&lt;p&gt;
To enable an intelligent, programmable and multi-vendor radio access network (RAN) for 6G networks, considerable efforts have been made in standardization and development of open RAN (O-RAN). So far, however, the applicability of O-RAN in controlling and optimizing RAN functions has not been widely investigated. In this paper, we jointly optimize the flow-split distribution, congestion control and scheduling (JFCS) to enable an intelligent traffic steering application in O-RAN. Combining tools from network utility maximization and stochastic optimization, we introduce a multi-layer optimization framework that provides fast convergence, long-term utility-optimality and significant delay reduction compared to the state-of-the-art and baseline RAN approaches. Our main contributions are three-fold: i) we propose the novel JFCS framework to efficiently and adaptively direct traffic to appropriate radio units; ii) we develop low-complexity algorithms based on the reinforcement learning, inne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20851;&#20110;&#21028;&#21035;&#24335;&#19982;&#29983;&#25104;&#24335;&#20998;&#31867;&#22120;&#30340;&#32463;&#20856;&#20027;&#39064;&#65292;&#21033;&#29992;&#22810;&#31867;$\mathcal{H}$-&#19968;&#33268;&#24615;&#19979;&#30028;&#65292;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#22810;&#31867;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#26679;&#26412;&#35201;&#27714;&#27604;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#22810;&#20102;$O(\log n)$&#12290;</title><link>http://arxiv.org/abs/2302.02334</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#21028;&#21035;&#24335;&#20998;&#31867;&#22120;&#19982;&#29983;&#25104;&#24335;&#20998;&#31867;&#22120;&#65306;&#29702;&#35770;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Discriminative vs. Generative Classifiers: Theory and Implications. (arXiv:2302.02334v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20851;&#20110;&#21028;&#21035;&#24335;&#19982;&#29983;&#25104;&#24335;&#20998;&#31867;&#22120;&#30340;&#32463;&#20856;&#20027;&#39064;&#65292;&#21033;&#29992;&#22810;&#31867;$\mathcal{H}$-&#19968;&#33268;&#24615;&#19979;&#30028;&#65292;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#22810;&#31867;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#26679;&#26412;&#35201;&#27714;&#27604;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#22810;&#20102;$O(\log n)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#28145;&#24230;&#27169;&#22411;&#39044;&#20808;&#22312;&#22823;&#35268;&#27169;&#26631;&#35760;&#25110;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#32447;&#24615;&#35780;&#20272;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#20923;&#32467;&#65292;&#24182;&#21333;&#29420;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#26377;&#21560;&#24341;&#21147;&#30340;&#36716;&#31227;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#32447;&#24615;&#35780;&#20272;&#20013;&#30340;&#20998;&#31867;&#22120;&#65292;&#38500;&#20102;&#40664;&#35748;&#30340;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#21463;&#21040;&#26420;&#32032;&#36125;&#21494;&#26031;&#30340;&#32479;&#35745;&#25928;&#29575;&#21551;&#21457;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;&#20851;&#20110;&#21028;&#21035;&#24335;&#19982;&#29983;&#25104;&#24335;&#20998;&#31867;&#22120;&#30340;&#32463;&#20856;&#20027;&#39064;&#12290;&#29702;&#35770;&#19978;&#65292;&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#20195;&#29702;&#25439;&#22833;&#32780;&#19981;&#26159;0-1&#25439;&#22833;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#23558;&#32463;&#20856;&#32467;&#26524;&#20174;&#20108;&#20803;&#24773;&#20917;&#25512;&#24191;&#21040;&#22810;&#31867;&#24773;&#20917;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#22810;&#31867;&#26420;&#32032;&#36125;&#21494;&#26031;&#38656;&#35201;$O(\log n)$&#20010;&#26679;&#26412;&#26469;&#25509;&#36817;&#20854;&#28176;&#36817;&#35823;&#24046;&#65292;&#32780;&#30456;&#24212;&#30340;&#22810;&#31867;&#36923;&#36753;&#22238;&#24402;&#38656;&#35201;$O(n)$&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;$n$&#26159;&#29305;&#24449;&#32500;&#24230;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#31867;$\mathcal{H}$-&#19968;&#33268;&#24615;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large-scale deep model pre-trained on massive labeled or unlabeled data transfers well to downstream tasks. Linear evaluation freezes parameters in the pre-trained model and trains a linear classifier separately, which is efficient and attractive for transfer. However, little work has investigated the classifier in linear evaluation except for the default logistic regression. Inspired by the statistical efficiency of naive Bayes, the paper revisits the classical topic on discriminative vs. generative classifiers. Theoretically, the paper considers the surrogate loss instead of the zero-one loss in analyses and generalizes the classical results from binary cases to multiclass ones. We show that, under mild assumptions, multiclass naive Bayes requires $O(\log n)$ samples to approach its asymptotic error while the corresponding multiclass logistic regression requires $O(n)$ samples, where $n$ is the feature dimension. To establish it, we present a multiclass $\mathcal{H}$-consistency bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#29289;&#32852;&#32593;&#20725;&#23608;&#32593;&#32476;&#25915;&#20987;&#20197;&#21450;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#36739;&#23567;&#30340;&#39044;&#31639;&#19979;&#21152;&#36895;&#35757;&#32451;&#21644;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#27604;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02013</link><description>&lt;p&gt;
&#22522;&#20110;&#32463;&#27982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;IoT&#20725;&#23608;&#32593;&#32476;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
IoT Botnet Detection Using an Economic Deep Learning Model. (arXiv:2302.02013v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#29289;&#32852;&#32593;&#20725;&#23608;&#32593;&#32476;&#25915;&#20987;&#20197;&#21450;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#36739;&#23567;&#30340;&#39044;&#31639;&#19979;&#21152;&#36895;&#35757;&#32451;&#21644;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#27604;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#21019;&#26032;&#30340;&#24555;&#36895;&#36827;&#27493;&#22686;&#21152;&#20102;&#36807;&#21435;&#21313;&#24180;&#30340;&#20351;&#29992;&#21644;&#20998;&#21457;&#12290;&#20840;&#29699;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#24555;&#36895;&#22686;&#38271;&#22686;&#21152;&#20102;&#30001;&#24694;&#24847;&#31532;&#19977;&#26041;&#21019;&#24314;&#30340;&#32593;&#32476;&#23433;&#20840;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#23433;&#20840;&#38382;&#39064;&#21644;&#29289;&#32852;&#32593;&#31995;&#32479;&#38480;&#21046;&#30340;&#21487;&#38752;&#20837;&#20405;&#26816;&#27979;&#21644;&#32593;&#32476;&#21462;&#35777;&#31995;&#32479;&#23545;&#20110;&#20445;&#25252;&#36825;&#20123;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#29289;&#32852;&#32593;&#20725;&#23608;&#32593;&#32476;&#25915;&#20987;&#26159;&#20225;&#19994;&#21644;&#20010;&#20154;&#38754;&#20020;&#30340;&#37325;&#22823;&#23041;&#32961;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#27982;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#29289;&#32852;&#32593;&#20725;&#23608;&#32593;&#32476;&#25915;&#20987;&#20197;&#21450;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#36739;&#23567;&#30340;&#23454;&#29616;&#39044;&#31639;&#19979;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#21644;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#33719;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress in technology innovation usage and distribution has increased in the last decade. The rapid growth of the Internet of Things (IoT) systems worldwide has increased network security challenges created by malicious third parties. Thus, reliable intrusion detection and network forensics systems that consider security concerns and IoT systems limitations are essential to protect such systems. IoT botnet attacks are one of the significant threats to enterprises and individuals. Thus, this paper proposed an economic deep learning-based model for detecting IoT botnet attacks along with different types of attacks. The proposed model achieved higher accuracy than the state-of-the-art detection models using a smaller implementation budget and accelerating the training and detecting processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#29305;&#24449;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#35777;&#26126;&#20102;&#22312;&#38543;&#26426;&#29305;&#24449;&#20013;&#65292;&#21363;&#20351;&#28385;&#36275;&#31283;&#20581;&#24615;&#30340;&#36890;&#29992;&#23450;&#24459;&#25152;&#38656;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#27169;&#22411;&#20063;&#19981;&#20855;&#26377;&#20219;&#20309;&#36807;&#24230;&#21442;&#25968;&#21270;&#31243;&#24230;&#30340;&#31283;&#20581;&#24615;&#12290;&#30456;&#23545;&#22320;&#65292;&#23545;&#20110;&#20598;&#28608;&#27963;&#24773;&#20917;&#65292;NTK&#27169;&#22411;&#28385;&#36275;&#26222;&#36941;&#19979;&#38480;&#65292;&#21482;&#35201;&#28385;&#36275;&#36807;&#21442;&#25968;&#26465;&#20214;&#23601;&#33021;&#31283;&#20581;&#12290;&#36825;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31283;&#20581;&#24615;&#25552;&#20379;&#20102;&#26356;&#23574;&#38160;&#30340;&#27861;&#21017;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#24314;&#31435;&#30340;&#26222;&#36866;&#23450;&#24459;&#12290;</title><link>http://arxiv.org/abs/2302.01629</link><description>&lt;p&gt;
&#36229;&#36234;&#31283;&#20581;&#24615;&#30340;&#26222;&#36866;&#23450;&#24459;&#65306;&#38543;&#26426;&#29305;&#24449;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#26356;&#23574;&#38160;&#27861;&#21017;
&lt;/p&gt;
&lt;p&gt;
Beyond the Universal Law of Robustness: Sharper Laws for Random Features and Neural Tangent Kernels. (arXiv:2302.01629v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#29305;&#24449;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#35777;&#26126;&#20102;&#22312;&#38543;&#26426;&#29305;&#24449;&#20013;&#65292;&#21363;&#20351;&#28385;&#36275;&#31283;&#20581;&#24615;&#30340;&#36890;&#29992;&#23450;&#24459;&#25152;&#38656;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#27169;&#22411;&#20063;&#19981;&#20855;&#26377;&#20219;&#20309;&#36807;&#24230;&#21442;&#25968;&#21270;&#31243;&#24230;&#30340;&#31283;&#20581;&#24615;&#12290;&#30456;&#23545;&#22320;&#65292;&#23545;&#20110;&#20598;&#28608;&#27963;&#24773;&#20917;&#65292;NTK&#27169;&#22411;&#28385;&#36275;&#26222;&#36941;&#19979;&#38480;&#65292;&#21482;&#35201;&#28385;&#36275;&#36807;&#21442;&#25968;&#26465;&#20214;&#23601;&#33021;&#31283;&#20581;&#12290;&#36825;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31283;&#20581;&#24615;&#25552;&#20379;&#20102;&#26356;&#23574;&#38160;&#30340;&#27861;&#21017;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#24314;&#31435;&#30340;&#26222;&#36866;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#24178;&#25200;&#65292;Bubeck&#21644;Sellke&#30340;&#19968;&#20010;&#26377;&#24605;&#24819;&#21551;&#31034;&#30340;&#25991;&#31456;&#36890;&#36807;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#35270;&#35282;&#20998;&#26512;&#20102;&#36825;&#19968;&#29616;&#35937;&#65306;&#24179;&#28369;&#22320;&#25554;&#20540;&#25968;&#25454;&#38656;&#35201;&#30340;&#21442;&#25968;&#26174;&#33879;&#22810;&#20110;&#31616;&#21333;&#22320;&#35760;&#24518;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#8220;&#26222;&#36866;&#8221;&#30340;&#27861;&#21017;&#20165;&#20026;&#31283;&#20581;&#24615;&#25552;&#20379;&#20102;&#24517;&#35201;&#26465;&#20214;&#65292;&#26080;&#27861;&#21306;&#20998;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#19987;&#27880;&#20110;&#38543;&#26426;&#29305;&#24449;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#20004;&#20010;&#20856;&#22411;&#35774;&#32622;&#20013;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26469;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#38543;&#26426;&#29305;&#24449;&#20013;&#65292;&#21363;&#20351;&#28385;&#36275;&#31283;&#20581;&#24615;&#30340;&#36890;&#29992;&#23450;&#24459;&#25152;&#38656;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#27169;&#22411;&#20063;&#19981;&#20855;&#26377;&#20219;&#20309;&#36807;&#24230;&#21442;&#25968;&#21270;&#31243;&#24230;&#30340;&#31283;&#20581;&#24615;&#12290;&#30456;&#21453;&#65292;&#23545;&#20110;&#20598;&#28608;&#27963;&#24773;&#20917;&#65292;NTK&#27169;&#22411;&#28385;&#36275;&#26222;&#36941;&#19979;&#38480;&#65292;&#21482;&#35201;&#28385;&#36275;&#36807;&#21442;&#25968;&#26465;&#20214;&#23601;&#33021;&#31283;&#20581;&#12290;&#36825;&#20063;&#35299;&#20915;&#20102;&#20808;&#21069;&#22312;NTK&#26550;&#26500;&#30340;&#26368;&#20248;&#24615;&#19978;&#30340;&#29468;&#24819;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31283;&#20581;&#24615;&#25552;&#20379;&#20102;&#26356;&#23574;&#38160;&#30340;&#27861;&#21017;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#24314;&#31435;&#30340;&#26222;&#36866;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are vulnerable to adversarial perturbations, and a thought-provoking paper by Bubeck and Sellke has analyzed this phenomenon through the lens of over-parameterization: interpolating smoothly the data requires significantly more parameters than simply memorizing it. However, this "universal" law provides only a necessary condition for robustness, and it is unable to discriminate between models. In this paper, we address these gaps by focusing on empirical risk minimization in two prototypical settings, namely, random features and the neural tangent kernel (NTK). We prove that, for random features, the model is not robust for any degree of over-parameterization, even when the necessary condition coming from the universal law of robustness is satisfied. In contrast, for even activations, the NTK model meets the universal lower bound, and it is robust as soon as the necessary condition on over-parameterization is fulfilled. This also addresses a conjecture in prior 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#38543;&#26426;&#38598;&#25104;&#31639;&#27861;&#22312;&#23545;&#25239;&#25915;&#20987;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861; BARRE&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#24481;&#24378;&#22823;&#30340; $\ell_\infty$ &#33539;&#22260;&#20869;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2302.01375</link><description>&lt;p&gt;
&#38024;&#23545;&#23545;&#25239;&#25200;&#21160;&#30340;&#38543;&#26426;&#38598;&#25104;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Randomized Ensembles to Adversarial Perturbations. (arXiv:2302.01375v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#38543;&#26426;&#38598;&#25104;&#31639;&#27861;&#22312;&#23545;&#25239;&#25915;&#20987;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861; BARRE&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#24481;&#24378;&#22823;&#30340; $\ell_\infty$ &#33539;&#22260;&#20869;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#38598;&#25104;&#20998;&#31867;&#22120; (Randomized ensemble classifiers, RECs) &#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26367;&#20195;&#20256;&#32479;&#38598;&#25104;&#26041;&#27861;&#30340;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#36739;&#23567;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26500;&#36896; RECs &#30340;&#26041;&#27861;&#27604;&#26368;&#21021;&#22768;&#31216;&#30340;&#26356;&#33030;&#24369;&#65292;&#23545;&#20854;&#21151;&#25928;&#20135;&#29983;&#20102;&#37325;&#22823;&#24576;&#30097;&#65292;&#24182;&#24341;&#21457;&#20102;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#65292;&#20363;&#22914;&#65306;&#8220;RECs &#20309;&#26102;&#26377;&#25928;&#65311;&#8221;&#65292;&#8220;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#26159;&#20160;&#20040;&#65311;&#8221;&#65292;&#8220;&#25105;&#20204;&#22914;&#20309;&#35757;&#32451;&#23427;&#20204;&#65311;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#25506;&#32034; RECs &#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#22522;&#26412;&#32467;&#26524;&#65292;&#20363;&#22914; RECs &#30340;&#29702;&#35770;&#38480;&#21046;&#12289;&#20351;&#29992;&#23427;&#20204;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#26465;&#20214;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#31639;&#27861; (BARRE) &#29992;&#20110;&#35757;&#32451;&#24378;&#40065;&#26834;&#30340; RECs&#65292;&#24182;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#65292;&#35777;&#26126;&#20102;&#20854;&#23545;&#25239;&#24378; $\ell_\infty$ &#33539;&#22260;&#20869;&#30340;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized ensemble classifiers (RECs), where one classifier is randomly selected during inference, have emerged as an attractive alternative to traditional ensembling methods for realizing adversarially robust classifiers with limited compute requirements. However, recent works have shown that existing methods for constructing RECs are more vulnerable than initially claimed, casting major doubts on their efficacy and prompting fundamental questions such as: "When are RECs useful?", "What are their limits?", and "How do we train them?". In this work, we first demystify RECs as we derive fundamental results regarding their theoretical limits, necessary and sufficient conditions for them to be useful, and more. Leveraging this new understanding, we propose a new boosting algorithm (BARRE) for training robust RECs, and empirically demonstrate its effectiveness at defending against strong $\ell_\infty$ norm-bounded adversaries across various network architectures and datasets. Our code can
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#8212;&#8212;&#19978;&#19979;&#25991;&#22871;&#32034;&#65292;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#35299;&#20915;&#35299;&#37322;&#24615;&#21644;&#25311;&#21512;&#33021;&#21147;&#30340;&#30683;&#30462;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#31232;&#30095;&#25311;&#21512;&#65292;&#24182;&#19988;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2302.00878</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#22871;&#32034;&#65306;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#23454;&#29616;&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The contextual lasso: Sparse linear models via deep neural networks. (arXiv:2302.00878v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#8212;&#8212;&#19978;&#19979;&#25991;&#22871;&#32034;&#65292;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#35299;&#20915;&#35299;&#37322;&#24615;&#21644;&#25311;&#21512;&#33021;&#21147;&#30340;&#30683;&#30462;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#31232;&#30095;&#25311;&#21512;&#65292;&#24182;&#19988;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#40644;&#37329;&#26631;&#20934;&#24037;&#20855;&#65292;&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#24378;&#22823;&#30340;&#25311;&#21512;&#33021;&#21147;&#12290;&#19978;&#19979;&#25991;&#22871;&#32034;&#26159;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#23427;&#23558;&#36755;&#20837;&#29305;&#24449;&#20998;&#25104;&#21487;&#35299;&#37322;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#20004;&#32452;&#65292;&#24182;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#36827;&#34892;&#31232;&#30095;&#25311;&#21512;&#65292;&#21516;&#26102;&#20854;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#36825;&#20010;&#36807;&#31243;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#38656;&#21442;&#25968;&#22320;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse linear models are a gold standard tool for interpretable machine learning, a field of emerging importance as predictive models permeate decision-making in many domains. Unfortunately, sparse linear models are far less flexible as functions of their input features than black-box models like deep neural networks. With this capability gap in mind, we study a not-uncommon situation where the input features dichotomize into two groups: explanatory features, which are candidates for inclusion as variables in an interpretable model, and contextual features, which select from the candidate variables and determine their effects. This dichotomy leads us to the contextual lasso, a new statistical estimator that fits a sparse linear model to the explanatory features such that the sparsity pattern and coefficients vary as a function of the contextual features. The fitting process learns this function nonparametrically via a deep neural network. To attain sparse coefficients, we train the net
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#26041;&#27861;&#65292;&#37319;&#29992;&#23545;&#25968;&#27431;&#27663;&#20960;&#20309;&#65292;&#24182;&#34701;&#21512;&#20102;&#20855;&#26377;&#19981;&#21516;&#20445;&#30495;&#24230;&#21644;&#25104;&#26412;&#30340;&#25968;&#25454;&#28304;&#23618;&#27425;&#32467;&#26500;&#30340;&#26679;&#26412;&#65292;&#20445;&#35777;&#20102;&#26041;&#24046;&#30340;&#38477;&#20302;&#21644;&#30830;&#23450;&#24615;&#30340;&#20445;&#25345;&#65292;&#20351;&#24471;&#22312;&#20223;&#30495;&#25110;&#25968;&#25454;&#25910;&#38598;&#26114;&#36149;&#30340;&#24212;&#29992;&#20013;&#65292;&#21327;&#26041;&#24046;&#20272;&#35745;&#25104;&#20026;&#21487;&#34892;&#30340;&#12290;&#35813;&#26041;&#27861;&#22312;&#24230;&#37327;&#23398;&#20064;&#12289;&#25968;&#25454;&#21516;&#21270;&#21644;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2301.13749</link><description>&lt;p&gt;
&#23545;&#25968;&#27431;&#27663;&#20960;&#20309;&#20013;&#30340;&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-Fidelity Covariance Estimation in the Log-Euclidean Geometry. (arXiv:2301.13749v2 [stat.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#26041;&#27861;&#65292;&#37319;&#29992;&#23545;&#25968;&#27431;&#27663;&#20960;&#20309;&#65292;&#24182;&#34701;&#21512;&#20102;&#20855;&#26377;&#19981;&#21516;&#20445;&#30495;&#24230;&#21644;&#25104;&#26412;&#30340;&#25968;&#25454;&#28304;&#23618;&#27425;&#32467;&#26500;&#30340;&#26679;&#26412;&#65292;&#20445;&#35777;&#20102;&#26041;&#24046;&#30340;&#38477;&#20302;&#21644;&#30830;&#23450;&#24615;&#30340;&#20445;&#25345;&#65292;&#20351;&#24471;&#22312;&#20223;&#30495;&#25110;&#25968;&#25454;&#25910;&#38598;&#26114;&#36149;&#30340;&#24212;&#29992;&#20013;&#65292;&#21327;&#26041;&#24046;&#20272;&#35745;&#25104;&#20026;&#21487;&#34892;&#30340;&#12290;&#35813;&#26041;&#27861;&#22312;&#24230;&#37327;&#23398;&#20064;&#12289;&#25968;&#25454;&#21516;&#21270;&#21644;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#25968;&#27431;&#27663;&#20960;&#20309;&#30340;&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#30340;&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#12290;&#35813;&#20272;&#35745;&#22120;&#34701;&#21512;&#20102;&#20855;&#26377;&#19981;&#21516;&#20445;&#30495;&#24230;&#21644;&#25104;&#26412;&#30340;&#25968;&#25454;&#28304;&#23618;&#27425;&#32467;&#26500;&#30340;&#26679;&#26412;&#65292;&#20197;&#23454;&#29616;&#26041;&#24046;&#30340;&#38477;&#20302;&#65292;&#21516;&#26102;&#20445;&#35777;&#30830;&#23450;&#24615;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#12290;&#26032;&#30340;&#20272;&#35745;&#22120;&#20351;&#24471;&#22312;&#20223;&#30495;&#25110;&#25968;&#25454;&#25910;&#38598;&#26114;&#36149;&#30340;&#24212;&#29992;&#20013;&#65292;&#21327;&#26041;&#24046;&#20272;&#35745;&#25104;&#20026;&#21487;&#34892;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26368;&#20339;&#26679;&#26412;&#20998;&#37197;&#26041;&#26696;&#65292;&#20197;&#22312;&#22266;&#23450;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;&#20445;&#35777;&#30830;&#23450;&#24615;&#23545;&#20110;&#24230;&#37327;&#23398;&#20064;&#12289;&#25968;&#25454;&#21516;&#21270;&#21644;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#24212;&#29992;&#65288;&#28909;&#20256;&#23548;&#12289;&#27969;&#20307;&#21160;&#21147;&#23398;&#65289;&#30340;&#25968;&#25454;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#27604;&#22522;&#20934;&#27979;&#35797;&#26356;&#20934;&#30830;&#30340;&#24230;&#37327;&#23398;&#20064;&#21644;&#36229;&#36807;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a multi-fidelity estimator of covariance matrices that employs the log-Euclidean geometry of the symmetric positive-definite manifold. The estimator fuses samples from a hierarchy of data sources of differing fidelities and costs for variance reduction while guaranteeing definiteness, in contrast with previous approaches. The new estimator makes covariance estimation tractable in applications where simulation or data collection is expensive; to that end, we develop an optimal sample allocation scheme that minimizes the mean-squared error of the estimator given a fixed budget. Guaranteed definiteness is crucial to metric learning, data assimilation, and other downstream tasks. Evaluations of our approach using data from physical applications (heat conduction, fluid dynamics) demonstrate more accurate metric learning and speedups of more than one order of magnitude compared to benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#65288;OLO&#65289;&#28041;&#21450;&#26080;&#32422;&#26463;&#38382;&#39064;&#21644;&#21160;&#24577;&#36951;&#25022;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#26500;&#36896;&#38382;&#39064;&#20026;&#31232;&#30095;&#32534;&#30721;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#24335;&#65292;&#22312;&#36866;&#24212;&#24615;&#21644;&#24212;&#29992;&#19978;&#26377;&#36739;&#22909;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2301.13349</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23454;&#29616;&#26080;&#32422;&#26463;&#21160;&#24577;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Unconstrained Dynamic Regret via Sparse Coding. (arXiv:2301.13349v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#65288;OLO&#65289;&#28041;&#21450;&#26080;&#32422;&#26463;&#38382;&#39064;&#21644;&#21160;&#24577;&#36951;&#25022;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#26500;&#36896;&#38382;&#39064;&#20026;&#31232;&#30095;&#32534;&#30721;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#24335;&#65292;&#22312;&#36866;&#24212;&#24615;&#21644;&#24212;&#29992;&#19978;&#26377;&#36739;&#22909;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#65288;OLO&#65289;&#22312;&#20004;&#20010;&#38382;&#39064;&#32467;&#26500;&#30340;&#32806;&#21512;&#19979;&#30340;&#24773;&#20917;&#65306;&#22495;&#26080;&#30028;&#65292;&#32780;&#31639;&#27861;&#30340;&#24615;&#33021;&#26159;&#36890;&#36807;&#21160;&#24577;&#36951;&#25022;&#26469;&#34913;&#37327;&#30340;&#12290;&#22788;&#29702;&#20219;&#19968;&#38382;&#39064;&#37117;&#35201;&#27714;&#36951;&#25022;&#30028;&#38480;&#20381;&#36182;&#20110;&#27604;&#36739;&#24207;&#21015;&#30340;&#26576;&#20123;&#22797;&#26434;&#24230;&#37327;&#24230; - &#29305;&#21035;&#26159;&#26080;&#32422;&#26463;OLO&#20013;&#30340;&#27604;&#36739;&#22120;&#33539;&#25968;&#65292;&#20197;&#21450;&#21160;&#24577;&#36951;&#25022;&#20013;&#30340;&#36335;&#24452;&#38271;&#24230;&#12290;&#19982;&#26368;&#36817;&#19968;&#31687;&#25991;&#31456;(Jacobsen&amp; Cutkosky&#65292;2022)&#36866;&#24212;&#36825;&#20004;&#20010;&#22797;&#26434;&#24230;&#37327;&#24230;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#26500;&#36896;&#38382;&#39064;&#20026;&#31232;&#30095;&#32534;&#30721;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#24335;&#12290;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#23454;&#29616;&#36866;&#24212;&#24615;&#65292;&#36825;&#20010;&#26694;&#26550;&#33258;&#28982;&#22320;&#21033;&#29992;&#20102;&#29615;&#22659;&#26356;&#22797;&#26434;&#30340;&#21069;&#32622;&#30693;&#35782;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38745;&#24577;&#26080;&#32422;&#26463;OLO&#26799;&#24230;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#36830;&#32493;&#26102;&#38388;&#26426;&#21046;&#35774;&#35745;&#12290;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#29420;&#31435;&#20852;&#36259;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by time series forecasting, we study Online Linear Optimization (OLO) under the coupling of two problem structures: the domain is unbounded, and the performance of an algorithm is measured by its dynamic regret. Handling either of them requires the regret bound to depend on certain complexity measure of the comparator sequence -- specifically, the comparator norm in unconstrained OLO, and the path length in dynamic regret. In contrast to a recent work (Jacobsen &amp; Cutkosky, 2022) that adapts to the combination of these two complexity measures, we propose an alternative complexity measure by recasting the problem into sparse coding. Adaptivity can be achieved by a simple modular framework, which naturally exploits more intricate prior knowledge of the environment. Along the way, we also present a new gradient adaptive algorithm for static unconstrained OLO, designed using novel continuous time machinery. This could be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#36870;Cholesky&#22240;&#23376;&#30340;&#39640;&#26031;&#20998;&#24067;&#30340;&#21464;&#20998;&#36924;&#36817;&#26041;&#27861;&#65292;&#32467;&#21512;&#21516;&#26679;&#39640;&#25928;&#30340;SIC&#32422;&#26463;&#30340;Kullback-Leibler&#26368;&#20248;&#20808;&#39564;&#36924;&#36817;&#65292;&#24182;&#22312;&#29305;&#23450;SIC&#25490;&#24207;&#21644;&#31232;&#30095;&#27169;&#24335;&#19979;&#65292;&#23454;&#29616;&#23545;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#30340;&#39640;&#24230;&#20934;&#30830;&#20808;&#39564;&#21644;&#21518;&#39564;&#36924;&#36817;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#31867;&#20284;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#24179;&#31283;&#26680;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2301.13303</link><description>&lt;p&gt;
&#21452;Kullback-Leibler&#26368;&#23567;&#21270;&#30340;&#21464;&#20998;&#31232;&#30095;&#36870;Cholesky&#36817;&#20284;&#29992;&#20110;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Variational sparse inverse Cholesky approximation for latent Gaussian processes via double Kullback-Leibler minimization. (arXiv:2301.13303v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#36870;Cholesky&#22240;&#23376;&#30340;&#39640;&#26031;&#20998;&#24067;&#30340;&#21464;&#20998;&#36924;&#36817;&#26041;&#27861;&#65292;&#32467;&#21512;&#21516;&#26679;&#39640;&#25928;&#30340;SIC&#32422;&#26463;&#30340;Kullback-Leibler&#26368;&#20248;&#20808;&#39564;&#36924;&#36817;&#65292;&#24182;&#22312;&#29305;&#23450;SIC&#25490;&#24207;&#21644;&#31232;&#30095;&#27169;&#24335;&#19979;&#65292;&#23454;&#29616;&#23545;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#30340;&#39640;&#24230;&#20934;&#30830;&#20808;&#39564;&#21644;&#21518;&#39564;&#36924;&#36817;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#31867;&#20284;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#24179;&#31283;&#26680;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#21487;&#25193;&#23637;&#21644;&#20934;&#30830;&#30340;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#25512;&#26029;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#26063;&#20855;&#26377;&#31232;&#30095;&#36870;Cholesky&#65288;SIC&#65289;&#22240;&#23376;&#30340;&#39640;&#26031;&#20998;&#24067;&#30340;&#21464;&#20998;&#36924;&#36817;&#12290;&#25105;&#20204;&#23558;&#35813;&#21464;&#20998;&#36924;&#36817;&#30340;&#21518;&#39564;&#19982;&#31867;&#20284;&#30340;&#39640;&#25928;SIC&#32422;&#26463;&#30340;Kullback-Leibler&#26368;&#20248;&#20808;&#39564;&#36924;&#36817;&#30456;&#32467;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#29305;&#23450;&#30340;SIC&#25490;&#24207;&#21644;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#31232;&#30095;&#27169;&#24335;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#39640;&#24230;&#20934;&#30830;&#30340;&#20808;&#39564;&#21644;&#21518;&#39564;&#36924;&#36817;&#12290;&#23545;&#20110;&#36825;&#31181;&#35774;&#32622;&#65292;&#25105;&#20204;&#30340;&#21464;&#20998;&#36924;&#36817;&#21487;&#20197;&#36890;&#36807;&#27599;&#27425;&#36845;&#20195;&#30340;&#23545;&#25968;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#23383;&#27604;&#36739;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#21452;Kullback-Leibler&#26368;&#20248;&#39640;&#26031;&#36807;&#31243;&#36924;&#36817;&#65288;DKLGP&#65289;&#26377;&#26102;&#21487;&#20197;&#27604;&#35832;&#22914;&#35825;&#23548;&#28857;&#21644;&#22343;&#20540;&#22330;&#36924;&#36817;&#31561;&#22312;&#31867;&#20284;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#24179;&#31283;&#26680;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
To achieve scalable and accurate inference for latent Gaussian processes, we propose a variational approximation based on a family of Gaussian distributions whose covariance matrices have sparse inverse Cholesky (SIC) factors. We combine this variational approximation of the posterior with a similar and efficient SIC-restricted Kullback-Leibler-optimal approximation of the prior. We then focus on a particular SIC ordering and nearest-neighbor-based sparsity pattern resulting in highly accurate prior and posterior approximations. For this setting, our variational approximation can be computed via stochastic gradient descent in polylogarithmic time per iteration. We provide numerical comparisons showing that the proposed double-Kullback-Leibler-optimal Gaussian-process approximation (DKLGP) can sometimes be vastly more accurate for stationary kernels than alternative approaches such as inducing-point and mean-field approximations at similar computational complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24102;&#26377;&#30690;&#37327;&#37327;&#21270;&#27169;&#22411;&#30340;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35782;&#21035;&#19987;&#23478;&#36712;&#36857;&#30340;&#23376;&#30446;&#26631;&#24182;&#24314;&#31435;&#30690;&#37327;&#37327;&#21270;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#23376;&#30446;&#26631;&#32423;&#21035;&#30340;&#35268;&#21010;&#65292;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#12289;&#38271;&#36828;&#30340;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12962</link><description>&lt;p&gt;
&#24102;&#26377;&#30690;&#37327;&#37327;&#21270;&#27169;&#22411;&#30340;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Imitation Learning with Vector Quantized Models. (arXiv:2301.12962v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24102;&#26377;&#30690;&#37327;&#37327;&#21270;&#27169;&#22411;&#30340;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35782;&#21035;&#19987;&#23478;&#36712;&#36857;&#30340;&#23376;&#30446;&#26631;&#24182;&#24314;&#31435;&#30690;&#37327;&#37327;&#21270;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#23376;&#30446;&#26631;&#32423;&#21035;&#30340;&#35268;&#21010;&#65292;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#12289;&#38271;&#36828;&#30340;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#22810;&#32423;&#25277;&#35937;&#30340;&#34892;&#21160;&#35268;&#21010;&#33021;&#21147;&#21487;&#20197;&#20351;&#26234;&#33021;&#20307;&#26377;&#25928;&#22320;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#20302;&#32423;&#35268;&#21010;&#27169;&#22411;&#21644;&#39640;&#32423;&#35268;&#21010;&#27169;&#22411;&#24182;&#24314;&#31435;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#32852;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#36755;&#20837;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20174;&#19987;&#23478;&#36712;&#36857;&#20013;&#35782;&#21035;&#23376;&#30446;&#26631;&#65292;&#36890;&#36807;&#23558;&#22870;&#21169;&#30340;&#22823;&#23567;&#19982;&#22312;&#32473;&#23450;&#29366;&#24577;&#21644;&#36873;&#25321;&#30340;&#23376;&#30446;&#26631;&#19979;&#21487;&#39044;&#27979;&#30340;&#20302;&#32423;&#34892;&#21160;&#30456;&#32852;&#31995;&#26469;&#23454;&#29616;&#35782;&#21035;&#12290;&#25105;&#20204;&#38024;&#23545;&#25152;&#35782;&#21035;&#30340;&#23376;&#30446;&#26631;&#24314;&#31435;&#30690;&#37327;&#37327;&#21270;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#23376;&#30446;&#26631;&#32423;&#21035;&#30340;&#35268;&#21010;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#31639;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#12289;&#38271;&#36828;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#35268;&#21010;&#65292;&#25152;&#20197;&#22312;&#35757;&#32451;&#38598;&#20013;&#27604;&#29616;&#26377;&#36712;&#36857;&#25214;&#21040;&#20102;&#26356;&#22909;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to plan actions on multiple levels of abstraction enables intelligent agents to solve complex tasks effectively. However, learning the models for both low and high-level planning from demonstrations has proven challenging, especially with higher-dimensional inputs. To address this issue, we propose to use reinforcement learning to identify subgoals in expert trajectories by associating the magnitude of the rewards with the predictability of low-level actions given the state and the chosen subgoal. We build a vector-quantized generative model for the identified subgoals to perform subgoal-level planning. In experiments, the algorithm excels at solving complex, long-horizon decision-making problems outperforming state-of-the-art. Because of its ability to plan, our algorithm can find better trajectories than the ones in the training set
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#26497;&#20540;&#20256;&#36755;(ET)&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#36827;&#34892;&#32473;&#23450;&#30456;&#20284;&#24615;&#20989;&#25968;&#19979;&#30340;&#19968;&#23545;&#22495;&#20043;&#38388;&#30340;&#26368;&#20339;&#21487;&#33021;&#30340;&#38750;&#37197;&#23545;&#32763;&#35793;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;(OT)&#30340;&#31639;&#27861;&#26469;&#36924;&#36817;ET&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2301.12874</link><description>&lt;p&gt;
&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#19979;&#30340;&#26497;&#20540;&#22495;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Extremal Domain Translation with Neural Optimal Transport. (arXiv:2301.12874v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#26497;&#20540;&#20256;&#36755;(ET)&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#36827;&#34892;&#32473;&#23450;&#30456;&#20284;&#24615;&#20989;&#25968;&#19979;&#30340;&#19968;&#23545;&#22495;&#20043;&#38388;&#30340;&#26368;&#20339;&#21487;&#33021;&#30340;&#38750;&#37197;&#23545;&#32763;&#35793;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;(OT)&#30340;&#31639;&#27861;&#26469;&#36924;&#36817;ET&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#26497;&#20540;&#20256;&#36755;(ET)&#8221;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#23545;&#20110;&#32473;&#23450;&#30456;&#20284;&#24615;&#20989;&#25968;&#30340;&#19968;&#23545;&#22495;&#20043;&#38388;&#30340;&#26368;&#20339;&#21487;&#33021;&#30340;&#38750;&#37197;&#23545;&#32763;&#35793;&#30340;&#25968;&#23398;&#24418;&#24335;&#21270;&#12290;&#21463;&#21040;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;(OT)&#36817;&#26399;&#21457;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#65292;&#20197;OT&#30340;&#37096;&#20998;&#26144;&#23556;&#26497;&#38480;&#26469;&#36924;&#36817;ET&#26144;&#23556;&#12290;&#25105;&#20204;&#22312;&#29609;&#20855;&#23454;&#20363;&#21644;&#38750;&#37197;&#23545;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#32763;&#35793;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the extremal transport (ET) which is a mathematical formalization of the theoretically best possible unpaired translation between a pair of domains w.r.t. the given similarity function. Inspired by the recent advances in neural optimal transport (OT), we propose a scalable algorithm to approximate ET maps as a limit of partial OT maps. We test our algorithm on toy examples and on the unpaired image-to-image translation task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#29420;&#31435;&#20998;&#35299;&#37319;&#26679;&#35268;&#21017;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#26641;&#20998;&#35299;&#37319;&#26679;&#22120;&#26377;&#21033;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20419;&#36827;&#20102;&#38543;&#26426;&#20998;&#35299;&#19978;&#32622;&#20449;&#24230;&#31639;&#27861;&#65288;RDUCB&#65289;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2301.12844</link><description>&lt;p&gt;
&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#65292;&#38543;&#26426;&#20998;&#35299;&#26159;&#21542;&#36275;&#22815;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Random Decompositions all we need in High Dimensional Bayesian Optimisation?. (arXiv:2301.12844v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#29420;&#31435;&#20998;&#35299;&#37319;&#26679;&#35268;&#21017;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#26641;&#20998;&#35299;&#37319;&#26679;&#22120;&#26377;&#21033;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20419;&#36827;&#20102;&#38543;&#26426;&#20998;&#35299;&#19978;&#32622;&#20449;&#24230;&#31639;&#27861;&#65288;RDUCB&#65289;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#20998;&#35299;&#26377;&#26395;&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#25214;&#21040;&#20934;&#30830;&#34920;&#31034;&#40657;&#30418;&#20989;&#25968;&#30340;&#36866;&#24403;&#20998;&#35299;&#12290;&#25105;&#20204;&#30740;&#31350;&#26412;&#25991;&#20013;&#20851;&#20110;&#25968;&#25454;&#29420;&#31435;&#20998;&#35299;&#37319;&#26679;&#35268;&#21017;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22522;&#20110;&#25968;&#25454;&#23398;&#20064;&#20998;&#35299;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#34987;&#35823;&#23548;&#21040;&#23616;&#37096;&#20998;&#35299;&#19978;&#65292;&#32780;&#36825;&#20123;&#20998;&#35299;&#22312;&#25972;&#20010;&#25628;&#32034;&#31354;&#38388;&#20013;&#24182;&#19981;&#20934;&#30830;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#22522;&#20110;&#38543;&#26426;&#26641;&#30340;&#20998;&#35299;&#37319;&#26679;&#22120;&#23637;&#29616;&#20102;&#26377;&#21033;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#21487;&#20197;&#26377;&#25928;&#26435;&#34913;&#26368;&#22823;&#20449;&#24687;&#22686;&#30410;&#21644;&#23454;&#38469;&#40657;&#30418;&#20989;&#25968;&#21450;&#20854;&#20998;&#35299;&#20043;&#38388;&#30340;&#20989;&#25968;&#22833;&#37197;&#12290;&#36825;&#20123;&#32467;&#26524;&#20419;&#36827;&#20102;&#38543;&#26426;&#20998;&#35299;&#19978;&#32622;&#20449;&#24230;&#31639;&#27861;&#65288;RDUCB&#65289;&#30340;&#21457;&#23637;&#65292;&#35813;&#31639;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#20960;&#20046;&#26159;&#21363;&#25554;&#21363;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning decompositions of expensive-to-evaluate black-box functions promises to scale Bayesian optimisation (BO) to high-dimensional problems. However, the success of these techniques depends on finding proper decompositions that accurately represent the black-box. While previous works learn those decompositions based on data, we investigate data-independent decomposition sampling rules in this paper. We find that data-driven learners of decompositions can be easily misled towards local decompositions that do not hold globally across the search space. Then, we formally show that a random tree-based decomposition sampler exhibits favourable theoretical guarantees that effectively trade off maximal information gain and functional mismatch between the actual black-box and its surrogate as provided by the decomposition. Those results motivate the development of the random decomposition upper-confidence bound algorithm (RDUCB) that is straightforward to implement - (almost) plug-and-play -
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38544;&#21464;&#37327;&#35889;&#27169;&#22411;&#30340;&#39640;&#32500;PDEs&#39640;&#25928;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#25237;&#24433;&#32593;&#32476;&#23558;&#39640;&#32500;&#25968;&#25454;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#32553;&#23567;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#28508;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#35889;&#26041;&#27861;&#22312;&#28508;&#31354;&#38388;&#20013;&#23398;&#20064;&#31639;&#23376;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#12290;&#25928;&#26524;&#21644;&#21487;&#25193;&#23637;&#24615;&#22312;Navier-Stokes&#21644;Schr&#246;dinger&#26041;&#31243;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.12664</link><description>&lt;p&gt;
&#22522;&#20110;&#38544;&#21464;&#37327;&#35889;&#27169;&#22411;&#27714;&#35299;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Solving High-Dimensional PDEs with Latent Spectral Models. (arXiv:2301.12664v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38544;&#21464;&#37327;&#35889;&#27169;&#22411;&#30340;&#39640;&#32500;PDEs&#39640;&#25928;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#25237;&#24433;&#32593;&#32476;&#23558;&#39640;&#32500;&#25968;&#25454;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#32553;&#23567;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#28508;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#35889;&#26041;&#27861;&#22312;&#28508;&#31354;&#38388;&#20013;&#23398;&#20064;&#31639;&#23376;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#12290;&#25928;&#26524;&#21644;&#21487;&#25193;&#23637;&#24615;&#22312;Navier-Stokes&#21644;Schr&#246;dinger&#26041;&#31243;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#20363;&#26159;&#23398;&#20064;&#31070;&#32463;&#31639;&#23376;&#26469;&#36817;&#20284;PDEs&#30340;&#36755;&#20837;&#36755;&#20986;&#26144;&#23556;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#28145;&#24230;&#27169;&#22411;&#24050;&#32463;&#25506;&#32034;&#20102;&#22810;&#23610;&#24230;&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#21508;&#31181;&#31639;&#23376;&#35774;&#35745;&#65292;&#20294;&#23427;&#20204;&#20165;&#38480;&#20110;&#22312;&#22352;&#26631;&#31354;&#38388;&#20013;&#25972;&#20307;&#23398;&#20064;&#31639;&#23376;&#12290;&#22312;&#23454;&#38469;&#29289;&#29702;&#31185;&#23398;&#38382;&#39064;&#20013;&#65292;PDEs&#26159;&#20855;&#26377;&#22797;&#26434;&#32806;&#21512;&#26041;&#31243;&#30340;&#65292;&#25968;&#20540;&#27714;&#35299;&#22120;&#20381;&#36182;&#20110;&#39640;&#32500;&#22352;&#26631;&#31354;&#38388;&#30340;&#31163;&#25955;&#21270;&#65292;&#36825;&#19981;&#33021;&#34987;&#21333;&#20010;&#31639;&#23376;&#20934;&#30830;&#22320;&#36817;&#20284;&#65292;&#20063;&#19981;&#33021;&#30001;&#20110;&#32500;&#24230;&#35781;&#21650;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#21464;&#37327;&#35889;&#27169;&#22411;&#65288;LSM&#65289;&#30340;&#39640;&#32500;PDEs&#39640;&#25928;&#31934;&#30830;&#27714;&#35299;&#22120;&#12290;LSM&#19981;&#20165;&#36229;&#20986;&#20102;&#22352;&#26631;&#31354;&#38388;&#65292;&#32780;&#19988;&#36824;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#25237;&#24433;&#32593;&#32476;&#23558;&#39640;&#32500;&#25968;&#25454;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#32553;&#23567;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#28508;&#31354;&#38388;&#12290;&#21463;&#25968;&#20540;&#20998;&#26512;&#20013;&#32463;&#20856;&#30340;&#35889;&#26041;&#27861;&#30340;&#21551;&#31034;&#65292;LSM&#21033;&#29992;&#35889;&#26041;&#27861;&#22312;&#28508;&#31354;&#38388;&#20013;&#23398;&#20064;&#31639;&#23376;&#65292;&#21487;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#39640;&#32500;PDEs&#19978;&#23637;&#31034;&#20102;LSM&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22914;Navier-Stokes&#26041;&#31243;&#21644;Schr&#246;dinger&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep models have achieved impressive progress in solving partial differential equations (PDEs). A burgeoning paradigm is learning neural operators to approximate the input-output mappings of PDEs. While previous deep models have explored the multiscale architectures and various operator designs, they are limited to learning the operators as a whole in the coordinate space. In real physical science problems, PDEs are complex coupled equations with numerical solvers relying on discretization into high-dimensional coordinate space, which cannot be precisely approximated by a single operator nor efficiently learned due to the curse of dimensionality. We present Latent Spectral Models (LSM) toward an efficient and precise solver for high-dimensional PDEs. Going beyond the coordinate space, LSM enables an attention-based hierarchical projection network to reduce the high-dimensional data into a compact latent space in linear time. Inspired by classical spectral methods in numerical analysis,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Few-shot&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#35757;&#32451;&#31639;&#27861;&#21644;&#36866;&#24212;&#31639;&#27861;&#65292;&#24182;&#23454;&#35777;&#35777;&#26126;&#36825;&#20004;&#20010;&#31639;&#27861;&#26159;&#21487;&#20197;&#23436;&#20840;&#20998;&#31163;&#30340;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#30340;&#20803;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;&#20851;&#20110;Few-shot&#20998;&#31867;&#30340;&#20851;&#38190;&#26041;&#38754;&#21644;&#19982;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65289;&#30340;&#32852;&#31995;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2301.12246</link><description>&lt;p&gt;
&#20877;&#27425;&#28145;&#20837;&#25506;&#31350;Few-shot&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Few-shot Classification Again. (arXiv:2301.12246v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Few-shot&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#35757;&#32451;&#31639;&#27861;&#21644;&#36866;&#24212;&#31639;&#27861;&#65292;&#24182;&#23454;&#35777;&#35777;&#26126;&#36825;&#20004;&#20010;&#31639;&#27861;&#26159;&#21487;&#20197;&#23436;&#20840;&#20998;&#31163;&#30340;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#30340;&#20803;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;&#20851;&#20110;Few-shot&#20998;&#31867;&#30340;&#20851;&#38190;&#26041;&#38754;&#21644;&#19982;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65289;&#30340;&#32852;&#31995;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Few-shot&#20998;&#31867;&#31639;&#27861;&#30001;&#35757;&#32451;&#38454;&#27573;&#21644;&#36866;&#24212;&#38454;&#27573;&#32452;&#25104;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#27169;&#22411;&#22312;&#19968;&#20010;&#30456;&#23545;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#36866;&#24212;&#38454;&#27573;&#65292;&#24050;&#23398;&#20064;&#30340;&#27169;&#22411;&#34987;&#35843;&#25972;&#20197;&#36866;&#24212;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#20165;&#26377;&#26377;&#38480;&#26631;&#27880;&#26679;&#26412;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126;&#65292;&#35757;&#32451;&#31639;&#27861;&#21644;&#36866;&#24212;&#31639;&#27861;&#26159;&#23436;&#20840;&#29420;&#31435;&#30340;&#65292;&#36825;&#20351;&#24471;&#21487;&#20197;&#20998;&#21035;&#20026;&#27599;&#20010;&#38454;&#27573;&#36827;&#34892;&#31639;&#27861;&#20998;&#26512;&#21644;&#35774;&#35745;&#12290;&#38024;&#23545;&#27599;&#20010;&#38454;&#27573;&#30340;&#20803;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35265;&#35299;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;Few-shot&#20998;&#31867;&#30340;&#20851;&#38190;&#26041;&#38754;&#21644;&#19982;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65289;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#25581;&#31034;&#30340;&#35265;&#35299;&#21644;&#30740;&#31350;&#25361;&#25112;&#33021;&#28608;&#21457;&#30456;&#20851;&#26041;&#21521;&#30340;&#26410;&#26469;&#24037;&#20316;&#12290;&#35770;&#25991;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#20351;&#29992;PyTorch&#65289;&#21487;&#22312; https://github.com/Frankluox/CloserLookAgainFewShot &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot classification consists of a training phase where a model is learned on a relatively large dataset and an adaptation phase where the learned model is adapted to previously-unseen tasks with limited labeled samples. In this paper, we empirically prove that the training algorithm and the adaptation algorithm can be completely disentangled, which allows algorithm analysis and design to be done individually for each phase. Our meta-analysis for each phase reveals several interesting insights that may help better understand key aspects of few-shot classification and connections with other fields such as visual representation learning and transfer learning. We hope the insights and research challenges revealed in this paper can inspire future work in related directions. Code and pre-trained models (in PyTorch) are available at https://github.com/Frankluox/CloserLookAgainFewShot.
&lt;/p&gt;</description></item><item><title>CAROL&#26159;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#27169;&#22411;&#21644;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#65292;&#23398;&#20064;&#20986;&#30340;&#31574;&#30053;&#20855;&#26377;&#26426;&#22120;&#21487;&#35777;&#26126;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#35777;&#20070;&#65292;&#22312;&#23454;&#39564;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#35748;&#35777;&#24615;&#33021;&#21644;&#21487;&#27604;&#36739;&#30340;&#23545;&#25239;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11374</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#27169;&#22411;&#30340;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Certifiably Robust Reinforcement Learning through Model-Based Abstract Interpretation. (arXiv:2301.11374v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11374
&lt;/p&gt;
&lt;p&gt;
CAROL&#26159;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#27169;&#22411;&#21644;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#65292;&#23398;&#20064;&#20986;&#30340;&#31574;&#30053;&#20855;&#26377;&#26426;&#22120;&#21487;&#35777;&#26126;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#35777;&#20070;&#65292;&#22312;&#23454;&#39564;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#35748;&#35777;&#24615;&#33021;&#21644;&#21487;&#27604;&#36739;&#30340;&#23545;&#25239;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; CAROL&#65292;&#20854;&#23398;&#20064;&#20986;&#30340;&#31574;&#30053;&#24102;&#26377;&#21487;&#35777;&#26126;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#35777;&#20070;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#29615;&#22659;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#27599;&#20010;&#23398;&#20064;&#36845;&#20195;&#20013;&#20351;&#29992;&#35813;&#27169;&#22411;&#21644;&#22806;&#37096;&#25277;&#35937;&#35299;&#37322;&#22120;&#26500;&#24314;&#21487;&#24494;&#20998;&#30340;&#35777;&#26126;&#40065;&#26834;&#24615;&#20449;&#21495;&#20197;&#24341;&#23548;&#23398;&#20064;&#65292;&#30452;&#21040;&#25910;&#25947;&#26102;&#65292;&#35813;&#25277;&#35937;&#35299;&#37322;&#23601;&#30452;&#25509;&#23548;&#33268;&#20102;&#21487;&#38752;&#24615;&#35777;&#20070;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#65292;&#30028;&#23450;&#20102; CAROL &#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#32047;&#31215;&#22870;&#21169;&#12290;&#25105;&#20204;&#36824;&#22312;&#22235;&#20010;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340; MuJoCo &#29615;&#22659;&#19978;&#23545; CAROL &#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;CAROL &#23398;&#20064;&#20986;&#30340;&#31574;&#30053;&#19982;&#29616;&#26377;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#65292;&#23637;&#31034;&#20986;&#26174;&#33879;&#22686;&#24378;&#30340;&#35748;&#35777;&#24615;&#33021;&#19979;&#38480;&#21644;&#21487;&#27604;&#36739;&#30340;&#23545;&#25239;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a reinforcement learning (RL) framework in which the learned policy comes with a machine-checkable certificate of provable adversarial robustness. Our approach, called CAROL, learns a model of the environment. In each learning iteration, it uses the current version of this model and an external abstract interpreter to construct a differentiable signal for provable robustness. This signal is used to guide learning, and the abstract interpretation used to construct it directly leads to the robustness certificate returned at convergence. We give a theoretical analysis that bounds the worst-case accumulative reward of CAROL. We also experimentally evaluate CAROL on four MuJoCo environments with continuous state and action spaces. On these tasks, CAROL learns policies that, when contrasted with policies from the state-of-the-art robust RL algorithms, exhibit: (i) markedly enhanced certified performance lower bounds; and (ii) comparable performance under empirical adversarial atta
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#30740;&#31350;&#32676;&#20307;&#20013;&#30340;&#20849;&#20139;&#21644;&#29305;&#23450;&#20110;&#32452;&#30340;&#20449;&#24687;&#65292;&#20351;&#29992;Causal Multi-task Deep Ensemble&#65288;CMDE&#65289;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#21644;&#22810;&#27169;&#24577;&#21327;&#21464;&#37327;&#65292;&#24182;&#25552;&#20379;&#22240;&#26524;&#25928;&#24212;&#30340;&#28857;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11351</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20219;&#21153;&#28145;&#24230;&#38598;&#21512;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Estimating Causal Effects using a Multi-task Deep Ensemble. (arXiv:2301.11351v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11351
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#30740;&#31350;&#32676;&#20307;&#20013;&#30340;&#20849;&#20139;&#21644;&#29305;&#23450;&#20110;&#32452;&#30340;&#20449;&#24687;&#65292;&#20351;&#29992;Causal Multi-task Deep Ensemble&#65288;CMDE&#65289;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#21644;&#22810;&#27169;&#24577;&#21327;&#21464;&#37327;&#65292;&#24182;&#25552;&#20379;&#22240;&#26524;&#25928;&#24212;&#30340;&#28857;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#26377;&#35768;&#22810;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#28982;&#32780;&#24456;&#23569;&#26377;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#20687;&#22270;&#20687;&#31561;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Causal Multi-task Deep Ensemble (CMDE)&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#30740;&#31350;&#32676;&#20307;&#20013;&#30340;&#20849;&#20139;&#21644;&#29305;&#23450;&#20110;&#32452;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;CMDE&#19982;&#20808;&#39564;&#20013;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20855;&#26377;&#21516;&#31561;&#25928;&#26524;&#12290;&#19982;&#22810;&#20219;&#21153;GP&#30456;&#27604;&#65292;CMDE&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#21644;&#22810;&#27169;&#24577;&#21327;&#21464;&#37327;&#65292;&#24182;&#25552;&#20379;&#22240;&#26524;&#25928;&#24212;&#30340;&#28857;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#21457;&#29616;CMDE&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A number of methods have been proposed for causal effect estimation, yet few have demonstrated efficacy in handling data with complex structures, such as images. To fill this gap, we propose Causal Multi-task Deep Ensemble (CMDE), a novel framework that learns both shared and group-specific information from the study population. We provide proofs demonstrating equivalency of CDME to a multi-task Gaussian process (GP) with a coregionalization kernel a priori. Compared to multi-task GP, CMDE efficiently handles high-dimensional and multi-modal covariates and provides pointwise uncertainty estimates of causal effects. We evaluate our method across various types of datasets and tasks and find that CMDE outperforms state-of-the-art methods on a majority of these tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26368;&#22823;&#26368;&#20248;&#24615;&#36793;&#38469;&#8221;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#39044;&#27979;-&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#19979;&#28216;&#20248;&#21270;&#30340;&#26368;&#20248;&#24615;&#26465;&#20214;&#35774;&#35745;&#26426;&#22120;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#65292;&#20860;&#20855;&#35745;&#31639;&#25928;&#29575;&#21644;&#36739;&#22909;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#20013;&#26368;&#20248;&#35299;&#30340;&#35266;&#27979;&#20540;&#12290;</title><link>http://arxiv.org/abs/2301.11260</link><description>&lt;p&gt;
&#26368;&#22823;&#26368;&#20248;&#24615;&#36793;&#38469;&#65306;&#19978;&#19979;&#25991;&#32447;&#24615;&#35268;&#21010;&#21644;&#36870;&#32447;&#24615;&#35268;&#21010;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Maximum Optimality Margin: A Unified Approach for Contextual Linear Programming and Inverse Linear Programming. (arXiv:2301.11260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26368;&#22823;&#26368;&#20248;&#24615;&#36793;&#38469;&#8221;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#39044;&#27979;-&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#19979;&#28216;&#20248;&#21270;&#30340;&#26368;&#20248;&#24615;&#26465;&#20214;&#35774;&#35745;&#26426;&#22120;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#65292;&#20860;&#20855;&#35745;&#31639;&#25928;&#29575;&#21644;&#36739;&#22909;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#20013;&#26368;&#20248;&#35299;&#30340;&#35266;&#27979;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;-&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20219;&#21153;&#30340;&#36755;&#20986;&#29992;&#20316;&#26576;&#20010;&#19979;&#28216;&#20248;&#21270;&#38382;&#39064;&#65288;&#20363;&#22914;&#32447;&#24615;&#35268;&#21010;&#30340;&#30446;&#26631;&#31995;&#25968;&#21521;&#37327;&#65289;&#30340;&#36755;&#20837;&#12290;&#35813;&#38382;&#39064;&#20063;&#34987;&#31216;&#20026;&#39044;&#27979;&#20998;&#26512;&#25110;&#19978;&#19979;&#25991;&#32447;&#24615;&#35268;&#21010;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#35201;&#20040;&#21463;&#21040;&#65288;i&#65289;&#20248;&#21270;&#19981;&#21487;&#35299;&#24615;&#65288;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#65289;/&#32479;&#35745;&#25928;&#29575;&#20302;&#19979;&#65288;&#23376;&#20248;&#19968;&#33324;&#21270;&#30028;&#38480;&#65289;&#30340;&#22256;&#25200;&#65292;&#35201;&#20040;&#35201;&#27714;&#24378;&#26465;&#20214;&#65288;&#20363;&#22914;&#27809;&#26377;&#32422;&#26463;&#25110;&#25439;&#22833;&#26657;&#20934;&#65289;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26368;&#22823;&#26368;&#20248;&#24615;&#36793;&#38469;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19979;&#28216;&#20248;&#21270;&#30340;&#26368;&#20248;&#24615;&#26465;&#20214;&#35774;&#35745;&#26426;&#22120;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#12290;&#26368;&#22823;&#36793;&#38469;&#20844;&#24335;&#26082;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#65292;&#21448;&#20855;&#26377;&#22909;&#30340;&#23398;&#20064;&#31243;&#24207;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#21482;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#20013;&#26368;&#20248;&#35299;&#30340;&#35266;&#27979;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the predict-then-optimize problem where the output of a machine learning prediction task is used as the input of some downstream optimization problem, say, the objective coefficient vector of a linear program. The problem is also known as predictive analytics or contextual linear programming. The existing approaches largely suffer from either (i) optimization intractability (a non-convex objective function)/statistical inefficiency (a suboptimal generalization bound) or (ii) requiring strong condition(s) such as no constraint or loss calibration. We develop a new approach to the problem called \textit{maximum optimality margin} which designs the machine learning loss function by the optimality condition of the downstream optimization. The max-margin formulation enjoys both computational efficiency and good theoretical properties for the learning procedure. More importantly, our new approach only needs the observations of the optimal solution in the training data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.10886</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21644;&#36866;&#24212;&#24615;&#30340;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#12290;AIRS&#21487;&#20197;&#26681;&#25454;&#23454;&#26102;&#20272;&#35745;&#30340;&#20219;&#21153;&#22238;&#25253;&#20174;&#39044;&#23450;&#20041;&#30340;&#20989;&#25968;&#38598;&#20013;&#36873;&#25321;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#21487;&#38752;&#30340;&#25506;&#32034;&#28608;&#21169;&#24182;&#35299;&#20915;&#20559;&#32622;&#30446;&#26631;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#22810;&#31181;&#20869;&#22312;&#22870;&#21169;&#26041;&#27861;&#30340;&#39640;&#25928;&#21487;&#38752;&#23454;&#29616;&#26041;&#24335;&#12290;&#25105;&#20204;&#23558;AIRS&#24212;&#29992;&#22312;MiniGrid&#12289;Procgen&#21644;DeepMind&#25511;&#21046;&#22871;&#20214;&#30340;&#22810;&#39033;&#20219;&#21153;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#22823;&#37327;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;AIRS&#21487;&#20197;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#21644;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning (RL). More specifically, AIRS selects shaping function from a predefined set based on the estimated task return in real-time, providing reliable exploration incentives and alleviating the biased objective problem. Moreover, we develop an intrinsic reward toolkit to provide efficient and reliable implementations of diverse intrinsic reward approaches. We test AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite. Extensive simulation demonstrates that AIRS can outperform the benchmarking schemes and achieve superior performance with simple architecture.
&lt;/p&gt;</description></item><item><title>&#38134;&#34892;&#23478;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#20026;&#22312;&#32447;&#21163;&#21290;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#22788;&#29702;&#21453;&#39304;&#24310;&#36831;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2301.10500</link><description>&lt;p&gt;
&#38134;&#34892;&#23478;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65306;&#19968;&#31181;&#29992;&#20110;&#24310;&#36831;&#22312;&#32447;&#21163;&#21290;&#23398;&#20064;&#30340;&#36890;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Banker Online Mirror Descent: A Universal Approach for Delayed Online Bandit Learning. (arXiv:2301.10500v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10500
&lt;/p&gt;
&lt;p&gt;
&#38134;&#34892;&#23478;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#20026;&#22312;&#32447;&#21163;&#21290;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#22788;&#29702;&#21453;&#39304;&#24310;&#36831;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#38134;&#34892;&#23478;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65288;Banker-OMD&#65289;&#65292;&#23427;&#25512;&#24191;&#20102;&#22312;&#32447;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#32463;&#20856;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65288;OMD&#65289;&#25216;&#26415;&#12290;Banker-OMD &#26694;&#26550;&#20960;&#20046;&#23436;&#20840;&#35299;&#32806;&#20102;&#21453;&#39304;&#24310;&#36831;&#22788;&#29702;&#21644;&#20219;&#21153;&#29305;&#23450; OMD &#31639;&#27861;&#35774;&#35745;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#35774;&#35745;&#26032;&#31639;&#27861;&#20197;&#26377;&#25928;&#21644;&#31283;&#20581;&#22320;&#22788;&#29702;&#21453;&#39304;&#24310;&#36831;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24310;&#36831;&#21453;&#39304;&#30340;&#22312;&#32447;&#21163;&#21290;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;$\widetilde{\mathcal O}(\sqrt{T} + \sqrt{D})$&#39118;&#26684;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#20854;&#20013;$T$&#26159;&#22238;&#21512;&#25968;&#65292;$D$&#26159;&#24635;&#21453;&#39304;&#24310;&#36831;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20110;&#20004;&#20010;&#37325;&#35201;&#30340;&#24102;&#24310;&#36831;&#21453;&#39304;&#30340;&#21163;&#21290;&#23398;&#20064;&#22330;&#26223;&#65288;&#21253;&#25324;&#24310;&#36831;&#30340;&#26080;&#26631;&#24230;&#23545;&#25239;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#21644;&#24310;&#36831;&#30340;&#23545;&#25239;&#32447;&#24615;&#36172;&#21338;&#26426;&#65289;&#26469;&#23637;&#31034;Banker-OMD &#30340;&#33021;&#21147;&#12290;Banker-OMD &#23548;&#33268;&#20102;&#31532;&#19968;&#20010;&#24310;&#36831;&#30340;&#26080;&#26631;&#24230;&#23545;&#25239; MAB &#31639;&#27861;&#65292;&#20854;&#23454;&#29616;&#20102;$\widetilde{\ma
&lt;/p&gt;
&lt;p&gt;
We propose Banker Online Mirror Descent (Banker-OMD), a novel framework generalizing the classical Online Mirror Descent (OMD) technique in the online learning literature. The Banker-OMD framework almost completely decouples feedback delay handling and the task-specific OMD algorithm design, thus facilitating the design of new algorithms capable of efficiently and robustly handling feedback delays. Specifically, it offers a general methodology for achieving $\widetilde{\mathcal O}(\sqrt{T} + \sqrt{D})$-style regret bounds in online bandit learning tasks with delayed feedback, where $T$ is the number of rounds and $D$ is the total feedback delay. We demonstrate the power of \texttt{Banker-OMD} by applications to two important bandit learning scenarios with delayed feedback, including delayed scale-free adversarial Multi-Armed Bandits (MAB) and delayed adversarial linear bandits. \texttt{Banker-OMD} leads to the first delayed scale-free adversarial MAB algorithm achieving $\widetilde{\ma
&lt;/p&gt;</description></item><item><title>DIFFormer&#26159;&#19968;&#31181;&#33021;&#37327;&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#34701;&#21512;&#20854;&#20182;&#23454;&#20363;&#20449;&#24687;&#30340;&#28436;&#21270;&#29366;&#24577;&#65292;&#23548;&#20986;&#20102;&#19968;&#31867;&#26032;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;&#31216;&#20026;DIFFormer&#65288;&#22522;&#20110;&#25193;&#25955;&#30340;Transformer&#65289;&#65292;&#33021;&#22815;&#25581;&#31034;&#30495;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2301.09474</link><description>&lt;p&gt;
DIFFormer&#65306;&#36890;&#36807;&#21463;&#33021;&#37327;&#38480;&#21046;&#30340;&#25193;&#25955;&#24341;&#20986;&#30340;&#21487;&#25193;&#23637;&#65288;&#22270;&#24418;&#65289;Transformer
&lt;/p&gt;
&lt;p&gt;
DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion. (arXiv:2301.09474v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09474
&lt;/p&gt;
&lt;p&gt;
DIFFormer&#26159;&#19968;&#31181;&#33021;&#37327;&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#34701;&#21512;&#20854;&#20182;&#23454;&#20363;&#20449;&#24687;&#30340;&#28436;&#21270;&#29366;&#24577;&#65292;&#23548;&#20986;&#20102;&#19968;&#31867;&#26032;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;&#31216;&#20026;DIFFormer&#65288;&#22522;&#20110;&#25193;&#25955;&#30340;Transformer&#65289;&#65292;&#33021;&#22815;&#25581;&#31034;&#30495;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#29983;&#25104;&#24120;&#24120;&#28041;&#21450;&#23454;&#20363;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20381;&#36182;&#65292;&#36829;&#21453;&#20102;&#26631;&#20934;&#23398;&#20064;&#33539;&#24335;&#30340;IID&#25968;&#25454;&#20551;&#35774;&#65292;&#20174;&#32780;&#23545;&#25581;&#31034;&#20960;&#20309;&#32467;&#26500;&#20197;&#23398;&#20064;&#25152;&#38656;&#35201;&#30340;&#23454;&#20363;&#34920;&#31034;&#24418;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#37327;&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#19968;&#25209;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#32534;&#30721;&#20026;&#36880;&#28176;&#34701;&#21512;&#20102;&#20854;&#20182;&#23454;&#20363;&#20449;&#24687;&#30340;&#28436;&#21270;&#29366;&#24577;&#12290;&#25193;&#25955;&#36807;&#31243;&#21463;&#38480;&#20110;&#22522;&#20110;&#21512;&#29702;&#33021;&#37327;&#20989;&#25968;&#30340;&#19979;&#38477;&#26631;&#20934;&#65292;&#35813;&#20989;&#25968;&#34920;&#24449;&#20102;&#28508;&#22312;&#32467;&#26500;&#19978;&#23454;&#20363;&#34920;&#31034;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#35880;&#30340;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#26263;&#31034;&#20102;&#20219;&#24847;&#23454;&#20363;&#23545;&#20043;&#38388;&#30340;&#26368;&#20248;&#25193;&#25955;&#24378;&#24230;&#30340;&#38381;&#21512;&#24418;&#24335;&#20272;&#35745;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#31867;&#26032;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#30340;&#20135;&#29983;&#65306;DIFFormer&#65288;&#22522;&#20110;&#25193;&#25955;&#30340;Transformer&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#29256;&#26412;&#65306;&#19968;&#20010;&#31616;&#21333;&#29256;&#26412;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#38754;&#20020;&#30528;&#31105;&#24524;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data generation often involves complex inter-dependencies among instances, violating the IID-data hypothesis of standard learning paradigms and posing a challenge for uncovering the geometric structures for learning desired instance representations. To this end, we introduce an energy constrained diffusion model which encodes a batch of instances from a dataset into evolutionary states that progressively incorporate other instances' information by their interactions. The diffusion process is constrained by descent criteria w.r.t.~a principled energy function that characterizes the global consistency of instance representations over latent structures. We provide rigorous theory that implies closed-form optimal estimates for the pairwise diffusion strength among arbitrary instance pairs, which gives rise to a new class of neural encoders, dubbed as DIFFormer (diffusion-based Transformers), with two instantiations: a simple version with linear complexity for prohibitive instanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22320;&#21306;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#27668;&#35937;&#25968;&#25454;&#24182;&#25552;&#20379;&#22825;&#27668;&#39044;&#25253;&#12290;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#22320;&#21306;&#38388;&#25968;&#25454;&#26333;&#20809;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;&#26032;&#39062;&#30340;&#25552;&#31034;&#23398;&#20064;&#26426;&#21046;&#28385;&#36275;&#20302;&#36164;&#28304;&#20256;&#24863;&#22120;&#30340;&#36890;&#20449;&#21644;&#35745;&#31639;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2301.09152</link><description>&lt;p&gt;
&#22825;&#27668;&#39044;&#25253;&#30340;Prompt&#32852;&#37030;&#23398;&#20064;&#65306;&#26500;&#24314;&#27668;&#35937;&#25968;&#25454;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prompt Federated Learning for Weather Forecasting: Toward Foundation Models on Meteorological Data. (arXiv:2301.09152v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22320;&#21306;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#27668;&#35937;&#25968;&#25454;&#24182;&#25552;&#20379;&#22825;&#27668;&#39044;&#25253;&#12290;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#22320;&#21306;&#38388;&#25968;&#25454;&#26333;&#20809;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;&#26032;&#39062;&#30340;&#25552;&#31034;&#23398;&#20064;&#26426;&#21046;&#28385;&#36275;&#20302;&#36164;&#28304;&#20256;&#24863;&#22120;&#30340;&#36890;&#20449;&#21644;&#35745;&#31639;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#20840;&#29699;&#27668;&#20505;&#25361;&#25112;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#19968;&#20010;&#21327;&#20316;&#24179;&#21488;&#65292;&#23545;&#22823;&#35268;&#27169;&#27668;&#35937;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#22825;&#27668;&#39044;&#25253;&#12290;&#23613;&#31649;&#24456;&#32039;&#36843;&#65292;&#20294;&#22269;&#23478;&#21644;&#22320;&#21306;&#20043;&#38388;&#24322;&#26500;&#30340;&#27668;&#35937;&#20256;&#24863;&#22120;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#22810;&#21464;&#37327;&#24322;&#36136;&#24615;&#21644;&#25968;&#25454;&#26333;&#20809;&#25104;&#20026;&#20027;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#36328;&#22320;&#21306;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#27668;&#35937;&#25968;&#25454;&#24182;&#25552;&#20379;&#22825;&#27668;&#39044;&#25253;&#12290;&#20026;&#20102;&#20943;&#36731;&#22320;&#21306;&#38388;&#30340;&#25968;&#25454;&#26333;&#20809;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21442;&#19982;&#32773;&#20043;&#38388;&#24322;&#26500;&#30340;&#27668;&#35937;&#25968;&#25454;&#21327;&#20316;&#23398;&#20064;&#20840;&#26032;&#30340;&#26102;&#31354;&#22522;&#20110;Transformer&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#23398;&#20064;&#26426;&#21046;&#65292;&#20197;&#28385;&#36275;&#20302;&#36164;&#28304;&#20256;&#24863;&#22120;&#30340;&#36890;&#20449;&#21644;&#35745;&#31639;&#32422;&#26463;&#12290;&#37319;&#29992;&#19977;&#20010;&#27668;&#35937;&#25968;&#25454;&#38598;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;, &#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To tackle the global climate challenge, it urgently needs to develop a collaborative platform for comprehensive weather forecasting on large-scale meteorological data. Despite urgency, heterogeneous meteorological sensors across countries and regions, inevitably causing multivariate heterogeneity and data exposure, become the main barrier. This paper develops a foundation model across regions capable of understanding complex meteorological data and providing weather forecasting. To relieve the data exposure concern across regions, a novel federated learning approach has been proposed to collaboratively learn a brand-new spatio-temporal Transformer-based foundation model across participants with heterogeneous meteorological data. Moreover, a novel prompt learning mechanism has been adopted to satisfy low-resourced sensors' communication and computational constraints. The effectiveness of the proposed method has been demonstrated on classical weather forecasting tasks using three meteoro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DiME&#30340;&#20449;&#24687;&#29702;&#35770;&#37327;&#65292;&#21487;&#20197;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#36991;&#20813;&#20102;&#24179;&#20961;&#35299;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.08164</link><description>&lt;p&gt;
DiME&#65306;&#36890;&#36807;&#29109;&#30697;&#38453;&#30340;&#24046;&#24322;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
DiME: Maximizing Mutual Information by a Difference of Matrix-Based Entropies. (arXiv:2301.08164v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DiME&#30340;&#20449;&#24687;&#29702;&#35770;&#37327;&#65292;&#21487;&#20197;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#36991;&#20813;&#20102;&#24179;&#20961;&#35299;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20449;&#24687;&#29702;&#35770;&#37327;&#65292;&#20855;&#26377;&#19982;&#30456;&#20114;&#20449;&#24687;&#31867;&#20284;&#30340;&#24615;&#36136;&#65292;&#24182;&#21487;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#28508;&#22312;&#20998;&#24067;&#36827;&#34892;&#26126;&#30830;&#20551;&#35774;&#12290;&#35813;&#25968;&#37327;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#30697;&#38453;&#30340;&#29109;&#65292;&#35813;&#29109;&#21033;&#29992;&#35268;&#33539;&#21270; Gram &#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#26469;&#35745;&#31639;&#20877;&#29983;&#26680; Hilbert &#31354;&#38388;&#20013;&#26410;&#38598;&#20013;&#21327;&#26041;&#24046;&#36816;&#31639;&#31526;&#30340;&#29305;&#24449;&#20540;&#30340;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#24046;&#24322;&#30697;&#38453;&#29109;&#65288;DiME&#65289;&#23545;&#20110;&#28041;&#21450;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30456;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#38750;&#24120;&#36866;&#29992;&#12290;&#34429;&#28982;&#35768;&#22810;&#27492;&#31867;&#20219;&#21153;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#24179;&#20961;&#35299;&#65292;&#20294; DiME &#33258;&#28982;&#20250;&#23545;&#36825;&#26679;&#30340;&#32467;&#26524;&#36827;&#34892;&#24809;&#32602;&#12290;&#25105;&#20204;&#23558; DiME &#19982;&#22810;&#20010;&#30456;&#20114;&#20449;&#24687;&#22522;&#20934;&#20272;&#35745;&#22120;&#22312;&#19968;&#20010;&#29609;&#20855;&#39640;&#26031;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102; DiME &#30340;&#29992;&#20363;&#31034;&#20363;&#65292;&#20363;&#22914;&#28508;&#22312;&#22240;&#23376;&#20998;&#35299;&#21644;&#22810;&#35270;&#22270;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013; DiME &#34987;&#29992;&#20110;&#23398;&#20064;&#35270;&#22270;&#20043;&#38388;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#20855;&#26377;&#39640;&#20114;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an information-theoretic quantity with similar properties to mutual information that can be estimated from data without making explicit assumptions on the underlying distribution. This quantity is based on a recently proposed matrix-based entropy that uses the eigenvalues of a normalized Gram matrix to compute an estimate of the eigenvalues of an uncentered covariance operator in a reproducing kernel Hilbert space. We show that a difference of matrix-based entropies (DiME) is well suited for problems involving the maximization of mutual information between random variables. While many methods for such tasks can lead to trivial solutions, DiME naturally penalizes such outcomes. We compare DiME to several baseline estimators of mutual information on a toy Gaussian dataset. We provide examples of use cases for DiME, such as latent factor disentanglement and a multiview representation learning problem where DiME is used to learn a shared representation among views with high mu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; ODoS &#28388;&#27874;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#26354;&#32447;&#32467;&#26500;&#20998;&#21106;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20013;&#26354;&#32447;&#23545;&#35937;&#30340;&#33258;&#21160;&#20998;&#21106;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.07475</link><description>&lt;p&gt;
&#22522;&#20110; ODoS &#28388;&#27874;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#21307;&#23398;&#22270;&#20687;&#26354;&#32447;&#23545;&#35937;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Curvilinear object segmentation in medical images based on ODoS filter and deep learning network. (arXiv:2301.07475v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07475
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; ODoS &#28388;&#27874;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#26354;&#32447;&#32467;&#26500;&#20998;&#21106;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20013;&#26354;&#32447;&#23545;&#35937;&#30340;&#33258;&#21160;&#20998;&#21106;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#65292;&#26354;&#32447;&#23545;&#35937;&#30340;&#33258;&#21160;&#20998;&#21106;&#23545;&#20110;&#20154;&#31867;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#35780;&#20272;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#22270;&#20687;&#22806;&#35266;&#65292;&#26354;&#32447;&#23545;&#35937;&#19982;&#20854;&#21608;&#22260;&#32972;&#26223;&#20043;&#38388;&#30340;&#20302;&#23545;&#27604;&#24230;&#65292;&#34180;&#32780;&#19981;&#22343;&#21248;&#30340;&#26354;&#32447;&#32467;&#26500;&#20197;&#21450;&#19981;&#36866;&#24403;&#30340;&#32972;&#26223;&#20809;&#29031;&#26465;&#20214;&#31561;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;ODoS&#28388;&#27874;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#29420;&#29305;&#26354;&#32447;&#32467;&#26500;&#20998;&#21106;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#36827;&#34892;&#26354;&#32447;&#23545;&#35937;&#20998;&#21106;&#12290;&#30446;&#21069;&#65292;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24378;&#35843;&#24320;&#21457;&#28145;&#24230;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#24573;&#30053;&#25429;&#33719;&#26354;&#32447;&#23545;&#35937;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;ODoS&#28388;&#27874;&#22120;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#19968;&#37096;&#20998;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#31354;&#38388;&#20449;&#24687;&#30340;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic segmentation of curvilinear objects in medical images plays an important role in the diagnosis and evaluation of human diseases, yet it is a challenging uncertainty in the complex segmentation tasks due to different issues such as various image appearances, low contrast between curvilinear objects and their surrounding backgrounds, thin and uneven curvilinear structures, and improper background illumination conditions. To overcome these challenges, we present a unique curvilinear structure segmentation framework based on an oriented derivative of stick (ODoS) filter and a deep learning network for curvilinear object segmentation in medical images. Currently, a large number of deep learning models emphasize developing deep architectures and ignore capturing the structural features of curvilinear objects, which may lead to unsatisfactory results. Consequently, a new approach that incorporates an ODoS filter as part of a deep learning network is presented to improve the spatial 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;Boltzmann&#23494;&#24230;&#21464;&#24418;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#25554;&#20540;&#33021;&#37327;&#20989;&#25968;&#31561;&#23454;&#29616;Boltzmann&#23494;&#24230;&#30340;&#21464;&#24418;&#65292;&#28982;&#21518;&#25214;&#21040;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#21521;&#37327;&#22330;&#65292;&#23558;&#26679;&#26412;&#20174;&#19968;&#20010;&#20998;&#24067;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#65292;&#20854;&#34920;&#29616;&#22312;&#39640;&#26031;&#28151;&#21512;&#21644;&#37327;&#23376;&#21147;&#23398;&#31890;&#23376;&#30340;Boltzmann&#23494;&#24230;&#19978;&#27604;KL-&#21453;&#25955;&#24230;&#26356;&#20855;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2301.07388</link><description>&lt;p&gt;
&#23398;&#20064;Boltzmann&#23494;&#24230;&#30340;&#21464;&#24418;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Learning Deformation Trajectories of Boltzmann Densities. (arXiv:2301.07388v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;Boltzmann&#23494;&#24230;&#21464;&#24418;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#25554;&#20540;&#33021;&#37327;&#20989;&#25968;&#31561;&#23454;&#29616;Boltzmann&#23494;&#24230;&#30340;&#21464;&#24418;&#65292;&#28982;&#21518;&#25214;&#21040;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#21521;&#37327;&#22330;&#65292;&#23558;&#26679;&#26412;&#20174;&#19968;&#20010;&#20998;&#24067;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#65292;&#20854;&#34920;&#29616;&#22312;&#39640;&#26031;&#28151;&#21512;&#21644;&#37327;&#23376;&#21147;&#23398;&#31890;&#23376;&#30340;Boltzmann&#23494;&#24230;&#19978;&#27604;KL-&#21453;&#25955;&#24230;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26679;&#26412;&#20294;&#23384;&#22312;&#33021;&#37327;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#33021;&#37327;&#20989;&#25968;$f_1$&#21644;&#24191;&#20041;&#39640;&#26031;&#20989;&#25968;$f_0$&#20043;&#38388;&#30340;&#39044;&#23450;&#25110;&#23398;&#20064;&#25554;&#20540;$f_t$&#12290;&#33021;&#37327;&#20989;&#25968;&#30340;&#25554;&#20540;&#24341;&#36215;Boltzmann&#23494;&#24230;$p_t\propto e^{-f_t}$&#30340;&#25554;&#20540;&#65292;&#25105;&#20204;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#27839;&#30528;&#26063;$p_t$&#30340;&#26102;&#38388;&#20381;&#36182;&#21521;&#37327;&#22330;$V_t$&#65292;&#23558;&#26679;&#26412;&#20174;&#19968;&#20010;&#20998;&#24067;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#12290;&#23558;&#26679;&#26412;&#27839;&#30528;&#26063;$p_t$&#20174;&#19968;&#20010;&#20998;&#24067;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#30340;&#26465;&#20214;&#21487;&#20197;&#36716;&#21270;&#20026;$V_t$&#21644;$f_t$&#20043;&#38388;&#30340;PDE&#65292;&#25105;&#20204;&#20248;&#21270;$V_t$&#21644;$f_t$&#20197;&#28385;&#36275;&#27492;PDE&#12290;&#25105;&#20204;&#22312;&#39640;&#26031;&#28151;&#21512;&#21644;&#21452;&#20117;&#21183;&#30340;&#37327;&#23376;&#21147;&#23398;&#31890;&#23376;&#30340;Boltzmann&#23494;&#24230;&#19978;&#23454;&#39564;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#30446;&#26631;&#19982;KL-&#21453;&#25955;&#24230;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a training objective for continuous normalizing flows that can be used in the absence of samples but in the presence of an energy function. Our method relies on either a prescribed or a learnt interpolation $f_t$ of energy functions between the target energy $f_1$ and the energy function of a generalized Gaussian $f_0(x) = ||x/\sigma||_p^p$. The interpolation of energy functions induces an interpolation of Boltzmann densities $p_t \propto e^{-f_t}$ and we aim to find a time-dependent vector field $V_t$ that transports samples along the family $p_t$ of densities. The condition of transporting samples along the family $p_t$ can be translated to a PDE between $V_t$ and $f_t$ and we optimize $V_t$ and $f_t$ to satisfy this PDE. We experimentally compare the proposed training objective to the reverse KL-divergence on Gaussian mixtures and on the Boltzmann density of a quantum mechanical particle in a double-well potential.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22343;&#22330;&#25511;&#21046;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#21363;&#20351;&#26234;&#33021;&#20307;&#20849;&#20139;&#19968;&#20010;&#38750;&#21487;&#20998;&#20840;&#23616;&#29366;&#24577;&#65292;&#20063;&#33021;&#20855;&#26377;&#36739;&#22909;&#30340;&#36866;&#29992;&#24615;&#21644;&#36817;&#20284;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.06889</link><description>&lt;p&gt;
&#22522;&#20110;&#22343;&#22330;&#25511;&#21046;&#30340;&#38750;&#21487;&#20998;&#20849;&#20139;&#20840;&#23616;&#29366;&#24577;&#19979;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State. (arXiv:2301.06889v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22343;&#22330;&#25511;&#21046;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#21363;&#20351;&#26234;&#33021;&#20307;&#20849;&#20139;&#19968;&#20010;&#38750;&#21487;&#20998;&#20840;&#23616;&#29366;&#24577;&#65292;&#20063;&#33021;&#20855;&#26377;&#36739;&#22909;&#30340;&#36866;&#29992;&#24615;&#21644;&#36817;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#22330;&#25511;&#21046;&#26159;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#24378;&#22823;&#30340;&#36817;&#20284;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22343;&#22330;&#25511;&#21046;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#22312;&#32473;&#23450;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#23616;&#37096;&#29366;&#24577;&#21644;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#30340;&#19979;&#19968;&#20010;&#65288;&#23616;&#37096;&#65289;&#29366;&#24577;&#20250;&#20114;&#30456;&#29420;&#31435;&#22320;&#28436;&#21464;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#26234;&#33021;&#20307;&#20849;&#20139;&#19968;&#20010;&#20840;&#23616;&#29366;&#24577;&#30340;MARL&#22330;&#26223;&#20013;&#65292;MFC&#20173;&#28982;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#22909;&#30340;&#36817;&#20284;&#24037;&#20855;&#65292;&#21069;&#25552;&#26159;&#26234;&#33021;&#20307;&#30340;&#23616;&#37096;&#29366;&#24577;&#20173;&#20855;&#26377;&#26465;&#20214;&#29420;&#31435;&#24615;&#12290;&#25105;&#20204;&#20551;&#35774;&#20840;&#23616;&#29366;&#24577;&#26159;&#38750;&#21487;&#20998;&#30340;&#65292;&#21363;&#19981;&#33021;&#23558;&#23427;&#34920;&#31034;&#20026;&#26234;&#33021;&#20307;&#30340;&#23616;&#37096;&#29366;&#24577;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#23558;&#36817;&#20284;&#35823;&#24046;&#35745;&#31639;&#20026;$\mathcal{O}(e)$&#65292;&#20854;&#20013;$e=\frac{1}{\sqrt{N}}\left[\sqrt{|\mathcal{X}|} +\sqrt{|\mathcal{U}|}\right]$&#65292;&#20195;&#34920;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#26415;&#35821;&#20026;$N$&#65292;$|\mathcal{X}|, |\mathcal{U}|$ &#34920;&#31034;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mean Field Control (MFC) is a powerful approximation tool to solve large-scale Multi-Agent Reinforcement Learning (MARL) problems. However, the success of MFC relies on the presumption that given the local states and actions of all the agents, the next (local) states of the agents evolve conditionally independent of each other. Here we demonstrate that even in a MARL setting where agents share a common global state in addition to their local states evolving conditionally independently (thus introducing a correlation between the state transition processes of individual agents), the MFC can still be applied as a good approximation tool. The global state is assumed to be non-decomposable i.e., it cannot be expressed as a collection of local states of the agents. We compute the approximation error as $\mathcal{O}(e)$ where $e=\frac{1}{\sqrt{N}}\left[\sqrt{|\mathcal{X}|} +\sqrt{|\mathcal{U}|}\right]$. The size of the agent population is denoted by the term $N$, and $|\mathcal{X}|, |\mathcal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#26696;&#26469;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#30340;&#36825;&#20123;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#27169;&#22411;&#30340;&#35821;&#29992;&#33021;&#21147;&#20173;&#38656;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2301.05149</link><description>&lt;p&gt;
&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#25913;&#36827;&#22522;&#20110;&#20219;&#21153;&#30340;&#35748;&#30693;&#33021;&#21147;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models. (arXiv:2301.05149v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#26696;&#26469;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#30340;&#36825;&#20123;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#27169;&#22411;&#30340;&#35821;&#29992;&#33021;&#21147;&#20173;&#38656;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#24037;&#20316;&#36890;&#36807;&#20026;&#20154;&#31867;&#35774;&#35745;&#30340;&#24515;&#29702;&#27979;&#35797;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#23613;&#31649;&#36825;&#20123;&#30740;&#31350;&#26377;&#21161;&#20110;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#33324;&#33021;&#21147;&#65292;&#20294;&#24182;&#19981;&#33021;&#20445;&#35777;&#19968;&#20010;&#25317;&#26377;&#36275;&#22815;&#33021;&#21147;&#36890;&#36807;&#36825;&#20123;&#27979;&#35797;&#30340;&#27169;&#22411;&#23454;&#38469;&#19978;&#20250;&#22312;&#25191;&#34892;&#23454;&#38469;&#20219;&#21153;&#26102;&#20351;&#29992;&#36825;&#20123;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#20154;&#31867;&#24335;&#35748;&#30693;&#33021;&#21147;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#26469;&#25191;&#34892;&#20219;&#21153;&#12290;&#36825;&#20123;&#33021;&#21147;&#21253;&#25324;&#65306;(i) &#24555;&#36895;&#29983;&#25104;&#33391;&#22909;&#30340;&#20505;&#36873;&#35805;&#35821;&#30340;&#33021;&#21147; (&#25628;&#32034;&#33021;&#21147;)&#65307;(ii) &#39044;&#27979;&#21548;&#32773;&#22914;&#20309;&#29702;&#35299;&#36825;&#20123;&#35805;&#35821;&#65292;&#24182;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35805;&#35821; (&#35821;&#29992;&#33021;&#21147;)&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#26041;&#26696;&#65292;&#20197;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#36825;&#20123;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#27492;&#26041;&#26696;&#24212;&#29992;&#20110;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#38382;&#39064;&#20013;&#30340;&#21508;&#31181;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35821;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work studies the cognitive capabilities of language models through psychological tests designed for humans. While these studies are helpful for understanding the general capabilities of these models, there is no guarantee that a model possessing sufficient capabilities to pass those tests would actually use those capabilities in performing real-life tasks. In this work, we formulate task-oriented cognitive capabilities, which are human-like cognitive capabilities that language models leverage to perform tasks. These capabilities are (i) the ability to quickly generate good candidate utterances (the search capability) (ii) the ability to predict how a listener interprets those utterances and choose the most appropriate one (the pragmatic capability). We design an evaluation scheme for comparing these capabilities of a language model with those of a human. Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic ca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26032;&#39062;&#35299;&#37322;&#25216;&#26415;&#65306;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#65292;&#36890;&#36807;&#36319;&#36394;&#27169;&#22411;&#39044;&#27979;&#19982;&#21487;&#29992;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#20851;&#31995;&#26469;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#26377;&#21033;&#20110;&#30740;&#31350;&#36828;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.14815</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#31350;&#40657;&#21283;&#23376;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Black-box language model explanation by context length probing. (arXiv:2212.14815v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14815
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26032;&#39062;&#35299;&#37322;&#25216;&#26415;&#65306;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#65292;&#36890;&#36807;&#36319;&#36394;&#27169;&#22411;&#39044;&#27979;&#19982;&#21487;&#29992;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#20851;&#31995;&#26469;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#26377;&#21033;&#20110;&#30740;&#31350;&#36828;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#37319;&#29992;&#24378;&#35843;&#20102;&#25913;&#21892;&#20854;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#25216;&#26415;&#65306;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#65292;&#23427;&#22522;&#20110;&#36319;&#36394;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#21487;&#29992;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#20989;&#25968;&#65292;&#24182;&#20801;&#35768;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#35813;&#25216;&#26415;&#26159;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#65292;&#19981;&#20381;&#36182;&#20110;&#38500;&#35745;&#31639;token&#32423;&#27010;&#29575;&#20043;&#22806;&#30340;&#27169;&#22411;&#20869;&#37096;&#35775;&#38382;&#12290;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#24212;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#21021;&#22987;&#30340;&#20998;&#26512;&#21644;&#35265;&#35299;&#65292;&#21253;&#25324;&#30740;&#31350;&#36828;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#28508;&#21147;&#12290;&#26041;&#27861;&#30340;&#28304;&#20195;&#30721;&#21644;&#20132;&#20114;&#24335;&#28436;&#31034;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly widespread adoption of large language models has highlighted the need for improving their explainability. We present context length probing, a novel explanation technique for causal language models, based on tracking the predictions of a model as a function of the length of available context, and allowing to assign differential importance scores to different contexts. The technique is model-agnostic and does not rely on access to model internals beyond computing token-level probabilities. We apply context length probing to large pre-trained language models and offer some initial analyses and insights, including the potential for studying long-range dependencies. The source code and an interactive demo of the method are available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#27010;&#29575;&#30475;&#20316;&#30456;&#23545;&#24230;&#37327;&#30340;&#35266;&#28857;&#65292;&#24314;&#31435;&#20102;&#26377;&#38480;&#32467;&#26524;&#31354;&#38388;&#19978;&#30456;&#23545;&#27010;&#29575;&#20989;&#25968;&#30340;&#20844;&#29702;&#21270;&#65292;&#25552;&#20379;&#20102;&#20854;&#23454;&#20363;&#21644;&#32452;&#21512;&#31995;&#32479;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#23545;&#36125;&#21494;&#26031;&#25512;&#26029;&#21450;&#20854;&#25968;&#23383;&#23454;&#29616;&#65292;&#35777;&#26126;&#20102;&#30456;&#23545;&#27010;&#29575;&#31354;&#38388;&#30340;&#25299;&#25169;&#38381;&#21253;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#26497;&#38480;&#19979;&#20445;&#30041;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.14555</link><description>&lt;p&gt;
&#26377;&#38480;&#32467;&#26524;&#31354;&#38388;&#19978;&#30340;&#30456;&#23545;&#27010;&#29575;&#65306;&#20854;&#20844;&#29702;&#21270;&#12289;&#23646;&#24615;&#21644;&#24212;&#29992;&#30340;&#31995;&#32479;&#32771;&#23519;
&lt;/p&gt;
&lt;p&gt;
Relative Probability on Finite Outcome Spaces: A Systematic Examination of its Axiomatization, Properties, and Applications. (arXiv:2212.14555v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#27010;&#29575;&#30475;&#20316;&#30456;&#23545;&#24230;&#37327;&#30340;&#35266;&#28857;&#65292;&#24314;&#31435;&#20102;&#26377;&#38480;&#32467;&#26524;&#31354;&#38388;&#19978;&#30456;&#23545;&#27010;&#29575;&#20989;&#25968;&#30340;&#20844;&#29702;&#21270;&#65292;&#25552;&#20379;&#20102;&#20854;&#23454;&#20363;&#21644;&#32452;&#21512;&#31995;&#32479;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#23545;&#36125;&#21494;&#26031;&#25512;&#26029;&#21450;&#20854;&#25968;&#23383;&#23454;&#29616;&#65292;&#35777;&#26126;&#20102;&#30456;&#23545;&#27010;&#29575;&#31354;&#38388;&#30340;&#25299;&#25169;&#38381;&#21253;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#26497;&#38480;&#19979;&#20445;&#30041;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#27010;&#29575;&#30475;&#20316;&#30456;&#23545;&#24230;&#37327;&#32780;&#38750;&#32477;&#23545;&#24230;&#37327;&#30340;&#35266;&#28857;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#27010;&#24565;&#65292;&#25105;&#20204;&#23558;&#28966;&#28857;&#25918;&#22312;&#26377;&#38480;&#32467;&#26524;&#31354;&#38388;&#19978;&#65292;&#24182;&#24314;&#31435;&#20102;&#19977;&#20010;&#22522;&#26412;&#20844;&#29702;&#65292;&#20197;&#30830;&#31435;&#30456;&#23545;&#27010;&#29575;&#20989;&#25968;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#36825;&#20123;&#20989;&#25968;&#30340;&#23454;&#20363;&#24211;&#21644;&#19968;&#20010;&#32452;&#21512;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30456;&#23545;&#36125;&#21494;&#26031;&#25512;&#26029;&#21450;&#20854;&#25968;&#23383;&#23454;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#23545;&#27010;&#29575;&#31354;&#38388;&#30340;&#25299;&#25169;&#38381;&#21253;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#26497;&#38480;&#19979;&#20445;&#30041;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a view of probability as a relative measure rather than an absolute one. To demonstrate this concept, we focus on finite outcome spaces and develop three fundamental axioms that establish requirements for relative probability functions. We then provide a library of examples of these functions and a system for composing them. Additionally, we discuss a relative version of Bayesian inference and its digital implementation. Finally, we prove the topological closure of the relative probability space, highlighting its ability to preserve information under limits.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#24179;&#28369;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#27861; (DSGDA)&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#65292;&#24182;&#19988;&#33021;&#22815;&#20840;&#23616;&#25910;&#25947;&#24182;&#28040;&#38500;&#26497;&#38480;&#29615;&#12290;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;DSGDA &#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#36798;&#21040;&#20102;&#25991;&#29486;&#20013;&#21333;&#24490;&#29615;&#31639;&#27861;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.12978</link><description>&lt;p&gt;
&#21452;&#37325;&#24179;&#28369;GDA&#65306;&#29992;&#20110;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#30340;&#20840;&#23616;&#25910;&#25947;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Doubly Smoothed GDA: Global Convergent Algorithm for Constrained Nonconvex-Nonconcave Minimax Optimization. (arXiv:2212.12978v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#24179;&#28369;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#27861; (DSGDA)&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#65292;&#24182;&#19988;&#33021;&#22815;&#20840;&#23616;&#25910;&#25947;&#24182;&#28040;&#38500;&#26497;&#38480;&#29615;&#12290;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;DSGDA &#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#36798;&#21040;&#20102;&#25991;&#29486;&#20013;&#21333;&#24490;&#29615;&#31639;&#27861;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#19981;&#33021;&#20445;&#35777;&#20840;&#23616;&#25910;&#25947;&#65292;&#29978;&#33267;&#20250;&#36973;&#21463;&#26497;&#38480;&#29615;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#24490;&#29615;&#31639;&#27861;&#65292;&#31216;&#20026;&#21452;&#37325;&#24179;&#28369;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#27861; (DSGDA)&#65292;&#23427;&#33021;&#22815;&#33258;&#28982;&#22320;&#24179;&#34913;&#21407;&#22987;&#19982;&#23545;&#20598;&#26356;&#26032;&#65292;&#24182;&#19988;&#23558;&#26497;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#20984;-&#38750;&#20985;&#20363;&#23376;&#20013;&#30340;&#26497;&#38480;&#29615;&#28040;&#38500;&#65292;&#21253;&#25324; Forsaken&#65292;Bilinearly-coupled minimax&#65292;Sixth-order polynomial &#21644; PolarGame&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#22312;&#19968;&#20010;&#21333;&#20391;&#30340; $\theta\in(0,1)$ Kurdyka-\L{}ojasiewicz&#26465;&#20214;&#65288;&#25110;&#20984;&#21407;&#22987;/&#20985;&#23545;&#20598;&#20989;&#25968;&#65289;&#19979;&#65292;DSGDA &#21487;&#20197;&#25214;&#21040;&#19968;&#20010;&#28216;&#25103;&#24179;&#34913;&#28857;&#65292;&#24182;&#19988;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230; $\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$&#65288;&#25110; $\mathcal{O}(\epsilon^{-4})$&#65289;&#65292;&#36825;&#20123;&#19982;&#25991;&#29486;&#20013;&#21333;&#24490;&#29615;&#31639;&#27861;&#30340;&#26368;&#20339;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonconvex-nonconcave minimax optimization has received intense attention over the last decade due to its broad applications in machine learning. Unfortunately, most existing algorithms cannot be guaranteed to converge globally and even suffer from limit cycles. To address this issue, we propose a novel single-loop algorithm called doubly smoothed gradient descent ascent method (DSGDA), which naturally balances the primal and dual updates. The proposed DSGDA can get rid of limit cycles in various challenging nonconvex-nonconcave examples in the literature, including Forsaken, Bilinearly-coupled minimax, Sixth-order polynomial, and PolarGame. We further show that under an one-sided Kurdyka-\L{}ojasiewicz condition with exponent $\theta\in(0,1)$ (resp. convex primal/concave dual function), DSGDA can find a game-stationary point with an iteration complexity of $\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$ (resp. $\mathcal{O}(\epsilon^{-4})$). These match the best results for single-loop al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10511</link><description>&lt;p&gt;
&#20309;&#26102;&#19981;&#20449;&#20219;&#35821;&#35328;&#27169;&#22411;&#65306;&#25506;&#32034;&#21442;&#25968;&#21644;&#38750;&#21442;&#25968;&#35760;&#24518;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories. (arXiv:2212.10511v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#36825;&#26263;&#31034;&#20102;&#20165;&#20381;&#38752;&#20854;&#21442;&#25968;&#26469;&#32534;&#30721;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#22312;PopQA&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#30693;&#35782;&#25506;&#27979;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#20107;&#23454;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#38271;&#23614;&#20013;&#65292;&#25193;&#23637;&#35268;&#27169;&#26080;&#27861;&#26126;&#26174;&#25913;&#21892;&#35760;&#24518;&#23454;&#38469;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32988;&#36807;&#32423;&#21035;&#22823;&#24471;&#22810;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26410;&#32463;&#21327;&#21161;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#28041;&#21450;&#39640;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#19978;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#22312;&#38656;&#35201;&#26102;&#26816;&#32034;&#38750;&#21442;&#25968;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only whe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;BLOOM&#27169;&#22411;&#20013;&#24212;&#29992;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#65292;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#35821;&#35328;&#19978;&#65292;&#24182;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#25552;&#31034;&#24615;&#33021;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;</title><link>http://arxiv.org/abs/2212.09535</link><description>&lt;p&gt;
BLOOM+1&#65306;&#20026;&#38646;&#26679;&#26412;&#25552;&#31034;&#28155;&#21152;&#35821;&#35328;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting. (arXiv:2212.09535v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;BLOOM&#27169;&#22411;&#20013;&#24212;&#29992;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#65292;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#35821;&#35328;&#19978;&#65292;&#24182;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#25552;&#31034;&#24615;&#33021;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLOOM&#27169;&#22411;&#26159;&#19968;&#20010;&#22823;&#22411;&#20844;&#24320;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#20854;&#39044;&#35757;&#32451;&#20165;&#38480;&#20110;46&#31181;&#35821;&#35328;&#12290;&#20026;&#20102;&#23558;BLOOM&#30340;&#22909;&#22788;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#36807;&#39640;&#30340;&#25104;&#26412;&#65292;&#26377;&#24517;&#35201;&#23558;BLOOM&#36866;&#24212;&#21040;&#26032;&#30340;&#35821;&#35328;&#19978;&#12290;&#26412;&#25991;&#23558;&#29616;&#26377;&#30340;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#24212;&#29992;&#20110;BLOOM&#65292;&#24182;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#23545;&#20854;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#36866;&#24212;&#23545;&#20110;&#25552;&#39640;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#26159;&#26377;&#25928;&#30340;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#24615;&#33021;&#19981;&#20250;&#21463;&#21040;&#35821;&#35328;&#29305;&#23450;&#24615;&#30340;&#26174;&#30528;&#24433;&#21709;&#65292;&#22914;&#20070;&#20889;&#31995;&#32479;&#12290;&#23427;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;&#25105;&#20204;&#36824;&#21521;BLOOMZ&#28155;&#21152;&#20102;&#26032;&#35821;&#35328;&#65292;&#36825;&#26159;BLOOM&#30340;&#22810;&#20219;&#21153;&#24494;&#35843;&#29256;&#26412;&#65292;&#33021;&#22815;&#36319;&#38543;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24352;&#37327;&#20998;&#35299;&#30340;&#30693;&#35782;&#22270;&#32534;&#30721;&#22120;&#65292;&#23558;&#37051;&#23621;&#23454;&#20307;&#20351;&#29992;&#30001;&#20851;&#31995;&#31867;&#22411;&#23450;&#20041;&#30340;&#20302;&#31209;&#24352;&#37327;&#30340;&#25237;&#24433;&#30697;&#38453;&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#20135;&#29983;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#20851;&#31995;&#24863;&#30693;&#24615;&#30340;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.05581</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#39640;&#25928;&#30340;&#20851;&#31995;&#24863;&#30693;&#37051;&#22495;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Efficient Relation-aware Neighborhood Aggregation in Graph Neural Networks via Tensor Decomposition. (arXiv:2212.05581v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24352;&#37327;&#20998;&#35299;&#30340;&#30693;&#35782;&#22270;&#32534;&#30721;&#22120;&#65292;&#23558;&#37051;&#23621;&#23454;&#20307;&#20351;&#29992;&#30001;&#20851;&#31995;&#31867;&#22411;&#23450;&#20041;&#30340;&#20302;&#31209;&#24352;&#37327;&#30340;&#25237;&#24433;&#30697;&#38453;&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#20135;&#29983;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#20851;&#31995;&#24863;&#30693;&#24615;&#30340;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#38754;&#21521;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#36825;&#31181;&#26041;&#27861;&#24573;&#30053;&#20102;&#20851;&#31995;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#23558;&#20854;&#19982;&#23454;&#20307;&#20449;&#24687;&#32452;&#21512;&#20351;&#29992;&#25928;&#29575;&#20302;&#19979;&#65292;&#23548;&#33268;&#34920;&#36798;&#33021;&#21147;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#20851;&#31995;&#22270;&#21367;&#31215;&#32593;&#32476;(R-GCN)&#30340;&#32858;&#21512;&#20989;&#25968;&#20013;&#24341;&#20837;&#20102;&#24352;&#37327;&#20998;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#30693;&#35782;&#22270;&#32534;&#30721;&#22120;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#30001;&#20851;&#31995;&#31867;&#22411;&#23450;&#20041;&#30340;&#20302;&#31209;&#24352;&#37327;&#30340;&#25237;&#24433;&#30697;&#38453;&#65292;&#23558;&#37051;&#23621;&#23454;&#20307;&#36827;&#34892;&#36716;&#25442;&#65292;&#20197;&#33719;&#24471;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#20851;&#31995;&#24863;&#30693;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;CP&#20998;&#35299;&#26469;&#20272;&#35745;&#26680;&#24515;&#24352;&#37327;&#30340;&#20302;&#31209;&#20272;&#35745;&#65292;&#20174;&#32780;&#21387;&#32553;&#21644;&#35268;&#33539;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22522;&#20110;1-N&#26041;&#27861;&#22312;&#22823;&#22411;&#22270;&#19978;&#30340;&#35757;&#32451;&#38480;&#21046;&#12290;&#25105;&#20204;&#20351;&#29992;&#20302;&#32500;&#24230;&#23884;&#20837;&#65292;&#22312;FB15k-237&#21644;WN18RR&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many Graph Neural Networks (GNNs) are proposed for Knowledge Graph Embedding (KGE). However, lots of these methods neglect the importance of the information of relations and combine it with the information of entities inefficiently, leading to low expressiveness. To address this issue, we introduce a general knowledge graph encoder incorporating tensor decomposition in the aggregation function of Relational Graph Convolutional Network (R-GCN). In our model, neighbor entities are transformed using projection matrices of a low-rank tensor which are defined by relation types to benefit from multi-task learning and produce expressive relation-aware representations. Besides, we propose a low-rank estimation of the core tensor using CP decomposition to compress and regularize our model. We use a training method inspired by contrastive learning, which relieves the training limitation of the 1-N method on huge graphs. We achieve favorably competitive results on FB15k-237 and WN18RR with embedd
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;ISAACS&#65292;&#36890;&#36807;&#23558;&#21338;&#24328;&#35770;&#23433;&#20840;&#20998;&#26512;&#19982;&#23545;&#25239;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20351;&#26426;&#22120;&#20154;&#31995;&#32479;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#40065;&#26834;&#23433;&#20840;&#25511;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#36991;&#20813;&#30896;&#25758;&#65292;&#24182;&#22312;&#23433;&#20840;&#38382;&#39064;&#19978;&#36229;&#36234;&#26631;&#20934;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.03228</link><description>&lt;p&gt;
ISAACS&#65306;&#23433;&#20840;&#30340;&#36845;&#20195;&#36719;&#23545;&#25239; Actor-Critic &#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ISAACS: Iterative Soft Adversarial Actor-Critic for Safety. (arXiv:2212.03228v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;ISAACS&#65292;&#36890;&#36807;&#23558;&#21338;&#24328;&#35770;&#23433;&#20840;&#20998;&#26512;&#19982;&#23545;&#25239;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20351;&#26426;&#22120;&#20154;&#31995;&#32479;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#40065;&#26834;&#23433;&#20840;&#25511;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#36991;&#20813;&#30896;&#25758;&#65292;&#24182;&#22312;&#23433;&#20840;&#38382;&#39064;&#19978;&#36229;&#36234;&#26631;&#20934;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#20013;&#37096;&#32626;&#26426;&#22120;&#20154;&#38656;&#35201;&#23427;&#20204;&#33021;&#22815;&#22312;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#24773;&#20917;&#19979;&#31283;&#20581;&#22320;&#36816;&#34892;&#65292;&#20363;&#22914;&#19981;&#35268;&#21017;&#30340;&#22320;&#24418;&#21644;&#39118;&#21147;&#26465;&#20214;&#12290;&#30001;&#20110;&#20248;&#21270;&#25511;&#21046;&#29702;&#35770;&#30340;&#20005;&#26684;&#23433;&#20840;&#26694;&#26550;&#38590;&#20197;&#25193;&#23637;&#21040;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65292;&#32780;&#26356;&#26131;&#22788;&#29702;&#30340;&#8220;&#28145;&#24230;&#8221;&#26041;&#27861;&#35745;&#31639;&#20986;&#30340;&#25511;&#21046;&#31574;&#30053;&#32570;&#20047;&#20445;&#35777;&#65292;&#24182;&#19988;&#24448;&#24448;&#22312;&#19981;&#30830;&#23450;&#30340;&#25805;&#20316;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#24456;&#23569;&#30340;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#21338;&#24328;&#35770;&#23433;&#20840;&#20998;&#26512;&#19982;&#20223;&#30495;&#20013;&#30340;&#23545;&#25239;&#24335;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20351;&#20855;&#26377;&#19968;&#33324;&#38750;&#32447;&#24615;&#21160;&#24577;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#40065;&#26834;&#23433;&#20840;&#25511;&#21046;&#22120;&#30340;&#32508;&#21512;&#21512;&#25104;&#65292;&#21463;&#21040;&#26377;&#30028;&#24314;&#27169;&#35823;&#24046;&#30340;&#38480;&#21046;&#12290;&#37319;&#29992;&#36719;&#24615; actor-critic &#26041;&#27861;&#65292;&#21516;&#26102;&#36827;&#34892;&#19968;&#20010;&#23433;&#20840;&#30340;&#22238;&#36864;&#31574;&#30053;&#21644;&#19968;&#20010;&#31216;&#20026;&#8220;&#24178;&#25200;&#8221;&#30340;&#23545;&#25239;&#26234;&#33021;&#20307;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#35813;&#23545;&#25239;&#26234;&#33021;&#20307;&#33268;&#21147;&#20110;&#21796;&#36215;&#35774;&#35745;&#32773;&#19981;&#30830;&#23450;&#24615;&#20551;&#35774;&#19979;&#20801;&#35768;&#30340;&#27169;&#22411;&#35823;&#24046;&#21644;&#35757;&#32451;-&#37096;&#32626;&#20559;&#24046;&#30340;&#26368;&#22351;&#24773;&#20917;&#23454;&#29616;&#12290;&#21516;&#26102;&#65292;&#19968;&#20010;&#35780;&#20215;&#32593;&#32476;&#34987;&#35757;&#32451;&#26469;&#35780;&#20272;&#20027;&#31574;&#30053;&#21644;&#22238;&#36864;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#65292;&#25552;&#20379;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#36719;&#32422;&#26463;&#26469;&#25351;&#23548;&#25506;&#32034;&#21644;&#38480;&#21046;&#19981;&#33391;&#34892;&#20026;&#12290;&#22312;&#21508;&#31181;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36991;&#20813;&#30896;&#25758;&#30340;&#31574;&#30053;&#65292;&#24182;&#36229;&#36234;&#20102;&#26631;&#20934;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#23433;&#20840;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#21363;&#20351;&#23384;&#22312;&#26174;&#33879;&#30340;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of robots in uncontrolled environments requires them to operate robustly under previously unseen scenarios, like irregular terrain and wind conditions. Unfortunately, while rigorous safety frameworks from robust optimal control theory scale poorly to high-dimensional nonlinear dynamics, control policies computed by more tractable "deep" methods lack guarantees and tend to exhibit little robustness to uncertain operating conditions. This work introduces a novel approach enabling scalable synthesis of robust safety-preserving controllers for robotic systems with general nonlinear dynamics subject to bounded modeling error by combining game-theoretic safety analysis with adversarial reinforcement learning in simulation. Following a soft actor-critic scheme, a safety-seeking fallback policy is co-trained with an adversarial "disturbance" agent that aims to invoke the worst-case realization of model error and training-to-deployment discrepancy allowed by the designer's uncert
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#19979;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#20998;&#26512;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#23545;&#20934;&#30830;&#24615;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#31283;&#23450;&#20998;&#24067;&#30340;&#19968;&#33324;&#23478;&#26063;&#20013;&#20063;&#23384;&#22312;&#31867;&#20284;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2211.15762</link><description>&lt;p&gt;
&#29702;&#35299;&#23545;&#20934;&#30830;&#24615;&#19981;&#24179;&#34913;&#24433;&#21709;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding the Impact of Adversarial Robustness on Accuracy Disparity. (arXiv:2211.15762v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#19979;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#20998;&#26512;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#23545;&#20934;&#30830;&#24615;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#31283;&#23450;&#20998;&#24067;&#30340;&#19968;&#33324;&#23478;&#26063;&#20013;&#20063;&#23384;&#22312;&#31867;&#20284;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38271;&#26399;&#20197;&#26469;&#24050;&#32463;&#20174;&#32463;&#39564;&#19978;&#35266;&#23519;&#21040;&#23545;&#25239;&#40065;&#26834;&#24615;&#21487;&#33021;&#19982;&#26631;&#20934;&#20934;&#30830;&#24615;&#23384;&#22312;&#19968;&#20123;&#30683;&#30462;&#65292;&#24182;&#19988;&#21487;&#33021;&#23545;&#19981;&#21516;&#31867;&#21035;&#20135;&#29983;&#19981;&#24179;&#31561;&#24433;&#21709;&#65292;&#20294;&#23427;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#21363;&#36825;&#20123;&#35266;&#23519;&#26377;&#22810;&#22823;&#31243;&#24230;&#30340;&#20445;&#25345;&#65292;&#20197;&#21450;&#31867;&#21035;&#19981;&#24179;&#34913;&#22312;&#20854;&#20013;&#25198;&#28436;&#20160;&#20040;&#26679;&#30340;&#35282;&#33394;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#19979;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#26469;&#29702;&#35299;&#36825;&#20010;&#20934;&#30830;&#24615;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#20998;&#35299;&#25104;&#20004;&#37096;&#20998;&#65306;&#19968;&#37096;&#20998;&#26159;&#22240;&#20026;&#40065;&#26834;&#24615;&#32422;&#26463;&#32780;&#20250;&#38477;&#20302;&#25152;&#26377;&#31867;&#21035;&#30340;&#26631;&#20934;&#20934;&#30830;&#24615;&#32780;&#22266;&#26377;&#30340;&#24433;&#21709;&#65292;&#21478;&#19968;&#37096;&#20998;&#26159;&#30001;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#27604;&#29575;&#24341;&#36215;&#30340;&#65292;&#36825;&#23558;&#22686;&#21152;&#19982;&#26631;&#20934;&#35757;&#32451;&#30456;&#27604;&#30340;&#20934;&#30830;&#24615;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#36825;&#20123;&#24433;&#21709;&#36229;&#36234;&#20102;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#27169;&#22411;&#25512;&#24191;&#21040;&#31283;&#23450;&#20998;&#24067;&#30340;&#19968;&#33324;&#23478;&#26063;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#34429;&#28982;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#32422;&#26463;&#19968;&#33268;&#20250;&#20943;&#23569;&#25152;&#26377;&#31867;&#21035;&#30340;&#26631;&#20934;&#20934;&#30830;&#24615;&#65292;&#20294;&#36890;&#24120;&#20250;&#22686;&#21152;&#23545;&#23569;&#25968;&#31867;&#21035;&#30340;&#20934;&#30830;&#24615;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
While it has long been empirically observed that adversarial robustness may be at odds with standard accuracy and may have further disparate impacts on different classes, it remains an open question to what extent such observations hold and how the class imbalance plays a role within. In this paper, we attempt to understand this question of accuracy disparity by taking a closer look at linear classifiers under a Gaussian mixture model. We decompose the impact of adversarial robustness into two parts: an inherent effect that will degrade the standard accuracy on all classes due to the robustness constraint, and the other caused by the class imbalance ratio, which will increase the accuracy disparity compared to standard training. Furthermore, we also show that such effects extend beyond the Gaussian mixture model, by generalizing our data model to the general family of stable distributions. More specifically, we demonstrate that while the constraint of adversarial robustness consistentl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28789;&#27963;&#30340;&#20998;&#24067;&#39044;&#27979;&#26041;&#27861;EMQ&#65292;&#29992;&#20110;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#36880;&#27493;&#20559;&#31163;&#39640;&#26031;&#20998;&#24067;&#24182;&#22312;&#25552;&#21319;&#20013;&#21457;&#29616;&#26368;&#20248;&#26465;&#20214;&#20998;&#24067;&#65292;&#22240;&#27492;&#20855;&#26377;&#36739;&#22909;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14545</link><description>&lt;p&gt;
&#38598;&#25104;&#22810;&#20998;&#20301;&#25968;&#31639;&#27861;&#65306;&#33258;&#36866;&#24212;&#28789;&#27963;&#30340;&#20998;&#24067;&#39044;&#27979;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Ensemble Multi-Quantiles: Adaptively Flexible Distribution Prediction for Uncertainty Quantification. (arXiv:2211.14545v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28789;&#27963;&#30340;&#20998;&#24067;&#39044;&#27979;&#26041;&#27861;EMQ&#65292;&#29992;&#20110;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#36880;&#27493;&#20559;&#31163;&#39640;&#26031;&#20998;&#24067;&#24182;&#22312;&#25552;&#21319;&#20013;&#21457;&#29616;&#26368;&#20248;&#26465;&#20214;&#20998;&#24067;&#65292;&#22240;&#27492;&#20855;&#26377;&#36739;&#22909;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#31616;&#27905;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23427;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#28789;&#27963;&#30340;&#20998;&#24067;&#39044;&#27979;&#65292;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#26465;&#20214;&#20998;&#24067;$\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;&#27010;&#29575;&#27700;&#24179;&#30340;&#20998;&#20301;&#25968;&#65288;&#35206;&#30422;&#21306;&#38388;$(0,1)$&#65289;&#29992;&#30001;&#25105;&#20204;&#35774;&#35745;&#30340;&#21152;&#27861;&#27169;&#22411;&#25552;&#21319;&#65292;&#26469;&#39044;&#27979;&#36825;&#20010;&#26465;&#20214;&#20998;&#24067;&#12290;&#25105;&#20204;&#23547;&#27714;$\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$&#30340;&#32467;&#26500;&#23436;&#25972;&#24615;&#21644;&#28789;&#27963;&#24615;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;&#24179;&#34913;&#65292;&#32780;&#39640;&#26031;&#20551;&#35774;&#23545;&#20110;&#30495;&#23454;&#25968;&#25454;&#30340;&#28789;&#27963;&#24615;&#19981;&#36275;&#65292;&#39640;&#24230;&#28789;&#27963;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#22312;&#27809;&#26377;&#20998;&#24067;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#20998;&#21035;&#20272;&#35745;&#20998;&#20301;&#25968;&#65289;&#19981;&#21487;&#36991;&#20813;&#22320;&#20855;&#26377;&#32570;&#38519;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#26080;&#27861;&#24456;&#22909;&#22320;&#27010;&#25324;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#38598;&#25104;&#22810;&#20998;&#20301;&#25968;&#26041;&#27861;EMQ&#23436;&#20840;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#21487;&#20197;&#36880;&#27493;&#20559;&#31163;&#39640;&#26031;&#20998;&#24067;&#24182;&#22312;&#25552;&#21319;&#20013;&#21457;&#29616;&#26368;&#20248;&#26465;&#20214;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel, succinct, and effective approach to quantify uncertainty in machine learning. It incorporates adaptively flexible distribution prediction for $\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$ in regression tasks. For predicting this conditional distribution, its quantiles of probability levels spreading the interval $(0,1)$ are boosted by additive models which are designed by us with intuitions and interpretability. We seek an adaptive balance between the structural integrity and the flexibility for $\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$, while Gaussian assumption results in a lack of flexibility for real data and highly flexible approaches (e.g., estimating the quantiles separately without a distribution structure) inevitably have drawbacks and may not lead to good generalization. This ensemble multi-quantiles approach called EMQ proposed by us is totally data-driven, and can gradually depart from Gaussian and discover the optimal conditional distribution in the boosting. On ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38754;&#26495;&#25968;&#25454;&#20570;&#20915;&#31574;&#21046;&#23450;&#26102;&#65292;&#22914;&#20309;&#24212;&#23545;&#29983;&#25104;&#25968;&#25454;&#30340;&#21333;&#20301;&#37319;&#21462;&#31574;&#30053;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#23545;&#21333;&#20301;&#36827;&#34892;&#27491;&#30830;&#24178;&#39044;&#30340;&#26080;&#27450;&#35784;&#24178;&#39044;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2211.14236</link><description>&lt;p&gt;
&#38754;&#26495;&#25968;&#25454;&#20013;&#26080;&#27450;&#35784;&#20915;&#31574;&#21046;&#23450;&#30340;&#30740;&#31350;&#21450;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
Strategyproof Decision-Making in Panel Data Settings and Beyond. (arXiv:2211.14236v3 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38754;&#26495;&#25968;&#25454;&#20570;&#20915;&#31574;&#21046;&#23450;&#26102;&#65292;&#22914;&#20309;&#24212;&#23545;&#29983;&#25104;&#25968;&#25454;&#30340;&#21333;&#20301;&#37319;&#21462;&#31574;&#30053;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#23545;&#21333;&#20301;&#36827;&#34892;&#27491;&#30830;&#24178;&#39044;&#30340;&#26080;&#27450;&#35784;&#24178;&#39044;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#38754;&#26495;&#25968;&#25454;&#30340;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#32773;&#24471;&#21040;&#20102;&#22810;&#20010;&#21333;&#20301;&#65288;&#25110;&#20195;&#29702;&#20154;&#65289;&#30340;&#26377;&#22122;&#22768;&#12289;&#37325;&#22797;&#30340;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;&#20854;&#20013;&#23384;&#22312;&#19968;&#20010;&#24178;&#39044;&#21069;&#26399;&#65292;&#24403;&#20915;&#31574;&#32773;&#35266;&#23519;&#27599;&#20010;&#21333;&#20301;&#30340;&#32467;&#26524;&#21518;&#65292;&#20250;&#26681;&#25454;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#20026;&#27599;&#20010;&#21333;&#20301;&#20998;&#37197;&#19968;&#20010;&#22788;&#29702;&#12290;&#19982;&#20256;&#32479;&#30340;&#35774;&#32622;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#20801;&#35768;&#29983;&#25104;&#38754;&#26495;&#25968;&#25454;&#30340;&#21333;&#20301;&#37319;&#21462;&#31574;&#30053;&#65292;&#21363;&#21333;&#20301;&#21487;&#33021;&#20250;&#20462;&#25913;&#20854;&#24178;&#39044;&#21069;&#30340;&#32467;&#26524;&#20197;&#33719;&#24471;&#26356;&#29702;&#24819;&#30340;&#24178;&#39044;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#26080;&#27450;&#35784;&#30340;&#24178;&#39044;&#31574;&#30053;&#65292;&#20063;&#23601;&#26159;&#19968;&#20010;&#33021;&#22815;&#23545;&#21333;&#20301;&#36827;&#34892;&#27491;&#30830;&#24178;&#39044;&#30340;&#31574;&#30053;&#65292;&#26080;&#35770;&#21333;&#20301;&#26159;&#21542;&#36827;&#34892;&#20102;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#26465;&#20214;&#26469;&#21028;&#26029;&#26080;&#27450;&#35784;&#30340;&#24178;&#39044;&#31574;&#30053;&#26159;&#21542;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#31616;&#21333;&#38381;&#21512;&#24418;&#24335;&#30340;&#26080;&#27450;&#35784;&#26426;&#21046;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#24212;&#29992;&#20110;&#21171;&#21160;&#21147;&#24066;&#22330;&#20449;&#21495;&#30340;&#25112;&#30053;&#24615;&#22810;&#31867;&#20998;&#31867;&#35774;&#32622;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the classical problem of decision-making using panel data, in which a decision-maker gets noisy, repeated measurements of multiple units (or agents). We consider a setup where there is a pre-intervention period, when the principal observes the outcomes of each unit, after which the principal uses these observations to assign a treatment to each unit. Unlike this classical setting, we permit the units generating the panel data to be strategic, i.e. units may modify their pre-intervention outcomes in order to receive a more desirable intervention. The principal's goal is to design a strategyproof intervention policy, i.e. a policy that assigns units to their correct interventions despite their potential strategizing. We first identify a necessary and sufficient condition under which a strategyproof intervention policy exists, and provide a strategyproof mechanism with a simple closed form when one does exist. Along the way, we prove impossibility results for strategic multicl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;$\mathcal{O}$-ICID&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#36830;&#32493;&#20248;&#21270;&#19968;&#31181;&#30697;&#38453;&#20998;&#35299;&#26469;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#65292;&#36866;&#29992;&#20110;&#21464;&#37327;&#25968;&#37327;&#24222;&#22823;&#30340;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22122;&#22768;&#26041;&#24046;&#24050;&#30693;&#26102;&#35782;&#21035;&#30495;&#23454;DAG, &#20063;&#21487;&#20197;&#22312;&#36739;&#24369;&#30340;&#20808;&#39564;&#20449;&#24687;&#19979;&#32473;&#20986;&#26377;&#29992;&#30340;&#23450;&#21521;&#22270;&#35299;</title><link>http://arxiv.org/abs/2211.14221</link><description>&lt;p&gt;
&#36890;&#36807;&#30697;&#38453;&#20998;&#35299;&#20174;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#20013;&#23398;&#20064;&#22823;&#22411;&#22240;&#26524;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning Large Causal Structures from Inverse Covariance Matrix via Matrix Decomposition. (arXiv:2211.14221v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;$\mathcal{O}$-ICID&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#36830;&#32493;&#20248;&#21270;&#19968;&#31181;&#30697;&#38453;&#20998;&#35299;&#26469;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#65292;&#36866;&#29992;&#20110;&#21464;&#37327;&#25968;&#37327;&#24222;&#22823;&#30340;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22122;&#22768;&#26041;&#24046;&#24050;&#30693;&#26102;&#35782;&#21035;&#30495;&#23454;DAG, &#20063;&#21487;&#20197;&#22312;&#36739;&#24369;&#30340;&#20808;&#39564;&#20449;&#24687;&#19979;&#32473;&#20986;&#26377;&#29992;&#30340;&#23450;&#21521;&#22270;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21464;&#37327;&#25968;&#37327;&#24222;&#22823;&#26102;&#65292;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#26159;&#19968;&#20010;&#22522;&#26412;&#20294;&#39640;&#24230;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#32447;&#24615;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEMs)&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#20174;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;$\mathcal{O}$-ICID(&#26469;&#33258;Oracle &#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29420;&#31435;&#20445;&#25345;&#20998;&#35299;)&#65292;&#22522;&#20110;&#19968;&#31181;&#30697;&#38453;&#20998;&#35299;&#30340;&#36830;&#32493;&#20248;&#21270;&#65292;&#35813;&#20998;&#35299;&#20445;&#30041;&#20102;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#38750;&#38646;&#27169;&#24335;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22122;&#22768;&#26041;&#24046;&#24050;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;$\mathcal{O}$-ICID&#20026;&#35782;&#21035;&#30495;&#23454;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;&#22312;&#36739;&#24369;&#30340;&#20808;&#39564;&#20449;&#24687;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#32473;&#20986;&#26377;&#29992;&#30340;&#23450;&#21521;&#22270;&#35299;&#65292;&#29992;&#20110;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#22240;&#26524;&#21457;&#29616;&#12290;&#24403;&#30495;&#23454;DAG&#20855;&#26377;&#26377;&#38480;&#30340;&#33410;&#28857;&#24230;&#25968;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#26102;&#38388;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning causal structures from observational data is a fundamental yet highly complex problem when the number of variables is large. In this paper, we start from linear structural equation models (SEMs) and investigate ways of learning causal structures from the inverse covariance matrix. The proposed method, called $\mathcal{O}$-ICID (for {\it Independence-preserving} Decomposition from Oracle Inverse Covariance matrix), is based on continuous optimization of a type of matrix decomposition that preserves the nonzero patterns of the inverse covariance matrix. We show that $\mathcal{O}$-ICID provides an efficient way for identifying the true directed acyclic graph (DAG) under the knowledge of noise variances. With weaker prior information, the proposed method gives directed graph solutions that are useful for making more refined causal discovery. The proposed method enjoys a low complexity when the true DAG has bounded node degrees, as reflected by its time efficiency in experiments in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#38454;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#21644;&#38646;&#38454;&#26041;&#24046;&#20943;&#23569;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#31867;&#38750;&#20984;&#38750;&#20985;&#30340;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20998;&#21035;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29615;&#22659;&#19979;&#12290;&#23427;&#20204;&#26159;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#31532;&#19968;&#21644;&#31532;&#20108;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20445;&#35777;&#30340;&#38646;&#38454;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.13668</link><description>&lt;p&gt;
&#19968;&#31867;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38646;&#38454;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Zeroth-Order Alternating Gradient Descent Ascent Algorithms for a Class of Nonconvex-Nonconcave Minimax Problems. (arXiv:2211.13668v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#38454;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#21644;&#38646;&#38454;&#26041;&#24046;&#20943;&#23569;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#31867;&#38750;&#20984;&#38750;&#20985;&#30340;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20998;&#21035;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29615;&#22659;&#19979;&#12290;&#23427;&#20204;&#26159;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#31532;&#19968;&#21644;&#31532;&#20108;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20445;&#35777;&#30340;&#38646;&#38454;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#19968;&#31867;&#38750;&#20984;&#38750;&#20985;&#30340;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#21363;NC-PL&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#20989;&#25968;&#38024;&#23545;&#20869;&#37096;&#21464;&#37327;&#28385;&#36275;Polyak-L&#244;jasiewicz&#65288;PL&#65289;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38646;&#38454;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;ZO-AGDA&#65289;&#31639;&#27861;&#21644;&#38646;&#38454;&#26041;&#24046;&#20943;&#23569;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;ZO-VRAGDA&#65289;&#31639;&#27861;&#65292;&#20998;&#21035;&#29992;&#20110;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29615;&#22659;&#19979;&#35299;&#20915;NC-PL&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#20351;&#29992;ZO-AGDA&#21644;ZO-VRAGDA&#31639;&#27861;&#24471;&#21040;NC-PL&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#949;-&#31283;&#23450;&#28857;&#25152;&#38656;&#30340;&#24635;&#20989;&#25968;&#20540;&#26597;&#35810;&#27425;&#25968;&#19978;&#30028;&#20998;&#21035;&#20026;O(&#949;^(-2))&#21644;O(&#949;^(-3))&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20204;&#26159;&#35299;&#20915;NC-PL&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#31532;&#19968;&#21644;&#31532;&#20108;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20445;&#35777;&#30340;&#38646;&#38454;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider a class of nonconvex-nonconcave minimax problems, i.e., NC-PL minimax problems, whose objective functions satisfy the Polyak-\L ojasiewicz (PL) condition with respect to the inner variable. We propose a zeroth-order alternating gradient descent ascent (ZO-AGDA) algorithm and a zeroth-order variance reduced alternating gradient descent ascent (ZO-VRAGDA) algorithm for solving NC-PL minimax problem under the deterministic and the stochastic setting, respectively. The total number of function value queries to obtain an $\epsilon$-stationary point of ZO-AGDA and ZO-VRAGDA algorithm for solving NC-PL minimax problem is upper bounded by $\mathcal{O}(\varepsilon^{-2})$ and $\mathcal{O}(\varepsilon^{-3})$, respectively. To the best of our knowledge, they are the first two zeroth-order algorithms with the iteration complexity gurantee for solving NC-PL minimax problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36974;&#30422;&#24335;&#20915;&#31574;&#39044;&#27979; (MaskDP) &#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#21487;&#25193;&#23637;&#30340;&#22686;&#24378;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#20013;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#22823;&#35268;&#27169;&#22810;&#26679;&#30340;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#19988;&#38646;&#26679;&#26412;&#36716;&#31227;&#33267;&#26032;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2211.12740</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#21644;&#21487;&#25512;&#24191;&#20915;&#31574;&#21046;&#23450;&#30340;&#36974;&#30422;&#33258;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoding for Scalable and Generalizable Decision Making. (arXiv:2211.12740v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36974;&#30422;&#24335;&#20915;&#31574;&#39044;&#27979; (MaskDP) &#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#21487;&#25193;&#23637;&#30340;&#22686;&#24378;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#20013;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#22823;&#35268;&#27169;&#22810;&#26679;&#30340;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#19988;&#38646;&#26679;&#26412;&#36716;&#31227;&#33267;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#23398;&#20064;&#21487;&#25193;&#23637;&#30340;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#31867;&#20284;&#20110;&#24403;&#21069;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22810;&#26679;&#30340;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36974;&#30422;&#24335;&#20915;&#31574;&#39044;&#27979; (MaskDP) &#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#12290;&#22312; MaskDP &#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36974;&#25377;&#33258;&#32534;&#30721;&#22120; (MAE) &#22788;&#29702;&#29366;&#24577;-&#21160;&#20316;&#36712;&#36857;&#65292;&#38543;&#26426;&#36974;&#30422;&#29366;&#24577;&#21644;&#25805;&#20316;&#26631;&#35760;&#24182;&#37325;&#24314;&#32570;&#22833;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#27169;&#22411;&#38656;&#35201;&#25512;&#26029;&#20986;&#36974;&#25377;&#30340;&#29366;&#24577;&#21644;&#25805;&#20316;&#65292;&#24182;&#25552;&#21462;&#20851;&#20110;&#21160;&#24577;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36974;&#30422;&#19981;&#21516;&#27604;&#20363;&#30340;&#36755;&#20837;&#24207;&#21015;&#26174;&#33879;&#26377;&#21161;&#20110;&#23398;&#20064;&#19968;&#20010;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25512;&#24191;&#21040;&#22810;&#20010;&#21518;&#32493;&#20219;&#21153;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616; MaskDP &#27169;&#22411;&#33719;&#24471;&#20102;&#38646;&#26679;&#26412;&#36716;&#31227;&#21040;&#26032;&#30340; BC &#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#21333;&#19968;&#21644;&#22810;&#20010;&#30446;&#26631;&#21040;&#36798;&#20219;&#21153;&#65292;&#24182;&#19988;&#21487;&#20197;&#38646;&#26679;&#26412;&#36827;&#34892;&#36830;&#32493;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are interested in learning scalable agents for reinforcement learning that can learn from large-scale, diverse sequential data similar to current large vision and language models. To this end, this paper presents masked decision prediction (MaskDP), a simple and scalable self-supervised pretraining method for reinforcement learning (RL) and behavioral cloning (BC). In our MaskDP approach, we employ a masked autoencoder (MAE) to state-action trajectories, wherein we randomly mask state and action tokens and reconstruct the missing data. By doing so, the model is required to infer masked-out states and actions and extract information about dynamics. We find that masking different proportions of the input sequence significantly helps with learning a better model that generalizes well to multiple downstream tasks. In our empirical study, we find that a MaskDP model gains the capability of zero-shot transfer to new BC tasks, such as single and multiple goal reaching, and it can zero-shot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#36136;&#32852;&#36187;&#35757;&#32451;&#65288;HLT&#65289;&#30340;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20026;&#35299;&#20915;&#24322;&#36136;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#20351;&#29992;&#31574;&#30053;&#27744;&#21644;&#36229;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#24322;&#36136;&#26234;&#33021;&#20307;&#30340;&#21512;&#20316;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.11616</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#32852;&#36187;&#35757;&#32451;&#23454;&#29616;&#24322;&#36136;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Heterogeneous Agent Cooperation via Multiagent League Training. (arXiv:2211.11616v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#36136;&#32852;&#36187;&#35757;&#32451;&#65288;HLT&#65289;&#30340;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20026;&#35299;&#20915;&#24322;&#36136;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#20351;&#29992;&#31574;&#30053;&#27744;&#21644;&#36229;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#24322;&#36136;&#26234;&#33021;&#20307;&#30340;&#21512;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#35768;&#22810;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21253;&#25324;&#22810;&#31181;&#33021;&#21147;&#21644;&#21151;&#33021;&#19981;&#21516;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#26679;&#30340;&#24322;&#36136;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#29992;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#19982;&#21516;&#36136;&#31995;&#32479;&#30456;&#27604;&#65292;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#38750;&#31283;&#24577;&#38382;&#39064;&#21644;&#31574;&#30053;&#29256;&#26412;&#36845;&#20195;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#36136;&#32852;&#36187;&#35757;&#32451;&#65288;HLT&#65289;&#30340;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#24322;&#36136;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#12290;HLT&#36319;&#36394;&#20195;&#29702;&#20154;&#22312;&#35757;&#32451;&#26399;&#38388;&#25506;&#32034;&#30340;&#19968;&#32452;&#31574;&#30053;&#65292;&#25910;&#38598;&#24322;&#36136;&#31574;&#30053;&#32852;&#30431;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#36229;&#32593;&#32476;&#65292;&#20197;&#22686;&#21152;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#22312;&#19982;&#20855;&#26377;&#19981;&#21516;&#32423;&#21035;&#30340;&#21512;&#20316;&#25216;&#33021;&#30340;&#38431;&#21451;&#21512;&#20316;&#26102;&#36827;&#34892;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#24322;&#36136;&#22522;&#20934;&#20219;&#21153;&#26469;&#35777;&#26126;HLT&#33021;&#22815;&#25552;&#39640;&#21327;&#20316;&#24322;&#36136;&#20219;&#21153;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many multiagent systems in the real world include multiple types of agents with different abilities and functionality. Such heterogeneous multiagent systems have significant practical advantages. However, they also come with challenges compared with homogeneous systems for multiagent reinforcement learning, such as the non-stationary problem and the policy version iteration issue. This work proposes a general-purpose reinforcement learning algorithm named Heterogeneous League Training (HLT) to address heterogeneous multiagent problems. HLT keeps track of a pool of policies that agents have explored during training, gathering a league of heterogeneous policies to facilitate future policy optimization. Moreover, a hyper-network is introduced to increase the diversity of agent behaviors when collaborating with teammates having different levels of cooperation skills. We use heterogeneous benchmark tasks to demonstrate that (1) HLT promotes the success rate in cooperative heterogeneous task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#38543;&#26426;&#25628;&#32034;&#21644;&#26631;&#20934;&#20027;&#35266;&#24615;&#31561;&#38382;&#39064;&#65292;&#20445;&#35777;&#25214;&#21040;&#26368;&#20339;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2211.11461</link><description>&lt;p&gt;
&#20840;&#38754;&#30340;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Exhaustive Symbolic Regression. (arXiv:2211.11461v2 [astro-ph.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#38543;&#26426;&#25628;&#32034;&#21644;&#26631;&#20934;&#20027;&#35266;&#24615;&#31561;&#38382;&#39064;&#65292;&#20445;&#35777;&#25214;&#21040;&#26368;&#20339;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#31639;&#27861;&#23581;&#35797;&#23398;&#20064;&#31934;&#30830;&#36866;&#21512;&#25968;&#25454;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#20197;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#21576;&#29616;&#12290;&#20256;&#32479;SR&#23384;&#22312;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#27492;&#22788;&#21152;&#20197;&#35299;&#20915;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#26041;&#27861;&#38543;&#26426;&#22320;&#25628;&#32034;&#31354;&#38388;&#65288;&#36890;&#24120;&#20351;&#29992;&#36951;&#20256;&#32534;&#31243;&#65289;&#65292;&#22240;&#27492;&#19981;&#19968;&#23450;&#25214;&#21040;&#26368;&#20339;&#20989;&#25968;&#12290;&#20854;&#27425;&#65292;&#29992;&#20110;&#36873;&#25321;&#26368;&#20248;&#26041;&#31243;&#30340;&#26631;&#20934;&#22312;&#31934;&#24230;&#21644;&#31616;&#27905;&#24615;&#20043;&#38388;&#24179;&#34913;&#26102;&#26159;&#21487;&#21464;&#21644;&#20027;&#35266;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20840;&#38754;&#30340;&#31526;&#21495;&#22238;&#24402;&#65288;ESR&#65289;&#65292;&#23427;&#31995;&#32479;&#22320;&#21644;&#39640;&#25928;&#22320;&#32771;&#34385;&#25152;&#26377;&#21487;&#33021;&#30340;&#26041;&#31243;&#8212;&#8212;&#20351;&#29992;&#32473;&#23450;&#30340;&#31639;&#23376;&#22522;&#30784;&#38598;&#21644;&#26368;&#22823;&#22797;&#26434;&#24230;&#8212;&#8212;&#22240;&#27492;&#65292;&#22914;&#26524;&#21442;&#25968;&#23436;&#32654;&#20248;&#21270;&#65292;&#21017;&#20445;&#35777;&#25214;&#21040;&#30495;&#27491;&#30340;&#26368;&#20248;&#35299;&#21644;&#22312;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#23436;&#25972;&#20989;&#25968;&#25490;&#21517;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#20316;&#20026;&#23558;&#36825;&#20123;&#20559;&#22909;&#21512;&#24182;&#20026;&#21333;&#20010;&#30446;&#26631;&#30340;&#20005;&#26684;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic Regression (SR) algorithms attempt to learn analytic expressions which fit data accurately and in a highly interpretable manner. Conventional SR suffers from two fundamental issues which we address here. First, these methods search the space stochastically (typically using genetic programming) and hence do not necessarily find the best function. Second, the criteria used to select the equation optimally balancing accuracy with simplicity have been variable and subjective. To address these issues we introduce Exhaustive Symbolic Regression (ESR), which systematically and efficiently considers all possible equations -- made with a given basis set of operators and up to a specified maximum complexity -- and is therefore guaranteed to find the true optimum (if parameters are perfectly optimised) and a complete function ranking subject to these constraints. We implement the minimum description length principle as a rigorous method for combining these preferences into a single objec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26679;&#26412;&#36873;&#25321;&#21644;&#24179;&#34913;&#25439;&#22833;&#30340;&#40065;&#26834;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#30340;&#22788;&#29702;&#38271;&#23614;&#22024;&#26434;&#25968;&#25454;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2211.10906</link><description>&lt;p&gt;
&#29992;&#26679;&#26412;&#36873;&#25321;&#21644;&#24179;&#34913;&#25439;&#22833;&#20174;&#38271;&#23614;&#22024;&#26434;&#25968;&#25454;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Long-Tailed Noisy Data with Sample Selection and Balanced Loss. (arXiv:2211.10906v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26679;&#26412;&#36873;&#25321;&#21644;&#24179;&#34913;&#25439;&#22833;&#30340;&#40065;&#26834;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#30340;&#22788;&#29702;&#38271;&#23614;&#22024;&#26434;&#25968;&#25454;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#19988;&#31934;&#24515;&#31574;&#21010;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28982;&#32780;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25968;&#25454;&#36890;&#24120;&#26159;&#38271;&#23614;&#30340;&#21644;&#22024;&#26434;&#30340;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#22788;&#29702;&#38271;&#23614;&#25968;&#25454;&#25110;&#22024;&#26434;&#25968;&#25454;&#65292;&#32780;&#24456;&#23569;&#30340;&#26041;&#27861;&#34987;&#24320;&#21457;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#22024;&#26434;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26679;&#26412;&#36873;&#25321;&#21644;&#24179;&#34913;&#25439;&#22833;&#20174;&#38271;&#23614;&#22024;&#26434;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#40065;&#26834;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#26679;&#26412;&#36873;&#25321;&#23558;&#22024;&#26434;&#35757;&#32451;&#25968;&#25454;&#20998;&#20026;&#26377;&#26631;&#31614;&#30340;&#24178;&#20928;&#38598;&#21512;&#21644;&#26080;&#26631;&#31614;&#38598;&#21512;&#65292;&#24182;&#20197;&#22522;&#20110;&#27169;&#22411;&#20559;&#24046;&#30340;&#24179;&#34913;&#25439;&#22833;&#30340;&#21322;&#30417;&#30563;&#26041;&#24335;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of deep learning depends on large-scale and well-curated training data, while data in real-world applications are commonly long-tailed and noisy. Many methods have been proposed to deal with long-tailed data or noisy data, while a few methods are developed to tackle long-tailed noisy data. To solve this, we propose a robust method for learning from long-tailed noisy data with sample selection and balanced loss. Specifically, we separate the noisy training data into clean labeled set and unlabeled set with sample selection, and train the deep neural network in a semi-supervised manner with a balanced loss based on model bias. Extensive experiments on benchmarks demonstrate that our method outperforms existing state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2211.09619</link><description>&lt;p&gt;
&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#31616;&#20171;
&lt;/p&gt;
&lt;p&gt;
Introduction to Online Nonstochastic Control. (arXiv:2211.09619v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09619
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#21160;&#24577;&#31995;&#32479;&#25511;&#21046;&#19982;&#21487;&#24494;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#65292;&#24182;&#24212;&#29992;&#22312;&#32447;&#20984;&#20248;&#21270;&#21644;&#20984;&#26494;&#24347;&#25216;&#26415;&#24471;&#21040;&#20102;&#20855;&#26377;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#26368;&#20339;&#21644;&#40065;&#26834;&#25511;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#19982;&#20854;&#20182;&#26694;&#26550;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#30340;&#30446;&#26631;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#22312;&#26080;&#27861;&#39044;&#27979;&#25200;&#21160;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
This text presents an introduction to an emerging paradigm in control of dynamical systems and differentiable reinforcement learning called online nonstochastic control. The new approach applies techniques from online convex optimization and convex relaxations to obtain new methods with provable guarantees for classical settings in optimal and robust control.  The primary distinction between online nonstochastic control and other frameworks is the objective. In optimal control, robust control, and other control methodologies that assume stochastic noise, the goal is to perform comparably to an offline optimal strategy. In online nonstochastic control, both the cost functions as well as the perturbations from the assumed dynamical model are chosen by an adversary. Thus the optimal policy is not defined a priori. Rather, the target is to attain low regret against the best policy in hindsight from a benchmark class of policies.  This objective suggests the use of the decision making frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#35768;&#22810;&#22270;&#20687;&#20219;&#21153;(&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;)&#26102;&#21487;&#20197;&#24573;&#30053;&#20559;&#32622;&#65292;&#24182;&#19988;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#20855;&#26377;&#26631;&#37327; (&#20056;&#27861;) &#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#22312;&#25913;&#21464;&#23545;&#27604;&#24230;&#26102;&#20173;&#33021;&#20445;&#25345;&#39044;&#27979;&#19981;&#21464;&#12290;</title><link>http://arxiv.org/abs/2211.08486</link><description>&lt;p&gt;
&#38646;&#20559;&#32622;&#26631;&#37327;&#19981;&#21464;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Scalar Invariant Networks with Zero Bias. (arXiv:2211.08486v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#35768;&#22810;&#22270;&#20687;&#20219;&#21153;(&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;)&#26102;&#21487;&#20197;&#24573;&#30053;&#20559;&#32622;&#65292;&#24182;&#19988;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#20855;&#26377;&#26631;&#37327; (&#20056;&#27861;) &#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#22312;&#25913;&#21464;&#23545;&#27604;&#24230;&#26102;&#20173;&#33021;&#20445;&#25345;&#39044;&#27979;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#26435;&#37325;&#19968;&#26679;&#65292;&#20559;&#32622;&#39033;&#20063;&#26159;&#35768;&#22810;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;(&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;)&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#20154;&#20204;&#35748;&#20026;&#20559;&#24046;&#33021;&#26377;&#25928;&#22320;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#33021;&#21147;&#26469;&#35299;&#20915;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#25105;&#20204;&#20174;&#31532;&#19968;&#21407;&#29702;&#32771;&#34385;&#22270;&#20687;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#20998;&#24067;&#20197;&#21450;&#27169;&#22411;&#24212;&#20855;&#26377;&#30340;&#19968;&#20123;&#26399;&#26395;&#29305;&#24615;&#65292;&#21017;&#20559;&#24046;&#21487;&#20197;&#23436;&#20840;&#24573;&#30053;&#65292;&#20197;&#35299;&#20915;&#35768;&#22810;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#65292;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21487;&#33021;&#19982;&#24102;&#20559;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#31216;&#20026;&#26631;&#37327;(&#20056;&#27861;)&#19981;&#21464;&#24615;&#30340;&#33391;&#22909;&#23646;&#24615;&#65292;&#36825;&#20351;&#24471;&#24403;&#25913;&#21464;&#36755;&#20837;&#22270;&#20687;&#30340;&#23545;&#27604;&#24230;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#26631;&#37327;&#19981;&#21464;&#24615;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#8230;
&lt;/p&gt;
&lt;p&gt;
Just like weights, bias terms are the learnable parameters of many popular machine learning models, including neural networks. Biases are believed to effectively increase the representational power of neural networks to solve a wide range of tasks in computer vision. However, we argue that if we consider the intrinsic distribution of images in the input space as well as some desired properties a model should have from the first principles, biases can be completely ignored in addressing many image-related tasks, such as image classification. Our observation indicates that zero-bias neural networks could perform comparably to neural networks with bias at least on practical image classification tasks. In addition, we prove that zero-bias neural networks possess a nice property called scalar (multiplication) invariance, which allows the prediction of neural networks remains the same when altering the contrast of the input image. We then extend scalar invariance to more general cases that a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#31070;&#32463;HMM TTS&#19982;&#24402;&#19968;&#21270;&#27969;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#32780;&#20840;&#38754;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#38656;&#35201;&#30340;&#25968;&#25454;&#26356;&#23569;&#65292;&#35757;&#32451;&#26356;&#26032;&#26356;&#23569;&#65292;&#21457;&#38899;&#20934;&#30830;&#24615;&#39640;&#65292;&#20027;&#35266;&#35821;&#38899;&#36136;&#37327;&#25509;&#36817;&#33258;&#28982;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2211.06892</link><description>&lt;p&gt;
OverFlow&#65306;&#22312;&#31070;&#32463;&#36716;&#24405;&#22120;&#19978;&#21472;&#21152;&#27969;&#26469;&#25552;&#39640;TTS&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
OverFlow: Putting flows on top of neural transducers for better TTS. (arXiv:2211.06892v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#31070;&#32463;HMM TTS&#19982;&#24402;&#19968;&#21270;&#27969;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#32780;&#20840;&#38754;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#38656;&#35201;&#30340;&#25968;&#25454;&#26356;&#23569;&#65292;&#35757;&#32451;&#26356;&#26032;&#26356;&#23569;&#65292;&#21457;&#38899;&#20934;&#30830;&#24615;&#39640;&#65292;&#20027;&#35266;&#35821;&#38899;&#36136;&#37327;&#25509;&#36817;&#33258;&#28982;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;HMM&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#36716;&#24405;&#22120;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#30340;&#24207;&#21015;&#27169;&#22411;&#12290;&#23427;&#20204;&#32467;&#21512;&#20102;&#32463;&#20856;&#30340;&#32479;&#35745;&#35821;&#38899;&#21512;&#25104;&#21644;&#29616;&#20195;&#30340;&#31070;&#32463;TTS&#30340;&#26368;&#20339;&#29305;&#24615;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#21644;&#36739;&#23569;&#30340;&#35757;&#32451;&#26356;&#26032;&#65292;&#24182;&#19988;&#19981;&#23481;&#26131;&#20986;&#29616;&#30001;&#20110;&#31070;&#32463;&#27880;&#24847;&#21147;&#22833;&#36133;&#23548;&#33268;&#30340;&#26080;&#24847;&#20041;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#31070;&#32463;HMM TTS&#19982;&#24402;&#19968;&#21270;&#27969;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#25551;&#36848;&#35821;&#38899;&#22768;&#23398;&#30340;&#39640;&#24230;&#38750;&#39640;&#26031;&#20998;&#24067;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#24378;&#22823;&#32780;&#20840;&#38754;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#20934;&#30830;&#30340;&#26368;&#22823;&#20284;&#28982;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#25552;&#35758;&#30340;&#31995;&#32479;&#38656;&#35201;&#27604;&#21487;&#27604;&#26041;&#27861;&#26356;&#23569;&#30340;&#26356;&#26032;&#65292;&#20197;&#20135;&#29983;&#20934;&#30830;&#30340;&#21457;&#38899;&#21644;&#25509;&#36817;&#33258;&#28982;&#35821;&#38899;&#30340;&#20027;&#35266;&#35821;&#38899;&#36136;&#37327;&#12290;&#35831;&#21442;&#35265;https://shivammehta25.github.io/OverFlow/&#33719;&#21462;&#38899;&#39057;&#31034;&#20363;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural HMMs are a type of neural transducer recently proposed for sequence-to-sequence modelling in text-to-speech. They combine the best features of classic statistical speech synthesis and modern neural TTS, requiring less data and fewer training updates, and are less prone to gibberish output caused by neural attention failures. In this paper, we combine neural HMM TTS with normalising flows for describing the highly non-Gaussian distribution of speech acoustics. The result is a powerful, fully probabilistic model of durations and acoustics that can be trained using exact maximum likelihood. Experiments show that a system based on our proposal needs fewer updates than comparable methods to produce accurate pronunciations and a subjective speech quality close to natural speech. Please see https://shivammehta25.github.io/OverFlow/ for audio examples and code.
&lt;/p&gt;</description></item><item><title>GCondNet&#21033;&#29992;&#39640;&#32500;&#34920;&#26684;&#25968;&#25454;&#30340;&#38544;&#21547;&#32467;&#26500;&#65292;&#36890;&#36807;&#21019;&#24314;&#22270;&#24418;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#26465;&#20214;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#28508;&#22312;&#39044;&#27979;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.06302</link><description>&lt;p&gt;
GCondNet: &#19968;&#31181;&#25913;&#36827;&#23567;&#22411;&#39640;&#32500;&#34920;&#26684;&#25968;&#25454;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GCondNet: A Novel Method for Improving Neural Networks on Small High-Dimensional Tabular Data. (arXiv:2211.06302v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06302
&lt;/p&gt;
&lt;p&gt;
GCondNet&#21033;&#29992;&#39640;&#32500;&#34920;&#26684;&#25968;&#25454;&#30340;&#38544;&#21547;&#32467;&#26500;&#65292;&#36890;&#36807;&#21019;&#24314;&#22270;&#24418;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#26465;&#20214;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#28508;&#22312;&#39044;&#27979;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#22788;&#29702;&#39640;&#32500;&#20294;&#26679;&#26412;&#25968;&#37327;&#36739;&#23567;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#26102;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#24403;&#21069;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#26041;&#27861;&#20551;&#23450;&#26435;&#37325;&#20043;&#38388;&#30456;&#20114;&#29420;&#31435;&#65292;&#24403;&#26679;&#26412;&#19981;&#36275;&#20197;&#20934;&#30830;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#26102;&#65292;&#36825;&#21487;&#33021;&#20250;&#20135;&#29983;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#23567;&#25968;&#25454;&#22330;&#26223;&#19979;&#65292;&#21033;&#29992;&#20854;&#20182;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GCondNet&#65292;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#38544;&#21547;&#32467;&#26500;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#32500;&#24230;&#22312;&#26679;&#26412;&#20043;&#38388;&#21019;&#24314;&#19968;&#20010;&#22270;&#24418;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#25552;&#21462;&#36825;&#31181;&#38544;&#21547;&#32467;&#26500;&#65292;&#20197;&#21450;&#35843;&#25972;&#28508;&#22312;&#39044;&#27979; MLP &#32593;&#32476;&#30340;&#31532;&#19968;&#23618;&#21442;&#25968;&#36827;&#34892;&#26465;&#20214;&#35757;&#32451;&#12290;&#36890;&#36807;&#21019;&#24314;&#35768;&#22810;&#23567;&#22270;&#65292;GCondNet &#21033;&#29992;&#20102;&#25968;&#25454;&#30340;&#39640;&#32500;&#29305;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28508;&#22312;&#39044;&#27979;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models often struggle with high-dimensional but small sample-size tabular datasets. One reason is that current weight initialisation methods assume independence between weights, which can be problematic when there are insufficient samples to estimate the model's parameters accurately. In such small data scenarios, leveraging additional structures can improve the model's training stability and performance. To address this, we propose GCondNet, a general approach to enhance neural networks by leveraging implicit structures present in tabular data. We create a graph between samples for each data dimension, and utilise Graph Neural Networks (GNNs) for extracting this implicit structure, and for conditioning the parameters of the first layer of an underlying predictor MLP network. By creating many small graphs, GCondNet exploits the data's high-dimensionality, and thus improves the performance of an underlying predictor network. We demonstrate the effectiveness of our method 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#36830;&#32493;&#24773;&#24863;&#24378;&#24230;&#30340;&#35821;&#38899;&#21512;&#25104;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#21644;&#22522;&#20110;&#24773;&#24863;&#30340;&#32479;&#19968;&#32593;&#26684;&#20960;&#20309;&#23884;&#20837;&#31354;&#38388;&#65292;&#20854;&#21487;&#25511;&#24615;&#21644;&#33258;&#28982;&#24230;&#37117;&#36739;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.06160</link><description>&lt;p&gt;
&#22522;&#20110;&#35299;&#32806;&#34920;&#31034;&#30340;&#36830;&#32493;&#24773;&#24863;&#24378;&#24230;&#21487;&#25511;&#35821;&#38899;&#21512;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning for continuous emotional intensity controllable speech synthesis with disentangled representations. (arXiv:2211.06160v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#36830;&#32493;&#24773;&#24863;&#24378;&#24230;&#30340;&#35821;&#38899;&#21512;&#25104;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#21644;&#22522;&#20110;&#24773;&#24863;&#30340;&#32479;&#19968;&#32593;&#26684;&#20960;&#20309;&#23884;&#20837;&#31354;&#38388;&#65292;&#20854;&#21487;&#25511;&#24615;&#21644;&#33258;&#28982;&#24230;&#37117;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#24050;&#32463;&#36798;&#21040;&#20102;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#35828;&#35805;&#30340;&#33258;&#28982;&#35821;&#38899;&#30340;&#27700;&#24179;&#12290;&#20294;&#22312;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#36824;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;&#24773;&#24863;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#20351;&#29992;&#24773;&#24863;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#32553;&#25918;&#21442;&#25968;&#36827;&#34892;&#25554;&#20540;&#29305;&#24449;&#25511;&#21046;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#21487;&#25511;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21253;&#21547;&#24773;&#24863;&#12289;&#35828;&#35805;&#32773;&#31561;&#29305;&#24449;&#30340;&#34701;&#21512;&#65292;&#29616;&#26377;&#27169;&#22411;&#29983;&#25104;&#30340;&#24773;&#24863;&#28508;&#22312;&#31354;&#38388;&#38590;&#20197;&#25511;&#21046;&#36830;&#32493;&#30340;&#24773;&#24863;&#24378;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20351;&#29992;&#22522;&#20110;&#38899;&#32032;&#32423;&#35821;&#38899;&#20449;&#24687;&#30340;&#20266;&#26631;&#31614;&#20135;&#29983;&#30340;&#20013;&#38388;&#24378;&#24230;&#30340;&#24773;&#24863;&#12290;&#20174;&#25552;&#20986;&#30340;&#27169;&#22411;&#20013;&#26500;&#24314;&#30340;&#23884;&#20837;&#31354;&#38388;&#28385;&#36275;&#24102;&#26377;&#24773;&#24863;&#22522;&#30784;&#30340;&#32479;&#19968;&#32593;&#26684;&#20960;&#20309;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21487;&#25511;&#24615;&#21644;&#33258;&#28982;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-speech models have reached the level of generating natural speech similar to what humans say. But there still have limitations in terms of expressiveness. The existing emotional speech synthesis models have shown controllability using interpolated features with scaling parameters in emotional latent space. However, the emotional latent space generated from the existing models is difficult to control the continuous emotional intensity because of the entanglement of features like emotions, speakers, etc. In this paper, we propose a novel method to control the continuous intensity of emotions using semi-supervised learning. The model learns emotions of intermediate intensity using pseudo-labels generated from phoneme-level sequences of speech information. An embedding space built from the proposed model satisfies the uniform grid geometry with an emotional basis. The experimental results showed that the proposed method was superior in controllability and naturalness.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#20837;&#22122;&#22768;&#30340;&#26032;&#30340;SNN&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;ANN&#21040;SNN&#36716;&#25442;&#21644;&#22522;&#20110;&#33033;&#20914;&#30340;&#21453;&#21521;&#20256;&#25773;&#65292;&#36890;&#36807;&#35757;&#32451;&#21333;&#27493;SNN&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#22810;&#27493;SNN&#26469;&#26174;&#33879;&#25552;&#39640;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.05453</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#30340;&#24555;&#36895;SNN&#35757;&#32451;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A noise based novel strategy for faster SNN training. (arXiv:2211.05453v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#20837;&#22122;&#22768;&#30340;&#26032;&#30340;SNN&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;ANN&#21040;SNN&#36716;&#25442;&#21644;&#22522;&#20110;&#33033;&#20914;&#30340;&#21453;&#21521;&#20256;&#25773;&#65292;&#36890;&#36807;&#35757;&#32451;&#21333;&#27493;SNN&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#22810;&#27493;SNN&#26469;&#26174;&#33879;&#25552;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22240;&#20854;&#20302;&#21151;&#32791;&#21644;&#24378;&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;SNN&#30340;&#20248;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65292;&#21363;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#21040;SNN&#30340;&#36716;&#25442;&#21644;&#22522;&#20110;&#33033;&#20914;&#30340;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#65292;&#37117;&#26377;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#23545;&#20110;ANN&#21040;SNN&#30340;&#36716;&#25442;&#65292;&#23427;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#25512;&#29702;&#26469;&#36924;&#36817;ANN&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;SNN&#30340;&#20248;&#21183;&#12290;&#20351;&#29992;&#22522;&#20110;&#33033;&#20914;&#30340;BP&#26469;&#35757;&#32451;&#39640;&#31934;&#24230;SNN&#36890;&#24120;&#38656;&#35201;&#27604;&#23427;&#20204;&#30340;ANN&#30456;&#23545;&#24212;&#22320;&#28040;&#32791;&#20960;&#21313;&#20493;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#20248;&#28857;&#30340;&#26032;&#30340;SNN&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;&#31070;&#32463;&#30005;&#20301;&#20998;&#24067;&#36817;&#20284;&#20026;&#38543;&#26426;&#22122;&#22768;&#26469;&#35757;&#32451;&#21333;&#27493;SNN(T=1)&#65292;&#28982;&#21518;&#26080;&#25439;&#22320;&#23558;&#21333;&#27493;SNN(T=1)&#36716;&#25442;&#20026;&#22810;&#27493;SNN(T=N)&#12290;&#24341;&#20837;&#39640;&#26031;&#20998;&#24067;&#22122;&#22768;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36716;&#25442;&#21518;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) are receiving increasing attention due to their low power consumption and strong bio-plausibility. Optimization of SNNs is a challenging task. Two main methods, artificial neural network (ANN)-to-SNN conversion and spike-based backpropagation (BP), both have their advantages and limitations. For ANN-to-SNN conversion, it requires a long inference time to approximate the accuracy of ANN, thus diminishing the benefits of SNN. With spike-based BP, training high-precision SNNs typically consumes dozens of times more computational resources and time than their ANN counterparts. In this paper, we propose a novel SNN training approach that combines the benefits of the two methods. We first train a single-step SNN(T=1) by approximating the neural potential distribution with random noise, then convert the single-step SNN(T=1) to a multi-step SNN(T=N) losslessly. The introduction of Gaussian distributed noise leads to a significant gain in accuracy after conversion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;lilGym&#65292;&#23427;&#30001;2661&#20010;&#39640;&#24230;&#32452;&#21512;&#30340;&#20154;&#31867;&#32534;&#20889;&#33258;&#28982;&#35821;&#35328;&#35821;&#21477;&#21644;&#20132;&#20114;&#24335;&#35270;&#35273;&#29615;&#22659;&#32452;&#25104;&#65292;&#24182;&#36890;&#36807;&#27880;&#37322;&#21487;&#25191;&#34892;Python&#31243;&#24207;&#26469;&#23454;&#29616;&#31934;&#30830;&#30340;&#22870;&#21169;&#35745;&#31639;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;lilGym&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.01994</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#65306;lilGym
&lt;/p&gt;
&lt;p&gt;
lilGym: Natural Language Visual Reasoning with Reinforcement Learning. (arXiv:2211.01994v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;lilGym&#65292;&#23427;&#30001;2661&#20010;&#39640;&#24230;&#32452;&#21512;&#30340;&#20154;&#31867;&#32534;&#20889;&#33258;&#28982;&#35821;&#35328;&#35821;&#21477;&#21644;&#20132;&#20114;&#24335;&#35270;&#35273;&#29615;&#22659;&#32452;&#25104;&#65292;&#24182;&#36890;&#36807;&#27880;&#37322;&#21487;&#25191;&#34892;Python&#31243;&#24207;&#26469;&#23454;&#29616;&#31934;&#30830;&#30340;&#22870;&#21169;&#35745;&#31639;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;lilGym&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#20851;&#35821;&#35328;&#26465;&#20214;&#19979;&#24378;&#21270;&#23398;&#20064;&#22312;&#35270;&#35273;&#29615;&#22659;&#19979;&#30340;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;lilGym&#12290;lilGym&#22522;&#20110;2661&#20010;&#39640;&#24230;&#32452;&#21512;&#30340;&#20154;&#31867;&#32534;&#20889;&#30340;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#65292;&#36825;&#20123;&#38472;&#36848;&#26159;&#22522;&#20110;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#29615;&#22659;&#30340;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27599;&#31181;&#21487;&#33021;&#30340;&#19990;&#30028;&#29366;&#24577;&#19979;&#65292;&#36890;&#36807;&#20026;&#25152;&#26377;&#35821;&#21477;&#27880;&#37322;&#21487;&#25191;&#34892;&#30340;Python&#31243;&#24207;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#22870;&#21169;&#35745;&#31639;&#12290;&#27599;&#20010;&#35821;&#21477;&#37117;&#19982;&#22810;&#20010;&#36215;&#22987;&#29366;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#37197;&#23545;&#65292;&#20197;&#24418;&#25104;&#25968;&#21315;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#23398;&#20064;&#26426;&#21046;&#36827;&#34892;&#20102;lilGym&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;lilGym&#24418;&#25104;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;lilGym&#21487;&#20197;&#22312; https://lil.nlp.cornell.edu/lilgym/ &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present lilGym, a new benchmark for language-conditioned reinforcement learning in visual environments. lilGym is based on 2,661 highly-compositional human-written natural language statements grounded in an interactive visual environment. We introduce a new approach for exact reward computation in every possible world state by annotating all statements with executable Python programs. Each statement is paired with multiple start states and reward functions to form thousands of distinct Markov Decision Processes of varying difficulty. We experiment with lilGym with different models and learning regimes. Our results and analysis show that while existing methods are able to achieve non-trivial performance, lilGym forms a challenging open problem. lilGym is available at https://lil.nlp.cornell.edu/lilgym/.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#24494;&#35843;&#23454;&#29616;&#36328;&#35821;&#35328;&#27867;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#33521;&#35821;&#25552;&#31034;&#19979;&#65292;&#23545;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33521;&#35821;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#20165;&#20986;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20219;&#21153;&#27867;&#21270;&#65292;&#24182;&#19988;&#20351;&#29992;&#33521;&#35821;&#25552;&#31034;&#36827;&#34892;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#24494;&#35843;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21508;&#31181;&#38646;-shot&#32467;&#26524;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.01786</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#24494;&#35843;&#23454;&#29616;&#36328;&#35821;&#35328;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Crosslingual Generalization through Multitask Finetuning. (arXiv:2211.01786v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#24494;&#35843;&#23454;&#29616;&#36328;&#35821;&#35328;&#27867;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#33521;&#35821;&#25552;&#31034;&#19979;&#65292;&#23545;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33521;&#35821;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#20165;&#20986;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20219;&#21153;&#27867;&#21270;&#65292;&#24182;&#19988;&#20351;&#29992;&#33521;&#35821;&#25552;&#31034;&#36827;&#34892;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#24494;&#35843;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21508;&#31181;&#38646;-shot&#32467;&#26524;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#22810;&#20219;&#21153;&#24494;&#35843;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#25512;&#24191;&#21040;&#26032;&#30340;&#20219;&#21153;&#65292;&#20294;&#30446;&#21069;MTF&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#33521;&#35821;&#25968;&#25454;&#21644;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#23558;MTF&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;BLOOM&#21644;mT5&#27169;&#22411;&#31995;&#21015;&#65292;&#29983;&#25104;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;&#21464;&#20307;BLOOMZ&#21644;mT0&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#33521;&#35821;&#25552;&#31034;&#19979;&#65292;&#23545;&#22823;&#22411;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33521;&#35821;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#20165;&#20986;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20219;&#21153;&#27867;&#21270;&#12290;&#20351;&#29992;&#33521;&#35821;&#25552;&#31034;&#36827;&#34892;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#24494;&#35843;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21508;&#31181;&#38646;-shot&#32467;&#26524;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#33521;&#35821;&#32763;&#35793;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#35821;&#35328;&#20219;&#21153;&#24494;&#35843;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#20123;&#26426;&#22120;&#32763;&#35793;&#25552;&#31034;&#19978;&#35757;&#32451;&#21487;&#20197;&#22312;&#21508;&#33258;&#35821;&#35328;&#20013;&#26356;&#22909;&#22320;&#23436;&#25104;&#20154;&#20889;&#30340;&#25552;&#31034;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;m
&lt;/p&gt;
&lt;p&gt;
Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25429;&#25417;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21046;&#29305;&#24449;&#30041;&#22312;&#39044;&#23450;&#20041;&#30340;&#29305;&#24449;&#30697;&#38453;&#30340;&#38468;&#36817;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#25200;&#21160;&#20998;&#26512;&#65292;&#22312;&#23567;&#37051;&#22495;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#31995;&#25968;&#33539;&#25968;&#19981;&#21464;&#24615;&#21644;&#29305;&#24449;&#25238;&#21160;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.16658</link><description>&lt;p&gt;
&#31070;&#32463;&#23849;&#28291;&#30340;&#25200;&#21160;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Perturbation Analysis of Neural Collapse. (arXiv:2210.16658v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25429;&#25417;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21046;&#29305;&#24449;&#30041;&#22312;&#39044;&#23450;&#20041;&#30340;&#29305;&#24449;&#30697;&#38453;&#30340;&#38468;&#36817;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#25200;&#21160;&#20998;&#26512;&#65292;&#22312;&#23567;&#37051;&#22495;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#31995;&#25968;&#33539;&#25968;&#19981;&#21464;&#24615;&#21644;&#29305;&#24449;&#25238;&#21160;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#22312;&#38646;&#35757;&#32451;&#35823;&#24046;&#28857;&#20043;&#21518;&#65292;&#36890;&#24120;&#28041;&#21450;&#26368;&#23567;&#21270;&#35757;&#32451;&#25439;&#22833;&#12290;&#22312;&#36825;&#19968;&#38454;&#27573;&#30340;&#35757;&#32451;&#20013;&#65292;&#35266;&#23519;&#21040;&#20102;&#19968;&#31181;&#8220;&#31070;&#32463;&#23849;&#28291;&#8221;&#34892;&#20026;&#65306;&#21516;&#19968;&#31867;&#21035;&#26679;&#26412;&#30340;&#29305;&#24449;&#65288;&#20498;&#25968;&#31532;&#20108;&#23618;&#30340;&#36755;&#20986;&#65289;&#30340;&#21464;&#24322;&#24615;&#20943;&#23569;&#65292;&#19981;&#21516;&#31867;&#21035;&#30340;&#24179;&#22343;&#29305;&#24449;&#36235;&#21521;&#20110;&#26576;&#20010;&#32039;&#23494;&#30340;&#26694;&#26550;&#32467;&#26500;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#29702;&#24819;&#21270;&#30340;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#20998;&#26512;&#20102;&#36825;&#31181;&#34892;&#20026;&#65292;&#22312;&#25152;&#26377;&#26368;&#23567;&#21270;&#22120;&#37117;&#20986;&#29616;&#23436;&#20840;&#23849;&#28291;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#20013;&#65292;&#29305;&#24449;&#36890;&#24120;&#19981;&#20250;&#36798;&#21040;&#23436;&#20840;&#23849;&#28291;&#30340;&#29366;&#24577;&#65292;&#20363;&#22914;&#65292;&#22240;&#20026;&#28145;&#23618;&#26080;&#27861;&#20219;&#24847;&#20462;&#25913;&#36828;&#31163;&#23849;&#28291;&#29366;&#24577;&#30340;&#20013;&#38388;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20016;&#23500;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#24378;&#21046;&#29305;&#24449;&#30041;&#22312;&#39044;&#23450;&#20041;&#30340;&#29305;&#24449;&#30697;&#38453;&#65288;&#20363;&#22914;&#20013;&#38388;&#29305;&#24449;&#65289;&#30340;&#38468;&#36817;&#26469;&#25429;&#25417;&#36825;&#31181;&#29616;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#25200;&#21160;&#20998;&#26512;&#25506;&#32034;&#20102;&#23567;&#37051;&#22495;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#65292;&#24182;&#24314;&#31435;&#20102;&#31995;&#25968;&#33539;&#25968;&#19981;&#21464;&#24615;&#21644;&#29305;&#24449;&#25238;&#21160;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep neural networks for classification often includes minimizing the training loss beyond the zero training error point. In this phase of training, a "neural collapse" behavior has been observed: the variability of features (outputs of the penultimate layer) of within-class samples decreases and the mean features of different classes approach a certain tight frame structure. Recent works analyze this behavior via idealized unconstrained features models where all the minimizers exhibit exact collapse. However, with practical networks and datasets, the features typically do not reach exact collapse, e.g., because deep layers cannot arbitrarily modify intermediate features that are far from being collapsed. In this paper, we propose a richer model that can capture this phenomenon by forcing the features to stay in the vicinity of a predefined features matrix (e.g., intermediate features). We explore the model in the small vicinity case via perturbation analysis and establish res
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;GitHub Copilot&#65292;&#21457;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35813;&#31995;&#32479;&#20132;&#20114;&#30340;&#19968;&#20123;&#24120;&#35265;&#27963;&#21160;&#65292;&#25581;&#31034;&#20102;&#20854;&#20302;&#25928;&#24615;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#20174;&#32780;&#20026;&#25913;&#36827;&#30028;&#38754;&#35774;&#35745;&#21644;&#24230;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#21160;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.14306</link><description>&lt;p&gt;
&#35835;&#25026;&#20195;&#30721;&#32972;&#21518;&#65306;&#27169;&#25311;AI&#36741;&#21161;&#32534;&#31243;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#21644;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming. (arXiv:2210.14306v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;GitHub Copilot&#65292;&#21457;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35813;&#31995;&#32479;&#20132;&#20114;&#30340;&#19968;&#20123;&#24120;&#35265;&#27963;&#21160;&#65292;&#25581;&#31034;&#20102;&#20854;&#20302;&#25928;&#24615;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#20174;&#32780;&#20026;&#25913;&#36827;&#30028;&#38754;&#35774;&#35745;&#21644;&#24230;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#65292;&#22914;Copilot&#21644;CodeWhisperer&#65292;&#36890;&#36807;&#33258;&#21160;&#24314;&#35758;&#21644;&#33258;&#21160;&#23436;&#25104;&#20195;&#30721;&#65292;&#26377;&#28508;&#21147;&#25552;&#39640;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#29575;&#12290;&#28982;&#32780;&#65292;&#35201;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#31243;&#24207;&#21592;&#22914;&#20309;&#19982;&#36825;&#20123;&#31995;&#32479;&#20132;&#20114;&#65292;&#24182;&#30830;&#23450;&#25913;&#36827;&#20132;&#20114;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#21462;&#24471;&#36827;&#23637;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27599;&#22825;&#30001;&#25968;&#30334;&#19975;&#31243;&#24207;&#21592;&#20351;&#29992;&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;GitHub Copilot&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24120;&#35265;&#31243;&#24207;&#21592;&#27963;&#21160;&#30340;&#20998;&#31867;&#31995;&#32479;CUPS&#65292;&#20197;&#20415;&#27169;&#25311;&#29992;&#25143;&#19982;Copilot&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#23545;21&#21517;&#23436;&#25104;&#32534;&#30721;&#20219;&#21153;&#24182;&#22238;&#39038;&#24615;&#22320;&#20351;&#29992;CUPS&#26631;&#35760;&#20854;&#20250;&#35805;&#30340;&#31243;&#24207;&#21592;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;CUPS&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#31243;&#24207;&#21592;&#22914;&#20309;&#19982;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#20132;&#20114;&#65292;&#25581;&#31034;&#20102;&#25928;&#29575;&#20302;&#19979;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#27934;&#35265;&#25581;&#31034;&#20102;&#31243;&#24207;&#21592;&#22914;&#20309;&#19982;Copilot&#20132;&#20114;&#65292;&#24182;&#28608;&#21457;&#20102;&#26032;&#30340;&#30028;&#38754;&#35774;&#35745;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-recommendation systems, such as Copilot and CodeWhisperer, have the potential to improve programmer productivity by suggesting and auto-completing code. However, to fully realize their potential, we must understand how programmers interact with these systems and identify ways to improve that interaction. To make progress, we studied GitHub Copilot, a code-recommendation system used by millions of programmers daily. We developed CUPS, a taxonomy of common programmer activities when interacting with Copilot. Our study of 21 programmers, who completed coding tasks and retrospectively labeled their sessions with CUPS, showed that CUPS can help us understand how programmers interact with code-recommendation systems, revealing inefficiencies and time costs. Our insights reveal how programmers interact with Copilot and motivate new interface designs and metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WaveBound&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#35823;&#24046;&#30028;&#38480;&#26469;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#20197;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.14303</link><description>&lt;p&gt;
WaveBound&#65306;&#31283;&#23450;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21160;&#24577;&#35823;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
WaveBound: Dynamic Error Bounds for Stable Time Series Forecasting. (arXiv:2210.14303v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WaveBound&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#35823;&#24046;&#30028;&#38480;&#26469;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#20197;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24050;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#39640;&#23454;&#29992;&#24615;&#65292;&#22914;&#20132;&#36890;&#12289;&#33021;&#28304;&#28040;&#32791;&#12289;&#32463;&#27982;&#21644;&#37329;&#34701;&#12289;&#30142;&#30149;&#20998;&#26512;&#31561;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21160;&#24577;&#24615;&#65292;&#28145;&#24230;&#32593;&#32476;&#20173;&#28982;&#38754;&#20020;&#19981;&#31283;&#23450;&#30340;&#35757;&#32451;&#21644;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#19981;&#19968;&#33268;&#27169;&#24335;&#23548;&#33268;&#27169;&#22411;&#23545;&#29305;&#23450;&#27169;&#24335;&#23384;&#22312;&#20559;&#24046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35757;&#32451;&#25439;&#22833;&#30340;&#21160;&#24577;&#35823;&#24046;&#30028;&#38480;&#65292;&#20197;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WaveBound&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20272;&#35745;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#21644;&#27599;&#20010;&#29305;&#24449;&#30340;&#35757;&#32451;&#25439;&#22833;&#30340;&#36866;&#24403;&#35823;&#24046;&#30028;&#38480;&#12290;&#36890;&#36807;&#20351;&#27169;&#22411;&#19987;&#27880;&#20110;&#19981;&#21487;&#39044;&#27979;&#30340;&#25968;&#25454;&#36739;&#23569;&#65292;WaveBound&#31283;&#23450;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#20174;&#32780;&#26174;&#30528;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting has become a critical task due to its high practicality in real-world applications such as traffic, energy consumption, economics and finance, and disease analysis. Recent deep-learning-based approaches have shown remarkable success in time series forecasting. Nonetheless, due to the dynamics of time series data, deep networks still suffer from unstable training and overfitting. Inconsistent patterns appearing in real-world data lead the model to be biased to a particular pattern, thus limiting the generalization. In this work, we introduce the dynamic error bounds on training loss to address the overfitting issue in time series forecasting. Consequently, we propose a regularization method called WaveBound which estimates the adequate error bounds of training loss for each time step and feature at each iteration. By allowing the model to focus less on unpredictable data, WaveBound stabilizes the training process, thus significantly improving generalization. With
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;motif&#30340;GNN&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;motif&#30340;&#35302;&#21457;&#22120;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#25552;&#39640;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#21644;&#25928;&#26524;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.13710</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#22270;&#30340;GNN&#21518;&#38376;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs. (arXiv:2210.13710v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;motif&#30340;GNN&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;motif&#30340;&#35302;&#21457;&#22120;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#25552;&#39640;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#21644;&#25928;&#26524;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24378;&#22823;&#34920;&#31034;&#33021;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#29289;&#22522;&#22240;&#39044;&#27979;&#12289;&#31038;&#20132;&#25512;&#33616;&#31561;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;GNN&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#29992;&#24694;&#24847;&#26679;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#24456;&#23481;&#26131;&#34987;&#20462;&#34917;&#21518;&#30340;&#26679;&#26412;&#27450;&#39575;&#12290;&#22823;&#22810;&#25968;&#30340;&#21518;&#38376;&#25915;&#20987;&#30740;&#31350;&#20351;&#29992;&#30340;&#35302;&#21457;&#22120;&#35201;&#20040;&#26159;&#38543;&#26426;&#29983;&#25104;&#30340;&#23376;&#22270;&#65288;&#20363;&#22914; erd\H{o}s-r\'enyi &#21518;&#38376;&#65289;&#65292;&#20197;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#65292;&#35201;&#20040;&#26159;&#22522;&#20110;&#26799;&#24230;&#30340;&#29983;&#25104;&#23376;&#22270;&#65288;&#20363;&#22914;&#22270;Trojan&#25915;&#20987;&#65289;&#65292;&#20197;&#20351;&#25915;&#20987;&#26356;&#21152;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#29702;&#35299;&#35302;&#21457;&#22120;&#32467;&#26500;&#19982;&#21518;&#38376;&#25915;&#20987;&#25928;&#26524;&#20043;&#38388;&#30340;&#20851;&#31995;&#21364;&#22312;&#24403;&#21069;&#25991;&#29486;&#20013;&#34987;&#24573;&#30053;&#20102;&#12290;&#22312;&#22270;&#20013;&#65292;&#37325;&#22797;&#24615;&#21644;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#23376;&#22270;&#65288;motif&#65289;&#21253;&#21547;&#20102;&#20016;&#23500;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20174;motif&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#35302;&#21457;&#22120;&#30340;&#35774;&#35745;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;motif&#30340;GNN&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;motif&#30340;&#35302;&#21457;&#22120;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;motif&#20449;&#24687;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#21644;&#25928;&#26524;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#38544;&#34109;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN) with a powerful representation capability has been widely applied to various areas, such as biological gene prediction, social recommendation, etc. Recent works have exposed that GNN is vulnerable to the backdoor attack, i.e., models trained with maliciously crafted training samples are easily fooled by patched samples. Most of the proposed studies launch the backdoor attack using a trigger that either is the randomly generated subgraph (e.g., erd\H{o}s-r\'enyi backdoor) for less computational burden, or the gradient-based generative subgraph (e.g., graph trojaning attack) to enable a more effective attack. However, the interpretation of how is the trigger structure and the effect of the backdoor attack related has been overlooked in the current literature. Motifs, recurrent and statistically significant sub-graphs in graphs, contain rich structure information. In this paper, we are rethinking the trigger from the perspective of motifs, and propose a motif-ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;PAC-Bayesian&#26041;&#27861;&#20998;&#26512;&#31163;&#32447;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#25552;&#20379;&#20102;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#38469;&#24773;&#22659;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2210.13132</link><description>&lt;p&gt;
&#20855;&#26377;&#20445;&#35777;&#30340;PAC-Bayesian&#31163;&#32447;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Offline Contextual Bandits With Guarantees. (arXiv:2210.13132v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;PAC-Bayesian&#26041;&#27861;&#20998;&#26512;&#31163;&#32447;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#25552;&#20379;&#20102;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#38469;&#24773;&#22659;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;PAC-Bayesian&#26041;&#27861;&#30340;&#31163;&#32447;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#19981;&#26159;&#20174;&#38590;&#20197;&#22788;&#29702;&#25110;&#19981;&#20934;&#30830;&#30340;&#30028;&#38480;&#25512;&#23548;&#23398;&#20064;&#21407;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;PAC-Bayesian&#26041;&#27861;&#20998;&#26512;&#38382;&#39064;&#65292;&#23558;&#31574;&#30053;&#35299;&#37322;&#20026;&#20915;&#31574;&#35268;&#21017;&#30340;&#28151;&#21512;&#29289;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#31639;&#27861;&#26469;&#20248;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#24471;&#30028;&#38480;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#32039;&#65292;&#21487;&#20197;&#30452;&#25509;&#20248;&#21270;&#20197;&#22312;&#31163;&#32447;&#24773;&#20917;&#19979;&#33258;&#20449;&#22320;&#25913;&#36827;&#35760;&#24405;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#24102;&#20445;&#35777;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#65292;&#24182;&#19981;&#38656;&#35201;&#22312;&#20445;&#30041;&#38598;&#19978;&#35843;&#25972;&#26356;&#22810;&#30340;&#36229;&#21442;&#25968;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#24773;&#26223;&#20013;&#25552;&#20379;&#24615;&#33021;&#20445;&#35777;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new principled approach for off-policy learning in contextual bandits. Unlike previous work, our approach does not derive learning principles from intractable or loose bounds. We analyse the problem through the PAC-Bayesian lens, interpreting policies as mixtures of decision rules. This allows us to propose novel generalization bounds and provide tractable algorithms to optimize them. We prove that the derived bounds are tighter than their competitors, and can be optimized directly to confidently improve upon the logging policy offline. Our approach learns policies with guarantees, uses all available data and does not require tuning additional hyperparameters on held-out sets. We demonstrate through extensive experiments the effectiveness of our approach in providing performance guarantees in practical scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#22686;&#24378;&#30340;&#20302;&#31209;&#24352;&#37327;&#26679;&#26465;&#20811;&#37324;&#37329;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#35266;&#27979;&#19979;&#36827;&#34892;&#22823;&#35268;&#27169;&#20132;&#36890;&#36895;&#24230;&#20272;&#35745;&#65292;&#20197;&#20174;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#20013;&#24674;&#22797;&#21487;&#20449;&#30340;&#20272;&#35745;&#20540;&#12290;</title><link>http://arxiv.org/abs/2210.11780</link><description>&lt;p&gt;
&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#22686;&#24378;&#30340;&#20302;&#31209;&#24352;&#37327;&#26679;&#26465;&#20811;&#37324;&#37329;&#26041;&#27861;&#30340;&#31232;&#30095;&#20256;&#24863;&#22823;&#35268;&#27169;&#20132;&#36890;&#36895;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Correlating sparse sensing for large-scale traffic speed estimation: A Laplacian-enhanced low-rank tensor kriging approach. (arXiv:2210.11780v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#22686;&#24378;&#30340;&#20302;&#31209;&#24352;&#37327;&#26679;&#26465;&#20811;&#37324;&#37329;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#35266;&#27979;&#19979;&#36827;&#34892;&#22823;&#35268;&#27169;&#20132;&#36890;&#36895;&#24230;&#20272;&#35745;&#65292;&#20197;&#20174;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#20013;&#24674;&#22797;&#21487;&#20449;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#36895;&#24230;&#26159;&#34920;&#24449;&#36947;&#36335;&#32593;&#32476;&#27969;&#21160;&#24615;&#30340;&#26680;&#24515;&#22240;&#32032;&#65292;&#35768;&#22810;&#20132;&#36890;&#24212;&#29992;&#31243;&#24207;&#37117;&#20381;&#36182;&#20110;&#23427;&#65292;&#22914;&#23454;&#26102;&#23548;&#33322;&#12289;&#21160;&#24577;&#36335;&#32447;&#35268;&#21010;&#21644;&#25317;&#22581;&#31649;&#29702;&#12290;&#20256;&#24863;&#21644;&#36890;&#20449;&#25216;&#26415;&#30340;&#24555;&#36895;&#36827;&#23637;&#20351;&#20132;&#36890;&#36895;&#24230;&#26816;&#27979;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#21152;&#23481;&#26131;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38745;&#24577;&#20256;&#24863;&#22120;&#30340;&#31232;&#30095;&#37096;&#32626;&#25110;&#31227;&#21160;&#20256;&#24863;&#22120;&#30340;&#20302;&#28183;&#36879;&#65292;&#26816;&#27979;&#21040;&#30340;&#36895;&#24230;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#24182;&#19988;&#36828;&#31163;&#20840;&#32593;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#20256;&#24863;&#22120;&#23481;&#26131;&#20986;&#29616;&#35823;&#24046;&#25110;&#32570;&#22833;&#25968;&#25454;&#65292;&#36825;&#20123;&#20256;&#24863;&#22120;&#26816;&#27979;&#21040;&#30340;&#36895;&#24230;&#20250;&#21464;&#24471;&#38750;&#24120;&#22024;&#26434;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#25216;&#26415;&#20174;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#20013;&#24674;&#22797;&#21487;&#20449;&#30340;&#20272;&#35745;&#20540;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#38382;&#39064;&#30830;&#23450;&#20026;&#19968;&#20010;&#26102;&#31354;&#20811;&#37324;&#37329;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25289;&#26222;&#25289;&#26031;&#22686;&#24378;&#30340;&#20302;&#31209;&#24352;&#37327;&#23436;&#25104;&#65288;LETC&#65289;&#26694;&#26550;&#65292;&#20854;&#20855;&#26377;&#20302;&#31209;&#24615;&#21644;&#22810;&#32500;&#30456;&#20851;&#24615;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#35266;&#27979;&#19979;&#36827;&#34892;&#22823;&#35268;&#27169;&#20132;&#36890;&#36895;&#24230;&#20811;&#37324;&#37329;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic speed is central to characterizing the fluidity of the road network. Many transportation applications rely on it, such as real-time navigation, dynamic route planning, and congestion management. Rapid advances in sensing and communication techniques make traffic speed detection easier than ever. However, due to sparse deployment of static sensors or low penetration of mobile sensors, speeds detected are incomplete and far from network-wide use. In addition, sensors are prone to error or missing data due to various kinds of reasons, speeds from these sensors can become highly noisy. These drawbacks call for effective techniques to recover credible estimates from the incomplete data. In this work, we first identify the issue as a spatiotemporal kriging problem and propose a Laplacian enhanced low-rank tensor completion (LETC) framework featuring both lowrankness and multi-dimensional correlations for large-scale traffic speed kriging under limited observations. To be specific, th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28304;&#20869;&#26679;&#24335;&#22686;&#24378;&#65288;ISSA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#25513;&#27169;&#22122;&#22768;&#32534;&#30721;&#22120;&#38543;&#26426;&#21270;&#26679;&#24335;&#21644;&#20869;&#23481;&#32452;&#21512;&#65292;ISSA&#26377;&#25928;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#24182;&#20943;&#23569;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#39550;&#39542;&#22330;&#26223;&#35821;&#20041;&#20998;&#21106;&#20013;&#33719;&#24471;&#20102;&#39640;&#36798;12.4&#65285;&#30340;mIoU&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.10175</link><description>&lt;p&gt;
&#22522;&#20110;&#28304;&#20869;&#26679;&#24335;&#22686;&#24378;&#30340;&#39046;&#22495;&#27867;&#21270;&#25913;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Intra-Source Style Augmentation for Improved Domain Generalization. (arXiv:2210.10175v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28304;&#20869;&#26679;&#24335;&#22686;&#24378;&#65288;ISSA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#25513;&#27169;&#22122;&#22768;&#32534;&#30721;&#22120;&#38543;&#26426;&#21270;&#26679;&#24335;&#21644;&#20869;&#23481;&#32452;&#21512;&#65292;ISSA&#26377;&#25928;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#24182;&#20943;&#23569;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#39550;&#39542;&#22330;&#26223;&#35821;&#20041;&#20998;&#21106;&#20013;&#33719;&#24471;&#20102;&#39640;&#36798;12.4&#65285;&#30340;mIoU&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24212;&#29992;&#20013;&#32463;&#24120;&#20986;&#29616;&#39046;&#22495;&#20559;&#31227;&#65292;&#27604;&#22914;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#27867;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28304;&#20869;&#26679;&#24335;&#22686;&#24378;&#65288;ISSA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#31181;&#26032;&#22411;&#30340;StyleGAN2&#21453;&#28436;&#25513;&#27169;&#22122;&#22768;&#32534;&#30721;&#22120;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22122;&#22768;&#39044;&#27979;&#23398;&#20064;&#24544;&#23454;&#37325;&#24314;&#22270;&#20687;&#24182;&#20445;&#30041;&#20854;&#35821;&#20041;&#24067;&#23616;&#12290;&#20272;&#35745;&#22122;&#22768;&#30340;&#38543;&#26426;&#25513;&#34109;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#26679;&#24335;&#28151;&#21512;&#33021;&#21147;&#65292;&#21363;&#23427;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#22270;&#20687;&#35821;&#20041;&#24067;&#23616;&#30340;&#24773;&#20917;&#19979;&#25913;&#21464;&#20840;&#23616;&#22806;&#35266;&#12290;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#25513;&#27169;&#22122;&#22768;&#32534;&#30721;&#22120;&#26469;&#38543;&#26426;&#21270;&#35757;&#32451;&#38598;&#20013;&#30340;&#26679;&#24335;&#21644;&#20869;&#23481;&#32452;&#21512;&#65292;ISSA&#26377;&#25928;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#24182;&#20943;&#23569;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#32467;&#26524;&#65292;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#39550;&#39542;&#22330;&#26223;&#35821;&#20041;&#20998;&#21106;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#39640;&#36798;12.4&#65285;&#30340;mIoU&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization with respect to domain shifts, as they frequently appear in applications such as autonomous driving, is one of the remaining big challenges for deep learning models. Therefore, we propose an intra-source style augmentation (ISSA) method to improve domain generalization in semantic segmentation. Our method is based on a novel masked noise encoder for StyleGAN2 inversion. The model learns to faithfully reconstruct the image preserving its semantic layout through noise prediction. Random masking of the estimated noise enables the style mixing capability of our model, i.e. it allows to alter the global appearance without affecting the semantic layout of an image. Using the proposed masked noise encoder to randomize style and content combinations in the training set, ISSA effectively increases the diversity of training data and reduces spurious correlation. As a result, we achieve up to $12.4\%$ mIoU improvements on driving-scene semantic segmentation under different type
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#20540;&#30340;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65288;STaSy&#65289;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#25216;&#26415;&#21644;&#24494;&#35843;&#31574;&#30053;&#36827;&#19968;&#27493;&#25552;&#39640;&#37319;&#26679;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.04018</link><description>&lt;p&gt;
STaSy&#65306;&#22522;&#20110;&#20998;&#20540;&#30340;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
STaSy: Score-based Tabular data Synthesis. (arXiv:2210.04018v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04018
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#20540;&#30340;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65288;STaSy&#65289;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#25216;&#26415;&#21644;&#24494;&#35843;&#31574;&#30053;&#36827;&#19968;&#27493;&#25552;&#39640;&#37319;&#26679;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20174;&#32479;&#35745;&#26041;&#27861;&#21040;&#28145;&#24230;&#29983;&#25104;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#34920;&#26684;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#23427;&#24182;&#19981;&#24635;&#26159;&#25104;&#21151;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#20998;&#20540;&#30340;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#65288;STaSy&#65289;&#30340;&#26032;&#27169;&#22411;&#21450;&#20854;&#22522;&#20110;&#20998;&#20540;&#29983;&#25104;&#24314;&#27169;&#33539;&#20363;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#23613;&#31649;&#22522;&#20110;&#20998;&#20540;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#35299;&#20915;&#20102;&#35768;&#22810;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#20294;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35757;&#32451;&#31574;&#30053;&#21253;&#25324;&#33258;&#36866;&#24212;&#23398;&#20064;&#25216;&#26415;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#31283;&#23450;&#21435;&#22122;&#20998;&#20540;&#21305;&#37197;&#35757;&#32451;&#36827;&#19968;&#27493;&#22686;&#21152;&#37319;&#26679;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#37319;&#26679;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#26102;&#38388;&#19977;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#22312;o
&lt;/p&gt;
&lt;p&gt;
Tabular data synthesis is a long-standing research topic in machine learning. Many different methods have been proposed over the past decades, ranging from statistical methods to deep generative methods. However, it has not always been successful due to the complicated nature of real-world tabular data. In this paper, we present a new model named Score-based Tabular data Synthesis (STaSy) and its training strategy based on the paradigm of score-based generative modeling. Despite the fact that score-based generative models have resolved many issues in generative models, there still exists room for improvement in tabular data synthesis. Our proposed training strategy includes a self-paced learning technique and a fine-tuning strategy, which further increases the sampling quality and diversity by stabilizing the denoising score matching training. Furthermore, we also conduct rigorous experimental studies in terms of the generative task trilemma: sampling quality, diversity, and time. In o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STSyn&#30340;&#26412;&#22320;SGD&#31574;&#30053;&#65292;&#36890;&#36807;&#23481;&#38169;&#21516;&#27493;&#25216;&#26415;&#65292;&#31561;&#24453;&#26368;&#24555;&#30340;&#24037;&#20154;&#65292;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#24037;&#20154;&#30340;&#26377;&#25928;&#26412;&#22320;&#26356;&#26032;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21516;&#27493;&#26412;&#22320;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#22240;&#33853;&#21518;&#24037;&#20154;&#21644;&#36890;&#20449;&#25928;&#29575;&#20302;&#19979;&#32780;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.03521</link><description>&lt;p&gt;
STSyn&#65306;&#36890;&#36807;&#23481;&#38169;&#21516;&#27493;&#21152;&#36895;&#26412;&#22320;SGD
&lt;/p&gt;
&lt;p&gt;
STSyn: Speeding Up Local SGD with Straggler-Tolerant Synchronization. (arXiv:2210.03521v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STSyn&#30340;&#26412;&#22320;SGD&#31574;&#30053;&#65292;&#36890;&#36807;&#23481;&#38169;&#21516;&#27493;&#25216;&#26415;&#65292;&#31561;&#24453;&#26368;&#24555;&#30340;&#24037;&#20154;&#65292;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#24037;&#20154;&#30340;&#26377;&#25928;&#26412;&#22320;&#26356;&#26032;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21516;&#27493;&#26412;&#22320;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#22240;&#33853;&#21518;&#24037;&#20154;&#21644;&#36890;&#20449;&#25928;&#29575;&#20302;&#19979;&#32780;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#27493;&#26412;&#22320;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;local SGD&#65289;&#20250;&#22240;&#20026;&#19968;&#20123;&#24037;&#20316;&#21333;&#20803;&#31354;&#38386;&#24182;&#22240;&#24930;&#24037;&#25110;&#33853;&#21518;&#24037;&#20154;&#32780;&#23384;&#22312;&#19968;&#20123;&#38543;&#26426;&#24310;&#36831;&#65292;&#22240;&#20026;&#23427;&#22312;&#31561;&#24453;&#24037;&#20154;&#23436;&#25104;&#30456;&#21516;&#25968;&#37327;&#30340;&#26412;&#22320;&#26356;&#26032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26412;&#22320;SGD&#31574;&#30053;&#8212;&#8212;STSyn&#65292;&#20197;&#20943;&#36731;&#33853;&#21518;&#32773;&#21644;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#20851;&#38190;&#26159;&#31561;&#24453;$K$&#20010;&#26368;&#24555;&#30340;&#24037;&#20154;&#65292;&#21516;&#26102;&#35753;&#25152;&#26377;&#24037;&#20154;&#22312;&#27599;&#20010;&#21516;&#27493;&#24490;&#29615;&#20013;&#22987;&#32456;&#35745;&#31639;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#24037;&#20154;&#30340;&#26377;&#25928;&#65288;&#24050;&#23436;&#25104;&#65289;&#26412;&#22320;&#26356;&#26032;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#33853;&#21518;&#32773;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#20851;&#20110;STSyn&#24615;&#33021;&#30340;&#24179;&#22343;&#25346;&#38047;&#26102;&#38388;&#12289;&#26412;&#22320;&#26356;&#26032;&#30340;&#24179;&#22343;&#25968;&#37327;&#21644;&#27599;&#36718;&#19978;&#20256;&#24037;&#20154;&#30340;&#24179;&#22343;&#25968;&#37327;&#30340;&#20998;&#26512;&#12290;&#21363;&#20351;&#30446;&#26631;&#20989;&#25968;&#26159;&#38750;&#20984;&#30340;&#65292;STSyn&#30340;&#25910;&#25947;&#24615;&#20063;&#24471;&#21040;&#20102;&#20005;&#26684;&#30340;&#35777;&#26126;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;&#23481;&#38169;&#21516;&#27493;&#25216;&#26415;&#65292;&#25152;&#25552;&#20986;&#30340;STSyn&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synchronous local stochastic gradient descent (local SGD) suffers from some workers being idle and random delays due to slow and straggling workers, as it waits for the workers to complete the same amount of local updates. In this paper, to mitigate stragglers and improve communication efficiency, a novel local SGD strategy, named STSyn, is developed. The key point is to wait for the $K$ fastest workers, while keeping all the workers computing continually at each synchronization round, and making full use of any effective (completed) local update of each worker regardless of stragglers. An analysis of the average wall-clock time, average number of local updates and average number of uploading workers per round is provided to gauge the performance of STSyn. The convergence of STSyn is also rigorously established even when the objective function is nonconvex. Experimental results show the superiority of the proposed STSyn against state-of-the-art schemes through utilization of the stragg
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#24335;&#8212;&#8212;&#22810;&#27169;&#24577;&#25552;&#31034;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;VIMA&#65292;&#21487;&#20197;&#22788;&#29702;&#25552;&#31034;&#24182;&#33258;&#22238;&#24402;&#22320;&#36755;&#20986;&#30005;&#26426;&#21160;&#20316;&#65292;&#23454;&#29616;&#20102;&#21508;&#31181;&#20219;&#21153;&#31867;&#22411;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#24182;&#33021;&#22815;&#38646;&#26679;&#26412;&#27867;&#21270;&#21040;&#26032;&#30340;&#23545;&#35937;&#31867;&#21035;&#65292;&#36825;&#23545;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#20855;&#26377;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2210.03094</link><description>&lt;p&gt;
VIMA&#65306;&#22810;&#27169;&#24577;&#25552;&#31034;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
VIMA: General Robot Manipulation with Multimodal Prompts. (arXiv:2210.03094v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#24335;&#8212;&#8212;&#22810;&#27169;&#24577;&#25552;&#31034;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;VIMA&#65292;&#21487;&#20197;&#22788;&#29702;&#25552;&#31034;&#24182;&#33258;&#22238;&#24402;&#22320;&#36755;&#20986;&#30005;&#26426;&#21160;&#20316;&#65292;&#23454;&#29616;&#20102;&#21508;&#31181;&#20219;&#21153;&#31867;&#22411;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#24182;&#33021;&#22815;&#38646;&#26679;&#26412;&#27867;&#21270;&#21040;&#26032;&#30340;&#23545;&#35937;&#31867;&#21035;&#65292;&#36825;&#23545;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#20855;&#26377;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#27169;&#24335;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25104;&#21151;&#33539;&#20363;&#65292;&#22312;&#27492;&#27169;&#24335;&#19979;&#65292;&#21333;&#20010;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25353;&#29031;&#36755;&#20837;&#25552;&#31034;&#25191;&#34892;&#20219;&#20309;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#20154;&#24037;&#31243;&#20013;&#65292;&#20219;&#21153;&#35268;&#33539;&#30340;&#24418;&#24335;&#22810;&#31181;&#22810;&#26679;&#65292;&#20363;&#22914;&#65292;&#27169;&#20223;&#21333;&#27425;&#28436;&#31034;&#12289;&#36981;&#24490;&#35821;&#35328;&#25351;&#20196;&#21644;&#36798;&#21040;&#35270;&#35273;&#30446;&#26631;&#31561;&#12290;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#30001;&#19987;&#38376;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#24191;&#27867;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#26469;&#34920;&#36798;&#65292;&#20132;&#38169;&#25991;&#26412;&#21644;&#35270;&#35273;&#20196;&#29260;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#20223;&#30495;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#31243;&#24207;&#29983;&#25104;&#30340;&#26700;&#38754;&#20219;&#21153;&#65292;&#20855;&#26377;&#22810;&#27169;&#24577;&#25552;&#31034;&#65292;60&#19975;&#20010;&#19987;&#23478;&#36712;&#36857;&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#24182;&#37319;&#29992;&#22235;&#32423;&#35780;&#20272;&#21327;&#35758;&#36827;&#34892;&#31995;&#32479;&#21270;&#30340;&#24191;&#20041;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;VIMA&#65292;&#35813;&#20195;&#29702;&#22788;&#29702;&#36825;&#20123;&#25552;&#31034;&#24182;&#33258;&#22238;&#24402;&#22320;&#36755;&#20986;&#30005;&#26426;&#21160;&#20316;&#12290;VIMA&#20855;&#26377;&#19968;&#22871;&#37197;&#26041;&#65292;&#23454;&#29616;&#20102;&#21508;&#31181;&#20219;&#21153;&#31867;&#22411;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#21253;&#25324;&#26410;&#35265;&#36807;&#30340;&#27169;&#24577;&#32452;&#21512;&#65292;&#29978;&#33267;&#21487;&#20197;&#38646;&#26679;&#26412;&#27867;&#21270;&#21040;&#26032;&#30340;&#23545;&#35937;&#31867;&#21035;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#32479;&#19968;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26694;&#26550;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#35757;&#32451;&#26041;&#27861;&#65292;&#22522;&#20110;&#21516;&#27493;&#21644;&#21516;&#20262;&#20248;&#21270;&#65292;&#21487;&#20197;&#29992;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#21160;&#21147;&#23398;&#35268;&#24459;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#20462;&#25913;&#12290;</title><link>http://arxiv.org/abs/2210.01407</link><description>&lt;p&gt;
&#22522;&#20110;&#21516;&#27493;&#21644;&#21516;&#20262;&#20248;&#21270;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#31934;&#30830;&#21160;&#21147;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Homotopy-based training of NeuralODEs for accurate dynamics discovery. (arXiv:2210.01407v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#35757;&#32451;&#26041;&#27861;&#65292;&#22522;&#20110;&#21516;&#27493;&#21644;&#21516;&#20262;&#20248;&#21270;&#65292;&#21487;&#20197;&#29992;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#21160;&#21147;&#23398;&#35268;&#24459;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NeuralODEs&#65289;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#21644;&#29289;&#29702;&#31185;&#23398;&#22522;&#20110;&#24494;&#20998;&#26041;&#31243;&#30340;&#24314;&#27169;&#33539;&#24335;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#26159;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#21160;&#21147;&#23398;&#35268;&#24459;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#20986;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21644;&#27425;&#20248;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#38271;&#26102;&#38388;&#27573;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21516;&#27493;&#21644;&#21516;&#20262;&#20248;&#21270;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#25913;&#21464;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#27169;&#22411;&#21160;&#21147;&#23398;&#21644;&#35757;&#32451;&#25968;&#25454;&#21516;&#27493;&#21487;&#20197;&#39535;&#26381;&#21407;&#26412;&#19981;&#35268;&#21017;&#30340;&#25439;&#22833;&#21644;&#30340;&#26223;&#35937;&#65292;&#21516;&#20262;&#20248;&#21270;&#21487;&#20197;&#21033;&#29992;&#36825;&#19968;&#28857;&#26469;&#22686;&#24378;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Ordinary Differential Equations (NeuralODEs) present an attractive way to extract dynamical laws from time series data, as they bridge neural networks with the differential equation-based modeling paradigm of the physical sciences. However, these models often display long training times and suboptimal results, especially for longer duration data. While a common strategy in the literature imposes strong constraints to the NeuralODE architecture to inherently promote stable model dynamics, such methods are ill-suited for dynamics discovery as the unknown governing equation is not guaranteed to satisfy the assumed constraints. In this paper, we develop a new training method for NeuralODEs, based on synchronization and homotopy optimization, that does not require changes to the model architecture. We show that synchronizing the model dynamics and the training data tames the originally irregular loss landscape, which homotopy optimization can then leverage to enhance training. Throug
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35838;&#31243;&#30340;&#24207;&#21015;&#31070;&#32463;&#35299;&#30721;&#22120;CRISP&#65292;&#21487;&#20197;&#29992;&#20110;&#26497;&#21270;&#32534;&#30721;&#26063;&#65292;&#30456;&#27604;&#36830;&#32493;&#21462;&#28040;(SC)&#35793;&#30721;&#22120;&#65292;CRISP&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#21487;&#36731;&#26494;&#22320;&#25299;&#23637;&#33267;&#26497;&#21270;&#35843;&#25972;&#21367;&#31215;&#65288;PAC&#65289;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2210.00313</link><description>&lt;p&gt;
&#22522;&#20110;&#35838;&#31243;&#30340;&#24207;&#21015;&#31070;&#32463;&#35299;&#30721;&#22120;&#29992;&#20110;&#26497;&#21270;&#32534;&#30721;&#26063;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
CRISP: Curriculum based Sequential Neural Decoders for Polar Code Family. (arXiv:2210.00313v3 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00313
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35838;&#31243;&#30340;&#24207;&#21015;&#31070;&#32463;&#35299;&#30721;&#22120;CRISP&#65292;&#21487;&#20197;&#29992;&#20110;&#26497;&#21270;&#32534;&#30721;&#26063;&#65292;&#30456;&#27604;&#36830;&#32493;&#21462;&#28040;(SC)&#35793;&#30721;&#22120;&#65292;CRISP&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#21487;&#36731;&#26494;&#22320;&#25299;&#23637;&#33267;&#26497;&#21270;&#35843;&#25972;&#21367;&#31215;&#65288;PAC&#65289;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#21270;&#32534;&#30721;&#26159;&#21487;&#38752;&#36890;&#20449;&#30340;&#26368;&#26032;&#35268;&#33539;&#65288;5G&#65289;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#65292;&#20294;&#22312;&#30701;&#22359;&#38271;&#24230;&#33539;&#22260;&#20869;&#35774;&#35745;&#26082;&#39640;&#25928;&#21448;&#21487;&#38752;&#30340;&#26497;&#21270;&#35793;&#30721;&#22120;&#20173;&#26377;&#31354;&#38388;&#12290;&#21463;&#25968;&#25454;&#39537;&#21160;&#20449;&#36947;&#35793;&#30721;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35838;&#31243;&#30340;&#24207;&#21015;&#31070;&#32463;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#26497;&#21270;&#32534;&#30721;&#65288;CRISP&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21463;&#20449;&#24687;&#29702;&#35770;&#21551;&#21457;&#30340;&#26377;&#21407;&#21017;&#30340;&#35838;&#31243;&#26469;&#35757;&#32451;CRISP&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;Polar&#65288;32,16&#65289;&#21644;Polar&#65288;64,22&#65289;&#20195;&#30721;&#19978;&#20248;&#20110;&#36830;&#32493;&#21462;&#28040;(SC)&#35793;&#30721;&#22120;&#24182;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340; &#21487;&#38752;&#24615;&#33021;&#12290;&#25152;&#25552;&#35758;&#30340;&#35838;&#31243;&#36873;&#25321;&#23545;&#20110;&#23454;&#29616;CRISP&#30340;&#20934;&#30830;&#24615;&#22686;&#30410;&#33267;&#20851;&#37325;&#35201;&#65292;&#27491;&#22914;&#25105;&#20204;&#36890;&#36807;&#19982;&#20854;&#20182;&#35838;&#31243;&#30340;&#27604;&#36739;&#25152;&#31034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CRISP&#21487;&#20197;&#36731;&#26494;&#22320;&#25193;&#23637;&#21040;&#26497;&#21270;&#35843;&#25972;&#21367;&#31215;&#65288;PAC&#65289;&#20195;&#30721;&#65292;&#20854;&#20013;&#29616;&#26377;SC&#35299;&#30721;&#22120;&#24615;&#33021;&#25439;&#22833;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polar codes are widely used state-of-the-art codes for reliable communication that have recently been included in the 5th generation wireless standards (5G). However, there remains room for the design of polar decoders that are both efficient and reliable in the short blocklength regime. Motivated by recent successes of data-driven channel decoders, we introduce a novel $\textbf{C}$ur$\textbf{RI}$culum based $\textbf{S}$equential neural decoder for $\textbf{P}$olar codes (CRISP). We design a principled curriculum, guided by information-theoretic insights, to train CRISP and show that it outperforms the successive-cancellation (SC) decoder and attains near-optimal reliability performance on the Polar(32,16) and Polar(64,22) codes. The choice of the proposed curriculum is critical in achieving the accuracy gains of CRISP, as we show by comparing against other curricula. More notably, CRISP can be readily extended to Polarization-Adjusted-Convolutional (PAC) codes, where existing SC decod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19981;&#21464;&#30340;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#29983;&#25104;&#24335;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;transformer-based&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#38750;&#32447;&#24615;&#37327;&#21270;&#26041;&#27861;&#26469;&#23398;&#20064;&#22686;&#24378;&#19981;&#21464;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#65292;&#24182;&#22312;&#35821;&#38899;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2209.15483</link><description>&lt;p&gt;
&#38754;&#21521;&#29983;&#25104;&#24335;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#30340;&#22686;&#24378;&#19981;&#21464;&#31163;&#25955;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling. (arXiv:2209.15483v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19981;&#21464;&#30340;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#29983;&#25104;&#24335;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;transformer-based&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#38750;&#32447;&#24615;&#37327;&#21270;&#26041;&#27861;&#26469;&#23398;&#20064;&#22686;&#24378;&#19981;&#21464;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#65292;&#24182;&#22312;&#35821;&#38899;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20351;&#29992;&#21407;&#22987;&#38899;&#39057;&#35760;&#24405;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#25991;&#26412;&#30417;&#30563;&#12290;&#36825;&#31181;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#20174;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#37327;&#21270;&#24471;&#21040;&#30340;&#31163;&#25955;&#21333;&#20301;&#36827;&#34892;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25913;&#21892;&#31163;&#25955;&#36755;&#20837;&#34920;&#31034;&#23545;&#29983;&#25104;&#24335;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22914;&#20309;&#27979;&#37327;&#36825;&#20123;&#34920;&#31034;&#23545;&#21508;&#31181;&#19981;&#20250;&#25913;&#21464;&#35821;&#38899;&#20449;&#24687;&#65288;&#20363;&#22914;&#26102;&#38388;&#25289;&#20280;&#65289;&#30340;&#20449;&#21495;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#34920;&#31034;&#27169;&#22411;&#32570;&#20047;&#23545;&#27492;&#31867;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#38754;&#21521;&#29983;&#25104;&#24335;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#30340;&#40065;&#26834;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#38024;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#22686;&#24378;&#19981;&#21464;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#40065;&#26834;&#24615;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#35821;&#38899;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Spoken Language Modeling research focuses on optimizing speech Language Models (LMs) using raw audio recordings without accessing any textual supervision. Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Although such units show impressive modeling results, their robustness capabilities have not been extensively investigated. This work focuses on improving the robustness of discrete input representations for generative spoken language modeling. First, we formally define how to measure the robustness of such representations to various signal variations that do not alter the spoken information (e.g., time-stretch). Next, we empirically demonstrate how current state-of-the-art representation models lack robustness to such variations. To overcome this, we propose an effective and efficient method to learn robust discrete speech representation for generative spoken language modeling. The proposed appr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20307;&#36924;&#36817;&#30340;&#26041;&#27861;&#26469;&#20998;&#35299;&#38750;&#36127;&#24352;&#37327;&#65292;&#36890;&#36807;&#33021;&#37327;&#24314;&#27169;&#26469;&#36991;&#20813;&#20840;&#23616;&#20248;&#21270;&#21644;&#30446;&#26631;&#31209;&#36873;&#25321;&#30340;&#22256;&#38590;&#65292;&#21487;&#36890;&#36807;&#32771;&#34385;&#27169;&#24335;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#20840;&#23616;&#20248;&#21270;; &#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#37117;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.15338</link><description>&lt;p&gt;
&#38750;&#36127;&#24352;&#37327;&#30340;&#22810;&#20307;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Many-body Approximation for Non-negative Tensors. (arXiv:2209.15338v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15338
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20307;&#36924;&#36817;&#30340;&#26041;&#27861;&#26469;&#20998;&#35299;&#38750;&#36127;&#24352;&#37327;&#65292;&#36890;&#36807;&#33021;&#37327;&#24314;&#27169;&#26469;&#36991;&#20813;&#20840;&#23616;&#20248;&#21270;&#21644;&#30446;&#26631;&#31209;&#36873;&#25321;&#30340;&#22256;&#38590;&#65292;&#21487;&#36890;&#36807;&#32771;&#34385;&#27169;&#24335;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#20840;&#23616;&#20248;&#21270;; &#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#37117;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#20998;&#35299;&#38750;&#36127;&#24352;&#37327;&#65292;&#31216;&#20026;&#22810;&#20307;&#36924;&#36817;&#12290;&#20256;&#32479;&#30340;&#20998;&#35299;&#26041;&#27861;&#20551;&#35774;&#34920;&#31034;&#20855;&#26377;&#20302;&#31209;&#24615;&#65292;&#23548;&#33268;&#20840;&#23616;&#20248;&#21270;&#21644;&#30446;&#26631;&#31209;&#36873;&#25321;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#36890;&#36807;&#24352;&#37327;&#30340;&#33021;&#37327;&#24314;&#27169;&#36991;&#20813;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20854;&#20013;&#24352;&#37327;&#21644;&#20854;&#27169;&#24335;&#20998;&#21035;&#23545;&#24212;&#20110;&#27010;&#29575;&#20998;&#24067;&#21644;&#38543;&#26426;&#21464;&#37327;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#27169;&#24335;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#36827;&#34892;&#20840;&#23616;&#20248;&#21270;&#65292;&#21487;&#20197;&#27604;&#31209;&#26356;&#30452;&#35266;&#22320;&#36827;&#34892;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27169;&#24335;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21487;&#35270;&#21270;&#20026;&#24352;&#37327;&#32593;&#32476;&#65292;&#25581;&#31034;&#20102;&#22810;&#20307;&#36924;&#36817;&#21644;&#20302;&#31209;&#36924;&#36817;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#24352;&#37327;&#23436;&#25104;&#21644;&#36924;&#36817;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an alternative approach to decompose non-negative tensors, called many-body approximation. Traditional decomposition methods assume low-rankness in the representation, resulting in difficulties in global optimization and target rank selection. We avoid these problems by energy-based modeling of tensors, where a tensor and its mode correspond to a probability distribution and a random variable, respectively. Our model can be globally optimized in terms of the KL divergence minimization by taking the interaction between variables, i.e. modes, into account that can be tuned more intuitively than ranks. Furthermore, we visualize interactions between modes as tensor networks and reveal a nontrivial relationship between many-body approximation and low-rank approximation. We demonstrate the effectiveness of our approach in tensor completion and approximation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#20989;&#25968;&#20462;&#27491;&#30340;&#21322;&#23450;&#35268;&#21010;&#26041;&#27861;&#29992;&#20110;&#24322;&#36136;&#25968;&#25454;&#32858;&#31867;&#12290;&#32463;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#26412;&#26041;&#27861;&#22312;&#22788;&#29702;&#32858;&#31867;&#24418;&#29366;&#19981;&#21516;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2209.15097</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#20989;&#25968;&#20462;&#27491;&#30340;&#21322;&#23450;&#35268;&#21010;&#26041;&#27861;&#29992;&#20110;&#24322;&#36136;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Likelihood Adjusted Semidefinite Programs for Clustering Heterogeneous Data. (arXiv:2209.15097v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#20989;&#25968;&#20462;&#27491;&#30340;&#21322;&#23450;&#35268;&#21010;&#26041;&#27861;&#29992;&#20110;&#24322;&#36136;&#25968;&#25454;&#32858;&#31867;&#12290;&#32463;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#26412;&#26041;&#27861;&#22312;&#22788;&#29702;&#32858;&#31867;&#24418;&#29366;&#19981;&#21516;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#24037;&#20855;&#12290;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#26469;&#22788;&#29702;&#32858;&#31867;&#20855;&#26377;&#19981;&#21516;&#24418;&#29366;&#30340;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#23545;&#20110;&#28151;&#21512;&#20998;&#24067;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#25512;&#26029;&#36890;&#24120;&#28041;&#21450;&#38750;&#20984;&#21644;&#39640;&#32500;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24102;&#26469;&#20102;&#22797;&#26434;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#20284;&#28982;&#20989;&#25968;&#20462;&#27491;&#30340;&#21322;&#23450;&#35268;&#21010;&#65288;LA-SDP&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#24322;&#36136;&#25968;&#25454;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#32452;&#26032;&#30340;&#30697;&#38453;&#19981;&#31561;&#24335;&#23454;&#29616;&#20102;&#20284;&#28982;&#20989;&#25968;&#35843;&#25972;&#30340;&#20984;&#26494;&#24347;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28151;&#21512;&#32452;&#20998;&#30340;&#19968;&#20123;&#28201;&#21644;&#30340;&#21069;&#25552;&#26465;&#20214;&#19979;&#65292;LA-SDP &#21487;&#20197;&#19968;&#33268;&#32780;&#26377;&#25928;&#22320;&#35745;&#31639;&#20986;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23588;&#20854;&#26159;&#24403;&#32858;&#31867;&#26174;&#33879;&#24322;&#36136;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering is a widely deployed unsupervised learning tool. Model-based clustering is a flexible framework to tackle data heterogeneity when the clusters have different shapes. Likelihood-based inference for mixture distributions often involves non-convex and high-dimensional objective functions, imposing difficult computational and statistical challenges. The classic expectation-maximization (EM) algorithm is a computationally thrifty iterative method that maximizes a surrogate function minorizing the log-likelihood of observed data in each iteration, which however suffers from bad local maxima even in the special case of the standard Gaussian mixture model with common isotropic covariance matrices. On the other hand, recent studies reveal that the unique global solution of a semidefinite programming (SDP) relaxed $K$-means achieves the information-theoretically sharp threshold for perfectly recovering the cluster labels under the standard Gaussian mixture model. In this paper, we ext
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29289;&#27969;&#39046;&#22495;&#20856;&#22411;&#30340;&#21368;&#36135;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22870;&#21169;&#22609;&#36896;&#31639;&#27861;&#65292;&#33021;&#22815;&#20943;&#36731;&#23545;&#20195;&#29702;&#20154;&#30340;&#30417;&#30563;&#32423;&#21035;&#65292;&#24182;&#25913;&#21892;RL&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.12350</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#35266;&#23519;&#30340;&#29289;&#27969;&#22330;&#26223;&#19979;&#26426;&#22120;&#20154;&#39034;&#24207;&#21462;&#36135;&#20219;&#21153;&#30340;&#26080;&#30417;&#30563;&#22870;&#21169;&#22609;&#36896;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Reward Shaping for a Robotic Sequential Picking Task from Visual Observations in a Logistics Scenario. (arXiv:2209.12350v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29289;&#27969;&#39046;&#22495;&#20856;&#22411;&#30340;&#21368;&#36135;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22870;&#21169;&#22609;&#36896;&#31639;&#27861;&#65292;&#33021;&#22815;&#20943;&#36731;&#23545;&#20195;&#29702;&#20154;&#30340;&#30417;&#30563;&#32423;&#21035;&#65292;&#24182;&#25913;&#21892;RL&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#29289;&#27969;&#39046;&#22495;&#20856;&#22411;&#30340;&#21368;&#36135;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#39034;&#24207;&#21462;&#36135;&#20219;&#21153;&#27169;&#22411;&#12290;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#65292;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34920;&#29616;&#24471;&#27604;&#20256;&#32479;&#31995;&#32479;&#26356;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#36866;&#24212;&#38543;&#26426;&#24615;&#65292;&#26356;&#33021;&#24212;&#23545;&#22823;&#37327;&#19981;&#30830;&#23450;&#24615;&#12290;&#23588;&#20854;&#26159;&#65292;&#21463;&#30417;&#30563;&#21644;&#27169;&#20223;&#23398;&#20064;&#22312;&#36825;&#26041;&#38754;&#21462;&#24471;&#20102;&#26480;&#20986;&#30340;&#25104;&#26524;&#65292;&#20294;&#32570;&#28857;&#26159;&#38656;&#35201;&#26576;&#31181;&#24418;&#24335;&#30340;&#30417;&#30563;&#65292;&#32780;&#36825;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#24182;&#19981;&#24635;&#26159;&#21487;&#33719;&#24471;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38656;&#35201;&#26356;&#36731;&#24494;&#30340;&#30417;&#30563;&#65292;&#20294;&#20173;&#28982;&#22240;&#20854;&#20302;&#25928;&#32780;&#38590;&#20197;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#22870;&#21169;&#22609;&#36896;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#19987;&#23478;&#30340;&#35266;&#23519;&#26469;&#20943;&#36731;&#23545;&#20195;&#29702;&#20154;&#25152;&#38656;&#30417;&#30563;&#32423;&#21035;&#65292;&#24182;&#25913;&#21892;&#25105;&#20204;&#20219;&#21153;&#30340;RL&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on an unloading problem, typical of the logistics sector, modeled as a sequential pick-and-place task. In this type of task, modern machine learning techniques have shown to work better than classic systems since they are more adaptable to stochasticity and better able to cope with large uncertainties. More specifically, supervised and imitation learning have achieved outstanding results in this regard, with the shortcoming of requiring some form of supervision which is not always obtainable for all settings. On the other hand, reinforcement learning (RL) requires much milder form of supervision but still remains impracticable due to its inefficiency. In this paper, we propose and theoretically motivate a novel Unsupervised Reward Shaping algorithm from expert's observations which relaxes the level of supervision required by the agent and works on improving RL performance in our task.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;smoothed PLDA&#30340;&#31639;&#27861;&#26469;&#26377;&#25928;&#22788;&#29702;&#24191;&#27867;&#30340;&#32467;&#26500;&#21270;&#38750;&#20809;&#28369;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#22797;&#26434;&#24230;&#20026;O(epsilon^(-2/3))&#12290;</title><link>http://arxiv.org/abs/2209.10825</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#30340;&#20840;&#23616;&#25910;&#25947;&#29575;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Global Convergence Rate Analysis of Nonsmooth Nonconvex-Nonconcave Minimax Optimization. (arXiv:2209.10825v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;smoothed PLDA&#30340;&#31639;&#27861;&#26469;&#26377;&#25928;&#22788;&#29702;&#24191;&#27867;&#30340;&#32467;&#26500;&#21270;&#38750;&#20809;&#28369;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#22797;&#26434;&#24230;&#20026;O(epsilon^(-2/3))&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#65288;GDA&#65289;&#31639;&#27861;&#30340;&#21508;&#31181;&#21464;&#20307;&#19978;&#65292;&#36825;&#20123;&#31639;&#27861;&#20165;&#36866;&#29992;&#20110;&#24179;&#28369;&#30340;&#38750;&#20984;&#20985;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21517;&#20026;&#24179;&#28369;&#30340;&#36817;&#31471;&#32447;&#24615;&#19979;&#38477;&#19978;&#21319;&#65288;smoothed PLDA&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#24191;&#27867;&#30340;&#32467;&#26500;&#21270;&#38750;&#20809;&#28369;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#21407;&#22987;&#20989;&#25968;&#20855;&#26377;&#38750;&#20809;&#28369;&#22797;&#21512;&#32467;&#26500;&#65292;&#23545;&#20598;&#20989;&#25968;&#20855;&#26377;Kurdyka-L{o}jasiewicz&#65288;K\L{}&#65289;&#24615;&#36136;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#25947;&#20998;&#26512;&#26694;&#26550;&#26469;&#20998;&#26512;smoothed PLDA&#31639;&#27861;&#65292;&#20854;&#20013;&#20851;&#38190;&#32452;&#20214;&#26159;&#25105;&#20204;&#26368;&#26032;&#24320;&#21457;&#30340;&#38750;&#20809;&#28369;&#21407;&#22987;&#35823;&#24046;&#30028;&#21644;&#23545;&#20598;&#35823;&#24046;&#30028;&#23646;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;smoothed PLDA&#21487;&#20197;&#22312;&#20855;&#26377;&#38750;&#20809;&#28369;&#22797;&#21512;&#21407;&#22987;&#20989;&#25968;&#21644;KL&#23545;&#20598;&#20989;&#25968;&#30340;&#24191;&#27867;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#20013;&#25214;&#21040;$\varepsilon$-game-stationary&#28857;&#21644;$\varepsilon$-&#26368;&#20248;&#21270;&#31283;&#23450;&#28857;&#65292;&#20854;&#22797;&#26434;&#24230;&#20026;$\mathcal{O}(\varepsilon^{-2/3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonconvex-nonconcave minimax optimization has gained widespread interest over the last decade. However, most existing work focuses on variants of gradient descent-ascent (GDA) algorithms, which are only applicable in smooth nonconvex-concave settings. To address this limitation, we propose a novel algorithm named smoothed proximal linear descent-ascent (smoothed PLDA), which can effectively handle a broad range of structured nonsmooth nonconvex-nonconcave minimax problems. Specifically, we consider the setting where the primal function has a nonsmooth composite structure and the dual function possesses the Kurdyka-\L{}ojasiewicz (K\L{}) property with exponent $\theta \in [0,1)$. We introduce a novel convergence analysis framework for smoothed PLDA, the key components of which are our newly developed nonsmooth primal error bound and dual error bound properties. Using this framework, we show that smoothed PLDA can find both $\epsilon$-game-stationary points and $\epsilon$-optimization-st
&lt;/p&gt;</description></item><item><title>MGG&#26159;&#19968;&#31181;&#36719;&#20214;&#27969;&#27700;&#32447;&#35774;&#35745;&#65292;&#21487;&#22312;&#22810;GPU&#24179;&#21488;&#19978;&#21152;&#36895;GNNs&#65292;&#36890;&#36807;&#37319;&#29992;GNN&#29305;&#27530;&#30340;&#27969;&#27700;&#32447;&#26500;&#24314;&#21644;GPU&#24863;&#30693;&#30340;&#27969;&#27700;&#32447;&#26144;&#23556;&#65292;&#23454;&#29616;&#31934;&#32454;&#35745;&#31639;&#36890;&#20449;&#37325;&#21472;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06800</link><description>&lt;p&gt;
MGG: &#22810;GPU&#24179;&#21488;&#19978;&#36890;&#36807;&#31934;&#32454;&#30340;&#20869;&#26680;&#36890;&#20449;&#35745;&#31639;&#27969;&#27700;&#32447;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MGG: Accelerating Graph Neural Networks with Fine-grained intra-kernel Communication-Computation Pipelining on Multi-GPU Platforms. (arXiv:2209.06800v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06800
&lt;/p&gt;
&lt;p&gt;
MGG&#26159;&#19968;&#31181;&#36719;&#20214;&#27969;&#27700;&#32447;&#35774;&#35745;&#65292;&#21487;&#22312;&#22810;GPU&#24179;&#21488;&#19978;&#21152;&#36895;GNNs&#65292;&#36890;&#36807;&#37319;&#29992;GNN&#29305;&#27530;&#30340;&#27969;&#27700;&#32447;&#26500;&#24314;&#21644;GPU&#24863;&#30693;&#30340;&#27969;&#27700;&#32447;&#26144;&#23556;&#65292;&#23454;&#29616;&#31934;&#32454;&#35745;&#31639;&#36890;&#20449;&#37325;&#21472;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36755;&#20837;&#22270;&#30340;&#22823;&#23567;&#36234;&#26469;&#36234;&#22823;&#65292;&#38656;&#35201;&#20351;&#29992;&#22810;GPU&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;GPU GNN&#31995;&#32479;&#20165;&#22522;&#20110;&#20256;&#32479;&#20570;&#27861;&#32553;&#25918;&#31264;&#23494;DNN&#65292;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25805;&#20316;&#65292;&#23545;&#20110;&#19981;&#35268;&#21017;&#31232;&#30095;&#30340;GNN&#24037;&#20316;&#36127;&#36733;&#65292;&#32570;&#22833;&#21516;&#26102;&#35843;&#24230;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25805;&#20316;&#20197;&#25552;&#39640;&#24615;&#33021;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MGG&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#35774;&#35745;&#65292;&#21487;&#20197;&#22312;&#22810;GPU&#24179;&#21488;&#19978;&#21152;&#36895;&#20840;&#22270;GNN&#12290;MGG&#30340;&#26680;&#24515;&#26159;&#20854;&#26032;&#39062;&#30340;&#21160;&#24577;&#36719;&#20214;&#27969;&#27700;&#32447;&#65292;&#20197;&#20419;&#36827;GPU&#20869;&#37096;&#31934;&#32454;&#30340;&#35745;&#31639;&#36890;&#20449;&#37325;&#21472;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MGG&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;GNN&#30340;&#27969;&#27700;&#32447;&#26500;&#24314;&#21644;GPU&#24863;&#30693;&#30340;&#27969;&#27700;&#32447;&#26144;&#23556;&#65292;&#20197;&#20419;&#36827;&#24037;&#20316;&#36127;&#36733;&#24179;&#34913;&#21644;&#25805;&#20316;&#37325;&#21472;&#12290;MGG&#36824;&#32467;&#21512;&#20102;&#26234;&#33021;&#30340;&#36816;&#34892;&#26102;&#35774;&#35745;&#21644;&#20998;&#26512;&#24314;&#27169;&#21644;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#21160;&#24577;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing size of input graphs for graph neural networks (GNNs) highlights the demand for using multi-GPU platforms. However, existing multi-GPU GNN systems optimize the computation and communication individually based on the conventional practice of scaling dense DNNs. For irregularly sparse and fine-grained GNN workloads, such solutions miss the opportunity to jointly schedule/optimize the computation and communication operations for high-performance delivery. To this end, we propose MGG, a novel system design to accelerate full-graph GNNs on multi-GPU platforms. The core of MGG is its novel dynamic software pipeline to facilitate fine-grained computation-communication overlapping within a GPU kernel. Specifically, MGG introduces GNN-tailored pipeline construction and GPU-aware pipeline mapping to facilitate workload balancing and operation overlapping. MGG also incorporates an intelligent runtime design with analytical modeling and optimization heuristics to dynamically improve
&lt;/p&gt;</description></item><item><title>AnaMeta&#26159;&#19968;&#20010;&#21253;&#21547;467k&#24352;&#34920;&#26684;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#25552;&#20379;&#20102;&#24120;&#29992;&#23383;&#27573;&#20803;&#25968;&#25454;&#30340;&#22235;&#31181;&#34893;&#29983;&#30417;&#30563;&#26631;&#31614;&#65292;&#21516;&#26102;&#25512;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#32534;&#30721;&#22120;&#26694;&#26550;&#65288;KDF&#65289;&#25552;&#39640;&#20102;&#34920;&#26684;&#27169;&#22411;&#30340;&#20803;&#25968;&#25454;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22235;&#20010;&#25509;&#21475;&#23558;&#20803;&#25968;&#25454;&#32435;&#20837;&#19979;&#28216;&#20998;&#26512;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2209.00946</link><description>&lt;p&gt;
AnaMeta&#65306;&#22810;&#32500;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20849;&#20139;&#30340;&#34920;&#26684;&#20803;&#25968;&#25454;&#30693;&#35782;&#30340;&#29702;&#35299;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
AnaMeta: A Table Understanding Dataset of Field Metadata Knowledge Shared by Multi-dimensional Data Analysis Tasks. (arXiv:2209.00946v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00946
&lt;/p&gt;
&lt;p&gt;
AnaMeta&#26159;&#19968;&#20010;&#21253;&#21547;467k&#24352;&#34920;&#26684;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#25552;&#20379;&#20102;&#24120;&#29992;&#23383;&#27573;&#20803;&#25968;&#25454;&#30340;&#22235;&#31181;&#34893;&#29983;&#30417;&#30563;&#26631;&#31614;&#65292;&#21516;&#26102;&#25512;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#32534;&#30721;&#22120;&#26694;&#26550;&#65288;KDF&#65289;&#25552;&#39640;&#20102;&#34920;&#26684;&#27169;&#22411;&#30340;&#20803;&#25968;&#25454;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22235;&#20010;&#25509;&#21475;&#23558;&#20803;&#25968;&#25454;&#32435;&#20837;&#19979;&#28216;&#20998;&#26512;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#22312;&#21508;&#20010;&#39046;&#22495;&#27599;&#22825;&#37117;&#22312;&#36827;&#34892;&#65292;&#23427;&#38656;&#35201;&#20934;&#30830;&#22320;&#29702;&#35299;&#23383;&#27573;&#35821;&#20041;&#25165;&#33021;&#27491;&#30830;&#22320;&#25805;&#20316;&#34920;&#26684;&#23383;&#27573;&#24182;&#22312;&#26085;&#24120;&#20998;&#26512;&#20013;&#25214;&#21040;&#20849;&#21516;&#27169;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AnaMeta&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;467k&#24352;&#34920;&#26684;&#20197;&#21450;&#24120;&#29992;&#23383;&#27573;&#20803;&#25968;&#25454;&#30340;&#22235;&#31181;&#34893;&#29983;&#30417;&#30563;&#26631;&#31614;&#65306;&#34913;&#37327;/&#32500;&#24230;&#20108;&#20998;&#27861;&#12289;&#24120;&#35265;&#23383;&#27573;&#35282;&#33394;&#12289;&#35821;&#20041;&#23383;&#27573;&#31867;&#22411;&#21644;&#40664;&#35748;&#32858;&#21512;&#20989;&#25968;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#27169;&#22411;&#26469;&#25512;&#26029;&#20803;&#25968;&#25454;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KDF&#30340;&#22810;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#24067;&#21644;&#30693;&#35782;&#20449;&#24687;&#26469;&#25552;&#39640;&#34920;&#26684;&#27169;&#22411;&#30340;&#20803;&#25968;&#25454;&#29702;&#35299;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#25509;&#21475;&#65292;&#23558;&#23383;&#27573;&#20803;&#25968;&#25454;&#32435;&#20837;&#19979;&#28216;&#20998;&#26512;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data analysis is performed every day across various domains. It requires an accurate understanding of field semantics to correctly operate on table fields and find common patterns in daily analysis. In this paper, we introduce the AnaMeta dataset, a collection of 467k tables with derived supervision labels for four types of commonly used field metadata: measure/dimension dichotomy, common field roles, semantic field type, and default aggregation function. We evaluate a wide range of models for inferring metadata as the benchmark. We also propose a multi-encoder framework, called KDF, which improves the metadata understanding capability of tabular models by incorporating distribution and knowledge information. Furthermore, we propose four interfaces for incorporating field metadata into downstream analysis tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#20248;&#21270;&#25490;&#21517;&#21644;&#26657;&#20934;&#33021;&#21147;&#30340;&#26041;&#27861;JRC&#65292;&#36890;&#36807;&#23545;&#27604;&#36755;&#20986;logit&#20540;&#26469;&#25552;&#39640;&#25490;&#21517;&#33021;&#21147;&#21644;&#26657;&#20934;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.06164</link><description>&lt;p&gt;
&#34701;&#21512;&#32972;&#26223;&#30340;&#28151;&#21512;&#27169;&#22411;&#19979;&#30340;&#25490;&#21517;&#21644;&#26657;&#20934;&#32852;&#21512;&#20248;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model. (arXiv:2208.06164v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#20248;&#21270;&#25490;&#21517;&#21644;&#26657;&#20934;&#33021;&#21147;&#30340;&#26041;&#27861;JRC&#65292;&#36890;&#36807;&#23545;&#27604;&#36755;&#20986;logit&#20540;&#26469;&#25552;&#39640;&#25490;&#21517;&#33021;&#21147;&#21644;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25490;&#21517;&#20248;&#21270;&#25216;&#26415;&#24471;&#21040;&#20102;&#21457;&#23637;&#65292;&#28857;&#23545;&#28857;&#25439;&#22833;&#20173;&#28982;&#26159;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#36825;&#21487;&#20197;&#24402;&#22240;&#20110;&#28857;&#23545;&#28857;&#25439;&#22833;&#30340;&#26657;&#20934;&#33021;&#21147;&#65292;&#22240;&#20026;&#39044;&#27979;&#21487;&#20197;&#34987;&#35270;&#20026;&#28857;&#20987;&#27010;&#29575;&#12290;&#23454;&#38469;&#19978;&#65292;CTR&#39044;&#27979;&#27169;&#22411;&#20063;&#36890;&#24120;&#36890;&#36807;&#25490;&#21517;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#20248;&#21270;&#25490;&#21517;&#33021;&#21147;&#65292;&#21487;&#20197;&#37319;&#29992;&#25490;&#21517;&#25439;&#22833;&#65288;&#20363;&#22914;&#25104;&#23545;&#25110;&#21015;&#34920;&#25439;&#22833;&#65289;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#27604;&#28857;&#23545;&#28857;&#25439;&#22833;&#23454;&#29616;&#26356;&#22909;&#30340;&#25490;&#21517;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#30452;&#25509;&#23558;&#20004;&#31181;&#25439;&#22833;&#32452;&#21512;&#36215;&#26469;&#20197;&#33719;&#24471;&#20004;&#31181;&#25439;&#22833;&#30340;&#30410;&#22788;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#25171;&#30772;&#20102;&#36755;&#20986;logit&#20316;&#20026;&#28857;&#20987;&#29575;&#30340;&#21547;&#20041;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25490;&#21517;&#21644;&#26657;&#20934;&#33021;&#21147;&#65288;&#31616;&#31216;JRC&#65289;&#12290;JRC&#36890;&#36807;&#23545;&#36755;&#20986;logit&#20540;&#36827;&#34892;&#23545;&#27604;&#26469;&#25552;&#39640;&#25490;&#21517;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the development of ranking optimization techniques, pointwise loss remains the dominating approach for click-through rate prediction. It can be attributed to the calibration ability of the pointwise loss since the prediction can be viewed as the click probability. In practice, a CTR prediction model is also commonly assessed with the ranking ability. To optimize the ranking ability, ranking loss (e.g., pairwise or listwise loss) can be adopted as they usually achieve better rankings than pointwise loss. Previous studies have experimented with a direct combination of the two losses to obtain the benefit from both losses and observed an improved performance. However, previous studies break the meaning of output logit as the click-through rate, which may lead to sub-optimal solutions. To address this issue, we propose an approach that can Jointly optimize the Ranking and Calibration abilities (JRC for short). JRC improves the ranking ability by contrasting the logit value for the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#25209;&#37327;&#35757;&#32451;&#20013;&#20855;&#26377;&#39640;&#24230;&#40065;&#26834;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#32447;&#24615;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22343;&#33021;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#20272;&#35745;&#36895;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#36827;&#34892;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2208.05447</link><description>&lt;p&gt;
&#27169;&#22411;&#35757;&#32451;&#20013;&#40065;&#26834;&#24615;&#39640;&#30340;&#39640;&#32500;&#32447;&#24615;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Methods for High-Dimensional Linear Learning. (arXiv:2208.05447v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#25209;&#37327;&#35757;&#32451;&#20013;&#20855;&#26377;&#39640;&#24230;&#40065;&#26834;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#32447;&#24615;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22343;&#33021;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#20272;&#35745;&#36895;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#36827;&#34892;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32500;&#25209;&#22788;&#29702;&#20013;&#20855;&#26377;&#32479;&#35745;&#40065;&#26834;&#24615;&#21644;&#35745;&#31639;&#26377;&#25928;&#24615;&#30340;&#32447;&#24615;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#29305;&#24449;&#25968;d&#21487;&#33021;&#36229;&#36807;&#26679;&#26412;&#25968;n&#12290;&#25105;&#20204;&#22312;&#36890;&#29992;&#23398;&#20064;&#35774;&#32622;&#20013;&#37319;&#29992;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#21462;&#20915;&#20110;&#25152;&#32771;&#34385;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#21542;&#26159;&#26799;&#24230;Lipschitz&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#20363;&#21270;&#21040;&#20960;&#20010;&#24212;&#29992;&#31243;&#24207;&#19978;&#65292;&#21253;&#25324;&#39321;&#33609;&#31232;&#30095;&#65292;&#32452;&#31232;&#30095;&#21644;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#12290;&#36825;&#23548;&#33268;&#20102;&#27599;&#20010;&#24212;&#29992;&#31243;&#24207;&#30340;&#39640;&#25928;&#21644;&#40065;&#26834;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#37325;&#23614;&#20998;&#24067;&#21644;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#20272;&#35745;&#36895;&#29575;&#12290;&#23545;&#20110;&#39321;&#33609;$s$-&#31232;&#30095;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#37325;&#23614;&#21644;$\eta$-&#27745;&#26579;&#19979;&#36798;&#21040;$s\log(d)/n$&#30340;&#36895;&#29575;&#65292;&#35745;&#31639;&#25104;&#26412;&#19982;&#38750;&#40065;&#26834;&#27169;&#25311;&#30456;&#24403;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;$\mathtt{Python}$&#24211;$\mathtt{linlearn}$&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36825;&#20010;&#24211;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose statistically robust and computationally efficient linear learning methods in the high-dimensional batch setting, where the number of features $d$ may exceed the sample size $n$. We employ, in a generic learning setting, two algorithms depending on whether the considered loss function is gradient-Lipschitz or not. Then, we instantiate our framework on several applications including vanilla sparse, group-sparse and low-rank matrix recovery. This leads, for each application, to efficient and robust learning algorithms, that reach near-optimal estimation rates under heavy-tailed distributions and the presence of outliers. For vanilla $s$-sparsity, we are able to reach the $s\log (d)/n$ rate under heavy-tails and $\eta$-corruption, at a computational cost comparable to that of non-robust analogs. We provide an efficient implementation of our algorithms in an open-source $\mathtt{Python}$ library called $\mathtt{linlearn}$, by means of which we carry out numerical experiments whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#21435;&#22122;&#22312;&#27969;&#24418;&#20551;&#35774;&#19979;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#39318;&#27425;&#25299;&#23637;&#21040;&#20102;&#30446;&#26631;&#20998;&#24067;&#21463;&#27969;&#24418;&#32422;&#26463;&#25110;&#36890;&#36807;&#32463;&#39564;&#20998;&#24067;&#32473;&#20986;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2208.05314</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#21435;&#22122;&#22312;&#27969;&#24418;&#20551;&#35774;&#19979;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence of denoising diffusion models under the manifold hypothesis. (arXiv:2208.05314v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#21435;&#22122;&#22312;&#27969;&#24418;&#20551;&#35774;&#19979;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#39318;&#27425;&#25299;&#23637;&#21040;&#20102;&#30446;&#26631;&#20998;&#24067;&#21463;&#27969;&#24418;&#32422;&#26463;&#25110;&#36890;&#36807;&#32463;&#39564;&#20998;&#24067;&#32473;&#20986;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#27969;&#27169;&#22411;&#26159;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#22270;&#20687;&#21644;&#38899;&#39057;&#21512;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#36817;&#20284;&#20110;&#20174;&#30446;&#26631;&#20998;&#24067;&#21040;&#21442;&#32771;&#23494;&#24230;&#65288;&#36890;&#24120;&#20026;&#39640;&#26031;&#20998;&#24067;&#65289;&#30340;&#27491;&#21521;&#22122;&#22768;&#36807;&#31243;&#30340;&#26102;&#38388;&#21453;&#28436;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#20294;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#29702;&#35770;&#20998;&#26512;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#22320;&#65292;&#25152;&#26377;&#24403;&#21069;&#30340;&#26041;&#27861;&#37117;&#20851;&#38190;&#22320;&#20551;&#35774;&#30446;&#26631;&#20998;&#24067;&#30456;&#23545;&#20110;&#21202;&#36125;&#26684;&#27979;&#24230;&#23384;&#22312;&#23494;&#24230;&#12290;&#36825;&#19981;&#28085;&#30422;&#30446;&#26631;&#20998;&#24067;&#21463;&#20302;&#32500;&#27969;&#24418;&#32422;&#26463;&#25110;&#36890;&#36807;&#26576;&#20123;&#32463;&#39564;&#20998;&#24067;&#32473;&#20986;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#27969;&#27169;&#22411;&#22312;&#36825;&#31181;&#26356;&#21152;&#26222;&#36941;&#30340;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#24182;&#25552;&#20379;&#19968;&#38454;Wasserstein&#36317;&#31163;&#37327;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models are a recent class of generative models exhibiting state-of-the-art performance in image and audio synthesis. Such models approximate the time-reversal of a forward noising process from a target distribution to a reference density, which is usually Gaussian. Despite their strong empirical results, the theoretical analysis of such models remains limited. In particular, all current approaches crucially assume that the target density admits a density w.r.t. the Lebesgue measure. This does not cover settings where the target distribution is supported on a lower-dimensional manifold or is given by some empirical distribution. In this paper, we bridge this gap by providing the first convergence results for diffusion models in this more general setting. In particular, we provide quantitative bounds on the Wasserstein distance of order one between the target data distribution and the generative distribution of the diffusion model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24515;&#33039;&#30142;&#30149;&#26465;&#20214;&#23398;&#20064;&#20840;&#26223;&#24515;&#30005;&#22270;&#34920;&#31034;&#30340;&#22810;&#35270;&#35282;ECG&#21512;&#25104;&#26041;&#27861;ME-GAN&#65292;&#21033;&#29992;&#26032;&#30340;&#28151;&#21512;&#24402;&#19968;&#21270;&#26041;&#27861;&#23558;&#30142;&#30149;&#20449;&#24687;&#27880;&#20837;&#21040;&#21512;&#36866;&#20301;&#32622;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#21644;&#30142;&#30149;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2207.10670</link><description>&lt;p&gt;
ME-GAN&#65306;&#22522;&#20110;&#24515;&#33039;&#30142;&#30149;&#26465;&#20214;&#23398;&#20064;&#20840;&#26223;&#24515;&#30005;&#22270;&#34920;&#31034;&#30340;&#22810;&#35270;&#35282;ECG&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
ME-GAN: Learning Panoptic Electrocardio Representations for Multi-view ECG Synthesis Conditioned on Heart Diseases. (arXiv:2207.10670v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24515;&#33039;&#30142;&#30149;&#26465;&#20214;&#23398;&#20064;&#20840;&#26223;&#24515;&#30005;&#22270;&#34920;&#31034;&#30340;&#22810;&#35270;&#35282;ECG&#21512;&#25104;&#26041;&#27861;ME-GAN&#65292;&#21033;&#29992;&#26032;&#30340;&#28151;&#21512;&#24402;&#19968;&#21270;&#26041;&#27861;&#23558;&#30142;&#30149;&#20449;&#24687;&#27880;&#20837;&#21040;&#21512;&#36866;&#20301;&#32622;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#21644;&#30142;&#30149;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;( ECG)&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#65292;&#29992;&#20110;&#26816;&#27979;&#24515;&#33039;&#30142;&#30149;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35774;&#35745;&#20102;ECG&#20998;&#26512;&#27169;&#22411;(&#22914;&#20998;&#31867;&#22120;)&#26469;&#36741;&#21161;&#35786;&#26029;&#12290;&#20316;&#20026;&#19968;&#20010;&#19978;&#28216;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#24314;&#31435;&#29983;&#25104;&#27169;&#22411;&#26469;&#21512;&#25104;ECG&#25968;&#25454;&#65292;&#36825;&#26377;&#21033;&#20110;&#25552;&#20379;&#35757;&#32451;&#26679;&#26412;&#12289;&#38544;&#31169;&#20445;&#25252;&#21644;&#27880;&#37322;&#20943;&#23569;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;ECG&#29983;&#25104;&#26041;&#27861;&#24448;&#24448;&#26082;&#27809;&#26377;&#21512;&#25104;&#22810;&#35270;&#22270;&#25968;&#25454;&#65292;&#20063;&#27809;&#26377;&#22788;&#29702;&#24515;&#33039;&#30142;&#30149;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20197;&#30142;&#30149;&#20026;&#23548;&#21521;&#30340;&#22810;&#35270;&#35282;ECG&#21512;&#25104;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;ME-GAN&#65292;&#23427;&#33719;&#24471;&#20102;&#22522;&#20110;&#24515;&#33039;&#30142;&#30149;&#26465;&#20214;&#30340;&#20840;&#26223;&#24515;&#30005;&#22270;&#34920;&#31034;&#65292;&#24182;&#23558;&#36825;&#20123;&#34920;&#31034;&#25237;&#24433;&#21040;&#22810;&#20010;&#26631;&#20934;&#35270;&#22270;&#19978;&#20197;&#20135;&#29983;ECG&#20449;&#21495;&#12290;&#30001;&#20110;&#24515;&#33039;&#30142;&#30149;&#30340;ECG&#34920;&#29616;&#36890;&#24120;&#23616;&#37096;&#21270;&#22312;&#29305;&#23450;&#30340;&#27874;&#24418;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;"&#28151;&#21512;&#24402;&#19968;&#21270;"&#26041;&#27861;&#65292;&#20197;&#23558;&#30142;&#30149;&#20449;&#24687;&#31934;&#30830;&#22320;&#27880;&#20837;&#21040;&#21512;&#36866;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram (ECG) is a widely used non-invasive diagnostic tool for heart diseases. Many studies have devised ECG analysis models (e.g., classifiers) to assist diagnosis. As an upstream task, researches have built generative models to synthesize ECG data, which are beneficial to providing training samples, privacy protection, and annotation reduction. However, previous generative methods for ECG often neither synthesized multi-view data, nor dealt with heart disease conditions. In this paper, we propose a novel disease-aware generative adversarial network for multi-view ECG synthesis called ME-GAN, which attains panoptic electrocardio representations conditioned on heart diseases and projects the representations onto multiple standard views to yield ECG signals. Since ECG manifestations of heart diseases are often localized in specific waveforms, we propose a new "mixup normalization" to inject disease information precisely into suitable locations. In addition, we propose a view 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#35889;&#22270;&#23618;&#32423;&#36716;&#25442;&#22120;&#23545;&#21683;&#22013;&#25110;&#21628;&#21560;&#22768;&#30340;&#38899;&#39057;&#35760;&#24405;&#36827;&#34892;COVID-19&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2207.09529</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#22270;&#23618;&#32423;&#36716;&#25442;&#22120;&#30340;&#21628;&#21560;&#22768;COVID-19&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
COVID-19 Detection from Respiratory Sounds with Hierarchical Spectrogram Transformers. (arXiv:2207.09529v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#35889;&#22270;&#23618;&#32423;&#36716;&#25442;&#22120;&#23545;&#21683;&#22013;&#25110;&#21628;&#21560;&#22768;&#30340;&#38899;&#39057;&#35760;&#24405;&#36827;&#34892;COVID-19&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#24615;&#31354;&#27668;&#20256;&#25773;&#30142;&#30149;&#65292;&#22914;COVID-19&#30340;&#30417;&#27979;&#36890;&#24120;&#28041;&#21450;&#21628;&#21560;&#35780;&#20272;&#12290;&#34429;&#28982;&#21548;&#35786;&#26159;&#21021;&#27493;&#31579;&#26597;&#30142;&#30149;&#30151;&#29366;&#30340;&#20027;&#27969;&#26041;&#27861;&#65292;&#20294;&#38656;&#35201;&#19987;&#38376;&#30340;&#21307;&#38498;&#35775;&#38382;&#65292;&#38480;&#21046;&#20102;&#20854;&#25928;&#29992;&#12290;&#22522;&#20110;&#20415;&#25658;&#24335;&#35774;&#22791;&#19978;&#21628;&#21560;&#22768;&#35760;&#24405;&#30340;&#36828;&#31243;&#30417;&#27979;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#21327;&#21161;&#26089;&#26399;&#35780;&#20272;&#39318;&#35201;&#24433;&#21709;&#19979;&#21628;&#21560;&#36947;&#30340;COVID-19&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21683;&#22013;&#25110;&#21628;&#21560;&#22768;&#30340;&#38899;&#39057;&#35760;&#24405;&#21306;&#20998;COVID-19&#24739;&#32773;&#21644;&#20581;&#24247;&#23545;&#29031;&#32452;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#21628;&#21560;&#22768;&#30340;&#35889;&#22270;&#34920;&#31034;&#20013;&#30340;&#19968;&#31181;&#26032;&#22411;&#35889;&#22270;&#23618;&#32423;&#36716;&#25442;&#22120;&#65288;HST&#65289;&#12290; HST&#22312;&#35889;&#22270;&#20013;&#30340;&#23616;&#37096;&#31383;&#21475;&#19978;&#20855;&#26377;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#65292;&#24182;&#19988;&#31383;&#21475;&#22823;&#23567;&#38543;&#30528;&#27169;&#22411;&#38454;&#27573;&#30340;&#36880;&#28176;&#22686;&#38271;&#32780;&#36880;&#28176;&#22686;&#21152;&#65292;&#20197;&#25429;&#33719;&#26412;&#22320;&#21040;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290; HST&#19982;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring of prevalent airborne diseases such as COVID-19 characteristically involves respiratory assessments. While auscultation is a mainstream method for preliminary screening of disease symptoms, its utility is hampered by the need for dedicated hospital visits. Remote monitoring based on recordings of respiratory sounds on portable devices is a promising alternative, which can assist in early assessment of COVID-19 that primarily affects the lower respiratory tract. In this study, we introduce a novel deep learning approach to distinguish patients with COVID-19 from healthy controls given audio recordings of cough or breathing sounds. The proposed approach leverages a novel hierarchical spectrogram transformer (HST) on spectrogram representations of respiratory sounds. HST embodies self-attention mechanisms over local windows in spectrograms, and window size is progressively grown over model stages to capture local to global context. HST is compared against state-of-the-art conve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#23558;&#31526;&#21495;&#22238;&#24402;&#25193;&#23637;&#21040;&#21442;&#25968;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;ODE &#21644; PDE&#65292;&#35777;&#26126;&#20854;&#22312;&#35757;&#32451;&#22495;&#20043;&#22806;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#36825;&#31181;&#20307;&#31995;&#32467;&#26500;&#36824;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#23545;&#39640;&#32500;&#25968;&#25454;&#36827;&#34892;&#31471;&#23545;&#31471;&#35757;&#32451;&#65292;&#24182;&#21487;&#29992;&#20110;&#20998;&#26512;&#22270;&#20687;&#31561;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2207.00529</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#22238;&#24402;&#21457;&#29616;&#21442;&#25968;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep Learning and Symbolic Regression for Discovering Parametric Equations. (arXiv:2207.00529v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#23558;&#31526;&#21495;&#22238;&#24402;&#25193;&#23637;&#21040;&#21442;&#25968;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;ODE &#21644; PDE&#65292;&#35777;&#26126;&#20854;&#22312;&#35757;&#32451;&#22495;&#20043;&#22806;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#36825;&#31181;&#20307;&#31995;&#32467;&#26500;&#36824;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#23545;&#39640;&#32500;&#25968;&#25454;&#36827;&#34892;&#31471;&#23545;&#31471;&#35757;&#32451;&#65292;&#24182;&#21487;&#29992;&#20110;&#20998;&#26512;&#22270;&#20687;&#31561;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#25968;&#25454;&#30340;&#25511;&#21046;&#20844;&#24335;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#25913;&#21464;&#31185;&#23398;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#31526;&#21495;&#22238;&#24402;&#22312;&#20998;&#26512;&#22797;&#26434;&#24230;&#21644;&#32500;&#24230;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#36890;&#36807;&#20854;&#20998;&#26512;&#26497;&#20854;&#22797;&#26434;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#23558;&#31526;&#21495;&#22238;&#24402;&#25193;&#23637;&#21040;&#21442;&#25968;&#31995;&#32479;&#65292;&#20854;&#20013;&#19968;&#20123;&#31995;&#25968;&#21487;&#33021;&#20250;&#21464;&#21270;&#65292;&#20294;&#28508;&#22312;&#30340;&#25511;&#21046;&#26041;&#31243;&#30340;&#32467;&#26500;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;ODE &#21644; PDE &#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31995;&#25968;&#21464;&#21270;&#19981;&#21516;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#35757;&#32451;&#22495;&#20043;&#22806;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20307;&#31995;&#32467;&#26500;&#36824;&#21487;&#20197;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#38598;&#25104;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#36827;&#34892;&#31471;&#23545;&#31471;&#35757;&#32451;&#30340;&#21516;&#26102;&#20998;&#26512;&#39640;&#32500;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#19982;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#65292;&#20197;&#20998;&#26512;&#26469;&#33258;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#19968;&#32452;&#22270;&#20687;&#65292;&#24182;&#21457;&#29616;&#30456;&#24212;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression is a machine learning technique that can learn the governing formulas of data and thus has the potential to transform scientific discovery. However, symbolic regression is still limited in the complexity and dimensionality of the systems that it can analyze. Deep learning on the other hand has transformed machine learning in its ability to analyze extremely complex and high-dimensional datasets. We propose a neural network architecture to extend symbolic regression to parametric systems where some coefficient may vary but the structure of the underlying governing equation remains constant. We demonstrate our method on various analytic expressions, ODEs, and PDEs with varying coefficients and show that it extrapolates well outside of the training domain. The neural network-based architecture can also integrate with other deep learning architectures so that it can analyze high-dimensional data while being trained end-to-end. To this end we integrate our architecture w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#27169;&#22411;&#39537;&#21160;&#30340;RL&#26694;&#26550;&#65292;&#23558;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#21160;&#21147;&#23398;&#23398;&#20064;&#20998;&#31163;&#65292;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#21644;&#28508;&#22312;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#20934;&#30830;&#24314;&#27169;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2206.14244</link><description>&lt;p&gt;
&#21487;&#25513;&#34109;&#19990;&#30028;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Masked World Models for Visual Control. (arXiv:2206.14244v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#27169;&#22411;&#39537;&#21160;&#30340;RL&#26694;&#26550;&#65292;&#23558;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#21160;&#21147;&#23398;&#23398;&#20064;&#20998;&#31163;&#65292;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#21644;&#28508;&#22312;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#20934;&#30830;&#24314;&#27169;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26377;&#28508;&#21147;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#35270;&#35273;&#35266;&#27979;&#20013;&#23454;&#29616;&#26679;&#26412;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#21333;&#19968;&#27169;&#22411;&#26469;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#21644;&#21160;&#21147;&#23398;&#65292;&#20351;&#24471;&#20934;&#30830;&#24314;&#27169;&#26426;&#22120;&#20154;&#19982;&#23567;&#29289;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#35270;&#35273;&#27169;&#22411;&#39537;&#21160;&#30340;RL&#26694;&#26550;&#65292;&#23558;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#21160;&#21147;&#23398;&#23398;&#20064;&#20998;&#31163;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#21367;&#31215;&#23618;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#26469;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#65292;&#20197;&#22312;&#32473;&#23450;&#25513;&#34109;&#21367;&#31215;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#37325;&#26500;&#20687;&#32032;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010;&#25805;&#20316;&#33258;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;&#28508;&#22312;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#32534;&#30721;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#22870;&#21169;&#39044;&#27979;&#30446;&#26631;&#26469;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#29615;&#22659;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#22312;&#32447;&#26679;&#26412;&#19981;&#26029;&#26356;&#26032;&#33258;&#32534;&#30721;&#22120;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#31163;&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual model-based reinforcement learning (RL) has the potential to enable sample-efficient robot learning from visual observations. Yet the current approaches typically train a single model end-to-end for learning both visual representations and dynamics, making it difficult to accurately model the interaction between robots and small objects. In this work, we introduce a visual model-based RL framework that decouples visual representation learning and dynamics learning. Specifically, we train an autoencoder with convolutional layers and vision transformers (ViT) to reconstruct pixels given masked convolutional features, and learn a latent dynamics model that operates on the representations from the autoencoder. Moreover, to encode task-relevant information, we introduce an auxiliary reward prediction objective for the autoencoder. We continually update both autoencoder and dynamics model using online samples collected from environment interaction. We demonstrate that our decoupling a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#32852;&#21512;&#32452;&#21512;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20195;&#29702;&#22312;&#20849;&#21516;&#23398;&#20064;&#26102;&#22914;&#20309;&#20445;&#25345;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#21518;&#24724;&#21644;&#38544;&#31169;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.13192</link><description>&lt;p&gt;
&#24102;&#32422;&#26463;&#26465;&#20214;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#21512;&#32452;&#21512;&#36172;&#21338;&#26426;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Federated Combinatorial Bandits with Constraints. (arXiv:2206.13192v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#32852;&#21512;&#32452;&#21512;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20195;&#29702;&#22312;&#20849;&#21516;&#23398;&#20064;&#26102;&#22914;&#20309;&#20445;&#25345;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#21518;&#24724;&#21644;&#38544;&#31169;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#23398;&#20064;&#27169;&#24335;&#20013;&#65292;&#21512;&#20316;&#23398;&#20064;&#33539;&#24335;&#65288;&#21363;&#32852;&#37030;&#23398;&#20064;&#65289;&#24555;&#36895;&#22686;&#38271;&#12290;&#19982;&#22823;&#22810;&#25968;&#32852;&#37030;&#23398;&#20064;&#24773;&#26223;&#19981;&#21516;&#30340;&#26159;&#65292;&#26377;&#24456;&#22810;&#24773;&#20917;&#19979;&#20195;&#29702;&#26159;&#31454;&#20105;&#30340;&#12290;&#27599;&#20010;&#20195;&#29702;&#37117;&#24819;&#20174;&#20854;&#20182;&#20154;&#37027;&#37324;&#23398;&#20064;&#65292;&#20294;&#23427;&#20998;&#20139;&#32473;&#20854;&#20182;&#20154;&#23398;&#20064;&#30340;&#20449;&#24687;&#26377;&#21487;&#33021;&#26159;&#25935;&#24863;&#30340;&#65292;&#22240;&#27492;&#23427;&#38656;&#35201;&#38544;&#31169;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#32452;&#20195;&#29702;&#21516;&#26102;&#35299;&#20915;&#31867;&#20284;&#30340;&#32452;&#21512;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#36136;&#37327;&#32422;&#26463;&#12290;&#36825;&#20123;&#20195;&#29702;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#26469;&#20445;&#25345;&#26426;&#23494;&#24615;&#65292;&#38598;&#20307;&#23398;&#20064;&#65311;&#25105;&#20204;&#35266;&#23519;&#21040;&#36890;&#20449;&#21487;&#20197;&#38477;&#20302;&#21518;&#24724;&#12290;&#20294;&#26159;&#65292;&#20445;&#25252;&#25935;&#24863;&#20449;&#24687;&#30340;&#24046;&#20998;&#38544;&#31169;&#25216;&#26415;&#20351;&#25968;&#25454;&#21464;&#24471;&#24456;&#22024;&#26434;&#65292;&#21487;&#33021;&#20250;&#24694;&#21270;&#32780;&#19981;&#26159;&#26377;&#24110;&#21161;&#22320;&#25552;&#39640;&#21518;&#24724;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25351;&#20986;&#20915;&#23450;&#20309;&#26102;&#36890;&#20449;&#20197;&#21450;&#23398;&#20064;&#21738;&#20123;&#20849;&#20139;&#25968;&#25454;&#26469;&#22312;&#21518;&#24724;&#21644;&#38544;&#31169;&#20043;&#38388;&#23454;&#29616;&#21151;&#33021;&#24179;&#34913;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a rapid increase in the cooperative learning paradigm in online learning settings, i.e., federated learning (FL). Unlike most FL settings, there are many situations where the agents are competitive. Each agent would like to learn from others, but the part of the information it shares for others to learn from could be sensitive; thus, it desires its privacy. This work investigates a group of agents working concurrently to solve similar combinatorial bandit problems while maintaining quality constraints. Can these agents collectively learn while keeping their sensitive information confidential by employing differential privacy? We observe that communicating can reduce the regret. However, differential privacy techniques for protecting sensitive information makes the data noisy and may deteriorate than help to improve regret. Hence, we note that it is essential to decide when to communicate and what shared data to learn to strike a functional balance between regret and privacy. F
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550; FedAMD&#65292;&#20854;&#20013;&#26680;&#24515;&#24605;&#24819;&#26159;&#38170;&#23450;&#25277;&#26679;&#65292;&#23558;&#21442;&#19982;&#32773;&#20998;&#20026;&#38170;&#23450;&#32452;&#21644;&#30719;&#24037;&#32452;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#24322;&#26500;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.05891</link><description>&lt;p&gt;
&#38024;&#23545;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#38170;&#23450;&#25277;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Anchor Sampling for Federated Learning with Partial Client Participation. (arXiv:2206.05891v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550; FedAMD&#65292;&#20854;&#20013;&#26680;&#24515;&#24605;&#24819;&#26159;&#38170;&#23450;&#25277;&#26679;&#65292;&#23558;&#21442;&#19982;&#32773;&#20998;&#20026;&#38170;&#23450;&#32452;&#21644;&#30719;&#24037;&#32452;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#36739;&#20110;&#20840;&#23458;&#25143;&#31471;&#21442;&#19982;&#65292;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#26356;&#24120;&#35265;&#30340;&#22330;&#26223;&#65292;&#20294;&#26159;&#20250;&#21152;&#37325;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#25968;&#25454;&#24322;&#26500;&#24615;&#12290;&#22312;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#32570;&#23569;&#38750;&#27963;&#21160;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#32858;&#21512;&#20559;&#31163;&#22522;&#20110;&#20840;&#23458;&#25143;&#31471;&#21442;&#19982;&#30340;&#32858;&#21512;&#12290;&#36890;&#24120;&#25552;&#20986;&#37319;&#29992;&#22312;&#20010;&#20307;&#23458;&#25143;&#31471;&#19978;&#20351;&#29992;&#22823;&#25209;&#37327;&#26469;&#36827;&#34892;&#35757;&#32451;&#20197;&#35299;&#20915;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#20294;&#20854;&#22312;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#19981;&#26126;&#30830;&#12290;&#22312;&#32771;&#34385;&#36825;&#20123;&#25361;&#25112;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;FedAMD&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#38170;&#23450;&#25277;&#26679;&#65292;&#23558;&#37096;&#20998;&#21442;&#19982;&#32773;&#20998;&#20026;&#38170;&#23450;&#32452;&#21644;&#30719;&#24037;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared with full client participation, partial client participation is a more practical scenario in federated learning, but it may amplify some challenges in federated learning, such as data heterogeneity. The lack of inactive clients' updates in partial client participation makes it more likely for the model aggregation to deviate from the aggregation based on full client participation. Training with large batches on individual clients is proposed to address data heterogeneity in general, but their effectiveness under partial client participation is not clear. Motivated by these challenges, we propose to develop a novel federated learning framework, referred to as FedAMD, for partial client participation. The core idea is anchor sampling, which separates partial participants into anchor and miner groups. Each client in the anchor group aims at the local bullseye with the gradient computation using a large batch. Guided by the bullseyes, clients in the miner group steer multiple near
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#19968;&#27491;&#26631;&#31614;&#30340;&#37319;&#26679;&#26041;&#27861;S2M&#65292;&#20197;&#23454;&#29616;&#23545;&#22810;&#26631;&#31614;&#26679;&#26412;&#30340;&#29983;&#25104;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#21046;&#20316;&#26102;&#39640;&#26114;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2206.05764</link><description>&lt;p&gt;
&#20174;&#21333;&#19968;&#27491;&#26631;&#31614;&#20013;&#25366;&#25496;&#22810;&#26631;&#31614;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Mining Multi-Label Samples from Single Positive Labels. (arXiv:2206.05764v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#19968;&#27491;&#26631;&#31614;&#30340;&#37319;&#26679;&#26041;&#27861;S2M&#65292;&#20197;&#23454;&#29616;&#23545;&#22810;&#26631;&#31614;&#26679;&#26412;&#30340;&#29983;&#25104;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#21046;&#20316;&#26102;&#39640;&#26114;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;cGAN&#65289;&#22312;&#31867;&#21035;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#21516;&#26102;&#25511;&#21046;&#22810;&#20010;&#26465;&#20214;&#65292;cGAN&#38656;&#35201;&#22810;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21487;&#20197;&#20026;&#27599;&#20010;&#25968;&#25454;&#23454;&#20363;&#20998;&#37197;&#22810;&#20010;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#24040;&#22823;&#30340;&#27880;&#37322;&#25104;&#26412;&#38480;&#21046;&#20102;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23454;&#38469;&#35774;&#32622;&#30340;&#31216;&#20026;&#21333;&#27491;&#26631;&#31614;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#20010;&#25968;&#25454;&#23454;&#20363;&#20165;&#30001;&#19968;&#20010;&#27491;&#26631;&#31614;&#27880;&#37322;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#36127;&#26631;&#31614;&#12290;&#20026;&#20102;&#22312;&#21333;&#27491;&#26631;&#31614;&#35774;&#32622;&#20013;&#29983;&#25104;&#22810;&#26631;&#31614;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#26032;&#22411;&#37319;&#26679;&#26041;&#27861;&#65292;&#31216;&#20026;&#21333;&#21040;&#22810;&#26631;&#31614;&#65288;S2M&#65289;&#37319;&#26679;&#12290;&#20316;&#20026;&#24191;&#27867;&#36866;&#29992;&#30340;&#8220;&#38468;&#21152;&#8221;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;S2M&#37319;&#26679;&#26041;&#27861;&#20351;&#29616;&#26377;&#30340;&#26080;&#26465;&#20214;&#21644;&#26465;&#20214;GAN&#33021;&#22815;&#20197;&#26368;&#23567;&#30340;&#27880;&#37322;&#25104;&#26412;&#32472;&#21046;&#39640;&#36136;&#37327;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#12290;&#22312;&#23454;&#38469;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional generative adversarial networks (cGANs) have shown superior results in class-conditional generation tasks. To simultaneously control multiple conditions, cGANs require multi-label training datasets, where multiple labels can be assigned to each data instance. Nevertheless, the tremendous annotation cost limits the accessibility of multi-label datasets in real-world scenarios. Therefore, in this study we explore the practical setting called the single positive setting, where each data instance is annotated by only one positive label with no explicit negative labels. To generate multi-label data in the single positive setting, we propose a novel sampling approach called single-to-multi-label (S2M) sampling, based on the Markov chain Monte Carlo method. As a widely applicable "add-on" method, our proposed S2M sampling method enables existing unconditional and conditional GANs to draw high-quality multi-label data with a minimal annotation cost. Extensive experiments on real im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38543;&#26426;&#36817;&#20284;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21033;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#32467;&#26500;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#36924;&#36817;&#35823;&#24046;&#24182;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;SGLD&#21644;RBM&#65292;&#25105;&#20204;&#20998;&#21035;&#35777;&#26126;&#20102;&#19981;&#21516;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#36739;&#20248;&#30340;&#25910;&#25947;&#29575;&#21644;&#21442;&#25968;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2206.03792</link><description>&lt;p&gt;
&#21033;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#32467;&#26500;&#30340;&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#26041;&#27861;&#65306;&#25913;&#36827;&#30340;&#20998;&#26512;&#21644;&#26356;&#24555;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Utilising the CLT Structure in Stochastic Gradient based Sampling : Improved Analysis and Faster Algorithms. (arXiv:2206.03792v3 [math.PR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38543;&#26426;&#36817;&#20284;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21033;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#32467;&#26500;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#36924;&#36817;&#35823;&#24046;&#24182;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;SGLD&#21644;RBM&#65292;&#25105;&#20204;&#20998;&#21035;&#35777;&#26126;&#20102;&#19981;&#21516;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#36739;&#20248;&#30340;&#25910;&#25947;&#29575;&#21644;&#21442;&#25968;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38543;&#26426;&#36817;&#20284;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#22914;&#38543;&#26426;&#26799;&#24230; langevin &#21160;&#21147;&#23398;&#65288;SGLD&#65289;&#21644;&#38543;&#26426;&#25209;&#22788;&#29702;&#26041;&#27861;&#65288;RBM&#65289;&#29992;&#20110;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#21160;&#21147;&#23398;&#65288;IPD&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30001;&#20110;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65288;CLT&#65289;&#65292;&#38543;&#26426;&#36924;&#36817;&#24341;&#20837;&#30340;&#22122;&#22768;&#20960;&#20046;&#26159;&#39640;&#26031;&#20998;&#24067;&#65292;&#32780;&#39537;&#21160;&#24067;&#26391;&#36816;&#21160;&#21017;&#26159;&#30830;&#20999;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#26469;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#36924;&#36817;&#35823;&#24046;&#65292;&#24182;&#33719;&#24471;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#25913;&#36827;&#25910;&#25947;&#20445;&#35777;&#12290;&#23545;&#20110; SGLD&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19981;&#38656;&#35201;&#32479;&#19968;&#28201;&#26262;&#21551;&#21160;&#30340;&#24773;&#20917;&#19979;KL&#25955;&#24230;&#30340;&#31532;&#19968;&#20010;&#31283;&#23450;&#25910;&#25947;&#29575;&#65292;&#20551;&#35774;&#30446;&#26631;&#23494;&#24230;&#28385;&#36275;&#19968;&#20010;&#23545;&#25968; Sobolev &#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#22312;&#26174;&#33879;&#36739;&#36731;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#65292;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#19968;&#38454; oracle &#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102; SGLD &#30340;&#31532;&#19968;&#20010;&#20445;&#35777;&#65292;&#23545;&#20110;&#26356;&#24369;&#30340;&#26465;&#20214;&#65292;&#22914; H\''{o}lder &#24179;&#28369;&#24615;&#21644; Poincare&#19981;&#31561;&#24335;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#29616;&#26377;&#25216;&#26415;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23545;&#20110; RBM&#65292;&#25105;&#20204;&#22312; IPD &#30340;&#24369;&#28151;&#21512;&#26465;&#20214;&#19979;&#33719;&#24471;&#20102;&#31532;&#19968;&#27425;&#25910;&#25947;&#20998;&#26512;&#21644;&#26368;&#20339;&#21442;&#25968;&#33539;&#22260;&#65292;&#36825;&#22312;&#32479;&#35745;&#29289;&#29702;&#21644;&#23398;&#20064;&#29702;&#35770;&#20013;&#20855;&#26377;&#20960;&#20010;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider stochastic approximations of sampling algorithms, such as Stochastic Gradient Langevin Dynamics (SGLD) and the Random Batch Method (RBM) for Interacting Particle Dynamcs (IPD). We observe that the noise introduced by the stochastic approximation is nearly Gaussian due to the Central Limit Theorem (CLT) while the driving Brownian motion is exactly Gaussian. We harness this structure to absorb the stochastic approximation error inside the diffusion process, and obtain improved convergence guarantees for these algorithms. For SGLD, we prove the first stable convergence rate in KL divergence without requiring uniform warm start, assuming the target density satisfies a Log-Sobolev Inequality. Our result implies superior first-order oracle complexity compared to prior works, under significantly milder assumptions. We also prove the first guarantees for SGLD under even weaker conditions such as H\"{o}lder smoothness and Poincare Inequality, thus bridging the gap between the state-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#20110;&#36866;&#24403;&#21442;&#25968;&#35268;&#33539;&#30340;&#21160;&#24577;&#25511;&#21046;&#32467;&#21512;&#22522;&#20110;&#21442;&#25968;&#35268;&#33539;&#30340; Rademacher &#22797;&#26434;&#24230;&#20272;&#35745;&#23548;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#65292;&#36866;&#29992;&#20110;&#21253;&#25324; MLP &#21644; CNN &#22312;&#20869;&#30340;&#24191;&#27867;&#32593;&#32476;&#26550;&#26500;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#20248;&#21270;&#22120;&#21644;&#32593;&#32476;&#36229;&#21442;&#25968;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2206.03299</link><description>&lt;p&gt;
&#30001; SGD &#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;
&lt;/p&gt;
&lt;p&gt;
Generalization Error Bounds for Deep Neural Networks Trained by SGD. (arXiv:2206.03299v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03299
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#20110;&#36866;&#24403;&#21442;&#25968;&#35268;&#33539;&#30340;&#21160;&#24577;&#25511;&#21046;&#32467;&#21512;&#22522;&#20110;&#21442;&#25968;&#35268;&#33539;&#30340; Rademacher &#22797;&#26434;&#24230;&#20272;&#35745;&#23548;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#65292;&#36866;&#29992;&#20110;&#21253;&#25324; MLP &#21644; CNN &#22312;&#20869;&#30340;&#24191;&#27867;&#32593;&#32476;&#26550;&#26500;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#20248;&#21270;&#22120;&#21644;&#32593;&#32476;&#36229;&#21442;&#25968;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#36866;&#24403;&#21442;&#25968;&#35268;&#33539;&#30340;&#21160;&#24577;&#25511;&#21046;&#21644;&#22522;&#20110;&#21442;&#25968;&#35268;&#33539;&#30340; Rademacher &#22797;&#26434;&#24230;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#23548;&#20986;&#20102;&#30001;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#12290;&#36825;&#20123;&#30028;&#26126;&#30830;&#21462;&#20915;&#20110;&#27839;&#35757;&#32451;&#36712;&#36857;&#30340;&#25439;&#22833;&#65292;&#24182;&#36866;&#29992;&#20110;&#21253;&#25324;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#20869;&#30340;&#24191;&#27867;&#32593;&#32476;&#26550;&#26500;&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#20381;&#36182;&#30340;&#27867;&#21270;&#20272;&#35745;&#65288;&#22914;&#22522;&#20110;&#20840;&#23616;&#31283;&#23450;&#24615;&#30340;&#30028;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#30028;&#19981;&#38656;&#35201;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340; $L$-&#24179;&#28369;&#24615;&#65292;&#24182;&#19988;&#30452;&#25509;&#36866;&#29992;&#20110; SGD&#65292;&#32780;&#19981;&#26159;&#38543;&#26426; Langevin &#26799;&#24230;&#19979;&#38477;&#65288;SGLD&#65289;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#30028;&#26159;&#38750;&#34394;&#20551;&#21644;&#24378;&#20581;&#30340;&#65292;&#33021;&#22815;&#36866;&#24212;&#20248;&#21270;&#22120;&#21644;&#32593;&#32476;&#36229;&#21442;&#25968;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization error bounds for deep neural networks trained by stochastic gradient descent (SGD) are derived by combining a dynamical control of an appropriate parameter norm and the Rademacher complexity estimate based on parameter norms. The bounds explicitly depend on the loss along the training trajectory, and work for a wide range of network architectures including multilayer perceptron (MLP) and convolutional neural networks (CNN). Compared with other algorithm-depending generalization estimates such as uniform stability-based bounds, our bounds do not require $L$-smoothness of the nonconvex loss function, and apply directly to SGD instead of Stochastic Langevin gradient descent (SGLD). Numerical results show that our bounds are non-vacuous and robust with the change of optimizer and network hyperparameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#22823;&#37096;&#20998;&#30340;&#36731;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#24179;&#26041;&#25439;&#22833;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#22312;&#26089;&#26399;&#38454;&#27573;&#20250;&#26377;&#24456;&#24555;&#30340;&#19979;&#38477;&#12290;&#23545;&#20110;&#25351;&#25968;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#19968;&#20123;&#23545;&#20110;&#25968;&#25454;&#30340;&#20551;&#35774;&#19979;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;GD&#20250;&#20840;&#23616;&#25910;&#25947;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#22522;&#20110;&#23545;&#31070;&#32463;&#20803;&#28608;&#27963;&#27169;&#24335;&#30340;&#24494;&#35266;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#8220;&#31070;&#32463;&#20803;&#21010;&#20998;&#8221;&#30340;&#32467;&#26524;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21160;&#24577;&#30340;&#34892;&#20026;&#65292;&#24182;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2206.02139</link><description>&lt;p&gt;
&#23545;&#36731;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26089;&#26399;&#25910;&#25947;&#21644;&#20840;&#23616;&#25910;&#25947;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Early Stage Convergence and Global Convergence of Training Mildly Parameterized Neural Networks. (arXiv:2206.02139v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#22823;&#37096;&#20998;&#30340;&#36731;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#24179;&#26041;&#25439;&#22833;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#22312;&#26089;&#26399;&#38454;&#27573;&#20250;&#26377;&#24456;&#24555;&#30340;&#19979;&#38477;&#12290;&#23545;&#20110;&#25351;&#25968;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#19968;&#20123;&#23545;&#20110;&#25968;&#25454;&#30340;&#20551;&#35774;&#19979;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;GD&#20250;&#20840;&#23616;&#25910;&#25947;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#22522;&#20110;&#23545;&#31070;&#32463;&#20803;&#28608;&#27963;&#27169;&#24335;&#30340;&#24494;&#35266;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#8220;&#31070;&#32463;&#20803;&#21010;&#20998;&#8221;&#30340;&#32467;&#26524;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21160;&#24577;&#30340;&#34892;&#20026;&#65292;&#24182;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#24773;&#20917;&#19979;&#35757;&#32451;&#36731;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;GD&#21644;SGD&#30340;&#25910;&#25947;&#24615;&#12290;&#23545;&#20110;&#21253;&#25324;&#26368;&#24120;&#29992;&#30340;&#24179;&#26041;&#25439;&#22833;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#22312;&#20869;&#30340;&#24191;&#27867;&#27169;&#22411;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#8220;&#26089;&#26399;&#25910;&#25947;&#8221;&#32467;&#26524;&#12290;&#25105;&#20204;&#34920;&#26126;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#25439;&#22833;&#20989;&#25968;&#20250;&#26377;&#36739;&#22823;&#31243;&#24230;&#30340;&#19979;&#38477;&#65292;&#36825;&#31181;&#19979;&#38477;&#26159;&#24456;&#24555;&#30340;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25351;&#25968;&#22411;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#20123;&#20551;&#35774;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GD&#30340;&#20840;&#23616;&#25910;&#25947;&#12290;&#19982;&#20381;&#38752;&#26497;&#31471;&#36807;&#24230;&#21442;&#25968;&#21270;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#22522;&#20110;&#23545;&#31070;&#32463;&#20803;&#28608;&#27963;&#27169;&#24335;&#30340;&#24494;&#35266;&#20998;&#26512;&#65292;&#36825;&#26377;&#21161;&#20110;&#25105;&#20204;&#25512;&#23548;&#20986;&#26356;&#24378;&#22823;&#30340;&#26799;&#24230;&#19979;&#30028;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#28608;&#27963;&#27169;&#24335;&#30340;&#32467;&#26524;&#20026;&#8220;&#31070;&#32463;&#20803;&#21010;&#20998;&#8221;&#65292;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21160;&#24577;&#30340;&#34892;&#20026;&#65292;&#24182;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
The convergence of GD and SGD when training mildly parameterized neural networks starting from random initialization is studied. For a broad range of models and loss functions, including the most commonly used square loss and cross entropy loss, we prove an ``early stage convergence'' result. We show that the loss is decreased by a significant amount in the early stage of the training, and this decrease is fast. Furthurmore, for exponential type loss functions, and under some assumptions on the training data, we show global convergence of GD. Instead of relying on extreme over-parameterization, our study is based on a microscopic analysis of the activation patterns for the neurons, which helps us derive more powerful lower bounds for the gradient. The results on activation patterns, which we call ``neuron partition'', help build intuitions for understanding the behavior of neural networks' training dynamics, and may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#22120;MetaLR&#65292;&#21487;&#20197;&#20351;&#19981;&#21516;&#23618;&#27425;&#30340;&#23618;&#27425;&#26681;&#25454;&#23427;&#20204;&#30340;&#21487;&#36716;&#31227;&#24615;&#33258;&#21160;&#21327;&#21516;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#22312;&#22810;&#20010;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;MetaLR&#22312;&#24494;&#35843;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#22810;&#31181;&#39046;&#20808;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.01408</link><description>&lt;p&gt;
MetaLR: &#21307;&#23398;&#22270;&#20687;&#36801;&#31227;&#23398;&#20064;&#20013;&#23398;&#20064;&#36895;&#29575;&#30340;&#20803;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging. (arXiv:2206.01408v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01408
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#22120;MetaLR&#65292;&#21487;&#20197;&#20351;&#19981;&#21516;&#23618;&#27425;&#30340;&#23618;&#27425;&#26681;&#25454;&#23427;&#20204;&#30340;&#21487;&#36716;&#31227;&#24615;&#33258;&#21160;&#21327;&#21516;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#22312;&#22810;&#20010;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;MetaLR&#22312;&#24494;&#35843;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#22810;&#31181;&#39046;&#20808;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#36801;&#31227;&#23398;&#20064;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#33021;&#22815;&#23545;&#26377;&#38480;&#30340;&#21307;&#23398;&#25968;&#25454;&#36827;&#34892;&#27867;&#21270;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#24320;&#21457;&#38024;&#23545;&#32954;&#37096;&#36229;&#22768;&#65292;&#33016;&#37096;X&#23556;&#32447;&#21644;&#32925;&#33039;CT&#31561;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#24357;&#21512;&#39046;&#22495;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#24494;&#35843;&#22312;&#36866;&#24212;&#30446;&#26631;&#20219;&#21153;&#20013;&#20063;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#24120;&#35265;&#30340;&#24494;&#35843;&#26041;&#27861;&#26159;&#25163;&#21160;&#36873;&#25321;&#21487;&#36716;&#31227;&#23618;&#65288;&#20363;&#22914;&#26368;&#21518;&#20960;&#23618;&#65289;&#36827;&#34892;&#26356;&#26032;&#65292;&#36825;&#26159;&#38750;&#24120;&#32791;&#26102;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#23398;&#20064;&#36895;&#29575;&#35843;&#25972;&#22120;&#65292;&#21517;&#20026;MetaLR&#65292;&#20351;&#19981;&#21516;&#23618;&#27425;&#30340;&#23618;&#27425;&#22312;&#36328;&#22495;&#20219;&#21153;&#20013;&#26681;&#25454;&#23427;&#20204;&#22312;&#39046;&#22495;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#33258;&#21160;&#21327;&#21516;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;MetaLR&#22312;&#32447;&#23398;&#20064;&#19981;&#21516;&#23618;&#27425;&#30340;&#36866;&#24403;&#23398;&#20064;&#29575;&#65292;&#38450;&#27490;&#39640;&#24230;&#21487;&#36716;&#31227;&#23618;&#24536;&#35760;&#20854;&#21307;&#23398;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#20419;&#20351;&#20854;&#20182;&#21487;&#36716;&#31227;&#23618;&#20027;&#21160;&#36866;&#24212;&#26032;&#39046;&#22495;&#12290;&#22312;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;MetaLR&#21487;&#20197;&#26174;&#30528;&#25913;&#21892;&#20960;&#31181;&#39046;&#20808;&#30340;&#24494;&#35843;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In medical image analysis, transfer learning is a powerful method for deep neural networks (DNNs) to generalize well on limited medical data. Prior efforts have focused on developing pre-training algorithms on domains such as lung ultrasound, chest X-ray, and liver CT to bridge domain gaps. However, we find that model fine-tuning also plays a crucial role in adapting medical knowledge to target tasks. The common fine-tuning method is manually picking transferable layers (e.g., the last few layers) to update, which is labor-expensive. In this work, we propose a meta-learning-based LR tuner, named MetaLR, to make different layers automatically co-adapt to downstream tasks based on their transferabilities across domains. MetaLR learns appropriate LRs for different layers in an online manner, preventing highly transferable layers from forgetting their medical representation abilities and driving less transferable layers to adapt actively to new domains. Extensive experiments on various med
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29420;&#31435;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19979;&#30340;&#22810;&#29615;&#22659;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#27979;&#35797;&#29420;&#31435;&#24615;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2205.13935</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#29615;&#22659;&#26041;&#27861;&#26816;&#27979;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#38544;&#24335;&#28151;&#28102;
&lt;/p&gt;
&lt;p&gt;
Detecting hidden confounding in observational data using multiple environments. (arXiv:2205.13935v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13935
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29420;&#31435;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19979;&#30340;&#22810;&#29615;&#22659;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#27979;&#35797;&#29420;&#31435;&#24615;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#65292;&#24120;&#35265;&#30340;&#20551;&#35774;&#26159;&#27809;&#26377;&#38544;&#24335;&#28151;&#28102;&#12290;&#28982;&#32780;&#65292;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#20013;&#19981;&#33021;&#30830;&#23450;&#36825;&#20010;&#20551;&#35774;&#36890;&#24120;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22312;&#29420;&#31435;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;&#22810;&#20010;&#26469;&#33258;&#19981;&#21516;&#29615;&#22659;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#21487;&#39564;&#35777;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#30340;&#29702;&#35770;&#65292;&#36825;&#31181;&#29420;&#31435;&#24615;&#20165;&#24403;&#23384;&#22312;&#28151;&#28102;&#22240;&#32032;&#26102;&#25165;&#19981;&#23384;&#22312;&#65292;&#24182;&#26816;&#26597;&#20102;&#36829;&#21453;&#20854;&#20551;&#35774;&#30340;&#24773;&#20917;&#65306;&#36864;&#21270;&#21644;&#20381;&#36182;&#26426;&#21046;&#20197;&#21450;&#24544;&#23454;&#24230;&#36829;&#21453;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31243;&#24207;&#26469;&#27979;&#35797;&#36825;&#20123;&#29420;&#31435;&#24615;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#21322;&#21512;&#25104;&#25968;&#25454;&#21644;&#27169;&#25311;&#30740;&#31350;&#30740;&#31350;&#20854;&#32463;&#39564;&#26377;&#38480;&#26679;&#26412;&#34892;&#20026;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#30340;&#31243;&#24207;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#23384;&#22312;&#38544;&#24335;&#28151;&#28102;&#65292;&#29305;&#21035;&#26159;&#24403;&#28151;&#28102;&#20559;&#24046;&#24456;&#22823;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common assumption in causal inference from observational data is that there is no hidden confounding. Yet it is, in general, impossible to verify this assumption from a single dataset. Under the assumption of independent causal mechanisms underlying the data-generating process, we demonstrate a way to detect unobserved confounders when having multiple observational datasets coming from different environments. We present a theory for testable conditional independencies that are only absent when there is hidden confounding and examine cases where we violate its assumptions: degenerate &amp; dependent mechanisms, and faithfulness violations. Additionally, we propose a procedure to test these independencies and study its empirical finite-sample behavior using simulation studies and semi-synthetic data based on a real-world dataset. In most cases, the proposed procedure correctly predicts the presence of hidden confounding, particularly when the confounding bias is large.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;PEAR&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#20010;&#33021;&#22815;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#26469;&#35745;&#31639;&#30446;&#26631;&#29992;&#25143;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#65292;&#28982;&#21518;&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31639;&#27861;&#24178;&#39044;&#30340;&#32463;&#27982;&#23454;&#29992;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13743</link><description>&lt;p&gt;
&#24102;&#26377;&#20559;&#22909;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#31639;&#27861;&#24178;&#39044;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Personalized Algorithmic Recourse with Preference Elicitation. (arXiv:2205.13743v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13743
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;PEAR&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#20010;&#33021;&#22815;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#26469;&#35745;&#31639;&#30446;&#26631;&#29992;&#25143;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#65292;&#28982;&#21518;&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31639;&#27861;&#24178;&#39044;&#30340;&#32463;&#27982;&#23454;&#29992;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#24178;&#39044;&#65288;AR&#65289;&#30340;&#38382;&#39064;&#26159;&#35745;&#31639;&#29992;&#25143;&#25191;&#34892;&#19968;&#31995;&#21015;&#25805;&#20316;&#20197;&#39072;&#35206;&#19981;&#33391;&#26426;&#22120;&#20915;&#31574;&#30340;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#30340;&#25805;&#20316;&#24207;&#21015;&#19981;&#24212;&#35813;&#23545;&#29992;&#25143;&#30340;&#23454;&#26045;&#25552;&#20986;&#36807;&#39640;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;AR&#26041;&#27861;&#37117;&#20551;&#35774;&#25152;&#26377;&#29992;&#25143;&#30340;&#25805;&#20316;&#25104;&#26412;&#30456;&#21516;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#21521;&#26576;&#20123;&#29992;&#25143;&#25512;&#33616;&#26114;&#36149;&#30340;&#34917;&#25937;&#35745;&#21010;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEAR&#65292;&#36825;&#26159;&#19968;&#31181;&#39318;&#20010;&#21487;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#65292;&#20197;&#28385;&#36275;&#20219;&#20309;&#26368;&#32456;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;PEAR&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#21521;&#30446;&#26631;&#29992;&#25143;&#21457;&#20986;&#36873;&#25321;&#38598;&#26597;&#35810;&#26469;&#36845;&#20195;&#22320;&#25913;&#21892;&#23545;&#25805;&#20316;&#25104;&#26412;&#30340;&#20272;&#35745;&#20540;&#12290;&#36825;&#20123;&#26597;&#35810;&#30340;&#35745;&#31639;&#26159;&#36890;&#36807;&#26368;&#22823;&#21270;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#26469;&#35745;&#31639;&#30340;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#25104;&#26412;&#20272;&#35745;&#21644;&#29992;&#25143;&#21709;&#24212;&#19981;&#30830;&#23450;&#24615;&#30340;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#12290;PEAR&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#23454;&#29616;AR&#20219;&#21153;&#25152;&#38656;&#36798;&#25104;&#30446;&#26631;&#30340;&#20559;&#22909;&#65292;&#20197;&#21450;&#25191;&#34892;&#27599;&#20010;&#25805;&#20316;&#25152;&#28041;&#21450;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;AR&#20219;&#21153;&#26469;&#35780;&#20272;PEAR&#65292;&#24182;&#26174;&#31034;&#20854;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#26356;&#20026;&#32463;&#27982;&#23454;&#29992;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#34917;&#25937;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Rein
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102; Masked Graph Modeling &#22312;&#22270;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33945;&#29256;&#30340;&#22270;&#24314;&#27169;&#65288;MGM&#65289;&#20316;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#19968;&#20010;&#21407;&#20808;&#30340;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#20248;&#21183;&#20316;&#29992;&#12290;&#22312;&#33719;&#24471;&#20102;&#29702;&#35770;&#21450;&#23454;&#35777;&#35777;&#25454;&#30340;&#25903;&#25345;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MaskGAE&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.10053</link><description>&lt;p&gt;
&#25506;&#31350;&#22270;&#33258;&#32534;&#30721;&#22120;&#20013; Masked Graph Modeling &#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
What's Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders. (arXiv:2205.10053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102; Masked Graph Modeling &#22312;&#22270;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33945;&#29256;&#30340;&#22270;&#24314;&#27169;&#65288;MGM&#65289;&#20316;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#19968;&#20010;&#21407;&#20808;&#30340;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#20248;&#21183;&#20316;&#29992;&#12290;&#22312;&#33719;&#24471;&#20102;&#29702;&#35770;&#21450;&#23454;&#35777;&#35777;&#25454;&#30340;&#25903;&#25345;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MaskGAE&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#19968;&#31181;&#39047;&#20855;&#28508;&#21147;&#30340;&#31574;&#30053;&#34987;&#31216;&#20026; Masked Autoencoding&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#22270;&#33258;&#32534;&#30721;&#22120;&#20013;&#22914;&#20309;&#23454;&#29616; Masking&#65292;&#29702;&#35770;&#19978;&#20173;&#28982;&#23384;&#22312;&#32570;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; Masked Graph Autoencoder &#65288;MaskGAE&#65289;&#65292;&#23427;&#26159;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;&#30456;&#36739;&#20110;&#20854;&#23427;&#26222;&#36890;&#30340; GAEs&#65292;MaskGAE &#37319;&#29992;&#22522;&#20110;&#33945;&#29256;&#30340;&#22270;&#24314;&#27169;&#65288;Masked Graph Modeling&#65292;MGM&#65289;&#20316;&#20026;&#19968;&#20010;&#21407;&#20808;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#8220;&#25513;&#34109;&#8221;&#19968;&#20010;&#37096;&#20998;&#36793;&#32536;&#65292;&#20197;&#37096;&#20998;&#21487;&#35270;&#12289;&#38750;&#25513;&#34109;&#30340;&#22270;&#32467;&#26500;&#65292;&#35797;&#22270;&#37325;&#26500;&#32570;&#22833;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21450;&#23454;&#35777;&#35777;&#25454;&#65292;&#20840;&#38754;&#35777;&#26126;&#20102;&#27492;&#39044;&#27979;&#20219;&#21153;&#20248;&#21183;&#30340;&#20316;&#29992;&#65292;&#20197;&#25506;&#31350; MGM &#23545; GAEs &#30340;&#36755;&#20986;&#34920;&#31034;&#30340;&#25913;&#21892;&#20316;&#29992;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102; GAEs &#19982;&#23545;&#27604;&#23398;&#20064;&#30340;&#32039;&#23494;&#20851;&#31995;&#65292;&#34920;&#26126; MGM &#26126;&#26174;&#25913;&#21892;&#20102; GAEs &#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#12290;&#22312;&#32463;&#39564;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102; MaskGAE &#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19979;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; GAEs&#12290;&#26412;&#30740;&#31350;&#20026;&#25506;&#31350; Masked Graph Modeling &#22312;&#22270;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#37325;&#35201;&#24615;&#25552;&#20379;&#20102;&#21551;&#31034;&#65292;&#24182;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last years have witnessed the emergence of a promising self-supervised learning strategy, referred to as masked autoencoding. However, there is a lack of theoretical understanding of how masking matters on graph autoencoders (GAEs). In this work, we present masked graph autoencoder (MaskGAE), a self-supervised learning framework for graph-structured data. Different from standard GAEs, MaskGAE adopts masked graph modeling (MGM) as a principled pretext task - masking a portion of edges and attempting to reconstruct the missing part with partially visible, unmasked graph structure. To understand whether MGM can help GAEs learn better representations, we provide both theoretical and empirical evidence to comprehensively justify the benefits of this pretext task. Theoretically, we establish close connections between GAEs and contrastive learning, showing that MGM significantly improves the self-supervised learning scheme of GAEs. Empirically, we conduct extensive experiments on a variet
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;</title><link>http://arxiv.org/abs/2204.11970</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#23545;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#36827;&#34892;&#35270;&#21147;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System. (arXiv:2204.11970v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#30524;&#31185;&#23398;&#20013;&#30340;&#29627;&#29827;&#20307;&#25163;&#26415;&#33647;&#29289;&#27835;&#30103;&#26159;&#27835;&#30103;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#12289;&#31958;&#23615;&#30149;&#24615;&#40644;&#26001;&#27700;&#32959;&#65288;DME&#65289;&#21644;&#35270;&#32593;&#33180;&#38745;&#33033;&#38459;&#22622;&#65288;RVO&#65289;&#30456;&#20851;&#30142;&#30149;&#30340;&#19968;&#31181;&#26222;&#36941;&#27835;&#30103;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#65292;&#24739;&#32773;&#24448;&#24448;&#20250;&#22312;&#22810;&#24180;&#26102;&#38388;&#20869;&#22833;&#21435;&#35270;&#21147;&#65292;&#23613;&#31649;&#25509;&#21463;&#27835;&#30103;&#12290;&#26412;&#25991;&#37319;&#29992;&#22810;&#31181;IT&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#34701;&#21512;&#20102;&#24503;&#22269;&#19968;&#23478;&#26368;&#20339;&#21307;&#30103;&#20445;&#20581;&#21307;&#38498;&#30340;&#30524;&#31185;&#37096;&#38376;&#30340;&#19981;&#21516;IT&#31995;&#32479;&#12290;&#32463;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#24739;&#32773;&#35270;&#21147;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#19977;&#31181;&#30142;&#30149;&#30340;&#39044;&#27979;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#24037;&#20855;&#65292;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ophthalmology, intravitreal operative medication therapy (IVOM) is a widespread treatment for diseases related to the age-related macular degeneration (AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion (RVO). However, in real-world settings, patients often suffer from loss of vision on time scales of years despite therapy, whereas the prediction of the visual acuity (VA) and the earliest possible detection of deterioration under real-life conditions is challenging due to heterogeneous and incomplete data. In this contribution, we present a workflow for the development of a research-compatible data corpus fusing different IT systems of the department of ophthalmology of a German maximum care hospital. The extensive data corpus allows predictive statements of the expected progression of a patient and his or her VA in each of the three diseases. We found out for the disease AMD a significant deterioration of the visual acuity over time. Within our proposed m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#32500;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;&#30340;&#26041;&#27861;&#65292;&#23548;&#20986;&#20102;&#20851;&#20110;&#36825;&#26679;&#19968;&#20010;&#36817;&#20284;&#30340;&#26680;&#24515;&#38598;&#24517;&#39035;&#26377;&#30340;&#22823;&#23567;&#30340;&#39640;&#27010;&#29575;&#19979;&#38480;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20123;&#25216;&#26415;&#20197;&#23558;&#21387;&#32553;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#20307;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.08847</link><description>&lt;p&gt;
&#26377;&#38480;&#32500;&#19979;&#30340;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;
&lt;/p&gt;
&lt;p&gt;
Compressed Empirical Measures (in finite dimensions). (arXiv:2204.08847v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#32500;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;&#30340;&#26041;&#27861;&#65292;&#23548;&#20986;&#20102;&#20851;&#20110;&#36825;&#26679;&#19968;&#20010;&#36817;&#20284;&#30340;&#26680;&#24515;&#38598;&#24517;&#39035;&#26377;&#30340;&#22823;&#23567;&#30340;&#39640;&#27010;&#29575;&#19979;&#38480;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20123;&#25216;&#26415;&#20197;&#23558;&#21387;&#32553;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#20307;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#38480;&#32500;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHSs&#65289;&#20013;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32463;&#39564;&#27979;&#24230;&#21253;&#21547;&#22312;&#19968;&#20010;&#33258;&#28982;&#30340;&#20984;&#38598;&#20013;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#20984;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#12290;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#36825;&#31181;&#36817;&#20284;&#20250;&#23548;&#33268;&#25968;&#25454;&#28857;&#30340;coreset&#12290;&#25511;&#21046;&#36825;&#26679;&#19968;&#20010;coreset&#24517;&#39035;&#26377;&#22810;&#22823;&#30340;&#19968;&#20010;&#20851;&#38190;&#25968;&#37327;&#26159;&#21253;&#21547;&#22312;&#32463;&#39564;&#20984;&#38598;&#20013;&#30340;&#32463;&#39564;&#27979;&#37327;&#21608;&#22260;&#30340;&#26368;&#22823;&#29699;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#26159;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#23548;&#20986;&#20851;&#20110;&#36825;&#26679;&#19968;&#20010;&#29699;&#30340;&#22823;&#23567;&#30340;&#39640;&#27010;&#29575;&#19979;&#38480;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#25216;&#26415;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#21387;&#32553;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#20307;&#30340;&#25512;&#26029;&#38382;&#39064;&#65292;&#22914;&#26680;&#23725;&#22238;&#24402;&#65292;&#26469;&#34917;&#20805;&#36825;&#31181;&#19979;&#38480;&#30340;&#27966;&#29983;&#12290;&#25105;&#20204;&#26368;&#21518;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38480;&#32500;RKHS&#30340;&#26500;&#36896;&#65292;&#20854;&#20013;&#21387;&#32553;&#24456;&#24046;&#65292;&#31361;&#20986;&#20102;&#25105;&#20204;&#38754;&#20020;&#30340;&#19968;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study approaches for compressing the empirical measure in the context of finite dimensional reproducing kernel Hilbert spaces (RKHSs).In this context, the empirical measure is contained within a natural convex set and can be approximated using convex optimization methods.Such an approximation gives under certain conditions rise to a coreset of data points. A key quantity that controls how large such a coreset has to be is the size of the largest ball around the empirical measure that is contained within the empirical convex set. The bulk of our work is concerned with deriving high probability lower bounds on the size of such a ball under various conditions. We complement this derivation of the lower bound by developing techniques that allow us to apply the compression approach to concrete inference problems such as kernel ridge regression. We conclude with a construction of an infinite dimensional RKHS for which the compression is poor, highlighting some of the difficulties one face
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#22122;&#22768;&#25239;&#24178;&#25200;&#35821;&#38899;&#35782;&#21035;&#30340;&#21452;&#36335;&#24452;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#26679;&#24335;&#23398;&#20064;&#21644;&#34701;&#21512;&#29305;&#24449;&#20197;&#20174;&#24178;&#20928;&#29305;&#24449;&#20013;&#23398;&#20064;&#28508;&#22312;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;10.6&#65285;&#21644;8.6&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2203.14838</link><description>&lt;p&gt;
&#21452;&#36335;&#24452;&#24335;&#23398;&#20064;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#22122;&#22768;&#25239;&#24178;&#25200;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition. (arXiv:2203.14838v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#22122;&#22768;&#25239;&#24178;&#25200;&#35821;&#38899;&#35782;&#21035;&#30340;&#21452;&#36335;&#24452;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#26679;&#24335;&#23398;&#20064;&#21644;&#34701;&#21512;&#29305;&#24449;&#20197;&#20174;&#24178;&#20928;&#29305;&#24449;&#20013;&#23398;&#20064;&#28508;&#22312;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;10.6&#65285;&#21644;8.6&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20250;&#26174;&#33879;&#38477;&#20302;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#35821;&#38899;&#22686;&#24378;&#65288;SE&#65289;&#20316;&#20026;&#21069;&#31471;&#65292;&#20197;&#20943;&#23569;ASR&#30340;&#22122;&#22768;&#65292;&#20294;&#26159;&#23427;&#20063;&#20250;&#21387;&#21046;&#19968;&#20123;&#37325;&#35201;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#21363;&#36807;&#24230;&#25233;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#36335;&#24452;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#22122;&#22768;&#25239;&#24178;&#25200;&#35821;&#38899;&#35782;&#21035;&#65288;DPSL-ASR&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#24178;&#20928;&#35821;&#38899;&#29305;&#24449;&#65292;&#20197;&#21450;&#26469;&#33258;IFF-Net&#30340;&#34701;&#21512;&#29305;&#24449;&#20316;&#20026;&#21452;&#36335;&#24452;&#36755;&#20837;&#65292;&#20197;&#24674;&#22797;&#34987;&#21387;&#21046;&#30340;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26679;&#24335;&#23398;&#20064;&#65292;&#23558;&#34701;&#21512;&#29305;&#24449;&#26144;&#23556;&#21040;&#25509;&#36817;&#24178;&#20928;&#29305;&#24449;&#65292;&#20197;&#20174;&#21518;&#32773;&#20013;&#23398;&#20064;&#28508;&#22312;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#21363;&#24178;&#20928;&#30340;&#8220;&#35821;&#38899;&#39118;&#26684;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#36335;&#24452;&#30340;&#26368;&#32456;ASR&#36755;&#20986;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#25552;&#39640;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20339;IFF-Net&#22522;&#32447;&#65292;&#22312;RATS&#21644;CHiME-4&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;10.6&#65285;&#21644;8.6&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) systems degrade significantly under noisy conditions. Recently, speech enhancement (SE) is introduced as front-end to reduce noise for ASR, but it also suppresses some important speech information, i.e., over-suppression. To alleviate this, we propose a dual-path style learning approach for end-to-end noise-robust speech recognition (DPSL-ASR). Specifically, we first introduce clean speech feature along with the fused feature from IFF-Net as dual-path inputs to recover the suppressed information. Then, we propose style learning to map the fused feature close to clean feature, in order to learn latent speech information from the latter, i.e., clean "speech style". Furthermore, we also minimize the distance of final ASR outputs in two paths to improve noise-robustness. Experiments show that the proposed approach achieves relative word error rate (WER) reductions of 10.6% and 8.6% over the best IFF-Net baseline, on RATS and CHiME-4 datasets respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#24418;&#24335;&#30340;&#25968;&#25454;&#26377;&#25928;&#12289;&#22522;&#20110;&#29305;&#24449;&#30340;&#21160;&#24577;&#35828;&#35805;&#32773;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35328;&#35821;&#38556;&#30861;&#21644;&#32769;&#24180;&#20154;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2203.14593</link><description>&lt;p&gt;
&#21160;&#24577;&#29305;&#24449;&#24555;&#36895;&#35828;&#35805;&#32773;&#36866;&#24212;&#22312;&#35328;&#35821;&#38556;&#30861;&#21644;&#32769;&#24180;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On-the-Fly Feature Based Rapid Speaker Adaptation for Dysarthric and Elderly Speech Recognition. (arXiv:2203.14593v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#24418;&#24335;&#30340;&#25968;&#25454;&#26377;&#25928;&#12289;&#22522;&#20110;&#29305;&#24449;&#30340;&#21160;&#24577;&#35828;&#35805;&#32773;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35328;&#35821;&#38556;&#30861;&#21644;&#32769;&#24180;&#20154;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20934;&#30830;&#35782;&#21035;&#35328;&#35821;&#38556;&#30861;&#21644;&#32769;&#24180;&#20154;&#30340;&#35821;&#38899;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30001;&#21475;&#38899;&#25110;&#24615;&#21035;&#24341;&#36215;&#30340;&#35828;&#35805;&#32773;&#32423;&#21035;&#30340;&#24322;&#36136;&#24615;&#65292;&#19982;&#24180;&#40836;&#21644;&#35328;&#35821;&#38556;&#30861;&#30456;&#32467;&#21512;&#65292;&#20351;&#36825;&#20123;&#35828;&#35805;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#24456;&#22823;&#12290;&#35828;&#35805;&#32773;&#32423;&#21035;&#25968;&#25454;&#30340;&#31232;&#32570;&#38480;&#21046;&#20102;&#22522;&#20110;&#25968;&#25454;&#23494;&#38598;&#22411;&#27169;&#22411;&#30340;&#35828;&#35805;&#32773;&#36866;&#24212;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#24418;&#24335;&#30340;&#25968;&#25454;&#26377;&#25928;&#12289;&#22522;&#20110;&#29305;&#24449;&#30340;&#21160;&#24577;&#35828;&#35805;&#32773;&#36866;&#24212;&#26041;&#27861;&#65306;&#26041;&#24046;&#27491;&#21017;&#21270;&#39057;&#35889;&#22522;&#20934;&#23884;&#20837;&#65288;SVR&#65289;&#21644;&#35889;&#29305;&#24449;&#39537;&#21160;&#30340;f-LHUC&#36716;&#25442;&#12290;&#22312;UASpeech&#35328;&#35821;&#38556;&#30861;&#21644;DementiaBank Pitt&#32769;&#24180;&#20154;&#35821;&#38899;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#35828;&#35805;&#32773;&#36866;&#24212;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#22320;&#20248;&#20110;&#22522;&#32447;iVector&#33258;&#36866;&#24212;&#28151;&#21512;DNN/TDNN&#21644;E2E Conformer&#31995;&#32479;&#65292;&#32479;&#35745;&#26174;&#33879;&#22320;&#20943;&#23569;WER 2.48%-2.85%&#65288;&#32477;&#23545;&#20540;&#65289;&#65288;7.92%-8.06%&#30456;&#20851;&#65289;&#65292;&#19982;&#31163;&#32447;&#27169;&#22411;&#22522;&#30784;LHUC&#36866;&#24212;&#20998;&#21035;&#20943;&#23567;1.82%&#65288;&#30456;&#23545;&#20540;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate recognition of dysarthric and elderly speech remain challenging tasks to date. Speaker-level heterogeneity attributed to accent or gender, when aggregated with age and speech impairment, create large diversity among these speakers. Scarcity of speaker-level data limits the practical use of data-intensive model based speaker adaptation methods. To this end, this paper proposes two novel forms of data-efficient, feature-based on-the-fly speaker adaptation methods: variance-regularized spectral basis embedding (SVR) and spectral feature driven f-LHUC transforms. Experiments conducted on UASpeech dysarthric and DementiaBank Pitt elderly speech corpora suggest the proposed on-the-fly speaker adaptation approaches consistently outperform baseline iVector adapted hybrid DNN/TDNN and E2E Conformer systems by statistically significant WER reduction of 2.48%-2.85% absolute (7.92%-8.06% relative), and offline model based LHUC adaptation by 1.82% absolute (5.63% relative) respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#33258;&#28982;&#30340;&#20844;&#29702;&#26469;&#23450;&#20041;&#38543;&#26426;&#32676;&#20307;&#20844;&#24179;&#25490;&#21517;&#65292;&#35777;&#26126;&#20102;&#20165;&#26377;&#20107;&#21518;&#32676;&#20307;&#20844;&#24179;&#25490;&#21517;&#30340;&#20998;&#24067;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#37319;&#26679;&#38543;&#26426;&#30340;&#32676;&#20307;&#20844;&#24179;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2203.00887</link><description>&lt;p&gt;
&#37319;&#26679;&#21518;&#20107;&#21518;&#32676;&#20307;&#20844;&#24179;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Sampling Ex-Post Group-Fair Rankings. (arXiv:2203.00887v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#33258;&#28982;&#30340;&#20844;&#29702;&#26469;&#23450;&#20041;&#38543;&#26426;&#32676;&#20307;&#20844;&#24179;&#25490;&#21517;&#65292;&#35777;&#26126;&#20102;&#20165;&#26377;&#20107;&#21518;&#32676;&#20307;&#20844;&#24179;&#25490;&#21517;&#30340;&#20998;&#24067;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#37319;&#26679;&#38543;&#26426;&#30340;&#32676;&#20307;&#20844;&#24179;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#38543;&#26426;&#25490;&#21517;&#21463;&#21040;&#20851;&#27880;&#65292;&#26088;&#22312;&#23454;&#29616;&#27604;&#30830;&#23450;&#24615;&#25490;&#21517;&#26356;&#20844;&#24179;&#30340;&#26333;&#20809;&#21644;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#33258;&#28982;&#30340;&#20844;&#29702;&#26469;&#23450;&#20041;&#38543;&#26426;&#32676;&#20307;&#20844;&#24179;&#25490;&#21517;&#65292;&#24182;&#35777;&#26126;&#23384;&#22312;&#19968;&#20010;&#28385;&#36275;&#25105;&#20204;&#20844;&#29702;&#30340;&#21807;&#19968;&#20998;&#24067; $D$&#65292;&#35813;&#20998;&#24067;&#20165;&#34987;&#25903;&#25345;&#20110;&#20107;&#21518;&#32676;&#20307;&#20844;&#24179;&#25490;&#21517;&#65292;&#21363;&#22312;&#21069; $k$ &#21517;&#20013;&#28385;&#36275;&#29305;&#23450;&#19979;&#38480;&#21644;&#19978;&#38480;&#30340;&#32676;&#20307;&#20195;&#34920;&#24615;&#12290;&#25105;&#20204;&#30340;&#38382;&#39064;&#24418;&#24335;&#21363;&#20351;&#23384;&#22312;&#38544;&#21547;&#20559;&#35265;&#12289;&#19981;&#23436;&#25972;&#30340;&#30456;&#20851;&#20449;&#24687;&#25110;&#32773;&#20165;&#26377;&#39034;&#24207;&#25490;&#21517;&#32780;&#26080;&#30456;&#20851;&#24471;&#20998;&#25110;&#25928;&#29992;&#20540;&#20063;&#21487;&#20197;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#20174;&#19978;&#36848;&#20998;&#24067; $D$ &#20013;&#37319;&#26679;&#38543;&#26426;&#30340;&#32676;&#20307;&#20844;&#24179;&#25490;&#21517;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312; $O(k^2\ell)$ &#30340;&#26102;&#38388;&#20869;&#22343;&#21248;&#38543;&#26426;&#37319;&#26679;&#20107;&#21518;&#32676;&#20307;&#20844;&#24179;&#25490;&#21517;&#65292;&#20854;&#20013; $\ell$ &#26159;&#32452;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#31639;&#27861;&#21487;&#20197;&#20174;&#20998;&#24067; $\de$ &#37319;&#26679;&#20107;&#21518;&#32676;&#20307;&#20844;&#24179;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized rankings have been of recent interest to achieve ex-ante fairer exposure and better robustness than deterministic rankings. We propose a set of natural axioms for randomized group-fair rankings and prove that there exists a unique distribution $D$ that satisfies our axioms and is supported only over ex-post group-fair rankings, i.e., rankings that satisfy given lower and upper bounds on group-wise representation in the top-$k$ ranks. Our problem formulation works even when there is implicit bias, incomplete relevance information, or only ordinal ranking is available instead of relevance scores or utility values.  We propose two algorithms to sample a random group-fair ranking from the distribution $D$ mentioned above. Our first dynamic programming-based algorithm samples ex-post group-fair rankings uniformly at random in time $O(k^2\ell)$, where $\ell$ is the number of groups. Our second random walk-based algorithm samples ex-post group-fair rankings from a distribution $\de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#27979;&#22320;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;Sion&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#21644;&#40654;&#26364;&#22806;&#25512;&#31639;&#27861;&#65292;&#22312;&#20445;&#25345;&#38382;&#39064;&#21487;&#22788;&#29702;&#30340;&#21516;&#26102;&#65292;&#20026;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#25512;&#24191;&#12290;</title><link>http://arxiv.org/abs/2202.06950</link><description>&lt;p&gt;
&#22312;&#27979;&#22320;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;Sion&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#21644;&#40654;&#26364;&#22806;&#25512;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sion's Minimax Theorem in Geodesic Metric Spaces and a Riemannian Extragradient Algorithm. (arXiv:2202.06950v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#27979;&#22320;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;Sion&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#21644;&#40654;&#26364;&#22806;&#25512;&#31639;&#27861;&#65292;&#22312;&#20445;&#25345;&#38382;&#39064;&#21487;&#22788;&#29702;&#30340;&#21516;&#26102;&#65292;&#20026;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21028;&#26029;&#38750;&#20984;-&#38750;&#20985;&#38382;&#39064;&#26159;&#21542;&#23384;&#22312;&#38797;&#28857;&#36890;&#24120;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#35813;&#35770;&#25991;&#21521;&#29702;&#35299;&#19968;&#31867;&#20445;&#25345;&#21487;&#22788;&#29702;&#30340;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#30740;&#31350;&#20102;&#27979;&#22320;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#36825;&#25552;&#20379;&#20102;&#36890;&#24120;&#30340;&#20984;-&#20985;&#38797;&#28857;&#38382;&#39064;&#30340;&#24191;&#27867;&#25512;&#24191;&#12290;&#35770;&#25991;&#30340;&#31532;&#19968;&#20010;&#20027;&#35201;&#32467;&#26524;&#26159;Sion&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#30340;&#27979;&#22320;&#24230;&#37327;&#31354;&#38388;&#29256;&#26412;; &#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#35777;&#26126;&#26159;&#26032;&#39062;&#19988;&#24191;&#27867;&#21487;&#29992;&#30340;&#65292;&#22240;&#20026;&#23427;&#20165;&#22522;&#20110;&#26377;&#38480;&#20132;&#21449;&#24615;&#36136;&#12290;&#31532;&#20108;&#20010;&#20027;&#35201;&#32467;&#26524;&#26159;&#38024;&#23545;&#23436;&#25972;&#27979;&#22320;&#40654;&#26364;&#27969;&#24418;&#30340;&#19987;&#19994;&#21270;&#65306;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#20809;&#28369;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#19968;&#38454;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deciding whether saddle points exist or are approximable for nonconvex-nonconcave problems is usually intractable. This paper takes a step towards understanding a broad class of nonconvex-nonconcave minimax problems that do remain tractable. Specifically, it studies minimax problems over geodesic metric spaces, which provide a vast generalization of the usual convex-concave saddle point problems. The first main result of the paper is a geodesic metric space version of Sion's minimax theorem; we believe our proof is novel and broadly accessible as it relies on the finite intersection property alone. The second main result is a specialization to geodesically complete Riemannian manifolds: here, we devise and analyze the complexity of first-order methods for smooth minimax problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;Transformers&#29992;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;SepFormer&#27169;&#22411;&#24182;&#22312;&#21253;&#25324;&#21547;&#22122;&#21644;&#21547;&#22122;&#22238;&#22768;&#30340;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;&#21516;&#26102;&#65292;&#36824;&#20570;&#20102;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#39318;&#27425;&#23558;Linformers&#12289;Lonformers&#21644;ReFormers&#31561;&#39640;&#25928;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#24212;&#29992;&#20110;&#35821;&#38899;&#20998;&#31163;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2202.02884</link><description>&lt;p&gt;
&#25506;&#31350;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#29992;&#20110;&#35821;&#38899;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Exploring Self-Attention Mechanisms for Speech Separation. (arXiv:2202.02884v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;Transformers&#29992;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;SepFormer&#27169;&#22411;&#24182;&#22312;&#21253;&#25324;&#21547;&#22122;&#21644;&#21547;&#22122;&#22238;&#22768;&#30340;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;&#21516;&#26102;&#65292;&#36824;&#20570;&#20102;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#39318;&#27425;&#23558;Linformers&#12289;Lonformers&#21644;ReFormers&#31561;&#39640;&#25928;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#24212;&#29992;&#20110;&#35821;&#38899;&#20998;&#31163;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#12290;&#23427;&#20204;&#24448;&#24448;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#20248;&#20110;&#24490;&#29615;&#21644;&#21367;&#31215;&#27169;&#22411;&#65292;&#21516;&#26102;&#21033;&#29992;&#24182;&#34892;&#22788;&#29702;&#30340;&#20248;&#21183;&#12290;&#26368;&#36817;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SepFormer&#65292;&#22312;WSJ0-2/3 Mix&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#35821;&#38899;&#20998;&#31163;&#26041;&#38754;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#29992;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;Transformers&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#21547;&#22122;&#21644;&#21547;&#22122;&#22238;&#22768;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;LibriMix&#12289;WHAM&#65281;&#21644;WHAMR&#65281;&#65289;&#19978;&#25552;&#20379;&#32467;&#26524;&#65292;&#25193;&#23637;&#20102;&#25105;&#20204;&#20043;&#21069;&#20851;&#20110;SepFormer&#30340;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#25193;&#23637;&#21040;&#25191;&#34892;&#35821;&#38899;&#22686;&#24378;&#65292;&#24182;&#25552;&#20379;&#20102;&#28040;&#38500;&#22122;&#22768;&#21644;&#28151;&#21709;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#25454;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#35821;&#38899;&#20998;&#31163;&#20013;&#30740;&#31350;&#20102;&#20351;&#29992;&#39640;&#25928;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22914;Linformers&#12289;Lonformers&#21644;ReFormers&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23427;&#20204;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#38656;&#27714;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Reformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#20110;&#27969;&#34892;&#30340;ConvS2s&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21516;&#26102;&#20165;&#20351;&#29992;&#32422;&#22235;&#20998;&#20043;&#19968;&#30340;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have enabled impressive improvements in deep learning. They often outperform recurrent and convolutional models in many tasks while taking advantage of parallel processing. Recently, we proposed the SepFormer, which obtains state-of-the-art performance in speech separation with the WSJ0-2/3 Mix datasets. This paper studies in-depth Transformers for speech separation. In particular, we extend our previous findings on the SepFormer by providing results on more challenging noisy and noisy-reverberant datasets, such as LibriMix, WHAM!, and WHAMR!. Moreover, we extend our model to perform speech enhancement and provide experimental evidence on denoising and dereverberation tasks. Finally, we investigate, for the first time in speech separation, the use of efficient self-attention mechanisms such as Linformers, Lonformers, and ReFormers. We found that they reduce memory requirements significantly. For example, we show that the Reformer-based attention outperforms the popular Con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#21644;&#21487;&#39564;&#35777;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27979;&#35797;&#65292;&#20174;&#22810;&#20010;&#27979;&#35797;&#32773;&#36873;&#25321;&#38750;&#20844;&#24320;&#25968;&#25454;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#24182;&#20351;&#29992;&#25143;&#23545;CNN&#24615;&#33021;&#30340;&#30495;&#23454;&#34920;&#29616;&#30830;&#20449;&#26080;&#30097;&#12290;</title><link>http://arxiv.org/abs/2201.09186</link><description>&lt;p&gt;
pvCNN: &#38544;&#31169;&#20445;&#25252;&#21644;&#21487;&#39564;&#35777;&#30340;CNN&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
pvCNN: Privacy-Preserving and Verifiable Convolutional Neural Network Testing. (arXiv:2201.09186v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#21644;&#21487;&#39564;&#35777;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27979;&#35797;&#65292;&#20174;&#22810;&#20010;&#27979;&#35797;&#32773;&#36873;&#25321;&#38750;&#20844;&#24320;&#25968;&#25454;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#24182;&#20351;&#29992;&#25143;&#23545;CNN&#24615;&#33021;&#30340;&#30495;&#23454;&#34920;&#29616;&#30830;&#20449;&#26080;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#21644;&#21487;&#39564;&#35777;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27979;&#35797;&#65292;&#20351;CNN&#27169;&#22411;&#30340;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#36890;&#36807;&#36866;&#24403;&#22320;&#38598;&#25104;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#21644;&#38646;&#30693;&#35782;&#31616;&#27905;&#38750;&#20132;&#20114;&#24335;&#30693;&#35782;&#65288;zk-SNARK&#65289;&#65292;&#20197;&#21450;&#23558; CNN&#27169;&#22411;&#21010;&#20998;&#20026;&#31169;&#26377;&#37096;&#20998;&#21644;&#20844;&#20849;&#37096;&#20998;&#24182;&#20998;&#21035;&#22788;&#29702;CNN&#27979;&#35797;&#20013;&#30340;&#19981;&#21516;&#37096;&#20998;&#65292;&#20174;&#32780;&#24179;&#34913;&#23433;&#20840;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#26041;&#27861;&#65292;&#20174;&#22810;&#20010;&#27979;&#35797;&#32773;&#36873;&#25321;&#38750;&#20844;&#24320;&#25968;&#25454;&#65292;&#20174;&#32780;&#20351;&#29992;&#25143;&#23545;CNN&#24615;&#33021;&#30340;&#30495;&#23454;&#34920;&#29616;&#30830;&#20449;&#26080;&#30097;&#65292;&#24182;&#19988;&#20063;&#20445;&#25252;&#20102;&#27169;&#22411;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new approach for privacy-preserving and verifiable convolutional neural network (CNN) testing, enabling a CNN model developer to convince a user of the truthful CNN performance over non-public data from multiple testers, while respecting model privacy. To balance the security and efficiency issues, three new efforts are done by appropriately integrating homomorphic encryption (HE) and zero-knowledge succinct non-interactive argument of knowledge (zk-SNARK) primitives with the CNN testing. First, a CNN model to be tested is strategically partitioned into a private part kept locally by the model developer, and a public part outsourced to an outside server. Then, the private part runs over HE-protected test data sent by a tester and transmits its outputs to the public part for accomplishing subsequent computations of the CNN testing. Second, the correctness of the above CNN testing is enforced by generating zk-SNARK based proofs, with an emphasis on optimizing provin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Paddle-HeterPS&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#24230;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#21033;&#29992;&#22810;&#31181;&#31867;&#22411;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#22810;&#23618;&#27425;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2111.10635</link><description>&lt;p&gt;
HeterPS&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#35843;&#24230;&#30340;&#24322;&#26500;&#29615;&#22659;&#19979;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HeterPS: Distributed Deep Learning With Reinforcement Learning Based Scheduling in Heterogeneous Environments. (arXiv:2111.10635v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10635
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Paddle-HeterPS&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#24230;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#21033;&#29992;&#22810;&#31181;&#31867;&#22411;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#22810;&#23618;&#27425;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#35768;&#22810;&#23618;&#21644;&#22823;&#37327;&#21442;&#25968;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;DNN&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#22788;&#29702;&#20855;&#26377;&#35768;&#22810;&#31232;&#30095;&#29305;&#24449;&#30340;&#22823;&#35268;&#27169;&#36755;&#20837;&#25968;&#25454;&#65292;&#36825;&#20250;&#20135;&#29983;&#39640;&#24310;&#36831;&#21644;I/O&#25104;&#26412;&#65292;&#32780;&#26576;&#20123;&#23618;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#21033;&#29992;&#20998;&#24067;&#24335;&#35745;&#31639;&#36164;&#28304;&#26469;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#22810;&#31181;&#31867;&#22411;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22914;CPU&#21644;GPU&#31561;&#65292;&#20063;&#21487;&#29992;&#20110;&#20998;&#24067;&#24335;&#35757;&#32451;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#22810;&#23618;&#27425;&#22320;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#23545;&#35757;&#32451;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#36890;&#36807;&#24322;&#26500;&#35745;&#31639;&#36164;&#28304;&#39640;&#25928;&#22320;&#35757;&#32451;DNN&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#26694;&#26550;Paddle-Heterogeneous Parameter Server&#65288;Paddle-HeterPS&#65289;&#65292;&#30001;&#20998;&#24067;&#24335;&#26550;&#26500;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#24230;&#26041;&#27861;&#32452;&#25104;&#12290;&#19982;&#29616;&#26377;&#26694;&#26550;&#30456;&#27604;&#65292;Paddle-HeterPS&#30340;&#20248;&#28857;&#26377;&#19977;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) exploit many layers and a large number of parameters to achieve excellent performance. The training process of DNN models generally handles large-scale input data with many sparse features, which incurs high Input/Output (IO) cost, while some layers are compute-intensive. The training process generally exploits distributed computing resources to reduce training time. In addition, heterogeneous computing resources, e.g., CPUs, GPUs of multiple types, are available for the distributed training process. Thus, the scheduling of multiple layers to diverse computing resources is critical for the training process. To efficiently train a DNN model using the heterogeneous computing resources, we propose a distributed framework, i.e., Paddle-Heterogeneous Parameter Server (Paddle-HeterPS), composed of a distributed architecture and a Reinforcement Learning (RL)-based scheduling method. The advantages of Paddle-HeterPS are three-fold compared with existing frameworks. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22797;&#21512;&#26816;&#39564;&#38382;&#39064;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#27491;&#30830;&#30340;&#27169;&#22411;&#35268;&#33539;&#30340;&#38646;&#20551;&#35774;&#19979;&#65292;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#21442;&#25968;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2111.10275</link><description>&lt;p&gt;
&#24102;&#26377;&#26680;&#30340;&#22797;&#21512;&#36866;&#21512;&#24615;&#26816;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Composite Goodness-of-fit Tests with Kernels. (arXiv:2111.10275v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22797;&#21512;&#26816;&#39564;&#38382;&#39064;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#27491;&#30830;&#30340;&#27169;&#22411;&#35268;&#33539;&#30340;&#38646;&#20551;&#35774;&#19979;&#65292;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#21442;&#25968;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#21487;&#33021;&#20250;&#23545;&#27010;&#29575;&#27169;&#22411;&#30340;&#23454;&#29616;&#36896;&#25104;&#37325;&#22823;&#25361;&#25112;&#65292;&#36825;&#20419;&#20351;&#24320;&#21457;&#20986;&#19968;&#20123;&#30452;&#25509;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#40065;&#26834;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26356;&#20026;&#22797;&#26434;&#30340;&#26041;&#27861;&#26159;&#21542;&#38656;&#35201;&#21462;&#20915;&#20110;&#27169;&#22411;&#26159;&#21542;&#30495;&#30340;&#38169;&#35823;&#65292;&#30446;&#21069;&#32570;&#20047;&#36890;&#29992;&#30340;&#26041;&#27861;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22797;&#21512;&#26816;&#39564;&#38382;&#39064;&#65292;&#21363;&#25105;&#20204;&#26159;&#21542;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26469;&#33258;&#26576;&#20123;&#21442;&#25968;&#27169;&#22411;&#26063;&#20013;&#30340;&#20219;&#20309;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#21033;&#29992;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;&#26680;Stein&#24046;&#24322;&#30340;&#26368;&#23567;&#36317;&#31163;&#20272;&#35745;&#22120;&#12290;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#21253;&#25324;&#24403;&#21442;&#25968;&#27169;&#22411;&#30340;&#23494;&#24230;&#24050;&#30693;&#38500;&#26631;&#20934;&#21270;&#24120;&#25968;&#22806;&#65292;&#25110;&#32773;&#22914;&#26524;&#27169;&#22411;&#37319;&#29992;&#27169;&#25311;&#22120;&#24418;&#24335;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#27491;&#30830;&#30340;&#27169;&#22411;&#35268;&#33539;&#30340;&#38646;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#21442;&#25968;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24314;&#31435;&#25105;&#20204;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#29702;&#35770;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#21644;&#24322;&#24120;&#26816;&#27979;&#24212;&#29992;&#26696;&#20363;&#28436;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model misspecification can create significant challenges for the implementation of probabilistic models, and this has led to development of a range of robust methods which directly account for this issue. However, whether these more involved methods are required will depend on whether the model is really misspecified, and there is a lack of generally applicable methods to answer this question. In this paper, we propose one such method. More precisely, we propose kernel-based hypothesis tests for the challenging composite testing problem, where we are interested in whether the data comes from any distribution in some parametric family. Our tests make use of minimum distance estimators based on the maximum mean discrepancy and the kernel Stein discrepancy. They are widely applicable, including whenever the density of the parametric model is known up to normalisation constant, or if the model takes the form of a simulator. As our main result, we show that we are able to estimate the param
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30340;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#26680;&#26816;&#39564;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#27979;&#35797;&#65292;&#31216;&#20026;MMDAgg&#65292;&#20197;&#35299;&#20915;&#24179;&#28369;&#21442;&#25968;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2110.15073</link><description>&lt;p&gt;
MMD&#32858;&#21512;&#21452;&#26679;&#26412;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
MMD Aggregated Two-Sample Test. (arXiv:2110.15073v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30340;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#26680;&#26816;&#39564;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#27979;&#35797;&#65292;&#31216;&#20026;MMDAgg&#65292;&#20197;&#35299;&#20915;&#24179;&#28369;&#21442;&#25968;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30340;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#26680;&#26816;&#39564;&#12290;&#39318;&#20808;&#65292;&#23545;&#20110;&#22266;&#23450;&#30340;&#26680;&#65292;&#25105;&#20204;&#20351;&#29992;&#25490;&#21015;&#25110;&#37326;&#34542;&#33258;&#20030;&#65288;wild bootstrap&#65289;&#26500;&#36896;&#20102;&#19968;&#20010;MMD&#26816;&#39564;&#65292;&#36825;&#20004;&#31181;&#27969;&#34892;&#30340;&#25968;&#20540;&#31243;&#24207;&#21487;&#30830;&#23450;&#27979;&#35797;&#38408;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#27979;&#35797;&#21487;&#20197;&#22312;&#38750;&#28176;&#36817;&#24773;&#20917;&#19979;&#25511;&#21046;I&#22411;&#38169;&#35823;&#30340;&#27010;&#29575;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#23427;&#20173;&#28982;&#20445;&#25345;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#65292;&#36825;&#19982;&#20197;&#21069;&#30340;MMD&#27979;&#35797;&#19981;&#21516;&#65292;&#21069;&#32773;&#21482;&#33021;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#20445;&#35777;&#27491;&#30830;&#30340;&#27979;&#35797;&#27700;&#24179;&#12290;&#24403;&#23494;&#24230;&#24046;&#24322;&#22312;Sobolev&#29699;&#20013;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;MMD&#26816;&#39564;&#22312;&#29305;&#23450;&#30340;&#26680;&#20989;&#25968;&#19979;&#26159;&#26368;&#20248;&#30340;&#65292;&#35813;&#26680;&#20989;&#25968;&#20381;&#36182;&#20110;Sobolev&#29699;&#30340;&#24179;&#28369;&#21442;&#25968;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20010;&#21442;&#25968;&#26159;&#26410;&#30693;&#30340;&#65292;&#22240;&#27492;&#19981;&#33021;&#20351;&#29992;&#20855;&#26377;&#29305;&#23450;&#26680;&#30340;&#26368;&#20248;MMD&#26816;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#24179;&#22343;&#27979;&#35797;&#65292;&#31216;&#20026;MMDAgg&#12290;&#27979;&#35797;&#21151;&#29575;&#22312;Sobolev&#29699;&#30340;&#24179;&#28369;&#21442;&#25968;&#19978;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose two novel nonparametric two-sample kernel tests based on the Maximum Mean Discrepancy (MMD). First, for a fixed kernel, we construct an MMD test using either permutations or a wild bootstrap, two popular numerical procedures to determine the test threshold. We prove that this test controls the probability of type I error non-asymptotically. Hence, it can be used reliably even in settings with small sample sizes as it remains well-calibrated, which differs from previous MMD tests which only guarantee correct test level asymptotically. When the difference in densities lies in a Sobolev ball, we prove minimax optimality of our MMD test with a specific kernel depending on the smoothness parameter of the Sobolev ball. In practice, this parameter is unknown and, hence, the optimal MMD test with this particular kernel cannot be used. To overcome this issue, we construct an aggregated test, called MMDAgg, which is adaptive to the smoothness parameter. The test power is maximised ove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Sinkhorn&#36317;&#31163;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65292;&#25512;&#23548;&#20986;&#26356;&#23481;&#26131;&#22788;&#29702;&#19988;&#22312;&#23454;&#38469;&#20013;&#26356;&#21512;&#29702;&#30340;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2109.11926</link><description>&lt;p&gt;
Sinkhorn&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sinkhorn Distributionally Robust Optimization. (arXiv:2109.11926v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.11926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Sinkhorn&#36317;&#31163;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65292;&#25512;&#23548;&#20986;&#26356;&#23481;&#26131;&#22788;&#29702;&#19988;&#22312;&#23454;&#38469;&#20013;&#26356;&#21512;&#29702;&#30340;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;Sinkhorn&#36317;&#31163; -&#19968;&#31181;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;&#30340;Wasserstein&#36317;&#31163;&#21464;&#20307;- &#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#12290;&#25105;&#20204;&#20026;&#19968;&#33324;&#21517;&#20041;&#20998;&#24067;&#25512;&#23548;&#20102;&#20984;&#35268;&#21010;&#23545;&#20598;&#37325;&#26500;&#12290;&#30456;&#27604;&#20110;Wasserstein DRO&#65292;&#23545;&#20110;&#26356;&#22823;&#31867;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#22312;&#35745;&#31639;&#19978;&#26356;&#23481;&#26131;&#22788;&#29702;&#65292;&#23427;&#30340;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#23545;&#23454;&#38469;&#24212;&#29992;&#26356;&#21512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#20598;&#37325;&#26500;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#20559;&#26799;&#24230;&#31070;&#32463;&#20803;&#30340;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#25968;&#20540;&#23454;&#20363;&#65292;&#20197;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distributionally robust optimization (DRO) with Sinkhorn distance -a variant of Wasserstein distance based on entropic regularization. We derive convex programming dual reformulation for a general nominal distribution. Compared with Wasserstein DRO, it is computationally tractable for a larger class of loss functions, and its worst-case distribution is more reasonable for practical applications. To solve the dual reformulation, we develop a stochastic mirror descent algorithm using biased gradient oracles and analyze its convergence rate. Finally, we provide numerical examples using synthetic and real data to demonstrate its superior performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#28418;&#31227;&#19979;&#20248;&#21270;&#20984;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#32463;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#22312;&#20302;&#28418;&#31227;-&#22122;&#22768;&#27604;&#30340;&#24773;&#20917;&#19979;&#65292;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#37319;&#29992;&#27493;&#38271;&#34928;&#20943;&#31574;&#30053;&#21487;&#26174;&#33879;&#25552;&#21319;&#36319;&#36394;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2108.07356</link><description>&lt;p&gt;
&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Optimization under Distributional Drift. (arXiv:2108.07356v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.07356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#28418;&#31227;&#19979;&#20248;&#21270;&#20984;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#32463;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#22312;&#20302;&#28418;&#31227;-&#22122;&#22768;&#27604;&#30340;&#24773;&#20917;&#19979;&#65292;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#37319;&#29992;&#27493;&#38271;&#34928;&#20943;&#31574;&#30053;&#21487;&#26174;&#33879;&#25552;&#21319;&#36319;&#36394;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#26368;&#23567;&#21270;&#38543;&#26426;&#28436;&#21270;&#20984;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#36825;&#20010;&#28436;&#21270;&#36807;&#31243;&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#20381;&#36182;&#20110;&#26102;&#38388;&#21644;&#20915;&#31574;&#21464;&#37327;&#26412;&#36523;&#12290;&#36825;&#31867;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#12289;&#38543;&#26426;&#36319;&#36394;&#21644;&#25191;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#20445;&#35777;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#26399;&#26395;&#20540;&#21644;&#39640;&#27010;&#29575;&#19979;&#25104;&#31435;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#33719;&#24471;&#30340;&#25928;&#29575;&#20272;&#35745;&#26126;&#30830;&#22320;&#35299;&#32806;&#20102;&#20248;&#21270;&#35823;&#24046;&#12289;&#26799;&#24230;&#22122;&#22768;&#21644;&#26102;&#38388;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#20302;&#28418;&#31227;-&#22122;&#22768;&#27604;&#30340;&#21306;&#22495;&#65292;&#22312;&#36825;&#20010;&#21306;&#22495;&#37324;&#65292;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#30340;&#36319;&#36394;&#25928;&#29575;&#22240;&#27493;&#38271;&#34928;&#20943;&#31574;&#30053;&#32780;&#21463;&#30410;&#26174;&#33879;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of minimizing a convex function that is evolving according to unknown and possibly stochastic dynamics, which may depend jointly on time and on the decision variable itself. Such problems abound in the machine learning and signal processing literature, under the names of concept drift, stochastic tracking, and performative prediction. We provide novel non-asymptotic convergence guarantees for stochastic algorithms with iterate averaging, focusing on bounds valid both in expectation and with high probability. The efficiency estimates we obtain clearly decouple the contributions of optimization error, gradient noise, and time drift. Notably, we identify a low drift-to-noise regime in which the tracking efficiency of the proximal stochastic gradient method benefits significantly from a step decay schedule. Numerical experiments illustrate our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#24102;&#26377;&#23450;&#24120;&#22823;&#23398;&#20064;&#29575;&#30340;SGD&#21487;&#33021;&#34920;&#29616;&#20986;&#35768;&#22810;&#22855;&#24618;&#19988;&#28508;&#22312;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#21253;&#25324;&#65306;&#25910;&#25947;&#20110;&#23616;&#37096;&#26368;&#22823;&#20540;&#12289;&#32531;&#24930;&#36234;&#36807;&#38797;&#28857;&#21644;&#26356;&#21916;&#27426;&#23574;&#38160;&#30340;&#26368;&#23567;&#20540;&#12290;&#36825;&#24378;&#35843;&#20102;&#28145;&#20837;&#20998;&#26512;SGD&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20316;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2107.11774</link><description>&lt;p&gt;
&#24102;&#26377;&#23450;&#24120;&#22823;&#23398;&#20064;&#29575;&#30340;SGD&#21487;&#33021;&#25910;&#25947;&#20110;&#23616;&#37096;&#26368;&#22823;&#20540;
&lt;/p&gt;
&lt;p&gt;
SGD with a Constant Large Learning Rate Can Converge to Local Maxima. (arXiv:2107.11774v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.11774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#24102;&#26377;&#23450;&#24120;&#22823;&#23398;&#20064;&#29575;&#30340;SGD&#21487;&#33021;&#34920;&#29616;&#20986;&#35768;&#22810;&#22855;&#24618;&#19988;&#28508;&#22312;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#21253;&#25324;&#65306;&#25910;&#25947;&#20110;&#23616;&#37096;&#26368;&#22823;&#20540;&#12289;&#32531;&#24930;&#36234;&#36807;&#38797;&#28857;&#21644;&#26356;&#21916;&#27426;&#23574;&#38160;&#30340;&#26368;&#23567;&#20540;&#12290;&#36825;&#24378;&#35843;&#20102;&#28145;&#20837;&#20998;&#26512;SGD&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20316;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#30740;&#31350;&#36890;&#24120;&#30528;&#30524;&#20110;&#20854;&#25104;&#21151;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#20808;&#21069;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#19981;&#25104;&#31435;&#30340;&#24773;&#20917;&#19979;&#65292;SGD&#21487;&#33021;&#34920;&#29616;&#20986;&#35768;&#22810;&#22855;&#24618;&#19988;&#28508;&#22312;&#30340;&#19981;&#33391;&#34892;&#20026;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26223;&#35266;&#21644;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#24471;&#65288;1&#65289;SGD&#25910;&#25947;&#20110;&#23616;&#37096;&#26368;&#22823;&#20540;&#65292;&#65288;2&#65289;SGD&#32531;&#24930;&#36234;&#36807;&#38797;&#28857;&#65292;(3) SGD&#26356;&#21916;&#27426;&#23574;&#38160;&#30340;&#26368;&#23567;&#20540;&#32780;&#38750;&#24179;&#22374;&#30340;&#26368;&#23567;&#20540;&#65292;(4) AMSGrad&#25910;&#25947;&#20110;&#23616;&#37096;&#26368;&#22823;&#20540;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#26497;&#31616;&#30340;&#31070;&#32463;&#32593;&#32476;&#31034;&#20363;&#36827;&#34892;&#20102;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#21516;&#26102;&#20998;&#26512;&#23567;&#25209;&#37327;&#37319;&#26679;&#12289;&#31163;&#25955;&#26102;&#38388;&#26356;&#26032;&#35268;&#21017;&#21644;&#29616;&#23454;&#26223;&#35266;&#20197;&#20102;&#35299;SGD&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous works on stochastic gradient descent (SGD) often focus on its success. In this work, we construct worst-case optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange and potentially undesirable behaviors. Specifically, we construct landscapes and data distributions such that (1) SGD converges to local maxima, (2) SGD escapes saddle points arbitrarily slowly, (3) SGD prefers sharp minima over flat ones, and (4) AMSGrad converges to local maxima. We also realize results in a minimal neural network-like example. Our results highlight the importance of simultaneously analyzing the minibatch sampling, discrete-time updates rules, and realistic landscapes to understand the role of SGD in deep learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#32441;&#26679;&#26412;&#24182;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#29256;&#26435;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#65292;&#20855;&#26377;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.11760</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Fingerprinting Generative Adversarial Networks. (arXiv:2106.11760v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#32441;&#26679;&#26412;&#24182;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#29256;&#26435;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#65292;&#20855;&#26377;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#30001;&#20110;&#21830;&#19994;GAN&#30340;&#29983;&#20135;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20154;&#21147;&#36164;&#28304;&#65292;&#22240;&#27492;&#36843;&#20999;&#38656;&#35201;&#29256;&#26435;&#20445;&#25252;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#12290;&#25105;&#20204;&#31361;&#30772;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#25152;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21019;&#36896;&#24615;&#22320;&#20174;&#30446;&#26631;GAN&#21644;&#20998;&#31867;&#22120;&#26500;&#24314;&#19968;&#20010;&#22797;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#36825;&#20010;&#22797;&#21512;&#27169;&#22411;&#20013;&#20135;&#29983;&#25351;&#32441;&#26679;&#26412;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#29256;&#26435;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#26696;&#21551;&#21457;&#20102;&#19968;&#20123;&#20855;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#25152;&#38656;&#35201;&#30340;&#19981;&#21516;&#23433;&#20840;&#35201;&#27714;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#35777;&#26126;&#35813;&#26041;&#26696;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have been widely used in various application scenarios. Since the production of a commercial GAN requires substantial computational and human resources, the copyright protection of GANs is urgently needed. In this paper, we present the first fingerprinting scheme for the Intellectual Property (IP) protection of GANs. We break through the stealthiness and robustness bottlenecks suffered by previous fingerprinting methods for classification models being naively transferred to GANs. Specifically, we innovatively construct a composite deep learning model from the target GAN and a classifier. Then we generate fingerprint samples from this composite model, and embed them in the classifier for effective ownership verification. This scheme inspires some concrete methodologies to practically protect the modern GAN models. Theoretical analysis proves that these methods can satisfy different security requirements necessary for IP protection. We also conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#28216;&#25103;&#20013;&#35782;&#21035;&#26368;&#20248;&#33218;&#30340;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#25506;&#32034;&#33218;&#30340;&#22870;&#21169;&#24046;&#36317;&#21644;&#26041;&#24046;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#20998;&#32452;&#20013;&#20301;&#25968;&#28120;&#27760;&#30340;&#26032;&#26041;&#27861;&#26681;&#25454;&#25910;&#38598;&#30340;&#20449;&#24687;&#20570;&#20986;&#26410;&#26469;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20445;&#35777;&#20197;&#27010;&#29575;(1-&#948;)&#36755;&#20986;&#26368;&#20248;&#33218;&#65292;&#24182;&#20351;&#29992;&#26368;&#22810;&#30340;O&#65288;&#931;(i=1)^n (&#963;i&#178;/&#916;i&#178;+1/&#916;i)(ln&#948;-1+ln ln&#916;i-1)&#65289;&#20010;&#26679;&#26412;&#65292;&#36825;&#27604;&#26041;&#24046;&#29420;&#31435;&#31639;&#27861;&#33719;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2106.10417</link><description>&lt;p&gt;
&#26041;&#24046;&#30456;&#20851;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Variance-Dependent Best Arm Identification. (arXiv:2106.10417v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.10417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#28216;&#25103;&#20013;&#35782;&#21035;&#26368;&#20248;&#33218;&#30340;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#25506;&#32034;&#33218;&#30340;&#22870;&#21169;&#24046;&#36317;&#21644;&#26041;&#24046;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#20998;&#32452;&#20013;&#20301;&#25968;&#28120;&#27760;&#30340;&#26032;&#26041;&#27861;&#26681;&#25454;&#25910;&#38598;&#30340;&#20449;&#24687;&#20570;&#20986;&#26410;&#26469;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20445;&#35777;&#20197;&#27010;&#29575;(1-&#948;)&#36755;&#20986;&#26368;&#20248;&#33218;&#65292;&#24182;&#20351;&#29992;&#26368;&#22810;&#30340;O&#65288;&#931;(i=1)^n (&#963;i&#178;/&#916;i&#178;+1/&#916;i)(ln&#948;-1+ln ln&#916;i-1)&#65289;&#20010;&#26679;&#26412;&#65292;&#36825;&#27604;&#26041;&#24046;&#29420;&#31435;&#31639;&#27861;&#33719;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#28216;&#25103;&#20013;&#35782;&#21035;&#26368;&#20248;&#33218;&#30340;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#20010;&#20174;1&#21040;n&#26631;&#21495;&#30340;&#33218;&#30340;&#38598;&#21512;&#65292;&#27599;&#20010;&#33218;i&#37117;&#19982;&#19968;&#20010;&#25903;&#25345;[0,1]&#19978;&#24179;&#22343;&#20540;&#20026;&#952;i&#21644;&#26041;&#24046;&#20026;&#963;i&#178;&#30340;&#26410;&#30693;&#22870;&#21169;&#20998;&#24067;&#30456;&#20851;&#32852;&#12290;&#20551;&#35774;&#952;1&gt;&#952;2&#8805;...&#8805;&#952;n&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#25506;&#32034;&#33218;&#30340;&#22870;&#21169;&#24046;&#36317;&#21644;&#26041;&#24046;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#20998;&#32452;&#20013;&#20301;&#25968;&#28120;&#27760;&#30340;&#26032;&#26041;&#27861;&#26681;&#25454;&#25910;&#38598;&#30340;&#20449;&#24687;&#20570;&#20986;&#26410;&#26469;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20445;&#35777;&#20197;&#27010;&#29575;(1-&#948;)&#36755;&#20986;&#26368;&#20248;&#33218;&#65292;&#24182;&#20351;&#29992;&#26368;&#22810;&#30340;O&#65288;&#931;(i=1)^n (&#963;i&#178;/&#916;i&#178;+1/&#916;i)(ln&#948;-1+ln ln&#916;i-1)&#65289;&#20010;&#26679;&#26412;&#65292;&#20854;&#20013; &#916;i (i&#8805;2)&#34920;&#31034;&#33218;i&#19982;&#26368;&#20248;&#33218;&#20043;&#38388;&#30340;&#22870;&#21169;&#24046;&#36317;&#65292;&#25105;&#20204;&#23450;&#20041; &#916;1 = &#916;2&#12290;&#36825;&#27604;&#26041;&#24046;&#29420;&#31435;&#31639;&#27861;&#33719;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of identifying the best arm in a stochastic multi-armed bandit game. Given a set of $n$ arms indexed from $1$ to $n$, each arm $i$ is associated with an unknown reward distribution supported on $[0,1]$ with mean $\theta_i$ and variance $\sigma_i^2$. Assume $\theta_1 &gt; \theta_2 \geq \cdots \geq\theta_n$. We propose an adaptive algorithm which explores the gaps and variances of the rewards of the arms and makes future decisions based on the gathered information using a novel approach called \textit{grouped median elimination}. The proposed algorithm guarantees to output the best arm with probability $(1-\delta)$ and uses at most $O \left(\sum_{i = 1}^n \left(\frac{\sigma_i^2}{\Delta_i^2} + \frac{1}{\Delta_i}\right)(\ln \delta^{-1} + \ln \ln \Delta_i^{-1})\right)$ samples, where $\Delta_i$ ($i \geq 2$) denotes the reward gap between arm $i$ and the best arm and we define $\Delta_1 = \Delta_2$. This achieves a significant advantage over the variance-independent algorit
&lt;/p&gt;</description></item><item><title>FairCanary&#26159;&#19968;&#20010;&#25345;&#32493;&#27169;&#22411;&#30417;&#25511;&#31995;&#32479;&#65292;&#20351;&#29992;&#37327;&#21270;&#20154;&#21475;&#28418;&#31227;(QDD)&#24230;&#37327;&#27169;&#22411;&#20559;&#24046;&#65292;&#36991;&#20813;&#20256;&#32479;&#38408;&#20540;&#20559;&#24046;&#24230;&#37327;&#30340;&#32479;&#35745;&#38480;&#21046;&#65292;&#21487;&#20197;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#20844;&#24179;&#24615;&#20998;&#26512;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#21487;&#25805;&#20316;&#24615;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2106.07057</link><description>&lt;p&gt;
FairCanary: &#24555;&#36895;&#25345;&#32493;&#30340;&#21487;&#35299;&#37322;&#20844;&#24179;&#24615;&#30417;&#25511;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FairCanary: Rapid Continuous Explainable Fairness. (arXiv:2106.07057v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07057
&lt;/p&gt;
&lt;p&gt;
FairCanary&#26159;&#19968;&#20010;&#25345;&#32493;&#27169;&#22411;&#30417;&#25511;&#31995;&#32479;&#65292;&#20351;&#29992;&#37327;&#21270;&#20154;&#21475;&#28418;&#31227;(QDD)&#24230;&#37327;&#27169;&#22411;&#20559;&#24046;&#65292;&#36991;&#20813;&#20256;&#32479;&#38408;&#20540;&#20559;&#24046;&#24230;&#37327;&#30340;&#32479;&#35745;&#38480;&#21046;&#65292;&#21487;&#20197;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#20844;&#24179;&#24615;&#20998;&#26512;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#21487;&#25805;&#20316;&#24615;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24050;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#27169;&#22411;&#30340;&#22833;&#36133;&#21644;&#26032;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#20986;&#29616;&#20102;&#25552;&#20379;&#25345;&#32493;&#27169;&#22411;&#30417;&#25511;&#30340;&#31995;&#32479;&#12290;&#29616;&#26377;&#30340;&#30417;&#25511;&#31995;&#32479;&#25345;&#32493;&#36319;&#36394;&#24050;&#37096;&#32626;&#30340;ML&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#27599;&#20010;&#39044;&#27979;&#35745;&#31639;&#29305;&#24449;&#37325;&#35201;&#24615;(&#21363;&#35299;&#37322;)&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#30830;&#23450;&#26032;&#20986;&#29616;&#30340;&#27169;&#22411;&#24615;&#33021;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#37327;&#21270;&#20154;&#21475;&#28418;&#31227;(QDD)&#65292;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20559;&#24046;&#37327;&#21270;&#24230;&#37327;&#65292;&#20351;&#29992;&#20998;&#20301;&#25968;&#20998;&#32452;&#26469;&#24230;&#37327;&#23376;&#32452;&#20043;&#38388;&#25972;&#20307;&#39044;&#27979;&#20998;&#24067;&#30340;&#24046;&#24322;&#12290;QDD&#38750;&#24120;&#36866;&#21512;&#20110;&#36830;&#32493;&#30417;&#25511;&#22330;&#26223;&#65292;&#19981;&#20250;&#21463;&#21040;&#20256;&#32479;&#22522;&#20110;&#38408;&#20540;&#20559;&#24046;&#24230;&#37327;&#30340;&#32479;&#35745;&#38480;&#21046;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#32467;&#26524;&#26631;&#31614;(&#21487;&#33021;&#26080;&#27861;&#22312;&#36816;&#34892;&#26102;&#33719;&#24471;)&#12290;&#25105;&#20204;&#23558;QDD&#32435;&#20837;&#36830;&#32493;&#27169;&#22411;&#30417;&#25511;&#31995;&#32479;FairCanary&#20013;&#65292;&#35813;&#31995;&#32479;&#37325;&#29992;&#29616;&#26377;&#30340;&#23454;&#39564;&#22522;&#30784;&#35774;&#26045;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#25552;&#20379;&#24555;&#36895;&#12289;&#25345;&#32493;&#19988;&#21487;&#35299;&#37322;&#30340;&#20844;&#24179;&#24615;&#20998;&#26512;&#12290;FairCanary&#22312;&#20960;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21521;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#21487;&#25805;&#20316;&#24615;&#35265;&#35299;&#65292;&#20197;&#25552;&#39640;&#20854;ML&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systems that offer continuous model monitoring have emerged in response to (1) well-documented failures of deployed Machine Learning (ML) and Artificial Intelligence (AI) models and (2) new regulatory requirements impacting these models. Existing monitoring systems continuously track the performance of deployed ML models and compute feature importance (a.k.a. explanations) for each prediction to help developers identify the root causes of emergent model performance problems.  We present Quantile Demographic Drift (QDD), a novel model bias quantification metric that uses quantile binning to measure differences in the overall prediction distributions over subgroups. QDD is ideal for continuous monitoring scenarios, does not suffer from the statistical limitations of conventional threshold-based bias metrics, and does not require outcome labels (which may not be available at runtime). We incorporate QDD into a continuous model monitoring system, called FairCanary, that reuses existing exp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#39640;&#26031;&#22122;&#22768;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#19982;&#20854;&#20182;&#24378;&#22823;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#22686;&#24378;&#22522;&#30784;&#30456;&#27604;&#65292;&#21487;&#20197;&#23558;&#40065;&#26834;&#24615;&#25552;&#39640;4.2-18.4&#65285;&#20197;&#24212;&#23545;&#26410;&#39044;&#35265;&#30340;&#22122;&#22768;&#27745;&#26579;&#12290;</title><link>http://arxiv.org/abs/2104.01231</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#39640;&#26031;&#22122;&#22768;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#29992;&#20110;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Diverse Gaussian Noise Consistency Regularization for Robustness and Uncertainty Calibration. (arXiv:2104.01231v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.01231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#39640;&#26031;&#22122;&#22768;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#19982;&#20854;&#20182;&#24378;&#22823;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#22686;&#24378;&#22522;&#30784;&#30456;&#27604;&#65292;&#21487;&#20197;&#23558;&#40065;&#26834;&#24615;&#25552;&#39640;4.2-18.4&#65285;&#20197;&#24212;&#23545;&#26410;&#39044;&#35265;&#30340;&#22122;&#22768;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19968;&#33268;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#39640;&#27700;&#24179;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#65292;&#21508;&#31181;&#31867;&#22411;&#30340;&#25439;&#22351;&#20250;&#23548;&#33268;&#34920;&#29616;&#20005;&#37325;&#19979;&#38477;&#65292;&#36825;&#19982;&#39044;&#26399;&#30340;&#24773;&#20917;&#26377;&#25152;&#20559;&#24046;&#12290;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#21487;&#20197;&#22312;&#20986;&#29616;&#26410;&#39044;&#35265;&#21040;&#30340;&#39046;&#22495;&#20559;&#31227;&#26102;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#22270;&#20687;&#33719;&#21462;&#38454;&#27573;&#65292;&#25968;&#23383;&#22122;&#22768;&#27745;&#26579;&#32463;&#24120;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#39640;&#26031;&#22122;&#22768;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22270;&#20687;&#20998;&#31867;&#22120;&#22312;&#21508;&#31181;&#27745;&#26579;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#26412;&#22320;&#25439;&#22833;&#26223;&#35266;&#20998;&#26512;&#65292;&#25105;&#20204;&#23548;&#20986;&#30028;&#38480;&#65292;&#20197;&#28608;&#21169;&#21644;&#29702;&#35299;&#25105;&#20204;&#30340;&#39640;&#26031;&#22122;&#22768;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#30340;&#34892;&#20026;&#12290;&#30456;&#27604;&#20110;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#20854;&#20182;&#24378;&#22823;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#22686;&#24378;&#22522;&#30784;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#40065;&#26834;&#24615;&#25552;&#39640;4.2-18.4&#65285;&#20197;&#24212;&#23545;&#26410;&#39044;&#35265;&#30340;&#22122;&#22768;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks achieve high prediction accuracy when the train and test distributions coincide. In practice though, various types of corruptions occur which deviate from this setup and cause severe performance degradations. Few methods have been proposed to address generalization in the presence of unforeseen domain shifts. In particular, digital noise corruptions arise commonly in practice during the image acquisition stage and present a significant challenge for current methods. In this paper, we propose a diverse Gaussian noise consistency regularization method for improving robustness of image classifiers under a variety of corruptions while still maintaining high clean accuracy. We derive bounds to motivate and understand the behavior of our Gaussian noise consistency regularization using a local loss landscape analysis. Our approach improves robustness against unforeseen noise corruptions by 4.2-18.4% over adversarial training and other strong diverse data augmentation base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#31574;&#30053;, &#19968;&#31181;&#26159;&#36880;&#27493;&#34928;&#20943;&#26412;&#22320;&#26799;&#24230;&#26435;&#37325;&#30340;&#34928;&#20943;&#27169;&#24335;, &#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#20195;&#20215;&#26368;&#23567;&#21270;&#35774;&#35745;&#30340;&#20849;&#35782;&#31639;&#27861;&#26469;&#20943;&#23569;&#27169;&#22411;&#20043;&#38388;&#30340;&#36890;&#20449;&#37327;, &#26377;&#25928;&#35299;&#20915;&#20102;&#29420;&#31435;&#23398;&#20064;&#29615;&#22659;&#24322;&#36136;&#24615;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#20449;&#24320;&#38144;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2103.13026</link><description>&lt;p&gt;
&#26377;&#25928;&#36890;&#20449;&#19979;&#32852;&#37030;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26799;&#24230;&#25910;&#25947;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
The Gradient Convergence Bound of Federated Multi-Agent Reinforcement Learning with Efficient Communication. (arXiv:2103.13026v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.13026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#31574;&#30053;, &#19968;&#31181;&#26159;&#36880;&#27493;&#34928;&#20943;&#26412;&#22320;&#26799;&#24230;&#26435;&#37325;&#30340;&#34928;&#20943;&#27169;&#24335;, &#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#20195;&#20215;&#26368;&#23567;&#21270;&#35774;&#35745;&#30340;&#20849;&#35782;&#31639;&#27861;&#26469;&#20943;&#23569;&#27169;&#22411;&#20043;&#38388;&#30340;&#36890;&#20449;&#37327;, &#26377;&#25928;&#35299;&#20915;&#20102;&#29420;&#31435;&#23398;&#20064;&#29615;&#22659;&#24322;&#36136;&#24615;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#20449;&#24320;&#38144;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#20013;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20915;&#31574;&#30340;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#65292;&#20294;&#26159;&#30001;&#20110;&#29420;&#31435;&#23398;&#20064;&#29615;&#22659;&#24322;&#36136;&#24615;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#20449;&#24320;&#38144;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#25910;&#25947;&#38382;&#39064;&#12290;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#31574;&#30053;&#65292;&#19968;&#31181;&#26159;&#36880;&#27493;&#34928;&#20943;&#26412;&#22320;&#26799;&#24230;&#26435;&#37325;&#30340;&#34928;&#20943;&#27169;&#24335;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#20195;&#20215;&#26368;&#23567;&#21270;&#35774;&#35745;&#30340;&#20849;&#35782;&#31639;&#27861;&#26469;&#20943;&#23569;&#27169;&#22411;&#20043;&#38388;&#30340;&#36890;&#20449;&#37327;&#65292;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper considers independent reinforcement learning (IRL) for multi-agent collaborative decision-making in the paradigm of federated learning (FL). However, FL generates excessive communication overheads between agents and a remote central server, especially when it involves a large number of agents or iterations. Besides, due to the heterogeneity of independent learning environments, multiple agents may undergo asynchronous Markov decision processes (MDPs), which will affect the training samples and the model's convergence performance. On top of the variation-aware periodic averaging (VPA) method and the policy-based deep reinforcement learning (DRL) algorithm (i.e., proximal policy optimization (PPO)), this paper proposes two advanced optimization schemes orienting to stochastic gradient descent (SGD): 1) A decay-based scheme gradually decays the weights of a model's local gradients with the progress of successive local updates, and 2) By representing the agents as a graph, a cons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#29616;&#26377;&#20010;&#24615;&#21270;FL&#30446;&#26631;&#30340;&#36890;&#29992;&#20248;&#21270;&#22120;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#24191;&#27867;&#19968;&#31867;&#24378;&#20984;&#24615;&#20010;&#24615;&#21270;FL&#27169;&#22411;&#30340;&#20840;&#38754;&#20248;&#21270;&#29702;&#35770;&#12290;&#20854;&#26041;&#27861;&#22312;&#36890;&#20449;&#21644;&#26412;&#22320;&#35745;&#31639;&#26041;&#38754;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#20248;&#36234;&#24615;&#65292;&#24182;&#33021;&#22815;&#24674;&#22797;&#24212;&#23545;&#29305;&#23450;&#20010;&#24615;&#21270;FL&#30446;&#26631;&#30340;&#26368;&#20339;&#35745;&#31639;&#21644;&#36890;&#20449;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2102.09743</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65306;&#32479;&#19968;&#26694;&#26550;&#21644;&#36890;&#29992;&#20248;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques. (arXiv:2102.09743v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.09743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#29616;&#26377;&#20010;&#24615;&#21270;FL&#30446;&#26631;&#30340;&#36890;&#29992;&#20248;&#21270;&#22120;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#24191;&#27867;&#19968;&#31867;&#24378;&#20984;&#24615;&#20010;&#24615;&#21270;FL&#27169;&#22411;&#30340;&#20840;&#38754;&#20248;&#21270;&#29702;&#35770;&#12290;&#20854;&#26041;&#27861;&#22312;&#36890;&#20449;&#21644;&#26412;&#22320;&#35745;&#31639;&#26041;&#38754;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#20248;&#36234;&#24615;&#65292;&#24182;&#33021;&#22815;&#24674;&#22797;&#24212;&#23545;&#29305;&#23450;&#20010;&#24615;&#21270;FL&#30446;&#26631;&#30340;&#26368;&#20339;&#35745;&#31639;&#21644;&#36890;&#20449;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20248;&#21270;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#20248;&#21270;&#22120;&#65292;&#21487;&#24212;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#20010;&#24615;&#21270;FL&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;&#19968;&#31181;&#23450;&#21046;&#30340;&#26412;&#22320;SGD&#21464;&#20307;&#21644;&#21152;&#36895;&#22352;&#26631;&#19979;&#38477;/&#21152;&#36895;SVRCD&#30340;&#21464;&#20307;&#12290;&#36890;&#36807;&#30740;&#31350;&#19968;&#33324;&#30340;&#20010;&#24615;&#21270;&#30446;&#26631;&#65292;&#33021;&#22815;&#23558;&#35768;&#22810;&#29616;&#26377;&#30340;&#20010;&#24615;&#21270;FL&#30446;&#26631;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#24674;&#22797;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#25991;&#29486;&#20013;&#24191;&#27867;&#30340;&#19968;&#31867;&#24378;&#20984;&#24615;&#20010;&#24615;&#21270;FL&#27169;&#22411;&#30340;&#20840;&#38754;&#20248;&#21270;&#29702;&#35770;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36890;&#20449;&#21644;&#26412;&#22320;&#35745;&#31639;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#21644;/&#25110;&#20248;&#36234;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#36890;&#29992;&#20248;&#21270;&#27714;&#35299;&#22120;&#21644;&#29702;&#35770;&#21487;&#20197;&#24674;&#22797;&#29992;&#20110;&#29305;&#23450;&#20010;&#24615;&#21270;FL&#30446;&#26631;&#30340;&#26368;&#20339;&#24050;&#30693;&#36890;&#20449;&#21644;&#35745;&#31639;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#20248;&#21270;&#22120;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21487;&#20197;&#20813;&#21435;&#35774;&#35745;&#29305;&#23450;&#20219;&#21153;&#20248;&#21270;&#22120;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the optimization aspects of personalized Federated Learning (FL). We propose general optimizers that can be applied to numerous existing personalized FL objectives, specifically a tailored variant of Local SGD and variants of accelerated coordinate descent/accelerated SVRCD. By examining a general personalized objective capable of recovering many existing personalized FL objectives as special cases, we develop a comprehensive optimization theory applicable to a wide range of strongly convex personalized FL models in the literature. We showcase the practicality and/or optimality of our methods in terms of communication and local computation. Remarkably, our general optimization solvers and theory can recover the best-known communication and computation guarantees for addressing specific personalized FL objectives. Consequently, our proposed methods can serve as universal optimizers, rendering the design of task-specific optimizers unnecessary in many instances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#25991;&#29486;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#19978;&#22522;&#20110;&#21407;&#21017;&#19988;&#39046;&#22495;&#26080;&#20851;&#30340;&#25968;&#25454;&#24322;&#24120;&#20998;&#31867;&#27861;&#65292;&#24182;&#21576;&#29616;&#20102;&#24322;&#24120;&#31867;&#22411;&#21644;&#23376;&#31867;&#22411;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2007.15634</link><description>&lt;p&gt;
&#20851;&#20110;&#24322;&#24120;&#30340;&#24615;&#36136;&#21644;&#31867;&#22411;&#65306;&#25968;&#25454;&#20559;&#24046;&#30340;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
On the Nature and Types of Anomalies: A Review of Deviations in Data. (arXiv:2007.15634v5 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.15634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#25991;&#29486;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#19978;&#22522;&#20110;&#21407;&#21017;&#19988;&#39046;&#22495;&#26080;&#20851;&#30340;&#25968;&#25454;&#24322;&#24120;&#20998;&#31867;&#27861;&#65292;&#24182;&#21576;&#29616;&#20102;&#24322;&#24120;&#31867;&#22411;&#21644;&#23376;&#31867;&#22411;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26159;&#25351;&#25968;&#25454;&#38598;&#20013;&#20197;&#26576;&#31181;&#26041;&#24335;&#19981;&#23547;&#24120;&#19988;&#19981;&#31526;&#21512;&#19968;&#33324;&#27169;&#24335;&#30340;&#20107;&#20214;&#12290;&#24322;&#24120;&#30340;&#27010;&#24565;&#36890;&#24120;&#27809;&#26377;&#26126;&#30830;&#23450;&#20041;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#27169;&#31946;&#30340;&#21644;&#20381;&#36182;&#20110;&#20855;&#20307;&#39046;&#22495;&#30340;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#24050;&#26377;&#32422;250&#24180;&#30340;&#20986;&#29256;&#29289;&#65292;&#20294;&#36804;&#20170;&#23578;&#26410;&#21457;&#34920;&#36807;&#20851;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#24322;&#24120;&#30340;&#32508;&#21512;&#21644;&#20855;&#20307;&#27010;&#36848;&#12290;&#36890;&#36807;&#24191;&#27867;&#25991;&#29486;&#32508;&#36848;&#65292;&#26412;&#30740;&#31350;&#22240;&#27492;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#19978;&#22522;&#20110;&#21407;&#21017;&#19988;&#39046;&#22495;&#26080;&#20851;&#30340;&#25968;&#25454;&#24322;&#24120;&#20998;&#31867;&#27861;&#65292;&#24182;&#21576;&#29616;&#20102;&#24322;&#24120;&#31867;&#22411;&#21644;&#23376;&#31867;&#22411;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#20026;&#20102;&#26126;&#30830;&#23450;&#20041;&#24322;&#24120;&#30340;&#27010;&#24565;&#21450;&#20854;&#19981;&#21516;&#30340;&#34920;&#29616;&#65292;&#35813;&#20998;&#31867;&#27861;&#36816;&#29992;&#20102;&#20116;&#20010;&#32500;&#24230;&#65306;&#25968;&#25454;&#31867;&#22411;&#12289;&#20851;&#31995;&#30340;&#22522;&#25968;&#12289;&#24322;&#24120;&#32423;&#21035;&#12289;&#25968;&#25454;&#32467;&#26500;&#21644;&#25968;&#25454;&#20998;&#24067;&#12290;&#36825;&#20123;&#22522;&#26412;&#19988;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#32500;&#24230;&#33258;&#28982;&#22320;&#20135;&#29983;&#20102;3&#20010;&#22823;&#32452;&#12289;9&#31181;&#22522;&#26412;&#31867;&#22411;&#21644;63&#20010;&#24322;&#24120;&#23376;&#31867;&#22411;&#12290;&#35813;&#20998;&#31867;&#27861;&#26377;&#21161;&#20110;&#35780;&#20272;&#26576;&#20010;&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#23545;&#24322;&#24120;&#30340;&#21151;&#33021;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomalies are occurrences in a dataset that are in some way unusual and do not fit the general patterns. The concept of the anomaly is typically ill-defined and perceived as vague and domain-dependent. Moreover, despite some 250 years of publications on the topic, no comprehensive and concrete overviews of the different types of anomalies have hitherto been published. By means of an extensive literature review this study therefore offers the first theoretically principled and domain-independent typology of data anomalies and presents a full overview of anomaly types and subtypes. To concretely define the concept of the anomaly and its different manifestations, the typology employs five dimensions: data type, cardinality of relationship, anomaly level, data structure, and data distribution. These fundamental and data-centric dimensions naturally yield 3 broad groups, 9 basic types, and 63 subtypes of anomalies. The typology facilitates the evaluation of the functional capabilities of an
&lt;/p&gt;</description></item><item><title>&#36825;&#26412;&#19987;&#33879;&#20171;&#32461;&#20102;&#22312;&#32447;&#23398;&#20064;&#30340;&#22522;&#26412;&#27010;&#24565;&#20197;&#21450;&#20984;&#20248;&#21270;&#32972;&#26223;&#19979;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#31639;&#27861;, &#21253;&#25324;&#27431;&#20960;&#37324;&#24471;&#21644;&#38750;&#27431;&#20960;&#37324;&#24471;&#35774;&#32622;&#20013;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#25110;&#36981;&#24490;&#27491;&#21017;&#21270;&#39046;&#23548;&#32773;&#31561;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/1912.13213</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#30340;&#29616;&#20195;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
A Modern Introduction to Online Learning. (arXiv:1912.13213v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.13213
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26412;&#19987;&#33879;&#20171;&#32461;&#20102;&#22312;&#32447;&#23398;&#20064;&#30340;&#22522;&#26412;&#27010;&#24565;&#20197;&#21450;&#20984;&#20248;&#21270;&#32972;&#26223;&#19979;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#31639;&#27861;, &#21253;&#25324;&#27431;&#20960;&#37324;&#24471;&#21644;&#38750;&#27431;&#20960;&#37324;&#24471;&#35774;&#32622;&#20013;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#25110;&#36981;&#24490;&#27491;&#21017;&#21270;&#39046;&#23548;&#32773;&#31561;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#26412;&#19987;&#33879;&#20013;&#65292;&#25105;&#36890;&#36807;&#29616;&#20195;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#35270;&#35282;&#20171;&#32461;&#20102;&#22312;&#32447;&#23398;&#20064;&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;&#22312;&#36825;&#37324;&#65292;&#22312;&#32447;&#23398;&#20064;&#25351;&#30340;&#26159;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#26694;&#26550;&#12290;&#25105;&#20171;&#32461;&#20102;&#19968;&#38454;&#21644;&#20108;&#38454;&#20855;&#26377;&#20984;&#25439;&#22833;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#27431;&#20960;&#37324;&#24471;&#21644;&#38750;&#27431;&#20960;&#37324;&#24471;&#35774;&#32622;&#20026;&#22522;&#30784;&#65292;&#25152;&#26377;&#31639;&#27861;&#37117;&#28165;&#26224;&#22320;&#21576;&#29616;&#20026;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#25110;&#36981;&#24490;&#27491;&#21017;&#21270;&#39046;&#23548;&#32773;&#21450;&#20854;&#21464;&#24418;&#30340;&#23454;&#20363;&#12290;&#29305;&#21035;&#20851;&#27880;&#31639;&#27861;&#21442;&#25968;&#30340;&#35843;&#25972;&#38382;&#39064;&#21644;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#26080;&#21442;&#25968;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#26080;&#30028;&#22495;&#20013;&#30340;&#23398;&#20064;&#12290; &#20984;&#25439;&#22833;&#36890;&#36807;&#20984;&#20195;&#29702;&#25439;&#22833;&#21644;&#38543;&#26426;&#21270;&#22788;&#29702;&#65292;&#26469;&#22788;&#29702;&#38750;&#20984;&#25439;&#22833;&#12290;&#21516;&#26102;&#36824;&#31616;&#35201;&#35752;&#35770;&#20102;&#36172;&#21338;&#35774;&#32622;&#65292;&#28041;&#21450;&#25932;&#23545;&#21644;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#12290;&#36825;&#20123;&#31508;&#35760;&#19981;&#38656;&#35201;&#20808;&#21069;&#23545;&#20984;&#20998;&#26512;&#30340;&#20102;&#35299;&#65292;&#24182;&#19988;&#25152;&#26377;&#25152;&#38656;&#30340;&#25968;&#23398;&#24037;&#20855;&#37117;&#24050;&#20005;&#35880;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this monograph, I introduce the basic concepts of Online Learning through a modern view of Online Convex Optimization. Here, online learning refers to the framework of regret minimization under worst-case assumptions. I present first-order and second-order algorithms for online learning with convex losses, in Euclidean and non-Euclidean settings. All the algorithms are clearly presented as instantiation of Online Mirror Descent or Follow-The-Regularized-Leader and their variants. Particular attention is given to the issue of tuning the parameters of the algorithms and learning in unbounded domains, through adaptive and parameter-free online learning algorithms. Non-convex losses are dealt through convex surrogate losses and through randomization. The bandit setting is also briefly discussed, touching on the problem of adversarial and stochastic multi-armed bandits. These notes do not require prior knowledge of convex analysis and all the required mathematical tools are rigorously ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#26041;&#27861;&#26469;&#24555;&#36895;&#35745;&#31639;Dirichlet&#20998;&#24067;&#30340;MLE&#21442;&#25968;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#23454;&#29616;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19968;&#36941;&#36941;&#21382;&#25968;&#25454;&#38598;&#23601;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/1405.0099</link><description>&lt;p&gt;
Dirichlet&#22810;&#39033;&#24335;&#30340;&#24555;&#36895;MLE&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Fast MLE Computation for the Dirichlet Multinomial. (arXiv:1405.0099v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1405.0099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#26041;&#27861;&#26469;&#24555;&#36895;&#35745;&#31639;Dirichlet&#20998;&#24067;&#30340;MLE&#21442;&#25968;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#23454;&#29616;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19968;&#36941;&#36941;&#21382;&#25968;&#25454;&#38598;&#23601;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24076;&#26395;&#25214;&#21040;&#19968;&#20010;Dirichlet&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#20351;&#24471;&#22312;&#35813;&#20998;&#24067;&#19979;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#20284;&#28982;&#20989;&#25968;&#26368;&#22823;&#21270;&#12290;&#36890;&#24120;&#21033;&#29992;&#29275;&#39039;&#36845;&#20195;&#27861;&#26469;&#27714;&#35299;&#65292;&#20294;&#30446;&#21069;&#30340;&#23454;&#29616;&#38656;&#35201;&#27599;&#27425;&#36845;&#20195;&#37117;&#35835;&#21462;&#25972;&#20010;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#36890;&#36807;&#25968;&#25454;&#38598;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#20102;&#36816;&#34892;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a collection of categorical data, we want to find the parameters of a Dirichlet distribution which maximizes the likelihood of that data. Newton's method is typically used for this purpose but current implementations require reading through the entire dataset on each iteration. In this paper, we propose a modification which requires only a single pass through the dataset and substantially decreases running time. Furthermore we analyze both theoretically and empirically the performance of the proposed algorithm, and provide an open source implementation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#23545;&#20598;&#38382;&#39064;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;Boosting&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;AdaBoost&#31561;&#31639;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#38388;&#38548;&#21644;&#25511;&#21046;&#38388;&#38548;&#26041;&#24046;&#26469;&#32500;&#25252;&#26356;&#22909;&#30340;&#38388;&#38548;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/0901.3590</link><description>&lt;p&gt;
&#20851;&#20110;Boosting&#31639;&#27861;&#30340;&#23545;&#20598;&#24418;&#24335;
&lt;/p&gt;
&lt;p&gt;
On the Dual Formulation of Boosting Algorithms. (arXiv:0901.3590v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/0901.3590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23545;&#20598;&#38382;&#39064;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;Boosting&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;AdaBoost&#31561;&#31639;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#38388;&#38548;&#21644;&#25511;&#21046;&#38388;&#38548;&#26041;&#24046;&#26469;&#32500;&#25252;&#26356;&#22909;&#30340;&#38388;&#38548;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#26032;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;Boosting&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AdaBoost&#12289;LogitBoost&#21644;&#24102;&#24191;&#20041;&#38128;&#38142;&#25439;&#22833;&#30340;&#36719;&#38388;&#38548;LPBoost&#31639;&#27861;&#30340;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#38382;&#39064;&#37117;&#26159;&#29109;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20123;Boosting&#31639;&#27861;&#30340;&#23545;&#20598;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Boosting&#31639;&#27861;&#30340;&#25104;&#21151;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#38388;&#38548;&#21644;&#25511;&#21046;&#38388;&#38548;&#26041;&#24046;&#26469;&#32500;&#25252;&#26356;&#22909;&#30340;&#38388;&#38548;&#20998;&#24067;&#26469;&#29702;&#35299;&#12290;&#25105;&#20204;&#36824;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#65292;&#36817;&#20284;&#22320;&#65292;AdaBoost&#26368;&#22823;&#21270;&#30340;&#26159;&#24179;&#22343;&#38388;&#38548;&#32780;&#38750;&#26368;&#23567;&#38388;&#38548;&#12290;&#23545;&#20598;&#24418;&#24335;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#26159;&#23436;&#20840;&#26657;&#27491;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#26631;&#20934;&#30340;&#36880;&#27493;&#21152;&#24615;Boosting&#31639;&#27861;&#20960;&#20046;&#23436;&#20840;&#30456;&#21516;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#20294;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#20248;&#21270;&#25216;&#26415;&#26500;&#24314;&#38598;&#21512;&#21482;&#38656;&#35201;&#36739;&#23569;&#30340;&#24369;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study boosting algorithms from a new perspective. We show that the Lagrange dual problems of AdaBoost, LogitBoost and soft-margin LPBoost with generalized hinge loss are all entropy maximization problems. By looking at the dual problems of these boosting algorithms, we show that the success of boosting algorithms can be understood in terms of maintaining a better margin distribution by maximizing margins and at the same time controlling the margin variance.We also theoretically prove that, approximately, AdaBoost maximizes the average margin, instead of the minimum margin. The duality formulation also enables us to develop column generation based optimization algorithms, which are totally corrective. We show that they exhibit almost identical classification results to that of standard stage-wise additive boosting algorithms but with much faster convergence rates. Therefore fewer weak classifiers are needed to build the ensemble using our proposed optimization technique.
&lt;/p&gt;</description></item></channel></rss>