<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>ForceSight&#26159;&#19968;&#20010;&#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#31227;&#21160;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#35270;&#35273;&#21147;&#23548;&#21521;&#30446;&#26631;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#22312;&#26410;&#35265;&#29615;&#22659;&#20013;&#36827;&#34892;&#31934;&#30830;&#25235;&#21462;&#12289;&#25277;&#23625;&#25171;&#24320;&#21644;&#29289;&#20307;&#20132;&#25509;&#31561;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.12312</link><description>&lt;p&gt;
ForceSight: &#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#35273;&#21147;&#23548;&#21521;&#31227;&#21160;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals. (arXiv:2309.12312v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12312
&lt;/p&gt;
&lt;p&gt;
ForceSight&#26159;&#19968;&#20010;&#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#31227;&#21160;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#35270;&#35273;&#21147;&#23548;&#21521;&#30446;&#26631;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#22312;&#26410;&#35265;&#29615;&#22659;&#20013;&#36827;&#34892;&#31934;&#30830;&#25235;&#21462;&#12289;&#25277;&#23625;&#25171;&#24320;&#21644;&#29289;&#20307;&#20132;&#25509;&#31561;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ForceSight&#30340;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#26469;&#39044;&#27979;&#35270;&#35273;&#21147;&#23548;&#21521;&#30340;&#30446;&#26631;&#12290;&#32473;&#23450;&#19968;&#24352;RGBD&#22270;&#29255;&#21644;&#19968;&#20010;&#25991;&#26412;&#25552;&#31034;&#65292;ForceSight&#21487;&#20197;&#30830;&#23450;&#30456;&#26426;&#22352;&#26631;&#31995;&#19979;&#30340;&#30446;&#26631;&#26411;&#31471;&#25191;&#34892;&#22120;&#20301;&#23039;&#65288;&#36816;&#21160;&#30446;&#26631;&#65289;&#21644;&#30456;&#20851;&#30340;&#21147;&#37327;&#65288;&#21147;&#37327;&#30446;&#26631;&#65289;&#12290;&#36825;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#20849;&#21516;&#24418;&#25104;&#20102;&#19968;&#20010;&#35270;&#35273;&#21147;&#23548;&#21521;&#30446;&#26631;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36755;&#20986;&#20154;&#21487;&#35299;&#37322;&#30340;&#36816;&#21160;&#30446;&#26631;&#30340;&#28145;&#24230;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#24039;&#22937;&#25805;&#20316;&#12290;&#21147;&#37327;&#22312;&#25805;&#20316;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#36739;&#20302;&#23618;&#27425;&#30340;&#25191;&#34892;&#20013;&#12290;&#24403;&#24212;&#29992;&#20110;&#24102;&#26377;&#25163;&#33218;&#21644;&#30524;&#30555;&#30340;&#31227;&#21160;&#25805;&#20316;&#35013;&#32622;&#30340;ForceSight&#26102;&#65292;&#22312;&#19982;&#35757;&#32451;&#25968;&#25454;&#24046;&#24322;&#26174;&#33879;&#30340;&#26410;&#35265;&#29615;&#22659;&#20013;&#65292;&#33021;&#22815;&#20197;81%&#30340;&#25104;&#21151;&#29575;&#23436;&#25104;&#35832;&#22914;&#31934;&#30830;&#25235;&#21462;&#12289;&#25277;&#23625;&#25171;&#24320;&#21644;&#29289;&#20307;&#20132;&#25509;&#31561;&#20219;&#21153;&#12290;&#22312;&#21478;&#19968;&#39033;&#29420;&#31435;&#23454;&#39564;&#20013;&#65292;ForceSight&#20165;&#20351;&#29992;&#35270;&#35273;&#20282;&#26381;&#65292;&#19981;&#32771;&#34385;&#21147;&#37327;&#20449;&#24687;&#65292;&#20294;&#20381;&#28982;&#26174;&#31034;&#20986;&#36739;&#39640;&#30340;&#25805;&#20316;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ForceSight, a system for text-guided mobile manipulation that predicts visual-force goals using a deep neural network. Given a single RGBD image combined with a text prompt, ForceSight determines a target end-effector pose in the camera frame (kinematic goal) and the associated forces (force goal). Together, these two components form a visual-force goal. Prior work has demonstrated that deep models outputting human-interpretable kinematic goals can enable dexterous manipulation by real robots. Forces are critical to manipulation, yet have typically been relegated to lower-level execution in these systems. When deployed on a mobile manipulator equipped with an eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps, drawer opening, and object handovers with an 81% success rate in unseen environments with object instances that differed significantly from the training data. In a separate experiment, relying exclusively on visual servoing and ignoring force 
&lt;/p&gt;</description></item><item><title>LLM-Grounder&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#12289;&#24320;&#25918;&#35789;&#27719;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;3D&#35270;&#35273;&#23450;&#20301;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20998;&#35299;&#26597;&#35810;&#24182;&#20351;&#29992;&#35270;&#35273;&#23450;&#20301;&#24037;&#20855;&#35782;&#21035;&#29289;&#20307;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#26032;&#22330;&#26223;&#21644;&#25991;&#26412;&#26597;&#35810;&#30340;&#26377;&#25928;&#23450;&#20301;&#12290;&#22312;ScanRefer&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12311</link><description>&lt;p&gt;
LLM-Grounder: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#30340;&#24320;&#25918;&#35789;&#27719;3D&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent. (arXiv:2309.12311v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12311
&lt;/p&gt;
&lt;p&gt;
LLM-Grounder&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#12289;&#24320;&#25918;&#35789;&#27719;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;3D&#35270;&#35273;&#23450;&#20301;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20998;&#35299;&#26597;&#35810;&#24182;&#20351;&#29992;&#35270;&#35273;&#23450;&#20301;&#24037;&#20855;&#35782;&#21035;&#29289;&#20307;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#26032;&#22330;&#26223;&#21644;&#25991;&#26412;&#26597;&#35810;&#30340;&#26377;&#25928;&#23450;&#20301;&#12290;&#22312;ScanRefer&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#35270;&#35273;&#23450;&#20301;&#26159;&#23478;&#29992;&#26426;&#22120;&#20154;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20351;&#20854;&#22312;&#29615;&#22659;&#20013;&#23548;&#33322;&#12289;&#25805;&#20316;&#29289;&#20307;&#24182;&#26681;&#25454;&#29615;&#22659;&#22238;&#31572;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#25110;&#32773;&#22312;&#22788;&#29702;&#22797;&#26434;&#35821;&#35328;&#26597;&#35810;&#26102;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLM-Grounder&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#12289;&#24320;&#25918;&#35789;&#27719;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;3D&#35270;&#35273;&#23450;&#20301;&#27969;&#31243;&#12290;LLM-Grounder&#21033;&#29992;&#19968;&#20010;LLM&#23558;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20998;&#35299;&#20026;&#35821;&#20041;&#25104;&#20998;&#65292;&#24182;&#20351;&#29992;&#35832;&#22914;OpenScene&#25110;LERF&#20043;&#31867;&#30340;&#35270;&#35273;&#23450;&#20301;&#24037;&#20855;&#26469;&#35782;&#21035;3D&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#12290;&#28982;&#21518;&#65292;LLM&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#29289;&#20307;&#20043;&#38388;&#30340;&#31354;&#38388;&#21644;&#24120;&#35782;&#20851;&#31995;&#65292;&#20197;&#20570;&#20986;&#26368;&#32456;&#30340;&#23450;&#20301;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;3D&#22330;&#26223;&#21644;&#20219;&#24847;&#25991;&#26412;&#26597;&#35810;&#12290;&#25105;&#20204;&#22312;ScanRefer&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;LLM-Grounder&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;3D&#35270;&#35273;&#23450;&#20301;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero-shot grounding accuracy. Our findings indicate that LLMs si
&lt;/p&gt;</description></item><item><title>LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12307</link><description>&lt;p&gt;
LongLoRA: &#39640;&#25928;&#30340;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12307
&lt;/p&gt;
&lt;p&gt;
LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#8212;&#8212;LongLoRA&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#36890;&#24120;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#23567;&#35757;&#32451;LLM&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;GPU&#36164;&#28304;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26041;&#38754;&#21152;&#24555;&#20102;LLM&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#12290;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#25512;&#29702;&#36807;&#31243;&#20013;&#38656;&#35201;&#31264;&#23494;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#20294;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#23436;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#19978;&#19979;&#25991;&#30340;&#25193;&#23637;&#65292;&#22312;&#19982;&#20351;&#29992;&#20256;&#32479;&#27880;&#24847;&#21147;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#26102;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#21482;&#29992;&#20004;&#34892;&#20195;&#30721;&#23454;&#29616;&#65292;&#22312;&#25512;&#29702;&#20013;&#26159;&#21487;&#36873;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21442;&#25968;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-effici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29615;&#22659;&#20559;&#21521;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#30340;&#29615;&#22659;&#20043;&#38388;&#20998;&#24067;&#26041;&#24046;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#39640;&#20998;&#29305;&#24449;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#19978;&#22343;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12301</link><description>&lt;p&gt;
&#29615;&#22659;&#20559;&#21521;&#29305;&#24449;&#25490;&#24207;&#29992;&#20110;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Environment-biased Feature Ranking for Novelty Detection Robustness. (arXiv:2309.12301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29615;&#22659;&#20559;&#21521;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#30340;&#29615;&#22659;&#20043;&#38388;&#20998;&#24067;&#26041;&#24046;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#39640;&#20998;&#29305;&#24449;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#19978;&#22343;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#40065;&#26834;&#24615;&#26032;&#39062;&#24615;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#26816;&#27979;&#35821;&#20041;&#20869;&#23481;&#26041;&#38754;&#30340;&#26032;&#39062;&#24615;&#65292;&#21516;&#26102;&#23545;&#20854;&#20182;&#26080;&#20851;&#22240;&#32032;&#30340;&#21464;&#21270;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#22810;&#20010;&#29615;&#22659;&#30340;&#35774;&#32622;&#20013;&#25805;&#20316;&#65292;&#30830;&#23450;&#19982;&#29615;&#22659;&#26356;&#30456;&#20851;&#32780;&#19981;&#26159;&#20219;&#21153;&#30456;&#20851;&#20869;&#23481;&#30340;&#29305;&#24449;&#38598;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#22810;&#29615;&#22659;&#35774;&#32622;&#24320;&#22987;&#65292;&#25104;&#21151;&#26681;&#25454;&#20854;&#29615;&#22659;&#20851;&#27880;&#24230;&#23545;&#29305;&#24449;&#36827;&#34892;&#25490;&#24207;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#29615;&#22659;&#20043;&#38388;&#30340;&#29305;&#24449;&#20998;&#24067;&#26041;&#24046;&#35745;&#31639;&#27599;&#20010;&#29305;&#24449;&#30340;&#24471;&#20998;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#33293;&#24323;&#24471;&#20998;&#36739;&#39640;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#21487;&#20197;&#21435;&#38500;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#27491;&#24577;&#21327;&#26041;&#24046;&#21644;&#23376;&#31181;&#32676;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#30495;&#23454;&#30340;&#36824;&#26159;&#23545;&#20110;&#25105;&#20204;&#20026;&#27492;&#20219;&#21153;&#24341;&#20837;&#30340;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of robust novelty detection, where we aim to detect novelties in terms of semantic content while being invariant to changes in other, irrelevant factors. Specifically, we operate in a setup with multiple environments, where we determine the set of features that are associated more with the environments, rather than to the content relevant for the task. Thus, we propose a method that starts with a pretrained embedding and a multi-env setup and manages to rank the features based on their environment-focus. First, we compute a per-feature score based on the feature distribution variance between envs. Next, we show that by dropping the highly scored ones, we manage to remove spurious correlations and improve the overall performance by up to 6%, both in covariance and sub-population shift cases, both for a real and a synthetic benchmark, that we introduce for this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#28608;&#21169;&#20248;&#21270;&#35302;&#35273;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#22810;&#25351;&#26426;&#22120;&#20154;&#30340;&#35302;&#35273;&#28789;&#24039;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.12300</link><description>&lt;p&gt;
See to Touch: &#36890;&#36807;&#35270;&#35273;&#28608;&#21169;&#23398;&#20064;&#35302;&#35273;&#28789;&#24039;&#24615;
&lt;/p&gt;
&lt;p&gt;
See to Touch: Learning Tactile Dexterity through Visual Incentives. (arXiv:2309.12300v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#28608;&#21169;&#20248;&#21270;&#35302;&#35273;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#22810;&#25351;&#26426;&#22120;&#20154;&#30340;&#35302;&#35273;&#28789;&#24039;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#20154;&#31867;&#25797;&#38271;&#30340;&#31934;&#30830;&#12289;&#23500;&#26377;&#25509;&#35302;&#12289;&#28789;&#24039;&#30340;&#25805;&#20316;&#65292;&#20026;&#22810;&#25351;&#26426;&#22120;&#20154;&#37197;&#22791;&#35302;&#35273;&#20256;&#24863;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;&#35302;&#35273;&#20256;&#24863;&#26080;&#27861;&#25552;&#20379;&#36275;&#22815;&#30340;&#32447;&#32034;&#26469;&#25512;&#29702;&#29289;&#20307;&#30340;&#31354;&#38388;&#37197;&#32622;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#32416;&#27491;&#38169;&#35823;&#21644;&#36866;&#24212;&#21464;&#21270;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#36890;&#36807;&#35270;&#35273;&#28608;&#21169;&#23454;&#29616;&#35302;&#35273;&#36866;&#24212;&#65288;TAVI&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#22870;&#21169;&#30340;&#20248;&#21270;&#35302;&#35273;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;&#35302;&#35273;&#30340;&#28789;&#24039;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#24615;&#30446;&#26631;&#26469;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#35270;&#35273;&#34920;&#31034;&#36890;&#36807;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#21305;&#37197;&#30340;&#26041;&#24335;&#26500;&#24314;&#22870;&#21169;&#20989;&#25968;&#65292;&#20854;&#20013;&#21442;&#32771;&#19968;&#20010;&#20154;&#31867;&#31034;&#33539;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#26426;&#22120;&#20154;&#19978;&#22522;&#20110;&#35302;&#35273;&#30340;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#35270;&#35273;&#22870;&#21169;&#12290;&#22312;&#20845;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#22914;&#25554;&#38144;&#25343;&#21462;&#12289;&#21368;&#19979;&#30871;&#21644;&#32763;&#36716;&#32454;&#38271;&#29289;&#20307;&#31561;&#65292;TAVI&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equipping multi-fingered robots with tactile sensing is crucial for achieving the precise, contact-rich, and dexterous manipulation that humans excel at. However, relying solely on tactile sensing fails to provide adequate cues for reasoning about objects' spatial configurations, limiting the ability to correct errors and adapt to changing situations. In this paper, we present Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances tactile-based dexterity by optimizing dexterous policies using vision-based rewards. First, we use a contrastive-based objective to learn visual representations. Next, we construct a reward function using these visual representations through optimal-transport based matching on one human demonstration. Finally, we use online reinforcement learning on our robot to optimize tactile-based policies that maximize the visual reward. On six challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping slender objects, TAVI ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#39550;&#39542;&#34892;&#20026;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#19979;&#39640;&#25928;&#22320;&#23398;&#20064;&#24182;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12295</link><description>&lt;p&gt;
&#23398;&#20064;&#39550;&#39542;&#21040;&#20219;&#20309;&#22320;&#26041;
&lt;/p&gt;
&lt;p&gt;
Learning to Drive Anywhere. (arXiv:2309.12295v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#39550;&#39542;&#34892;&#20026;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#19979;&#39640;&#25928;&#22320;&#23398;&#20064;&#24182;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#39550;&#39542;&#21592;&#21487;&#20197;&#26080;&#32541;&#22320;&#36866;&#24212;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#30340;&#39550;&#39542;&#20915;&#31574;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#36947;&#36335;&#26465;&#20214;&#21644;&#20132;&#36890;&#35268;&#21017;&#65292;&#20363;&#22914;&#24038;&#39550;&#39542;&#21644;&#21491;&#39550;&#39542;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#21482;&#33021;&#22312;&#38480;&#23450;&#30340;&#25805;&#20316;&#39046;&#22495;&#20869;&#37096;&#32626;&#65292;&#19981;&#33021;&#32771;&#34385;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#20043;&#38388;&#30340;&#39550;&#39542;&#34892;&#20026;&#24046;&#24322;&#21644;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AnyD&#65292;&#19968;&#31181;&#21333;&#19968;&#30340;&#20855;&#26377;&#22320;&#29702;&#24863;&#30693;&#30340;&#26465;&#20214;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;CIL&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#20855;&#26377;&#21160;&#24577;&#29615;&#22659;&#12289;&#20132;&#36890;&#21644;&#31038;&#20250;&#29305;&#24449;&#30340;&#24322;&#26500;&#21644;&#20840;&#29699;&#20998;&#24067;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#24341;&#20837;&#19968;&#20010;&#39640;&#23481;&#37327;&#30340;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#19979;&#26377;&#25928;&#22320;&#36866;&#24212;&#26412;&#22320;&#32454;&#24494;&#24046;&#24322;&#24182;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#20248;&#21270;&#23545;&#27604;&#24615;&#27169;&#20223;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#36866;&#24212;&#22266;&#26377;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#21644;&#22320;&#29702;&#20301;&#32622;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human drivers can seamlessly adapt their driving decisions across geographical locations with diverse conditions and rules of the road, e.g., left vs. right-hand traffic. In contrast, existing models for autonomous driving have been thus far only deployed within restricted operational domains, i.e., without accounting for varying driving behaviors across locations or model scalability. In this work, we propose AnyD, a single geographically-aware conditional imitation learning (CIL) model that can efficiently learn from heterogeneous and globally distributed data with dynamic environmental, traffic, and social characteristics. Our key insight is to introduce a high-capacity geo-location-based channel attention mechanism that effectively adapts to local nuances while also flexibly modeling similarities among regions in a data-driven manner. By optimizing a contrastive imitation objective, our proposed approach can efficiently scale across inherently imbalanced data distributions and loca
&lt;/p&gt;</description></item><item><title>LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12288</link><description>&lt;p&gt;
&#32763;&#36716;&#35781;&#21650;: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;"A&#26159;B"&#26080;&#27861;&#23398;&#20064;"B&#26159;A"
&lt;/p&gt;
&lt;p&gt;
The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12288
&lt;/p&gt;
&lt;p&gt;
LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27867;&#21270;&#19978;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#12290;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"A&#26159;B"&#24418;&#24335;&#30340;&#21477;&#23376;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#25512;&#24191;&#21040;&#30456;&#21453;&#30340;&#26041;&#21521;"B&#26159;A"&#12290;&#36825;&#23601;&#26159;&#32763;&#36716;&#35781;&#21650;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"Olaf Scholz&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;"&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#33021;&#22815;&#22238;&#31572;&#38382;&#39064;"&#35841;&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;&#65311;"&#12290;&#27492;&#22806;&#65292;&#27491;&#30830;&#31572;&#26696;&#65288;"Olaf Scholz"&#65289;&#30340;&#21487;&#33021;&#24615;&#19981;&#20250;&#27604;&#38543;&#26426;&#21517;&#23383;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#20250;&#25512;&#24191;&#21040;&#23427;&#20204;&#35757;&#32451;&#38598;&#20013;&#30340;&#26222;&#36941;&#27169;&#24335;&#65288;&#21363;&#22914;&#26524;&#20986;&#29616;"A&#26159;B"&#65292;&#21017;"B&#26159;A"&#26356;&#21487;&#33021;&#20986;&#29616;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#34394;&#26500;&#30340;&#38472;&#36848;&#65288;&#22914;"Uriah Hawthorne&#26159;'Abyssal Melodies'&#30340;&#20316;&#26354;&#23478;"&#65289;&#19978;&#23545;GPT-3&#21644;Llama-1&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#26080;&#27861;&#27491;&#30830;&#22238;&#31572;"&#35841;&#21019;&#20316;&#20102;'Abyssal Melodies'?"&#26469;&#25552;&#20379;&#32763;&#36716;&#35781;&#21650;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#20048;&#22120;&#38899;&#20048;&#21512;&#25104;&#24615;&#33021;&#35843;&#33410;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29305;&#23450;&#30340;&#28436;&#22863;&#21644;&#24405;&#38899;&#29615;&#22659;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#38899;&#33394;&#21644;&#39118;&#26684;&#24341;&#23548;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.12283</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#20048;&#22120;&#38899;&#20048;&#21512;&#25104;&#30340;&#24615;&#33021;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
Performance Conditioning for Diffusion-Based Multi-Instrument Music Synthesis. (arXiv:2309.12283v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#20048;&#22120;&#38899;&#20048;&#21512;&#25104;&#24615;&#33021;&#35843;&#33410;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29305;&#23450;&#30340;&#28436;&#22863;&#21644;&#24405;&#38899;&#29615;&#22659;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#38899;&#33394;&#21644;&#39118;&#26684;&#24341;&#23548;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31526;&#21495;&#38899;&#20048;&#34920;&#31034;&#29983;&#25104;&#22810;&#20048;&#22120;&#38899;&#20048;&#26159;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#19968;&#20010;&#38598;&#20013;&#20294;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20197;&#38899;&#20048;&#21644;&#22768;&#23398;&#20026;&#22522;&#30784;&#30340;&#25511;&#21046;&#12290;&#20316;&#20026;&#26412;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23545;&#29305;&#23450;&#30340;&#28436;&#22863;&#21644;&#24405;&#38899;&#29615;&#22659;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#22686;&#24378;&#22810;&#20048;&#22120;&#21512;&#25104;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24341;&#23548;&#38899;&#33394;&#21644;&#39118;&#26684;&#12290;&#22312;&#29616;&#26377;&#20808;&#36827;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24615;&#33021;&#35843;&#33410;-&#19968;&#31181;&#31616;&#21333;&#30340;&#24037;&#20855;&#65292;&#25351;&#31034;&#29983;&#25104;&#27169;&#22411;&#29992;&#29305;&#23450;&#28436;&#22863;&#20013;&#29305;&#23450;&#20048;&#22120;&#30340;&#39118;&#26684;&#21644;&#38899;&#33394;&#21512;&#25104;&#38899;&#20048;&#12290;&#25105;&#20204;&#30340;&#21407;&#22411;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#21270;&#20202;&#22120;&#30340;&#38750;&#31574;&#21010;&#34920;&#28436;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#20445;&#30041;&#20102;&#26032;&#39062;&#38899;&#33394;&#21644;&#39118;&#26684;&#25511;&#21046;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;FAD&#36924;&#30495;&#24230;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#39029;&#38754;&#21253;&#25324;&#26679;&#26412;&#21644;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating multi-instrument music from symbolic music representations is an important task in Music Information Retrieval (MIR). A central but still largely unsolved problem in this context is musically and acoustically informed control in the generation process. As the main contribution of this work, we propose enhancing control of multi-instrument synthesis by conditioning a generative model on a specific performance and recording environment, thus allowing for better guidance of timbre and style. Building on state-of-the-art diffusion-based music generative models, we introduce performance conditioning - a simple tool indicating the generative model to synthesize music with style and timbre of specific instruments taken from specific performances. Our prototype is evaluated using uncurated performances with diverse instrumentation and achieves state-of-the-art FAD realism scores while allowing novel timbre and style control. Our project page, including samples and demonstrations, is
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#29305;&#24449;&#27169;&#20223;&#32593;&#32476;&#65288;FIN&#65289;&#22312;&#37329;&#34701;&#12289;&#35821;&#38899;&#21644;&#29983;&#29702;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;FIN&#22312;&#27604;&#29305;&#24065;&#20215;&#26684;&#39044;&#27979;&#12289;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#21644;&#24930;&#24615;&#39048;&#30171;&#26816;&#27979;&#26041;&#38754;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#24615;&#33021;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2309.12279</link><description>&lt;p&gt;
&#29305;&#24449;&#27169;&#20223;&#30340;&#24191;&#27867;&#24433;&#21709;&#65306;&#37329;&#34701;&#12289;&#35821;&#38899;&#21644;&#29983;&#29702;&#39046;&#22495;&#20013;&#30340;&#31070;&#32463;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
The Broad Impact of Feature Imitation: Neural Enhancements Across Financial, Speech, and Physiological Domains. (arXiv:2309.12279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#29305;&#24449;&#27169;&#20223;&#32593;&#32476;&#65288;FIN&#65289;&#22312;&#37329;&#34701;&#12289;&#35821;&#38899;&#21644;&#29983;&#29702;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;FIN&#22312;&#27604;&#29305;&#24065;&#20215;&#26684;&#39044;&#27979;&#12289;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#21644;&#24930;&#24615;&#39048;&#30171;&#26816;&#27979;&#26041;&#38754;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#24615;&#33021;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30340;&#21021;&#22987;&#21270;&#22312;&#30830;&#23450;&#23427;&#20204;&#30340;&#24615;&#33021;&#26041;&#38754;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#29305;&#24449;&#27169;&#20223;&#32593;&#32476;&#65288;FIN&#65289;&#36890;&#36807;&#23558;&#26435;&#37325;&#21021;&#22987;&#21270;&#20026;&#36817;&#20284;&#29305;&#23450;&#30340;&#38381;&#21512;&#32479;&#35745;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22880;&#23450;&#20102;&#26377;&#24076;&#26395;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;FIN&#30340;&#36866;&#29992;&#24615;&#20027;&#35201;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20294;&#26412;&#30740;&#31350;&#23558;&#20854;&#25193;&#23637;&#21040;&#20102;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#23454;&#39564;&#65292;&#20197;&#27979;&#35797;&#27169;&#20223;Tsallis&#29109;&#20197;&#25552;&#39640;&#24615;&#33021;&#30340;&#36866;&#29992;&#24615;&#65306;&#27604;&#29305;&#24065;&#20215;&#26684;&#39044;&#27979;&#65292;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#21644;&#24930;&#24615;&#39048;&#30171;&#26816;&#27979;&#12290;&#22312;&#27604;&#29305;&#24065;&#20215;&#26684;&#39044;&#27979;&#20013;&#65292;&#23884;&#20837;&#26377;FIN&#30340;&#27169;&#22411;&#23558;&#22343;&#26041;&#26681;&#35823;&#24046;&#20943;&#23569;&#20102;&#32422;1000&#19982;&#22522;&#20934;&#30456;&#27604;&#12290;&#22312;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;FIN&#22686;&#24378;&#27169;&#22411;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;3&#65285;&#20197;&#19978;&#12290;&#26368;&#21518;&#65292;&#22312;CNP&#26816;&#27979;&#23454;&#39564;&#20013;&#65292;&#25913;&#36827;&#32422;&#20026;7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Initialization of neural network weights plays a pivotal role in determining their performance. Feature Imitating Networks (FINs) offer a novel strategy by initializing weights to approximate specific closed-form statistical features, setting a promising foundation for deep learning architectures. While the applicability of FINs has been chiefly tested in biomedical domains, this study extends its exploration into other time series datasets. Three different experiments are conducted in this study to test the applicability of imitating Tsallis entropy for performance enhancement: Bitcoin price prediction, speech emotion recognition, and chronic neck pain detection. For the Bitcoin price prediction, models embedded with FINs reduced the root mean square error by around 1000 compared to the baseline. In the speech emotion recognition task, the FIN-augmented model increased classification accuracy by over 3 percent. Lastly, in the CNP detection experiment, an improvement of about 7 percent
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;VTE&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#20934;&#30830;&#35782;&#21035;VTE&#20107;&#20214;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2309.12273</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#22522;&#20110;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#25913;&#36827;VTE&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports. (arXiv:2309.12273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12273
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;VTE&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#20934;&#30830;&#35782;&#21035;VTE&#20107;&#20214;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#38745;&#33033;&#34880;&#26643;&#26643;&#22622;&#65288;VTE&#65289;&#65292;&#21253;&#25324;&#28145;&#38745;&#33033;&#34880;&#26643;&#65288;DVT&#65289;&#21644;&#32954;&#26643;&#22622;&#65288;PE&#65289;&#65292;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#24050;&#32463;&#22312;&#20174;&#22238;&#39038;&#24615;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;VTE&#20107;&#20214;&#25110;&#24110;&#21161;&#20020;&#24202;&#19987;&#23478;&#35782;&#21035;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;VTE&#20107;&#20214;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#35760;&#26377;&#38480;&#30340;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#12289;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22797;&#26434;&#24615;&#21644;&#24322;&#36136;&#24615;&#20197;&#21450;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#26377;&#25928;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;NLP&#27169;&#22411;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DL&#26041;&#27861;&#30340;&#26032;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#12289;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;NLP&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#20197;&#25552;&#39640;&#38750;&#32467;&#26500;&#21270;&#65288;&#33258;&#30001;&#25991;&#26412;&#65289;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;VTE&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid and accurate identification of Venous thromboembolism (VTE), a severe cardiovascular condition including deep vein thrombosis (DVT) and pulmonary embolism (PE), is important for effective treatment. Leveraging Natural Language Processing (NLP) on radiology reports, automated methods have shown promising advancements in identifying VTE events from retrospective data cohorts or aiding clinical experts in identifying VTE events from radiology reports. However, effectively training Deep Learning (DL) and the NLP models is challenging due to limited labeled medical text data, the complexity and heterogeneity of radiology reports, and data imbalance. This study proposes novel method combinations of DL methods, along with data augmentation, adaptive pre-trained NLP model selection, and a clinical expert NLP rule-based classifier, to improve the accuracy of VTE identification in unstructured (free-text) radiology reports. Our experimental results demonstrate the model's efficacy, achievi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20272;&#35745;&#24179;&#22343;&#32858;&#21512;&#65288;EMA&#65289;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#31995;&#32479;&#23433;&#20840;&#30340;&#25361;&#25112;&#12290;EMA&#36890;&#36807;&#20462;&#21098;&#22343;&#20540;&#22788;&#29702;&#24694;&#24847;&#24322;&#24120;&#20540;&#65292;&#24182;&#25581;&#31034;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#20197;&#30830;&#20445;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#19981;&#21516;&#30340;&#23458;&#25143;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;EMA&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#21644;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65292;&#25104;&#20026;&#20808;&#36827;&#32858;&#21512;&#25216;&#26415;&#30340;&#22522;&#20934;&#32447;&#12290;</title><link>http://arxiv.org/abs/2309.12267</link><description>&lt;p&gt;
&#20351;&#22522;&#20110;&#22235;&#20998;&#20301;&#25968;&#30340;&#20272;&#35745;&#24179;&#22343;&#26799;&#24230;&#32858;&#21512;&#25104;&#20026;&#32852;&#37030;&#22270;&#20687;&#20998;&#31867;&#30340;&#22522;&#20934;&#32447;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Enabling Quartile-based Estimated-Mean Gradient Aggregation As Baseline for Federated Image Classifications. (arXiv:2309.12267v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20272;&#35745;&#24179;&#22343;&#32858;&#21512;&#65288;EMA&#65289;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#31995;&#32479;&#23433;&#20840;&#30340;&#25361;&#25112;&#12290;EMA&#36890;&#36807;&#20462;&#21098;&#22343;&#20540;&#22788;&#29702;&#24694;&#24847;&#24322;&#24120;&#20540;&#65292;&#24182;&#25581;&#31034;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#20197;&#30830;&#20445;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#19981;&#21516;&#30340;&#23458;&#25143;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;EMA&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#21644;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65292;&#25104;&#20026;&#20808;&#36827;&#32858;&#21512;&#25216;&#26415;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#23454;&#29616;&#20998;&#25955;&#21327;&#20316;&#12289;&#20445;&#25252;&#25935;&#24863;&#25968;&#25454;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;FL&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#20010;&#20307;&#23458;&#25143;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#20197;&#21450;FL&#31995;&#32479;&#26131;&#21463;&#23433;&#20840;&#28431;&#27934;&#24433;&#21709;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#20272;&#35745;&#24179;&#22343;&#32858;&#21512;&#65288;EMA&#65289;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20165;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#32780;&#19988;&#20316;&#20026;FL&#31995;&#32479;&#20013;&#20808;&#36827;&#32858;&#21512;&#25216;&#26415;&#30340;&#22522;&#20934;&#32447;&#12290;EMA&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#20854;&#21452;&#37325;&#20316;&#29992;&#65306;&#36890;&#36807;&#20462;&#21098;&#22343;&#20540;&#26377;&#25928;&#22788;&#29702;&#24694;&#24847;&#24322;&#24120;&#20540;&#65292;&#25581;&#31034;&#25968;&#25454;&#24322;&#36136;&#24615;&#20197;&#30830;&#20445;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#23458;&#25143;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;EMA&#22987;&#32456;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#21644;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65292;&#30830;&#31435;&#33258;&#36523;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has revolutionized how we train deep neural networks by enabling decentralized collaboration while safeguarding sensitive data and improving model performance. However, FL faces two crucial challenges: the diverse nature of data held by individual clients and the vulnerability of the FL system to security breaches. This paper introduces an innovative solution named Estimated Mean Aggregation (EMA) that not only addresses these challenges but also provides a fundamental reference point as a $\mathsf{baseline}$ for advanced aggregation techniques in FL systems. EMA's significance lies in its dual role: enhancing model security by effectively handling malicious outliers through trimmed means and uncovering data heterogeneity to ensure that trained models are adaptable across various client datasets. Through a wealth of experiments, EMA consistently demonstrates high accuracy and area under the curve (AUC) compared to alternative methods, establishing itself as a ro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#40065;&#26834;&#30340;&#36719;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#38376;&#21442;&#25968;&#65292;&#21033;&#29992;&#30828;&#20855;&#20307;&#20998;&#24067;&#20056;&#20197;$l_0$&#33539;&#25968;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#27169;&#22411;&#30340;&#24555;&#36895;&#21512;&#24182;&#65292;&#24182;&#22686;&#24378;&#20102;&#23545;&#20855;&#26377;&#26497;&#31471;&#20540;&#30340;&#24694;&#24847;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#21512;&#24182;&#36807;&#31243;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.12259</link><description>&lt;p&gt;
&#26580;&#24615;&#21512;&#24182;&#65306;&#19968;&#31181;&#28789;&#27963;&#19988;&#40065;&#26834;&#30340;&#36719;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Soft Merging: A Flexible and Robust Soft Model Merging Approach for Enhanced Neural Network Performance. (arXiv:2309.12259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#40065;&#26834;&#30340;&#36719;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#38376;&#21442;&#25968;&#65292;&#21033;&#29992;&#30828;&#20855;&#20307;&#20998;&#24067;&#20056;&#20197;$l_0$&#33539;&#25968;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#27169;&#22411;&#30340;&#24555;&#36895;&#21512;&#24182;&#65292;&#24182;&#22686;&#24378;&#20102;&#23545;&#20855;&#26377;&#26497;&#31471;&#20540;&#30340;&#24694;&#24847;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#21512;&#24182;&#36807;&#31243;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#30001;&#20110;&#38382;&#39064;&#30340;&#38750;&#20984;&#24615;&#65292;&#23427;&#36890;&#24120;&#21482;&#33021;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#21033;&#29992;&#36825;&#20123;&#23616;&#37096;&#26368;&#20248;&#35299;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#37492;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#31616;&#21333;&#30340;&#31639;&#26415;&#24179;&#22343;&#20250;&#23548;&#33268;&#19981;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#36719;&#21512;&#24182;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38376;&#21442;&#25968;&#65292;&#21033;&#29992;&#30828;&#20855;&#20307;&#20998;&#24067;&#20056;&#20197;$l_0$&#33539;&#25968;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#27169;&#22411;&#30340;&#24555;&#36895;&#21512;&#24182;&#65292;&#31616;&#21270;&#20102;&#31070;&#32463;&#32593;&#32476;&#29305;&#23450;&#37096;&#20998;&#30340;&#21512;&#24182;&#65292;&#24182;&#22686;&#24378;&#20102;&#23545;&#20855;&#26377;&#26497;&#31471;&#20540;&#30340;&#24694;&#24847;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#20010;&#21512;&#24182;&#36807;&#31243;&#19981;&#20165;&#36890;&#36807;&#25910;&#25947;&#21040;&#26356;&#22909;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#26368;&#23567;&#21270;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#19988;&#26126;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent (SGD), a widely used optimization algorithm in deep learning, is often limited to converging to local optima due to the non-convex nature of the problem. Leveraging these local optima to improve model performance remains a challenging task. Given the inherent complexity of neural networks, the simple arithmetic averaging of the obtained local optima models in undesirable results. This paper proposes a {\em soft merging} method that facilitates rapid merging of multiple models, simplifies the merging of specific parts of neural networks, and enhances robustness against malicious models with extreme values. This is achieved by learning gate parameters through a surrogate of the $l_0$ norm using hard concrete distribution without modifying the model weights of the given local optima models. This merging process not only enhances the model performance by converging to a better local optimum, but also minimizes computational costs, offering an efficient and expli
&lt;/p&gt;</description></item><item><title>SALSA-CLRS&#26159;&#19968;&#31181;&#31232;&#30095;&#19988;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#65292;&#20248;&#20808;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#34920;&#31034;&#30340;&#21033;&#29992;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#36866;&#24212;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;CLRS&#22522;&#20934;&#20013;&#20869;&#23384;&#38656;&#27714;&#39640;&#21644;&#36816;&#34892;&#26102;&#38388;&#38590;&#20197;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12253</link><description>&lt;p&gt;
SALSA-CLRS:&#19968;&#31181;&#31232;&#30095;&#19988;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning. (arXiv:2309.12253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12253
&lt;/p&gt;
&lt;p&gt;
SALSA-CLRS&#26159;&#19968;&#31181;&#31232;&#30095;&#19988;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#65292;&#20248;&#20808;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#34920;&#31034;&#30340;&#21033;&#29992;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#36866;&#24212;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;CLRS&#22522;&#20934;&#20013;&#20869;&#23384;&#38656;&#27714;&#39640;&#21644;&#36816;&#34892;&#26102;&#38388;&#38590;&#20197;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CLRS&#31639;&#27861;&#23398;&#20064;&#22522;&#20934;&#30340;&#25193;&#23637;&#65292;&#20248;&#20808;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#34920;&#31034;&#30340;&#21033;&#29992;&#12290;CLRS&#20013;&#30340;&#35768;&#22810;&#31639;&#27861;&#38656;&#35201;&#20840;&#23616;&#23384;&#20648;&#22120;&#25110;&#20449;&#24687;&#20132;&#25442;&#65292;&#22312;&#20854;&#25191;&#34892;&#27169;&#22411;&#20013;&#38236;&#20687;&#34920;&#36798;&#20026;&#22522;&#20110;&#24213;&#23618;&#38382;&#39064;&#26500;&#24314;&#23436;&#20840;&#36830;&#25509;&#65288;&#32780;&#38750;&#31232;&#30095;&#65289;&#22270;&#30340;&#25805;&#20316;&#12290;&#23613;&#31649;CLRS&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#23398;&#20064;&#31639;&#27861;&#22312;&#26356;&#22823;&#23454;&#20363;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#29616;&#26377;&#30340;&#25191;&#34892;&#27169;&#22411;&#30001;&#20110;&#20854;&#35201;&#27714;&#39640;&#30340;&#20869;&#23384;&#38656;&#27714;&#21644;&#36816;&#34892;&#26102;&#38388;&#32780;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65288;&#38590;&#20197;&#25193;&#23637;&#65289;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#37325;&#35201;&#30340;&#31639;&#27861;&#24182;&#19981;&#38656;&#35201;&#23436;&#20840;&#36830;&#25509;&#30340;&#22270;&#65307;&#36825;&#20123;&#20027;&#35201;&#20998;&#24067;&#24335;&#31639;&#27861;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#37319;&#29992;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SALSA-CLRS&#65292;&#19968;&#20010;&#19987;&#38376;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#24615;&#30340;CLRS&#22522;&#20934;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20174;&#21407;&#22987;CLRS&#22522;&#20934;&#20013;&#25913;&#32534;&#30340;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an extension to the CLRS algorithmic learning benchmark, prioritizing scalability and the utilization of sparse representations. Many algorithms in CLRS require global memory or information exchange, mirrored in its execution model, which constructs fully connected (not sparse) graphs based on the underlying problem. Despite CLRS's aim of assessing how effectively learned algorithms can generalize to larger instances, the existing execution model becomes a significant constraint due to its demanding memory requirements and runtime (hard to scale). However, many important algorithms do not demand a fully connected graph; these algorithms, primarily distributed in nature, align closely with the message-passing paradigm employed by Graph Neural Networks. Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmark specifically with scalability and sparseness in mind. Our approach includes adapted algorithms from the original CLRS benchmark and introduces new probl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#31639;&#27861;&#65292;&#33021;&#22815;&#21152;&#36895;GPU&#23545;&#20110;&#39034;&#24207;&#27169;&#22411;&#30340;&#35780;&#20272;&#36895;&#24230;&#65292;&#25552;&#39640;&#20102;3&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#38477;&#20302;&#36755;&#20986;&#20934;&#30830;&#24615;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#65292;&#24182;&#22312;&#38271;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#21457;&#29616;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12252</link><description>&lt;p&gt;
&#22312;&#24207;&#21015;&#38271;&#24230;&#19978;&#24182;&#34892;&#21270;&#38750;&#32447;&#24615;&#39034;&#24207;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Parallelizing non-linear sequential models over the sequence length. (arXiv:2309.12252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#31639;&#27861;&#65292;&#33021;&#22815;&#21152;&#36895;GPU&#23545;&#20110;&#39034;&#24207;&#27169;&#22411;&#30340;&#35780;&#20272;&#36895;&#24230;&#65292;&#25552;&#39640;&#20102;3&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#38477;&#20302;&#36755;&#20986;&#20934;&#30830;&#24615;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#65292;&#24182;&#22312;&#38271;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#21457;&#29616;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#27169;&#22411;&#65292;&#20363;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19968;&#30452;&#30001;&#20110;&#20854;&#26412;&#36136;&#19978;&#30340;&#39034;&#24207;&#29305;&#24615;&#32780;&#23384;&#22312;&#35757;&#32451;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#22810;&#24180;&#26469;&#36825;&#20010;&#29942;&#39048;&#19968;&#30452;&#23384;&#22312;&#65292;&#22240;&#20026;&#24456;&#22810;&#20154;&#35748;&#20026;&#39034;&#24207;&#27169;&#22411;&#26080;&#27861;&#24182;&#34892;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#24182;&#34892;&#31639;&#27861;&#25361;&#25112;&#20102;&#36825;&#20010;&#38271;&#26399;&#20197;&#26469;&#30340;&#20449;&#24565;&#65292;&#21152;&#36895;&#20102;GPU&#23545;&#20110;&#39034;&#24207;&#27169;&#22411;&#30340;&#35780;&#20272;&#36895;&#24230;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;3&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#29306;&#29298;&#36755;&#20986;&#20934;&#30830;&#24615;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#39034;&#24207;&#27169;&#22411;&#26550;&#26500;&#20013;&#30340;&#20219;&#20309;&#29305;&#27530;&#32467;&#26500;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#39034;&#24207;&#27169;&#22411;&#21487;&#20197;&#27604;&#24120;&#35268;&#30340;&#39034;&#24207;&#26041;&#27861;&#24555;10&#20493;&#20197;&#19978;&#65292;&#32780;&#35757;&#32451;&#32467;&#26524;&#27809;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;&#20511;&#21161;&#36825;&#31181;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;17k&#20010;&#26102;&#38388;&#26679;&#26412;&#30340;&#38271;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#21457;&#29616;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#20811;&#26381;&#35757;&#32451;&#29942;&#39048;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20351;&#24471;&#39034;&#24207;&#27169;&#22411;&#30340;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential models, such as Recurrent Neural Networks and Neural Ordinary Differential Equations, have long suffered from slow training due to their inherent sequential nature. For many years this bottleneck has persisted, as many thought sequential models could not be parallelized. We challenge this long-held belief with our parallel algorithm that accelerates GPU evaluation of sequential models by up to 3 orders of magnitude faster without compromising output accuracy. The algorithm does not need any special structure in the sequential models' architecture, making it applicable to a wide range of architectures. Using our method, training sequential models can be more than 10 times faster than the common sequential method without any meaningful difference in the training results. Leveraging this accelerated training, we discovered the efficacy of the Gated Recurrent Unit in a long time series classification problem with 17k time samples. By overcoming the training bottleneck, our work 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#31572;&#31995;&#32479;&#35780;&#20272;&#25351;&#26631;SQuArE&#65292;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#31572;&#26696;&#36827;&#34892;&#21477;&#23376;&#32423;&#38382;&#31572;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#39640;&#24230;&#30456;&#20851;&#24615;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.12250</link><description>&lt;p&gt;
SQUARE: &#20351;&#29992;&#22810;&#20010;&#27491;&#36127;&#21442;&#32771;&#31572;&#26696;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References. (arXiv:2309.12250v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12250
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#31572;&#31995;&#32479;&#35780;&#20272;&#25351;&#26631;SQuArE&#65292;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#31572;&#26696;&#36827;&#34892;&#21477;&#23376;&#32423;&#38382;&#31572;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#39640;&#24230;&#30456;&#20851;&#24615;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#31995;&#32479;&#30340;&#35780;&#20272;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26114;&#36149;&#30340;&#65292;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#38382;&#39064;&#30340;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer LM&#32534;&#30721;&#22120;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#22312;&#38382;&#31572;&#35780;&#20272;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#36801;&#31227;&#24615;&#65292;&#20294;&#23427;&#20204;&#30340;&#20351;&#29992;&#21463;&#38480;&#20110;&#21333;&#20010;&#27491;&#30830;&#21442;&#32771;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;SQuArE&#65288;&#21477;&#23376;&#32423;&#38382;&#31572;&#35780;&#20272;&#65289;&#65292;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#31572;&#26696;&#65288;&#32452;&#21512;&#22810;&#20010;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;&#21442;&#32771;&#31572;&#26696;&#65289;&#36827;&#34892;&#21477;&#23376;&#24418;&#24335;&#30340;&#38382;&#31572;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#21477;&#23376;&#32423;&#25552;&#21462;&#24335;&#65288;&#31572;&#26696;&#36873;&#25321;&#65289;&#21644;&#29983;&#25104;&#24335;&#65288;GenQA&#65289;&#38382;&#31572;&#31995;&#32479;&#19978;&#35780;&#20272;&#20102;SQuArE&#65292;&#22312;&#22810;&#20010;&#23398;&#26415;&#21644;&#24037;&#19994;&#25968;&#25454;&#38598;&#19978;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;&#20808;&#21069;&#30340;&#22522;&#20934;&#65292;&#24182;&#19982;&#20154;&#24037;&#26631;&#27880;&#20855;&#26377;&#26368;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of QA systems is very challenging and expensive, with the most reliable approach being human annotations of correctness of answers for questions. Recent works (AVA, BEM) have shown that transformer LM encoder based similarity metrics transfer well for QA evaluation, but they are limited by the usage of a single correct reference answer. We propose a new evaluation metric: SQuArE (Sentence-level QUestion AnsweRing Evaluation), using multiple reference answers (combining multiple correct and incorrect references) for sentence-form QA. We evaluate SQuArE on both sentence-level extractive (Answer Selection) and generative (GenQA) QA systems, across multiple academic and industrial datasets, and show that it outperforms previous baselines and obtains the highest correlation with human annotations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#23545;&#21512;&#25104;X&#23556;&#32447;&#22270;&#20687;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23558;&#33258;&#36866;&#24212;&#36755;&#20837;&#22270;&#20687;&#24402;&#19968;&#21270;&#26041;&#27861;&#19982;&#28145;&#24230;&#27169;&#22411;&#32467;&#21512;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.12245</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#36755;&#20837;&#22270;&#20687;&#24402;&#19968;&#21270;&#26041;&#27861;&#35299;&#20915;&#22522;&#20110;GAN&#30340;X&#23556;&#32447;&#22270;&#20687;&#30340;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images. (arXiv:2309.12245v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#23545;&#21512;&#25104;X&#23556;&#32447;&#22270;&#20687;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23558;&#33258;&#36866;&#24212;&#36755;&#20837;&#22270;&#20687;&#24402;&#19968;&#21270;&#26041;&#27861;&#19982;&#28145;&#24230;&#27169;&#22411;&#32467;&#21512;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30142;&#30149;&#30340;&#32597;&#35265;&#24615;&#65292;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#21487;&#33021;&#23384;&#22312;&#19981;&#24179;&#34913;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#26469;&#25193;&#20805;&#25968;&#25454;&#38598;&#65292;&#36215;&#21040;&#20102;&#35299;&#20915;&#36825;&#31181;&#19981;&#24179;&#34913;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#38656;&#35201;&#21253;&#21547;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#65292;&#20197;&#20934;&#30830;&#34920;&#31034;&#35757;&#32451;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#29305;&#24449;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#21512;&#25104;&#22270;&#20687;&#20013;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#29305;&#24449;&#20250;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#24433;&#21709;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#22810;&#26679;&#21270;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#24182;&#20998;&#20026;&#31867;&#20869;&#21644;&#31867;&#38388;&#20004;&#31181;&#31867;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20004;&#31181;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#21512;&#25104;X&#23556;&#32447;&#22270;&#20687;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#23558;&#33258;&#36866;&#24212;&#36755;&#20837;&#22270;&#20687;&#24402;&#19968;&#21270;&#26041;&#27861;&#19982;&#28145;&#24230;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#35299;&#20915;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical image datasets can be imbalanced due to the rarity of targeted diseases. Generative Adversarial Networks play a key role in addressing this imbalance by enabling the generation of synthetic images to augment datasets. It is important to generate synthetic images that incorporate a diverse range of features to accurately represent the distribution of features present in the training imagery. Furthermore, the absence of diverse features in synthetic images can degrade the performance of machine learning classifiers. The mode collapse problem impacts Generative Adversarial Networks' capacity to generate diversified images. Mode collapse comes in two varieties: intra-class and inter-class. In this paper, both varieties of the mode collapse problem are investigated, and their subsequent impact on the diversity of synthetic X-ray images is evaluated. This work contributes an empirical demonstration of the benefits of integrating the adaptive input-image normalization with the Deep
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20165;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;-&#38899;&#39057;&#23545;&#27604;&#27169;&#22411;&#65288;CLAP&#65289;&#26469;&#24369;&#30417;&#30563;&#35757;&#32451;&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#23569;&#23545;&#37197;&#23545;&#38899;&#39057;&#21644;&#23383;&#24149;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#37319;&#29992;&#31574;&#30053;&#26469;&#24357;&#21512;&#38899;&#39057;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.12242</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#30340;&#32431;&#25991;&#26412;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Weakly-supervised Automated Audio Captioning via text only training. (arXiv:2309.12242v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12242
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20165;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;-&#38899;&#39057;&#23545;&#27604;&#27169;&#22411;&#65288;CLAP&#65289;&#26469;&#24369;&#30417;&#30563;&#35757;&#32451;&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#23569;&#23545;&#37197;&#23545;&#38899;&#39057;&#21644;&#23383;&#24149;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#37319;&#29992;&#31574;&#30053;&#26469;&#24357;&#21512;&#38899;&#39057;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#37197;&#23545;&#30340;&#38899;&#39057;&#21644;&#23383;&#24149;&#25968;&#25454;&#38598;&#22312;&#33258;&#21160;&#29983;&#25104;&#38899;&#39057;&#29255;&#27573;&#25551;&#36848;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#21363;&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#65288;AAC&#65289;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#36275;&#22815;&#25968;&#37327;&#30340;&#37197;&#23545;&#38899;&#39057;&#21644;&#23383;&#24149;&#25968;&#25454;&#26159;&#19968;&#39033;&#36153;&#26102;&#36153;&#21147;&#30340;&#24037;&#20316;&#12290;&#21463;&#21040;&#23545;&#27604;&#24615;&#35821;&#35328;-&#38899;&#39057;&#39044;&#35757;&#32451;&#65288;CLAP&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21482;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#30340;CLAP&#27169;&#22411;&#26469;&#35757;&#32451;AAC&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#23545;&#37197;&#23545;&#30446;&#26631;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;CLAP&#20013;&#38899;&#39057;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23398;&#20064;&#20174;CLAP&#25991;&#26412;&#23884;&#20837;&#20013;&#37325;&#24314;&#25991;&#26412;&#65292;&#32780;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#38899;&#39057;&#23884;&#20837;&#36827;&#34892;&#35299;&#30721;&#12290;&#20026;&#20102;&#20943;&#23567;&#38899;&#39057;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#37319;&#29992;&#20102;&#31574;&#30053;&#26469;&#24357;&#21512;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;Clotho&#21644;AudioCaps&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
In recent years, datasets of paired audio and captions have enabled remarkable success in automatically generating descriptions for audio clips, namely Automated Audio Captioning (AAC). However, it is labor-intensive and time-consuming to collect a sufficient number of paired audio and captions. Motivated by the recent advances in Contrastive Language-Audio Pretraining (CLAP), we propose a weakly-supervised approach to train an AAC model assuming only text data and a pre-trained CLAP model, alleviating the need for paired target data. Our approach leverages the similarity between audio and text embeddings in CLAP. During training, we learn to reconstruct the text from the CLAP text embedding, and during inference, we decode using the audio embeddings. To mitigate the modality gap between the audio and text embeddings we employ strategies to bridge the gap during training and inference stages. We evaluate our proposed method on Clotho and AudioCaps datasets demonstrating its ability to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;t-EER&#36825;&#19968;&#26032;&#30340;&#26080;&#21442;&#25968;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#32852;&#21512;&#35780;&#20272;&#22312;&#29983;&#29289;&#29305;&#24449;&#39564;&#35777;&#20013;&#19982;&#25915;&#20987;&#26816;&#27979;&#21516;&#26102;&#25805;&#20316;&#30340;&#23545;&#31574;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#32852;&#21512;&#35780;&#20272;&#26041;&#27861;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#20998;&#24320;&#35780;&#20272;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12237</link><description>&lt;p&gt;
t-EER: &#26080;&#21442;&#25968;&#30340;&#32852;&#21512;&#35780;&#20272;&#23545;&#31574;&#21644;&#29983;&#29289;&#29305;&#24449;&#27604;&#36739;&#22120;
&lt;/p&gt;
&lt;p&gt;
t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric Comparators. (arXiv:2309.12237v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;t-EER&#36825;&#19968;&#26032;&#30340;&#26080;&#21442;&#25968;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#32852;&#21512;&#35780;&#20272;&#22312;&#29983;&#29289;&#29305;&#24449;&#39564;&#35777;&#20013;&#19982;&#25915;&#20987;&#26816;&#27979;&#21516;&#26102;&#25805;&#20316;&#30340;&#23545;&#31574;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#32852;&#21512;&#35780;&#20272;&#26041;&#27861;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#20998;&#24320;&#35780;&#20272;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#25915;&#20987;&#65288;&#20266;&#20882;&#65289;&#26816;&#27979;&#65288;PAD&#65289;&#36890;&#24120;&#19982;&#29983;&#29289;&#29305;&#24449;&#39564;&#35777;&#21516;&#26102;&#25805;&#20316;&#20197;&#25552;&#39640;&#22312;&#38754;&#23545;&#20266;&#20882;&#25915;&#20987;&#26102;&#30340;&#21487;&#38752;&#24615;&#12290;&#23613;&#31649;&#36825;&#20004;&#20010;&#23376;&#31995;&#32479;&#21516;&#26102;&#20316;&#29992;&#20110;&#21487;&#38752;&#30340;&#29983;&#29289;&#29305;&#24449;&#39564;&#35777;&#30340;&#21333;&#19968;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#35299;&#20915;&#19981;&#21516;&#30340;&#26816;&#27979;&#20219;&#21153;&#65292;&#22240;&#27492;&#36890;&#24120;&#34987;&#20998;&#24320;&#35780;&#20272;&#12290;&#35777;&#25454;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#27425;&#20248;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#29992;&#20110;&#22312;&#19982;&#29983;&#29289;&#29305;&#24449;&#39564;&#35777;&#21516;&#26102;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#32852;&#21512;&#35780;&#20272;PAD&#35299;&#20915;&#26041;&#26696;&#12290;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#20018;&#32852;&#26816;&#27979;&#25104;&#26412;&#20989;&#25968;&#30456;&#21453;&#65292;&#26032;&#30340;&#20018;&#32852;&#31561;&#35823;&#24046;&#29575;&#65288;t-EER&#65289;&#26159;&#26080;&#21442;&#25968;&#30340;&#12290;&#28982;&#32780;&#65292;&#20004;&#20010;&#20998;&#31867;&#22120;&#30340;&#32452;&#21512;&#23548;&#33268;&#20102;&#19968;&#32452;&#24037;&#20316;&#28857;&#65292;&#22312;&#36825;&#20123;&#24037;&#20316;&#28857;&#19978;&#65292;&#35823;&#25253;&#21644;&#28431;&#25253;&#29575;&#26159;&#30456;&#31561;&#30340;&#65292;&#20063;&#21462;&#20915;&#20110;&#25915;&#20987;&#30340;&#26222;&#36941;&#31243;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#24182;&#21457;&#8221;&#30340;t-EER&#65292;&#36825;&#26159;&#19968;&#20010;&#19982;&#25915;&#20987;&#30340;&#26222;&#36941;&#31243;&#24230;&#26080;&#20851;&#30340;&#29420;&#29305;&#24037;&#20316;&#28857;&#12290;&#21033;&#29992;&#27169;&#24577;&#65288;&#29978;&#33267;&#24212;&#29992;&#65289;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#26174;&#31034;&#20102;t-EER&#30340;&#23454;&#29992;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Presentation attack (spoofing) detection (PAD) typically operates alongside biometric verification to improve reliablity in the face of spoofing attacks. Even though the two sub-systems operate in tandem to solve the single task of reliable biometric verification, they address different detection tasks and are hence typically evaluated separately. Evidence shows that this approach is suboptimal. We introduce a new metric for the joint evaluation of PAD solutions operating in situ with biometric verification. In contrast to the tandem detection cost function proposed recently, the new tandem equal error rate (t-EER) is parameter free. The combination of two classifiers nonetheless leads to a \emph{set} of operating points at which false alarm and miss rates are equal and also dependent upon the prevalence of attacks. We therefore introduce the \emph{concurrent} t-EER, a unique operating point which is invariable to the prevalence of attacks. Using both modality (and even application) ag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26680;&#24179;&#28369;&#25216;&#26415;&#20462;&#27491;&#20256;&#32479;&#26657;&#20934;&#24230;&#37327;&#21644;&#21487;&#38752;&#24615;&#22270;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24179;&#28369;&#26657;&#20934;&#24230;&#37327; SmoothECE&#65292;&#21516;&#26102;&#21487;&#35270;&#21270;&#35813;&#24230;&#37327;&#30340;&#21487;&#38752;&#24615;&#22270;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#33391;&#22909;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2309.12236</link><description>&lt;p&gt;
&#24179;&#28369;&#30340;ECE: &#36890;&#36807;&#26680;&#24179;&#28369;&#23454;&#29616;&#30340;&#21487;&#38752;&#24615;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing. (arXiv:2309.12236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26680;&#24179;&#28369;&#25216;&#26415;&#20462;&#27491;&#20256;&#32479;&#26657;&#20934;&#24230;&#37327;&#21644;&#21487;&#38752;&#24615;&#22270;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24179;&#28369;&#26657;&#20934;&#24230;&#37327; SmoothECE&#65292;&#21516;&#26102;&#21487;&#35270;&#21270;&#35813;&#24230;&#37327;&#30340;&#21487;&#38752;&#24615;&#22270;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#33391;&#22909;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26657;&#20934;&#24230;&#37327;&#21644;&#21487;&#38752;&#24615;&#22270;&#26159;&#34913;&#37327;&#21644;&#35299;&#37322;&#27010;&#29575;&#39044;&#27979;&#22120;&#26657;&#20934;&#24615;&#30340;&#20004;&#20010;&#22522;&#26412;&#24037;&#20855;&#12290;&#26657;&#20934;&#24230;&#37327;&#29992;&#20110;&#37327;&#21270;&#26657;&#20934;&#20559;&#24046;&#30340;&#31243;&#24230;&#65292;&#21487;&#38752;&#24615;&#22270;&#21487;&#35270;&#21270;&#35813;&#26657;&#20934;&#20559;&#24046;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#26657;&#20934;&#24230;&#37327;&#21644;&#21487;&#38752;&#24615;&#22270;&#26500;&#24314;&#26041;&#27861;-&#20998;&#31665;&#21644;ECE -- &#37117;&#23384;&#22312;&#24050;&#30693;&#30340;&#32570;&#38519;&#65288;&#22914;&#19981;&#36830;&#32493;&#24615;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25913;&#36827;&#21487;&#20197;&#20462;&#22797;&#36825;&#20004;&#31181;&#26500;&#24314;&#26041;&#27861;&#65306;&#39318;&#20808;&#20351;&#29992;RBF&#26680;&#24179;&#28369;&#35266;&#27979;&#32467;&#26524;&#65292;&#28982;&#21518;&#35745;&#31639;&#24179;&#28369;&#20989;&#25968;&#30340;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#23567;&#24515;&#36873;&#25321;&#24102;&#23485;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#30340;&#26657;&#20934;&#24230;&#37327;&#26159;&#33391;&#22909;&#34892;&#20026;&#30340;&#65288;&#30001;B{\l}asiok, Gopalan, Hu, and Nakkiran 2023a&#23450;&#20041;&#30340;&#19968;&#33268;&#26657;&#20934;&#24230;&#37327;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24230;&#37327;&#31216;&#20026;SmoothECE&#12290;&#27492;&#22806;&#65292;&#20174;&#36825;&#20010;&#24179;&#28369;&#20989;&#25968;&#24471;&#21040;&#30340;&#21487;&#38752;&#24615;&#22270;&#21487;&#20197;&#30452;&#35266;&#22320;&#34920;&#31034;SmoothECE&#65292;&#23601;&#20687;&#20998;&#31665;&#26041;&#27861;&#19968;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibration measures and reliability diagrams are two fundamental tools for measuring and interpreting the calibration of probabilistic predictors. Calibration measures quantify the degree of miscalibration, and reliability diagrams visualize the structure of this miscalibration. However, the most common constructions of reliability diagrams and calibration measures -binning and ECE -- both suffer from well-known flaws (e.g. discontinuity). We show that a simple modification fixes both constructions: first smooth the observations using an RBF kernel, then compute the Expected Calibration Error (ECE) of this smoothed function. We prove that with a careful choice of bandwidth, this method yields a calibration measure that is well-behaved in the sense of (B{\l}asiok, Gopalan, Hu, and Nakkiran 2023a) -- a consistent calibration measure. We call this measure the SmoothECE. Moreover, the reliability diagram obtained from this smoothed function visually encodes the SmoothECE, just as binned
&lt;/p&gt;</description></item><item><title>&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#26159;&#32435;&#20160;&#22343;&#34913;&#30340;&#19968;&#20010;&#26494;&#24347;&#21464;&#31181;&#65292;&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#19982;&#26368;&#20339;&#20809;&#28369;&#31574;&#30053;&#30340;&#20559;&#31163;&#30456;&#21516;&#30340;&#25928;&#29992;&#26469;&#36798;&#21040;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#24378;&#21644;&#24369;&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#35745;&#31639;&#24615;&#36136;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2309.12226</link><description>&lt;p&gt;
&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#65306;&#31639;&#27861;&#21644;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Smooth Nash Equilibria: Algorithms and Complexity. (arXiv:2309.12226v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12226
&lt;/p&gt;
&lt;p&gt;
&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#26159;&#32435;&#20160;&#22343;&#34913;&#30340;&#19968;&#20010;&#26494;&#24347;&#21464;&#31181;&#65292;&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#19982;&#26368;&#20339;&#20809;&#28369;&#31574;&#30053;&#30340;&#20559;&#31163;&#30456;&#21516;&#30340;&#25928;&#29992;&#26469;&#36798;&#21040;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#24378;&#21644;&#24369;&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#35745;&#31639;&#24615;&#36136;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32435;&#20160;&#22343;&#34913;&#30340;&#19968;&#20010;&#22522;&#26412;&#32570;&#28857;&#26159;&#20854;&#35745;&#31639;&#22797;&#26434;&#24615;&#65306;&#22312;&#27491;&#21017;&#24418;&#24335;&#30340;&#21338;&#24328;&#20013;&#65292;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#26159;PPAD&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#24179;&#28369;&#20998;&#26512;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#30340;&#26494;&#24347;&#21464;&#31181;&#65292;&#20854;&#20013;$\sigma$&#26159;&#20809;&#28369;&#24615;&#21442;&#25968;&#12290;&#22312;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#20013;&#65292;&#29609;&#23478;&#20204;&#21482;&#38656;&#35201;&#23454;&#29616;&#33267;&#23569;&#19982;&#20182;&#20204;&#26368;&#20339;$\sigma$-&#20809;&#28369;&#31574;&#30053;&#30340;&#20559;&#31163;&#30456;&#21516;&#30340;&#25928;&#29992;&#65292;&#32780;&#36825;&#20010;$\sigma$-&#20809;&#28369;&#31574;&#30053;&#26159;&#19981;&#20250;&#23545;&#20219;&#20309;&#22266;&#23450;&#21160;&#20316;&#20135;&#29983;&#36807;&#22810;&#36136;&#37327;&#65288;&#26681;&#25454;$\sigma$&#21442;&#25968;&#21270;&#65289;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#20004;&#31181;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#30340;&#21464;&#31181;&#65306;&#24378;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29609;&#23478;&#20204;&#38656;&#35201;&#22312;&#22343;&#34913;&#20013;&#37319;&#29992;$\sigma$-&#20809;&#28369;&#31574;&#30053;&#36827;&#34892;&#28216;&#25103;&#65307;&#24369;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#20013;&#65292;&#27809;&#26377;&#36825;&#26679;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26080;&#35770;&#26159;&#24369;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#36824;&#26159;&#24378;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#65292;&#37117;&#27604;&#32435;&#20160;&#22343;&#34913;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental shortcoming of the concept of Nash equilibrium is its computational intractability: approximating Nash equilibria in normal-form games is PPAD-hard. In this paper, inspired by the ideas of smoothed analysis, we introduce a relaxed variant of Nash equilibrium called $\sigma$-smooth Nash equilibrium, for a smoothness parameter $\sigma$. In a $\sigma$-smooth Nash equilibrium, players only need to achieve utility at least as high as their best deviation to a $\sigma$-smooth strategy, which is a distribution that does not put too much mass (as parametrized by $\sigma$) on any fixed action. We distinguish two variants of $\sigma$-smooth Nash equilibria: strong $\sigma$-smooth Nash equilibria, in which players are required to play $\sigma$-smooth strategies under equilibrium play, and weak $\sigma$-smooth Nash equilibria, where there is no such requirement.  We show that both weak and strong $\sigma$-smooth Nash equilibria have superior computational properties to Nash equilibri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#24555;&#36895;&#27169;&#25311;&#39640;&#32500;&#39057;&#29575;&#36873;&#25321;&#24615;&#34920;&#38754;&#12290;&#36890;&#36807;&#24341;&#20837;&#29289;&#29702;&#27934;&#23519;&#21147;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#35757;&#32451;&#21518;&#20351;&#29992;&#20943;&#23569;&#30340;&#25968;&#25454;&#38598;&#20934;&#30830;&#39044;&#27979;&#26576;&#31181;&#32467;&#26500;&#30340;S&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#21508;&#31181;&#39057;&#29575;&#36873;&#25321;&#24615;&#34920;&#38754;&#65292;&#26080;&#35770;&#26159;&#22522;&#20110;&#23380;&#38553;&#36824;&#26159;&#20219;&#24847;&#20960;&#20309;&#24418;&#29366;&#30340;&#36148;&#29255;&#12290;</title><link>http://arxiv.org/abs/2309.12223</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#39640;&#32500;&#21608;&#26399;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Model-based Deep Learning for High-Dimensional Periodic Structures. (arXiv:2309.12223v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#24555;&#36895;&#27169;&#25311;&#39640;&#32500;&#39057;&#29575;&#36873;&#25321;&#24615;&#34920;&#38754;&#12290;&#36890;&#36807;&#24341;&#20837;&#29289;&#29702;&#27934;&#23519;&#21147;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#35757;&#32451;&#21518;&#20351;&#29992;&#20943;&#23569;&#30340;&#25968;&#25454;&#38598;&#20934;&#30830;&#39044;&#27979;&#26576;&#31181;&#32467;&#26500;&#30340;S&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#21508;&#31181;&#39057;&#29575;&#36873;&#25321;&#24615;&#34920;&#38754;&#65292;&#26080;&#35770;&#26159;&#22522;&#20110;&#23380;&#38553;&#36824;&#26159;&#20219;&#24847;&#20960;&#20309;&#24418;&#29366;&#30340;&#36148;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#27169;&#25311;&#39640;&#32500;&#39057;&#29575;&#36873;&#25321;&#24615;&#34920;&#38754;&#30340;&#28145;&#24230;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#30001;&#22810;&#20010;&#36830;&#25509;&#30340;&#23631;&#24149;&#22534;&#21472;&#32452;&#25104;&#30340;&#21333;&#20803;&#26684;&#65292;&#20854;&#35774;&#35745;&#38656;&#35201;&#23545;&#35768;&#22810;&#20960;&#20309;&#33258;&#30001;&#24230;&#36827;&#34892;&#25511;&#21046;&#12290;&#36890;&#36807;&#23558;&#29289;&#29702;&#27934;&#23519;&#21147;&#24341;&#20837;&#27169;&#22411;&#65292;&#32463;&#36807;&#35757;&#32451;&#20351;&#29992;&#20943;&#23569;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#21487;&#20197;&#23545;&#29305;&#23450;&#32467;&#26500;&#30340;S&#21442;&#25968;&#36827;&#34892;&#31934;&#30830;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#38750;&#24120;&#22810;&#21151;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#31867;&#22411;&#30340;&#39057;&#29575;&#36873;&#25321;&#24615;&#34920;&#38754;&#19968;&#36215;&#20351;&#29992;&#65292;&#26080;&#35770;&#26159;&#22522;&#20110;&#23380;&#38553;&#36824;&#26159;&#20219;&#24847;&#20960;&#20309;&#24418;&#29366;&#30340;&#36148;&#29255;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#39057;&#29575;&#36873;&#25321;&#24615;&#34920;&#38754;&#30001;&#20855;&#26377;&#30697;&#24418;&#23380;&#38553;&#30340;&#23631;&#24149;&#32452;&#25104;&#30340;&#25968;&#20540;&#31034;&#20363;&#65292;&#26174;&#31034;&#20986;&#39044;&#27979;&#24615;&#33021;&#19982;&#20351;&#29992;&#20840;&#27874;&#27169;&#25311;&#22120;&#33719;&#24471;&#30340;&#24615;&#33021;&#20043;&#38388;&#30340;&#26497;&#22909;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a deep learning surrogate model for the fast simulation of high-dimensional frequency selective surfaces. We consider unit-cells which are built as multiple concatenated stacks of screens and their design requires the control over many geometrical degrees of freedom. Thanks to the introduction of physical insight into the model, it can produce accurate predictions of the S-parameters of a certain structure after training with a reduced dataset.The proposed model is highly versatile and it can be used with any kind of frequency selective surface, based on either perforations or patches of any arbitrary geometry. Numeric examples are presented here for the case of frequency selective surfaces composed of screens with rectangular perforations, showing an excellent agreement between the predicted performance and such obtained with a full-wave simulator.
&lt;/p&gt;</description></item><item><title>SR-PredictAO&#26159;&#19968;&#31181;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#20302;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#38543;&#26426;&#29992;&#25143;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.12218</link><description>&lt;p&gt;
SR-PredictAO: &#20855;&#26377;&#39640;&#33021;&#21147;&#39044;&#27979;&#22120;&#38468;&#21152;&#20214;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SR-PredictAO: Session-based Recommendation with High-Capability Predictor Add-On. (arXiv:2309.12218v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12218
&lt;/p&gt;
&lt;p&gt;
SR-PredictAO&#26159;&#19968;&#31181;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#20302;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#38543;&#26426;&#29992;&#25143;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#20165;&#22522;&#20110;&#21333;&#20010;&#20250;&#35805;&#20013;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#39033;&#30446;&#28857;&#20987;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#26576;&#20123;&#38543;&#26426;&#29992;&#25143;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#38656;&#35201;&#19968;&#20010;&#39640;&#33021;&#21147;&#30340;&#39044;&#27979;&#29992;&#25143;&#19979;&#19968;&#20010;&#21160;&#20316;&#30340;&#27169;&#22411;&#12290;&#22823;&#22810;&#25968;&#65288;&#22914;&#26524;&#19981;&#26159;&#20840;&#37096;&#65289;&#29616;&#26377;&#27169;&#22411;&#36981;&#24490;&#32534;&#30721;&#22120;-&#39044;&#27979;&#22120;&#33539;&#24335;&#65292;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#25152;&#26377;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#22914;&#20309;&#24191;&#27867;&#20248;&#21270;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#20294;&#23427;&#20204;&#24573;&#35270;&#20102;&#22914;&#20309;&#20248;&#21270;&#39044;&#27979;&#22120;&#27169;&#22359;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#20302;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#23384;&#22312;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#31216;&#20026;\emph{\underline{S}ession-based \underline{R}ecommendation with \underline{Pred}ictor \underline{A}dd-\underline{O}n} (SR-PredictAO)&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#33021;&#21147;&#30340;&#39044;&#27979;&#22120;&#27169;&#22359;&#65292;&#21487;&#20197;&#20943;&#36731;&#38543;&#26426;&#29992;&#25143;&#34892;&#20026;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#20540;&#24471;&#19968;&#25552;&#30340;&#26159;&#65292;
&lt;/p&gt;
&lt;p&gt;
Session-based recommendation, aiming at making the prediction of the user's next item click based on the information in a single session only even in the presence of some random user's behavior, is a complex problem. This complex problem requires a high-capability model of predicting the user's next action. Most (if not all) existing models follow the encoder-predictor paradigm where all studies focus on how to optimize the encoder module extensively in the paradigm but they ignore how to optimize the predictor module. In this paper, we discover the existing critical issue of the low-capability predictor module among existing models. Motivated by this, we propose a novel framework called \emph{\underline{S}ession-based \underline{R}ecommendation with \underline{Pred}ictor \underline{A}dd-\underline{O}n} (SR-PredictAO). In this framework, we propose a high-capability predictor module which could alleviate the effect of random user's behavior for prediction. It is worth mentioning that t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#38382;&#39064;&#36716;&#25442;&#30340;&#26041;&#24335;&#26377;&#25928;&#25552;&#39640;&#22522;&#20110;&#32908;&#30005;&#20449;&#21495;&#30340;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#24555;&#36895;&#26657;&#20934;&#24182;&#35782;&#21035;&#32452;&#21512;&#25163;&#21183;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12217</link><description>&lt;p&gt;
&#22686;&#24378;&#22522;&#20110;&#32908;&#30005;&#20449;&#21495;&#30340;&#25163;&#21183;&#35782;&#21035;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Multi-label Classification Approach to Increase Expressivity of EMG-based Gesture Recognition. (arXiv:2309.12217v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#38382;&#39064;&#36716;&#25442;&#30340;&#26041;&#24335;&#26377;&#25928;&#25552;&#39640;&#22522;&#20110;&#32908;&#30005;&#20449;&#21495;&#30340;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#24555;&#36895;&#26657;&#20934;&#24182;&#35782;&#21035;&#32452;&#21512;&#25163;&#21183;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#26412;&#30740;&#31350;&#26088;&#22312;&#26377;&#25928;&#25552;&#39640;&#22522;&#20110;&#32908;&#30005;&#20449;&#21495;&#30340;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#37319;&#29992;&#38382;&#39064;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#23558;&#21160;&#20316;&#20998;&#20026;&#20004;&#20010;&#29983;&#29289;&#21147;&#23398;&#29420;&#31435;&#30340;&#32452;&#25104;&#37096;&#20998;&#8212;&#8212;&#19968;&#32452;&#25163;&#33109;&#26041;&#21521;&#21644;&#19968;&#32452;&#25163;&#25351;&#20462;&#39280;&#22120;&#12290;&#20026;&#20102;&#20445;&#25345;&#24555;&#36895;&#26657;&#20934;&#26102;&#38388;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#21333;&#20010;&#25163;&#21183;&#35757;&#32451;&#27599;&#20010;&#32452;&#20214;&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#22806;&#25512;&#21040;&#32452;&#21512;&#25163;&#21183;&#30340;&#23436;&#25972;&#20135;&#21697;&#31354;&#38388;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26377;&#39640;&#32622;&#20449;&#24230;&#26631;&#31614;&#30340;&#30417;&#30563;&#24335;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21463;&#35797;&#32773;&#22312;&#25345;&#26377;&#28216;&#25103;&#25163;&#26564;&#30340;&#21516;&#26102;&#36827;&#34892;&#32452;&#21512;&#25163;&#21183;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#20998;&#26512;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#20998;&#31867;&#22120;&#31639;&#27861;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31574;&#30053;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20027;&#35201;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#24182;&#34892;&#27169;&#22411;&#26550;&#26500;&#21644;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#38382;&#39064;&#36716;&#25442;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The objective of the study is to efficiently increase the expressivity of surface electromyography-based (sEMG) gesture recognition systems. Approach: We use a problem transformation approach, in which actions were subset into two biomechanically independent components - a set of wrist directions and a set of finger modifiers. To maintain fast calibration time, we train models for each component using only individual gestures, and extrapolate to the full product space of combination gestures by generating synthetic data. We collected a supervised dataset with high-confidence ground truth labels in which subjects performed combination gestures while holding a joystick, and conducted experiments to analyze the impact of model architectures, classifier algorithms, and synthetic data generation strategies on the performance of the proposed approach. Main Results: We found that a problem transformation approach using a parallel model architecture in combination with a non-linear 
&lt;/p&gt;</description></item><item><title>&#21306;&#22495;&#21487;&#21152;&#27169;&#22411; (RAMs) &#26159;&#19968;&#31181;&#35774;&#35745;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20869;&#35782;&#21035;&#23376;&#21306;&#22495;&#26469;&#26368;&#23567;&#21270;&#29305;&#24449;&#30340;&#20132;&#20114;&#12290;&#30456;&#27604;&#20110;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;(GAMs)&#65292;RAMs&#33021;&#25317;&#26377;&#26356;&#20016;&#23500;&#30340;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12215</link><description>&lt;p&gt;
&#21306;&#22495;&#21487;&#21152;&#27169;&#22411;: &#26368;&#23567;&#21270;&#29305;&#24449;&#20132;&#20114;&#30340;&#35774;&#35745;&#21487;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Regionally Additive Models: Explainable-by-design models minimizing feature interactions. (arXiv:2309.12215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12215
&lt;/p&gt;
&lt;p&gt;
&#21306;&#22495;&#21487;&#21152;&#27169;&#22411; (RAMs) &#26159;&#19968;&#31181;&#35774;&#35745;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20869;&#35782;&#21035;&#23376;&#21306;&#22495;&#26469;&#26368;&#23567;&#21270;&#29305;&#24449;&#30340;&#20132;&#20114;&#12290;&#30456;&#27604;&#20110;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;(GAMs)&#65292;RAMs&#33021;&#25317;&#26377;&#26356;&#20016;&#23500;&#30340;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#21487;&#21152;&#27169;&#22411; (GAMs) &#26159;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#35774;&#35745;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290; GAMs&#20551;&#35774;&#36755;&#20986;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#32452;&#21333;&#21464;&#37327;&#20989;&#25968;&#30340;&#21644;&#65292;&#31216;&#20026;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#22312;&#36755;&#20986;&#20381;&#36182;&#20110;&#22810;&#20010;&#29305;&#24449;&#21516;&#26102;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#36825;&#31181;&#20551;&#35774;&#19981;&#25104;&#31435;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;GAMs&#26080;&#27861;&#25429;&#25417;&#21040;&#24213;&#23618;&#20989;&#25968;&#30340;&#20132;&#20114;&#39033;&#65292;&#23548;&#33268;&#20934;&#30830;&#24615;&#19981;&#20339;&#12290;&#20026;&#20102;(&#37096;&#20998;)&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21306;&#22495;&#21487;&#21152;&#27169;&#22411; (RAMs)&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#35745;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;RAMs&#35782;&#21035;&#29305;&#24449;&#31354;&#38388;&#20869;&#30340;&#23376;&#21306;&#22495;&#65292;&#22312;&#36825;&#20123;&#23376;&#21306;&#22495;&#20013;&#26368;&#23567;&#21270;&#20102;&#29305;&#24449;&#30340;&#20132;&#20114;&#12290;&#22312;&#36825;&#20123;&#21306;&#22495;&#20869;&#65292;&#25226;&#36755;&#20986;&#34920;&#31034;&#20026;&#19968;&#32452;&#21333;&#21464;&#37327;&#20989;&#25968; (&#32452;&#20214;) &#30456;&#23545;&#20110;&#25226;&#36755;&#20986;&#34920;&#31034;&#20026;&#19968;&#20010;&#29305;&#24449;&#30340;&#21333;&#21464;&#37327;&#20989;&#25968;&#26356;&#21152;&#20934;&#30830;&#12290;&#22240;&#27492;&#65292;RAMs&#30456;&#27604;&#20110;GAMs&#25317;&#26377;&#26356;&#20016;&#23500;&#30340;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;RAM&#26694;&#26550;&#30001;&#19977;&#20010;&#27493;&#39588;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Additive Models (GAMs) are widely used explainable-by-design models in various applications. GAMs assume that the output can be represented as a sum of univariate functions, referred to as components. However, this assumption fails in ML problems where the output depends on multiple features simultaneously. In these cases, GAMs fail to capture the interaction terms of the underlying function, leading to subpar accuracy. To (partially) address this issue, we propose Regionally Additive Models (RAMs), a novel class of explainable-by-design models. RAMs identify subregions within the feature space where interactions are minimized. Within these regions, it is more accurate to express the output as a sum of univariate functions (components). Consequently, RAMs fit one component per subregion of each feature instead of one component per feature. This approach yields a more expressive model compared to GAMs while retaining interpretability. The RAM framework consists of three step
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32477;&#28909;&#36229;&#23548;Josephson&#22120;&#20214;&#30340;&#38543;&#26426;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#26694;&#26550;SupeRBNN&#65292;&#36890;&#36807;&#36719;&#30828;&#20214;&#21327;&#21516;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#32477;&#28909;&#37327;&#23376;&#27969;&#21442;&#25968;&#20803;&#22312;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.12212</link><description>&lt;p&gt;
SupeRBNN: &#20351;&#29992;&#32477;&#28909;&#36229;&#23548;Josephson&#22120;&#20214;&#30340;&#38543;&#26426;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SupeRBNN: Randomized Binary Neural Network Using Adiabatic Superconductor Josephson Devices. (arXiv:2309.12212v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12212
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32477;&#28909;&#36229;&#23548;Josephson&#22120;&#20214;&#30340;&#38543;&#26426;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#26694;&#26550;SupeRBNN&#65292;&#36890;&#36807;&#36719;&#30828;&#20214;&#21327;&#21516;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#32477;&#28909;&#37327;&#23376;&#27969;&#21442;&#25968;&#20803;&#22312;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32477;&#28909;&#37327;&#23376;&#27969;&#21442;&#25968;&#20803;(AQFP)&#26159;&#19968;&#31181;&#20855;&#26377;&#26497;&#39640;&#33021;&#37327;&#25928;&#29575;&#30340;&#36229;&#23548;&#36923;&#36753;&#12290;&#36890;&#36807;&#21033;&#29992;&#30005;&#27969;&#30340;&#19981;&#21516;&#26497;&#24615;&#26469;&#34920;&#31034;&#36923;&#36753;&#8220;0&#8221;&#21644;&#8220;1&#8221;&#65292;AQFP&#22120;&#20214;&#25104;&#20026;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;(BNN)&#35745;&#31639;&#30340;&#20248;&#31168;&#36733;&#20307;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#24320;&#21457;&#22522;&#20110;AQFP&#30340;BNN&#21152;&#36895;&#22120;&#26041;&#38754;&#21462;&#24471;&#20102;&#21021;&#27493;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#35813;&#35774;&#35745;&#25104;&#20026;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SupeRBNN&#65292;&#19968;&#31181;&#22522;&#20110;AQFP&#30340;&#38543;&#26426;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#30828;&#20214;&#21327;&#21516;&#20248;&#21270;&#26368;&#32456;&#20351;AQFP&#22120;&#20214;&#25104;&#20026;BNN&#21152;&#36895;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;AQFP&#22120;&#20214;&#30340;&#38543;&#26426;&#34892;&#20026;&#65292;&#24182;&#20998;&#26512;&#20102;&#20132;&#21449;&#24320;&#20851;&#23610;&#23544;&#23545;&#30005;&#27969;&#34928;&#20943;&#30340;&#24433;&#21709;&#65292;&#38543;&#21518;&#23558;&#30005;&#27969;&#25391;&#24133;&#36716;&#21270;&#20026;&#36866;&#21512;&#29992;&#20110;BNN&#35745;&#31639;&#30340;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#31215;&#32047;&#38382;&#39064;&#24182;&#25552;&#39640;&#25972;&#20307;&#30828;&#20214;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adiabatic Quantum-Flux-Parametron (AQFP) is a superconducting logic with extremely high energy efficiency. By employing the distinct polarity of current to denote logic `0' and `1', AQFP devices serve as excellent carriers for binary neural network (BNN) computations. Although recent research has made initial strides toward developing an AQFP-based BNN accelerator, several critical challenges remain, preventing the design from being a comprehensive solution. In this paper, we propose SupeRBNN, an AQFP-based randomized BNN acceleration framework that leverages software-hardware co-optimization to eventually make the AQFP devices a feasible solution for BNN acceleration. Specifically, we investigate the randomized behavior of the AQFP devices and analyze the impact of crossbar size on current attenuation, subsequently formulating the current amplitude into the values suitable for use in BNN computation. To tackle the accumulation problem and improve overall hardware performance, we propo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#29289;&#29702;&#20449;&#24687;&#35302;&#21457;&#29366;&#24577;&#31354;&#38388;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;PSMs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#21270;&#23398;&#12289;&#29983;&#29289;&#21307;&#23398;&#21644;&#30005;&#21378;&#31561;&#20256;&#36755;&#29616;&#35937;&#20013;&#23454;&#29616;&#23454;&#26102;&#20248;&#21270;&#21644;&#23481;&#38169;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#29289;&#29702;&#27169;&#22411;&#36827;&#34892;&#20256;&#36755;&#31995;&#32479;&#24314;&#27169;&#65292;PSMs&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2309.12211</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#35302;&#21457;&#29366;&#24577;&#31354;&#38388;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20256;&#36755;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Physics-informed State-space Neural Networks for Transport Phenomena. (arXiv:2309.12211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12211
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#29289;&#29702;&#20449;&#24687;&#35302;&#21457;&#29366;&#24577;&#31354;&#38388;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;PSMs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#21270;&#23398;&#12289;&#29983;&#29289;&#21307;&#23398;&#21644;&#30005;&#21378;&#31561;&#20256;&#36755;&#29616;&#35937;&#20013;&#23454;&#29616;&#23454;&#26102;&#20248;&#21270;&#21644;&#23481;&#38169;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#29289;&#29702;&#27169;&#22411;&#36827;&#34892;&#20256;&#36755;&#31995;&#32479;&#24314;&#27169;&#65292;PSMs&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#29289;&#29702;&#20449;&#24687;&#35302;&#21457;&#29366;&#24577;&#31354;&#38388;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;PSMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#23454;&#29616;&#23454;&#26102;&#20248;&#21270;&#12289;&#28789;&#27963;&#24615;&#21644;&#23481;&#38169;&#24615;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21270;&#23398;&#12289;&#29983;&#29289;&#21307;&#23398;&#21644;&#30005;&#21378;&#31561;&#20197;&#20256;&#36755;&#20026;&#20027;&#23548;&#30340;&#31995;&#32479;&#12290;&#20256;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30001;&#20110;&#32570;&#20047;&#20687;&#36136;&#37327;&#23432;&#24658;&#36825;&#26679;&#30340;&#29289;&#29702;&#32422;&#26463;&#32780;&#26377;&#25152;&#19981;&#36275;&#12290;PSMs&#36890;&#36807;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#37096;&#20998;&#24494;&#20998;&#26041;&#31243;&#23545;&#29289;&#29702;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#24471;&#21040;&#20855;&#26377;&#29289;&#29702;&#32422;&#26463;&#30340;&#21487;&#36845;&#20195;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#36890;&#36807;&#20004;&#20010;&#20223;&#30495;&#23454;&#39564; - &#21152;&#28909;&#36890;&#36947;&#21644;&#20919;&#21364;&#31995;&#32479;&#22238;&#36335;&#65292;&#25105;&#20204;&#35777;&#26126;PSMs&#27604;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;&#38500;&#20102;&#20934;&#30830;&#24615;&#20043;&#22806;&#65292;PSMs&#36824;&#20855;&#26377;&#22810;&#31181;&#20196;&#20154;&#20449;&#26381;&#30340;&#29992;&#20363;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20854;&#20013;&#30340;&#20004;&#20010;&#65306;&#36890;&#36807;&#39034;&#24207;&#26356;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#21019;&#24314;&#38750;&#32447;&#24615;&#30417;&#25511;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
This work introduces Physics-informed State-space neural network Models (PSMs), a novel solution to achieving real-time optimization, flexibility, and fault tolerance in autonomous systems, particularly in transport-dominated systems such as chemical, biomedical, and power plants. Traditional data-driven methods fall short due to a lack of physical constraints like mass conservation; PSMs address this issue by training deep neural networks with sensor data and physics-informing using components' Partial Differential Equations (PDEs), resulting in a physics-constrained, end-to-end differentiable forward dynamics model. Through two in silico experiments - a heated channel and a cooling system loop - we demonstrate that PSMs offer a more accurate approach than purely data-driven models.  Beyond accuracy, there are several compelling use cases for PSMs. In this work, we showcase two: the creation of a nonlinear supervisory controller through a sequentially updated state-space representatio
&lt;/p&gt;</description></item><item><title>Boolformer&#26159;&#31532;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#25191;&#34892;&#31471;&#21040;&#31471;&#30340;&#24067;&#23572;&#20989;&#25968;&#31526;&#21495;&#22238;&#24402;&#12290;&#23427;&#21487;&#20197;&#39044;&#27979;&#22797;&#26434;&#20989;&#25968;&#30340;&#31616;&#27905;&#20844;&#24335;&#65292;&#24182;&#22312;&#25552;&#20379;&#19981;&#23436;&#25972;&#21644;&#26377;&#22122;&#22768;&#35266;&#27979;&#26102;&#25214;&#21040;&#36817;&#20284;&#34920;&#36798;&#24335;&#12290;Boolformer&#22312;&#30495;&#23454;&#20108;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#28508;&#21147;&#20316;&#20026;&#21487;&#35299;&#37322;&#24615;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#22312;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#21160;&#21147;&#23398;&#24314;&#27169;&#20219;&#21153;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#36951;&#20256;&#31639;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12207</link><description>&lt;p&gt;
Boolformer: &#29992;Transformer&#36827;&#34892;&#36923;&#36753;&#20989;&#25968;&#30340;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Boolformer: Symbolic Regression of Logic Functions with Transformers. (arXiv:2309.12207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12207
&lt;/p&gt;
&lt;p&gt;
Boolformer&#26159;&#31532;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#25191;&#34892;&#31471;&#21040;&#31471;&#30340;&#24067;&#23572;&#20989;&#25968;&#31526;&#21495;&#22238;&#24402;&#12290;&#23427;&#21487;&#20197;&#39044;&#27979;&#22797;&#26434;&#20989;&#25968;&#30340;&#31616;&#27905;&#20844;&#24335;&#65292;&#24182;&#22312;&#25552;&#20379;&#19981;&#23436;&#25972;&#21644;&#26377;&#22122;&#22768;&#35266;&#27979;&#26102;&#25214;&#21040;&#36817;&#20284;&#34920;&#36798;&#24335;&#12290;Boolformer&#22312;&#30495;&#23454;&#20108;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#28508;&#21147;&#20316;&#20026;&#21487;&#35299;&#37322;&#24615;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#22312;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#21160;&#21147;&#23398;&#24314;&#27169;&#20219;&#21153;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#36951;&#20256;&#31639;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Boolformer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#25191;&#34892;&#31471;&#21040;&#31471;&#30340;&#24067;&#23572;&#20989;&#25968;&#31526;&#21495;&#22238;&#24402;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25552;&#20379;&#24178;&#20928;&#30340;&#30495;&#20540;&#34920;&#26102;&#65292;&#23427;&#21487;&#20197;&#39044;&#27979;&#22797;&#26434;&#20989;&#25968;&#30340;&#31616;&#27905;&#20844;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#25552;&#20379;&#19981;&#23436;&#25972;&#21644;&#26377;&#22122;&#22768;&#35266;&#27979;&#26102;&#25214;&#21040;&#36817;&#20284;&#34920;&#36798;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#30495;&#23454;&#20108;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Boolformer&#65292;&#35777;&#26126;&#20102;&#23427;&#20316;&#20026;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#26367;&#20195;&#21697;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#24314;&#27169;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#21160;&#21147;&#23398;&#30340;&#24120;&#35265;&#20219;&#21153;&#12290;&#20351;&#29992;&#26368;&#36817;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Boolformer&#19982;&#26368;&#20808;&#36827;&#30340;&#36951;&#20256;&#31639;&#27861;&#30456;&#27604;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce Boolformer, the first Transformer architecture trained to perform end-to-end symbolic regression of Boolean functions. First, we show that it can predict compact formulas for complex functions which were not seen during training, when provided a clean truth table. Then, we demonstrate its ability to find approximate expressions when provided incomplete and noisy observations. We evaluate the Boolformer on a broad set of real-world binary classification datasets, demonstrating its potential as an interpretable alternative to classic machine learning methods. Finally, we apply it to the widespread task of modelling the dynamics of gene regulatory networks. Using a recent benchmark, we show that Boolformer is competitive with state-of-the art genetic algorithms with a speedup of several orders of magnitude. Our code and models are available publicly.
&lt;/p&gt;</description></item><item><title>PrNet&#26159;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;Android&#21407;&#22987;GNSS&#27979;&#37327;&#23450;&#20301;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26657;&#27491;&#20266;&#36317;&#20559;&#24046;&#26469;&#25552;&#39640;&#23450;&#20301;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#22788;&#29702;&#20266;&#36317;&#20559;&#24046;&#21644;&#22122;&#22768;&#30340;&#28151;&#21512;&#31649;&#36947;&#12290;</title><link>http://arxiv.org/abs/2309.12204</link><description>&lt;p&gt;
PrNet&#65306;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;Android&#21407;&#22987;GNSS&#27979;&#37327;&#23450;&#20301;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26657;&#27491;&#20266;&#36317;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
PrNet: A Neural Network for Correcting Pseudoranges to Improve Positioning with Android Raw GNSS Measurements. (arXiv:2309.12204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12204
&lt;/p&gt;
&lt;p&gt;
PrNet&#26159;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;Android&#21407;&#22987;GNSS&#27979;&#37327;&#23450;&#20301;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26657;&#27491;&#20266;&#36317;&#20559;&#24046;&#26469;&#25552;&#39640;&#23450;&#20301;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#22788;&#29702;&#20266;&#36317;&#20559;&#24046;&#21644;&#22122;&#22768;&#30340;&#28151;&#21512;&#31649;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#36731;&#20266;&#36317;&#20559;&#24046;&#65292;&#20197;&#25552;&#39640;&#20351;&#29992;&#20174;&#23433;&#21331;&#26234;&#33021;&#25163;&#26426;&#25910;&#38598;&#30340;&#25968;&#25454;&#30340;&#23450;&#20301;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#23454;&#29992;&#30340;&#22522;&#20110;&#21355;&#26143;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#34920;&#31034;&#20266;&#36317;&#20559;&#24046;&#65292;&#20854;&#36755;&#20837;&#26159;&#20174;Android&#21407;&#22987;&#20840;&#29699;&#23548;&#33322;&#21355;&#26143;&#31995;&#32479;&#65288;GNSS&#65289;&#27979;&#37327;&#20013;&#24471;&#20986;&#30340;&#19982;&#21355;&#26143;-&#25509;&#25910;&#22120;-&#29615;&#22659;&#30456;&#20851;&#30340;&#20845;&#20010;&#29305;&#24449;&#12290;&#20026;&#20102;&#30417;&#30563;&#35757;&#32451;&#36807;&#31243;&#65292;&#25105;&#20204;&#20180;&#32454;&#35745;&#31639;&#20266;&#36317;&#20559;&#24046;&#30340;&#30446;&#26631;&#20540;&#65292;&#24182;&#21033;&#29992;&#22320;&#29702;&#30495;&#23454;&#20301;&#32622;&#21644;&#24179;&#28369;&#25216;&#26415;&#20248;&#21270;&#20102;&#19968;&#20010;&#21253;&#21547;&#26234;&#33021;&#25163;&#26426;&#26102;&#38047;&#20559;&#24046;&#20272;&#35745;&#27531;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#23450;&#20301;&#24341;&#25806;&#26657;&#27491;&#31070;&#32463;&#32593;&#32476;&#30340;&#20266;&#36317;&#35745;&#31639;&#20301;&#32622;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#28151;&#21512;&#31649;&#36947;&#21487;&#20197;&#22788;&#29702;&#20266;&#36317;&#20559;&#24046;&#21644;&#22122;&#22768;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#65292;&#24182;&#32771;&#34385;&#20102;&#22235;&#20010;&#24212;&#29992;&#22330;&#26223;&#26469;&#30740;&#31350;&#25351;&#32441;&#23450;&#20301;&#21644;&#20132;&#21449;&#36319;&#36394;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a neural network for mitigating pseudorange bias to improve localization performance with data collected from Android smartphones. We represent pseudorange bias using a pragmatic satellite-wise Multiple Layer Perceptron (MLP), the inputs of which are six satellite-receiver-context-related features derived from Android raw Global Navigation Satellite System (GNSS) measurements. To supervise the training process, we carefully calculate the target values of pseudorange bias using location ground truth and smoothing techniques and optimize a loss function containing the estimation residuals of smartphone clock bias. During the inference process, we employ model-based localization engines to compute locations with pseudoranges corrected by the neural network. Consequently, this hybrid pipeline can attend to both pseudorange bias and noise. We evaluate the framework on an open dataset and consider four application scenarios for investigating fingerprinting and cross-trace localiza
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#33041;&#30005;&#20449;&#21495;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.12202</link><description>&lt;p&gt;
&#25552;&#21319;&#31934;&#20934;&#21307;&#23398;&#65306;&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#65306;2002-2023&#24180;&#23436;&#25972;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Empowering Precision Medicine: AI-Driven Schizophrenia Diagnosis via EEG Signals: A Comprehensive Review from 2002-2023. (arXiv:2309.12202v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#33041;&#30005;&#20449;&#21495;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#31070;&#20998;&#35010;&#30151;&#65288;SZ&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31934;&#31070;&#38556;&#30861;&#65292;&#20854;&#29305;&#24449;&#20026;&#35748;&#30693;&#12289;&#24773;&#32490;&#21644;&#34892;&#20026;&#25913;&#21464;&#12290;SZ&#30340;&#30151;&#29366;&#21253;&#25324;&#24187;&#35273;&#12289;&#38169;&#35273;&#12289;&#22916;&#24819;&#12289;&#32570;&#20047;&#21160;&#21147;&#21644;&#27880;&#24847;&#21147;&#38590;&#24230;&#12290;&#35786;&#26029;SZ&#38656;&#35201;&#20351;&#29992;&#21508;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20020;&#24202;&#35775;&#35848;&#12289;&#20307;&#26684;&#26816;&#26597;&#12289;&#24515;&#29702;&#35780;&#20272;&#12289;&#12298;&#31934;&#31070;&#38556;&#30861;&#35786;&#26029;&#19982;&#32479;&#35745;&#25163;&#20876;&#12299;&#65288;DSM&#65289;&#21644;&#31070;&#32463;&#24433;&#20687;&#25216;&#26415;&#12290;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35760;&#24405;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#21151;&#33021;&#24615;&#31070;&#32463;&#24433;&#20687;&#27169;&#24577;&#65292;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;SZ&#26399;&#38388;&#33041;&#21151;&#33021;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#33041;&#30005;&#20449;&#21495;&#20998;&#26512;&#23545;&#31070;&#32463;&#23398;&#23478;&#21644;&#31185;&#23398;&#23478;&#26469;&#35828;&#23384;&#22312;&#33402;&#26415;&#21697;&#12289;&#38271;&#26399;&#35760;&#24405;&#21644;&#22810;&#36890;&#36947;&#20351;&#29992;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#65292;&#20197;&#36741;&#21161;SZ&#35786;&#26029;&#12290;&#26412;&#30740;&#31350;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Schizophrenia (SZ) is a prevalent mental disorder characterized by cognitive, emotional, and behavioral changes. Symptoms of SZ include hallucinations, illusions, delusions, lack of motivation, and difficulties in concentration. Diagnosing SZ involves employing various tools, including clinical interviews, physical examinations, psychological evaluations, the Diagnostic and Statistical Manual of Mental Disorders (DSM), and neuroimaging techniques. Electroencephalography (EEG) recording is a significant functional neuroimaging modality that provides valuable insights into brain function during SZ. However, EEG signal analysis poses challenges for neurologists and scientists due to the presence of artifacts, long-term recordings, and the utilization of multiple channels. To address these challenges, researchers have introduced artificial intelligence (AI) techniques, encompassing conventional machine learning (ML) and deep learning (DL) methods, to aid in SZ diagnosis. This study reviews
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#21644;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#23618;&#23545;&#33041;&#30005;&#22270;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#20943;&#23569;EEG&#20449;&#21495;&#30340;&#20887;&#20313;&#65292;&#24182;&#22312;&#20445;&#25345;&#31232;&#30095;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12201</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#23618;&#30340;&#38750;&#23545;&#31216;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#23545;&#33041;&#30005;&#22270;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Electroencephalogram Sensor Data Compression Using An Asymmetrical Sparse Autoencoder With A Discrete Cosine Transform Layer. (arXiv:2309.12201v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#21644;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#23618;&#23545;&#33041;&#30005;&#22270;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#20943;&#23569;EEG&#20449;&#21495;&#30340;&#20887;&#20313;&#65292;&#24182;&#22312;&#20445;&#25345;&#31232;&#30095;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#21387;&#32553;&#23545;&#20110;&#26080;&#32447;&#35760;&#24405;&#24212;&#29992;&#26469;&#35828;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#20943;&#23569;&#38656;&#35201;&#20256;&#36755;&#30340;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#65288;DCT&#65289;&#23618;&#30340;&#38750;&#23545;&#31216;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#26469;&#21387;&#32553;EEG&#20449;&#21495;&#30340;&#26041;&#27861;&#12290;&#33258;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#22120;&#27169;&#22359;&#37319;&#29992;&#20840;&#36830;&#25509;&#32447;&#24615;&#23618;&#21644;DCT&#23618;&#30340;&#32452;&#21512;&#65292;&#20351;&#29992;&#30828;&#38408;&#20540;&#38750;&#32447;&#24615;&#38477;&#20302;&#20887;&#20313;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;DCT&#23618;&#21253;&#25324;&#21487;&#35757;&#32451;&#30340;&#30828;&#38408;&#20540;&#21442;&#25968;&#21644;&#32553;&#25918;&#23618;&#65292;&#21487;&#24378;&#35843;&#25110;&#20943;&#24369;&#21333;&#20010;DCT&#31995;&#25968;&#12290;&#26368;&#21518;&#65292;&#19968;&#23545;&#19968;&#21367;&#31215;&#23618;&#29983;&#25104;&#28508;&#31354;&#38388;&#12290;&#22312;&#28508;&#31354;&#38388;&#20013;&#65292;&#37319;&#29992;&#31232;&#30095;&#24809;&#32602;&#22411;&#25104;&#26412;&#20989;&#25968;&#20351;&#29305;&#24449;&#22270;&#23613;&#21487;&#33021;&#31232;&#30095;&#12290;&#28508;&#31354;&#38388;&#25968;&#25454;&#34987;&#20256;&#36755;&#21040;&#25509;&#25910;&#31471;&#12290;&#33258;&#32534;&#30721;&#22120;&#30340;&#35299;&#30721;&#22120;&#27169;&#22359;&#20351;&#29992;&#36870;DCT&#21644;&#20004;&#20010;&#20840;&#36830;&#25509;&#32447;&#24615;&#23618;&#26469;&#25552;&#39640;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalogram (EEG) data compression is necessary for wireless recording applications to reduce the amount of data that needs to be transmitted. In this paper, an asymmetrical sparse autoencoder with a discrete cosine transform (DCT) layer is proposed to compress EEG signals. The encoder module of the autoencoder has a combination of a fully connected linear layer and the DCT layer to reduce redundant data using hard-thresholding nonlinearity. Furthermore, the DCT layer includes trainable hard-thresholding parameters and scaling layers to give emphasis or de-emphasis on individual DCT coefficients. Finally, the one-by-one convolutional layer generates the latent space. The sparsity penalty-based cost function is employed to keep the feature map as sparse as possible in the latent space. The latent space data is transmitted to the receiver. The decoder module of the autoencoder is designed using the inverse DCT and two fully connected linear layers to improve the accuracy of data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#23460;&#20869;&#23450;&#20301;&#26041;&#26696;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#20540;&#24182;&#23558;&#22810;&#39057;&#27573;&#20449;&#24687;&#25340;&#25509;&#22312;&#19968;&#36215;&#65292;&#25552;&#39640;&#20102;&#23460;&#20869;&#25351;&#32441;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12200</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#23460;&#20869;&#23450;&#20301;&#30340;&#22810;&#39057;&#27573;&#20449;&#36947;&#39044;&#27979;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Variational Auto-Encoder Enabled Multi-Band Channel Prediction Scheme for Indoor Localization. (arXiv:2309.12200v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#23460;&#20869;&#23450;&#20301;&#26041;&#26696;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#20540;&#24182;&#23558;&#22810;&#39057;&#27573;&#20449;&#24687;&#25340;&#25509;&#22312;&#19968;&#36215;&#65292;&#25552;&#39640;&#20102;&#23460;&#20869;&#25351;&#32441;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#23450;&#20301;&#22312;&#34394;&#25311;/&#22686;&#24378;&#29616;&#23454;&#21644;&#26234;&#33021;&#23478;&#23621;&#31561;&#21069;&#27839;&#25216;&#26415;&#20013;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#22823;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#23450;&#20301;&#22312;&#35745;&#31639;&#36127;&#25285;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38382;&#39064;&#65292;&#25152;&#20197;&#25351;&#32441;&#23450;&#20301;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#23427;&#38656;&#35201;&#22312;&#24314;&#31435;&#25351;&#32441;&#25968;&#25454;&#24211;&#21518;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#23460;&#20869;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#21463;&#21040;&#22797;&#26434;&#23460;&#20869;&#29615;&#22659;&#24102;&#26469;&#30340;&#22810;&#24452;&#20449;&#21495;&#25240;&#23556;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#26696;&#65292;&#36890;&#36807;&#20174;&#21478;&#19968;&#20010;&#21457;&#36865;&#36890;&#36947;&#39044;&#27979;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#20540;&#65292;&#24182;&#23558;&#22810;&#39057;&#27573;&#20449;&#24687;&#25340;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#25552;&#39640;&#23460;&#20869;&#25351;&#32441;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;COST 2100&#27169;&#25311;&#25968;&#25454;&#21644;&#20174;&#21150;&#20844;&#22330;&#26223;&#25910;&#38598;&#30340;&#23454;&#26102;&#27491;&#20132;&#39057;&#20998;&#22797;&#29992;&#65288;OFDM&#65289;WiFi&#25968;&#25454;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indoor localization is getting increasing demands for various cutting-edged technologies, like Virtual/Augmented reality and smart home. Traditional model-based localization suffers from significant computational overhead, so fingerprint localization is getting increasing attention, which needs lower computation cost after the fingerprint database is built. However, the accuracy of indoor localization is limited by the complicated indoor environment which brings the multipath signal refraction. In this paper, we provided a scheme to improve the accuracy of indoor fingerprint localization from the frequency domain by predicting the channel state information (CSI) values from another transmitting channel and spliced the multi-band information together to get more precise localization results. We tested our proposed scheme on COST 2100 simulation data and real time orthogonal frequency division multiplexing (OFDM) WiFi data collected from an office scenario.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;ResNet50&#65292;&#26469;&#25913;&#36827;&#33041;&#32959;&#30244;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#20197;&#25506;&#32034;&#33258;&#21160;&#21270;&#26816;&#27979;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12193</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#33041;&#32959;&#30244;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Brain Tumor Detection Using Deep Learning Approaches. (arXiv:2309.12193v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;ResNet50&#65292;&#26469;&#25913;&#36827;&#33041;&#32959;&#30244;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#20197;&#25506;&#32034;&#33258;&#21160;&#21270;&#26816;&#27979;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32959;&#30244;&#26159;&#19968;&#20123;&#24322;&#24120;&#32454;&#32990;&#30340;&#32858;&#38598;&#20307;&#65292;&#21487;&#20197;&#21457;&#23637;&#25104;&#32959;&#22359;&#25110;&#22242;&#31751;&#12290;&#30001;&#20110;&#23427;&#20204;&#26377;&#28508;&#22312;&#30340;&#28024;&#28070;&#20854;&#20182;&#32452;&#32455;&#30340;&#21487;&#33021;&#24615;&#65292;&#23545;&#24739;&#32773;&#26500;&#25104;&#39118;&#38505;&#12290;&#20027;&#35201;&#30340;&#25104;&#20687;&#25216;&#26415;MRI&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#33041;&#32959;&#30244;&#12290;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#26500;&#24314;&#30340;&#25913;&#36827;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#24471;&#20197;&#24555;&#36895;&#21457;&#23637;&#65292;&#20026;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#36817;&#20284;&#12290;&#23545;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#38656;&#27714;&#26159;&#36825;&#20010;&#25193;&#23637;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#21033;&#29992;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25913;&#36827;&#33041;&#32959;&#30244;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;ResNet50&#65292;&#36827;&#34892;&#33041;&#32959;&#30244;&#35782;&#21035;&#30340;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#21270;&#26816;&#27979;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain tumors are collections of abnormal cells that can develop into masses or clusters. Because they have the potential to infiltrate other tissues, they pose a risk to the patient. The main imaging technique used, MRI, may be able to identify a brain tumor with accuracy. The fast development of Deep Learning methods for use in computer vision applications has been facilitated by a vast amount of training data and improvements in model construction that offer better approximations in a supervised setting. The need for these approaches has been the main driver of this expansion. Deep learning methods have shown promise in improving the precision of brain tumor detection and classification using magnetic resonance imaging (MRI). The study on the use of deep learning techniques, especially ResNet50, for brain tumor identification is presented in this abstract. As a result, this study investigates the possibility of automating the detection procedure using deep learning techniques. In thi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#36827;&#34892;&#26465;&#20214;&#25512;&#26029;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20351;&#29992;&#26368;&#21518;&#19968;&#25209;&#32467;&#26524;&#36827;&#34892;&#25512;&#26029;&#26159;&#26368;&#20248;&#30340;&#65307;&#24403;&#23454;&#39564;&#30340;&#33258;&#36866;&#24212;&#26041;&#38754;&#26159;&#20301;&#32622;&#19981;&#21464;&#30340;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#65307;&#22312;&#20572;&#27490;&#26102;&#38388;&#12289;&#20998;&#37197;&#27010;&#29575;&#21644;&#30446;&#26631;&#21442;&#25968;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#22810;&#38754;&#20307;&#20107;&#20214;&#38598;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#35745;&#31639;&#21487;&#34892;&#19988;&#26368;&#20248;&#30340;&#26465;&#20214;&#25512;&#26029;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2309.12162</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#30340;&#26368;&#20248;&#26465;&#20214;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Optimal Conditional Inference in Adaptive Experiments. (arXiv:2309.12162v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12162
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#36827;&#34892;&#26465;&#20214;&#25512;&#26029;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20351;&#29992;&#26368;&#21518;&#19968;&#25209;&#32467;&#26524;&#36827;&#34892;&#25512;&#26029;&#26159;&#26368;&#20248;&#30340;&#65307;&#24403;&#23454;&#39564;&#30340;&#33258;&#36866;&#24212;&#26041;&#38754;&#26159;&#20301;&#32622;&#19981;&#21464;&#30340;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#65307;&#22312;&#20572;&#27490;&#26102;&#38388;&#12289;&#20998;&#37197;&#27010;&#29575;&#21644;&#30446;&#26631;&#21442;&#25968;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#22810;&#38754;&#20307;&#20107;&#20214;&#38598;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#35745;&#31639;&#21487;&#34892;&#19988;&#26368;&#20248;&#30340;&#26465;&#20214;&#25512;&#26029;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25209;&#37327;&#36172;&#24466;&#23454;&#39564;&#65292;&#24182;&#32771;&#34385;&#20102;&#22312;&#23454;&#29616;&#20572;&#27490;&#26102;&#38388;&#12289;&#20998;&#37197;&#27010;&#29575;&#21644;&#30446;&#26631;&#21442;&#25968;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#25512;&#26029;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#25152;&#26377;&#36825;&#20123;&#21487;&#33021;&#37117;&#26159;&#26681;&#25454;&#23454;&#39564;&#30340;&#26368;&#21518;&#19968;&#25209;&#20449;&#24687;&#36827;&#34892;&#33258;&#36866;&#24212;&#36873;&#25321;&#30340;&#12290;&#22312;&#27809;&#26377;&#23545;&#23454;&#39564;&#36827;&#34892;&#36827;&#19968;&#27493;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20165;&#20351;&#29992;&#26368;&#21518;&#19968;&#25209;&#32467;&#26524;&#36827;&#34892;&#25512;&#26029;&#26159;&#26368;&#20248;&#30340;&#12290;&#24403;&#23454;&#39564;&#30340;&#33258;&#36866;&#24212;&#26041;&#38754;&#34987;&#35748;&#20026;&#26159;&#20301;&#32622;&#19981;&#21464;&#30340;&#65292;&#21363;&#24403;&#25105;&#20204;&#23558;&#25152;&#26377;&#25209;&#27425;-&#33218;&#30340;&#24179;&#22343;&#20540;&#37117;&#21521;&#19968;&#20010;&#24120;&#25968;&#31227;&#21160;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#25968;&#25454;&#20013;&#36824;&#23384;&#22312;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#39069;&#22806;&#30340;&#25209;&#27425;-&#33218;&#22343;&#20540;&#30340;&#32447;&#24615;&#20989;&#25968;&#26469;&#25429;&#25417;&#12290;&#22312;&#26356;&#20005;&#26684;&#30340;&#24773;&#20917;&#19979;&#65292;&#20572;&#27490;&#26102;&#38388;&#12289;&#20998;&#37197;&#27010;&#29575;&#21644;&#30446;&#26631;&#21442;&#25968;&#34987;&#35748;&#20026;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#36890;&#36807;&#19968;&#20010;&#22810;&#38754;&#20307;&#20107;&#20214;&#30340;&#38598;&#21512;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#35745;&#31639;&#21487;&#34892;&#19988;&#26368;&#20248;&#30340;&#26465;&#20214;&#25512;&#26029;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study batched bandit experiments and consider the problem of inference conditional on the realized stopping time, assignment probabilities, and target parameter, where all of these may be chosen adaptively using information up to the last batch of the experiment. Absent further restrictions on the experiment, we show that inference using only the results of the last batch is optimal. When the adaptive aspects of the experiment are known to be location-invariant, in the sense that they are unchanged when we shift all batch-arm means by a constant, we show that there is additional information in the data, captured by one additional linear function of the batch-arm means. In the more restrictive case where the stopping time, assignment probabilities, and target parameter are known to depend on the data only through a collection of polyhedral events, we derive computationally tractable and optimal conditional inference procedures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#30340;&#24403;&#21069;&#21457;&#23637;&#24773;&#20917;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#40065;&#26834;&#24615;&#21644;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.12158</link><description>&lt;p&gt;
&#36808;&#21521;&#40065;&#26834;&#21644;&#30495;&#27491;&#22823;&#35268;&#27169;&#30340;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval. (arXiv:2309.12158v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#30340;&#24403;&#21069;&#21457;&#23637;&#24773;&#20917;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#40065;&#26834;&#24615;&#21644;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#30340;&#19968;&#31995;&#21015;&#24212;&#29992;&#38598;&#20013;&#22312;&#35299;&#20915;&#23558;&#22823;&#37327;&#20048;&#35889;&#22270;&#20687;&#19982;&#30456;&#24212;&#30340;&#38899;&#39057;&#35760;&#24405;&#36830;&#25509;&#36215;&#26469;&#30340;&#38382;&#39064;&#65292;&#21363;&#35782;&#21035;&#24341;&#29992;&#30456;&#21516;&#38899;&#20048;&#20869;&#23481;&#30340;&#38899;&#39057;&#21644;&#20048;&#35889;&#25688;&#24405;&#12290;&#26368;&#36817;&#30340;&#19968;&#31181;&#20856;&#22411;&#26041;&#27861;&#26159;&#20351;&#29992;&#36328;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26469;&#23398;&#20064;&#23558;&#20004;&#31181;&#19981;&#21516;&#27169;&#24577;&#65288;&#38899;&#39057;&#21644;&#20048;&#35889;&#22270;&#20687;&#65289;&#36830;&#25509;&#36215;&#26469;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#12290;&#23613;&#31649;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#20102;&#31283;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#26377;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#38459;&#30861;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;&#24403;&#21069;&#22312;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#26041;&#38754;&#30340;&#21457;&#23637;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#23454;&#29616;&#40065;&#26834;&#21644;&#22823;&#35268;&#27169;&#36328;&#27169;&#24577;&#38899;&#20048;&#26816;&#32034;&#30340;&#19968;&#31995;&#21015;&#20027;&#35201;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#36804;&#20170;&#20026;&#27490;&#25105;&#20204;&#24050;&#32463;&#37319;&#21462;&#30340;&#19968;&#20123;&#27493;&#39588;&#26469;&#35299;&#20915;&#20854;&#20013;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
A range of applications of multi-modal music information retrieval is centred around the problem of connecting large collections of sheet music (images) to corresponding audio recordings, that is, identifying pairs of audio and score excerpts that refer to the same musical content. One of the typical and most recent approaches to this task employs cross-modal deep learning architectures to learn joint embedding spaces that link the two distinct modalities - audio and sheet music images. While there has been steady improvement on this front over the past years, a number of open problems still prevent large-scale employment of this methodology. In this article we attempt to provide an insightful examination of the current developments on audio-sheet music retrieval via deep learning methods. We first identify a set of main challenges on the road towards robust and large-scale cross-modal music retrieval in real scenarios. We then highlight the steps we have taken so far to address some o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#30340;&#37325;&#22797;&#36941;&#21382;&#26469;&#36866;&#24212;&#29289;&#20307;&#26816;&#27979;&#22120;&#21040;&#26032;&#30340;&#39550;&#39542;&#29615;&#22659;&#12290;&#36890;&#36807;&#32467;&#21512;&#37325;&#22797;&#30340;&#28608;&#20809;&#38647;&#36798;&#25195;&#25551;&#35745;&#31639;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#24341;&#23548;&#33258;&#36866;&#24212;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#22238;&#24402;&#22836;&#21644;&#33258;&#35757;&#32451;&#36807;&#31243;&#26469;&#22686;&#24378;&#26816;&#27979;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#22810;&#36798;20&#20010;&#30334;&#20998;&#28857;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.12140</link><description>&lt;p&gt;
&#22522;&#20110;&#36807;&#21435;&#36941;&#21382;&#29305;&#24449;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features. (arXiv:2309.12140v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#30340;&#37325;&#22797;&#36941;&#21382;&#26469;&#36866;&#24212;&#29289;&#20307;&#26816;&#27979;&#22120;&#21040;&#26032;&#30340;&#39550;&#39542;&#29615;&#22659;&#12290;&#36890;&#36807;&#32467;&#21512;&#37325;&#22797;&#30340;&#28608;&#20809;&#38647;&#36798;&#25195;&#25551;&#35745;&#31639;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#24341;&#23548;&#33258;&#36866;&#24212;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#22238;&#24402;&#22836;&#21644;&#33258;&#35757;&#32451;&#36807;&#31243;&#26469;&#22686;&#24378;&#26816;&#27979;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#22810;&#36798;20&#20010;&#30334;&#20998;&#28857;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22914;&#20170;&#65292;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#30340;&#24555;&#36895;&#21457;&#23637;&#26497;&#22823;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#27867;&#21270;&#21040;&#22810;&#26679;&#21270;&#30340;&#39550;&#39542;&#29615;&#22659;&#20013;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#26816;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#26102;&#21457;&#29983;&#23433;&#20840;&#20851;&#38190;&#24615;&#30340;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#30340;&#22810;&#20010;&#20301;&#32622;&#37325;&#22797;&#36941;&#21382;&#26469;&#36866;&#24212;&#29289;&#20307;&#26816;&#27979;&#22120;&#21040;&#26032;&#30340;&#39550;&#39542;&#29615;&#22659;&#12290;&#36890;&#36807;&#32467;&#21512;&#37325;&#22797;&#30340;&#28608;&#20809;&#38647;&#36798;&#25195;&#25551;&#35745;&#31639;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#24341;&#23548;&#33258;&#36866;&#24212;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#37327;&#21270;&#30340;&#21382;&#21490;&#29305;&#24449;&#26469;&#22686;&#24378;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22238;&#24402;&#22836;&#26469;&#21033;&#29992;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#29305;&#24449;&#27491;&#21017;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#35757;&#32451;&#36807;&#31243;&#26469;&#31283;&#23450;&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#20219;&#20309;&#26816;&#27979;&#22120;&#27169;&#22411;&#65292;&#24182;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21462;&#24471;&#20102;&#22810;&#36798;20&#20010;&#30334;&#20998;&#28857;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of 3D object detection systems for self-driving cars has significantly improved accuracy. However, these systems struggle to generalize across diverse driving environments, which can lead to safety-critical failures in detecting traffic participants. To address this, we propose a method that utilizes unlabeled repeated traversals of multiple locations to adapt object detectors to new driving environments. By incorporating statistics computed from repeated LiDAR scans, we guide the adaptation process effectively. Our approach enhances LiDAR-based detection models using spatial quantized historical features and introduces a lightweight regression head to leverage the statistics for feature regularization. Additionally, we leverage the statistics for a novel self-training process to stabilize the training. The framework is detector model-agnostic and experiments on real-world datasets demonstrate significant improvements, achieving up to a 20-point performance gain, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#36830;&#25509;&#29702;&#35770;&#21644;&#23454;&#36341;&#65292;&#25552;&#20379;&#20102;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#36870;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#21644;&#24674;&#22797;&#24615;&#33021;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24471;&#20986;&#20102;&#23545;&#20110;&#20004;&#23618;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#36870;&#20808;&#39564;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#21270;&#30028;&#38480;&#65292;&#35813;&#32593;&#32476;&#23558;&#20174;&#25105;&#20204;&#30340;&#20445;&#35777;&#20013;&#21463;&#30410;&#12290;</title><link>http://arxiv.org/abs/2309.12128</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#22312;&#36870;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#21644;&#24674;&#22797;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems. (arXiv:2309.12128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#36830;&#25509;&#29702;&#35770;&#21644;&#23454;&#36341;&#65292;&#25552;&#20379;&#20102;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#36870;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#21644;&#24674;&#22797;&#24615;&#33021;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24471;&#20986;&#20102;&#23545;&#20110;&#20004;&#23618;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#36870;&#20808;&#39564;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#21270;&#30028;&#38480;&#65292;&#35813;&#32593;&#32476;&#23558;&#20174;&#25105;&#20204;&#30340;&#20445;&#35777;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#36825;&#26679;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#32463;&#39564;&#24615;&#22320;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#26126;&#30830;&#29702;&#35770;&#20445;&#35777;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#36890;&#36807;&#36807;&#21442;&#25968;&#21270;&#26469;&#25511;&#21046;&#31070;&#32463;&#20999;&#21521;&#26680;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#19979;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#22914;&#20309;&#36830;&#25509;&#36825;&#20004;&#20010;&#39046;&#22495;&#65292;&#24182;&#20026;&#26080;&#30417;&#30563;&#21069;&#39304;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#35757;&#32451;&#36807;&#31243;&#25552;&#20379;&#30830;&#23450;&#24615;&#30340;&#25910;&#25947;&#21644;&#24674;&#22797;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#36229;&#21442;&#25968;&#21270;&#30028;&#38480;&#65292;&#22312;&#36825;&#20123;&#30028;&#38480;&#19979;&#65292;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#28145;&#24230;&#36870;&#20808;&#39564;&#32593;&#32476;&#23558;&#21463;&#30410;&#20110;&#25105;&#20204;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have become a prominent approach to solve inverse problems in recent years. While a plethora of such methods was developed to solve inverse problems empirically, we are still lacking clear theoretical guarantees for these methods. On the other hand, many works proved convergence to optimal solutions of neural networks in a more general setting using overparametrization as a way to control the Neural Tangent Kernel. In this work we investigate how to bridge these two worlds and we provide deterministic convergence and recovery guarantees for the class of unsupervised feedforward multilayer neural networks trained to solve inverse problems. We also derive overparametrization bounds under which a two-layers Deep Inverse Prior network with smooth activation function will benefit from our guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#24490;&#29615;&#27169;&#22411;&#30340;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36328;&#27169;&#24577;&#24490;&#29615;&#32593;&#32476;&#29983;&#25104;&#21487;&#20197;&#25688;&#35201;&#23545;&#24212;&#38899;&#39057;&#21644;&#20048;&#35889;&#30340;&#26356;&#38271;&#27573;&#33853;&#30340;&#32852;&#21512;&#23884;&#20837;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#24369;&#23545;&#40784;&#30340;&#38899;&#39057;-&#20048;&#35889;&#23545;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38899;&#39057;&#21644;&#20048;&#35889;&#20043;&#38388;&#30340;&#33410;&#22863;&#21464;&#21270;&#23548;&#33268;&#30340;&#38750;&#32447;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12111</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;&#27169;&#22411;&#30340;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#30340;&#27573;&#33853;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval. (arXiv:2309.12111v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#24490;&#29615;&#27169;&#22411;&#30340;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36328;&#27169;&#24577;&#24490;&#29615;&#32593;&#32476;&#29983;&#25104;&#21487;&#20197;&#25688;&#35201;&#23545;&#24212;&#38899;&#39057;&#21644;&#20048;&#35889;&#30340;&#26356;&#38271;&#27573;&#33853;&#30340;&#32852;&#21512;&#23884;&#20837;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#24369;&#23545;&#40784;&#30340;&#38899;&#39057;-&#20048;&#35889;&#23545;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38899;&#39057;&#21644;&#20048;&#35889;&#20043;&#38388;&#30340;&#33410;&#22863;&#21464;&#21270;&#23548;&#33268;&#30340;&#38750;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#38899;&#20048;&#26816;&#32034;&#30340;&#35768;&#22810;&#24212;&#29992;&#19982;&#23558;&#20048;&#35889;&#22270;&#20687;&#19982;&#38899;&#39057;&#24405;&#38899;&#36830;&#25509;&#22312;&#19968;&#36215;&#26377;&#20851;&#12290;&#30446;&#21069;&#30340;&#20856;&#22411;&#26041;&#27861;&#26159;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19968;&#20010;&#20851;&#32852;&#30701;&#22266;&#23450;&#22823;&#23567;&#30340;&#38899;&#39057;&#21644;&#20048;&#35889;&#29255;&#27573;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#30456;&#20284;&#24615;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#24102;&#26469;&#30340;&#20004;&#20010;&#25361;&#25112;&#26159;&#35757;&#32451;&#32593;&#32476;&#38656;&#35201;&#24378;&#23545;&#40784;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#30001;&#20110;&#23616;&#37096;&#21644;&#20840;&#23616;&#36895;&#24230;&#24046;&#24322;&#32780;&#36896;&#25104;&#30340;&#38899;&#39057;&#21644;&#20048;&#35889;&#29255;&#27573;&#20043;&#38388;&#30340;&#38899;&#20048;&#20869;&#23481;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#36328;&#27169;&#24577;&#24490;&#29615;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#32570;&#28857;&#65292;&#35813;&#32593;&#32476;&#23398;&#20064;&#21487;&#20197;&#25688;&#35201;&#23545;&#24212;&#38899;&#39057;&#21644;&#20048;&#35889;&#30340;&#26356;&#38271;&#27573;&#33853;&#30340;&#32852;&#21512;&#23884;&#20837;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#22909;&#22788;&#26159;&#23427;&#21482;&#38656;&#35201;&#24369;&#23545;&#40784;&#30340;&#38899;&#39057;-&#20048;&#35889;&#23545;&#65292;&#20197;&#21450;&#24490;&#29615;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#38899;&#39057;&#21644;&#20048;&#35889;&#20043;&#38388;&#30340;&#33410;&#22863;&#21464;&#21270;&#23548;&#33268;&#30340;&#38750;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications of cross-modal music retrieval are related to connecting sheet music images to audio recordings. A typical and recent approach to this is to learn, via deep neural networks, a joint embedding space that correlates short fixed-size snippets of audio and sheet music by means of an appropriate similarity structure. However, two challenges that arise out of this strategy are the requirement of strongly aligned data to train the networks, and the inherent discrepancies of musical content between audio and sheet music snippets caused by local and global tempo differences. In this paper, we address these two shortcomings by designing a cross-modal recurrent network that learns joint embeddings that can summarize longer passages of corresponding audio and sheet music. The benefits of our method are that it only requires weakly aligned audio-sheet music pairs, as well as that the recurrent network handles the non-linearities caused by tempo variations between audio and sheet m
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#32553;&#20943;&#20316;&#20026;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#20462;&#21098;&#27169;&#22411;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.12095</link><description>&lt;p&gt;
&#20855;&#26377;&#36125;&#21494;&#26031;&#27169;&#22411;&#32553;&#20943;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian sparsification for deep neural networks with Bayesian model reduction. (arXiv:2309.12095v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12095
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#32553;&#20943;&#20316;&#20026;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#20462;&#21098;&#27169;&#22411;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#24040;&#22823;&#33021;&#21147;&#24120;&#24120;&#21463;&#21040;&#20854;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#23545;&#20110;&#26377;&#25928;&#30340;&#31232;&#30095;&#25216;&#26415;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#36125;&#21494;&#26031;&#31232;&#30095;&#24615;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#32780;&#35328;&#26159;&#19968;&#31181;&#20851;&#38190;&#26041;&#27861;&#65292;&#21487;&#20197;&#20419;&#36827;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#35774;&#35745;&#26082;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21448;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#30446;&#21069;&#65292;&#36125;&#21494;&#26031;&#31232;&#30095;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#25216;&#26415;&#26159;&#23558;&#32467;&#26500;&#25910;&#32553;&#20808;&#39564;&#24212;&#29992;&#20110;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#40657;&#30418;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#30340;&#36817;&#20284;&#25512;&#26029;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#19982;&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#28857;&#20272;&#35745;&#30456;&#27604;&#65292;&#23436;&#25972;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#21453;&#28436;&#22312;&#35745;&#31639;&#26041;&#38754;&#38750;&#24120;&#32791;&#36153;&#26102;&#38388;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20513;&#20351;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#32553;&#20943;&#65288;BMR&#65289;&#20316;&#20026;&#27169;&#22411;&#26435;&#37325;&#20462;&#21098;&#30340;&#26356;&#39640;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#20316;&#20026;&#20915;&#31574;&#29575;&#30340;&#25512;&#24191;&#65292;BMR&#20801;&#35768;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#20107;&#21518;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Deep learning's immense capabilities are often constrained by the complexity of its models, leading to an increasing demand for effective sparsification techniques. Bayesian sparsification for deep learning emerges as a crucial approach, facilitating the design of models that are both computationally efficient and competitive in terms of performance across various deep learning applications. The state-of-the-art -- in Bayesian sparsification of deep neural networks -- combines structural shrinkage priors on model weights with an approximate inference scheme based on black-box stochastic variational inference. However, model inversion of the full generative model is exceptionally computationally demanding, especially when compared to standard deep learning of point estimates. In this context, we advocate for the use of Bayesian model reduction (BMR) as a more efficient alternative for pruning of model weights. As a generalization of the Savage-Dickey ratio, BMR allows a post-hoc elimina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#25552;&#20379;&#20219;&#21153;&#26356;&#25913;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#32858;&#31867;&#26041;&#27861;&#26377;&#25928;&#22320;&#23545;&#25239;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2309.12078</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Clustering-based Domain-Incremental Learning. (arXiv:2309.12078v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#25552;&#20379;&#20219;&#21153;&#26356;&#25913;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#32858;&#31867;&#26041;&#27861;&#26377;&#25928;&#22320;&#23545;&#25239;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#20013;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;&#30340;&#25968;&#25454;&#20197;&#27969;&#24335;&#21576;&#29616;&#32473;&#23398;&#20064;&#22120;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#25152;&#35859;&#30340;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#8221;&#65292;&#21363;&#22312;&#8220;&#26087;&#20219;&#21153;&#8221;&#19978;&#36827;&#34892;&#21518;&#32493;&#35757;&#32451;&#26102;&#23398;&#20064;&#22120;&#22312;&#8220;&#26032;&#20219;&#21153;&#8221;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#24179;&#22343;&#26799;&#24230;&#24773;&#33410;&#24615;&#35760;&#24518;&#65288;A-GEM&#65289;&#21644;&#27491;&#20132;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#24403;&#21069;&#20219;&#21153;&#30340;&#25439;&#22833;&#32780;&#19981;&#22686;&#21152;&#20808;&#21069;&#20219;&#21153;&#30340;&#25439;&#22833;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#23398;&#20064;&#22120;&#30693;&#36947;&#20219;&#21153;&#20309;&#26102;&#25913;&#21464;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#21160;&#24577;&#26356;&#26032;&#30340;&#26377;&#38480;&#26679;&#26412;&#25110;&#26799;&#24230;&#27744;&#20013;&#20351;&#29992;&#22312;&#32447;&#32858;&#31867;&#26041;&#27861;&#26469;&#28040;&#38500;&#20026;&#31639;&#27861;&#25552;&#20379;&#26377;&#20851;&#20219;&#21153;&#26356;&#25913;&#30340;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;&#20174;&#32780;&#25104;&#21151;&#22320;&#23545;&#25239;&#20102;&#20854;&#20013;&#19968;&#20010;&#22256;&#38590;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning multiple tasks in a continual learning setting in which data from different tasks is presented to the learner in a streaming fashion. A key challenge in this setting is the so-called "catastrophic forgetting problem", in which the performance of the learner in an "old task" decreases when subsequently trained on a "new task". Existing continual learning methods, such as Averaged Gradient Episodic Memory (A-GEM) and Orthogonal Gradient Descent (OGD), address catastrophic forgetting by minimizing the loss for the current task without increasing the loss for previous tasks. However, these methods assume the learner knows when the task changes, which is unrealistic in practice. In this paper, we alleviate the need to provide the algorithm with information about task changes by using an online clustering-based approach on a dynamically updated finite pool of samples or gradients. We thereby successfully counteract catastrophic forgetting in one of the har
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#36275;&#29699;&#20013;&#21160;&#20316;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#26102;&#31354;&#23450;&#20301;&#30340;&#32508;&#36848;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#25972;&#21512;&#20102;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#24182;&#20197;&#22810;&#31181;&#26041;&#24335;&#34920;&#31034;&#19968;&#31181;&#26469;&#28304;&#65292;&#22312;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12067</link><description>&lt;p&gt;
&#36275;&#29699;&#20013;&#21160;&#20316;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#26102;&#31354;&#23450;&#20301;&#30340;&#35843;&#26597;--&#24403;&#21069;&#36235;&#21183;&#21644;&#30740;&#31350;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer -- Current Trends and Research Perspectives. (arXiv:2309.12067v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#36275;&#29699;&#20013;&#21160;&#20316;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#26102;&#31354;&#23450;&#20301;&#30340;&#32508;&#36848;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#25972;&#21512;&#20102;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#24182;&#20197;&#22810;&#31181;&#26041;&#24335;&#34920;&#31034;&#19968;&#31181;&#26469;&#28304;&#65292;&#22312;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36275;&#29699;&#27604;&#36187;&#30340;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#24615;&#20197;&#21450;&#29699;&#21592;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#23545;&#36275;&#29699;&#20013;&#30340;&#21160;&#20316;&#22330;&#26223;&#36827;&#34892;&#29702;&#35299;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#23558;&#27492;&#20219;&#21153;&#20998;&#20026;&#21160;&#20316;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#26102;&#31354;&#21160;&#20316;&#23450;&#20301;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#25152;&#20351;&#29992;&#30340;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#28304;&#21644;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#20256;&#32479;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#26041;&#27861;&#12290;&#25105;&#20204;&#20851;&#27880;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#65288;&#22914;&#35270;&#39057;&#21644;&#38899;&#39057;&#25968;&#25454;&#65289;&#30340;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#20197;&#21450;&#20197;&#22810;&#31181;&#26041;&#24335;&#34920;&#31034;&#19968;&#31181;&#26469;&#28304;&#30340;&#26041;&#27861;&#12290;&#35752;&#35770;&#20102;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#25913;&#36827;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20123;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action scene understanding in soccer is a challenging task due to the complex and dynamic nature of the game, as well as the interactions between players. This article provides a comprehensive overview of this task divided into action recognition, spotting, and spatio-temporal action localization, with a particular emphasis on the modalities used and multimodal methods. We explore the publicly available data sources and metrics used to evaluate models' performance. The article reviews recent state-of-the-art methods that leverage deep learning techniques and traditional methods. We focus on multimodal methods, which integrate information from multiple sources, such as video and audio data, and also those that represent one source in various ways. The advantages and limitations of methods are discussed, along with their potential for improving the accuracy and robustness of models. Finally, the article highlights some of the open research questions and future directions in the field of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#39640;&#25928;&#25239;&#30284;&#32957;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;Word2Vec&#21644;FastText&#20316;&#20026;&#35789;&#23884;&#20837;&#25216;&#26415;&#65292;&#20197;&#21450;CNN&#12289;LSTM&#12289;BiLSTM&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12058</link><description>&lt;p&gt;
&#19968;&#20010;&#39640;&#25928;&#25972;&#21512;&#35789;&#23884;&#20837;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#25239;&#30284;&#32957;&#20998;&#31867;&#26041;&#27861;&#65306;FastText+BiLSTM
&lt;/p&gt;
&lt;p&gt;
An Efficient Consolidation of Word Embedding and Deep Learning Techniques for Classifying Anticancer Peptides: FastText+BiLSTM. (arXiv:2309.12058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#39640;&#25928;&#25239;&#30284;&#32957;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;Word2Vec&#21644;FastText&#20316;&#20026;&#35789;&#23884;&#20837;&#25216;&#26415;&#65292;&#20197;&#21450;CNN&#12289;LSTM&#12289;BiLSTM&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#30284;&#32957;&#65288;ACP&#65289;&#26159;&#19968;&#31867;&#20855;&#22791;&#25239;&#32959;&#30244;&#29305;&#24615;&#30340;&#32957;&#12290;&#20351;&#29992;ACP&#22312;&#30284;&#30151;&#39044;&#38450;&#20013;&#21487;&#20197;&#20316;&#20026;&#20256;&#32479;&#30284;&#30151;&#27835;&#30103;&#30340;&#26367;&#20195;&#21697;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#26356;&#39640;&#30340;&#36873;&#25321;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#26368;&#36817;&#31185;&#23398;&#30340;&#36827;&#23637;&#24341;&#36215;&#20102;&#23545;&#22522;&#20110;&#32957;&#30340;&#27835;&#30103;&#30340;&#20852;&#36259;&#65292;&#23427;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#27835;&#30103;&#30446;&#26631;&#32454;&#32990;&#32780;&#19981;&#23545;&#27491;&#24120;&#32454;&#32990;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32957;&#24207;&#21015;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#24320;&#21457;&#21487;&#38752;&#21644;&#31934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#35789;&#23884;&#20837;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#25552;&#20986;&#19968;&#20010;&#39640;&#25928;&#30340;&#25239;&#30284;&#32957;&#20998;&#31867;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#35780;&#20272;&#20102;Word2Vec&#21644;FastText&#20316;&#20026;&#25552;&#21462;&#32957;&#24207;&#21015;&#30340;&#35789;&#23884;&#20837;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#23558;&#35789;&#23884;&#20837;&#27169;&#22411;&#30340;&#36755;&#20986;&#36755;&#20837;&#21040;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CNN&#12289;LSTM&#12289;BiLSTM&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticancer peptides (ACPs) are a group of peptides that exhibite antineoplastic properties. The utilization of ACPs in cancer prevention can present a viable substitute for conventional cancer therapeutics, as they possess a higher degree of selectivity and safety. Recent scientific advancements generate an interest in peptide-based therapies which offer the advantage of efficiently treating intended cells without negatively impacting normal cells. However, as the number of peptide sequences continues to increase rapidly, developing a reliable and precise prediction model becomes a challenging task. In this work, our motivation is to advance an efficient model for categorizing anticancer peptides employing the consolidation of word embedding and deep learning models. First, Word2Vec and FastText are evaluated as word embedding techniques for the purpose of extracting peptide sequences. Then, the output of word embedding models are fed into deep learning approaches CNN, LSTM, BiLSTM. To
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#25552;&#21319;&#24314;&#27169;&#19982;&#32463;&#20856;&#39044;&#27979;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#30740;&#31350;&#36129;&#29486;&#21253;&#25324;&#26032;&#30340;&#21033;&#28070;&#24230;&#37327;&#20844;&#24335;&#65292;&#25910;&#25947;&#24615;&#35777;&#26126;&#20197;&#21450;&#26465;&#20214;&#30340;&#27169;&#25311;&#35828;&#26126;&#12290;</title><link>http://arxiv.org/abs/2309.12036</link><description>&lt;p&gt;
&#25552;&#20379;&#20030;&#25253;&#19982;&#39044;&#27979;&#27169;&#22411;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Uplift vs. predictive modeling: a theoretical analysis. (arXiv:2309.12036v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#25552;&#21319;&#24314;&#27169;&#19982;&#32463;&#20856;&#39044;&#27979;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#30740;&#31350;&#36129;&#29486;&#21253;&#25324;&#26032;&#30340;&#21033;&#28070;&#24230;&#37327;&#20844;&#24335;&#65292;&#25910;&#25947;&#24615;&#35777;&#26126;&#20197;&#21450;&#26465;&#20214;&#30340;&#27169;&#25311;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20915;&#31574;&#21046;&#23450;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#22240;&#26524;&#23450;&#21521;&#31574;&#30053;&#30456;&#23545;&#20110;&#32431;&#31929;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22686;&#20540;&#24456;&#23569;&#22312;&#25991;&#29486;&#20013;&#36827;&#34892;&#37327;&#21270;&#12290;&#36825;&#20123;&#31574;&#30053;&#23545;&#20110;&#33829;&#38144;&#12289;&#30005;&#20449;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#20174;&#19994;&#32773;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20174;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#24320;&#22987;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#24433;&#21709;&#25552;&#21319;&#21644;&#39044;&#27979;&#26041;&#27861;&#24615;&#33021;&#30340;&#21442;&#25968;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#20108;&#20803;&#32467;&#26524;&#21644;&#20108;&#20803;&#25805;&#20316;&#30340;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#27604;&#36739;&#20102;&#25552;&#21319;&#24314;&#27169;&#19982;&#32463;&#20856;&#39044;&#27979;&#26041;&#27861;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30740;&#31350;&#36129;&#29486;&#21253;&#25324;&#23545;&#21033;&#28070;&#24230;&#37327;&#30340;&#26032;&#24418;&#24335;&#21270;&#20844;&#24335;&#65292;&#25552;&#21319;&#26354;&#32447;&#25910;&#25947;&#21040;&#21033;&#28070;&#24230;&#37327;&#30340;&#24418;&#24335;&#21270;&#35777;&#26126;&#65292;&#20197;&#21450;&#36890;&#36807;&#27169;&#25311;&#23545;&#26465;&#20214;&#36827;&#34892;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the growing popularity of machine-learning techniques in decision-making, the added value of causal-oriented strategies with respect to pure machine-learning approaches has rarely been quantified in the literature. These strategies are crucial for practitioners in various domains, such as marketing, telecommunications, health care and finance. This paper presents a comprehensive treatment of the subject, starting from firm theoretical foundations and highlighting the parameters that influence the performance of the uplift and predictive approaches. The focus of the paper is on a binary outcome case and a binary action, and the paper presents a theoretical analysis of uplift modeling, comparing it with the classical predictive approach. The main research contributions of the paper include a new formulation of the measure of profit, a formal proof of the convergence of the uplift curve to the measure of profit ,and an illustration, through simulations, of the conditions under whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;PluGeN4Faces&#25554;&#20214;&#65292;&#21033;&#29992;&#23545;&#27604;&#25439;&#22833;&#26469;&#26126;&#30830;&#35299;&#32544;&#33080;&#37096;&#23646;&#24615;&#19982;&#20154;&#30340;&#36523;&#20221;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38754;&#37096;&#22270;&#20687;&#20462;&#25913;&#20013;&#23545;&#36523;&#20221;&#20914;&#31361;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.12033</link><description>&lt;p&gt;
StyleGAN&#20013;&#30340;&#33080;&#37096;&#36523;&#20221;&#24863;&#30693;&#35299;&#32544;&#35770;
&lt;/p&gt;
&lt;p&gt;
Face Identity-Aware Disentanglement in StyleGAN. (arXiv:2309.12033v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;PluGeN4Faces&#25554;&#20214;&#65292;&#21033;&#29992;&#23545;&#27604;&#25439;&#22833;&#26469;&#26126;&#30830;&#35299;&#32544;&#33080;&#37096;&#23646;&#24615;&#19982;&#20154;&#30340;&#36523;&#20221;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38754;&#37096;&#22270;&#20687;&#20462;&#25913;&#20013;&#23545;&#36523;&#20221;&#20914;&#31361;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#32463;&#24120;&#29992;&#20110;&#25805;&#32437;&#38754;&#37096;&#22270;&#20687;&#30340;&#23646;&#24615;&#65292;&#22914;&#34920;&#24773;&#12289;&#21457;&#22411;&#12289;&#23039;&#21183;&#25110;&#24180;&#40836;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#20462;&#25913;&#20102;&#35831;&#27714;&#30340;&#23646;&#24615;&#65292;&#20294;&#21516;&#26102;&#20063;&#20462;&#25913;&#20102;&#22270;&#20687;&#30340;&#20854;&#20182;&#37325;&#35201;&#29305;&#24449;&#65292;&#22914;&#19968;&#20010;&#20154;&#30340;&#36523;&#20221;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#36890;&#36807;&#24341;&#20837;PluGeN4Faces&#65288;&#19968;&#31181;StyleGAN&#25554;&#20214;&#65289;&#65292;&#20174;&#20154;&#30340;&#36523;&#20221;&#20013;&#26126;&#30830;&#35299;&#32544;&#38754;&#37096;&#23646;&#24615;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23545;&#20174;&#30005;&#24433;&#24103;&#20013;&#26816;&#32034;&#21040;&#30340;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#22270;&#20687;&#20013;&#30340;&#21516;&#19968;&#20154;&#20197;&#19981;&#21516;&#30340;&#23039;&#21183;&#21644;&#23646;&#24615;&#20986;&#29616;&#12290;&#36890;&#36807;&#24212;&#29992;&#23545;&#27604;&#25439;&#22833;&#30340;&#19968;&#31181;&#31867;&#22411;&#65292;&#25105;&#20204;&#40723;&#21169;&#27169;&#22411;&#23558;&#21516;&#19968;&#20154;&#30340;&#22270;&#20687;&#20998;&#32452;&#21040;&#28508;&#22312;&#31354;&#38388;&#30340;&#30456;&#20284;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PluGeN4Faces&#25152;&#25191;&#34892;&#30340;&#38754;&#37096;&#23646;&#24615;&#20462;&#25913;&#23545;&#22270;&#20687;&#30340;&#20854;&#20182;&#29305;&#24449;&#30340;&#20405;&#20837;&#24615;&#26126;&#26174;&#23567;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional GANs are frequently used for manipulating the attributes of face images, such as expression, hairstyle, pose, or age. Even though the state-of-the-art models successfully modify the requested attributes, they simultaneously modify other important characteristics of the image, such as a person's identity. In this paper, we focus on solving this problem by introducing PluGeN4Faces, a plugin to StyleGAN, which explicitly disentangles face attributes from a person's identity. Our key idea is to perform training on images retrieved from movie frames, where a given person appears in various poses and with different attributes. By applying a type of contrastive loss, we encourage the model to group images of the same person in similar regions of latent space. Our experiments demonstrate that the modifications of face attributes performed by PluGeN4Faces are significantly less invasive on the remaining characteristics of the image than in the existing state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21327;&#21516;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#25353;&#29031;&#22522;&#20110;&#35780;&#20998;&#20989;&#25968;&#30340;&#20449;&#24565;&#20998;&#24067;&#37319;&#26679;&#31062;&#20808;&#22270;&#65292;&#24182;&#24341;&#20837;&#26368;&#20339;&#23454;&#39564;&#35774;&#35745;&#19982;&#19987;&#23478;&#20114;&#21160;&#65292;&#20197;&#25552;&#20379;&#19987;&#23478;&#21487;&#39564;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36845;&#20195;&#25913;&#36827;&#22240;&#26524;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2309.12032</link><description>&lt;p&gt;
&#20154;&#26426;&#21327;&#21516;&#19979;&#20351;&#29992;&#31062;&#20808;GFlowNets&#36827;&#34892;&#28508;&#22312;&#28151;&#28102;&#30340;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets. (arXiv:2309.12032v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21327;&#21516;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#25353;&#29031;&#22522;&#20110;&#35780;&#20998;&#20989;&#25968;&#30340;&#20449;&#24565;&#20998;&#24067;&#37319;&#26679;&#31062;&#20808;&#22270;&#65292;&#24182;&#24341;&#20837;&#26368;&#20339;&#23454;&#39564;&#35774;&#35745;&#19982;&#19987;&#23478;&#20114;&#21160;&#65292;&#20197;&#25552;&#20379;&#19987;&#23478;&#21487;&#39564;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36845;&#20195;&#25913;&#36827;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#23398;&#20064;&#26159;&#22240;&#26524;&#25512;&#26029;&#30340;&#20851;&#38190;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#25968;&#25454;&#31232;&#32570;&#26102;&#65292;&#22240;&#26524;&#21457;&#29616;&#65288;CD&#65289;&#31639;&#27861;&#24456;&#33030;&#24369;&#65292;&#21487;&#33021;&#25512;&#26029;&#20986;&#19982;&#19987;&#23478;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#19981;&#20934;&#30830;&#22240;&#26524;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#26102;&#26356;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#21152;&#37325;&#36825;&#20010;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;CD&#26041;&#27861;&#24182;&#19981;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#20351;&#24471;&#29992;&#25143;&#38590;&#20197;&#35299;&#37322;&#32467;&#26524;&#21644;&#25913;&#36827;&#25512;&#26029;&#36807;&#31243;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;CD&#26159;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20107;&#21153;&#65292;&#20294;&#27809;&#26377;&#20219;&#20309;&#30740;&#31350;&#19987;&#27880;&#20110;&#26500;&#24314;&#26082;&#33021;&#36755;&#20986;&#19987;&#23478;&#21487;&#39564;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21448;&#33021;&#19982;&#19987;&#23478;&#36827;&#34892;&#20132;&#20114;&#36845;&#20195;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#65292;&#26681;&#25454;&#22522;&#20110;&#35780;&#20998;&#20989;&#25968;&#65288;&#22914;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#65289;&#30340;&#20449;&#24565;&#20998;&#24067;&#65292;&#25353;&#27604;&#20363;&#23545;&#65288;&#22240;&#26524;&#65289;&#31062;&#20808;&#22270;&#36827;&#34892;&#37319;&#26679;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20505;&#36873;&#22270;&#30340;&#22810;&#26679;&#24615;&#24182;&#24341;&#20837;&#26368;&#20339;&#23454;&#39564;&#35774;&#35745;&#65292;&#20197;&#36845;&#20195;&#24615;&#22320;&#25506;&#32034;&#23454;&#39564;&#26469;&#19982;&#19987;&#23478;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure learning is the crux of causal inference. Notably, causal discovery (CD) algorithms are brittle when data is scarce, possibly inferring imprecise causal relations that contradict expert knowledge -- especially when considering latent confounders. To aggravate the issue, most CD methods do not provide uncertainty estimates, making it hard for users to interpret results and improve the inference process. Surprisingly, while CD is a human-centered affair, no works have focused on building methods that both 1) output uncertainty estimates that can be verified by experts and 2) interact with those experts to iteratively refine CD. To solve these issues, we start by proposing to sample (causal) ancestral graphs proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC), using generative flow networks. Then, we leverage the diversity in candidate graphs and introduce an optimal experimental design to iteratively probe the expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;(DyHSL)&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#24314;&#27169;&#22797;&#26434;&#30340;&#20132;&#36890;&#32593;&#32476;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#39640;&#38454;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.12028</link><description>&lt;p&gt;
&#21160;&#24577;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting. (arXiv:2309.12028v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;(DyHSL)&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#24314;&#27169;&#22797;&#26434;&#30340;&#20132;&#36890;&#32593;&#32476;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#39640;&#38454;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#38382;&#39064;&#65292;&#26088;&#22312;&#22522;&#20110;&#36807;&#21435;&#30340;&#36947;&#36335;&#32593;&#32476;&#21644;&#20132;&#36890;&#29366;&#20917;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#20132;&#36890;&#26465;&#20214;&#12290;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#24314;&#27169;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#20196;&#20154;&#28385;&#24847;&#65292;&#22240;&#20026;&#24403;&#28041;&#21450;&#21040;&#22797;&#26434;&#30340;&#20132;&#36890;&#32593;&#32476;&#26102;&#65292;GNNs&#30340;&#34920;&#31034;&#33021;&#21147;&#36890;&#24120;&#26377;&#38480;&#12290;&#22270;&#24418;&#26412;&#36136;&#19978;&#26080;&#27861;&#25429;&#25417;&#38750;&#37197;&#23545;&#20851;&#31995;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#36981;&#24490;&#20449;&#24687;&#20256;&#36882;&#30340;&#33539;&#24335;&#65292;&#32447;&#24615;&#22320;&#32858;&#21512;&#37051;&#22495;&#20449;&#24687;&#65292;&#26080;&#27861;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#39640;&#38454;&#20132;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;(DyHSL)&#30340;&#26032;&#27169;&#22411;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#12290;&#20026;&#20102;&#23398;&#20064;&#38750;&#37197;&#23545;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;DyHSL&#25552;&#21462;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of traffic flow forecasting, which aims to predict future traffic conditions on the basis of road networks and traffic conditions in the past. The problem is typically solved by modeling complex spatio-temporal correlations in traffic data using spatio-temporal graph neural networks (GNNs). However, the performance of these methods is still far from satisfactory since GNNs usually have limited representation capacity when it comes to complex traffic networks. Graphs, by nature, fall short in capturing non-pairwise relations. Even worse, existing methods follow the paradigm of message passing that aggregates neighborhood information linearly, which fails to capture complicated spatio-temporal high-order interactions. To tackle these issues, in this paper, we propose a novel model named Dynamic Hypergraph Structure Learning (DyHSL) for traffic flow prediction. To learn non-pairwise relationships, our DyHSL extracts hypergraph structural information to model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#24102;&#26377;&#32972;&#21253;&#32422;&#26463;&#30340;&#38750;&#21333;&#35843;k-&#27425;&#27169;&#23376;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#20004;&#20010;&#40065;&#26834;&#36924;&#36817;&#31639;&#27861;&#65292;&#22823;&#22823;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#65292;&#24182;&#20998;&#21035;&#25552;&#20379;&#20102;$1/19$&#21644;$1/5-\epsilon$&#30340;&#36817;&#20284;&#27604;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.12025</link><description>&lt;p&gt;
&#38024;&#23545;&#24102;&#26377;&#32972;&#21253;&#32422;&#26463;&#30340;&#38750;&#21333;&#35843;k-&#27425;&#27169;&#23376;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#40065;&#26834;&#36924;&#36817;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Approximation Algorithms for Non-monotone $k$-Submodular Maximization under a Knapsack Constraint. (arXiv:2309.12025v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#24102;&#26377;&#32972;&#21253;&#32422;&#26463;&#30340;&#38750;&#21333;&#35843;k-&#27425;&#27169;&#23376;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#20004;&#20010;&#40065;&#26834;&#36924;&#36817;&#31639;&#27861;&#65292;&#22823;&#22823;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#65292;&#24182;&#20998;&#21035;&#25552;&#20379;&#20102;$1/19$&#21644;$1/5-\epsilon$&#30340;&#36817;&#20284;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#25968;&#25454;&#27719;&#24635;&#12289;&#20449;&#24687;&#20256;&#25773;&#31561;&#65292;&#38750;&#21333;&#35843;k-&#27425;&#27169;&#23376;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#38382;&#39064;&#65288;$\kSMK$&#65289;&#22312;&#24213;&#23618;&#38598;&#21512;&#22823;&#23567;$n$&#19978;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#38754;&#20020;&#22914;&#20309;&#20811;&#26381;&#38750;&#21333;&#35843;&#24773;&#20917;&#20197;&#21450;&#22312;&#25968;&#25454;&#22823;&#23567;&#22823;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#24555;&#36895;&#36820;&#22238;&#19968;&#20010;&#22909;&#35299;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#20004;&#20010;&#30830;&#23450;&#24615;&#36924;&#36817;&#31639;&#27861;&#65292;&#31454;&#20105;&#24615;&#22320;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#31639;&#27861;$\LAA$&#22312;$O(nk)$&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#20869;&#36820;&#22238;&#20102;$1/19$&#30340;&#36817;&#20284;&#27604;&#12290;&#31532;&#20108;&#20010;&#31639;&#27861;$\RLA$&#22312;$O(nk)$&#30340;&#26597;&#35810;&#20013;&#25913;&#36827;&#20102;&#36817;&#20284;&#27604;&#29575;&#21040;$1/5-\epsilon$&#65292;&#20854;&#20013;$\epsilon$&#26159;&#19968;&#20010;&#36755;&#20837;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#22312;&#38750;&#21333;&#35843;&#30446;&#26631;&#20989;&#25968;&#20013;&#20165;&#20351;&#29992;$O(nk)$&#26597;&#35810;&#22797;&#26434;&#24230;&#25552;&#20379;&#24658;&#23450;&#36817;&#20284;&#27604;&#29575;&#30340;&#31639;&#27861;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#38656;&#35201;&#26356;&#23569;&#30340;&#26597;&#35810;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of non-monotone $k$-submodular maximization under a knapsack constraint ($\kSMK$) over the ground set size $n$ has been raised in many applications in machine learning, such as data summarization, information propagation, etc. However, existing algorithms for the problem are facing questioning of how to overcome the non-monotone case and how to fast return a good solution in case of the big size of data. This paper introduces two deterministic approximation algorithms for the problem that competitively improve the query complexity of existing algorithms.  Our first algorithm, $\LAA$, returns an approximation ratio of $1/19$ within $O(nk)$ query complexity. The second one, $\RLA$, improves the approximation ratio to $1/5-\epsilon$ in $O(nk)$ queries, where $\epsilon$ is an input parameter.  Our algorithms are the first ones that provide constant approximation ratios within only $O(nk)$ query complexity for the non-monotone objective. They, therefore, need fewer the number of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31435;&#26041;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#30340;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#20248;&#20808;&#32423;&#25490;&#24207;&#21644;&#33021;&#32791;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#23433;&#20840;&#19988;&#23481;&#38169;&#30340;&#31995;&#32479;&#65292;&#24182;&#22312;&#22810;&#20010;&#31435;&#26041;&#21355;&#26143;&#37197;&#32622;&#19979;&#20248;&#20110;MADDPG&#27169;&#22411;&#21644;&#20256;&#32479;&#38543;&#26426;&#35843;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.12004</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#32791;&#30340;&#31435;&#26041;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#30340;&#23433;&#20840;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Hierarchical Reinforcement Learning for CubeSat Task Scheduling Based on Energy Consumption. (arXiv:2309.12004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31435;&#26041;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#30340;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#20248;&#20808;&#32423;&#25490;&#24207;&#21644;&#33021;&#32791;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#23433;&#20840;&#19988;&#23481;&#38169;&#30340;&#31995;&#32479;&#65292;&#24182;&#22312;&#22810;&#20010;&#31435;&#26041;&#21355;&#26143;&#37197;&#32622;&#19979;&#20248;&#20110;MADDPG&#27169;&#22411;&#21644;&#20256;&#32479;&#38543;&#26426;&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#20013;&#31435;&#26041;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#36827;&#34892;&#20248;&#21270;&#30340;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#29992;&#20110;&#20840;&#23616;&#20219;&#21153;&#20998;&#37197;&#30340;&#39640;&#32423;&#31574;&#30053;&#21644;&#29992;&#20110;&#23454;&#26102;&#35843;&#25972;&#30340;&#20302;&#32423;&#31574;&#30053;&#20316;&#20026;&#23433;&#20840;&#26426;&#21046;&#65292;&#25972;&#21512;&#20102;&#22522;&#20110;&#30456;&#20284;&#24230;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#65288;SABE&#65289;&#36827;&#34892;&#20219;&#21153;&#20248;&#20808;&#32423;&#25490;&#24207;&#20197;&#21450;&#29992;&#20110;&#33021;&#32791;&#39044;&#27979;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20272;&#35745;&#22120;&#12290;&#35813;&#26426;&#21046;&#30340;&#25972;&#21512;&#20026;&#31435;&#26041;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#21019;&#24314;&#20102;&#19968;&#20010;&#23433;&#20840;&#19988;&#23481;&#38169;&#30340;&#31995;&#32479;&#12290;&#20223;&#30495;&#32467;&#26524;&#35777;&#26126;&#20102;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#20248;&#20110;MADDPG&#27169;&#22411;&#21644;&#20256;&#32479;&#38543;&#26426;&#35843;&#24230;&#22312;&#22810;&#20010;&#31435;&#26041;&#21355;&#26143;&#37197;&#32622;&#19979;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a Hierarchical Reinforcement Learning methodology tailored for optimizing CubeSat task scheduling in Low Earth Orbits (LEO). Incorporating a high-level policy for global task distribution and a low-level policy for real-time adaptations as a safety mechanism, our approach integrates the Similarity Attention-based Encoder (SABE) for task prioritization and an MLP estimator for energy consumption forecasting. Integrating this mechanism creates a safe and fault-tolerant system for CubeSat task scheduling. Simulation results validate the Hierarchical Reinforcement Learning superior convergence and task success rate, outperforming both the MADDPG model and traditional random scheduling across multiple CubeSat configurations.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#33016;&#37096;X&#23556;&#32447;&#29255;&#19978;&#35782;&#21035;&#32954;&#28814;&#30340;&#36719;&#20214;&#65292;&#20855;&#26377;98&#65285;&#30340;&#25935;&#24863;&#24615;&#21644;97.3&#65285;&#30340;&#29305;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11995</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22312;&#33016;&#37096;X&#20809;&#22270;&#20687;&#19978;&#35782;&#21035;&#32954;&#28814;
&lt;/p&gt;
&lt;p&gt;
Identification of pneumonia on chest x-ray images through machine learning. (arXiv:2309.11995v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#33016;&#37096;X&#23556;&#32447;&#29255;&#19978;&#35782;&#21035;&#32954;&#28814;&#30340;&#36719;&#20214;&#65292;&#20855;&#26377;98&#65285;&#30340;&#25935;&#24863;&#24615;&#21644;97.3&#65285;&#30340;&#29305;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#28814;&#26159;&#20840;&#29699;&#23156;&#20799;&#27515;&#20129;&#30340;&#20027;&#35201;&#24863;&#26579;&#21407;&#22240;&#12290;&#26089;&#26399;&#35782;&#21035;&#21487;&#20197;&#25913;&#21464;&#24739;&#32773;&#30340;&#39044;&#21518;&#65292;&#21487;&#20197;&#21033;&#29992;&#25104;&#20687;&#26816;&#26597;&#26469;&#24110;&#21161;&#35786;&#26029;&#30830;&#35748;&#12290;&#23613;&#26089;&#25191;&#34892;&#21644;&#35299;&#35835;&#26816;&#26597;&#23545;&#20110;&#33391;&#22909;&#30340;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#65292;&#23545;&#20110;&#36825;&#31181;&#30149;&#29702;&#23398;&#65292;&#26368;&#24120;&#35265;&#30340;&#26816;&#26597;&#26159;&#33016;&#37096;X&#32447;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#22312;&#33016;&#37096;X&#23556;&#32447;&#29255;&#20013;&#35782;&#21035;&#32954;&#28814;&#26159;&#21542;&#23384;&#22312;&#30340;&#36719;&#20214;&#12290;&#35813;&#36719;&#20214;&#22522;&#20110;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#35745;&#31639;&#27169;&#22411;&#24320;&#21457;&#32780;&#25104;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20174;&#20013;&#22269;&#30340;&#19968;&#23478;&#21307;&#38498;&#25910;&#38598;&#20102;&#20799;&#31461;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#22312;&#32447;&#25968;&#25454;&#24211;&#12290;&#32463;&#36807;&#35757;&#32451;&#21518;&#65292;&#35813;&#27169;&#22411;&#26292;&#38706;&#20110;&#26032;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;&#35782;&#21035;&#35813;&#30149;&#29702;&#23398;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#20851;&#32467;&#26524;&#65292;&#22312;&#29992;&#20110;&#27979;&#35797;&#30340;&#26679;&#26412;&#20013;&#36798;&#21040;&#20102;98&#65285;&#30340;&#25935;&#24863;&#24615;&#21644;97.3&#65285;&#30340;&#29305;&#24322;&#24615;&#12290;&#21487;&#20197;&#24471;&#20986;&#32467;&#35770;&#65292;&#24320;&#21457;&#33021;&#22815;&#22312;&#33016;&#37096;X&#23556;&#32447;&#29255;&#19978;&#35782;&#21035;&#32954;&#28814;&#30340;&#36719;&#20214;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pneumonia is the leading infectious cause of infant death in the world. When identified early, it is possible to alter the prognosis of the patient, one could use imaging exams to help in the diagnostic confirmation. Performing and interpreting the exams as soon as possible is vital for a good treatment, with the most common exam for this pathology being chest X-ray. The objective of this study was to develop a software that identify the presence or absence of pneumonia in chest radiographs. The software was developed as a computational model based on machine learning using transfer learning technique. For the training process, images were collected from a database available online with children's chest X-rays images taken at a hospital in China. After training, the model was then exposed to new images, achieving relevant results on identifying such pathology, reaching 98% sensitivity and 97.3% specificity for the sample used for testing. It can be concluded that it is possible to deve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#35780;&#20272;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#25552;&#39640;&#20195;&#29702;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;SAEAs&#65289;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#35782;&#21035;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#30452;&#25509;&#29983;&#25104;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.11994</link><description>&lt;p&gt;
&#29992;&#26410;&#35780;&#20272;&#30340;&#35299;&#20915;&#26041;&#26696;&#22686;&#24378;SAEAs&#65306;&#26114;&#36149;&#20248;&#21270;&#38382;&#39064;&#30340;&#20851;&#31995;&#27169;&#22411;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing SAEAs with Unevaluated Solutions: A Case Study of Relation Model for Expensive Optimization. (arXiv:2309.11994v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#35780;&#20272;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#25552;&#39640;&#20195;&#29702;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;SAEAs&#65289;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#35782;&#21035;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#30452;&#25509;&#29983;&#25104;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20195;&#29702;&#30340;&#36827;&#21270;&#31639;&#27861;&#65288;SAEAs&#65289;&#22312;&#35299;&#20915;&#26114;&#36149;&#20248;&#21270;&#38382;&#39064;&#65288;EOPs&#65289;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#24320;&#21457;&#39640;&#25928;&#30340;&#27169;&#22411;&#36741;&#21161;&#36873;&#25321;&#26041;&#27861;&#65292;&#20154;&#20204;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#25552;&#39640;SAEAs&#30340;&#25928;&#33021;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#36873;&#25321;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#22312;SAEAs&#30340;&#27599;&#19968;&#20195;&#20013;&#20165;&#35780;&#20272;&#26377;&#38480;&#25968;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#22522;&#26412;&#33539;&#24335;&#20943;&#23569;&#20102;&#30456;&#37051;&#31181;&#32676;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#21518;&#20195;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;&#36825;&#26159;&#19968;&#20010;&#32463;&#24120;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#20294;&#36824;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#35780;&#20272;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#25552;&#39640;SAEAs&#30340;&#25928;&#29575;&#12290;&#20195;&#29702;&#27169;&#22411;&#34987;&#29992;&#26469;&#35782;&#21035;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#30452;&#25509;&#29983;&#25104;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35780;&#20272;&#12290;&#20026;&#20102;&#30830;&#20445;&#21487;&#38752;&#30340;&#36873;&#25321;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#23450;&#21046;&#30340;&#20851;&#31995;&#27169;&#22411;&#65292;&#29992;&#20110;&#36873;&#25321;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#35780;&#20272;&#31181;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate-assisted evolutionary algorithms (SAEAs) hold significant importance in resolving expensive optimization problems~(EOPs). Extensive efforts have been devoted to improving the efficacy of SAEAs through the development of proficient model-assisted selection methods. However, generating high-quality solutions is a prerequisite for selection. The fundamental paradigm of evaluating a limited number of solutions in each generation within SAEAs reduces the variance of adjacent populations, thus impacting the quality of offspring solutions. This is a frequently encountered issue, yet it has not gained widespread attention. This paper presents a framework using unevaluated solutions to enhance the efficiency of SAEAs. The surrogate model is employed to identify high-quality solutions for direct generation of new solutions without evaluation. To ensure dependable selection, we have introduced two tailored relation models for the selection of the optimal solution and the unevaluated pop
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#21518;&#32622;&#35299;&#37322;&#26041;&#27861;&#20013;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#12290;&#21457;&#29616;&#24403;&#35299;&#37322;&#38598;&#20013;&#22312;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#26679;&#26412;&#26102;&#65292;SHAP&#30340;&#21487;&#29702;&#35299;&#24615;&#26174;&#33879;&#38477;&#20302;&#12290;&#21478;&#22806;&#65292;&#21457;&#29616;&#21453;&#20107;&#23454;&#35299;&#37322;&#21644;&#38169;&#35823;&#20998;&#31867;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29992;&#25143;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#22686;&#24378;&#21518;&#32622;&#35299;&#37322;&#26041;&#27861;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#30340;&#35774;&#35745;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.11987</link><description>&lt;p&gt;
&#21518;&#32622;&#35299;&#37322;&#26041;&#27861;&#20013;&#30340;&#21487;&#39044;&#27979;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65306;&#19968;&#39033;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Predictability and Comprehensibility in Post-Hoc XAI Methods: A User-Centered Analysis. (arXiv:2309.11987v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11987
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#21518;&#32622;&#35299;&#37322;&#26041;&#27861;&#20013;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#12290;&#21457;&#29616;&#24403;&#35299;&#37322;&#38598;&#20013;&#22312;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#26679;&#26412;&#26102;&#65292;SHAP&#30340;&#21487;&#29702;&#35299;&#24615;&#26174;&#33879;&#38477;&#20302;&#12290;&#21478;&#22806;&#65292;&#21457;&#29616;&#21453;&#20107;&#23454;&#35299;&#37322;&#21644;&#38169;&#35823;&#20998;&#31867;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29992;&#25143;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#22686;&#24378;&#21518;&#32622;&#35299;&#37322;&#26041;&#27861;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#30340;&#35774;&#35745;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#32622;&#35299;&#37322;&#26041;&#27861;&#26088;&#22312;&#28548;&#28165;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#23545;&#25552;&#20379;&#30340;&#35299;&#37322;&#26377;&#22810;&#22909;&#29702;&#35299;&#20197;&#21450;&#36825;&#20123;&#35299;&#37322;&#26159;&#21542;&#22686;&#24378;&#20102;&#29992;&#25143;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#39044;&#27979;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#65288;LIME&#21644;SHAP&#65289;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#21644;&#38169;&#35823;&#20998;&#31867;&#23545;&#29992;&#25143;&#29702;&#35299;&#21644;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#20026;&#25509;&#36817;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#25552;&#20379;&#35299;&#37322;&#26102;&#65292;SHAP&#30340;&#21487;&#29702;&#35299;&#24615;&#26174;&#33879;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#21453;&#20107;&#23454;&#35299;&#37322;&#21644;&#38169;&#35823;&#20998;&#31867;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#29992;&#25143;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#21518;&#32622;&#35299;&#37322;&#26041;&#27861;&#30340;&#35774;&#35745;&#24314;&#35758;&#65292;&#20197;&#22686;&#24378;&#20854;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-hoc explainability methods aim to clarify predictions of black-box machine learning models. However, it is still largely unclear how well users comprehend the provided explanations and whether these increase the users ability to predict the model behavior. We approach this question by conducting a user study to evaluate comprehensibility and predictability in two widely used tools: LIME and SHAP. Moreover, we investigate the effect of counterfactual explanations and misclassifications on users ability to understand and predict the model behavior. We find that the comprehensibility of SHAP is significantly reduced when explanations are provided for samples near a model's decision boundary. Furthermore, we find that counterfactual explanations and misclassifications can significantly increase the users understanding of how a machine learning model is making decisions. Based on our findings, we also derive design recommendations for future post-hoc explainability methods with increas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#19982;&#21464;&#20998;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#29256;&#26412;&#30340;&#26032;&#22411;&#21464;&#20998;CTC&#65292;&#29992;&#20110;&#35757;&#32451;&#26356;&#20855;&#26222;&#36866;&#24615;&#30340;&#20445;&#24207;&#24207;&#21015;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20248;&#21270;&#27169;&#22411;&#23545;&#25968;&#20284;&#28982;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#24182;&#35299;&#20915;&#20102;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.11983</link><description>&lt;p&gt;
&#21464;&#20998;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#29992;&#20110;&#20445;&#24207;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling. (arXiv:2309.11983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#19982;&#21464;&#20998;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#29256;&#26412;&#30340;&#26032;&#22411;&#21464;&#20998;CTC&#65292;&#29992;&#20110;&#35757;&#32451;&#26356;&#20855;&#26222;&#36866;&#24615;&#30340;&#20445;&#24207;&#24207;&#21015;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20248;&#21270;&#27169;&#22411;&#23545;&#25968;&#20284;&#28982;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#24182;&#35299;&#20915;&#20102;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#24120;&#34987;&#29992;&#20110;&#20445;&#24207;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#65292;&#27604;&#22914;&#35821;&#38899;&#35782;&#21035;&#65292;&#20854;&#20013;&#20445;&#25345;&#36755;&#20837;&#21644;&#30446;&#26631;&#24207;&#21015;&#30340;&#39034;&#24207;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;CTC&#20165;&#24212;&#29992;&#20110;&#30830;&#23450;&#24615;&#24207;&#21015;&#27169;&#22411;&#65292;&#20854;&#20013;&#28508;&#22312;&#31354;&#38388;&#26159;&#19981;&#36830;&#32493;&#19988;&#31232;&#30095;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#22788;&#29702;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#26041;&#38754;&#27604;&#21464;&#20998;&#27169;&#22411;&#33021;&#21147;&#26356;&#24369;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;CTC&#19982;&#21464;&#20998;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24182;&#23548;&#20986;&#20102;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#26356;&#20855;&#26222;&#36866;&#24615;&#30340;&#24207;&#21015;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#20445;&#25345;&#39034;&#24207;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#20004;&#20010;&#21512;&#29702;&#30340;&#20551;&#35774;&#23548;&#20986;&#20102;&#20004;&#20010;&#29256;&#26412;&#30340;&#26032;&#22411;&#21464;&#20998;CTC&#65292;&#31532;&#19968;&#20010;&#20551;&#35774;&#26159;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#21464;&#20998;&#28508;&#22312;&#21464;&#37327;&#22312;&#26465;&#20214;&#19979;&#26159;&#29420;&#31435;&#30340;&#65307;&#31532;&#20108;&#20010;&#20551;&#35774;&#26159;&#36825;&#20123;&#28508;&#22312;&#21464;&#37327;&#26159;&#39532;&#23572;&#21487;&#22827;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#37117;&#20801;&#35768;&#30452;&#25509;&#20248;&#21270;&#27169;&#22411;&#23545;&#25968;&#20284;&#28982;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#35745;&#31639;&#19978;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connectionist temporal classification (CTC) is commonly adopted for sequence modeling tasks like speech recognition, where it is necessary to preserve order between the input and target sequences. However, CTC is only applied to deterministic sequence models, where the latent space is discontinuous and sparse, which in turn makes them less capable of handling data variability when compared to variational models. In this paper, we integrate CTC with a variational model and derive loss functions that can be used to train more generalizable sequence models that preserve order. Specifically, we derive two versions of the novel variational CTC based on two reasonable assumptions, the first being that the variational latent variables at each time step are conditionally independent; and the second being that these latent variables are Markovian. We show that both loss functions allow direct optimization of the variational lower bound for the model log-likelihood, and present computationally t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;BERT&#24182;&#23545;&#20854;&#36827;&#34892;fine-tune&#65292;&#23454;&#29616;&#20102;&#32929;&#24066;&#24773;&#32490;&#30340;&#20998;&#31867;&#21644;&#22522;&#20110;&#35813;&#27169;&#22411;&#30340;&#22238;&#27979;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;fine-tuned&#27169;&#22411;&#30456;&#27604;&#21407;&#22987;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11979</link><description>&lt;p&gt;
&#32929;&#24066;&#24773;&#32490;&#20998;&#31867;&#19982;&#22522;&#20110;Fine-tuned BERT&#30340;&#22238;&#27979;
&lt;/p&gt;
&lt;p&gt;
Stock Market Sentiment Classification and Backtesting via Fine-tuned BERT. (arXiv:2309.11979v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;BERT&#24182;&#23545;&#20854;&#36827;&#34892;fine-tune&#65292;&#23454;&#29616;&#20102;&#32929;&#24066;&#24773;&#32490;&#30340;&#20998;&#31867;&#21644;&#22522;&#20110;&#35813;&#27169;&#22411;&#30340;&#22238;&#27979;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;fine-tuned&#27169;&#22411;&#30456;&#27604;&#21407;&#22987;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#25968;&#25454;&#21644;&#35745;&#31639;&#35774;&#22791;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22522;&#20110;&#23454;&#26102;&#20449;&#24687;&#33719;&#21462;&#30340;&#20302;&#24310;&#36831;&#33258;&#21160;&#20132;&#26131;&#24179;&#21488;&#25104;&#20026;&#32929;&#31080;&#20132;&#26131;&#24066;&#22330;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#27492;&#37327;&#21270;&#20132;&#26131;&#30340;&#20027;&#39064;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23545;&#20110;&#38750;&#24378;&#26377;&#25928;&#30340;&#20132;&#26131;&#24066;&#22330;&#26469;&#35828;&#65292;&#20154;&#31867;&#24773;&#32490;&#21644;&#26399;&#26395;&#24635;&#26159;&#20027;&#23548;&#24066;&#22330;&#36235;&#21183;&#21644;&#20132;&#26131;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20174;&#24773;&#32490;&#29702;&#35770;&#20986;&#21457;&#65292;&#20197;&#19996;&#26041;&#36130;&#23500;&#20026;&#20363;&#65292;&#20174;&#20854;&#23545;&#24212;&#30340;&#32929;&#21543;&#29228;&#21462;&#29992;&#25143;&#35780;&#35770;&#26631;&#39064;&#25968;&#25454;&#24182;&#36827;&#34892;&#25968;&#25454;&#28165;&#27927;&#12290;&#38543;&#21518;&#65292;&#26500;&#24314;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;BERT&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#30340;&#24102;&#26377;&#26631;&#27880;&#25968;&#25454;&#38598;&#23545;BERT&#27169;&#22411;&#36827;&#34892;fine-tune&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21407;&#22987;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;fine-tuned&#27169;&#22411;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#38543;&#21518;&#65292;&#22312;&#20197;&#19978;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;&#29228;&#21462;&#30340;&#29992;&#25143;&#35780;&#35770;&#25968;&#25454;&#36827;&#34892;&#20102;&#24773;&#32490;&#26497;&#24615;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of big data and computing devices, low-latency automatic trading platforms based on real-time information acquisition have become the main components of the stock trading market, so the topic of quantitative trading has received widespread attention. And for non-strongly efficient trading markets, human emotions and expectations always dominate market trends and trading decisions. Therefore, this paper starts from the theory of emotion, taking East Money as an example, crawling user comment titles data from its corresponding stock bar and performing data cleaning. Subsequently, a natural language processing model BERT was constructed, and the BERT model was fine-tuned using existing annotated data sets. The experimental results show that the fine-tuned model has different degrees of performance improvement compared to the original model and the baseline model. Subsequently, based on the above model, the user comment data crawled is labeled with emotional pola
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23618;&#27425;&#20998;&#21106;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#20998;&#35010;&#20989;&#25968;&#25552;&#39640;&#22810;&#31867;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#23618;&#27425;&#32467;&#26500;&#32780;&#26080;&#38656;&#26174;&#24335;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#32570;&#20047;&#23618;&#27425;&#20808;&#39564;&#30693;&#35782;&#30340;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#21322;&#20197;&#19978;&#30340;&#25968;&#25454;&#38598;&#20013;&#26174;&#33879;&#25552;&#21319;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11963</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#20998;&#35010;&#20989;&#25968;&#29983;&#25104;&#23618;&#27425;&#32467;&#26500;&#20197;&#25913;&#21892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Generating Hierarchical Structures for Improved Time Series Classification Using Stochastic Splitting Functions. (arXiv:2309.11963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23618;&#27425;&#20998;&#21106;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#20998;&#35010;&#20989;&#25968;&#25552;&#39640;&#22810;&#31867;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#23618;&#27425;&#32467;&#26500;&#32780;&#26080;&#38656;&#26174;&#24335;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#32570;&#20047;&#23618;&#27425;&#20808;&#39564;&#30693;&#35782;&#30340;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#21322;&#20197;&#19978;&#30340;&#25968;&#25454;&#38598;&#20013;&#26174;&#33879;&#25552;&#21319;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20351;&#29992;&#38543;&#26426;&#20998;&#35010;&#20989;&#25968;&#65288;SSFs&#65289;&#30340;&#23618;&#27425;&#20998;&#21106;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23618;&#27425;&#20998;&#31867;&#65288;HC&#65289;&#25552;&#39640;&#22810;&#31867;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#23618;&#27425;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#32570;&#20047;&#23618;&#27425;&#20808;&#39564;&#30693;&#35782;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#26681;&#25454;&#20998;&#31867;&#22120;&#30340;&#21487;&#36776;&#21035;&#24615;&#23558;&#31867;&#21035;&#31995;&#32479;&#22320;&#21010;&#20998;&#20026;&#20004;&#20010;&#23376;&#38598;&#65292;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#23618;&#27425;&#31867;&#21035;&#30340;&#20108;&#21449;&#26641;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;46&#20010;&#22810;&#31867;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#27969;&#34892;&#30340;&#20998;&#31867;&#22120;&#65288;svm&#21644;rocket&#65289;&#21644;SSFs&#65288;potr&#12289;srtr&#21644;lsoo&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;rocket&#21644;svm&#20316;&#20026;&#20998;&#31867;&#22120;&#26102;&#65292;&#35813;&#26041;&#27861;&#22312;&#36817;&#19968;&#21322;&#21644;&#19977;&#20998;&#20043;&#19968;&#30340;&#25968;&#25454;&#38598;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#19982;HC&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces a novel hierarchical divisive clustering approach with stochastic splitting functions (SSFs) to enhance classification performance in multi-class datasets through hierarchical classification (HC). The method has the unique capability of generating hierarchy without requiring explicit information, making it suitable for datasets lacking prior knowledge of hierarchy. By systematically dividing classes into two subsets based on their discriminability according to the classifier, the proposed approach constructs a binary tree representation of hierarchical classes. The approach is evaluated on 46 multi-class time series datasets using popular classifiers (svm and rocket) and SSFs (potr, srtr, and lsoo). The results reveal that the approach significantly improves classification performance in approximately half and a third of the datasets when using rocket and svm as the classifier, respectively. The study also explores the relationship between dataset features and HC 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#22312;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#19982;&#21453;&#21521;&#20256;&#25773;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2309.11955</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Forward-Forward Algorithm for Self-Supervised Learning. (arXiv:2309.11955v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#22312;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#19982;&#21453;&#21521;&#20256;&#25773;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20854;&#20013;&#19968;&#20123;&#26368;&#26032;&#26041;&#27861;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20986;&#26377;&#29992;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#20102;&#21453;&#21521;&#20256;&#25773;&#20316;&#20026;&#35757;&#32451;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#26368;&#36817;&#65292;Geoffrey Hinton&#25552;&#20986;&#20102;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#20102;&#20004;&#27425;&#21521;&#21069;&#20256;&#36882;&#21644;&#27599;&#23618;&#37117;&#26377;&#19968;&#20010;&#21333;&#29420;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#32593;&#32476;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#21453;&#21521;&#20256;&#25773;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#19982;&#21453;&#21521;&#20256;&#25773;&#22312;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#31354;&#38388;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20351;&#29992;&#20102;&#22235;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;MNIST&#12289;F-MNIST&#12289;SVHN&#21644;CIFAR-10&#65292;&#20197;&#21450;&#19977;&#31181;&#24120;&#29992;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#21363;&#26059;&#36716;&#12289;&#32763;&#36716;&#21644;&#25340;&#22270;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#22312;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#19982;&#21453;&#21521;&#20256;&#25773;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning has seen remarkable progress in the last few years, with some of the recent methods being able to learn useful image representations without labels. These methods are trained using backpropagation, the de facto standard. Recently, Geoffrey Hinton proposed the forward-forward algorithm as an alternative training method. It utilizes two forward passes and a separate loss function for each layer to train the network without backpropagation.  In this study, for the first time, we study the performance of forward-forward vs. backpropagation for self-supervised representation learning and provide insights into the learned representation spaces. Our benchmark employs four standard datasets, namely MNIST, F-MNIST, SVHN and CIFAR-10, and three commonly used self-supervised representation learning techniques, namely rotation, flip and jigsaw.  Our main finding is that while the forward-forward algorithm performs comparably to backpropagation during (self-)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20813;&#30123;&#30340;&#27010;&#29575;&#65292;&#25552;&#20986;&#20102;&#20813;&#30123;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#21450;&#949;-&#26377;&#30028;&#20813;&#30123;&#30340;&#26465;&#20214;&#12290;&#21516;&#26102;&#65292;&#20511;&#21161;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20272;&#35745;&#21463;&#30410;&#27010;&#29575;&#65292;&#24182;&#24471;&#21040;&#27604;&#29616;&#26377;&#36793;&#30028;&#26356;&#32039;&#23494;&#30340;&#27010;&#29575;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#20171;&#32461;&#20102;&#38388;&#25509;&#20813;&#30123;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#26410;&#27979;&#37327;&#28151;&#28102;&#30340;&#20813;&#30123;&#27010;&#29575;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11942</link><description>&lt;p&gt;
&#20851;&#20110;&#20813;&#30123;&#30340;&#27010;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Probability of Immunity. (arXiv:2309.11942v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20813;&#30123;&#30340;&#27010;&#29575;&#65292;&#25552;&#20986;&#20102;&#20813;&#30123;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#21450;&#949;-&#26377;&#30028;&#20813;&#30123;&#30340;&#26465;&#20214;&#12290;&#21516;&#26102;&#65292;&#20511;&#21161;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20272;&#35745;&#21463;&#30410;&#27010;&#29575;&#65292;&#24182;&#24471;&#21040;&#27604;&#29616;&#26377;&#36793;&#30028;&#26356;&#32039;&#23494;&#30340;&#27010;&#29575;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#20171;&#32461;&#20102;&#38388;&#25509;&#20813;&#30123;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#26410;&#27979;&#37327;&#28151;&#28102;&#30340;&#20813;&#30123;&#27010;&#29575;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#30740;&#31350;&#20813;&#30123;&#30340;&#27010;&#29575;&#65292;&#21363;&#26080;&#35770;&#26292;&#38706;&#19982;&#21542;&#65292;&#25928;&#26524;&#37117;&#20250;&#21457;&#29983;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#20813;&#30123;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#20197;&#21450;&#949;-&#26377;&#30028;&#20813;&#30123;&#30340;&#26465;&#20214;&#65292;&#21069;&#32773;&#20801;&#35768;&#25105;&#20204;&#20174;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#20272;&#35745;&#21463;&#30410;&#30340;&#27010;&#29575;&#65288;&#21363;&#21482;&#26377;&#22312;&#26292;&#38706;&#30340;&#24773;&#20917;&#19979;&#25928;&#26524;&#25165;&#20250;&#21457;&#29983;&#65289;&#65292;&#21518;&#32773;&#20801;&#35768;&#25105;&#20204;&#24471;&#21040;&#27604;&#29616;&#26377;&#30340;&#36793;&#30028;&#26356;&#32039;&#23494;&#30340;&#21463;&#30410;&#27010;&#29575;&#36793;&#30028;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#38388;&#25509;&#20813;&#30123;&#30340;&#27010;&#24565;&#65288;&#36890;&#36807;&#20171;&#36136;&#65289;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#21069;&#36848;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#26410;&#27979;&#37327;&#28151;&#28102;&#24773;&#20917;&#19979;&#36827;&#34892;&#20813;&#30123;&#27010;&#29575;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is devoted to the study of the probability of immunity, i.e. the effect occurs whether exposed or not. We derive necessary and sufficient conditions for non-immunity and $\epsilon$-bounded immunity, i.e. the probability of immunity is zero and $\epsilon$-bounded, respectively. The former allows us to estimate the probability of benefit (i.e., the effect occurs if and only if exposed) from a randomized controlled trial, and the latter allows us to produce bounds of the probability of benefit that are tighter than the existing ones. We also introduce the concept of indirect immunity (i.e., through a mediator) and repeat our previous analysis for it. Finally, we propose a method for sensitivity analysis of the probability of immunity under unmeasured confounding.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#24494;&#22411;&#26426;&#22120;&#23398;&#20064;&#65288;TinyML&#65289;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#32508;&#36848;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#19982;&#28508;&#22312;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.11932</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24494;&#22411;&#26426;&#22120;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning-oriented Survey on Tiny Machine Learning. (arXiv:2309.11932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#24494;&#22411;&#26426;&#22120;&#23398;&#20064;&#65288;TinyML&#65289;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#32508;&#36848;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#19982;&#28508;&#22312;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#22411;&#26426;&#22120;&#23398;&#20064;&#65288;TinyML&#65289;&#30340;&#20986;&#29616;&#36890;&#36807;&#25512;&#21160;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#30828;&#20214;&#35774;&#22791;&#19982;&#22522;&#20110;&#23398;&#20064;&#30340;&#36719;&#20214;&#26550;&#26500;&#30340;&#32852;&#21512;&#35774;&#35745;&#65292;&#31215;&#26497;&#38761;&#21629;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;TinyML&#22312;&#31532;&#22235;&#21644;&#31532;&#20116;&#27425;&#24037;&#19994;&#38761;&#21629;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24110;&#21161;&#31038;&#20250;&#12289;&#32463;&#27982;&#21644;&#20010;&#20154;&#24212;&#29992;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#34701;&#20837;&#35745;&#31639;&#25216;&#26415;&#65288;&#22914;&#26234;&#24935;&#22478;&#24066;&#12289;&#27773;&#36710;&#21644;&#21307;&#30103;&#26426;&#22120;&#20154;&#65289;&#12290;&#30001;&#20110;&#20854;&#22810;&#23398;&#31185;&#24615;&#36136;&#65292;TinyML&#39046;&#22495;&#24050;&#32463;&#20174;&#35768;&#22810;&#19981;&#21516;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#30740;&#31350;&#65306;&#26412;&#32508;&#36848;&#24076;&#26395;&#25552;&#20379;&#19968;&#20010;&#26368;&#26032;&#30340;&#32508;&#36848;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#20110;TinyML&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#25152;&#26377;&#23398;&#20064;&#31639;&#27861;&#12290;&#26412;&#32508;&#36848;&#22522;&#20110;&#31995;&#32479;&#32508;&#36848;&#21644;&#20803;&#20998;&#26512;&#30340;&#39318;&#36873;&#25253;&#21578;&#39033;&#65288;PRISMA&#65289;&#26041;&#27861;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#31995;&#32479;&#21644;&#23436;&#25972;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#25105;&#20204;&#23558;&#30740;&#31350;&#23454;&#29616;TinyML&#22522;&#30784;&#35299;&#20915;&#26041;&#26696;&#30340;&#19977;&#31181;&#19981;&#21516;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly we will examine the three different workflows for implementing a TinyML-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26041;&#27861;&#65292;&#19968;&#20010;&#26159;&#33258;&#36866;&#24212;&#36793;&#30028;&#25439;&#22833;&#65292;&#36890;&#36807;&#35843;&#25972;&#36793;&#30028;&#26469;&#21516;&#27493;&#23398;&#20064;&#36895;&#24230;&#65307;&#21478;&#19968;&#20010;&#26159;&#20266;&#26631;&#31614;&#23545;&#27604;&#32858;&#31867;&#65292;&#36890;&#36807;&#32858;&#38598;&#26679;&#26412;&#26469;&#22686;&#24378;&#26032;&#31867;&#21035;&#30340;&#21457;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24179;&#34913;&#24050;&#35265;&#21644;&#26032;&#31867;&#21035;&#65292;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;3%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.11930</link><description>&lt;p&gt;
&#24357;&#21512;&#24046;&#36317;&#65306;&#38024;&#23545;&#24320;&#25918;&#19990;&#30028;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#23398;&#20064;&#36895;&#24230;&#21516;&#27493;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: Learning Pace Synchronization for Open-World Semi-Supervised Learning. (arXiv:2309.11930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26041;&#27861;&#65292;&#19968;&#20010;&#26159;&#33258;&#36866;&#24212;&#36793;&#30028;&#25439;&#22833;&#65292;&#36890;&#36807;&#35843;&#25972;&#36793;&#30028;&#26469;&#21516;&#27493;&#23398;&#20064;&#36895;&#24230;&#65307;&#21478;&#19968;&#20010;&#26159;&#20266;&#26631;&#31614;&#23545;&#27604;&#32858;&#31867;&#65292;&#36890;&#36807;&#32858;&#38598;&#26679;&#26412;&#26469;&#22686;&#24378;&#26032;&#31867;&#21035;&#30340;&#21457;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24179;&#34913;&#24050;&#35265;&#21644;&#26032;&#31867;&#21035;&#65292;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;3%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#19990;&#30028;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#35201;&#27714;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#21457;&#29616;&#26032;&#30340;&#31867;&#21035;&#65292;&#21516;&#26102;&#22312;&#26377;&#26631;&#31614;&#25968;&#25454;&#20013;&#20445;&#25345;&#23545;&#24050;&#35265;&#31867;&#21035;&#30340;&#34920;&#29616;&#12290;&#20854;&#20013;&#30340;&#26680;&#24515;&#25361;&#25112;&#26159;&#24050;&#35265;&#21644;&#26032;&#31867;&#21035;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#23398;&#20064;&#24046;&#36317;&#65292;&#22240;&#20026;&#30001;&#20110;&#20934;&#30830;&#30340;&#30417;&#30563;&#20449;&#24687;&#65292;&#27169;&#22411;&#23398;&#20064;&#24050;&#35265;&#31867;&#21035;&#30340;&#36895;&#24230;&#26356;&#24555;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20197;&#19979;&#20004;&#20010;&#26041;&#27861;&#65306;1&#65289;&#22522;&#20110;&#20272;&#35745;&#31867;&#21035;&#20998;&#24067;&#30340;&#33258;&#36866;&#24212;&#36793;&#30028;&#25439;&#22833;&#65292;&#40723;&#21169;&#23545;&#24050;&#35265;&#31867;&#21035;&#26679;&#26412;&#20351;&#29992;&#36739;&#22823;&#30340;&#36127;&#36793;&#30028;&#65292;&#20197;&#21516;&#27493;&#23398;&#20064;&#36895;&#24230;&#65307;2&#65289;&#20266;&#26631;&#31614;&#23545;&#27604;&#32858;&#31867;&#65292;&#23558;&#21487;&#33021;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#26679;&#26412;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#32858;&#38598;&#22312;&#19968;&#36215;&#65292;&#22686;&#24378;&#26032;&#31867;&#21035;&#21457;&#29616;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#29616;&#26377;&#27169;&#22411;&#20173;&#28982;&#38459;&#30861;&#26032;&#31867;&#21035;&#30340;&#23398;&#20064;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#24179;&#34913;&#20102;&#24050;&#35265;&#21644;&#26032;&#31867;&#21035;&#65292;&#19982;ImageNet&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;3%&#12290;
&lt;/p&gt;
&lt;p&gt;
In open-world semi-supervised learning, a machine learning model is tasked with uncovering novel categories from unlabeled data while maintaining performance on seen categories from labeled data. The central challenge is the substantial learning gap between seen and novel categories, as the model learns the former faster due to accurate supervisory information. To address this, we introduce 1) an adaptive margin loss based on estimated class distribution, which encourages a large negative margin for samples in seen classes, to synchronize learning paces, and 2) pseudo-label contrastive clustering, which pulls together samples which are likely from the same class in the output space, to enhance novel class discovery. Our extensive evaluations on multiple datasets demonstrate that existing models still hinder novel class learning, whereas our approach strikingly balances both seen and novel classes, achieving a remarkable 3% average accuracy increase on the ImageNet dataset compared to t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;Timoshenko&#26753;&#30340;&#21018;&#24230;&#35782;&#21035;&#21644;&#21709;&#24212;&#20272;&#35745;&#12290;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#36827;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24471;&#21040;&#32467;&#26500;&#21442;&#25968;&#30340;&#38543;&#26426;&#27169;&#22411;&#12290;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#27010;&#29575;&#39044;&#27979;&#26410;&#35266;&#27979;&#21040;&#30340;&#21709;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11875</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#39640;&#26031;&#36807;&#31243;&#30340;Timoshenko&#26753;&#30340;&#38543;&#26426;&#21018;&#24230;&#35782;&#21035;&#21644;&#21709;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Stochastic stiffness identification and response estimation of Timoshenko beams via physics-informed Gaussian processes. (arXiv:2309.11875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;Timoshenko&#26753;&#30340;&#21018;&#24230;&#35782;&#21035;&#21644;&#21709;&#24212;&#20272;&#35745;&#12290;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#36827;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24471;&#21040;&#32467;&#26500;&#21442;&#25968;&#30340;&#38543;&#26426;&#27169;&#22411;&#12290;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#27010;&#29575;&#39044;&#27979;&#26410;&#35266;&#27979;&#21040;&#30340;&#21709;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#20026;&#31995;&#32479;&#35782;&#21035;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;Timoshenko&#26753;&#20803;&#32032;&#12290;&#35813;&#27169;&#22411;&#26159;&#19968;&#20010;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#65292;&#20854;&#21327;&#26041;&#24046;&#21644;&#20132;&#21449;&#21327;&#26041;&#24046;&#26680;&#26681;&#25454;&#25376;&#24230;&#12289;&#36716;&#21160;&#12289;&#24212;&#21464;&#12289;&#24367;&#30697;&#12289;&#21098;&#21147;&#21644;&#26045;&#21152;&#36733;&#33655;&#30340;&#24494;&#20998;&#26041;&#31243;&#35299;&#26512;&#22320;&#25512;&#23548;&#32780;&#26469;&#12290;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#22312;&#36125;&#21494;&#26031;&#26684;&#24335;&#19979;&#36827;&#34892;&#21018;&#24230;&#35782;&#21035;&#65292;&#26368;&#22823;&#21270;&#21518;&#39564;&#27169;&#22411;&#65292;&#24471;&#21040;&#32467;&#26500;&#21442;&#25968;&#30340;&#38543;&#26426;&#27169;&#22411;&#12290;&#20248;&#21270;&#21518;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#36827;&#19968;&#27493;&#29992;&#20110;&#23545;&#26410;&#35266;&#27979;&#21040;&#30340;&#21709;&#24212;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#29289;&#29702;&#20449;&#24687;&#20256;&#24863;&#22120;&#24067;&#32622;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#24322;&#36136;&#20256;&#24863;&#22120;&#20301;&#32622;&#20449;&#24687;&#21644;&#23884;&#20837;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#32467;&#26500;&#36793;&#30028;&#26465;&#20214;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#26377;&#25928;&#35782;&#21035;&#21018;&#24230;&#24182;&#39044;&#27979;&#26410;&#35266;&#27979;&#21040;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models trained with structural health monitoring data have become a powerful tool for system identification. This paper presents a physics-informed Gaussian process (GP) model for Timoshenko beam elements. The model is constructed as a multi-output GP with covariance and cross-covariance kernels analytically derived based on the differential equations for deflections, rotations, strains, bending moments, shear forces and applied loads. Stiffness identification is performed in a Bayesian format by maximising a posterior model through a Markov chain Monte Carlo method, yielding a stochastic model for the structural parameters. The optimised GP model is further employed for probabilistic predictions of unobserved responses. Additionally, an entropy-based method for physics-informed sensor placement optimisation is presented, exploiting heterogeneous sensor position information and structural boundary conditions built into the GP model. Results demonstrate that the propose
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25913;&#36827;&#30340;&#26041;&#24046;&#26368;&#23567;&#21270;&#30340;&#20998;&#22359;&#37327;&#21270;&#31574;&#30053;&#65292;&#29992;&#20110;&#21387;&#32553;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28608;&#27963;&#65292;&#23454;&#29616;&#20869;&#23384;&#28040;&#32791;&#30340;&#38477;&#20302;&#21644;&#36816;&#34892;&#26102;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2309.11856</link><description>&lt;p&gt;
&#20351;&#29992;&#25913;&#36827;&#30340;&#26041;&#24046;&#26368;&#23567;&#21270;&#30340;&#20998;&#22359;&#37327;&#21270;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#28608;&#27963;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization. (arXiv:2309.11856v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25913;&#36827;&#30340;&#26041;&#24046;&#26368;&#23567;&#21270;&#30340;&#20998;&#22359;&#37327;&#21270;&#31574;&#30053;&#65292;&#29992;&#20110;&#21387;&#32553;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28608;&#27963;&#65292;&#23454;&#29616;&#20869;&#23384;&#28040;&#32791;&#30340;&#38477;&#20302;&#21644;&#36816;&#34892;&#26102;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#39640;&#25928;&#35757;&#32451;&#65292;&#37325;&#28857;&#26159;&#20943;&#23569;&#20854;&#20869;&#23384;&#28040;&#32791;&#12290;Liu&#31561;&#20154;&#65288;2022&#24180;&#65289;&#25552;&#20986;&#20102;&#26497;&#38480;&#28608;&#27963;&#21387;&#32553;&#65288;EXACT&#65289;&#65292;&#36890;&#36807;&#23558;&#20013;&#38388;&#28608;&#27963;&#22270;&#30340;&#37327;&#21270;&#38477;&#33267;INT2&#31934;&#24230;&#65292;&#23454;&#29616;&#20102;&#20869;&#23384;&#28040;&#32791;&#30340;&#21095;&#28872;&#20943;&#23569;&#12290;&#20182;&#20204;&#22312;&#23454;&#29616;&#22823;&#24133;&#20943;&#23569;GPU&#20869;&#23384;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#34920;&#29616;&#20960;&#20046;&#27809;&#26377;&#38477;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#28608;&#27963;&#22270;&#30340;&#20998;&#22359;&#37327;&#21270;&#65292;&#23545;EXACT&#31574;&#30053;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#23454;&#39564;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#22359;&#22823;&#23567;&#65292;&#24182;&#23637;&#31034;&#20102;&#36827;&#19968;&#27493;&#30340;&#20869;&#23384;&#28040;&#32791;&#38477;&#20302;&#65288;&gt;15%&#65289;&#21644;&#27599;&#20010;epoch&#30340;&#36816;&#34892;&#26102;&#21152;&#36895;&#65288;&#32422;5%&#65289;&#65292;&#21363;&#20351;&#36827;&#34892;&#20102;&#26497;&#20854;&#22823;&#30340;&#37327;&#21270;&#31243;&#24230;&#65292;&#20063;&#33021;&#33719;&#24471;&#19982;&#21407;&#22987;EXACT&#30456;&#20284;&#30340;&#24615;&#33021;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;EXACT&#20013;&#20851;&#20110;&#20013;&#38388;&#28608;&#27963;&#22270;&#20998;&#24067;&#30340;&#20551;&#35774;&#36827;&#34892;&#20102;&#32416;&#27491;&#65288;&#20551;&#35774;&#20026;u
&lt;/p&gt;
&lt;p&gt;
Efficient training of large-scale graph neural networks (GNNs) has been studied with a specific focus on reducing their memory consumption. Work by Liu et al. (2022) proposed extreme activation compression (EXACT) which demonstrated drastic reduction in memory consumption by performing quantization of the intermediate activation maps down to using INT2 precision. They showed little to no reduction in performance while achieving large reductions in GPU memory consumption. In this work, we present an improvement to the EXACT strategy by using block-wise quantization of the intermediate activation maps. We experimentally analyze different block sizes and show further reduction in memory consumption (&gt;15%), and runtime speedup per epoch (about 5%) even when performing extreme extents of quantization with similar performance trade-offs as with the original EXACT. Further, we present a correction to the assumptions on the distribution of intermediate activation maps in EXACT (assumed to be u
&lt;/p&gt;</description></item><item><title>TMac&#26159;&#19968;&#20010;&#26102;&#24577;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#22270;&#23398;&#20064;&#30340;&#26041;&#24335;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#26102;&#24577;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11845</link><description>&lt;p&gt;
TMac&#65306;&#29992;&#20110;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#30340;&#26102;&#24577;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TMac: Temporal Multi-Modal Graph Learning for Acoustic Event Classification. (arXiv:2309.11845v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11845
&lt;/p&gt;
&lt;p&gt;
TMac&#26159;&#19968;&#20010;&#26102;&#24577;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#22270;&#23398;&#20064;&#30340;&#26041;&#24335;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#26102;&#24577;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#25968;&#23383;&#26102;&#20195;&#65292;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#38543;&#22788;&#21487;&#35265;&#65292;&#36825;&#23545;&#20110;&#23545;&#23427;&#20204;&#24320;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#26356;&#39640;&#30340;&#35201;&#27714;&#12290;&#26377;&#25928;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20449;&#24687;&#26159;&#26356;&#22909;&#30340;&#38899;&#39057;&#35270;&#35273;&#27169;&#22411;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#33258;&#28982;&#20855;&#26377;&#26102;&#38388;&#23646;&#24615;&#65292;&#20363;&#22914;&#35270;&#39057;&#20013;&#27599;&#19968;&#24103;&#30340;&#26102;&#38388;&#20449;&#24687;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#36825;&#20123;&#25968;&#25454;&#26681;&#25454;&#38899;&#39057;&#21644;&#35270;&#35273;&#32447;&#32034;&#33258;&#28982;&#24418;&#25104;&#22810;&#27169;&#24577;&#65292;&#24182;&#19988;&#20005;&#26684;&#25353;&#29031;&#26102;&#38388;&#39034;&#24207;&#36827;&#34892;&#12290;&#36825;&#34920;&#26126;&#65292;&#22312;&#22810;&#27169;&#24577;&#22768;&#38899;&#20107;&#20214;&#24314;&#27169;&#20013;&#65292;&#26102;&#24577;&#20449;&#24687;&#23545;&#20110;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#37117;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#29420;&#31435;&#22320;&#22788;&#29702;&#27599;&#20010;&#27169;&#24577;&#29305;&#24449;&#65292;&#20165;&#31616;&#21333;&#22320;&#23558;&#23427;&#20204;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#24573;&#35270;&#20102;&#26102;&#24577;&#20851;&#31995;&#30340;&#25366;&#25496;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#30340;&#26102;&#24577;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;TMac&#65292;&#36890;&#36807;&#22270;&#23398;&#20064;&#23545;&#36825;&#31181;&#26102;&#24577;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audiovisual data is everywhere in this digital age, which raises higher requirements for the deep learning models developed on them. To well handle the information of the multi-modal data is the key to a better audiovisual modal. We observe that these audiovisual data naturally have temporal attributes, such as the time information for each frame in the video. More concretely, such data is inherently multi-modal according to both audio and visual cues, which proceed in a strict chronological order. It indicates that temporal information is important in multi-modal acoustic event modeling for both intra- and inter-modal. However, existing methods deal with each modal feature independently and simply fuse them together, which neglects the mining of temporal relation and thus leads to sub-optimal performance. With this motivation, we propose a Temporal Multi-modal graph learning method for Acoustic event Classification, called TMac, by modeling such temporal information via graph learning
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#31038;&#21306;&#32467;&#26500;&#26159;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#31185;&#23398;&#23478;&#20204;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.11798</link><description>&lt;p&gt;
&#22270;&#20013;&#31038;&#21306;&#26816;&#27979;&#30340;&#32508;&#21512;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of Community Detection in Graphs. (arXiv:2309.11798v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#31038;&#21306;&#32467;&#26500;&#26159;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#31185;&#23398;&#23478;&#20204;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#30740;&#31350;&#26174;&#33879;&#20419;&#36827;&#20102;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#31038;&#21306;&#32467;&#26500;&#30340;&#29702;&#35299;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;&#23613;&#31649;&#36328;&#23398;&#31185;&#31185;&#23398;&#23478;&#31038;&#21306;&#30340;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;&#22270;&#20013;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#30340;&#32452;&#32455;&#21644;&#21151;&#33021;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#23427;&#25351;&#30340;&#26159;&#23558;&#39030;&#28857;&#21010;&#20998;&#20026;&#20855;&#26377;&#24378;&#20869;&#37096;&#36830;&#25509;&#21644;&#36739;&#24369;&#36830;&#25509;&#30340;&#38598;&#32676;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#38416;&#36848;&#65292;&#21253;&#25324;&#25105;&#20204;&#35774;&#35745;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#31038;&#21306;&#26816;&#27979;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#30495;&#23454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of complex networks has significantly advanced our understanding of community structures which serves as a crucial feature of real-world graphs. Detecting communities in graphs is a challenging problem with applications in sociology, biology, and computer science. Despite the efforts of an interdisciplinary community of scientists, a satisfactory solution to this problem has not yet been achieved. This review article delves into the topic of community detection in graphs, which serves as a crucial role in understanding the organization and functioning of complex systems. We begin by introducing the concept of community structure, which refers to the arrangement of vertices into clusters, with strong internal connections and weaker connections between clusters. Then, we provide a thorough exposition of various community detection methods, including a new method designed by us. Additionally, we explore real-world applications of community detection in diverse networks. In concl
&lt;/p&gt;</description></item><item><title>DimCL&#26159;&#19968;&#31181;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25913;&#36827;&#29305;&#24449;&#22810;&#26679;&#24615;&#30340;&#32500;&#24230;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;DimCL&#30340;&#30828;&#26679;&#26412;&#29305;&#24615;&#26159;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#23558;DimCL&#34701;&#20837;SSL&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11782</link><description>&lt;p&gt;
DimCL: &#29992;&#20110;&#25913;&#36827;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32500;&#24230;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DimCL: Dimensional Contrastive Learning For Improving Self-Supervised Learning. (arXiv:2309.11782v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11782
&lt;/p&gt;
&lt;p&gt;
DimCL&#26159;&#19968;&#31181;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25913;&#36827;&#29305;&#24449;&#22810;&#26679;&#24615;&#30340;&#32500;&#24230;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;DimCL&#30340;&#30828;&#26679;&#26412;&#29305;&#24615;&#26159;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#23558;DimCL&#34701;&#20837;SSL&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#21457;&#23637;&#36215;&#26469;&#30340;&#26032;&#30340;&#38750;CL&#26694;&#26550;&#24050;&#32463;&#21462;&#24471;&#20102;&#21487;&#27604;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#28508;&#21147;&#65292;&#36825;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#25552;&#21319;&#36825;&#20123;&#26694;&#26550;&#12290;&#23558;CL&#34701;&#20837;&#38750;CL&#26694;&#26550;&#34987;&#35748;&#20026;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#27809;&#26377;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32500;&#24230;&#26041;&#21521;&#19978;&#25191;&#34892;CL&#32780;&#19981;&#26159;&#20197;&#20256;&#32479;&#30340;&#25209;&#27425;&#26041;&#21521;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#21629;&#21517;&#20026;Dimensional Contrastive Learning&#65288;DimCL&#65289;&#12290;DimCL&#26088;&#22312;&#22686;&#24378;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;&#20808;&#21069;&#30340;SSL&#26694;&#26550;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#32467;&#26524;&#21457;&#29616;DimCL&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#30828;&#26679;&#26412;&#23545;&#20854;&#25104;&#21151;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23558;DimCL&#34701;&#20837;SSL&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has gained remarkable success, for which contrastive learning (CL) plays a key role. However, the recent development of new non-CL frameworks has achieved comparable or better performance with high improvement potential, prompting researchers to enhance these frameworks further. Assimilating CL into non-CL frameworks has been thought to be beneficial, but empirical evidence indicates no visible improvements. In view of that, this paper proposes a strategy of performing CL along the dimensional direction instead of along the batch direction as done in conventional contrastive learning, named Dimensional Contrastive Learning (DimCL). DimCL aims to enhance the feature diversity, and it can serve as a regularizer to prior SSL frameworks. DimCL has been found to be effective, and the hardness-aware property is identified as a critical reason for its success. Extensive experimental results reveal that assimilating DimCL into SSL frameworks leads to performance 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20351;&#29992;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#35760;&#24405;&#30340;&#27493;&#24577;&#27169;&#24335;&#36827;&#34892;&#35748;&#35777;&#30340;&#25932;&#23545;&#27169;&#22411;&#12290;&#30740;&#31350;&#35843;&#26597;&#20102;&#26159;&#21542;&#21487;&#33021;&#26500;&#24314;&#19968;&#26412;IMUGait&#27169;&#24335;&#30340;&#23383;&#20856;&#65292;&#29992;&#20110;&#21457;&#21160;&#25915;&#20987;&#25110;&#25214;&#21040;&#33021;&#22815;&#21305;&#37197;&#30446;&#26631;IMUGait&#27169;&#24335;&#30340;&#27169;&#20223;&#32773;&#12290;&#36890;&#36807;&#23545;&#38169;&#35823;&#29575;&#30340;&#36827;&#19968;&#27493;&#20998;&#26512;&#65292;&#25361;&#25112;&#20102;&#22522;&#20110;IMUGait&#27169;&#24335;&#30340;&#35748;&#35777;&#31995;&#32479;&#26159;&#21542;&#26368;&#22256;&#38590;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.11766</link><description>&lt;p&gt;
&#22522;&#20110;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;(IMU)&#30340;&#27493;&#24577;&#35748;&#35777;&#30340;&#23383;&#20856;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Dictionary Attack on IMU-based Gait Authentication. (arXiv:2309.11766v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11766
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20351;&#29992;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#35760;&#24405;&#30340;&#27493;&#24577;&#27169;&#24335;&#36827;&#34892;&#35748;&#35777;&#30340;&#25932;&#23545;&#27169;&#22411;&#12290;&#30740;&#31350;&#35843;&#26597;&#20102;&#26159;&#21542;&#21487;&#33021;&#26500;&#24314;&#19968;&#26412;IMUGait&#27169;&#24335;&#30340;&#23383;&#20856;&#65292;&#29992;&#20110;&#21457;&#21160;&#25915;&#20987;&#25110;&#25214;&#21040;&#33021;&#22815;&#21305;&#37197;&#30446;&#26631;IMUGait&#27169;&#24335;&#30340;&#27169;&#20223;&#32773;&#12290;&#36890;&#36807;&#23545;&#38169;&#35823;&#29575;&#30340;&#36827;&#19968;&#27493;&#20998;&#26512;&#65292;&#25361;&#25112;&#20102;&#22522;&#20110;IMUGait&#27169;&#24335;&#30340;&#35748;&#35777;&#31995;&#32479;&#26159;&#21542;&#26368;&#22256;&#38590;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20869;&#32622;&#30340;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;(IMU)&#35760;&#24405;&#30340;&#27493;&#24577;&#27169;&#24335;&#36827;&#34892;&#35748;&#35777;&#30340;&#25932;&#23545;&#27169;&#22411;&#12290;&#25915;&#20987;&#24605;&#36335;&#21463;&#21040;&#23383;&#20856;&#25915;&#20987;&#30693;&#35782;(PIN&#25110;&#23494;&#30721;)&#22522;&#30784;&#35748;&#35777;&#31995;&#32479;&#30340;&#27010;&#24565;&#21551;&#21457;&#65292;&#24182;&#20197;&#27492;&#21629;&#21517;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#26159;&#21542;&#21487;&#33021;&#26500;&#24314;&#19968;&#26412;IMUGait&#27169;&#24335;&#30340;&#23383;&#20856;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#21457;&#21160;&#25915;&#20987;&#65292;&#25110;&#25214;&#21040;&#19968;&#20010;&#33021;&#22815;&#20027;&#21160;&#22797;&#21046;&#19982;&#30446;&#26631;IMUGait&#27169;&#24335;&#21305;&#37197;&#30340;&#27169;&#20223;&#32773;&#12290;&#20061;&#20010;&#36523;&#20307;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#22810;&#26679;&#21270;&#30340;&#20010;&#20307;&#20197;&#19981;&#21516;&#27700;&#24179;&#30340;&#22235;&#20010;&#39044;&#23450;&#20041;&#21487;&#25511;&#21644;&#21487;&#35843;&#33410;&#30340;&#27493;&#24577;&#22240;&#32032;(&#36895;&#24230;&#12289;&#27493;&#38271;&#12289;&#27493;&#23485;&#21644;&#22823;&#33151;&#25260;&#36215;)&#34892;&#36208;&#65292;&#20135;&#29983;&#20102;178&#31181;&#29420;&#29305;&#30340;IMUGait&#27169;&#24335;&#12290;&#27599;&#20010;&#27169;&#24335;&#37117;&#20250;&#25915;&#20987;&#21508;&#31181;&#29992;&#25143;&#35748;&#35777;&#27169;&#22411;&#12290;&#23545;&#38169;&#35823;&#29575;&#30340;&#28145;&#20837;&#20998;&#26512;(&#25915;&#20987;&#21069;&#21518;)&#25361;&#25112;&#20102;&#22522;&#20110;IMUGait&#27169;&#24335;&#30340;&#35748;&#35777;&#31995;&#32479;&#26159;&#26368;&#22256;&#38590;&#30340;&#36825;&#31181;&#20449;&#20208;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel adversarial model for authentication systems that use gait patterns recorded by the inertial measurement unit (IMU) built into smartphones. The attack idea is inspired by and named after the concept of a dictionary attack on knowledge (PIN or password) based authentication systems. In particular, this work investigates whether it is possible to build a dictionary of IMUGait patterns and use it to launch an attack or find an imitator who can actively reproduce IMUGait patterns that match the target's IMUGait pattern. Nine physically and demographically diverse individuals walked at various levels of four predefined controllable and adaptable gait factors (speed, step length, step width, and thigh-lift), producing 178 unique IMUGait patterns. Each pattern attacked a wide variety of user authentication models. The deeper analysis of error rates (before and after the attack) challenges the belief that authentication systems based on IMUGait patterns are the most difficul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#21512;&#25104;&#23569;&#37327;&#31034;&#33539;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;ICL&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#24378;&#38544;&#31169;&#32423;&#21035;&#19979;&#33021;&#22815;&#21462;&#24471;&#31454;&#20105;&#24615;&#33021;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#39046;&#22495;&#30340;&#38544;&#31169;&#20445;&#25252;&#19979;ICL&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11765</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#24046;&#20998;&#38544;&#31169;&#24369;&#30417;&#30563;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation. (arXiv:2309.11765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#21512;&#25104;&#23569;&#37327;&#31034;&#33539;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;ICL&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#24378;&#38544;&#31169;&#32423;&#21035;&#19979;&#33021;&#22815;&#21462;&#24471;&#31454;&#20105;&#24615;&#33021;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#39046;&#22495;&#30340;&#38544;&#31169;&#20445;&#25252;&#19979;ICL&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#24773;&#26223;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;LLM&#21487;&#33021;&#27844;&#28431;&#25110;&#22797;&#36848;&#22312;&#25552;&#31034;&#20013;&#23637;&#31034;&#30340;&#31169;&#26377;&#31034;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#31169;&#26377;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#20855;&#26377;&#24418;&#24335;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#21512;&#25104;&#23569;&#37327;&#31034;&#33539;&#65292;&#24182;&#22312;&#23454;&#35777;&#19978;&#35777;&#26126;&#23427;&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#30340;ICL&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#38750;&#31169;&#26377;ICL&#21644;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#24378;&#38544;&#31169;&#32423;&#21035;&#19979;&#36798;&#21040;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;ICL&#22312;&#24191;&#27867;&#24212;&#29992;&#39046;&#22495;&#25171;&#24320;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of in-context learning (ICL) with large language models (LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak or regurgitate the private examples demonstrated in the prompt. We propose a novel algorithm that generates synthetic few-shot demonstrations from the private dataset with formal differential privacy (DP) guarantees, and show empirically that it can achieve effective ICL. We conduct extensive experiments on standard benchmarks and compare our algorithm with non-private ICL and zero-shot solutions. Our results demonstrate that our algorithm can achieve competitive performance with strong privacy levels. These results open up new possibilities for ICL with privacy protection for a broad range of applications.
&lt;/p&gt;</description></item><item><title>SAM-OCTA&#26159;&#19968;&#31181;&#23558;&#22522;&#30784;&#27169;&#22411;&#24212;&#29992;&#20110;OCTA&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#22312;&#20844;&#24320;&#30340;OCTA-500&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23616;&#37096;&#34880;&#31649;&#20998;&#21106;&#21644;&#26377;&#25928;&#30340;&#21160;&#33033;-&#38745;&#33033;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2309.11758</link><description>&lt;p&gt;
SAM-OCTA&#65306;&#19968;&#31181;&#23558;&#22522;&#30784;&#27169;&#22411;&#24212;&#29992;&#20110;OCTA&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#24494;&#35843;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks. (arXiv:2309.11758v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11758
&lt;/p&gt;
&lt;p&gt;
SAM-OCTA&#26159;&#19968;&#31181;&#23558;&#22522;&#30784;&#27169;&#22411;&#24212;&#29992;&#20110;OCTA&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#22312;&#20844;&#24320;&#30340;OCTA-500&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23616;&#37096;&#34880;&#31649;&#20998;&#21106;&#21644;&#26377;&#25928;&#30340;&#21160;&#33033;-&#38745;&#33033;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#25104;&#20687;(OCTA)&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#20998;&#21106;&#29305;&#23450;&#30446;&#26631;&#26159;&#24517;&#35201;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#26377;&#38480;&#26679;&#26412;&#65288;&#22823;&#32422;&#20960;&#30334;&#20010;&#65289;&#30340;&#30417;&#30563;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37319;&#29992;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#36827;&#34892;&#22522;&#30784;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#25552;&#31034;&#28857;&#29983;&#25104;&#31574;&#30053;&#65292;&#20197;&#22788;&#29702;OCTA&#25968;&#25454;&#38598;&#19978;&#30340;&#21508;&#31181;&#20998;&#21106;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#34987;&#21629;&#21517;&#20026;SAM-OCTA&#65292;&#24182;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;OCTA-500&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#25351;&#26631;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#23616;&#37096;&#34880;&#31649;&#20998;&#21106;&#20197;&#21450;&#26377;&#25928;&#30340;&#21160;&#33033;-&#38745;&#33033;&#20998;&#21106;&#65292;&#36825;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/ShellRedia/SAM-OCTA&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the analysis of optical coherence tomography angiography (OCTA) images, the operation of segmenting specific targets is necessary. Existing methods typically train on supervised datasets with limited samples (approximately a few hundred), which can lead to overfitting. To address this, the low-rank adaptation technique is adopted for foundation model fine-tuning and proposed corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been experimented on the publicly available OCTA-500 dataset. While achieving state-of-the-art performance metrics, this method accomplishes local vessel segmentation as well as effective artery-vein segmentation, which was not well-solved in previous works. The code is available at: https://github.com/ShellRedia/SAM-OCTA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Google&#30340;Bard&#22312;&#23545;&#25239;&#22270;&#20687;&#25915;&#20987;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#34987;&#25915;&#20987;&#20197;&#36755;&#20986;&#38169;&#35823;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;&#36825;&#19968;&#25915;&#20987;&#36824;&#21487;&#20197;&#23545;&#20854;&#20182;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24433;&#21709;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;Bard&#30340;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.11751</link><description>&lt;p&gt;
Google&#30340;Bard&#22312;&#23545;&#25239;&#22270;&#20687;&#25915;&#20987;&#26041;&#38754;&#26377;&#22810;&#24378;&#22823;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Robust is Google's Bard to Adversarial Image Attacks?. (arXiv:2309.11751v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Google&#30340;Bard&#22312;&#23545;&#25239;&#22270;&#20687;&#25915;&#20987;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#34987;&#25915;&#20987;&#20197;&#36755;&#20986;&#38169;&#35823;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;&#36825;&#19968;&#25915;&#20987;&#36824;&#21487;&#20197;&#23545;&#20854;&#20182;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24433;&#21709;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;Bard&#30340;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#25991;&#26412;&#21644;&#20854;&#20182;&#27169;&#24577;&#65288;&#23588;&#20854;&#26159;&#35270;&#35273;&#65289;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35270;&#35273;&#27169;&#22411;&#30340;&#26410;&#35299;&#20915;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#35270;&#35273;&#36755;&#20837;&#21487;&#33021;&#20351;MLLM&#38754;&#20020;&#26356;&#20005;&#37325;&#30340;&#23433;&#20840;&#39118;&#38505;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Google&#30340;Bard&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23427;&#26159;&#19968;&#20010;&#31454;&#20105;&#24615;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#26368;&#36817;&#21457;&#24067;&#20102;&#20854;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#21830;&#19994;MLLM&#30340;&#28431;&#27934;&#12290;&#36890;&#36807;&#25915;&#20987;&#30333;&#30418;&#23376;&#20195;&#29702;&#35270;&#35273;&#32534;&#30721;&#22120;&#25110;MLLM&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#21487;&#20197;&#20351;Bard&#20197;22&#65285;&#30340;&#25104;&#21151;&#29575;&#20165;&#22522;&#20110;&#21487;&#36716;&#31227;&#24615;&#36755;&#20986;&#38169;&#35823;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#36824;&#21487;&#20197;&#25915;&#20987;&#20854;&#20182;MLLM&#65292;&#20363;&#22914;&#65292;&#23545;Bing Chat&#30340;&#25104;&#21151;&#25915;&#20987;&#29575;&#20026;26&#65285;&#65292;&#23545;ERNIE bot&#30340;&#25104;&#21151;&#25915;&#20987;&#29575;&#20026;86&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;Bard&#30340;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#65292;&#21253;&#25324;&#38754;&#37096;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection
&lt;/p&gt;</description></item><item><title>PIE&#26159;&#19968;&#20010;&#26032;&#30340;&#28176;&#36827;&#22270;&#20687;&#32534;&#36753;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;&#24615;&#22320;&#25805;&#32437;&#22270;&#20687;&#29305;&#24449;&#26469;&#20934;&#30830;&#27169;&#25311;&#20010;&#20307;&#24739;&#32773;&#30340;&#30142;&#30149;&#36827;&#23637;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#30340;&#30142;&#30149;&#36827;&#23637;&#27169;&#25311;&#65292;&#24182;&#22312;&#39564;&#35777;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11745</link><description>&lt;p&gt;
PIE: &#36890;&#36807;&#28176;&#36827;&#22270;&#20687;&#32534;&#36753;&#27169;&#25311;&#30142;&#30149;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
PIE: Simulating Disease Progression via Progressive Image Editing. (arXiv:2309.11745v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11745
&lt;/p&gt;
&lt;p&gt;
PIE&#26159;&#19968;&#20010;&#26032;&#30340;&#28176;&#36827;&#22270;&#20687;&#32534;&#36753;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;&#24615;&#22320;&#25805;&#32437;&#22270;&#20687;&#29305;&#24449;&#26469;&#20934;&#30830;&#27169;&#25311;&#20010;&#20307;&#24739;&#32773;&#30340;&#30142;&#30149;&#36827;&#23637;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#30340;&#30142;&#30149;&#36827;&#23637;&#27169;&#25311;&#65292;&#24182;&#22312;&#39564;&#35777;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30142;&#30149;&#36827;&#23637;&#27169;&#25311;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#20020;&#24202;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#27835;&#30103;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#23545;&#20010;&#20307;&#24739;&#32773;&#36827;&#34892;&#36830;&#32493;&#30340;&#21307;&#23398;&#25104;&#20687;&#30417;&#27979;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28176;&#36827;&#22270;&#20687;&#32534;&#36753;&#65288;PIE&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#25511;&#21046;&#24615;&#22320;&#25805;&#32437;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#31934;&#30830;&#21644;&#36924;&#30495;&#30340;&#30142;&#30149;&#36827;&#23637;&#27169;&#25311;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20934;&#30830;&#22320;&#27169;&#25311;&#30142;&#30149;&#36827;&#23637;&#24182;&#38024;&#23545;&#27599;&#20010;&#24739;&#32773;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#65292;&#23558;&#20854;&#35270;&#20026;&#20855;&#26377;&#25351;&#25968;&#34928;&#20943;&#23398;&#20064;&#29575;&#30340;&#26799;&#24230;&#19979;&#38477;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;PIE&#30456;&#23545;&#20110;&#31283;&#23450;&#25193;&#25955;&#28459;&#28216;&#21644;&#22522;&#20110;&#39118;&#26684;&#30340;&#29305;&#24449;&#29983;&#25104;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disease progression simulation is a crucial area of research that has significant implications for clinical diagnosis, prognosis, and treatment. One major challenge in this field is the lack of continuous medical imaging monitoring of individual patients over time. To address this issue, we develop a novel framework termed Progressive Image Editing (PIE) that enables controlled manipulation of disease-related image features, facilitating precise and realistic disease progression simulation. Specifically, we leverage recent advancements in text-to-image generative models to simulate disease progression accurately and personalize it for each patient. We theoretically analyze the iterative refining process in our framework as a gradient descent with an exponentially decayed learning rate. To validate our framework, we conduct experiments in three medical imaging domains. Our results demonstrate the superiority of PIE over existing methods such as Stable Diffusion Walk and Style-Based Mani
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UGPIG&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#21098;&#26525;&#29992;&#25143;&#22270;&#21644;&#24847;&#22270;&#22270;&#35299;&#20915;&#31354;&#38388;&#24322;&#36136;&#24615;&#21644;&#31232;&#30095;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#25512;&#33616;&#36866;&#29992;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.11741</link><description>&lt;p&gt;
&#25581;&#31034;&#26368;&#20339;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#36335;&#24452;&#65306;&#21033;&#29992;&#22270;&#21098;&#26525;&#21644;&#24847;&#22270;&#22270;&#36827;&#34892;&#26377;&#25928;&#25512;&#33616;&#30340;&#21019;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unveiling Optimal SDG Pathways: An Innovative Approach Leveraging Graph Pruning and Intent Graph for Effective Recommendations. (arXiv:2309.11741v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UGPIG&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#21098;&#26525;&#29992;&#25143;&#22270;&#21644;&#24847;&#22270;&#22270;&#35299;&#20915;&#31354;&#38388;&#24322;&#36136;&#24615;&#21644;&#31232;&#30095;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#25512;&#33616;&#36866;&#29992;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#22320;&#21306;&#20419;&#36827;&#29983;&#24577;&#12289;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#36164;&#28304;&#21487;&#25345;&#32493;&#24615;&#30340;&#37325;&#35201;&#25163;&#27573;&#26159;&#25512;&#33616;&#36866;&#24403;&#30340;&#21457;&#23637;&#36335;&#24452;&#65292;&#20063;&#31216;&#20026;&#29983;&#24577;&#25991;&#26126;&#27169;&#24335;&#65288;&#21363;&#21487;&#25345;&#32493;&#21457;&#23637;&#27169;&#24335;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#25512;&#33616;&#31639;&#27861;&#22312;&#20805;&#20998;&#35299;&#20915;&#19982;&#29615;&#22659;&#30456;&#20851;&#30340;&#31354;&#38388;&#24322;&#36136;&#24615;&#21644;&#21306;&#22495;&#21382;&#21490;&#20114;&#21160;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25512;&#33616;&#21487;&#25345;&#32493;&#21457;&#23637;&#27169;&#24335;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21098;&#26525;&#29992;&#25143;&#22270;&#21644;&#24847;&#22270;&#22270;&#65288;UGPIG&#65289;&#8221;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation of appropriate development pathways, also known as ecological civilization patterns for achieving Sustainable Development Goals (namely, sustainable development patterns), are of utmost importance for promoting ecological, economic, social, and resource sustainability in a specific region. To achieve this, the recommendation process must carefully consider the region's natural, environmental, resource, and economic characteristics. However, current recommendation algorithms in the field of computer science fall short in adequately addressing the spatial heterogeneity related to environment and sparsity of regional historical interaction data, which limits their effectiveness in recommending sustainable development patterns. To overcome these challenges, this paper proposes a method called User Graph after Pruning and Intent Graph (UGPIG). Firstly, we utilize the high-density linking capability of the pruned User Graph to address the issue of spatial heterogeneity neg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#30340;&#25968;&#25454;&#20849;&#20139;&#21338;&#24328;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#21338;&#24328;&#35770;&#30340;&#26041;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#36873;&#26680;&#28608;&#21169;&#26426;&#21046;&#65292;&#20197;&#28608;&#21169;&#30495;&#23454;&#36755;&#20837;&#25968;&#25454;&#24182;&#20419;&#36827;&#31283;&#23450;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.11722</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#36873;&#26680;&#28608;&#21169;&#26426;&#21046;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Efficient Core-selecting Incentive Mechanism for Data Sharing in Federated Learning. (arXiv:2309.11722v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#30340;&#25968;&#25454;&#20849;&#20139;&#21338;&#24328;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#21338;&#24328;&#35770;&#30340;&#26041;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#36873;&#26680;&#28608;&#21169;&#26426;&#21046;&#65292;&#20197;&#28608;&#21169;&#30495;&#23454;&#36755;&#20837;&#25968;&#25454;&#24182;&#20419;&#36827;&#31283;&#23450;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#25913;&#36827;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#21442;&#19982;&#32773;&#21512;&#20316;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#19988;&#20182;&#20204;&#23558;&#33719;&#24471;&#20840;&#23616;&#27169;&#22411;&#21644;&#25903;&#20184;&#12290;&#29702;&#24615;&#30340;&#21442;&#19982;&#32773;&#35797;&#22270;&#26368;&#22823;&#21270;&#33258;&#24049;&#30340;&#20010;&#20307;&#25928;&#29992;&#65292;&#38500;&#38750;&#20182;&#20204;&#22522;&#20110;&#25968;&#25454;&#36136;&#37327;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#25903;&#20184;&#65292;&#21542;&#21017;&#20182;&#20204;&#23558;&#19981;&#20250;&#30495;&#23454;&#22320;&#36755;&#20837;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#32852;&#37030;&#23398;&#20064;&#21463;&#30410;&#20110;&#21442;&#19982;&#32773;&#30340;&#21512;&#20316;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#24314;&#31435;&#19968;&#20010;&#26082;&#28608;&#21169;&#30495;&#23454;&#36755;&#20837;&#25968;&#25454;&#21448;&#20419;&#36827;&#31283;&#23450;&#21512;&#20316;&#30340;&#28608;&#21169;&#26426;&#21046;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#30340;&#25968;&#25454;&#20849;&#20139;&#21338;&#24328;&#27169;&#22411;&#65292;&#24182;&#36816;&#29992;&#21338;&#24328;&#35770;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#19968;&#20010;&#36873;&#26680;&#28608;&#21169;&#26426;&#21046;&#65292;&#21033;&#29992;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#19968;&#20010;&#27969;&#34892;&#27010;&#24565;&#65292;&#21363;&#26680;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a distributed machine learning system that uses participants' data to train an improved global model. In federated learning, participants cooperatively train a global model, and they will receive the global model and payments. Rational participants try to maximize their individual utility, and they will not input their high-quality data truthfully unless they are provided with satisfactory payments based on their data quality. Furthermore, federated learning benefits from the cooperative contributions of participants. Accordingly, how to establish an incentive mechanism that both incentivizes inputting data truthfully and promotes stable cooperation has become an important issue to consider. In this paper, we introduce a data sharing game model for federated learning and employ game-theoretic approaches to design a core-selecting incentive mechanism by utilizing a popular concept in cooperative games, the core. In federated learning, the core can be empty, resulti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#39046;&#22495;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;EEG&#30340;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#20013;&#30340;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#20010;&#20307;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.11714</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;EEG&#30340;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#30340;&#21160;&#24577;&#39046;&#22495;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Dynamic Domain Adaptation Deep Learning Network for EEG-based Motor Imagery Classification. (arXiv:2309.11714v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#39046;&#22495;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;EEG&#30340;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#20013;&#30340;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#20010;&#20307;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#33041;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30456;&#37051;&#36890;&#36947;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#22914;&#20309;&#34920;&#31034;&#36825;&#31181;&#30456;&#20851;&#24615;&#26159;&#24403;&#21069;&#30740;&#31350;&#30340;&#19968;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;EEG&#20449;&#21495;&#22312;&#19981;&#21516;&#20010;&#20307;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#31181;&#24046;&#24322;&#23548;&#33268;&#26032;&#20027;&#20307;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#26657;&#20934;&#26102;&#38388;&#29992;&#20110;&#22522;&#20110;EEG&#30340;&#36816;&#21160;&#24819;&#35937;&#33041;&#26426;&#25509;&#21475;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#36866;&#24212;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65288;DADL-Net&#65289;&#12290;&#39318;&#20808;&#65292;&#23558;EEG&#25968;&#25454;&#26144;&#23556;&#21040;&#19977;&#32500;&#20960;&#20309;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;3D&#21367;&#31215;&#27169;&#22359;&#23398;&#20064;&#20854;&#26102;&#31354;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#31354;&#38388;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#24378;&#21270;&#29305;&#24449;&#65292;&#26368;&#21518;&#30340;&#21367;&#31215;&#27169;&#22359;&#21487;&#20197;&#36827;&#19968;&#27493;&#23398;&#20064;&#29305;&#24449;&#30340;&#26102;&#31354;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#32771;&#34385;&#20010;&#20307;&#38388;&#21644;&#36328;&#20250;&#35805;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#37319;&#29992;&#21160;&#24577;&#39046;&#22495;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#26368;&#22823;&#21270;&#36317;&#31163;&#26469;&#20943;&#23567;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a correlation between adjacent channels of electroencephalogram (EEG), and how to represent this correlation is an issue that is currently being explored. In addition, due to inter-individual differences in EEG signals, this discrepancy results in new subjects need spend a amount of calibration time for EEG-based motor imagery brain-computer interface. In order to solve the above problems, we propose a Dynamic Domain Adaptation Based Deep Learning Network (DADL-Net). First, the EEG data is mapped to the three-dimensional geometric space and its temporal-spatial features are learned through the 3D convolution module, and then the spatial-channel attention mechanism is used to strengthen the features, and the final convolution module can further learn the spatial-temporal information of the features. Finally, to account for inter-subject and cross-sessions differences, we employ a dynamic domain-adaptive strategy, the distance between features is reduced by introducing a Maximum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20934;&#33945;&#29305;&#21345;&#27931;&#65288;QMC&#65289;&#26041;&#27861;&#29992;&#20110;&#19977;&#32500;&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#30340;&#36817;&#20284;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#26041;&#27861;&#22312;&#19977;&#32500;&#21333;&#20301;&#36229;&#29699;&#38754;&#19978;&#26500;&#36896;&#20102;QMC&#28857;&#38598;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#23558;QSW&#25193;&#23637;&#20026;&#38543;&#26426;&#20934;&#20999;&#29255;Wasserstein&#65288;RQSW&#65289;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11713</link><description>&lt;p&gt;
&#19977;&#32500;&#20999;&#29255;Wasserstein&#30340;&#20934;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quasi-Monte Carlo for 3D Sliced Wasserstein. (arXiv:2309.11713v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20934;&#33945;&#29305;&#21345;&#27931;&#65288;QMC&#65289;&#26041;&#27861;&#29992;&#20110;&#19977;&#32500;&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#30340;&#36817;&#20284;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#26041;&#27861;&#22312;&#19977;&#32500;&#21333;&#20301;&#36229;&#29699;&#38754;&#19978;&#26500;&#36896;&#20102;QMC&#28857;&#38598;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#23558;QSW&#25193;&#23637;&#20026;&#38543;&#26426;&#20934;&#20999;&#29255;Wasserstein&#65288;RQSW&#65289;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Monte Carlo (MC)&#26041;&#27861;&#34987;&#29992;&#20316;&#35745;&#31639;&#20999;&#29255;Wasserstein (SW)&#36317;&#31163;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#22312;&#20998;&#26512;&#24418;&#24335;&#20013;&#20855;&#26377;&#26840;&#25163;&#30340;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;MC&#26041;&#27861;&#22312;&#26368;&#23567;&#21270;&#32477;&#23545;&#36817;&#20284;&#35823;&#24046;&#26041;&#38754;&#24182;&#19981;&#20248;&#21270;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#22909;&#30340;&#32463;&#39564;SW&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20934;&#33945;&#29305;&#21345;&#27931;&#65288;QMC&#65289;&#26041;&#27861;&#30340;&#20934;&#20999;&#29255;Wasserstein&#65288;QSW&#65289;&#36924;&#36817;&#12290;&#20026;&#20102;&#23545;SW&#30340;QMC&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19977;&#32500;&#35774;&#32622;&#65292;&#29305;&#21035;&#26159;&#35745;&#31639;&#19977;&#32500;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;SW&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#20102;&#22312;&#19977;&#32500;&#21333;&#20301;&#36229;&#29699;&#38754;&#19978;&#26500;&#36896;QMC&#28857;&#38598;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#39640;&#26031;&#30340;&#26144;&#23556;&#65292;&#31561;&#38754;&#31215;&#26144;&#23556;&#65292;&#24191;&#20041;&#34746;&#26059;&#28857;&#21644;&#26368;&#20248;&#21270;&#24046;&#24322;&#33021;&#37327;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#33719;&#24471;&#38543;&#26426;&#20248;&#21270;&#30340;&#26080;&#20559;&#20272;&#35745;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#25152;&#35752;&#35770;&#30340;&#20302;&#32500;&#35774;&#32622;&#20013;&#24341;&#20837;&#38543;&#26426;&#24615;&#65292;&#23558;QSW&#25193;&#23637;&#20026;&#38543;&#26426;&#20934;&#20999;&#29255;Wasserstein&#65288;RQSW&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo (MC) approximation has been used as the standard computation approach for the Sliced Wasserstein (SW) distance, which has an intractable expectation in its analytical form. However, the MC method is not optimal in terms of minimizing the absolute approximation error. To provide a better class of empirical SW, we propose quasi-sliced Wasserstein (QSW) approximations that rely on Quasi-Monte Carlo (QMC) methods. For a comprehensive investigation of QMC for SW, we focus on the 3D setting, specifically computing the SW between probability measures in three dimensions. In greater detail, we empirically verify various ways of constructing QMC points sets on the 3D unit-hypersphere, including Gaussian-based mapping, equal area mapping, generalized spiral points, and optimizing discrepancy energies. Furthermore, to obtain an unbiased estimation for stochastic optimization, we extend QSW into Randomized Quasi-Sliced Wasserstein (RQSW) by introducing randomness to the discussed low-d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#25345;&#32493;&#36866;&#24212;&#24615;OOD&#26816;&#27979;&#27169;&#22411;&#65292;&#38024;&#23545;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;ID&#21644;OOD&#20998;&#24067;&#25345;&#32493;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#37096;&#32626;&#26102;&#21160;&#24577;&#24555;&#36895;&#22320;&#36866;&#24212;&#26032;&#21040;&#36798;&#30340;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#37096;&#32626;&#26399;&#38388;ID&#26679;&#26412;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2309.11705</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#25345;&#32493;&#36866;&#24212;&#24615;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Meta OOD Learning for Continuously Adaptive OOD Detection. (arXiv:2309.11705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#25345;&#32493;&#36866;&#24212;&#24615;OOD&#26816;&#27979;&#27169;&#22411;&#65292;&#38024;&#23545;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;ID&#21644;OOD&#20998;&#24067;&#25345;&#32493;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#37096;&#32626;&#26102;&#21160;&#24577;&#24555;&#36895;&#22320;&#36866;&#24212;&#26032;&#21040;&#36798;&#30340;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#37096;&#32626;&#26399;&#38388;ID&#26679;&#26412;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#35686;&#25253;&#19981;&#24212;&#34987;&#27979;&#35797;&#25110;&#29992;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;OOD&#26679;&#26412;&#26159;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#20851;&#38190;&#12290;&#30446;&#21069;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#20174;&#38745;&#24577;&#20998;&#24067;&#20013;&#25552;&#21462;&#30340;ID&#21644;OOD&#26679;&#26412;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#26102;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;ID&#21644;OOD&#20998;&#24067;&#24448;&#24448;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21457;&#29983;&#25345;&#32493;&#30340;&#21464;&#21270;&#21644;&#36716;&#31227;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#26377;&#25928;&#24212;&#29992;&#65292;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#36866;&#24212;&#36825;&#20123;&#21160;&#24577;&#21644;&#28436;&#21270;&#20998;&#24067;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26356;&#36924;&#30495;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#25345;&#32493;&#36866;&#24212;&#24615;OOD&#26816;&#27979;&#65288;CAOOD&#65289;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;OOD&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#37096;&#32626;&#26102;&#21160;&#24577;&#24555;&#36895;&#22320;&#36866;&#24212;&#26032;&#21040;&#36798;&#30340;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#37096;&#32626;&#26399;&#38388;ID&#26679;&#26412;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;CAOOD&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is crucial to modern deep learning applications by identifying and alerting about the OOD samples that should not be tested or used for making predictions. Current OOD detection methods have made significant progress when in-distribution (ID) and OOD samples are drawn from static distributions. However, this can be unrealistic when applied to real-world systems which often undergo continuous variations and shifts in ID and OOD distributions over time. Therefore, for an effective application in real-world systems, the development of OOD detection methods that can adapt to these dynamic and evolving distributions is essential. In this paper, we propose a novel and more realistic setting called continuously adaptive out-of-distribution (CAOOD) detection which targets on developing an OOD detection model that enables dynamic and quick adaptation to a new arriving distribution, with insufficient ID samples during deployment time. To address CAOOD, we deve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#40723;&#21169;&#24335;&#36890;&#20449;&#38382;&#39064;&#65292;&#21363;&#38024;&#23545;&#33258;&#21033;&#30340;&#32852;&#37030;&#36172;&#33218;&#26426;&#20013;&#65292;&#26381;&#21153;&#22120;&#36890;&#36807;&#25552;&#20379;&#28608;&#21169;&#26469;&#20419;&#20351;&#23458;&#25143;&#26426;&#20998;&#20139;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#23454;&#38469;&#25805;&#20316;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11702</link><description>&lt;p&gt;
&#40723;&#21169;&#24335;&#27807;&#36890;&#30340;&#32852;&#37030;&#36172;&#33218;&#26426;
&lt;/p&gt;
&lt;p&gt;
Incentivized Communication for Federated Bandits. (arXiv:2309.11702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#40723;&#21169;&#24335;&#36890;&#20449;&#38382;&#39064;&#65292;&#21363;&#38024;&#23545;&#33258;&#21033;&#30340;&#32852;&#37030;&#36172;&#33218;&#26426;&#20013;&#65292;&#26381;&#21153;&#22120;&#36890;&#36807;&#25552;&#20379;&#28608;&#21169;&#26469;&#20419;&#20351;&#23458;&#25143;&#26426;&#20998;&#20139;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#23454;&#38469;&#25805;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20851;&#20110;&#32852;&#37030;&#36172;&#33218;&#26426;&#30340;&#24037;&#20316;&#37117;&#40664;&#35748;&#25152;&#26377;&#23458;&#25143;&#26426;&#37117;&#24895;&#24847;&#22312;&#38656;&#35201;&#30340;&#26102;&#20505;&#23558;&#20854;&#25968;&#25454;&#26080;&#31169;&#22320;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#20197;&#33719;&#21462;&#38598;&#20307;&#21033;&#30410;&#12290;&#23613;&#31649;&#36825;&#31181;&#20551;&#35774;&#22312;&#24615;&#33021;&#21644;&#36890;&#20449;&#25928;&#29575;&#19978;&#26377;&#20196;&#20154;&#20449;&#26381;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#36807;&#20110;&#29702;&#24819;&#21270;&#65292;&#24182;&#19988;&#32463;&#24120;&#34987;&#36829;&#32972;&#65292;&#29305;&#21035;&#26159;&#24403;&#31639;&#27861;&#22312;&#33258;&#21033;&#30340;&#23458;&#25143;&#26426;&#19978;&#36816;&#34892;&#26102;&#65292;&#36825;&#20123;&#23458;&#25143;&#26426;&#19981;&#24895;&#24847;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#22909;&#22788;&#30340;&#24773;&#20917;&#19979;&#20998;&#20139;&#25968;&#25454;&#12290;&#24573;&#35270;&#36825;&#31181;&#33258;&#21033;&#34892;&#20026;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;&#32852;&#37030;&#36172;&#33218;&#26426;&#23398;&#20064;&#30340;&#25928;&#29575;&#29978;&#33267;&#23454;&#38469;&#21487;&#25805;&#20316;&#24615;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#27491;&#24335;&#24341;&#20837;&#19968;&#31181;&#40723;&#21169;&#24335;&#36890;&#20449;&#38382;&#39064;&#26469;&#20026;&#36825;&#20010;&#26410;&#34987;&#20805;&#20998;&#24320;&#21457;&#30340;&#30740;&#31350;&#39046;&#22495;&#24102;&#26469;&#26032;&#30340;&#35265;&#35299;&#65292;&#26381;&#21153;&#22120;&#36890;&#36807;&#25552;&#20379;&#28608;&#21169;&#26469;&#28608;&#21169;&#23458;&#25143;&#26426;&#20998;&#20139;&#25968;&#25454;&#12290;&#22312;&#19981;&#22833;&#19968;&#33324;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36172;&#33218;&#26426;&#38382;&#39064;&#23454;&#20363;&#21270;&#20026;&#19978;&#19979;&#25991;&#32447;&#24615;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing works on federated bandits take it for granted that all clients are altruistic about sharing their data with the server for the collective good whenever needed. Despite their compelling theoretical guarantee on performance and communication efficiency, this assumption is overly idealistic and oftentimes violated in practice, especially when the algorithm is operated over self-interested clients, who are reluctant to share data without explicit benefits. Negligence of such self-interested behaviors can significantly affect the learning efficiency and even the practical operability of federated bandit learning. In light of this, we aim to spark new insights into this under-explored research area by formally introducing an incentivized communication problem for federated bandits, where the server shall motivate clients to share data by providing incentives. Without loss of generality, we instantiate this bandit problem with the contextual linear setting and propose the first
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25552;&#39640;&#20998;&#23376;&#34394;&#25311;&#31579;&#36873;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.11687</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25913;&#21892;&#20102;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#20998;&#23376;&#34394;&#25311;&#31579;&#36873;&#30340;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Large-scale Pretraining Improves Sample Efficiency of Active Learning based Molecule Virtual Screening. (arXiv:2309.11687v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25552;&#39640;&#20998;&#23376;&#34394;&#25311;&#31579;&#36873;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#35268;&#27169;&#21270;&#21512;&#29289;&#24211;&#36827;&#34892;&#34394;&#25311;&#31579;&#36873;&#20197;&#23547;&#25214;&#28508;&#22312;&#30340;&#21629;&#20013;&#20505;&#36873;&#29289;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#26159;&#26368;&#26089;&#30340;&#27493;&#39588;&#20043;&#19968;&#12290;&#38543;&#30528;&#21830;&#19994;&#21487;&#24471;&#21270;&#21512;&#29289;&#24211;&#30340;&#35268;&#27169;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20351;&#29992;&#20256;&#32479;&#24037;&#20855;&#22914;&#23545;&#25509;&#36827;&#34892;&#26292;&#21147;&#34394;&#25311;&#31579;&#36873;&#22312;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#26041;&#38754;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#26368;&#36817;&#65292;&#20027;&#21160;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#34987;&#35777;&#26126;&#26159;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#20351;&#29992;&#23567;&#22411;&#24211;&#23376;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#26367;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21270;&#21512;&#29289;&#30340;&#25152;&#38656;&#29305;&#24615;&#12290;&#20934;&#30830;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20165;&#34394;&#25311;&#31579;&#36873;&#25972;&#20010;&#24211;&#30340;&#19968;&#23567;&#37096;&#20998;&#26469;&#23454;&#29616;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#21457;&#29616;&#26368;&#26377;&#21069;&#36884;&#30340;&#21270;&#21512;&#29289;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#20013;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual screening of large compound libraries to identify potential hit candidates is one of the earliest steps in drug discovery. As the size of commercially available compound collections grows exponentially to the scale of billions, brute-force virtual screening using traditional tools such as docking becomes infeasible in terms of time and computational resources. Active learning and Bayesian optimization has recently been proven as effective methods of narrowing down the search space. An essential component in those methods is a surrogate machine learning model that is trained with a small subset of the library to predict the desired properties of compounds. Accurate model can achieve high sample efficiency by finding the most promising compounds with only a fraction of the whole library being virtually screened. In this study, we examined the performance of pretrained transformer-based language model and graph neural network in Bayesian optimization active learning framework. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38543;&#26426;&#20998;&#24067;&#40065;&#26834;&#30340;&#20844;&#24179;&#24615;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#26102;&#20844;&#24179;&#27169;&#22411;&#34920;&#29616;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#30693;&#36947;&#22240;&#26524;&#22270;&#65292;&#20063;&#25903;&#25345;&#20351;&#29992;&#23567;&#25209;&#37327;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.11682</link><description>&lt;p&gt;
Dr. FERMI&#65306;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20998;&#24067;&#40065;&#26834;&#30340;&#20844;&#24179;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework. (arXiv:2309.11682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38543;&#26426;&#20998;&#24067;&#40065;&#26834;&#30340;&#20844;&#24179;&#24615;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#26102;&#20844;&#24179;&#27169;&#22411;&#34920;&#29616;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#30693;&#36947;&#22240;&#26524;&#22270;&#65292;&#20063;&#25903;&#25345;&#20351;&#29992;&#23567;&#25209;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#36817;&#20960;&#24180;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#35757;&#32451;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#30456;&#20284;&#30340;&#20998;&#24067;&#30340;&#20551;&#35774;&#12290;&#22312;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20844;&#24179;&#27169;&#22411;&#21487;&#33021;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20844;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38024;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#20844;&#24179;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#37117;&#22522;&#20110;&#20855;&#26377;&#25551;&#36848;&#19981;&#21516;&#29305;&#24449;&#20132;&#20114;&#30340;&#22240;&#26524;&#22270;&#30340;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#38656;&#35201;&#23436;&#20840;&#35775;&#38382;&#25968;&#25454;&#65292;&#19981;&#33021;&#22312;&#20351;&#29992;&#23567;&#25209;&#37327;&#65288;&#38543;&#26426;/&#25209;&#37327;&#23454;&#29616;&#65289;&#26102;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#38543;&#26426;&#20998;&#24067;&#40065;&#26834;&#20844;&#24179;&#24615;&#26694;&#26550;&#65292;&#19981;&#38656;&#35201;&#23545;&#22240;&#26524;&#22270;&#26377;&#20219;&#20309;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#22312;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#30340;&#20844;&#24179;&#25512;&#26029;&#38382;&#39064;&#21046;&#23450;&#20026;$L_p$-&#33539;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
While training fair machine learning models has been studied extensively in recent years, most developed methods rely on the assumption that the training and test data have similar distributions. In the presence of distribution shifts, fair models may behave unfairly on test data. There have been some developments for fair learning robust to distribution shifts to address this shortcoming. However, most proposed solutions are based on the assumption of having access to the causal graph describing the interaction of different features. Moreover, existing algorithms require full access to data and cannot be used when small batches are used (stochastic/batch implementation). This paper proposes the first stochastic distributionally robust fairness framework with convergence guarantees that do not require knowledge of the causal graph. More specifically, we formulate the fair inference in the presence of the distribution shift as a distributionally robust optimization problem under $L_p$ n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNGMs&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#31070;&#32463;&#22270;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11680</link><description>&lt;p&gt;
&#20855;&#26377;&#31070;&#32463;&#22270;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Neural Graphical Models. (arXiv:2309.11680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNGMs&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#31070;&#32463;&#22270;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#35299;&#20915;&#20102;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#20445;&#30041;&#23545;&#25968;&#25454;&#30340;&#29420;&#21344;&#25511;&#21046;&#30340;&#21516;&#26102;&#65292;&#22522;&#20110;&#19987;&#26377;&#25968;&#25454;&#21019;&#24314;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#36817;&#26399;&#25552;&#20986;&#30340;&#31070;&#32463;&#22270;&#27169;&#22411;&#65288;NGMs&#65289;&#26159;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#23398;&#20064;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;&#23427;&#20204;&#23398;&#20250;&#25429;&#25417;&#24213;&#23618;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#30340;&#25512;&#29702;&#21644;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;FL&#26694;&#26550;&#65292;&#23427;&#32500;&#25252;&#19968;&#20010;&#20840;&#23616;&#30340;NGM&#27169;&#22411;&#65292;&#20174;&#26412;&#22320;NGM&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#24179;&#22343;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#22312;&#23458;&#25143;&#31471;&#30340;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;FedNGMs&#36991;&#20813;&#20102;&#31070;&#32463;&#20803;&#21305;&#37197;&#26694;&#26550;&#65288;&#22914;&#32852;&#37030;&#21305;&#37197;&#24179;&#22343;&#65289;&#20013;&#27169;&#22411;&#21442;&#25968;&#29190;&#28856;&#30340;&#32570;&#28857;&#21644;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#20840;&#23616;&#27169;&#22411;&#22823;&#23567;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) addresses the need to create models based on proprietary data in such a way that multiple clients retain exclusive control over their data, while all benefit from improved model accuracy due to pooled resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic Graphical models that utilize the expressive power of neural networks to learn complex non-linear dependencies between the input features. They learn to capture the underlying data distribution and have efficient algorithms for inference and sampling. We develop a FL framework which maintains a global NGM model that learns the averaged information from the local NGM models while keeping the training data within the client's environment. Our design, FedNGMs, avoids the pitfalls and shortcomings of neuron matching frameworks like Federated Matched Averaging that suffers from model parameter explosion. Our global model size remains constant throughout the process. In the cases where clients 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#34892;&#24230;&#34928;&#20943;&#20559;&#24046;&#23545;&#26412;&#22320;&#38899;&#20048;&#25512;&#33616;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#25512;&#33616;&#27969;&#34892;&#33402;&#26415;&#23478;&#26041;&#38754;&#65292;&#26435;&#37325;&#30456;&#20851;&#30697;&#38453;&#20998;&#35299;&#21644;&#22810;&#39033;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#23545;&#20110;&#19981;&#22826;&#27969;&#34892;&#30340;&#33402;&#26415;&#23478;&#26469;&#35828;&#65292;&#22810;&#39033;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#30456;&#23545;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.11671</link><description>&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26412;&#22320;&#38899;&#20048;&#25512;&#33616;&#20013;&#30340;&#27969;&#34892;&#24230;&#34928;&#20943;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Popularity Degradation Bias in Local Music Recommendation. (arXiv:2309.11671v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#34892;&#24230;&#34928;&#20943;&#20559;&#24046;&#23545;&#26412;&#22320;&#38899;&#20048;&#25512;&#33616;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#25512;&#33616;&#27969;&#34892;&#33402;&#26415;&#23478;&#26041;&#38754;&#65292;&#26435;&#37325;&#30456;&#20851;&#30697;&#38453;&#20998;&#35299;&#21644;&#22810;&#39033;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#23545;&#20110;&#19981;&#22826;&#27969;&#34892;&#30340;&#33402;&#26415;&#23478;&#26469;&#35828;&#65292;&#22810;&#39033;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#30456;&#23545;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#34892;&#24230;&#34928;&#20943;&#20559;&#24046;&#23545;&#26412;&#22320;&#38899;&#20048;&#25512;&#33616;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26816;&#39564;&#20102;&#20004;&#31181;&#26368;&#20339;&#25512;&#33616;&#31639;&#27861;&#65292;&#21363;&#26435;&#37325;&#30456;&#20851;&#30697;&#38453;&#20998;&#35299;&#65288;WRMF&#65289;&#21644;&#22810;&#39033;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;Mult-VAE&#65289;&#22312;&#25512;&#33616;&#33402;&#26415;&#23478;&#26102;&#30340;&#20934;&#30830;&#24615;&#19982;&#33402;&#26415;&#23478;&#27969;&#34892;&#24230;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20004;&#20010;&#31639;&#27861;&#22312;&#25512;&#33616;&#27969;&#34892;&#33402;&#26415;&#23478;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#34920;&#29616;&#20986;&#27969;&#34892;&#24230;&#34928;&#20943;&#20559;&#24046;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#31639;&#27861;&#22312;&#25512;&#33616;&#27969;&#34892;&#33402;&#26415;&#23478;&#26041;&#38754;&#30340;&#34920;&#29616;&#27700;&#24179;&#30456;&#20284;&#65292;&#20294;&#23545;&#20110;&#19981;&#22826;&#27969;&#34892;&#30340;&#33402;&#26415;&#23478;&#26469;&#35828;&#65292;Mult-VAE&#30456;&#23545;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#34920;&#26126;&#22312;&#26412;&#22320;&#65288;&#38271;&#23614;&#65289;&#38899;&#20048;&#33402;&#26415;&#23478;&#25512;&#33616;&#20013;&#65292;&#24212;&#35813;&#20248;&#20808;&#36873;&#25321;&#36825;&#20010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the effect of popularity degradation bias in the context of local music recommendations. Specifically, we examine how accurate two top-performing recommendation algorithms, Weight Relevance Matrix Factorization (WRMF) and Multinomial Variational Autoencoder (Mult-VAE), are at recommending artists as a function of artist popularity. We find that both algorithms improve recommendation performance for more popular artists and, as such, exhibit popularity degradation bias. While both algorithms produce a similar level of performance for more popular artists, Mult-VAE shows better relative performance for less popular artists. This suggests that this algorithm should be preferred for local (long-tail) music artist recommendation.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#22238;&#24402;&#38382;&#39064;&#20013;&#22788;&#29702;&#21152;&#27861;&#26080;&#24847;&#35782;&#22122;&#22768;&#30340;&#31639;&#27861;&#12290;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#26679;&#26412;&#35775;&#38382;&#26469;&#20934;&#30830;&#22320;&#24674;&#22797;&#21442;&#25968;&#21521;&#37327;&#65292;&#20351;&#24471;&#27169;&#22411;&#30340;&#39044;&#27979;&#19982;&#30495;&#23454;&#20540;&#30340;&#35823;&#24046;&#23613;&#21487;&#33021;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.11657</link><description>&lt;p&gt;
GLM&#22238;&#24402;&#19982;&#26080;&#24847;&#35782;&#25968;&#25454;&#25439;&#22351;
&lt;/p&gt;
&lt;p&gt;
GLM Regression with Oblivious Corruptions. (arXiv:2309.11657v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11657
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#22238;&#24402;&#38382;&#39064;&#20013;&#22788;&#29702;&#21152;&#27861;&#26080;&#24847;&#35782;&#22122;&#22768;&#30340;&#31639;&#27861;&#12290;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#26679;&#26412;&#35775;&#38382;&#26469;&#20934;&#30830;&#22320;&#24674;&#22797;&#21442;&#25968;&#21521;&#37327;&#65292;&#20351;&#24471;&#27169;&#22411;&#30340;&#39044;&#27979;&#19982;&#30495;&#23454;&#20540;&#30340;&#35823;&#24046;&#23613;&#21487;&#33021;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLMs&#65289;&#30340;&#22238;&#24402;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#21152;&#27861;&#26080;&#24847;&#35782;&#22122;&#22768;&#30340;&#31532;&#19968;&#20010;&#31639;&#27861;&#12290;&#25105;&#20204;&#20551;&#35774;&#25105;&#20204;&#26377;&#26679;&#26412;&#35775;&#38382;&#21040;&#30340;&#20363;&#23376;$(x, y)$&#65292;&#20854;&#20013;$y$&#26159;$g(w^* \cdot x)$&#30340;&#24102;&#22122;&#22768;&#27979;&#37327;&#20540;&#12290;&#29305;&#21035;&#22320;&#65292;&#22122;&#22768;&#26631;&#31614;&#30340;&#24418;&#24335;&#20026;$y = g(w^* \cdot x) + \xi + \epsilon$&#65292;&#20854;&#20013;$\xi$&#26159;&#19982;$x$&#29420;&#31435;&#25277;&#21462;&#30340;&#26080;&#24847;&#35782;&#22122;&#22768;&#28385;&#36275;$\Pr[\xi = 0] \geq o(1)$&#65292;&#32780;$\epsilon \sim \mathcal N(0, \sigma^2)$&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20934;&#30830;&#22320;&#24674;&#22797;&#19968;&#20010;&#21442;&#25968;&#21521;&#37327;$w$&#65292;&#20351;&#24471;&#20989;&#25968;$g(w \cdot x)$&#19982;&#30495;&#23454;&#20540;$g(w^* \cdot x)$&#30456;&#27604;&#20855;&#26377;&#20219;&#24847;&#23567;&#30340;&#35823;&#24046;&#65292;&#32780;&#19981;&#26159;&#19982;&#22122;&#22768;&#27979;&#37327;$y$&#30456;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#26368;&#19968;&#33324;&#30340;&#19982;&#20998;&#24067;&#26080;&#20851;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#35299;&#21487;&#33021;&#29978;&#33267;&#19981;&#21487;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36820;&#22238;&#19968;&#20010;&#20934;&#30830;&#30340;&#20272;&#35745;&#65292;&#22914;&#26524;&#23427;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#21542;&#21017;
&lt;/p&gt;
&lt;p&gt;
We demonstrate the first algorithms for the problem of regression for generalized linear models (GLMs) in the presence of additive oblivious noise. We assume we have sample access to examples $(x, y)$ where $y$ is a noisy measurement of $g(w^* \cdot x)$. In particular, \new{the noisy labels are of the form} $y = g(w^* \cdot x) + \xi + \epsilon$, where $\xi$ is the oblivious noise drawn independently of $x$ \new{and satisfies} $\Pr[\xi = 0] \geq o(1)$, and $\epsilon \sim \mathcal N(0, \sigma^2)$. Our goal is to accurately recover a \new{parameter vector $w$ such that the} function $g(w \cdot x)$ \new{has} arbitrarily small error when compared to the true values $g(w^* \cdot x)$, rather than the noisy measurements $y$.  We present an algorithm that tackles \new{this} problem in its most general distribution-independent setting, where the solution may not \new{even} be identifiable. \new{Our} algorithm returns \new{an accurate estimate of} the solution if it is identifiable, and otherwise
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#28418;&#31227;&#25511;&#21046;&#39640;&#32500;RBMs&#12290;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11651</link><description>&lt;p&gt;
&#39640;&#32500;RBM&#30340;&#28418;&#31227;&#25511;&#21046;&#65306;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Drift Control of High-Dimensional RBM: A Computational Method Based on Neural Networks. (arXiv:2309.11651v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11651
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#28418;&#31227;&#25511;&#21046;&#39640;&#32500;RBMs&#12290;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#25490;&#38431;&#29702;&#35770;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29366;&#24577;&#31354;&#38388;&#20026;d&#32500;&#27491;&#21322;&#36724;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#12290;&#25511;&#21046;&#36807;&#31243;Z&#25353;&#29031;&#19968;&#20010;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#28436;&#21270;&#65292;&#20854;&#21327;&#26041;&#24046;&#30697;&#38453;&#26159;&#22806;&#29983;&#25351;&#23450;&#30340;&#65292;&#21453;&#23556;&#26041;&#21521;&#26159;&#20174;&#27491;&#21322;&#36724;&#36793;&#30028;&#34920;&#38754;&#21453;&#23556;&#12290;&#31995;&#32479;&#31649;&#29702;&#21592;&#26681;&#25454;Z&#30340;&#21382;&#21490;&#36873;&#25321;&#27599;&#20010;&#26102;&#38388;&#28857;t&#19978;&#30340;&#28418;&#31227;&#21521;&#37327;&#952;(t)&#65292;&#32780;&#26102;&#38388;&#28857;t&#19978;&#30340;&#25104;&#26412;&#29575;&#21462;&#20915;&#20110;Z(t)&#21644;&#952;(t)&#12290;&#22312;&#25105;&#20204;&#30340;&#21021;&#22987;&#38382;&#39064;&#34920;&#36848;&#20013;&#65292;&#30446;&#26631;&#26159;&#22312;&#26080;&#38480;&#35268;&#21010;&#26102;&#38388;&#33539;&#22260;&#20869;&#26368;&#23567;&#21270;&#26399;&#26395;&#36148;&#29616;&#25104;&#26412;&#65292;&#20043;&#21518;&#25105;&#20204;&#22788;&#29702;&#30456;&#24212;&#30340;&#20154;&#22343;&#25511;&#21046;&#38382;&#39064;&#12290;&#20511;&#37492;&#38889;&#28023;&#20142;&#31561;&#20154;&#65288;&#22269;&#23478;&#31185;&#23398;&#38498;&#23398;&#25253;&#65292;2018, 8505-8510&#65289;&#30340;&#26089;&#26399;&#24037;&#20316;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#25105;&#20204;&#30740;&#31350;&#30340;&#27979;&#35797;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#31934;&#24230;&#22312;&#19968;&#20010;&#23567;&#25968;&#33539;&#22260;&#20869;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by applications in queueing theory, we consider a stochastic control problem whose state space is the $d$-dimensional positive orthant. The controlled process $Z$ evolves as a reflected Brownian motion whose covariance matrix is exogenously specified, as are its directions of reflection from the orthant's boundary surfaces. A system manager chooses a drift vector $\theta(t)$ at each time $t$ based on the history of $Z$, and the cost rate at time $t$ depends on both $Z(t)$ and $\theta(t)$. In our initial problem formulation, the objective is to minimize expected discounted cost over an infinite planning horizon, after which we treat the corresponding ergodic control problem. Extending earlier work by Han et al. (Proceedings of the National Academy of Sciences, 2018, 8505-8510), we develop and illustrate a simulation-based computational method that relies heavily on deep neural network technology. For test problems studied thus far, our method is accurate to within a fraction o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23548;&#33322;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#36712;&#36947;&#19978;&#30340;&#21487;&#35265;&#27874;&#38271;&#25668;&#20687;&#22836;&#20316;&#20026;&#20027;&#35201;&#20256;&#24863;&#22120;&#65292;&#20943;&#23569;&#23545;&#28608;&#20809;&#38647;&#36798;&#30340;&#20381;&#36182;&#65292;&#24182;&#22823;&#22823;&#38477;&#20302;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.11648</link><description>&lt;p&gt;
&#22522;&#20110;&#36712;&#36947;&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#20027;&#21152;&#27833;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Orbital AI-based Autonomous Refuelling Solution. (arXiv:2309.11648v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23548;&#33322;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#36712;&#36947;&#19978;&#30340;&#21487;&#35265;&#27874;&#38271;&#25668;&#20687;&#22836;&#20316;&#20026;&#20027;&#35201;&#20256;&#24863;&#22120;&#65292;&#20943;&#23569;&#23545;&#28608;&#20809;&#38647;&#36798;&#30340;&#20381;&#36182;&#65292;&#24182;&#22823;&#22823;&#38477;&#20302;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#23567;&#22411;&#23610;&#23544;&#21644;&#20302;&#25104;&#26412;&#30340;&#21151;&#29575;&#12289;&#36136;&#37327;&#21644;&#20307;&#31215;&#65292;&#25668;&#20687;&#22836;&#27491;&#36805;&#36895;&#25104;&#20026;&#22826;&#31354;&#20132;&#20250;&#30340;&#36873;&#25321;&#26426;&#36733;&#20256;&#24863;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#25509;&#26041;&#38754;&#65292;&#23427;&#20204;&#36890;&#24120;&#36215;&#21040;&#27425;&#35201;&#20316;&#29992;&#65292;&#32780;&#20027;&#35201;&#24037;&#20316;&#30001;&#28608;&#20809;&#38647;&#36798;&#31561;&#20027;&#21160;&#20256;&#24863;&#22120;&#23436;&#25104;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25552;&#20986;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23548;&#33322;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#26088;&#22312;&#20351;&#33337;&#36733;&#21487;&#35265;&#27874;&#38271;&#25668;&#20687;&#22836;&#20316;&#20026;&#23545;&#25509;&#21644;&#36712;&#36947;&#26381;&#21153;&#30340;&#20027;&#35201;&#20256;&#24863;&#22120;&#25104;&#29087;&#36215;&#26469;&#65292;&#20943;&#23569;&#23545;&#28608;&#20809;&#38647;&#36798;&#30340;&#20381;&#36182;&#24182;&#22823;&#22823;&#38477;&#20302;&#25104;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20351;&#24471;&#30456;&#23545;&#23548;&#33322;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#25193;&#23637;&#21040;&#22810;&#31181;&#24773;&#20917;&#65292;&#20363;&#22914;&#30446;&#26631;&#25110;&#29031;&#26126;&#26465;&#20214;&#65292;&#22312;&#20256;&#32479;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#20013;&#65292;&#36825;&#20123;&#24773;&#20917;&#37117;&#24517;&#39035;&#36827;&#34892;&#20010;&#26696;&#22788;&#29702;&#12290;&#22312;&#21512;&#25104;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#23545;&#22810;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#39592;&#24178;&#26550;&#26500;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cameras are rapidly becoming the choice for on-board sensors towards space rendezvous due to their small form factor and inexpensive power, mass, and volume costs. When it comes to docking, however, they typically serve a secondary role, whereas the main work is done by active sensors such as lidar. This paper documents the development of a proposed AI-based (artificial intelligence) navigation algorithm intending to mature the use of on-board visible wavelength cameras as a main sensor for docking and on-orbit servicing (OOS), reducing the dependency on lidar and greatly reducing costs. Specifically, the use of AI enables the expansion of the relative navigation solution towards multiple classes of scenarios, e.g., in terms of targets or illumination conditions, which would otherwise have to be crafted on a case-by-case manner using classical image processing methods. Multiple convolutional neural network (CNN) backbone architectures are benchmarked on synthetically generated data of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#22312;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#19982;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#30830;&#31435;&#20102;&#20854;&#39640;&#25928;&#21435;&#37327;&#21270;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;PQC&#26550;&#26500;&#35774;&#35745;&#24314;&#35758;&#21644;&#35782;&#21035;&#20102;&#28508;&#22312;&#37327;&#23376;&#20248;&#21183;&#30340;&#24517;&#35201;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.11647</link><description>&lt;p&gt;
&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#22312;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#19982;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Potential and limitations of random Fourier features for dequantizing quantum machine learning. (arXiv:2309.11647v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#22312;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#19982;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#30830;&#31435;&#20102;&#20854;&#39640;&#25928;&#21435;&#37327;&#21270;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;PQC&#26550;&#26500;&#35774;&#35745;&#24314;&#35758;&#21644;&#35782;&#21035;&#20102;&#28508;&#22312;&#37327;&#23376;&#20248;&#21183;&#30340;&#24517;&#35201;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26159;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#26368;&#24191;&#27867;&#25506;&#32034;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#30446;&#21069;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#21464;&#20998;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#20854;&#20013;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#34987;&#29992;&#20316;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#27169;&#22411;&#20855;&#26377;&#20016;&#23500;&#30340;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#33021;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#36827;&#34892;&#39640;&#25928;&#30340;&#21435;&#37327;&#21270;&#12290;&#26412;&#25991;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#30830;&#31435;&#20102;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#22312;&#21464;&#20998;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#25552;&#20379;&#39640;&#25928;&#21435;&#37327;&#21270;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#26550;&#26500;&#35774;&#35745;&#24314;&#35758;&#65292;&#20197;&#21450;&#35782;&#21035;&#20102;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#21462;&#24471;&#28508;&#22312;&#37327;&#23376;&#20248;&#21183;&#30340;&#24517;&#35201;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning is arguably one of the most explored applications of near-term quantum devices. Much focus has been put on notions of variational quantum machine learning where parameterized quantum circuits (PQCs) are used as learning models. These PQC models have a rich structure which suggests that they might be amenable to efficient dequantization via random Fourier features (RFF). In this work, we establish necessary and sufficient conditions under which RFF does indeed provide an efficient dequantization of variational quantum machine learning for regression. We build on these insights to make concrete suggestions for PQC architecture design, and to identify structures which are necessary for a regression problem to admit a potential quantum advantage via PQC based optimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#25552;&#20379;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#30340;&#26089;&#26399;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#31639;&#27861;&#23547;&#25214;&#26368;&#26174;&#33879;&#30340;&#29305;&#24449;&#65292;&#33258;&#21160;&#21270;&#35786;&#26029;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.11646</link><description>&lt;p&gt;
&#26089;&#26399;&#35786;&#26029;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Early diagnosis of autism spectrum disorder using machine learning approaches. (arXiv:2309.11646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#25552;&#20379;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#30340;&#26089;&#26399;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#31639;&#27861;&#23547;&#25214;&#26368;&#26174;&#33879;&#30340;&#29305;&#24449;&#65292;&#33258;&#21160;&#21270;&#35786;&#26029;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#26159;&#19968;&#31181;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#34920;&#29616;&#20026;&#31038;&#20132;&#20114;&#21160;&#22256;&#38590;&#12289;&#35821;&#35328;&#27807;&#36890;&#22256;&#38590;&#21644;&#37325;&#22797;&#34892;&#20026;&#12290;&#36825;&#20123;&#22256;&#38590;&#30340;&#20005;&#37325;&#31243;&#24230;&#21508;&#19981;&#30456;&#21516;&#65292;&#34987;&#35786;&#26029;&#20026;ASD&#30340;&#20154;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#23613;&#26089;&#35782;&#21035;&#21644;&#22788;&#29702;ASD&#21487;&#20197;&#20419;&#36827;&#35813;&#30142;&#30149;&#30340;&#25913;&#21892;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#35786;&#26029;&#20316;&#20026;&#20256;&#32479;&#20020;&#24202;&#26041;&#27861;&#30340;&#34917;&#20805;&#20986;&#29616;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#28508;&#22312;&#32570;&#28857;&#12290;&#26412;&#25991;&#21033;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23547;&#25214;ASD&#30340;&#26368;&#26174;&#33879;&#29305;&#24449;&#24182;&#33258;&#21160;&#21270;&#35786;&#26029;&#36807;&#31243;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20845;&#31181;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#25214;&#21040;&#26368;&#36866;&#21512;&#35782;&#21035;ASD&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#20116;&#31181;&#27969;&#34892;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20197;&#23545;&#36825;&#20123;ASD&#25968;&#25454;&#38598;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autistic Spectrum Disorder (ASD) is a neurological disease characterized by difficulties with social interaction, communication, and repetitive activities. The severity of these difficulties varies, and those with this diagnosis face unique challenges. While its primary origin lies in genetics, identifying and addressing it early can contribute to the enhancement of the condition. In recent years, machine learning-driven intelligent diagnosis has emerged as a supplement to conventional clinical approaches, aiming to address the potential drawbacks of time-consuming and costly traditional methods. In this work, we utilize different machine learning algorithms to find the most significant traits responsible for ASD and to automate the diagnostic process. We study six classification models to see which model works best to identify ASD and also study five popular clustering methods to get a meaningful insight of these ASD datasets. To find the best classifier for these binary datasets, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#31526;&#21512;&#21152;&#36733;&#26465;&#20214;&#30340;&#32467;&#26500;&#32452;&#20214;&#35774;&#35745;&#12290;&#19982;&#20854;&#20182;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#23545;&#29616;&#26377;&#35774;&#35745;&#36827;&#34892;&#32534;&#36753;&#65292;&#24182;&#19988;&#20855;&#26377;&#36817;&#20248;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#29983;&#25104;&#35774;&#35745;&#30340;&#32467;&#26500;&#24615;&#33021;&#21644;&#28508;&#22312;&#20505;&#36873;&#35774;&#35745;&#30340;&#21487;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11601</link><description>&lt;p&gt;
&#32467;&#26500;&#32452;&#20214;&#35774;&#35745;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Models for Structural Component Design. (arXiv:2309.11601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#31526;&#21512;&#21152;&#36733;&#26465;&#20214;&#30340;&#32467;&#26500;&#32452;&#20214;&#35774;&#35745;&#12290;&#19982;&#20854;&#20182;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#23545;&#29616;&#26377;&#35774;&#35745;&#36827;&#34892;&#32534;&#36753;&#65292;&#24182;&#19988;&#20855;&#26377;&#36817;&#20248;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#29983;&#25104;&#35774;&#35745;&#30340;&#32467;&#26500;&#24615;&#33021;&#21644;&#28508;&#22312;&#20505;&#36873;&#35774;&#35745;&#30340;&#21487;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#24050;&#32463;&#22312;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#31526;&#21512;&#29992;&#25143;&#38656;&#27714;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#32467;&#26500;&#32452;&#20214;&#30340;&#35774;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28385;&#36275;&#19968;&#32452;&#20855;&#20307;&#21152;&#36733;&#26465;&#20214;&#30340;&#32452;&#20214;&#28508;&#22312;&#35774;&#35745;&#12290;&#30456;&#27604;&#20110;&#20854;&#20182;&#29983;&#25104;&#26041;&#27861;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#19968;&#20010;&#26126;&#26174;&#20248;&#21183;&#26159;&#21487;&#20197;&#32534;&#36753;&#29616;&#26377;&#35774;&#35745;&#12290;&#25105;&#20204;&#20351;&#29992;&#20351;&#29992;SIMP&#31639;&#27861;&#24471;&#21040;&#30340;&#32467;&#26500;&#25299;&#25169;&#20248;&#21270;&#20960;&#20309;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26694;&#26550;&#29983;&#25104;&#30340;&#35774;&#35745;&#20855;&#26377;&#22266;&#26377;&#30340;&#36817;&#20248;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#25903;&#25345;&#29983;&#25104;&#35774;&#35745;&#32467;&#26500;&#24615;&#33021;&#21644;&#28508;&#22312;&#20505;&#36873;&#35774;&#35745;&#30340;&#21487;&#21464;&#24615;&#30340;&#23450;&#37327;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26694;&#26550;&#30340;&#21487;&#25193;&#23637;&#24615;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative modeling, namely Diffusion models, have revolutionized generative modeling, enabling high-quality image generation tailored to user needs. This paper proposes a framework for the generative design of structural components. Specifically, we employ a Latent Diffusion model to generate potential designs of a component that can satisfy a set of problem-specific loading conditions. One of the distinct advantages our approach offers over other generative approaches, such as generative adversarial networks (GANs), is that it permits the editing of existing designs. We train our model using a dataset of geometries obtained from structural topology optimization utilizing the SIMP algorithm. Consequently, our framework generates inherently near-optimal designs. Our work presents quantitative results that support the structural performance of the generated designs and the variability in potential candidate designs. Furthermore, we provide evidence of the scalability 
&lt;/p&gt;</description></item><item><title>CATS&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22320;&#29702;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#36712;&#36857;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#21457;&#24067;&#12290;&#23427;&#37319;&#29992;K-&#21311;&#21517;&#25216;&#26415;&#20445;&#38556;&#20102;&#20998;&#24067;&#32423;&#38544;&#31169;&#65292;&#36890;&#36807;&#26465;&#20214;&#23545;&#25239;&#35757;&#32451;&#21644;&#24490;&#29615;&#20108;&#37096;&#22270;&#21305;&#37197;&#31561;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#36712;&#36857;&#25968;&#25454;&#30340;&#21512;&#25104;&#21644;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.11587</link><description>&lt;p&gt;
CATS: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#26465;&#20214;&#23545;&#25239;&#36712;&#36857;&#21512;&#25104;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#36712;&#36857;&#25968;&#25454;&#21457;&#24067;
&lt;/p&gt;
&lt;p&gt;
CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches. (arXiv:2309.11587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11587
&lt;/p&gt;
&lt;p&gt;
CATS&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22320;&#29702;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#36712;&#36857;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#21457;&#24067;&#12290;&#23427;&#37319;&#29992;K-&#21311;&#21517;&#25216;&#26415;&#20445;&#38556;&#20102;&#20998;&#24067;&#32423;&#38544;&#31169;&#65292;&#36890;&#36807;&#26465;&#20214;&#23545;&#25239;&#35757;&#32451;&#21644;&#24490;&#29615;&#20108;&#37096;&#22270;&#21305;&#37197;&#31561;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#36712;&#36857;&#25968;&#25454;&#30340;&#21512;&#25104;&#21644;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26080;&#22788;&#19981;&#22312;&#30340;&#23450;&#20301;&#24863;&#30693;&#35774;&#22791;&#21644;&#31227;&#21160;&#20114;&#32852;&#32593;&#30340;&#26222;&#21450;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#29992;&#25143;&#37027;&#37324;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#20010;&#20307;&#32423;&#36712;&#36857;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#36712;&#36857;&#22823;&#25968;&#25454;&#20026;&#20154;&#31867;&#31227;&#21160;&#24615;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20851;&#20110;&#20301;&#32622;&#38544;&#31169;&#30340;&#20844;&#20247;&#20851;&#20999;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22320;&#29702;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#21517;&#20026; Conditional Adversarial Trajectory Synthesis (CATS)&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#36712;&#36857;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#21457;&#24067;&#12290;CATS &#23558; K-&#21311;&#21517;&#24212;&#29992;&#20110;&#20154;&#31867;&#31227;&#21160;&#24615;&#30340;&#26102;&#31354;&#20998;&#24067;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20998;&#24067;&#32423;&#38544;&#31169;&#20445;&#38556;&#12290;&#36890;&#36807;&#21033;&#29992;&#26465;&#20214;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#12289;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36712;&#36857;&#20840;&#23616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#21450;&#30456;&#37051;&#36712;&#36857;&#28857;&#30340;&#24490;&#29615;&#20108;&#37096;&#22270;&#21305;&#37197;&#65292;CATS &#33021;&#22815;&#20174;&#26465;&#20214;&#37319;&#26679;&#20301;&#32622;&#20013;&#37325;&#26500;&#36712;&#36857;&#25299;&#25169;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#20307;&#36712;&#36857;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of ubiquitous location-aware devices and mobile Internet enables us to collect massive individual-level trajectory dataset from users. Such trajectory big data bring new opportunities to human mobility research but also raise public concerns with regard to location privacy. In this work, we present the Conditional Adversarial Trajectory Synthesis (CATS), a deep-learning-based GeoAI methodological framework for privacy-preserving trajectory data generation and publication. CATS applies K-anonymity to the underlying spatiotemporal distributions of human movements, which provides a distributional-level strong privacy guarantee. By leveraging conditional adversarial training on K-anonymized human mobility matrices, trajectory global context learning using the attention-based mechanism, and recurrent bipartite graph matching of adjacent trajectory points, CATS is able to reconstruct trajectory topology from conditionally sampled locations and generate high-quality individual-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;Adversarial Nibbler&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#28508;&#22312;&#23545;&#25239;&#36755;&#20837;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#23545;&#25552;&#31034;&#21644;&#22270;&#20687;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24403;&#21069;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#31995;&#32479;&#24615;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.11575</link><description>&lt;p&gt;
&#20174;&#23433;&#20840;&#22522;&#20934;&#20013;&#25552;&#28860;&#23545;&#25239;&#24615;&#25552;&#31034;&#65306;&#23545;&#23545;&#25239;&#24615;Nibbler&#25361;&#25112;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge. (arXiv:2309.11575v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;Adversarial Nibbler&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#28508;&#22312;&#23545;&#25239;&#36755;&#20837;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#23545;&#25552;&#31034;&#21644;&#22270;&#20687;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24403;&#21069;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#31995;&#32479;&#24615;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#22270;&#20687;&#36136;&#37327;&#21644;&#23545;&#40784;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#34987;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#20110;&#20174;&#32593;&#32476;&#38543;&#26426;&#29228;&#21462;&#30340;&#25968;&#21313;&#20159;&#20010;&#25968;&#25454;&#38598;&#65292;&#23427;&#20204;&#20063;&#20250;&#20135;&#29983;&#19981;&#23433;&#20840;&#30340;&#20869;&#23481;&#12290;&#20316;&#20026;&#23545;Adversarial Nibbler&#25361;&#25112;&#30340;&#36129;&#29486;&#65292;&#25105;&#20204;&#20174;&#29616;&#26377;&#30340;&#23433;&#20840;&#22522;&#20934;&#20013;&#25552;&#28860;&#20102;&#19968;&#32452;&#36229;&#36807;1000&#20010;&#28508;&#22312;&#30340;&#23545;&#25239;&#36755;&#20837;&#12290;&#25105;&#20204;&#23545;&#25910;&#38598;&#21040;&#30340;&#25552;&#31034;&#21644;&#30456;&#24212;&#30340;&#22270;&#20687;&#36827;&#34892;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#36755;&#20837;&#36807;&#28388;&#22120;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#24403;&#21069;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#31995;&#32479;&#24615;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models have recently achieved astonishing image quality and alignment results. Consequently, they are employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also produce unsafe content. As a contribution to the Adversarial Nibbler challenge, we distill a large set of over 1,000 potential adversarial inputs from existing safety benchmarks. Our analysis of the gathered prompts and corresponding images demonstrates the fragility of input filters and provides further insights into systematic safety issues in current generative image models.
&lt;/p&gt;</description></item><item><title>BTLM-3B-8K&#26159;&#19968;&#20010;30&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;30&#20159;&#21644;70&#20159;&#21442;&#25968;&#27169;&#22411;&#65292;&#23427;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;2-5.5%&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21516;&#26102;&#22312;&#38271;&#25991;&#26412;&#20219;&#21153;&#19978;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#23558;70&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#21387;&#32553;&#21040;30&#20159;&#21442;&#25968;&#65292;&#24182;&#19988;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#24433;&#21709;&#30340;&#26041;&#27861;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.11568</link><description>&lt;p&gt;
BTLM-3B-8K: &#19968;&#20010;3B&#21442;&#25968;&#27169;&#22411;&#20013;&#20351;&#29992;7B&#21442;&#25968;&#24615;&#33021;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model. (arXiv:2309.11568v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11568
&lt;/p&gt;
&lt;p&gt;
BTLM-3B-8K&#26159;&#19968;&#20010;30&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;30&#20159;&#21644;70&#20159;&#21442;&#25968;&#27169;&#22411;&#65292;&#23427;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;2-5.5%&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21516;&#26102;&#22312;&#38271;&#25991;&#26412;&#20219;&#21153;&#19978;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#23558;70&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#21387;&#32553;&#21040;30&#20159;&#21442;&#25968;&#65292;&#24182;&#19988;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#24433;&#21709;&#30340;&#26041;&#27861;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Bittensor&#35821;&#35328;&#27169;&#22411;, &#21517;&#20026;"BTLM-3B-8K", &#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#12289;&#25317;&#26377;30&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;. BTLM-3B-8K&#22312;SlimPajama&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#35757;&#32451;&#25968;&#25454;&#20026;627B&#20010;token&#65292;&#37319;&#29992;&#20102;2048&#21644;8192&#30340;&#28151;&#21512;&#19978;&#19979;&#25991;&#38271;&#24230;. BTLM-3B-8K&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#27604;&#25152;&#26377;&#29616;&#26377;&#30340;30&#20159;&#21442;&#25968;&#27169;&#22411;&#25552;&#39640;&#20102;2-5.5% &#65292;&#29978;&#33267;&#19982;&#19968;&#20123;70&#20159;&#21442;&#25968;&#27169;&#22411;&#30456;&#23218;&#32654;. &#21478;&#22806;&#65292;BTLM-3B-8K&#22312;&#38271;&#25991;&#26412;&#19978;&#30340;&#34920;&#29616;&#20063;&#24456;&#22909;&#65292;&#22312;&#38271;&#24230;&#20026;8192&#30340;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;MPT-7B-8K&#21644;XGen-7B-8K. &#25105;&#20204;&#22312;&#28165;&#29702;&#21644;&#21435;&#37325;&#30340;SlimPajama&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#27169;&#22411;&#65292;&#23545;&#181;P&#36229;&#21442;&#25968;&#21644;&#35843;&#24230;&#36827;&#34892;&#20102;&#35843;&#20248;&#65292;&#20351;&#29992;&#20102;ALiBi&#20301;&#32622;&#23884;&#20837;&#21644;SwiGLU&#38750;&#32447;&#24615;. &#22312;Hugging Face&#19978;&#65292;&#26368;&#21463;&#27426;&#36814;&#30340;&#27169;&#22411;&#26159;70&#20159;&#21442;&#25968;&#65292;&#36825;&#34920;&#26126;&#29992;&#25143;&#26356;&#20542;&#21521;&#20110;&#36136;&#37327;&#22823;&#23567;&#27604;&#20026;70&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;. &#23558;70&#20159;&#21442;&#25968;&#27169;&#22411;&#21387;&#32553;&#20026;30&#20159;&#21442;&#25968;&#65292;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#37324;&#31243;&#30865;.
&lt;/p&gt;
&lt;p&gt;
We introduce the Bittensor Language Model, called "BTLM-3B-8K", a new state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and 8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B parameter models. Additionally, BTLM-3B-8K provides excellent long context performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192 context length. We trained the model on a cleaned and deduplicated SlimPajama dataset; aggressively tuned the \textmu P hyperparameters and schedule; used ALiBi position embeddings; and adopted the SwiGLU nonlinearity.  On Hugging Face, the most popular models have 7B parameters, indicating that users prefer the quality-size ratio of 7B models. Compacting the 7B parameter model to one with 3B parameters, with little performance impact, is an important milestone
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#26469;&#33258;&#20154;&#31867;&#35299;&#20915;&#20219;&#21153;&#30340;&#25968;&#25454;&#26469;&#30417;&#30563;&#19968;&#32452;&#38271;&#31243;&#20219;&#21153;&#30340;&#30446;&#26631;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26469;&#25551;&#36848;&#36825;&#20010;&#31354;&#38388;&#65292;&#35813;&#26041;&#27861;&#22312;&#20811;&#38534;&#19987;&#23478;&#34892;&#20026;&#30340;&#20195;&#29702;&#21644;&#26080;&#30417;&#30563;&#23376;&#30446;&#26631;&#31354;&#38388;&#30340;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.11564</link><description>&lt;p&gt;
&#24102;&#26377;&#33258;&#28982;&#35821;&#35328;&#23376;&#30446;&#26631;&#30340;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning with natural language subgoals. (arXiv:2309.11564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#26469;&#33258;&#20154;&#31867;&#35299;&#20915;&#20219;&#21153;&#30340;&#25968;&#25454;&#26469;&#30417;&#30563;&#19968;&#32452;&#38271;&#31243;&#20219;&#21153;&#30340;&#30446;&#26631;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26469;&#25551;&#36848;&#36825;&#20010;&#31354;&#38388;&#65292;&#35813;&#26041;&#27861;&#22312;&#20811;&#38534;&#19987;&#23478;&#34892;&#20026;&#30340;&#20195;&#29702;&#21644;&#26080;&#30417;&#30563;&#23376;&#30446;&#26631;&#31354;&#38388;&#30340;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#19968;&#30452;&#26159;&#19968;&#31181;&#23454;&#29616;&#38271;&#24207;&#21015;&#21160;&#20316;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#25110;&#24320;&#25918;&#29615;&#22659;&#20013;&#23454;&#29616;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20027;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#25214;&#21040;&#36866;&#21512;&#23454;&#20363;&#21270;&#23618;&#27425;&#30340;&#23376;&#30446;&#26631;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#30340;&#25968;&#25454;&#65292;&#23545;3D&#36527;&#20307;&#29615;&#22659;&#20013;&#19968;&#32452;&#38271;&#31243;&#20219;&#21153;&#30340;&#30446;&#26631;&#31354;&#38388;&#36827;&#34892;&#36719;&#30417;&#30563;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20351;&#29992;&#38750;&#32422;&#26463;&#30340;&#33258;&#28982;&#35821;&#35328;&#26469;&#21442;&#25968;&#21270;&#36825;&#20010;&#31354;&#38388;&#12290;&#36825;&#26377;&#20004;&#20010;&#20248;&#28857;&#65306;&#39318;&#20808;&#65292;&#21487;&#20197;&#20174;&#22825;&#30495;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#37027;&#37324;&#36731;&#26494;&#29983;&#25104;&#36825;&#20123;&#25968;&#25454;&#65307;&#20854;&#27425;&#65292;&#23427;&#36275;&#22815;&#28789;&#27963;&#65292;&#33021;&#22815;&#34920;&#31034;&#20154;&#31867;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#19968;&#22823;&#33539;&#22260;&#23376;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#20248;&#20110;&#20811;&#38534;&#19987;&#23478;&#34892;&#20026;&#30340;&#20195;&#29702;&#21644;&#27809;&#26377;&#36825;&#31181;&#21463;&#30417;&#30563;&#23376;&#30446;&#26631;&#31354;&#38388;&#30340;&#20174;&#22836;&#24320;&#22987;&#30340;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#19987;&#23478;&#30417;&#30563;&#19982;&#36825;&#31181;&#21463;&#30410;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning has been a compelling approach for achieving goal directed behavior over long sequences of actions. However, it has been challenging to implement in realistic or open-ended environments. A main challenge has been to find the right space of sub-goals over which to instantiate a hierarchy. We present a novel approach where we use data from humans solving these tasks to softly supervise the goal space for a set of long range tasks in a 3D embodied environment. In particular, we use unconstrained natural language to parameterize this space. This has two advantages: first, it is easy to generate this data from naive human participants; second, it is flexible enough to represent a vast range of sub-goals in human-relevant tasks. Our approach outperforms agents that clone expert behavior on these tasks, as well as HRL from scratch without this supervised sub-goal space. Our work presents a novel approach to combining human expert supervision with the benefi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPTQ&#30340;&#22686;&#24378;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#21152;&#26435;&#23618;&#21644;&#26080;&#26631;&#31614;Hessian&#36817;&#20284;&#25216;&#26415;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11531</link><description>&lt;p&gt;
EPTQ:&#36890;&#36807;&#26080;&#26631;&#31614;Hessian&#22686;&#24378;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian. (arXiv:2309.11531v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPTQ&#30340;&#22686;&#24378;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#21152;&#26435;&#23618;&#21644;&#26080;&#26631;&#31614;Hessian&#36817;&#20284;&#25216;&#26415;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#37327;&#21270;&#24050;&#25104;&#20026;&#23558;&#36825;&#20123;&#32593;&#32476;&#23884;&#20837;&#21040;&#26368;&#32456;&#29992;&#25143;&#35774;&#22791;&#19978;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#20005;&#37325;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPTQ&#30340;&#22686;&#24378;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#21152;&#26435;&#23618;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#26631;&#31614;Hessian&#36817;&#20284;&#25216;&#26415;&#65292;&#21517;&#20026;Label-Free Hessian&#12290;&#36825;&#31181;&#25216;&#26415;&#28040;&#38500;&#20102;&#35745;&#31639;Hessian&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#35201;&#27714;&#12290;&#33258;&#36866;&#24212;&#30693;&#35782;&#33976;&#39311;&#21033;&#29992;Label-Free Hessian&#25216;&#26415;&#65292;&#22312;&#36827;&#34892;&#20248;&#21270;&#26102;&#26356;&#21152;&#20851;&#27880;&#27169;&#22411;&#30340;&#25935;&#24863;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;EPTQ&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;ImageNet&#20998;&#31867;&#12289;COCO&#30446;&#26631;&#26816;&#27979;&#21644;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;Pascal-VOC&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization of deep neural networks (DNN) has become a key element in the efforts of embedding such networks on end-user devices. However, current quantization methods usually suffer from costly accuracy degradation. In this paper, we propose a new method for Enhanced Post Training Quantization named EPTQ. The method is based on knowledge distillation with an adaptive weighting of layers. In addition, we introduce a new label-free technique for approximating the Hessian trace of the task loss, named Label-Free Hessian. This technique removes the requirement of a labeled dataset for computing the Hessian. The adaptive knowledge distillation uses the Label-Free Hessian technique to give greater attention to the sensitive parts of the model while performing the optimization. Empirically, by employing EPTQ we achieve state-of-the-art results on a wide variety of models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation.
&lt;/p&gt;</description></item><item><title>TrueLearn&#26159;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#26500;&#24314;&#20010;&#24615;&#21270;&#30340;&#20449;&#24687;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25991;&#26723;&#21644;&#32534;&#30721;&#31034;&#20363;&#65292;&#21487;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#20351;&#29992;&#12290;&#23427;&#37319;&#29992;&#20102;&#24320;&#25918;&#23398;&#20064;&#32773;&#30340;&#27010;&#24565;&#21644;&#20154;&#24615;&#21270;&#30340;&#29992;&#25143;&#34920;&#36798;&#26041;&#24335;&#65292;&#21516;&#26102;&#25903;&#25345;&#29992;&#25143;&#21487;&#35270;&#21270;&#21644;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.11527</link><description>&lt;p&gt;
TrueLearn: &#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#20449;&#24687;&#25512;&#33616;&#30340;Python&#24211;&#65288;&#24102;&#26377;&#65288;&#38544;&#24335;&#65289;&#21453;&#39304;&#65289;
&lt;/p&gt;
&lt;p&gt;
TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback. (arXiv:2309.11527v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11527
&lt;/p&gt;
&lt;p&gt;
TrueLearn&#26159;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#26500;&#24314;&#20010;&#24615;&#21270;&#30340;&#20449;&#24687;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25991;&#26723;&#21644;&#32534;&#30721;&#31034;&#20363;&#65292;&#21487;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#20351;&#29992;&#12290;&#23427;&#37319;&#29992;&#20102;&#24320;&#25918;&#23398;&#20064;&#32773;&#30340;&#27010;&#24565;&#21644;&#20154;&#24615;&#21270;&#30340;&#29992;&#25143;&#34920;&#36798;&#26041;&#24335;&#65292;&#21516;&#26102;&#25903;&#25345;&#29992;&#25143;&#21487;&#35270;&#21270;&#21644;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TrueLearn Python&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#32452;&#22312;&#32447;&#23398;&#20064;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#25945;&#32946;&#65288;&#25110;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#20449;&#24687;&#65289;&#25512;&#33616;&#31995;&#32479;&#12290;&#36825;&#32452;&#27169;&#22411;&#26159;&#26681;&#25454;&#8220;&#24320;&#25918;&#23398;&#20064;&#32773;&#8221;&#30340;&#27010;&#24565;&#35774;&#35745;&#30340;&#65292;&#20351;&#29992;&#30452;&#35266;&#30340;&#29992;&#25143;&#34920;&#36798;&#12290;&#20026;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#35753;&#29992;&#25143;&#26377;&#25511;&#21046;&#24863;&#65292;TrueLearn&#24211;&#36824;&#21253;&#21547;&#19981;&#21516;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20197;&#24110;&#21161;&#26368;&#32456;&#29992;&#25143;&#21487;&#35270;&#21270;&#23398;&#20064;&#32773;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#23558;&#26469;&#29992;&#25143;&#19982;&#33258;&#24049;&#30340;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#12290;&#19982;&#35813;&#24211;&#19968;&#36215;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20808;&#21069;&#20844;&#24320;&#21457;&#24067;&#30340;&#38544;&#24335;&#21453;&#39304;&#25945;&#32946;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#34913;&#37327;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20016;&#23500;&#30340;&#25991;&#26723;&#21644;&#32534;&#30721;&#31034;&#20363;&#20351;&#35813;&#24211;&#23545;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20154;&#21592;&#21644;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#21644;&#23398;&#20064;&#20998;&#26512;&#20174;&#19994;&#32773;&#37117;&#38750;&#24120;&#26131;&#20110;&#20351;&#29992;&#12290;&#35813;&#24211;&#21644;&#24102;&#26377;&#31034;&#20363;&#30340;&#25903;&#25345;&#25991;&#26723;&#21487;&#22312;https&#65306;//&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work describes the TrueLearn Python library, which contains a family of online learning Bayesian models for building educational (or more generally, informational) recommendation systems. This family of models was designed following the "open learner" concept, using humanly-intuitive user representations. For the sake of interpretability and putting the user in control, the TrueLearn library also contains different representations to help end-users visualise the learner models, which may in the future facilitate user interaction with their own models. Together with the library, we include a previously publicly released implicit feedback educational dataset with evaluation metrics to measure the performance of the models. The extensive documentation and coding examples make the library highly accessible to both machine learning developers and educational data mining and learning analytic practitioners. The library and the support documentation with examples are available at https:/
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11526</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#20013;&#20256;&#24863;&#22120;&#26657;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems. (arXiv:2309.11526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#24863;&#22120;&#25216;&#26415;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#26159;&#23558;&#19968;&#20010;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#32467;&#26524;&#39640;&#25928;&#22320;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#20855;&#26377;&#30456;&#21516;&#35774;&#35745;&#30340;&#20256;&#24863;&#22120;&#12290;&#19968;&#31181;&#24819;&#27861;&#26159;&#20351;&#29992;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#20223;&#23556;&#21464;&#25442;&#20272;&#35745;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#19987;&#23478;&#30340;&#30693;&#35782;&#36827;&#34892;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Glacier Research&#22312;1973&#24180;&#21457;&#34920;&#30340;&#25913;&#36827;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#29992;&#20110;&#20256;&#24863;&#22120;&#30340;&#36719;&#20214;&#26657;&#20934;&#12289;&#22522;&#20110;&#19987;&#23478;&#30340;&#36866;&#24212;&#21644;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;8&#20010;&#30456;&#21516;&#20256;&#24863;&#22120;&#30340;&#22810;&#20256;&#24863;&#22120;&#26495;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#27169;&#25311;&#36824;&#26159;&#23454;&#39564;&#25968;&#25454;&#65292;&#37117;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. One idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. This paper presents an improved solution from Glacier Research that was published back in 1973. It is shown that this solution can be adapted for software calibration of sensors, implementation of expert-based adaptation, and federated learning methods. We evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. The results show an improvement for both the simulation and the experiments with real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#31163;&#32447;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21644;&#19978;&#19979;&#25991;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#24182;&#26368;&#22823;&#21270;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#25910;&#20837;&#12290;</title><link>http://arxiv.org/abs/2309.11518</link><description>&lt;p&gt;
&#22312;&#20869;&#23481;&#24066;&#22330;&#20013;&#30340;&#31163;&#32447;&#23398;&#20064;&#19979;&#30340;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Ad-load Balancing via Off-policy Learning in a Content Marketplace. (arXiv:2309.11518v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#31163;&#32447;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21644;&#19978;&#19979;&#25991;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#24182;&#26368;&#22823;&#21270;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#25910;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#26159;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#23588;&#20854;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#32972;&#26223;&#19979;&#65292;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#29992;&#25143;&#20307;&#39564;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#25910;&#20837;&#12290;&#20256;&#32479;&#30340;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#26041;&#27861;&#20381;&#36182;&#20110;&#38745;&#24577;&#20998;&#37197;&#31574;&#30053;&#65292;&#26080;&#27861;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21644;&#19978;&#19979;&#25991;&#22240;&#32032;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#23398;&#20064;&#21644;&#20381;&#25454;&#35760;&#24405;&#30340;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#24191;&#21578;&#25910;&#20837;&#20043;&#38388;&#30340;&#20914;&#31361;&#30446;&#26631;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#30001;&#20110;&#29992;&#25143;&#24322;&#36136;&#24615;&#21644;&#29992;&#25143;&#22312;&#20250;&#35805;&#20013;&#30340;&#20301;&#32622;&#30340;&#20381;&#36182;&#24615;&#32780;&#24341;&#36215;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#22312;&#29305;&#23450;&#30340;&#20869;&#23481;&#33719;&#21462;&#20013;&#30830;&#23450;&#26368;&#20248;&#24191;&#21578;&#36127;&#36733;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ad-load balancing is a critical challenge in online advertising systems, particularly in the context of social media platforms, where the goal is to maximize user engagement and revenue while maintaining a satisfactory user experience. This requires the optimization of conflicting objectives, such as user satisfaction and ads revenue. Traditional approaches to ad-load balancing rely on static allocation policies, which fail to adapt to changing user preferences and contextual factors. In this paper, we present an approach that leverages off-policy learning and evaluation from logged bandit feedback. We start by presenting a motivating analysis of the ad-load balancing problem, highlighting the conflicting objectives between user satisfaction and ads revenue. We emphasize the nuances that arise due to user heterogeneity and the dependence on the user's position within a session. Based on this analysis, we define the problem as determining the optimal ad-load for a particular feed fetch.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20844;&#20849;&#39033;&#30446;&#29305;&#24449;&#36827;&#34892;&#31169;&#26377;&#30697;&#38453;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#23545;&#25512;&#33616;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#12289;&#26131;&#35843;&#25972;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11516</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#20849;&#39033;&#30446;&#29305;&#24449;&#30340;&#31169;&#26377;&#30697;&#38453;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Private Matrix Factorization with Public Item Features. (arXiv:2309.11516v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11516
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20844;&#20849;&#39033;&#30446;&#29305;&#24449;&#36827;&#34892;&#31169;&#26377;&#30697;&#38453;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#23545;&#25512;&#33616;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#12289;&#26131;&#35843;&#25972;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#20855;&#26377;&#20844;&#20849;&#39033;&#30446;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#31169;&#26377;&#25512;&#33616;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#21487;&#20197;&#25552;&#20379;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#38556;&#65292;&#20294;&#20250;&#23548;&#33268;&#25512;&#33616;&#36136;&#37327;&#19979;&#38477;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#20844;&#20849;&#39033;&#30446;&#29305;&#24449;&#21487;&#20197;&#24110;&#21161;&#32531;&#35299;&#25512;&#33616;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#20307;&#30697;&#38453;&#20998;&#35299;&#65288;CMF&#65289;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23545;&#20004;&#20010;&#30697;&#38453;&#36827;&#34892;&#20998;&#35299;&#65306;&#29992;&#25143;&#21453;&#39304;&#30697;&#38453;&#65288;&#20195;&#34920;&#25935;&#24863;&#25968;&#25454;&#65289;&#21644;&#19968;&#20010;&#32534;&#30721;&#20844;&#24320;&#21487;&#29992;&#65288;&#38750;&#25935;&#24863;&#65289;&#39033;&#30446;&#20449;&#24687;&#30340;&#39033;&#30446;&#29305;&#24449;&#30697;&#38453;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#65292;&#26131;&#20110;&#35843;&#25972;&#65292;&#32780;&#19988;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#20844;&#20849;&#39033;&#30446;&#25968;&#25454;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#20998;&#31867;&#39033;&#30446;&#29305;&#24449;&#65307;&#65288;2&#65289;&#20174;&#20844;&#20849;&#26469;&#28304;&#23398;&#20064;&#30340;&#39033;&#30446;&#38388;&#30456;&#20284;&#24615;&#65307;&#20197;&#21450;&#65288;3&#65289;&#20844;&#24320;&#21487;&#29992;&#30340;&#29992;&#25143;&#21453;&#39304;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25968;&#25454;&#27169;&#24577;&#21487;&#20197;&#20849;&#21516;&#21033;&#29992;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of training private recommendation models with access to public item features. Training with Differential Privacy (DP) offers strong privacy guarantees, at the expense of loss in recommendation quality. We show that incorporating public item features during training can help mitigate this loss in quality. We propose a general approach based on collective matrix factorization (CMF), that works by simultaneously factorizing two matrices: the user feedback matrix (representing sensitive data) and an item feature matrix that encodes publicly available (non-sensitive) item information.  The method is conceptually simple, easy to tune, and highly scalable. It can be applied to different types of public item data, including: (1) categorical item features; (2) item-item similarities learned from public sources; and (3) publicly available user feedback. Furthermore, these data modalities can be collectively utilized to fully leverage public data.  Evaluating our method o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24046;&#20998;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#22312;&#21160;&#24577;&#21644;&#20381;&#36182;&#20851;&#31995;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#20063;&#20851;&#27880;&#20102;&#25935;&#24863;&#29992;&#25143;&#29305;&#24449;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.11515</link><description>&lt;p&gt;
&#36816;&#29992;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#30340;&#39034;&#24207;&#25512;&#33616;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Differential Privacy in Sequential Recommendation: A Noisy Graph Neural Network Approach. (arXiv:2309.11515v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11515
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24046;&#20998;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#22312;&#21160;&#24577;&#21644;&#20381;&#36182;&#20851;&#31995;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#20063;&#20851;&#27880;&#20102;&#25935;&#24863;&#29992;&#25143;&#29305;&#24449;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#31181;&#22312;&#32447;&#24179;&#21488;&#20013;&#39640;&#35843;&#30340;&#38544;&#31169;&#27844;&#38706;&#20107;&#20214;&#39057;&#32321;&#21457;&#29983;&#65292;&#29992;&#25143;&#23545;&#38544;&#31169;&#36234;&#26469;&#36234;&#20851;&#27880;&#12290;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#22312;&#32447;&#24179;&#21488;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20854;&#38544;&#31169;&#20445;&#25252;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#24046;&#20998;&#38544;&#31169;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24046;&#20998;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#21482;&#32771;&#34385;&#38745;&#24577;&#21644;&#29420;&#31435;&#30340;&#29992;&#25143;&#20132;&#20114;&#65292;&#22240;&#27492;&#26080;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#21160;&#24577;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#39034;&#24207;&#25512;&#33616;&#12290;&#21516;&#26102;&#65292;&#23545;&#20110;&#25935;&#24863;&#29992;&#25143;&#29305;&#24449;&#30340;&#38544;&#31169;&#39118;&#38505;&#20851;&#27880;&#36739;&#23569;&#65292;&#22823;&#22810;&#25968;&#21482;&#20445;&#25252;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;&#31216;&#20026;DIPSGNN&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increasing frequency of high-profile privacy breaches in various online platforms, users are becoming more concerned about their privacy. And recommender system is the core component of online platforms for providing personalized service, consequently, its privacy preservation has attracted great attention. As the gold standard of privacy protection, differential privacy has been widely adopted to preserve privacy in recommender systems. However, existing differentially private recommender systems only consider static and independent interactions, so they cannot apply to sequential recommendation where behaviors are dynamic and dependent. Meanwhile, little attention has been paid on the privacy risk of sensitive user features, most of them only protect user feedbacks. In this work, we propose a novel DIfferentially Private Sequential recommendation framework with a noisy Graph Neural Network approach (denoted as DIPSGNN) to address these limitations. To the best of our knowledge, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;fusionACS&#39033;&#30446;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#32654;&#22269;&#23478;&#24237;&#35843;&#26597;&#30340;&#25968;&#25454;&#36827;&#34892;&#34701;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#22815;&#20998;&#26512;&#23478;&#24237;&#23646;&#24615;&#21644;&#31119;&#31049;&#32500;&#24230;&#30340;&#32508;&#21512;&#24494;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20351;&#30740;&#31350;&#32773;&#21487;&#20197;&#22238;&#31572;&#26356;&#21152;&#22810;&#26679;&#21270;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.11512</link><description>&lt;p&gt;
&#20351;&#29992;&#34701;&#21512;&#30340;&#23478;&#24237;&#35843;&#26597;&#22312;&#31934;&#32454;&#31354;&#38388;&#23610;&#24230;&#19978;&#35780;&#20272;&#32654;&#22269;&#23478;&#24237;&#30340;&#22810;&#32500;&#31119;&#31049;&#65306;fusionACS.
&lt;/p&gt;
&lt;p&gt;
Multidimensional well-being of US households at a fine spatial scale using fused household surveys: fusionACS. (arXiv:2309.11512v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11512
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;fusionACS&#39033;&#30446;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#32654;&#22269;&#23478;&#24237;&#35843;&#26597;&#30340;&#25968;&#25454;&#36827;&#34892;&#34701;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#22815;&#20998;&#26512;&#23478;&#24237;&#23646;&#24615;&#21644;&#31119;&#31049;&#32500;&#24230;&#30340;&#32508;&#21512;&#24494;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20351;&#30740;&#31350;&#32773;&#21487;&#20197;&#22238;&#31572;&#26356;&#21152;&#22810;&#26679;&#21270;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#31185;&#23398;&#24120;&#24120;&#20381;&#36182;&#20110;&#23478;&#24237;&#21644;&#20010;&#20307;&#30340;&#35843;&#26597;&#12290;&#32654;&#22269;&#25919;&#24220;&#32463;&#24120;&#36827;&#34892;&#25968;&#21313;&#20010;&#36825;&#26679;&#30340;&#35843;&#26597;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26159;&#29420;&#31435;&#30340;&#12289;&#19981;&#30456;&#20851;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#26377;&#19987;&#38376;&#30340;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#21482;&#33021;&#36890;&#36807;&#19968;&#20010;&#35843;&#26597;&#26469;&#22238;&#31572;&#30740;&#31350;&#38382;&#39064;&#12290;fusionACS&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#32479;&#35745;&#23398;&#19978;&#23558;&#8220;&#20379;&#20307;&#8221;&#35843;&#26597;&#30340;&#21464;&#37327;&#19982;&#32654;&#22269;&#20154;&#31038;&#21306;&#35843;&#26597; (ACS) &#30340;&#24494;&#35266;&#25968;&#25454;&#36827;&#34892;&#8220;&#34701;&#21512;&#8221;&#65292;&#20174;&#32780;&#25972;&#21512;&#22810;&#20010;&#32654;&#22269;&#23478;&#24237;&#35843;&#26597;&#30340;&#25968;&#25454;&#12290;&#36825;&#20135;&#29983;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#23478;&#24237;&#23646;&#24615;&#21644;&#31119;&#31049;&#32500;&#24230;&#30340;&#24494;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#30740;&#31350;&#38382;&#39064;&#65292;&#36825;&#26159;&#30446;&#21069;&#26080;&#27861;&#23454;&#29616;&#30340;&#12290;&#25152;&#21576;&#29616;&#30340;&#25968;&#25454;&#21253;&#25324;&#23545;2015&#24180;&#20303;&#23429;&#33021;&#28304;&#28040;&#36153;&#35843;&#26597;(RECS)&#12289;2017&#24180;&#20840;&#22269;&#23478;&#24237;&#20132;&#36890;&#35843;&#26597;(NHTS)&#12289;2019&#24180;&#32654;&#22269;&#20303;&#25151;&#35843;&#26597;(AHS)&#21644;2015-2019&#24180;&#28040;&#36153;&#32773;&#25903;&#20986;&#35843;&#26597;-&#35775;&#35848;(CEI)&#30340;&#36873;&#25321;&#20379;&#20307;&#21464;&#37327;&#36827;&#34892;&#34701;&#21512;&#21040;ACS&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social science often relies on surveys of households and individuals. Dozens of such surveys are regularly administered by the U.S. government. However, they field independent, unconnected samples with specialized questions, limiting research questions to those that can be answered by a single survey. The fusionACS project seeks to integrate data from multiple U.S. household surveys by statistically "fusing" variables from "donor" surveys onto American Community Survey (ACS) microdata. This results in an integrated microdataset of household attributes and well-being dimensions that can be analyzed to address research questions in ways that are not currently possible. The presented data comprise the fusion onto the ACS of select donor variables from the Residential Energy Consumption Survey (RECS) of 2015, the National Household Transportation Survey (NHTS) of 2017, the American Housing Survey (AHS) of 2019, and the Consumer Expenditure Survey - Interview (CEI) for the years 2015-2019. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#20010;&#24314;&#31569;&#33021;&#28304;&#28040;&#32791;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20013;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#65292;&#21457;&#29616;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#35780;&#20272;&#21644;&#39046;&#22495;&#30693;&#35782;&#31579;&#36873;&#26080;&#27861;&#25490;&#38500;&#20559;&#20506;&#32467;&#26524;&#65292;&#22240;&#27492;&#22312;&#36873;&#25321;&#29305;&#24449;&#26102;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#22240;&#26524;&#20851;&#31995;&#65292;&#32780;&#22240;&#26524;&#20998;&#26512;&#30340;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#35774;&#35745;&#21644;&#21442;&#25968;&#26816;&#26597;&#65292;&#36991;&#20813;&#35748;&#30693;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.11509</link><description>&lt;p&gt;
&#20351;&#29992;&#22240;&#26524;&#25512;&#26029;&#36991;&#20813;&#25968;&#25454;&#39537;&#21160;&#21442;&#25968;&#20998;&#26512;&#30340;&#38382;&#39064;&#65306;&#22312;&#24314;&#31569;&#12289;&#24037;&#31243;&#21644;&#26045;&#24037;&#34892;&#19994;&#30340;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using causal inference to avoid fallouts in data-driven parametric analysis: a case study in the architecture, engineering, and construction industry. (arXiv:2309.11509v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#20010;&#24314;&#31569;&#33021;&#28304;&#28040;&#32791;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20013;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#65292;&#21457;&#29616;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#35780;&#20272;&#21644;&#39046;&#22495;&#30693;&#35782;&#31579;&#36873;&#26080;&#27861;&#25490;&#38500;&#20559;&#20506;&#32467;&#26524;&#65292;&#22240;&#27492;&#22312;&#36873;&#25321;&#29305;&#24449;&#26102;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#22240;&#26524;&#20851;&#31995;&#65292;&#32780;&#22240;&#26524;&#20998;&#26512;&#30340;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#35774;&#35745;&#21644;&#21442;&#25968;&#26816;&#26597;&#65292;&#36991;&#20813;&#35748;&#30693;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#23454;&#26045;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#21463;&#21040;&#23545;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#30340;&#20381;&#36182;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12289;&#32463;&#39564;&#39046;&#22495;&#30693;&#35782;&#21644;&#31532;&#19968;&#21407;&#29702;&#27169;&#25311;&#20043;&#38388;&#30340;&#21327;&#21516;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#22240;&#26524;&#20998;&#26512;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#20559;&#20506;&#32467;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#36890;&#36807;&#23545;&#19968;&#26635;&#24314;&#31569;&#30340;&#22810;&#20010;&#35774;&#35745;&#26041;&#26696;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#36807;&#31243;&#20013;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#24471;&#20986;&#20197;&#19979;&#32467;&#35770;&#65306;(a) &#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#35780;&#20272;&#25110;&#39046;&#22495;&#30693;&#35782;&#31579;&#36873;&#21487;&#33021;&#26080;&#27861;&#25490;&#38500;&#20559;&#20506;&#21644;&#34394;&#20551;&#32467;&#26524;&#65307;(b) &#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#29305;&#24449;&#36873;&#25321;&#24212;&#35813;&#20180;&#32454;&#32771;&#34385;&#22240;&#26524;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#21327;&#21464;&#37327;&#65307;(c) &#22240;&#26524;&#20998;&#26512;&#30340;&#32467;&#26524;&#21487;&#20197;&#20316;&#20026;&#31532;&#19968;&#21407;&#29702;&#27169;&#25311;&#35774;&#35745;&#21644;&#21442;&#25968;&#26816;&#26597;&#30340;&#36741;&#21161;&#24037;&#20855;&#65292;&#20197;&#36991;&#20813;&#35748;&#30693;&#20559;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22240;&#26524;&#20998;&#26512;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The decision-making process in real-world implementations has been affected by a growing reliance on data-driven models. We investigated the synergetic pattern between the data-driven methods, empirical domain knowledge, and first-principles simulations. We showed the potential risk of biased results when using data-driven models without causal analysis. Using a case study assessing the implication of several design solutions on the energy consumption of a building, we proved the necessity of causal analysis during the data-driven modeling process. We concluded that: (a) Data-driven models' accuracy assessment or domain knowledge screening may not rule out biased and spurious results; (b) Data-driven models' feature selection should involve careful consideration of causal relationships, especially colliders; (c) Causal analysis results can be used as an aid to first-principles simulation design and parameter checking to avoid cognitive biases. We proved the benefits of causal analysis 
&lt;/p&gt;</description></item><item><title>AdBooster&#36890;&#36807;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#22806;&#26223;&#26550;&#26500;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#24191;&#21578;&#21019;&#20316;&#30340;&#21019;&#24847;&#20248;&#21270;&#65292;&#20854;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2309.11507</link><description>&lt;p&gt;
AdBooster&#65306;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#22806;&#26223;&#29983;&#25104;&#30340;&#20010;&#24615;&#21270;&#24191;&#21578;&#21019;&#24847;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
AdBooster: Personalized Ad Creative Generation using Stable Diffusion Outpainting. (arXiv:2309.11507v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11507
&lt;/p&gt;
&lt;p&gt;
AdBooster&#36890;&#36807;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#22806;&#26223;&#26550;&#26500;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#24191;&#21578;&#21019;&#20316;&#30340;&#21019;&#24847;&#20248;&#21270;&#65292;&#20854;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#24191;&#21578;&#20013;&#65292;&#26368;&#20339;&#25512;&#33616;&#39033;&#30446;&#65288;&#25512;&#33616;&#39033;&#65289;&#30340;&#36873;&#25321;&#21644;&#26368;&#20339;&#21019;&#24847;&#23637;&#31034;&#65288;&#21019;&#24847;&#20248;&#21270;&#65289;&#20256;&#32479;&#19978;&#34987;&#35748;&#20026;&#26159;&#19981;&#21516;&#30340;&#23398;&#31185;&#12290;&#28982;&#32780;&#65292;&#20004;&#32773;&#23545;&#29992;&#25143;&#28385;&#24847;&#24230;&#37117;&#26377;&#26174;&#33879;&#36129;&#29486;&#65292;&#36825;&#26159;&#22522;&#20110;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#23427;&#20204;&#20381;&#36182;&#20110;&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#21644;&#23637;&#31034;&#26041;&#24335;&#65292;&#23588;&#20854;&#26159;&#22312;&#35270;&#35273;&#21019;&#24847;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#29983;&#25104;&#21019;&#24847;&#20248;&#21270;&#65288;GCO&#65289;&#8221;&#30340;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#25552;&#20986;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#21019;&#24847;&#29983;&#25104;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#32467;&#21512;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#20197;&#21450;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#22806;&#26223;&#26550;&#26500;&#30340;&#8220;AdBooster&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20010;&#24615;&#21270;&#24191;&#21578;&#21019;&#24847;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#24494;&#35843;&#21644;&#29983;&#25104;&#26102;&#37117;&#21487;&#20197;&#29420;&#29305;&#22320;&#32467;&#21512;&#29992;&#25143;&#30340;&#20852;&#36259;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;AdBooster&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#27969;&#31243;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;AdBooster&#22312;&#29983;&#25104;&#26356;&#22810;&#31934;&#24425;&#21019;&#24847;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In digital advertising, the selection of the optimal item (recommendation) and its best creative presentation (creative optimization) have traditionally been considered separate disciplines. However, both contribute significantly to user satisfaction, underpinning our assumption that it relies on both an item's relevance and its presentation, particularly in the case of visual creatives. In response, we introduce the task of {\itshape Generative Creative Optimization (GCO)}, which proposes the use of generative models for creative generation that incorporate user interests, and {\itshape AdBooster}, a model for personalized ad creatives based on the Stable Diffusion outpainting architecture. This model uniquely incorporates user interests both during fine-tuning and at generation time. To further improve AdBooster's performance, we also introduce an automated data augmentation pipeline. Through our experiments on simulated data, we validate AdBooster's effectiveness in generating more 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#20844;&#24179;&#24615;&#19982;&#20010;&#24615;&#21270;&#20043;&#38388;&#30340;&#22256;&#22659;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#35748;&#30693;&#25928;&#29992;&#32972;&#26223;&#19979;&#23454;&#29616;&#20844;&#27491;&#30340;&#20844;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.11503</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#19982;&#20010;&#24615;&#21270;&#65306;&#36808;&#21521;&#35748;&#30693;&#25928;&#29992;&#30340;&#20844;&#27491;
&lt;/p&gt;
&lt;p&gt;
Fairness Vs. Personalization: Towards Equity in Epistemic Utility. (arXiv:2309.11503v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11503
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#20844;&#24179;&#24615;&#19982;&#20010;&#24615;&#21270;&#20043;&#38388;&#30340;&#22256;&#22659;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#35748;&#30693;&#25928;&#29992;&#32972;&#26223;&#19979;&#23454;&#29616;&#20844;&#27491;&#30340;&#20844;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#24212;&#29992;&#33539;&#22260;&#27491;&#22312;&#36805;&#36895;&#25193;&#23637;&#65292;&#28085;&#30422;&#31038;&#20132;&#23186;&#20307;&#12289;&#22312;&#32447;&#36141;&#29289;&#12289;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#31561;&#39046;&#22495;&#12290;&#36825;&#20123;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#26469;&#27983;&#35272;&#22823;&#37327;&#21487;&#29992;&#30340;&#29289;&#21697;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#21457;&#23637;&#65292;&#20154;&#20204;&#23545;&#31639;&#27861;&#31995;&#32479;&#21487;&#33021;&#23384;&#22312;&#24182;&#24310;&#32493;&#20559;&#35265;&#30340;&#28508;&#21147;&#26377;&#20102;&#26356;&#22810;&#35748;&#35782;&#65292;&#23384;&#22312;&#20010;&#24615;&#21270;&#39046;&#22495;&#20013;&#30340;&#19981;&#20844;&#24179;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#20010;&#24615;&#21270;&#19982;&#20256;&#32479;&#20844;&#24179;&#23454;&#29616;&#20043;&#38388;&#22266;&#26377;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#35748;&#30693;&#25928;&#29992;&#32972;&#26223;&#19979;&#23454;&#29616;&#20844;&#27491;&#30340;&#20844;&#24179;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30446;&#26631;&#21644;&#23454;&#38469;&#23454;&#29616;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#20851;&#38190;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#25919;&#31574;&#24314;&#35758;&#65292;&#20197;&#22312;&#20010;&#24615;&#21270;&#31995;&#32479;&#20013;&#23454;&#29616;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The applications of personalized recommender systems are rapidly expanding: encompassing social media, online shopping, search engine results, and more. These systems offer a more efficient way to navigate the vast array of items available. However, alongside this growth, there has been increased recognition of the potential for algorithmic systems to exhibit and perpetuate biases, risking unfairness in personalized domains. In this work, we explicate the inherent tension between personalization and conventional implementations of fairness. As an alternative, we propose equity to achieve fairness in the context of epistemic utility. We provide a mapping between goals and practical implementations and detail policy recommendations across key stakeholders to forge a path towards achieving fairness in personalized systems.
&lt;/p&gt;</description></item><item><title>Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11489</link><description>&lt;p&gt;
Text2Reward&#65306;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11489
&lt;/p&gt;
&lt;p&gt;
Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#65307;&#23427;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#25110;&#39046;&#22495;&#25968;&#25454;&#65292;&#23548;&#33268;&#24320;&#21457;&#25104;&#26412;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Text2Reward&#65292;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#21487;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#30446;&#26631;&#65292;Text2Reward&#29983;&#25104;&#20316;&#20026;&#29615;&#22659;&#32039;&#20945;&#34920;&#31034;&#30340;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#19982;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#36817;&#20351;&#29992;LLM&#32534;&#20889;&#31232;&#30095;&#22870;&#21169;&#20195;&#30721;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;Text2Reward&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20195;&#30721;&#65292;&#21487;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#65292;&#21033;&#29992;&#29616;&#26377;&#36719;&#20214;&#21253;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26426;&#22120;&#20154;&#25805;&#20316;&#22522;&#20934;&#65288;ManiSkill2&#65292;MetaWorld&#65289;&#21644;&#20004;&#20010;MuJoCo&#30340;&#36816;&#21160;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;Text2Reward&#12290;&#22312;17&#20010;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;13&#20010;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#22870;&#21169;&#20195;&#30721;&#35757;&#32451;&#30340;&#25919;&#31574;&#23454;&#29616;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20262;&#25958;&#30340;&#22478;&#24066;&#21464;&#21270;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;1500&#19975;&#24352;&#34903;&#26223;&#22270;&#20687;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#20102;&#20303;&#25151;&#20379;&#24212;&#30340;&#21464;&#21270;&#65292;&#24182;&#21306;&#20998;&#20102;&#20027;&#35201;&#21644;&#27425;&#35201;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.11354</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#25581;&#31034;&#20102;&#34903;&#26223;&#22270;&#20687;&#20013;&#22478;&#24066;&#20303;&#25151;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning unveils change in urban housing from street-level images. (arXiv:2309.11354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20262;&#25958;&#30340;&#22478;&#24066;&#21464;&#21270;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;1500&#19975;&#24352;&#34903;&#26223;&#22270;&#20687;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#20102;&#20303;&#25151;&#20379;&#24212;&#30340;&#21464;&#21270;&#65292;&#24182;&#21306;&#20998;&#20102;&#20027;&#35201;&#21644;&#27425;&#35201;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#21508;&#22320;&#30340;&#22478;&#24066;&#37117;&#38754;&#20020;&#30528;&#21487;&#36127;&#25285;&#21644;&#20307;&#38754;&#20303;&#25151;&#20005;&#37325;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#23545;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#25105;&#20204;&#22312;&#26377;&#25928;&#30417;&#27979;&#21644;&#36861;&#36394;&#22478;&#24066;&#20303;&#25151;&#36827;&#23637;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#24212;&#29992;&#20110;&#34903;&#26223;&#22270;&#20687;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#22312;&#27979;&#37327;&#31038;&#20250;&#32463;&#27982;&#21644;&#29615;&#22659;&#19981;&#24179;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#26102;&#21464;&#26631;&#31614;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#23427;&#20204;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#22270;&#20687;&#26469;&#36319;&#36394;&#22478;&#24066;&#21464;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#22312;2008&#24180;&#33267;2021&#24180;&#20043;&#38388;&#20351;&#29992;1500&#19975;&#24352;&#20262;&#25958;&#34903;&#26223;&#22270;&#20687;&#26469;&#27979;&#37327;&#21464;&#21270;&#12290;&#25105;&#20204;&#23545;Barlow Twins&#30340;&#26032;&#39062;&#25913;&#36827;Street2Vec&#65292;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#23884;&#20837;&#20102;&#22478;&#24066;&#32467;&#26500;&#65292;&#24182;&#23545;&#23395;&#33410;&#24615;&#21644;&#26085;&#24120;&#21464;&#21270;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#23427;&#20248;&#20110;&#36890;&#29992;&#23884;&#20837;&#65292;&#25104;&#21151;&#22320;&#20174;&#34903;&#26223;&#22270;&#20687;&#20013;&#35782;&#21035;&#20102;&#20262;&#25958;&#20303;&#25151;&#20379;&#24212;&#30340;&#28857;&#32423;&#21464;&#21270;&#65292;&#24182;&#21306;&#20998;&#20102;&#20027;&#35201;&#21644;&#27425;&#35201;&#21464;&#21270;&#12290;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#20026;&#22478;&#24066;&#35268;&#21010;&#25552;&#20379;&#21450;&#26102;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cities around the world face a critical shortage of affordable and decent housing. Despite its critical importance for policy, our ability to effectively monitor and track progress in urban housing is limited. Deep learning-based computer vision methods applied to street-level images have been successful in the measurement of socioeconomic and environmental inequalities but did not fully utilize temporal images to track urban change as time-varying labels are often unavailable. We used self-supervised methods to measure change in London using 15 million street images taken between 2008 and 2021. Our novel adaptation of Barlow Twins, Street2Vec, embeds urban structure while being invariant to seasonal and daily changes without manual annotations. It outperformed generic embeddings, successfully identified point-level change in London's housing supply from street-level images, and distinguished between major and minor change. This capability can provide timely information for urban plann
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#24320;&#25918;&#19990;&#30028;&#28145;&#24230;&#20266;&#36896;&#24402;&#22240;&#20219;&#21153;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#24402;&#22240;&#24615;&#33021;&#30340;&#26032;&#22522;&#20934;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#20840;&#23616;-&#23616;&#37096;&#25237;&#31080;&#27169;&#22359;&#21644;&#35774;&#35745;&#32622;&#20449;&#24230;-based&#30340;&#36719;&#20266;&#26631;&#31614;&#31574;&#30053;&#26469;&#25552;&#39640;&#24402;&#22240;&#20934;&#30830;&#24615;&#65292;&#24182;&#32531;&#35299;&#30456;&#20284;&#36896;&#25104;&#30340;&#20266;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2309.11132</link><description>&lt;p&gt;
&#23545;&#24320;&#25918;&#19990;&#30028;&#28145;&#24230;&#20266;&#36896;&#24402;&#22240;&#30340;&#23545;&#27604;&#20266;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Pseudo Learning for Open-World DeepFake Attribution. (arXiv:2309.11132v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11132
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#24320;&#25918;&#19990;&#30028;&#28145;&#24230;&#20266;&#36896;&#24402;&#22240;&#20219;&#21153;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#24402;&#22240;&#24615;&#33021;&#30340;&#26032;&#22522;&#20934;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#20840;&#23616;-&#23616;&#37096;&#25237;&#31080;&#27169;&#22359;&#21644;&#35774;&#35745;&#32622;&#20449;&#24230;-based&#30340;&#36719;&#20266;&#26631;&#31614;&#31574;&#30053;&#26469;&#25552;&#39640;&#24402;&#22240;&#20934;&#30830;&#24615;&#65292;&#24182;&#32531;&#35299;&#30456;&#20284;&#36896;&#25104;&#30340;&#20266;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#25104;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20026;&#20266;&#36896;&#38754;&#37096;&#36827;&#34892;&#24402;&#22240;&#30340;&#25361;&#25112;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#22312;GAN&#29983;&#25104;&#30340;&#38754;&#37096;&#26041;&#38754;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#65292;&#20294;&#19982;&#36523;&#20221;&#20132;&#25442;&#25110;&#34920;&#24773;&#36716;&#31227;&#30456;&#20851;&#30340;&#26356;&#20855;&#23041;&#32961;&#24615;&#30340;&#25915;&#20987;&#20173;&#28982;&#34987;&#24573;&#35270;&#12290;&#32780;&#22312;&#24320;&#25918;&#19990;&#30028;&#30340;&#26410;&#26631;&#35760;&#38754;&#37096;&#20013;&#38544;&#34255;&#30340;&#20266;&#36896;&#30165;&#36857;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#20026;&#20102;&#25512;&#21160;&#30456;&#20851;&#30340;&#21069;&#27839;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Open-World DeepFake Attribution (OW-DFA)&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22312;&#24320;&#25918;&#19990;&#30028;&#22330;&#26223;&#19979;&#23545;&#21508;&#31181;&#31867;&#22411;&#20266;&#36896;&#38754;&#37096;&#30340;&#24402;&#22240;&#24615;&#33021;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#20266;&#23398;&#20064;(Contrastive Pseudo Learning, CPL)&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;OW-DFA&#20219;&#21153;&#65292;&#36890;&#36807;1)&#24341;&#20837;&#20840;&#23616;-&#23616;&#37096;&#25237;&#31080;&#27169;&#22359;&#26469;&#24341;&#23548;&#19981;&#21516;&#25805;&#32437;&#21306;&#22495;&#30340;&#20266;&#36896;&#38754;&#37096;&#29305;&#24449;&#23545;&#40784;&#65292;2)&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#36719;&#20266;&#26631;&#31614;&#31574;&#30053;&#26469;&#32531;&#35299;&#30001;&#30456;&#20284;&#36896;&#25104;&#30340;&#20266;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge in sourcing attribution for forgery faces has gained widespread attention due to the rapid development of generative techniques. While many recent works have taken essential steps on GAN-generated faces, more threatening attacks related to identity swapping or expression transferring are still overlooked. And the forgery traces hidden in unknown attacks from the open-world unlabeled faces still remain under-explored. To push the related frontier research, we introduce a new benchmark called Open-World DeepFake Attribution (OW-DFA), which aims to evaluate attribution performance against various types of fake faces under open-world scenarios. Meanwhile, we propose a novel framework named Contrastive Pseudo Learning (CPL) for the OW-DFA task through 1) introducing a Global-Local Voting module to guide the feature alignment of forged faces with different manipulated regions, 2) designing a Confidence-based Soft Pseudo-label strategy to mitigate the pseudo-noise caused by simi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#24179;&#21488;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#35782;&#21035;&#20551;&#26032;&#38395;&#65292;&#21516;&#26102;&#25552;&#20379;&#23454;&#26102;&#20998;&#26512;&#21644;&#39564;&#35777;&#26032;&#38395;&#25991;&#31456;&#30495;&#23454;&#24615;&#30340;&#29992;&#25143;&#21451;&#22909;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2309.11052</link><description>&lt;p&gt;
Fake News BR: &#19968;&#31181;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Fake News BR: A Fake News Detection Platform for Brazilian Portuguese. (arXiv:2309.11052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#24179;&#21488;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#35782;&#21035;&#20551;&#26032;&#38395;&#65292;&#21516;&#26102;&#25552;&#20379;&#23454;&#26102;&#20998;&#26512;&#21644;&#39564;&#35777;&#26032;&#38395;&#25991;&#31456;&#30495;&#23454;&#24615;&#30340;&#29992;&#25143;&#21451;&#22909;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20551;&#26032;&#38395;&#20256;&#25773;&#35823;&#23548;&#20844;&#20247;&#33286;&#35770;&#30340;&#28508;&#21147;&#65292;&#20854;&#20256;&#25773;&#24050;&#25104;&#20026;&#36817;&#26399;&#20851;&#27880;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#23545;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20013;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#26032;&#38395;&#31867;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#21253;&#25324;TF-IDF&#21644;Word2Vec&#65292;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#20998;&#31867;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#22914;&#36923;&#36753;&#22238;&#24402;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;AdaBoost&#21644;LightGBM&#65292;&#20351;&#29992;&#21253;&#21547;&#30495;&#23454;&#21644;&#20551;&#26032;&#38395;&#25991;&#31456;&#30340;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;F1&#24471;&#20998;&#19978;&#37117;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#65292;&#35777;&#26126;&#20102;&#20854;&#35782;&#21035;&#20551;&#26032;&#38395;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#32593;&#31449;&#24179;&#21488;FAKENEWSBR.COM&#65292;&#20197;&#20415;&#39564;&#35777;&#26032;&#38395;&#25991;&#31456;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#30340;&#24179;&#21488;&#25552;&#20379;&#23454;&#26102;&#20998;&#26512;&#65292;&#20801;&#35768;&#29992;&#25143;&#26816;&#26597;&#26032;&#38395;&#25991;&#31456;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of fake news has become a significant concern in recent times due to its potential to spread misinformation and manipulate public opinion. In this paper, we present a comprehensive study on the detection of fake news in Brazilian Portuguese, focusing on journalistic-type news. We propose a machine learning-based approach that leverages natural language processing techniques, including TF-IDF and Word2Vec, to extract features from textual data. We evaluate the performance of various classification algorithms, such as logistic regression, support vector machine, random forest, AdaBoost, and LightGBM, on a dataset containing both true and fake news articles. The proposed approach achieves a high level of accuracy and F1-Score, demonstrating its effectiveness in identifying fake news. Additionally, we develop a user-friendly web platform, FAKENEWSBR.COM, to facilitate the verification of news articles' veracity. Our platform provides real-time analysis, allowing users to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#25216;&#26415;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#30340;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#21644;&#20219;&#21153;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.10916</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#21644;&#24433;&#21709;&#20989;&#25968;&#65292;&#25105;&#20204;&#33021;&#20174;&#23545;&#25239;&#26679;&#26412;&#20013;&#33719;&#24471;&#20160;&#20040;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#25216;&#26415;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#30340;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#21644;&#20219;&#21153;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#26159;&#36890;&#36807;&#24494;&#23567;&#25200;&#21160;&#26469;&#27450;&#39575;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#65292;&#36215;&#21021;&#22312;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#30740;&#31350;&#65292;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20063;&#24320;&#22987;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26816;&#27979;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36755;&#20837;&#25200;&#21160;&#30340;&#25628;&#32034;&#65292;&#20294;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#24050;&#32463;&#21457;&#23637;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#34920;&#24449;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#12290;&#26412;&#25991;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#65292;&#19968;&#31181;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#12290;&#29305;&#21035;&#26159;&#21069;&#32773;&#30456;&#27604;&#20960;&#20010;&#24378;&#22522;&#20934;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65307;&#27492;&#22806;&#65292;&#23545;&#24433;&#21709;&#20989;&#25968;&#30340;&#26032;&#39062;&#20351;&#29992;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#26681;&#25454;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples, deliberately crafted using small perturbations to fool deep neural networks, were first studied in image processing and more recently in NLP. While approaches to detecting adversarial examples in NLP have largely relied on search over input perturbations, image processing has seen a range of techniques that aim to characterise adversarial subspaces over the learned representations.  In this paper, we adapt two such approaches to NLP, one based on nearest neighbors and influence functions and one on Mahalanobis distances. The former in particular produces a state-of-the-art detector when compared against several strong baselines; moreover, the novel use of influence functions provides insight into how the nature of adversarial example subspaces in NLP relate to those in image processing, and also how they differ depending on the kind of NLP task.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#19981;&#21516;&#27169;&#24335;&#30340;&#36861;&#36394;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#26469;&#21306;&#20998;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#21644;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#12290;</title><link>http://arxiv.org/abs/2309.10688</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#19981;&#21516;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
On the different regimes of Stochastic Gradient Descent. (arXiv:2309.10688v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10688
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#19981;&#21516;&#27169;&#24335;&#30340;&#36861;&#36394;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#26469;&#21306;&#20998;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#21644;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#32593;&#32476;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20851;&#38190;&#21442;&#25968;&#26159;&#27599;&#20010;&#27493;&#39588;&#32771;&#34385;&#30340;&#25968;&#25454;&#37327;&#25110;&#25209;&#37327;&#22823;&#23567;B&#20197;&#21450;&#27493;&#38271;&#25110;&#23398;&#20064;&#29575;&#951;&#12290;&#23545;&#20110;&#23567;&#30340;B&#21644;&#22823;&#30340;&#951;&#65292;SGD&#23545;&#24212;&#20110;&#21442;&#25968;&#30340;&#38543;&#26426;&#28436;&#21270;&#65292;&#20854;&#22122;&#22768;&#24133;&#24230;&#30001;&#8220;&#28201;&#24230;&#8221;T=&#951;/B&#25511;&#21046;&#12290;&#28982;&#32780;&#24403;&#25209;&#37327;&#22823;&#23567;B&#8805;B^*&#36275;&#22815;&#22823;&#26102;&#65292;&#36825;&#31181;&#25551;&#36848;&#34987;&#35266;&#23519;&#21040;&#22833;&#25928;&#65292;&#25110;&#32773;&#22312;&#28201;&#24230;&#36275;&#22815;&#23567;&#26102;&#31616;&#21270;&#20026;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#12290;&#29702;&#35299;&#36825;&#20123;&#20132;&#21449;&#21457;&#29983;&#30340;&#20301;&#32622;&#20173;&#28982;&#26159;&#19968;&#20010;&#20013;&#24515;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#19968;&#20010;&#25945;&#24072;-&#23398;&#29983;&#24863;&#30693;&#22120;&#20998;&#31867;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20851;&#38190;&#39044;&#27979;&#20173;&#36866;&#29992;&#20110;&#28145;&#24230;&#32593;&#32476;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;B-&#951;&#24179;&#38754;&#19978;&#33719;&#24471;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#65292;&#23558;&#19977;&#20010;&#21160;&#24577;&#38454;&#27573;&#20998;&#24320;&#65306;&#65288;i&#65289;&#21463;&#28201;&#24230;&#25511;&#21046;&#30340;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#65292;&#65288;ii&#65289;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#21644;
&lt;/p&gt;
&lt;p&gt;
Modern deep networks are trained with stochastic gradient descent (SGD) whose key parameters are the number of data considered at each step or batch size $B$, and the step size or learning rate $\eta$. For small $B$ and large $\eta$, SGD corresponds to a stochastic evolution of the parameters, whose noise amplitude is governed by the `temperature' $T\equiv \eta/B$. Yet this description is observed to break down for sufficiently large batches $B\geq B^*$, or simplifies to gradient descent (GD) when the temperature is sufficiently small. Understanding where these cross-overs take place remains a central challenge. Here we resolve these questions for a teacher-student perceptron classification model, and show empirically that our key predictions still apply to deep networks. Specifically, we obtain a phase diagram in the $B$-$\eta$ plane that separates three dynamical phases: $\textit{(i)}$ a noise-dominated SGD governed by temperature, $\textit{(ii)}$ a large-first-step-dominated SGD and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#23398;&#20064;&#20449;&#36947;&#20998;&#24067;&#20174;&#32780;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#31471;&#21040;&#31471;&#31526;&#21495;&#35823;&#30721;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10505</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Learning End-to-End Channel Coding with Diffusion Models. (arXiv:2309.10505v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#23398;&#20064;&#20449;&#36947;&#20998;&#24067;&#20174;&#32780;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#31471;&#21040;&#31471;&#31526;&#21495;&#35823;&#30721;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#36817;&#20284;&#20449;&#36947;&#20998;&#24067;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;&#36890;&#36807;&#19982;&#21508;&#31181;&#20449;&#36947;&#27169;&#22411;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25193;&#25955;&#27169;&#22411;&#31934;&#30830;&#23398;&#20064;&#20449;&#36947;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#31471;&#21040;&#31471;&#31526;&#21495;&#35823;&#30721;&#29575;&#65288;SER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of neural encoders via deep learning necessitates a differentiable channel model due to the backpropagation algorithm. This requirement can be sidestepped by approximating either the channel distribution or its gradient through pilot signals in real-world scenarios. The initial approach draws upon the latest advancements in image generation, utilizing generative adversarial networks (GANs) or their enhanced variants to generate channel distributions. In this paper, we address this channel approximation challenge with diffusion models, which have demonstrated high sample quality in image generation. We offer an end-to-end channel coding framework underpinned by diffusion models and propose an efficient training algorithm. Our simulations with various channel models establish that our diffusion models learn the channel distribution accurately, thereby achieving near-optimal end-to-end symbol error rates (SERs). We also note a significant advantage of diffusion models: A robu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#35821;&#38899;&#38899;&#39057;&#30340;&#20154;&#32676;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer&#27169;&#22411;&#23454;&#29616;&#21307;&#38498;&#20505;&#35786;&#23460;&#30340;&#21344;&#29992;&#39044;&#27979;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#26159;&#39318;&#27425;&#25552;&#20986;&#20351;&#29992;&#38750;&#35821;&#38899;&#38899;&#39057;&#20449;&#21495;&#36827;&#34892;&#21344;&#29992;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10280</link><description>&lt;p&gt;
Crowdotic&#65306;&#22522;&#20110;Transformer&#30340;&#21307;&#38498;&#20505;&#35786;&#23460;&#38750;&#35821;&#38899;&#38899;&#39057;&#19982;&#24046;&#20998;&#38544;&#31169;&#30340;&#21344;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Crowdotic: Transformer-based Occupancy Estimation for Hospital Waiting Rooms with Non-speech Audio and Differential Privacy. (arXiv:2309.10280v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#35821;&#38899;&#38899;&#39057;&#30340;&#20154;&#32676;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer&#27169;&#22411;&#23454;&#29616;&#21307;&#38498;&#20505;&#35786;&#23460;&#30340;&#21344;&#29992;&#39044;&#27979;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#26159;&#39318;&#27425;&#25552;&#20986;&#20351;&#29992;&#38750;&#35821;&#38899;&#38899;&#39057;&#20449;&#21495;&#36827;&#34892;&#21344;&#29992;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#20154;&#32676;&#23494;&#24230;&#20998;&#26512;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#22312;&#25552;&#39640;&#26234;&#33021;&#24314;&#31569;&#36816;&#33829;&#31649;&#29702;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#19981;&#21516;&#31354;&#38388;&#38544;&#31169;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#35821;&#38899;&#38899;&#39057;&#30340;&#20154;&#32676;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#38750;&#35821;&#38899;&#38899;&#39057;&#23601;&#21487;&#20197;&#23454;&#29616;&#36825;&#31181;&#20998;&#26512;&#65292;&#32780;&#19988;&#20934;&#30830;&#24615;&#21313;&#20998;&#20986;&#33394;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#25552;&#20986;&#20351;&#29992;&#38750;&#35821;&#38899;&#38899;&#39057;&#20449;&#21495;&#26469;&#39044;&#27979;&#21344;&#29992;&#24773;&#20917;&#12290;&#25130;&#33267;&#30446;&#21069;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;&#26377;&#20854;&#20182;&#31867;&#20284;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#19968;&#23478;&#22823;&#22411;&#21307;&#38498;&#30340;&#20505;&#35786;&#23460;&#20013;&#37096;&#32626;&#20102;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#24179;&#21488;&#65292;&#24182;&#33719;&#24471;&#20102;IRB&#25209;&#20934;&#65292;&#22312;&#25968;&#20010;&#26376;&#30340;&#26102;&#38388;&#37324;&#25429;&#33719;&#20102;&#38750;&#35821;&#38899;&#38899;&#39057;&#21644;&#28909;&#20687;&#22270;&#20197;&#29992;&#20110;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#38750;&#35821;&#38899;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#22522;&#20110;&#28909;&#20687;&#25668;&#20687;&#22836;&#30340;&#27169;&#22411;&#21644;&#20854;&#20182;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving crowd density analysis finds application across a wide range of scenarios, substantially enhancing smart building operation and management while upholding privacy expectations in various spaces. We propose a non-speech audio-based approach for crowd analytics, leveraging a transformer-based model. Our results demonstrate that non-speech audio alone can be used to conduct such analysis with remarkable accuracy. To the best of our knowledge, this is the first time when non-speech audio signals are proposed for predicting occupancy. As far as we know, there has been no other similar approach of its kind prior to this. To accomplish this, we deployed our sensor-based platform in the waiting room of a large hospital with IRB approval over a period of several months to capture non-speech audio and thermal images for the training and evaluation of our models. The proposed non-speech-based approach outperformed the thermal camera-based model and all other baselines. In addit
&lt;/p&gt;</description></item><item><title>FedGKD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#31471;&#22270;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#25552;&#21462;&#26356;&#22909;&#30340;&#20219;&#21153;&#29305;&#24449;&#24182;&#24341;&#20837;&#24863;&#30693;&#20840;&#23616;&#21327;&#20316;&#32467;&#26500;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;GNN&#31995;&#32479;&#20013;&#22270;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09517</link><description>&lt;p&gt;
FedGKD:&#22312;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#37322;&#25918;&#21327;&#20316;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
FedGKD: Unleashing the Power of Collaboration in Federated Graph Neural Networks. (arXiv:2309.09517v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09517
&lt;/p&gt;
&lt;p&gt;
FedGKD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#31471;&#22270;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#25552;&#21462;&#26356;&#22909;&#30340;&#20219;&#21153;&#29305;&#24449;&#24182;&#24341;&#20837;&#24863;&#30693;&#20840;&#23616;&#21327;&#20316;&#32467;&#26500;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;GNN&#31995;&#32479;&#20013;&#22270;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#30001;&#20110;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#33021;&#22815;&#22312;&#25968;&#25454;&#38548;&#31163;&#22330;&#26223;&#19979;&#25191;&#34892;&#19982;&#22270;&#30456;&#20851;&#30340;&#20219;&#21153;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#32852;&#37030;&#35757;&#32451;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;GNN&#31995;&#32479;&#20013;&#30340;&#22270;&#24322;&#26500;&#24615;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#32479;&#35745;&#37327;&#26469;&#34920;&#31034;&#23616;&#37096;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#32858;&#21512;&#26426;&#21046;&#23558;&#23427;&#20204;&#32852;&#31995;&#36215;&#26469;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#25928;&#29575;&#26377;&#38480;&#65306;&#20219;&#21153;&#30456;&#20851;&#24615;&#37327;&#21270;&#30340;&#36136;&#37327;&#20302;&#21644;&#21033;&#29992;&#21327;&#20316;&#32467;&#26500;&#30340;&#26080;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedGKD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;GNN&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23458;&#25143;&#31471;&#22270;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#25552;&#21462;&#26356;&#22909;&#22320;&#25551;&#36848;&#20219;&#21153;&#30456;&#20851;&#24615;&#30340;&#20219;&#21153;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#33021;&#22815;&#24863;&#30693;&#21040;&#20840;&#23616;&#30340;&#21327;&#20316;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;FedGKD&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated training of Graph Neural Networks (GNN) has become popular in recent years due to its ability to perform graph-related tasks under data isolation scenarios while preserving data privacy. However, graph heterogeneity issues in federated GNN systems continue to pose challenges. Existing frameworks address the problem by representing local tasks using different statistics and relating them through a simple aggregation mechanism. However, these approaches suffer from limited efficiency from two aspects: low quality of task-relatedness quantification and inefficacy of exploiting the collaboration structure. To address these issues, we propose FedGKD, a novel federated GNN framework that utilizes a novel client-side graph dataset distillation method to extract task features that better describe task-relatedness, and introduces a novel server-side aggregation mechanism that is aware of the global collaboration structure. We conduct extensive experiments on six real-world datasets of
&lt;/p&gt;</description></item><item><title>Drifter&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#31995;&#32479;&#65292;&#36890;&#36807;&#25935;&#25463;&#12289;&#21709;&#24212;&#21644;&#36866;&#24212;&#24615;&#30340;&#25968;&#25454;&#36136;&#37327;&#30417;&#25511;&#65292;&#23454;&#26102;&#20998;&#26512;&#12289;&#26816;&#27979;&#21644;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#38382;&#39064;&#65292;&#20351;&#24471;&#23454;&#26102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.08617</link><description>&lt;p&gt;
Drifter: &#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25928;&#30340;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#20197;&#25552;&#39640;&#25968;&#25454;&#23436;&#25972;&#24615;
&lt;/p&gt;
&lt;p&gt;
Drifter: Efficient Online Feature Monitoring for Improved Data Integrity in Large-Scale Recommendation Systems. (arXiv:2309.08617v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08617
&lt;/p&gt;
&lt;p&gt;
Drifter&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#31995;&#32479;&#65292;&#36890;&#36807;&#25935;&#25463;&#12289;&#21709;&#24212;&#21644;&#36866;&#24212;&#24615;&#30340;&#25968;&#25454;&#36136;&#37327;&#30417;&#25511;&#65292;&#23454;&#26102;&#20998;&#26512;&#12289;&#26816;&#27979;&#21644;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#38382;&#39064;&#65292;&#20351;&#24471;&#23454;&#26102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#29983;&#20135;&#31995;&#32479;&#36890;&#24120;&#38754;&#20020;&#22312;&#22823;&#35268;&#27169;&#12289;&#21160;&#24577;&#27969;&#20013;&#32500;&#25252;&#25968;&#25454;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Drifter&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#21644;&#39564;&#35777;&#30340;&#39640;&#25928;&#19988;&#36731;&#37327;&#32423;&#30340;&#31995;&#32479;&#12290;Drifter&#36890;&#36807;&#25552;&#20379;&#25935;&#25463;&#12289;&#21709;&#24212;&#21644;&#36866;&#24212;&#24615;&#30340;&#25968;&#25454;&#36136;&#37327;&#30417;&#25511;&#65292;&#33021;&#22815;&#23454;&#26102;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12289;&#28418;&#31227;&#26816;&#27979;&#20197;&#21450;&#23545;&#26377;&#38382;&#39064;&#30340;&#29983;&#20135;&#20107;&#20214;&#30340;&#27934;&#23519;&#12290;Drifter&#38598;&#25104;&#20102;&#26368;&#20808;&#36827;&#30340;&#31232;&#30095;&#25968;&#25454;&#22312;&#32447;&#29305;&#24449;&#25490;&#21517;&#21644;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#36164;&#28304;&#25928;&#29575;&#24615;&#65292;&#27599;&#20998;&#38047;&#22788;&#29702;&#25968;&#30334;&#19975;&#20010;&#23454;&#20363;&#20165;&#38656;&#35201;&#20004;&#20010;&#32447;&#31243;&#21644;&#23569;&#20110;&#19968;GB&#30340;RAM&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;Drifter&#22312;&#35686;&#25253;&#21644;&#32531;&#35299;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#23454;&#26102;&#23454;&#20917;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world production systems often grapple with maintaining data quality in large-scale, dynamic streams. We introduce Drifter, an efficient and lightweight system for online feature monitoring and verification in recommendation use cases. Drifter addresses limitations of existing methods by delivering agile, responsive, and adaptable data quality monitoring, enabling real-time root cause analysis, drift detection and insights into problematic production events. Integrating state-of-the-art online feature ranking for sparse data and anomaly detection ideas, Drifter is highly scalable and resource-efficient, requiring only two threads and less than a gigabyte of RAM per production deployments that handle millions of instances per minute. Evaluation on real-world data sets demonstrates Drifter's effectiveness in alerting and mitigating data quality issues, substantially improving reliability and performance of real-time live recommender systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#21160;&#20316;&#25968;&#25454;&#30340;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#12290;&#36890;&#36807;&#31526;&#21495;&#35745;&#21010;&#12289;&#35270;&#39057;&#25193;&#25955;&#21644;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#26032;&#29615;&#22659;&#20013;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08587</link><description>&lt;p&gt;
&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Compositional Foundation Models for Hierarchical Planning. (arXiv:2309.08587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#21160;&#20316;&#25968;&#25454;&#30340;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#12290;&#36890;&#36807;&#31526;&#21495;&#35745;&#21010;&#12289;&#35270;&#39057;&#25193;&#25955;&#21644;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#26032;&#29615;&#22659;&#20013;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26032;&#29615;&#22659;&#20013;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#38656;&#35201;&#36827;&#34892;&#36328;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#30340;&#23618;&#27425;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#20998;&#21035;&#23545;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#21160;&#20316;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20849;&#21516;&#35299;&#20915;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#22312;&#29615;&#22659;&#20013;&#25166;&#26681;&#30340;&#31526;&#21495;&#35745;&#21010;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#29983;&#25104;&#30340;&#35270;&#39057;&#35745;&#21010;&#36890;&#36807;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#19982;&#35270;&#35273;-&#21160;&#20316;&#25511;&#21046;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#22312;&#27492;&#23618;&#27425;&#32467;&#26500;&#20013;&#36827;&#34892;&#26377;&#25928;&#25512;&#29702;&#65292;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#24378;&#21046;&#20445;&#25345;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make effective decisions in novel environments with long-horizon goals, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, visually reasoning about the underlying plans, and executing actions in accordance with the devised plan through visual-motor control. We propose Compositional Foundation Models for Hierarchical Planning (HiP), a foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks. We use a large language model to construct symbolic plans that are grounded in the environment through a large video diffusion model. Generated video plans are then grounded to visual-motor control, through an inverse dynamics model that infers actions from generated videos. To enable effective reasoning within this hierarchy, we enforce consistency between the models via iterative refinement. We illustr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#39640;&#26031;-&#21202;&#35753;&#24503;&#33410;&#28857;&#19978;&#36827;&#34892;&#28508;&#22312;&#31354;&#38388;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#30830;&#20445;&#22312;&#21387;&#32553;&#36807;&#31243;&#20013;&#20445;&#25345;&#25299;&#25169;&#25968;&#25454;&#32467;&#26500;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08228</link><description>&lt;p&gt;
&#22312;&#39640;&#26031;&#65293;&#21202;&#35753;&#24503;&#33410;&#28857;&#19978;&#30001;&#20110;&#28508;&#22312;&#31354;&#38388;&#27491;&#21017;&#21270;&#32780;&#21387;&#32553;&#20013;&#20445;&#25345;&#25299;&#25169;&#25968;&#25454;&#32467;&#26500;&#23436;&#25972;&#24615;&#30340;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
Ensuring Toplogical Data-Structure Preservation under Autoencoder Compression due to Latent Space Regularization in Gauss--Legendre nodes. (arXiv:2309.08228v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08228
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#39640;&#26031;-&#21202;&#35753;&#24503;&#33410;&#28857;&#19978;&#36827;&#34892;&#28508;&#22312;&#31354;&#38388;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#30830;&#20445;&#22312;&#21387;&#32553;&#36807;&#31243;&#20013;&#20445;&#25345;&#25299;&#25169;&#25968;&#25454;&#32467;&#26500;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#19968;&#33324;&#30340;&#26080;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#21046;&#23450;&#20102;&#19968;&#20010;&#25968;&#25454;&#26080;&#20851;&#30340;&#28508;&#22312;&#31354;&#38388;&#27491;&#21017;&#21270;&#32422;&#26463;&#12290;&#35813;&#27491;&#21017;&#21270;&#22522;&#20110;&#22312;&#21202;&#35753;&#24503;&#33410;&#28857;&#19978;&#23545;&#33258;&#32534;&#30721;&#22120;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#20123;&#33410;&#28857;&#26159;&#39640;&#26031;-&#21202;&#35753;&#24503;&#31215;&#20998;&#30340;&#20013;&#24515;&#12290;&#37325;&#26032;&#23457;&#35270;&#36825;&#20010;&#32463;&#20856;&#38382;&#39064;&#33021;&#22815;&#35777;&#26126;&#65292;&#32463;&#36807;&#27491;&#21017;&#21270;&#30340;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#23558;&#21021;&#22987;&#25968;&#25454;&#27969;&#24418;&#19968;&#23545;&#19968;&#22320;&#37325;&#26032;&#23884;&#20837;&#21040;&#20854;&#28508;&#22312;&#34920;&#31034;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20043;&#21069;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#65288;&#22914;&#25910;&#32553;&#33258;&#32534;&#30721;&#65289;&#22312;&#31616;&#21333;&#31034;&#20363;&#20013;&#24050;&#32463;&#23548;&#33268;&#20102;&#25299;&#25169;&#32570;&#38519;&#65292;&#32780;&#22522;&#20110;&#21367;&#31215;&#30340;&#65288;&#21464;&#20998;&#65289;&#33258;&#32534;&#30721;&#22120;&#20063;&#26159;&#22914;&#27492;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#36129;&#29486;&#65292;&#26631;&#20934;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#22312;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#24050;&#32463;&#30830;&#20445;&#20102;&#25299;&#25169;&#23436;&#25972;&#24615;&#12290;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#36866;&#29992;&#20110;&#32463;&#20856;&#30340;FashionMNIST&#25968;&#25454;&#38598;&#20197;&#21450;MRI&#33041;&#37096;&#25195;&#25551;&#30340;&#30495;&#23454;&#19990;&#30028;&#32534;&#30721;&#38382;&#39064;&#65292;&#36825;&#34920;&#26126;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#39640;&#32500;&#25968;&#25454;&#65292;&#21487;&#38752;&#30340;&#20302;&#32500;&#34920;&#31034;&#24050;&#24471;&#20197;&#30830;&#20445;&#12290;
&lt;/p&gt;
&lt;p&gt;
We formulate a data independent latent space regularisation constraint for general unsupervised autoencoders. The regularisation rests on sampling the autoencoder Jacobian in Legendre nodes, being the centre of the Gauss-Legendre quadrature. Revisiting this classic enables to prove that regularised autoencoders ensure a one-to-one re-embedding of the initial data manifold to its latent representation. Demonstrations show that prior proposed regularisation strategies, such as contractive autoencoding, cause topological defects already for simple examples, and so do convolutional based (variational) autoencoders. In contrast, topological preservation is ensured already by standard multilayer perceptron neural networks when being regularised due to our contribution. This observation extends through the classic FashionMNIST dataset up to real world encoding problems for MRI brain scans, suggesting that, across disciplines, reliable low dimensional representations of complex high-dimensiona
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#20018;&#34892;&#39134;&#31186;&#26230;&#20307;&#23398;&#20013;&#36890;&#36807;&#24369;&#30417;&#30563;&#31639;&#27861;&#23545;&#34893;&#23556;&#22270;&#36827;&#34892;&#20998;&#31867;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#23613;&#21487;&#33021;&#20943;&#23569;&#35757;&#32451;&#25152;&#38656;&#30340;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#12290;</title><link>http://arxiv.org/abs/2309.04474</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#22312;&#20018;&#34892;&#39134;&#31186;&#26230;&#20307;&#23398;&#20013;&#30340;&#27169;&#24335;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning for pattern classification in serial femtosecond crystallography. (arXiv:2309.04474v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#20018;&#34892;&#39134;&#31186;&#26230;&#20307;&#23398;&#20013;&#36890;&#36807;&#24369;&#30417;&#30563;&#31639;&#27861;&#23545;&#34893;&#23556;&#22270;&#36827;&#34892;&#20998;&#31867;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#23613;&#21487;&#33021;&#20943;&#23569;&#35757;&#32451;&#25152;&#38656;&#30340;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
X&#23556;&#32447;&#33258;&#30001;&#30005;&#23376;&#28608;&#20809;&#35774;&#26045;&#19978;&#30340;&#20018;&#34892;&#39134;&#31186;&#26230;&#20307;&#23398;&#20026;&#26230;&#20307;&#32467;&#26500;&#30340;&#30830;&#23450;&#24320;&#36767;&#20102;&#26032;&#30340;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23454;&#39564;&#30340;&#25968;&#25454;&#22788;&#29702;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#30830;&#23450;&#39640;&#20998;&#36776;&#29575;&#32467;&#26500;&#25152;&#38656;&#30340;&#34893;&#23556;&#22270;&#30340;&#24635;&#25968;&#24040;&#22823;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22788;&#29702;&#22914;&#27492;&#22823;&#37327;&#30340;&#25968;&#25454;&#26041;&#38754;&#21487;&#33021;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#24335;&#20998;&#31867;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#38656;&#35201;&#24102;&#26377;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#23545;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#24378;&#20381;&#36182;&#20005;&#37325;&#38480;&#21046;&#20102;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#22240;&#20026;&#27880;&#37322;&#22823;&#37327;&#34893;&#23556;&#22270;&#38750;&#24120;&#26114;&#36149;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#24369;&#30417;&#30563;&#31639;&#27861;&#19979;&#23545;&#34893;&#23556;&#22270;&#36827;&#34892;&#20998;&#31867;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#23613;&#21487;&#33021;&#20943;&#23569;&#35757;&#32451;&#25152;&#38656;&#30340;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Serial femtosecond crystallography at X-ray free electron laser facilities opens a new era for the determination of crystal structure. However, the data processing of those experiments is facing unprecedented challenge, because the total number of diffraction patterns needed to determinate a high-resolution structure is huge. Machine learning methods are very likely to play important roles in dealing with such a large volume of data. Convolutional neural networks have made a great success in the field of pattern classification, however, training of the networks need very large datasets with labels. Th is heavy dependence on labeled datasets will seriously restrict the application of networks, because it is very costly to annotate a large number of diffraction patterns. In this article we present our job on the classification of diffraction pattern by weakly supervised algorithms, with the aim of reducing as much as possible the size of the labeled dataset required for training. Our res
&lt;/p&gt;</description></item><item><title>&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550; ConDA&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#33719;&#21462;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.03992</link><description>&lt;p&gt;
ConDA: &#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03992
&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550; ConDA&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#33719;&#21462;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#21508;&#31181;&#29992;&#36884;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#21253;&#25324;&#26032;&#38395;&#25253;&#36947;&#12290;&#37492;&#20110;&#36825;&#20123;LLMs&#21487;&#33021;&#34987;&#24694;&#24847;&#20351;&#29992;&#26469;&#22823;&#35268;&#27169;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24037;&#20855;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#26032;&#30340;LLMs&#19981;&#26029;&#34987;&#24320;&#21457;&#65292;&#33719;&#21462;&#29992;&#20110;&#30417;&#30563;&#24335;&#26816;&#27979;&#22120;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#25104;&#20026;&#19968;&#20010;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#27809;&#26377;&#20851;&#20110;&#20854;&#29983;&#25104;&#22120;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27492;&#25968;&#25454;&#38382;&#39064;&#65292;&#21363;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#65292;&#24182;&#23558;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#20219;&#21153;&#12290;&#36825;&#37324;&#30340;&#22495;&#26159;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#22120;&#65292;&#21363;LLMs&#65292;&#25105;&#20204;&#20551;&#35774;&#21482;&#33021;&#35775;&#38382;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;ConDA&#30340;&#23545;&#27604;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#23558;&#26631;&#20934;&#30340;&#22495;&#36866;&#24212;&#25216;&#26415;&#19982;&#34920;&#31034;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.10792</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#25351;&#20197;&#30417;&#30563;&#26041;&#24335;&#22312;&#21253;&#21547;&#8220;&#25351;&#20196;-&#36755;&#20986;&#8221;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;LLM&#65292;&#36825;&#23558;LLM&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#30446;&#26631;&#19982;&#29992;&#25143;&#24076;&#26395;LLM&#36981;&#23432;&#20154;&#31867;&#25351;&#20196;&#30340;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;IT&#30340;&#24120;&#35268;&#26041;&#27861;&#12289;IT&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#12289;IT&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#24212;&#29992;&#20110;&#19981;&#21516;&#27169;&#24577;&#12289;&#39046;&#22495;&#21644;&#24212;&#29992;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23545;&#24433;&#21709;IT&#32467;&#26524;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20998;&#26512;&#65288;&#20363;&#22914;&#65292;&#25351;&#20196;&#36755;&#20986;&#30340;&#29983;&#25104;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;IT&#30340;&#28508;&#22312;&#38382;&#39064;&#20197;&#21450;&#38024;&#23545;&#20854;&#30340;&#25209;&#35780;&#65292;&#20197;&#21450;&#25351;&#20986;&#24403;&#21069;&#19981;&#36275;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
&lt;/p&gt;</description></item><item><title>ALI-DPFL&#26159;&#19968;&#31181;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#26469;&#20248;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.10457</link><description>&lt;p&gt;
ALI-DPFL: &#20855;&#26377;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations. (arXiv:2308.10457v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10457
&lt;/p&gt;
&lt;p&gt;
ALI-DPFL&#26159;&#19968;&#31181;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#26469;&#20248;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#20849;&#20139;&#35757;&#32451;&#21442;&#25968;&#32780;&#19981;&#26159;&#21407;&#22987;&#25968;&#25454;&#65292;&#20801;&#35768;&#22810;&#20010;&#35774;&#22791;&#25110;&#32452;&#32455;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#23545;&#36825;&#20123;&#35757;&#32451;&#21442;&#25968;&#30340;&#25512;&#29702;&#25915;&#20987;&#65288;&#20363;&#22914;&#24046;&#20998;&#25915;&#20987;&#65289;&#26469;&#25512;&#26029;&#20010;&#20307;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#24046;&#20998;&#38544;&#31169;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#20197;&#38450;&#27490;&#27492;&#31867;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#22330;&#26223;&#20013;&#32771;&#34385;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#65292;&#20854;&#20013;&#26082;&#26377;&#38544;&#31169;&#39044;&#31639;&#21463;&#38480;&#65292;&#21448;&#26377;&#36890;&#20449;&#36718;&#27425;&#21463;&#38480;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25910;&#25947;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#25214;&#21040;&#22312;&#20219;&#24847;&#20004;&#20010;&#39034;&#24207;&#20840;&#23616;&#26356;&#26032;&#20043;&#38388;&#30340;&#23458;&#25143;&#26426;&#20043;&#38388;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#26412;&#22320;&#36845;&#20195;&#27425;&#25968;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;ALI-DPFL&#65289;&#12290;&#25105;&#20204;&#22312;FashionMNIST&#21644;CIFAR10&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#26174;&#33879;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning technique that allows model training among multiple devices or organizations by sharing training parameters instead of raw data. However, adversaries can still infer individual information through inference attacks (e.g. differential attacks) on these training parameters. As a result, Differential Privacy (DP) has been widely used in FL to prevent such attacks. We consider differentially private federated learning in a resource-constrained scenario, where both privacy budget and communication round are constrained. By theoretically analyzing the convergence, we can find the optimal number of differentially private local iterations for clients between any two sequential global updates. Based on this, we design an algorithm of differentially private federated learning with adaptive local iterations (ALI-DPFL). We experiment our algorithm on the FashionMNIST and CIFAR10 datasets, and demonstrate significantly better performances th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#65292;&#36890;&#36807;&#24456;&#23569;&#30340;&#31034;&#20363;&#21644;&#26368;&#23567;&#30340;&#35745;&#31639;&#36164;&#28304;&#39640;&#25928;&#22320;&#24494;&#35843;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20351;&#20854;&#26356;&#31526;&#21512;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07929</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#36827;&#34892;&#24555;&#36895;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Fast Adaptation with Bradley-Terry Preference Models in Text-To-Image Classification and Generation. (arXiv:2308.07929v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#65292;&#36890;&#36807;&#24456;&#23569;&#30340;&#31034;&#20363;&#21644;&#26368;&#23567;&#30340;&#35745;&#31639;&#36164;&#28304;&#39640;&#25928;&#22320;&#24494;&#35843;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20351;&#20854;&#26356;&#31526;&#21512;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22914;CLIP&#21644;Stable Diffusion&#22312;&#22522;&#30784;&#29702;&#35770;&#21644;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#22823;&#23567;&#21644;&#35745;&#31639;&#35201;&#27714;&#22686;&#21152;&#65292;&#29992;&#25143;&#20026;&#29305;&#23450;&#20219;&#21153;&#25110;&#20559;&#22909;&#20010;&#24615;&#21270;&#23427;&#20204;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#20043;&#21069;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#29305;&#23450;&#20154;&#31867;&#20559;&#22909;&#38598;&#21512;&#30340;&#38382;&#39064;&#65292;&#23558;&#26816;&#32034;&#25110;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#29992;&#25143;&#30340;&#20559;&#22909;&#23545;&#40784;&#12290;&#25105;&#20204;&#21033;&#29992;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24456;&#23569;&#30340;&#31034;&#20363;&#21644;&#26368;&#23567;&#30340;&#35745;&#31639;&#36164;&#28304;&#39640;&#25928;&#22320;&#24494;&#35843;&#21407;&#22987;&#27169;&#22411;&#12290;&#36890;&#36807;&#19982;&#22810;&#27169;&#24577;&#25991;&#26412;&#21644;&#22270;&#20687;&#29702;&#35299;&#30456;&#20851;&#30340;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#39564;&#35777;&#25454;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20010;&#26694;&#26550;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large multimodal models, such as CLIP and Stable Diffusion have experimented tremendous successes in both foundations and applications. However, as these models increase in parameter size and computational requirements, it becomes more challenging for users to personalize them for specific tasks or preferences. In this work, we address the problem of adapting the previous models towards sets of particular human preferences, aligning the retrieved or generated images with the preferences of the user. We leverage the Bradley-Terry preference model to develop a fast adaptation method that efficiently fine-tunes the original model, with few examples and with minimal computing resources. Extensive evidence of the capabilities of this framework is provided through experiments in different domains related to multimodal text and image understanding, including preference prediction as a reward model, and generation tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07037</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Flow Networks. (arXiv:2308.07037v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;BFNs&#20013;&#65292;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#20250;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#24433;&#21709;&#19979;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#20462;&#25913;&#65292;&#28982;&#21518;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#20174;&#31616;&#21333;&#30340;&#20808;&#39564;&#24320;&#22987;&#65292;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#36825;&#20004;&#20010;&#20998;&#24067;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#31867;&#20284;&#20110;&#25193;&#25955;&#27169;&#22411;&#21453;&#21521;&#36807;&#31243;&#30340;&#29983;&#25104;&#36807;&#31243;&#65307;&#19981;&#36807;&#65292;&#36825;&#20010;&#36807;&#31243;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#65292;&#26080;&#38656;&#21069;&#21521;&#36807;&#31243;&#12290;&#23545;&#20110;&#36830;&#32493;&#12289;&#31163;&#25955;&#21270;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#25512;&#23548;&#20986;&#20102;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21450;&#26679;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;&#31163;&#25955;&#25968;&#25454;&#65292;&#32593;&#32476;&#30340;&#36755;&#20837;&#20301;&#20110;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#65292;&#22240;&#27492;&#26412;&#36136;&#19978;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#20026;&#22522;&#20110;&#26799;&#24230;&#30340;&#26679;&#26412;&#24341;&#23548;&#21644;&#22312;&#35821;&#35328;&#24314;&#27169;&#31561;&#31163;&#25955;&#39046;&#22495;&#36827;&#34892;&#23569;&#37327;&#27493;&#39588;&#29983;&#25104;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#20248;&#21270;&#20102;&#25968;&#25454;&#21387;&#32553;&#65292;&#24182;&#19988;&#19981;&#25918;&#32622;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#20108;&#20803;&#20551;&#35774;&#31867;&#20855;&#26377;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#32780;&#22810;&#31867;&#21035;&#20551;&#35774;&#31867;&#21017;&#19981;&#20855;&#22791;&#36825;&#20010;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2308.06424</link><description>&lt;p&gt;
&#23398;&#20064;&#33021;&#21147;&#19982;&#26679;&#26412;&#21387;&#32553;&#24182;&#19981;&#30456;&#21516;&#30340;&#22810;&#31867;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multiclass Learnability Does Not Imply Sample Compression. (arXiv:2308.06424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06424
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20108;&#20803;&#20551;&#35774;&#31867;&#20855;&#26377;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#32780;&#22810;&#31867;&#21035;&#20551;&#35774;&#31867;&#21017;&#19981;&#20855;&#22791;&#36825;&#20010;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#19968;&#20010;&#20551;&#35774;&#31867;&#33021;&#22815;&#36890;&#36807;&#21482;&#20445;&#30041;&#19968;&#20010;&#23567;&#30340;&#23376;&#26679;&#26412;&#25512;&#26029;&#20986;&#25972;&#20010;&#26679;&#26412;&#30340;&#26631;&#31614;&#65292;&#37027;&#20040;&#23427;&#23601;&#20855;&#26377;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#12290;&#23398;&#20064;&#20108;&#20803;&#20551;&#35774;&#31867;&#65288;&#24517;&#39035;&#20855;&#26377;&#26377;&#38480;&#30340;VC&#32500;&#24230;&#65289;&#37117;&#21487;&#20197;&#36890;&#36807;VC&#32500;&#24230;&#30340;&#19968;&#20010;&#26377;&#38480;&#20989;&#25968;&#22823;&#23567;&#30340;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#31867;&#21035;&#20551;&#35774;&#31867;&#26469;&#35828;&#65292;DS&#32500;&#24230;&#26159;&#30456;&#23545;&#24212;&#30340;&#65292;&#25105;&#20204;&#21457;&#29616;&#23398;&#20064;&#22810;&#31867;&#21035;&#20551;&#35774;&#31867;&#65288;&#24517;&#39035;&#20855;&#26377;&#26377;&#38480;&#30340;DS&#32500;&#24230;&#65289;&#24182;&#19981;&#33021;&#36890;&#36807;&#19968;&#20010;DS&#32500;&#24230;&#30340;&#26377;&#38480;&#20989;&#25968;&#22823;&#23567;&#30340;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hypothesis class admits a sample compression scheme, if for every sample labeled by a hypothesis from the class, it is possible to retain only a small subsample, using which the labels on the entire sample can be inferred. The size of the compression scheme is an upper bound on the size of the subsample produced. Every learnable binary hypothesis class (which must necessarily have finite VC dimension) admits a sample compression scheme of size only a finite function of its VC dimension, independent of the sample size. For multiclass hypothesis classes, the analog of VC dimension is the DS dimension. We show that the analogous statement pertaining to sample compression is not true for multiclass hypothesis classes: every learnable multiclass hypothesis class, which must necessarily have finite DS dimension, does not admit a sample compression scheme of size only a finite function of its DS dimension.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#36873;&#25321;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#36127;&#33655;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15299</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#36127;&#33655;&#39044;&#27979;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting. (arXiv:2307.15299v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#36873;&#25321;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#36127;&#33655;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#36127;&#33655;&#39044;&#27979;&#22312;&#20247;&#22810;&#39046;&#22495;&#37117;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20934;&#30830;&#25429;&#25417;&#21160;&#21147;&#31995;&#32479;&#30340;&#22797;&#26434;&#21160;&#24577;&#20173;&#28982;&#26159;&#20256;&#32479;&#32479;&#35745;&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;ARIMA&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;ANN&#65292;LSTM&#65292;GRU&#31561;&#65289;&#32463;&#24120;&#34987;&#20351;&#29992;&#65292;&#24182;&#19988;&#36890;&#24120;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#25104;&#21151;&#29575;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#26368;&#36817;&#24320;&#21457;&#30340;Transformer-based&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;Transformer&#27169;&#22411;&#26377;&#26395;&#25913;&#36827;&#36127;&#33655;&#39044;&#27979;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20854;Attention&#26426;&#21046;&#23398;&#20064;&#21040;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36816;&#29992;&#20102;&#20960;&#31181;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22914;&#24046;&#20998;&#36827;&#21270;&#65292;&#20197;&#23547;&#25214;Transformer-based&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#65292;&#20197;&#20135;&#29983;&#31934;&#30830;&#30340;&#39044;&#27979;&#12290;&#24046;&#20998;&#36827;&#21270;&#20026;&#38750;&#21487;&#24494;&#20998;&#12289;&#22810;&#30446;&#26631;&#25110;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#12289;&#24378;&#20581;&#21644;&#20840;&#23616;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#19982;&#20854;&#20182;&#27169;&#22411;&#22312;&#36127;&#33655;&#39044;&#27979;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate load forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of dynamic power systems remains a challenge for traditional statistical models. For these reasons, time-series models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly deployed and often experience higher success. In this paper, we analyze the efficacy of the recently developed Transformer-based Neural Network model in Load forecasting. Transformer models have the potential to improve Load forecasting because of their ability to learn long-range dependencies derived from their Attention Mechanism. We apply several metaheuristics namely Differential Evolution to find the optimal hyperparameters of the Transformer-based Neural Network to produce accurate forecasts. Differential Evolution provides scalable, robust, global solutions to non-differentiable, multi-objective, or constrained optimization problems. Our work compares the proposed Transformer based Ne
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24773;&#22659;&#36172;&#21338;&#35774;&#32622;&#30340;&#26032;&#22411;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#36172;&#21338;&#31639;&#27861;&#65292;&#20855;&#26377;&#31616;&#21333;&#21644;&#32047;&#31215;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20248;&#21183;&#65292;&#24182;&#21487;&#33258;&#36866;&#24212;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#21644;&#36830;&#32493;&#33218;&#35774;&#32622;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;"&#19968;&#33268;&#33218;&#38598;"&#65288;CAS&#65289;&#26469;&#25552;&#20379;&#22312;&#27599;&#20010;&#24773;&#22659;&#19979;&#22218;&#25324;&#24773;&#22659;&#29305;&#23450;&#30340;&#26368;&#20339;&#33218;&#30340;&#19968;&#32452;&#33218;&#65292;&#36328;&#36234;&#24773;&#22659;&#20998;&#24067;&#12290;&#36825;&#31687;&#35770;&#25991;&#23545;&#31616;&#21333;&#21644;&#32047;&#31215;&#36951;&#25022;&#20445;&#35777;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#27491;&#38754;&#32467;&#26524;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#26080;&#27861;&#23454;&#29616;&#23454;&#20363;&#20381;&#36182;&#24615;&#30340;&#31616;&#21333;&#36951;&#25022;&#20445;&#35777;&#30340;&#28040;&#26497;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.02108</link><description>&lt;p&gt;
&#27604;&#20363;&#21709;&#24212;&#65306;&#29992;&#20110;&#31616;&#21333;&#21644;&#32047;&#31215;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization. (arXiv:2307.02108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02108
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24773;&#22659;&#36172;&#21338;&#35774;&#32622;&#30340;&#26032;&#22411;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#36172;&#21338;&#31639;&#27861;&#65292;&#20855;&#26377;&#31616;&#21333;&#21644;&#32047;&#31215;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20248;&#21183;&#65292;&#24182;&#21487;&#33258;&#36866;&#24212;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#21644;&#36830;&#32493;&#33218;&#35774;&#32622;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;"&#19968;&#33268;&#33218;&#38598;"&#65288;CAS&#65289;&#26469;&#25552;&#20379;&#22312;&#27599;&#20010;&#24773;&#22659;&#19979;&#22218;&#25324;&#24773;&#22659;&#29305;&#23450;&#30340;&#26368;&#20339;&#33218;&#30340;&#19968;&#32452;&#33218;&#65292;&#36328;&#36234;&#24773;&#22659;&#20998;&#24067;&#12290;&#36825;&#31687;&#35770;&#25991;&#23545;&#31616;&#21333;&#21644;&#32047;&#31215;&#36951;&#25022;&#20445;&#35777;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#27491;&#38754;&#32467;&#26524;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#26080;&#27861;&#23454;&#29616;&#23454;&#20363;&#20381;&#36182;&#24615;&#30340;&#31616;&#21333;&#36951;&#25022;&#20445;&#35777;&#30340;&#28040;&#26497;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#30005;&#23376;&#21830;&#21153;&#31561;&#39046;&#22495;&#65292;&#31616;&#21333;&#36951;&#25022;&#26368;&#23567;&#21270;&#26159;&#23398;&#20064;&#26368;&#20339;&#27835;&#30103;&#20998;&#37197;&#31574;&#30053;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24773;&#22659;&#36172;&#21338;&#35774;&#32622;&#20013;&#30340;&#31616;&#21333;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#20173;&#26410;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#36172;&#21338;&#31639;&#27861;&#26063;&#65292;&#38024;&#23545;&#38543;&#26426;&#24773;&#22659;&#36172;&#21338;&#35774;&#32622;&#65292;&#22312;&#32047;&#31215;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#26497;&#23567;&#26497;&#22823;&#20445;&#35777;&#65289;&#21644;&#31616;&#21333;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;&#20855;&#26377;SOTA&#20445;&#35777;&#65289;&#26041;&#38754;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#24182;&#25193;&#23637;&#21040;&#36830;&#32493;&#33218;&#35774;&#32622;&#12290;&#36825;&#20123;&#20248;&#21183;&#26469;&#33258;&#20110;&#26500;&#24314;&#21644;&#20381;&#36182;&#20110;&#8220;&#19968;&#33268;&#33218;&#38598;&#8221;&#65288;CAS&#65289;&#65292;CAS&#22312;&#27599;&#20010;&#24773;&#22659;&#19979;&#25552;&#20379;&#19968;&#32452;&#33218;&#65292;&#36825;&#20123;&#33218;&#20197;&#19968;&#23450;&#30340;&#27010;&#29575;&#22218;&#25324;&#20102;&#24773;&#22659;&#29305;&#23450;&#30340;&#26368;&#20339;&#33218;&#65292;&#36328;&#36234;&#20102;&#24773;&#22659;&#20998;&#24067;&#12290;&#25105;&#20204;&#20851;&#20110;&#31616;&#21333;&#21644;&#32047;&#31215;&#36951;&#25022;&#20445;&#35777;&#30340;&#31215;&#26497;&#32467;&#26524;&#19982;&#19968;&#20010;&#28040;&#26497;&#32467;&#26524;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#34920;&#26126;&#19968;&#20010;&#31639;&#27861;&#26080;&#27861;&#23454;&#29616;&#23454;&#20363;&#20381;&#36182;&#24615;&#30340;&#31616;&#21333;&#36951;&#25022;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simple regret minimization is a critical problem in learning optimal treatment assignment policies across various domains, including healthcare and e-commerce. However, it remains understudied in the contextual bandit setting. We propose a new family of computationally efficient bandit algorithms for the stochastic contextual bandit settings, with the flexibility to be adapted for cumulative regret minimization (with near-optimal minimax guarantees) and simple regret minimization (with SOTA guarantees). Furthermore, our algorithms adapt to model misspecification and extend to the continuous arm settings. These advantages come from constructing and relying on "conformal arm sets" (CASs), which provide a set of arms at every context that encompass the context-specific optimal arm with some probability across the context distribution. Our positive results on simple and cumulative regret guarantees are contrasted by a negative result, which shows that an algorithm can't achieve instance-de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31181;&#26500;&#36896;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#31995;&#32479;&#21160;&#21147;&#23398;&#26469;&#21051;&#30011;&#21487;&#20197;&#23454;&#29616;&#30340;&#20989;&#25968;&#31867;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#39640;&#22797;&#26434;&#24230;&#30340;&#25511;&#21046;&#12290;&#23454;&#29616;&#26041;&#27861;&#21253;&#25324;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12289;&#30830;&#23450;&#24615;&#21160;&#21147;&#31995;&#32479;&#21644;&#36755;&#20986;&#26144;&#23556;&#30340;&#32423;&#32852;&#36830;&#25509;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#39640;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#23454;&#38469;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00215</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#20989;&#25968;&#23454;&#29616;&#30340;&#26500;&#36896;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Constructive Approach to Function Realization by Neural Stochastic Differential Equations. (arXiv:2307.00215v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31181;&#26500;&#36896;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#31995;&#32479;&#21160;&#21147;&#23398;&#26469;&#21051;&#30011;&#21487;&#20197;&#23454;&#29616;&#30340;&#20989;&#25968;&#31867;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#39640;&#22797;&#26434;&#24230;&#30340;&#25511;&#21046;&#12290;&#23454;&#29616;&#26041;&#27861;&#21253;&#25324;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12289;&#30830;&#23450;&#24615;&#21160;&#21147;&#31995;&#32479;&#21644;&#36755;&#20986;&#26144;&#23556;&#30340;&#32423;&#32852;&#36830;&#25509;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#39640;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#23454;&#38469;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#36827;&#34892;&#20989;&#25968;&#36924;&#36817;&#30340;&#38382;&#39064;&#36890;&#24120;&#37319;&#29992;&#33258;&#19978;&#32780;&#19979;&#30340;&#26041;&#27861;&#65306;&#29992;&#32473;&#23450;&#32467;&#26500;&#30340;&#22797;&#26434;&#27169;&#22411;&#21487;&#20197;&#23558;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#36924;&#36817;&#21040;&#20219;&#24847;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#24212;&#29992;&#20013;&#19981;&#23454;&#38469;&#30340;&#39640;&#22797;&#26434;&#24230;&#25511;&#21046;&#12290;&#26412;&#25991;&#37319;&#29992;&#30456;&#21453;&#30340;&#26500;&#36896;&#24615;&#26041;&#27861;&#65306;&#25105;&#20204;&#23545;&#31995;&#32479;&#21160;&#21147;&#23398;&#26045;&#21152;&#21508;&#31181;&#32467;&#26500;&#38480;&#21046;&#65292;&#20174;&#32780;&#21051;&#30011;&#20102;&#21487;&#20197;&#36890;&#36807;&#36825;&#31181;&#31995;&#32479;&#23454;&#29616;&#30340;&#20989;&#25968;&#31867;&#12290;&#31995;&#32479;&#23454;&#29616;&#20026;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDE&#65289;&#12289;&#30830;&#23450;&#24615;&#21160;&#21147;&#31995;&#32479;&#21644;&#19968;&#20010;&#36755;&#20986;&#26144;&#23556;&#30340;&#32423;&#32852;&#36830;&#25509;&#12290;&#37319;&#29992;&#27010;&#29575;&#21644;&#20960;&#20309;&#65288;&#26446;&#35770;&#65289;&#26041;&#27861;&#26469;&#21051;&#30011;&#36825;&#20123;&#31995;&#32479;&#23454;&#29616;&#30340;&#20989;&#25968;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of function approximation by neural dynamical systems has typically been approached in a top-down manner: Any continuous function can be approximated to an arbitrary accuracy by a sufficiently complex model with a given architecture. This can lead to high-complexity controls which are impractical in applications. In this paper, we take the opposite, constructive approach: We impose various structural restrictions on system dynamics and consequently characterize the class of functions that can be realized by such a system. The systems are implemented as a cascade interconnection of a neural stochastic differential equation (Neural SDE), a deterministic dynamical system, and a readout map. Both probabilistic and geometric (Lie-theoretic) methods are used to characterize the classes of functions realized by such systems.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;$\lambda$-AC&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#35774;&#35745;&#36873;&#25321;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.17366</link><description>&lt;p&gt;
$\lambda$-AC&#65306;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
$\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces. (arXiv:2306.17366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;$\lambda$-AC&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#35774;&#35745;&#36873;&#25321;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#22312;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21363;&#27169;&#22411;&#22312;&#20915;&#31574;&#21046;&#23450;&#26102;&#24212;&#35813;&#26159;&#20934;&#30830;&#30340;&#12290;&#23613;&#31649;&#24050;&#32463;&#24314;&#31435;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#20294;&#26159;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#21033;&#29992;&#20915;&#31574;&#24863;&#30693;&#25439;&#22833;&#30340;&#31639;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#33021;&#22815;&#23454;&#29616;&#33391;&#22909;&#31639;&#27861;&#24615;&#33021;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#31639;&#27861;&#24605;&#24819;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#22312;MuZero&#31995;&#21015;&#24037;&#20316;&#20013;&#25152;&#24314;&#31435;&#30340;&#32463;&#39564;&#24615;&#35774;&#35745;&#20915;&#31574;&#23545;&#20110;&#30456;&#20851;&#31639;&#27861;&#30340;&#33391;&#22909;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#65292;&#19981;&#21516;&#30340;&#20215;&#20540;&#24863;&#30693;&#31639;&#27861;&#23454;&#20363;&#20043;&#38388;&#34892;&#20026;&#24046;&#24322;&#12290;&#22312;&#36825;&#20123;&#35265;&#35299;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#27169;&#22411;&#39537;&#21160;&#20915;&#31574;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;$\lambda$-AC&#12290;
&lt;/p&gt;
&lt;p&gt;
The idea of decision-aware model learning, that models should be accurate where it matters for decision-making, has gained prominence in model-based reinforcement learning. While promising theoretical results have been established, the empirical performance of algorithms leveraging a decision-aware loss has been lacking, especially in continuous control problems. In this paper, we present a study on the necessary components for decision-aware reinforcement learning models and we showcase design choices that enable well-performing algorithms. To this end, we provide a theoretical and empirical investigation into prominent algorithmic ideas in the field. We highlight that empirical design decisions established in the MuZero line of works are vital to achieving good performance for related algorithms, and we showcase differences in behavior between different instantiations of value-aware algorithms in stochastic environments. Using these insights, we propose the Latent Model-Based Decisio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#39715;&#29399;&#30340;&#26032;&#22411;&#31070;&#32463;&#31639;&#23376;&#65292;&#23427;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#28388;&#27874;&#22120;&#26469;&#35299;&#20915;PDE&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#27169;&#22411;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#19981;&#21516;&#30340;PDE&#23454;&#20363;&#25552;&#20379;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27714;&#35299;PDE&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.16524</link><description>&lt;p&gt;
HNO&#65306;&#29992;&#20110;&#35299;&#20915;PDE&#30340;&#39715;&#29399;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
HNO: Hyena Neural Operator for solving PDEs. (arXiv:2306.16524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#39715;&#29399;&#30340;&#26032;&#22411;&#31070;&#32463;&#31639;&#23376;&#65292;&#23427;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#28388;&#27874;&#22120;&#26469;&#35299;&#20915;PDE&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#27169;&#22411;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#19981;&#21516;&#30340;PDE&#23454;&#20363;&#25552;&#20379;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27714;&#35299;PDE&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#36890;&#24120;&#38656;&#35201;&#31934;&#32454;&#31163;&#25955;&#21270;&#20197;&#35299;&#26512;&#24517;&#35201;&#30340;&#26102;&#31354;&#23610;&#24230;&#65292;&#36825;&#21487;&#33021;&#20250;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;PDE&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#12290;&#31070;&#32463;&#31639;&#23376;&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#23398;&#20064;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#33021;&#22815;&#22522;&#20110;&#25968;&#25454;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;&#39715;&#29399;&#65288;Hyena&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#37319;&#29992;&#30001;&#22810;&#23618;&#24863;&#30693;&#22120;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#28388;&#27874;&#22120;&#12290;&#39715;&#29399;&#31639;&#23376;&#26159;&#19968;&#31181;&#20855;&#26377;&#27425;&#32447;&#24615;&#22797;&#26434;&#24615;&#30340;&#25805;&#20316;&#65292;&#23427;&#20351;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26469;&#21442;&#25968;&#21270;&#20855;&#26377;&#20840;&#23616;&#24863;&#21463;&#37326;&#30340;&#38271;&#21367;&#31215;&#12290;&#36825;&#31181;&#26426;&#21046;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#24182;&#33021;&#22815;&#20026;&#19981;&#21516;&#30340;PDE&#23454;&#20363;&#25552;&#20379;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#12290;&#20026;&#20102;&#34913;&#37327;&#21508;&#20010;&#23618;&#22312;&#35299;&#20915;PDE&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerically solving partial differential equations (PDEs) typically requires fine discretization to resolve necessary spatiotemporal scales, which can be computationally expensive. Recent advances in deep learning have provided a new approach to solving PDEs that involves the use of neural operators. Neural operators are neural network architectures that learn mappings between function spaces and have the capability to solve partial differential equations based on data. This study utilizes a novel neural operator called Hyena, which employs a long convolutional filter that is parameterized by a multilayer perceptron. The Hyena operator is an operation that enjoys sub-quadratic complexity and state space model to parameterize long convolution that enjoys global receptive field. This mechanism enhances the model's comprehension of the input's context and enables data-dependent weight for different PDE instances. To measure how effective the layers are in solving PDEs, we conduct experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.06101</link><description>&lt;p&gt;
Prodigy: &#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#38646;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#20013;&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#20004;&#31181;&#25216;&#26415;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#35777;&#26126;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20197;&#20415;&#26368;&#20248;&#35774;&#32622;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#22522;&#20110;&#23398;&#20064;&#29575;&#33258;&#30001;&#30340;D-Adaptation&#26041;&#27861;&#30340;&#20462;&#25913;&#65292;&#24182;&#36890;&#36807;$O(\sqrt{\log(D/d_0)})$&#30340;&#22240;&#23376;&#25552;&#39640;&#20102;D-Adaptation&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$d_0$&#26159;$D$&#30340;&#21021;&#22987;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#22312;12&#20010;&#24120;&#35265;&#30340;&#36923;&#36753;&#22238;&#24402;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#22312;CIFAR10&#19978;&#35757;&#32451;&#30340;VGG11&#21644;ResNet-50&#12289;&#22312;Imagenet&#19978;&#35757;&#32451;&#30340;ViT&#12289;&#22312;IWSLT14&#19978;&#35757;&#32451;&#30340;LSTM&#12289;&#22312;Criteo&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;DLRM&#12289;&#22312;Knee MRI&#25968;&#25454;&#38598;&#19978;&#30340;VarNet&#65292;&#20197;&#21450;&#22312;BookWiki&#19978;&#35757;&#32451;&#30340;RoBERTa&#21644;GPT transformer&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;D-Adaptation&#65292;&#24182;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating the learning rate in adaptive methods, such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to provably estimate the distance to the solution $D$, which is needed to set the learning rate optimally. Our techniques are modifications of the D-Adaptation method for learning-rate-free learning. Our methods improve upon the convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where $d_0$ is the initial estimate of $D$. We test our methods on 12 common logistic-regression benchmark datasets, VGG11 and ResNet-50 training on CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT transformer training on BookWiki. Our experimental results show that our approaches consistently outperform D-Adaptation and reach test accuracy values close to that of hand-tuned Adam.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#23436;&#25104;&#20840;&#23616;&#20989;&#25968;&#36924;&#36817;&#12290;&#36825;&#19968;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#65292;&#36824;&#21487;&#29992;&#20110;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#30340;&#36924;&#36817;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#36924;&#36817;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#12290;</title><link>http://arxiv.org/abs/2306.03303</link><description>&lt;p&gt;
&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#21151;&#33021;&#24615;&#36755;&#20837;&#26144;&#23556;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Global universal approximation of functional input maps on weighted spaces. (arXiv:2306.03303v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#23436;&#25104;&#20840;&#23616;&#20989;&#25968;&#36924;&#36817;&#12290;&#36825;&#19968;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#65292;&#36824;&#21487;&#29992;&#20110;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#30340;&#36924;&#36817;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#36924;&#36817;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#23450;&#20041;&#22312;&#21487;&#33021;&#26159;&#26080;&#38480;&#32500;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#65292;&#20854;&#20540;&#20063;&#22312;&#21487;&#33021;&#26159;&#26080;&#38480;&#32500;&#30340;&#36755;&#20986;&#31354;&#38388;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21152;&#24615;&#26063;&#20316;&#20026;&#38544;&#34255;&#23618;&#26144;&#23556;&#65292;&#20197;&#21450;&#19968;&#20010;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#24212;&#29992;&#20110;&#27599;&#20010;&#38544;&#34255;&#23618;&#12290;&#20381;&#38752;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#30340;Stone-Weierstrass&#23450;&#29702;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;&#32467;&#26524;&#65292;&#36229;&#36234;&#20102;&#24120;&#35268;&#32039;&#38598;&#36924;&#36817;&#12290;&#36825;&#29305;&#21035;&#36866;&#29992;&#20110;&#36890;&#36807;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#65288;&#38750;&#20808;&#35265;&#20043;&#26126;&#30340;&#65289;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#12290;&#20316;&#20026;&#24102;&#26435;Stone-Weierstrass&#23450;&#29702;&#30340;&#36827;&#19968;&#27493;&#24212;&#29992;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#24341;&#20837;&#20102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#31614;&#21517;&#20869;&#26680;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26159;&#26576;&#20123;&#39640;&#26031;&#36807;&#31243;&#30340;Cameron-Martin&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family as hidden layer maps and a non-linear activation function applied to each hidden layer. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result for generalizations of continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and show that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gauss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#36171;&#20104;&#27169;&#22411;&#20808;&#21069;&#30456;&#20851;&#30340;&#32534;&#36753;&#30693;&#35782;&#65292;&#26469;&#35299;&#20915;&#20195;&#30721;&#22810;&#26679;&#24615;&#21644;&#24320;&#21457;&#20154;&#21592;&#24847;&#22270;&#38590;&#20197;&#25429;&#25417;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14129</link><description>&lt;p&gt;
GrACE&#65306;&#20351;&#29992;&#30456;&#20851;&#20195;&#30721;&#32534;&#36753;&#29983;&#25104;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
GrACE: Generation using Associated Code Edits. (arXiv:2305.14129v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#36171;&#20104;&#27169;&#22411;&#20808;&#21069;&#30456;&#20851;&#30340;&#32534;&#36753;&#30693;&#35782;&#65292;&#26469;&#35299;&#20915;&#20195;&#30721;&#22810;&#26679;&#24615;&#21644;&#24320;&#21457;&#20154;&#21592;&#24847;&#22270;&#38590;&#20197;&#25429;&#25417;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20154;&#21592;&#20250;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#32534;&#36753;&#20195;&#30721;&#65292;&#20854;&#21407;&#22240;&#21253;&#25324;&#20462;&#22797;&#38169;&#35823;&#25110;&#28155;&#21152;&#26032;&#21151;&#33021;&#12290;&#35774;&#35745;&#26377;&#25928;&#30340;&#20195;&#30721;&#32534;&#36753;&#39044;&#27979;&#26041;&#27861;&#19968;&#30452;&#26159;&#19968;&#20010;&#27963;&#36291;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20026;&#20195;&#30721;&#32534;&#36753;&#30340;&#22810;&#26679;&#24615;&#21644;&#25429;&#25417;&#24320;&#21457;&#20154;&#21592;&#24847;&#22270;&#30340;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36171;&#20104;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20808;&#21069;&#30456;&#20851;&#30340;&#32534;&#36753;&#30693;&#35782;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#26377;&#21161;&#20110;&#35299;&#20915;&#20195;&#30721;&#26356;&#25913;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;&#23558;&#20195;&#30721;&#29983;&#25104;&#30340;&#26465;&#20214;&#35774;&#23450;&#20026;&#20808;&#21069;&#32534;&#36753;&#26377;&#21161;&#20110;&#25429;&#25417;&#28508;&#22312;&#30340;&#24320;&#21457;&#20154;&#21592;&#24847;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#20004;&#31181;&#30693;&#21517;&#30340;LLMs&#65292;Codex&#21644;CodeT5&#65292;&#20998;&#21035;&#36827;&#34892;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#35774;&#32622;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#20808;&#21069;&#32534;&#36753;&#30340;&#30693;&#35782;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#20351;&#20854;&#22312;&#21069;1&#20010;&#24314;&#35758;&#20013;&#29983;&#25104;29&#65285;&#21644;54&#65285;&#26356;&#27491;&#30830;&#30340;&#32534;&#36753;&#20195;&#30721;&#65292;&#30456;&#23545;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31526;&#21495;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers expend a significant amount of time in editing code for a variety of reasons such as bug fixing or adding new features. Designing effective methods to predict code edits has been an active yet challenging area of research due to the diversity of code edits and the difficulty of capturing the developer intent. In this work, we address these challenges by endowing pre-trained large language models (LLMs) of code with the knowledge of prior, relevant edits. The generative capability of the LLMs helps address the diversity in code changes and conditioning code generation on prior edits helps capture the latent developer intent. We evaluate two well-known LLMs, Codex and CodeT5, in zero-shot and fine-tuning settings respectively. In our experiments with two datasets, the knowledge of prior edits boosts the performance of the LLMs significantly and enables them to generate 29% and 54% more correctly edited code in top-1 suggestions relative to the current state-of-the-art symbolic
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35843;&#24615;&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;6G&#26102;&#20195;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#24863;&#30693;&#20256;&#36755;&#35843;&#24230;&#38382;&#39064;&#12290;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13706</link><description>&lt;p&gt;
&#35821;&#20041;&#24863;&#30693;&#30340;&#20256;&#36755;&#35843;&#24230;&#65306;&#19968;&#31181;&#22522;&#20110;&#21333;&#35843;&#24615;&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semantic-aware Transmission Scheduling: a Monotonicity-driven Deep Reinforcement Learning Approach. (arXiv:2305.13706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13706
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35843;&#24615;&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;6G&#26102;&#20195;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#24863;&#30693;&#20256;&#36755;&#35843;&#24230;&#38382;&#39064;&#12290;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;6G&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#65292;&#38656;&#35201;&#35821;&#20041;&#20256;&#36755;&#26469;&#36830;&#25509;&#20998;&#24067;&#24335;&#35774;&#22791;&#65292;&#20197;&#20445;&#35777;&#24212;&#29992;&#23618;&#24615;&#33021;&#65292;&#19981;&#20165;&#20165;&#26159;&#38598;&#20013;&#20110;&#36890;&#20449;&#23618;&#24615;&#33021;&#12290;&#35821;&#20041;&#22312;&#36825;&#37324;&#26159;&#20449;&#24687;&#20256;&#36755;&#26377;&#29992;&#24615;&#30340;&#34913;&#37327;&#12290;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#35821;&#20041;&#24863;&#30693;&#20256;&#36755;&#35843;&#24230;&#24120;&#24120;&#28041;&#21450;&#24222;&#22823;&#30340;&#20915;&#31574;&#31354;&#38388;&#65292;&#29616;&#26377;&#31639;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#33719;&#24471;&#26368;&#20248;&#31574;&#30053;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#26368;&#20248;&#35821;&#20041;&#24863;&#30693;&#35843;&#24230;&#31574;&#30053;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#28982;&#21518;&#26681;&#25454;&#29702;&#35770;&#25351;&#23548;&#21407;&#21017;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
For cyber-physical systems in the 6G era, semantic communications connecting distributed devices for dynamic control and remote state estimation are required to guarantee application-level performance, not merely focus on communication-centric performance. Semantics here is a measure of the usefulness of information transmissions. Semantic-aware transmission scheduling of a large system often involves a large decision-making space, and the optimal policy cannot be obtained by existing algorithms effectively. In this paper, we first investigate the fundamental properties of the optimal semantic-aware scheduling policy and then develop advanced deep reinforcement learning (DRL) algorithms by leveraging the theoretical guidelines. Our numerical results show that the proposed algorithms can substantially reduce training time and enhance training performance compared to benchmark algorithms.
&lt;/p&gt;</description></item><item><title>ZeroFlow&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#26041;&#27861;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#36827;&#34892;&#23454;&#26102;&#22330;&#26223;&#27969;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.10424</link><description>&lt;p&gt;
ZeroFlow: &#36890;&#36807;&#33976;&#39311;&#23454;&#29616;&#24555;&#36895;&#38646;&#26631;&#31614;&#22330;&#26223;&#27969;
&lt;/p&gt;
&lt;p&gt;
ZeroFlow: Fast Zero Label Scene Flow via Distillation. (arXiv:2305.10424v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10424
&lt;/p&gt;
&lt;p&gt;
ZeroFlow&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#26041;&#27861;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#36827;&#34892;&#23454;&#26102;&#22330;&#26223;&#27969;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#27969;&#20272;&#35745;&#26159;&#25551;&#36848;&#36830;&#32493;&#28857;&#20113;&#20043;&#38388;&#30340;&#19977;&#32500;&#36816;&#21160;&#22330;&#30340;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#24378;&#22823;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#27979;&#35797;&#26102;&#20248;&#21270;&#25216;&#26415;&#65292;&#20294;&#23545;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#38656;&#35201;&#25968;&#21313;&#31186;&#30340;&#26102;&#38388;&#65292;&#20351;&#20854;&#26080;&#27861;&#20316;&#20026;&#23454;&#26102;&#24212;&#29992;&#31243;&#24207;&#65288;&#22914;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#65289;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20803;&#20351;&#29992;&#12290;&#21069;&#21521;&#20256;&#36882;&#26041;&#27861;&#30456;&#23545;&#24555;&#36895;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#30340;&#36816;&#34892;&#26102;&#38388;&#22312;&#25968;&#21313;&#33267;&#25968;&#30334;&#27627;&#31186;&#20043;&#38388;&#65292;&#20294;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#21147;&#30417;&#30563;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#26694;&#26550; Scene Flow via Distillation&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#20248;&#21270;&#26041;&#27861;&#26469;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#26694;&#26550;&#20013;&#30340; ZeroFlow&#65292;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#65292;&#22312;&#22823;&#35268;&#27169;&#28857;&#20113;&#19978;&#23454;&#26102;&#29983;&#25104;&#22330;&#26223;&#27969;&#20272;&#35745;&#32467;&#26524;&#65292;&#21516;&#26102;&#36136;&#37327;&#31454;&#20105;&#29366;&#24577;&#19979;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102; ZeroFlow
&lt;/p&gt;
&lt;p&gt;
Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is ove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SCPT&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#31354;&#38388;&#39044;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#26410;&#35265;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#26032;&#36947;&#36335;&#30340;&#20132;&#36890;&#39044;&#27979;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.05237</link><description>&lt;p&gt;
&#21033;&#29992;&#31354;&#38388;&#23545;&#27604;&#39044;&#35757;&#32451;&#36827;&#34892;&#26410;&#35265;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#36947;&#36335;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Traffic Forecasting on New Roads Unseen in the Training Data Using Spatial Contrastive Pre-Training. (arXiv:2305.05237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SCPT&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#31354;&#38388;&#39044;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#26410;&#35265;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#26032;&#36947;&#36335;&#30340;&#20132;&#36890;&#39044;&#27979;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#20250;&#19981;&#26029;&#24314;&#35774;&#26032;&#30340;&#36947;&#36335;&#65292;&#20294;&#26159;&#20043;&#21069;&#30340;&#28145;&#24230;&#39044;&#27979;&#27169;&#22411;&#23545;&#20110;&#26032;&#36947;&#36335;&#65288;&#26410;&#35265;&#25968;&#25454;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#26102;&#31354;&#65288;ST&#65289;&#20998;&#21106;&#30340;&#26032;&#35774;&#32622;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#35757;&#32451;&#26102;&#20351;&#29992;&#19968;&#37096;&#20998;&#30340;&#36947;&#36335;&#25968;&#25454;&#65292;&#20294;&#27979;&#35797;&#26102;&#20351;&#29992;&#26410;&#35265;&#25968;&#25454;&#30340;&#36947;&#36335;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#31354;&#38388;&#23545;&#27604;&#39044;&#35757;&#32451;&#65288;SCPT&#65289;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22359;&#26469;&#25552;&#21462;&#25512;&#29702;&#26102;&#26410;&#35265;&#36947;&#36335;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#36825;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#26159;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#30340;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#31354;&#38388;&#32534;&#30721;&#22120;&#20165;&#38656;&#35201;&#26032;&#36947;&#36335;&#30340;&#20004;&#22825;&#20132;&#36890;&#25968;&#25454;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31354;&#38388;&#32534;&#30721;&#22120;&#30340;&#36755;&#20986;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#25512;&#26029;&#26410;&#35265;&#36947;&#36335;&#19978;&#30340;&#28508;&#22312;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
New roads are being constructed all the time. However, the capabilities of previous deep forecasting models to generalize to new roads not seen in the training data (unseen roads) are rarely explored. In this paper, we introduce a novel setup called a spatio-temporal (ST) split to evaluate the models' capabilities to generalize to unseen roads. In this setup, the models are trained on data from a sample of roads, but tested on roads not seen in the training data. Moreover, we also present a novel framework called Spatial Contrastive Pre-Training (SCPT) where we introduce a spatial encoder module to extract latent features from unseen roads during inference time. This spatial encoder is pre-trained using contrastive learning. During inference, the spatial encoder only requires two days of traffic data on the new roads and does not require any re-training. We also show that the output from the spatial encoder can be used effectively to infer latent node embeddings on unseen roads during 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#32858;&#31867;&#36807;&#28388;&#26041;&#27861;&#65288;MCF&#65289;&#65292;&#29992;&#20110;&#25551;&#36848;&#19981;&#21516;&#23610;&#24230;&#19979;&#30340;&#25968;&#25454;&#32858;&#31867;&#65292;&#20854;&#20013;&#30340;&#25345;&#20037;&#21516;&#35843;&#21487;&#27979;&#37327;&#20998;&#21306;&#24207;&#21015;&#30340;&#23618;&#27425;&#20851;&#31995;&#21644;&#32858;&#31867;&#20998;&#37197;&#20914;&#31361;&#30340;&#20986;&#29616;&#21644;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2305.04281</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#32858;&#31867;&#36807;&#28388;&#20013;&#30340;&#25345;&#20037;&#21516;&#35843;
&lt;/p&gt;
&lt;p&gt;
Persistent Homology of the Multiscale Clustering Filtration. (arXiv:2305.04281v2 [math.AT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#32858;&#31867;&#36807;&#28388;&#26041;&#27861;&#65288;MCF&#65289;&#65292;&#29992;&#20110;&#25551;&#36848;&#19981;&#21516;&#23610;&#24230;&#19979;&#30340;&#25968;&#25454;&#32858;&#31867;&#65292;&#20854;&#20013;&#30340;&#25345;&#20037;&#21516;&#35843;&#21487;&#27979;&#37327;&#20998;&#21306;&#24207;&#21015;&#30340;&#23618;&#27425;&#20851;&#31995;&#21644;&#32858;&#31867;&#20998;&#37197;&#20914;&#31361;&#30340;&#20986;&#29616;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25968;&#25454;&#32858;&#31867;&#24212;&#29992;&#20013;&#65292;&#19981;&#20165;&#24076;&#26395;&#25214;&#21040;&#19968;&#31181;&#21333;&#19968;&#30340;&#20998;&#21306;&#26041;&#24335;&#65292;&#36824;&#24076;&#26395;&#25214;&#21040;&#25551;&#36848;&#19981;&#21516;&#23610;&#24230;&#25110;&#31895;&#31961;&#23618;&#27425;&#19979;&#30340;&#25968;&#25454;&#30340;&#19968;&#31995;&#21015;&#20998;&#21306;&#26041;&#24335;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#20998;&#26512;&#21644;&#27604;&#36739;&#25903;&#25745;&#36825;&#31181;&#22810;&#23610;&#24230;&#25968;&#25454;&#25551;&#36848;&#30340;&#65288;&#19981;&#19968;&#23450;&#26159;&#23618;&#27425;&#24615;&#30340;&#65289;&#20998;&#21306;&#24207;&#21015;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25277;&#35937;&#21333;&#32431;&#22797;&#24418;&#30340;&#36807;&#28388;&#65292;&#31216;&#20026;&#22810;&#23610;&#24230;&#32858;&#31867;&#36807;&#28388;&#65288;MCF&#65289;&#65292;&#23427;&#32534;&#30721;&#20102;&#36328;&#23610;&#24230;&#30340;&#20219;&#24847;&#27169;&#24335;&#30340;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#35777;&#26126;&#20102;MCF&#20135;&#29983;&#31283;&#23450;&#30340;&#25345;&#20037;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MCF&#30340;&#38646;&#32500;&#25345;&#20037;&#21516;&#35843;&#27979;&#37327;&#20102;&#20998;&#21306;&#24207;&#21015;&#20013;&#30340;&#23618;&#27425;&#20851;&#31995;&#31243;&#24230;&#65292;&#32780;&#39640;&#32500;&#25345;&#20037;&#21516;&#35843;&#21017;&#36319;&#36394;&#20102;&#20998;&#21306;&#24207;&#21015;&#20013;&#32858;&#31867;&#20998;&#37197;&#20914;&#31361;&#30340;&#20986;&#29616;&#21644;&#35299;&#20915;&#12290;&#20026;&#20102;&#25299;&#23485;MCF&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31561;&#20215;&#30340;&#26500;&#36896;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications in data clustering, it is desirable to find not just a single partition into clusters but a sequence of partitions describing the data at different scales, or levels of coarseness. A natural problem then is to analyse and compare the (not necessarily hierarchical) sequences of partitions that underpin such multiscale descriptions of data. Here, we introduce a filtration of abstract simplicial complexes, denoted the Multiscale Clustering Filtration (MCF), which encodes arbitrary patterns of cluster assignments across scales, and we prove that the MCF produces stable persistence diagrams. We then show that the zero-dimensional persistent homology of the MCF measures the degree of hierarchy in the sequence of partitions, and that the higher-dimensional persistent homology tracks the emergence and resolution of conflicts between cluster assignments across the sequence of partitions. To broaden the theoretical foundations of the MCF, we also provide an equivalent constr
&lt;/p&gt;</description></item><item><title>CoDi &#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#20849;&#21516;&#28436;&#21270;&#30340;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#21333;&#29420;&#22788;&#29702;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#24182;&#30456;&#20114;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#32465;&#23450;&#65292;&#23637;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12654</link><description>&lt;p&gt;
CoDi: &#28151;&#21512;&#31867;&#22411;&#34920;&#26684;&#29983;&#25104;&#30340;&#20849;&#21516;&#28436;&#21270;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis. (arXiv:2304.12654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12654
&lt;/p&gt;
&lt;p&gt;
CoDi &#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#20849;&#21516;&#28436;&#21270;&#30340;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#21333;&#29420;&#22788;&#29702;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#24182;&#30456;&#20114;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#32465;&#23450;&#65292;&#23637;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#27880;&#24847;&#21147;&#34987;&#25918;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#65292;&#23558;&#32508;&#21512;&#34920;&#26684;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;&#23581;&#35797;&#24050;&#32463;&#21521;&#21508;&#31181;&#22330;&#26223;&#25193;&#23637;&#12290;&#30001;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#34920;&#26684;&#25968;&#25454;&#32508;&#21512;&#27169;&#22411;&#29983;&#25104;&#30340;&#34394;&#20551;&#25968;&#25454;&#21464;&#24471;&#22797;&#26434;&#32780;&#30495;&#23454;&#12290;&#20294;&#26159;&#65292;&#24314;&#27169;&#34920;&#26684;&#25968;&#25454;&#30340;&#31163;&#25955;&#21464;&#37327;&#65288;&#21015;&#65289;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20004;&#20010;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#21333;&#29420;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#21464;&#37327;&#65288;&#20294;&#30456;&#20114;&#26465;&#20214;&#21270;&#65289;&#12290;&#20004;&#20010;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#24444;&#27492;&#35835;&#21462;&#26465;&#20214;&#22312;&#35757;&#32451;&#20013;&#20849;&#21516;&#28436;&#21270;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#32465;&#23450;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36127;&#37319;&#26679;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;11&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#21644;8&#20010;&#22522;&#20934;&#26041;&#27861;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861; CoDi &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With growing attention to tabular data these days, the attempt to apply a synthetic table to various tasks has been expanded toward various scenarios. Owing to the recent advances in generative modeling, fake data generated by tabular data synthesis models become sophisticated and realistic. However, there still exists a difficulty in modeling discrete variables (columns) of tabular data. In this work, we propose to process continuous and discrete variables separately (but being conditioned on each other) by two diffusion models. The two diffusion models are co-evolved during training by reading conditions from each other. In order to further bind the diffusion models, moreover, we introduce a contrastive learning method with a negative sampling method. In our experiments with 11 real-world tabular datasets and 8 baseline methods, we prove the efficacy of the proposed method, called CoDi.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#32422;&#26463;&#38381;&#29615;&#25511;&#21046;&#31995;&#32479;&#30340;&#22312;&#32447;&#24615;&#33021;&#20248;&#21270;&#65292;&#21516;&#26102;&#28385;&#36275;&#25152;&#38656;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.06104</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24102;&#26102;&#38388;&#24179;&#22343;&#32422;&#26463;&#30340;&#25511;&#21046;&#31995;&#32479;&#22312;&#32447;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Primal-Dual Contextual Bayesian Optimization for Control System Online Optimization with Time-Average Constraints. (arXiv:2304.06104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06104
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#32422;&#26463;&#38381;&#29615;&#25511;&#21046;&#31995;&#32479;&#30340;&#22312;&#32447;&#24615;&#33021;&#20248;&#21270;&#65292;&#21516;&#26102;&#28385;&#36275;&#25152;&#38656;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24102;&#26377;&#22806;&#29983;&#26102;&#38388;&#21464;&#21270;&#19978;&#19979;&#25991;&#24178;&#25200;&#30340;&#26410;&#30693;&#40657;&#30418;&#20989;&#25968;&#30340;&#32422;&#26463;&#38381;&#29615;&#25511;&#21046;&#31995;&#32479;&#22312;&#32447;&#24615;&#33021;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#26368;&#20248;&#35299;&#30340;&#20122;&#32447;&#24615;&#32047;&#31215;&#36951;&#25022;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#38646;&#26102;&#38388;&#24179;&#22343;&#32422;&#26463;&#36829;&#35268;&#65292;&#30830;&#20445;&#20102;&#32422;&#26463;&#20989;&#25968;&#30340;&#24179;&#22343;&#20540;&#28385;&#36275;&#25152;&#38656;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#37319;&#26679;&#23454;&#20363;&#21644;&#36830;&#32493;&#25605;&#25292;&#27133;&#21453;&#24212;&#22120;&#21442;&#25968;&#35843;&#33410;&#38382;&#39064;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#21644;&#24179;&#22343;&#20445;&#25345;&#32422;&#26463;&#21487;&#34892;&#24615;&#65292;&#36825;&#19982;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#35201;&#20040;&#36973;&#21463;&#22823;&#37327;&#32047;&#31215;&#36951;&#25022;&#65292;&#35201;&#20040;&#23384;&#22312;&#20005;&#37325;&#32422;&#26463;&#36829;&#35268;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of online performance optimization of constrained closed-loop control systems, where both the objective and the constraints are unknown black-box functions affected by exogenous time-varying contextual disturbances. A primal-dual contextual Bayesian optimization algorithm is proposed that achieves sublinear cumulative regret with respect to the dynamic optimal solution under certain regularity conditions. Furthermore, the algorithm achieves zero time-average constraint violation, ensuring that the average value of the constraint function satisfies the desired constraint. The method is applied to both sampled instances from Gaussian processes and a continuous stirred tank reactor parameter tuning problem; simulation results show that the method simultaneously provides close-to-optimal performance and maintains constraint feasibility on average. This contrasts current state-of-the-art methods, which either suffer from large cumulative regret or severe const
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#37327;&#23376;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#35770;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12289;&#25293;&#25668;&#27425;&#25968;&#12289;ansatz&#12289;&#35757;&#32451;&#31639;&#27861;&#20197;&#21450;&#37327;&#23376;&#30828;&#20214;&#22122;&#22768;&#30340;&#23384;&#22312;&#22914;&#20309;&#12290;</title><link>http://arxiv.org/abs/2304.03398</link><description>&lt;p&gt;
&#37327;&#23376;&#30456;&#23481;&#39044;&#27979;&#29992;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quantum Conformal Prediction for Reliable Uncertainty Quantification in Quantum Machine Learning. (arXiv:2304.03398v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#37327;&#23376;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#35770;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12289;&#25293;&#25668;&#27425;&#25968;&#12289;ansatz&#12289;&#35757;&#32451;&#31639;&#27861;&#20197;&#21450;&#37327;&#23376;&#30828;&#20214;&#22122;&#22768;&#30340;&#23384;&#22312;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26159;&#22312;&#24403;&#21069;&#30340;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;(NISQ)&#35745;&#31639;&#26426;&#26102;&#20195;&#20013;&#20248;&#21270;&#37327;&#23376;&#31639;&#27861;&#30340;&#26377;&#21069;&#36884;&#30340;&#32534;&#31243;&#33539;&#24335;&#12290;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#27867;&#21270;&#24615;&#33021;&#65292;&#22240;&#20026;&#35774;&#35745;&#32773;&#30340;&#30446;&#26631;&#26159;&#22312;&#27979;&#35797;&#26465;&#20214;&#19979;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#29616;&#26377;&#30340;&#27867;&#21270;&#20998;&#26512;&#34429;&#28982;&#33021;&#22815;&#35782;&#21035;&#37325;&#35201;&#30340;&#19968;&#33324;&#36235;&#21183;&#21644;&#35268;&#27169;&#23450;&#24459;&#65292;&#20294;&#19981;&#33021;&#29992;&#20110;&#20026;&#37327;&#23376;&#27169;&#22411;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#20998;&#37197;&#21487;&#38752;&#21644;&#26377;&#20449;&#24687;&#37327;&#30340;&#8220;&#35823;&#24046;&#26465;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#37327;&#23376;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#35770;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12289;&#25293;&#25668;&#27425;&#25968;&#12289;ansatz&#12289;&#35757;&#32451;&#31639;&#27861;&#20197;&#21450;&#37327;&#23376;&#30828;&#20214;&#22122;&#22768;&#30340;&#23384;&#22312;&#22914;&#20309;&#65292;&#22312;&#27010;&#29575;&#24615;&#30456;&#23481;&#39044;&#27979;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#37327;&#23376;&#27169;&#22411;&#30340;&#20219;&#24847;&#21487;&#33021;&#23567;&#30340;&#25293;&#25668;&#27425;&#25968;&#36716;&#25442;&#20026;&#19968;&#32452;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning is a promising programming paradigm for the optimization of quantum algorithms in the current era of noisy intermediate scale quantum (NISQ) computers. A fundamental challenge in quantum machine learning is generalization, as the designer targets performance under testing conditions, while having access only to limited training data. Existing generalization analyses, while identifying important general trends and scaling laws, cannot be used to assign reliable and informative "error bars" to the decisions made by quantum models. In this article, we propose a general methodology that can reliably quantify the uncertainty of quantum models, irrespective of the amount of training data, of the number of shots, of the ansatz, of the training algorithm, and of the presence of quantum hardware noise. The approach, which builds on probabilistic conformal prediction, turns an arbitrary, possibly small, number of shots from a pre-trained quantum model into a set predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL&#31639;&#27861;&#65292;&#23558;&#36328;&#23610;&#24230;&#20851;&#31995;&#26174;&#24335;&#32858;&#21512;&#21040;&#19968;&#20010;&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;MIL&#32593;&#32476;&#20013;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24573;&#30053;&#23545;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#30340;&#36328;&#23610;&#24230;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00216</link><description>&lt;p&gt;
&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;&#36328;&#23610;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-scale Multi-instance Learning for Pathological Image Diagnosis. (arXiv:2304.00216v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL&#31639;&#27861;&#65292;&#23558;&#36328;&#23610;&#24230;&#20851;&#31995;&#26174;&#24335;&#32858;&#21512;&#21040;&#19968;&#20010;&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;MIL&#32593;&#32476;&#20013;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24573;&#30053;&#23545;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#30340;&#36328;&#23610;&#24230;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#65292;&#36328;&#22810;&#20010;&#23610;&#24230;&#20998;&#26512;&#39640;&#20998;&#36776;&#29575;&#30340;&#20840;&#24133;&#22270;&#20687; (WSIs) &#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22810;&#23454;&#20363;&#23398;&#20064; (MIL) &#26159;&#21033;&#29992;&#20998;&#31867;&#23545;&#35937;&#38598; (&#20363;&#22914;&#36739;&#23567;&#30340;&#22270;&#20687;&#22359;&#38598;) &#23545;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22788;&#29702;&#36890;&#24120;&#22312;WSIs&#30340;&#21333;&#20010;&#23610;&#24230;&#65288;&#20363;&#22914;20&#20493;&#25918;&#22823;&#65289;&#19978;&#36827;&#34892;&#65292;&#24573;&#30053;&#20102;&#23545;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#30340;&#36328;&#23610;&#24230;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL&#31639;&#27861;&#65292;&#23558;&#36328;&#23610;&#24230;&#20851;&#31995;&#26174;&#24335;&#32858;&#21512;&#21040;&#19968;&#20010;&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;MIL&#32593;&#32476;&#20013;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;(1) &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL (CS-MIL)&#31639;&#27861;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#36328;&#23610;&#24230;&#20851;&#31995;&#65307;(2) &#21019;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#29609;&#20855;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#23610;&#24230;&#29305;&#24322;&#24615;&#24418;&#24577;&#29305;&#24449;&#65292;&#20197;&#26816;&#26597;&#21644;&#21487;&#35270;&#21270;&#19981;&#21516;&#30340;&#36328;&#23610;&#24230;&#20851;&#31995;&#65307;(3)&#22312;&#22235;&#20010;WSI&#30340;&#32454;&#32990;&#32954;&#30284;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;CS-MIL&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing high resolution whole slide images (WSIs) with regard to information across multiple scales poses a significant challenge in digital pathology. Multi-instance learning (MIL) is a common solution for working with high resolution images by classifying bags of objects (i.e. sets of smaller image patches). However, such processing is typically performed at a single scale (e.g., 20x magnification) of WSIs, disregarding the vital inter-scale information that is key to diagnoses by human pathologists. In this study, we propose a novel cross-scale MIL algorithm to explicitly aggregate inter-scale relationships into a single MIL network for pathological image diagnosis. The contribution of this paper is three-fold: (1) A novel cross-scale MIL (CS-MIL) algorithm that integrates the multi-scale information and the inter-scale relationships is proposed; (2) A toy dataset with scale-specific morphological features is created and released to examine and visualize differential cross-scale a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;GNN&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20248;&#21270;&#26381;&#21153;&#36136;&#37327;&#65292;&#35299;&#20915;ONTS&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13773</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65306;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#27169;&#22411;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
A Graph Neural Network Approach to Nanosatellite Task Scheduling: Insights into Learning Mixed-Integer Models. (arXiv:2303.13773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;GNN&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20248;&#21270;&#26381;&#21153;&#36136;&#37327;&#65292;&#35299;&#20915;ONTS&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26356;&#26377;&#25928;&#22320;&#35843;&#24230;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#12290;&#22312;&#31163;&#32447;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#65288;ONTS&#65289;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#22312;&#36712;&#36947;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26368;&#20339;&#23433;&#25490;&#65292;&#21516;&#26102;&#32771;&#34385;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#26041;&#38754;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#20248;&#20808;&#32423;&#65292;&#26368;&#23567;&#21644;&#26368;&#22823;&#28608;&#27963;&#20107;&#20214;&#65292;&#25191;&#34892;&#26102;&#38388;&#26694;&#26550;&#65292;&#21608;&#26399;&#21644;&#25191;&#34892;&#31383;&#21475;&#65292;&#20197;&#21450;&#21355;&#26143;&#30005;&#21147;&#36164;&#28304;&#21644;&#33021;&#37327;&#25910;&#38598;&#21644;&#31649;&#29702;&#30340;&#22797;&#26434;&#24615;&#30340;&#32422;&#26463;&#12290;ONTS&#38382;&#39064;&#24050;&#32463;&#20351;&#29992;&#20256;&#32479;&#30340;&#25968;&#23398;&#20844;&#24335;&#21644;&#31934;&#30830;&#26041;&#27861;&#36827;&#34892;&#20102;&#22788;&#29702;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#38382;&#39064;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20351;&#29992;GNN&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#20248;&#21270;&#38382;&#39064;&#65292;&#21253;&#25324;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#35843;&#24230;&#38382;&#39064;&#21644;&#35774;&#26045;&#25918;&#32622;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ONTS&#38382;&#39064;&#30340;MILP&#23454;&#20363;&#23436;&#20840;&#34920;&#31034;&#25104;&#20108;&#20998;&#22270;&#32593;&#32476;&#32467;&#26500;&#26469;&#24212;&#29992;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates how to schedule nanosatellite tasks more efficiently using Graph Neural Networks (GNN). In the Offline Nanosatellite Task Scheduling (ONTS) problem, the goal is to find the optimal schedule for tasks to be carried out in orbit while taking into account Quality-of-Service (QoS) considerations such as priority, minimum and maximum activation events, execution time-frames, periods, and execution windows, as well as constraints on the satellite's power resources and the complexity of energy harvesting and management. The ONTS problem has been approached using conventional mathematical formulations and precise methods, but their applicability to challenging cases of the problem is limited. This study examines the use of GNNs in this context, which has been effectively applied to many optimization problems, including traveling salesman problems, scheduling problems, and facility placement problems. Here, we fully represent MILP instances of the ONTS problem in biparti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#20266;&#30417;&#30563;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#26080;&#30417;&#30563;&#22270;&#29255;&#21040;&#22270;&#29255;&#32763;&#35793;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.10310</link><description>&lt;p&gt;
&#20266;&#30417;&#30563;&#24230;&#37327;&#65306;&#22312;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013;&#35780;&#20272;&#26080;&#30417;&#30563;&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks. (arXiv:2303.10310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#20266;&#30417;&#30563;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#26080;&#30417;&#30563;&#22270;&#29255;&#21040;&#22270;&#29255;&#32763;&#35793;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#39640;&#25928;&#24615;&#21462;&#20915;&#20110;&#35775;&#38382;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#30340;&#30456;&#21516;&#39046;&#22495;&#19978;&#27979;&#35797;&#25968;&#25454;&#12290;&#24403;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26032;&#25968;&#25454;&#26102;&#65292;&#20998;&#31867;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25910;&#38598;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#24182;&#20174;&#22836;&#35757;&#32451;&#26032;&#20998;&#31867;&#22120;&#32791;&#26102;&#12289;&#26114;&#36149;&#65292;&#26377;&#26102;&#26159;&#19981;&#21487;&#34892;&#25110;&#19981;&#21487;&#33021;&#30340;&#12290;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#22270;&#20687;&#23545;&#22270;&#20687; (UI2I) &#32763;&#35793;&#27169;&#22411;&#23558;&#36755;&#20837;&#22270;&#20687;&#20174;&#26410;&#26631;&#35760;&#30340;&#22495;&#36716;&#25442;&#20026;&#26631;&#35760;&#22495;&#26469;&#22788;&#29702;&#36825;&#20010;&#25968;&#25454;&#22495;&#28418;&#31227;&#38382;&#39064;&#12290;&#36825;&#20123;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#38382;&#39064;&#22312;&#20110;&#23427;&#20204;&#26159;&#26080;&#30417;&#30563;&#30340;&#12290;&#30001;&#20110;&#32570;&#23569;&#27880;&#37322;&#65292;&#26080;&#27861;&#20351;&#29992;&#20256;&#32479;&#30340;&#30417;&#30563;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#32763;&#35793;&#27169;&#22411;&#20197;&#36873;&#25321;&#26368;&#20339;&#30340;&#26816;&#26597;&#28857;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20266;&#30417;&#30563;&#24230;&#37327;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013; UI2I &#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to classify images accurately and efficiently is dependent on having access to large labeled datasets and testing on data from the same domain that the model is trained on. Classification becomes more challenging when dealing with new data from a different domain, where collecting a large labeled dataset and training a new classifier from scratch is time-consuming, expensive, and sometimes infeasible or impossible. Cross-domain classification frameworks were developed to handle this data domain shift problem by utilizing unsupervised image-to-image (UI2I) translation models to translate an input image from the unlabeled domain to the labeled domain. The problem with these unsupervised models lies in their unsupervised nature. For lack of annotations, it is not possible to use the traditional supervised metrics to evaluate these translation models to pick the best-saved checkpoint model. In this paper, we introduce a new method called Pseudo Supervised Metrics that was desig
&lt;/p&gt;</description></item><item><title>Neural-BO&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#32553;&#25918;&#21644;&#32500;&#25968;&#38382;&#39064;&#65292;&#20855;&#26377;&#39640;&#25928;&#25910;&#25947;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01682</link><description>&lt;p&gt;
Neural-BO: &#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks. (arXiv:2303.01682v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01682
&lt;/p&gt;
&lt;p&gt;
Neural-BO&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#32553;&#25918;&#21644;&#32500;&#25968;&#38382;&#39064;&#65292;&#20855;&#26377;&#39640;&#25928;&#25910;&#25947;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#23545;&#40657;&#30418;&#20989;&#25968;&#36827;&#34892;&#20840;&#23616;&#20248;&#21270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24403;&#20989;&#25968;&#35780;&#20272;&#20195;&#20215;&#39640;&#26102;&#12290;&#20043;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#27169;&#25311;&#40657;&#30418;&#20989;&#25968;&#65292;&#28982;&#32780;&#65292;&#39640;&#26031;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#26680;&#20989;&#25968;&#23548;&#33268;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#19968;&#26159;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#28857;&#25968;&#37327;&#36739;&#22823;&#26102;&#32553;&#25918;&#22256;&#38590;&#65292;&#20108;&#26159;&#26680;&#26041;&#27861;&#22312;&#22797;&#26434;&#32467;&#26500;&#39640;&#32500;&#25968;&#25454;&#19978;&#36890;&#24120;&#25928;&#26524;&#19981;&#20339;&#65292;&#22240;&#20026;&#32500;&#25968;&#28798;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20013;&#40657;&#30418;&#20989;&#25968;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#27492;&#35745;&#31639;&#19978;&#26356;&#21152;&#26377;&#21033;&#12290;&#25105;&#20204;&#20351;&#29992;NTK&#29702;&#35770;&#30340;&#36827;&#23637;&#20998;&#26512;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#29702;&#35770;&#34892;&#20026;&#65292;&#23637;&#31034;&#20102;&#20854;&#25910;&#25947;&#30340;&#39640;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is an effective approach for global optimization of black-box functions when function evaluations are expensive. Most prior works use Gaussian processes to model the black-box function, however, the use of kernels in Gaussian processes leads to two problems: first, the kernel-based methods scale poorly with the number of data points and second, kernel methods are usually not effective on complex structured high dimensional data due to curse of dimensionality. Therefore, we propose a novel black-box optimization algorithm where the black-box function is modeled using a neural network. Our algorithm does not need a Bayesian neural network to estimate predictive uncertainty and is therefore computationally favorable. We analyze the theoretical behavior of our algorithm in terms of regret bound using advances in NTK theory showing its efficient convergence. We perform experiments with both synthetic and real-world optimization tasks and show that our algorithm is
&lt;/p&gt;</description></item><item><title>&#23376;&#37319;&#26679;&#26159;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#20165;&#38656;&#22522;&#20110;&#38543;&#26426;&#23376;&#26679;&#26412;&#21644;&#23569;&#37327;&#27604;&#29305;&#36755;&#20986;&#30340;&#26597;&#35810;&#65292;&#21363;&#21487;&#20445;&#35777;&#20195;&#34920;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.08661</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#23376;&#37319;&#26679;&#36275;&#22815;
&lt;/p&gt;
&lt;p&gt;
Subsampling Suffices for Adaptive Data Analysis. (arXiv:2302.08661v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08661
&lt;/p&gt;
&lt;p&gt;
&#23376;&#37319;&#26679;&#26159;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#20165;&#38656;&#22522;&#20110;&#38543;&#26426;&#23376;&#26679;&#26412;&#21644;&#23569;&#37327;&#27604;&#29305;&#36755;&#20986;&#30340;&#26597;&#35810;&#65292;&#21363;&#21487;&#20445;&#35777;&#20195;&#34920;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#23545;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#20195;&#34920;&#25972;&#20010;&#26679;&#26412;&#24635;&#20307;&#26159;&#32479;&#35745;&#23398;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#20043;&#19968;&#12290;&#22823;&#22810;&#25968;&#32463;&#20856;&#25216;&#26415;&#20551;&#35774;&#25968;&#25454;&#38598;&#19982;&#20998;&#26512;&#24072;&#30340;&#26597;&#35810;&#26080;&#20851;&#65292;&#24182;&#22312;&#22810;&#27425;&#12289;&#33258;&#36866;&#24212;&#36873;&#25321;&#30340;&#26597;&#35810;&#20013;&#22833;&#25928;&#12290;&#36825;&#20010;&#8220;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#8221;&#38382;&#39064;&#22312;Dwork&#31561;&#20154;&#65288;STOC&#65292;2015&#65289;&#21644;Hardt&#21644;Ullman&#65288;FOCS&#65292;2014&#65289;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20013;&#24471;&#21040;&#20102;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#20551;&#35774;&#38598;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#33258;&#36866;&#24212;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#65292;&#26597;&#35810;&#20173;&#28982;&#20855;&#26377;&#20195;&#34920;&#24615;&#65306;&#21807;&#19968;&#30340;&#35201;&#27714;&#26159;&#27599;&#20010;&#26597;&#35810;&#37319;&#29992;&#38543;&#26426;&#23376;&#26679;&#26412;&#20316;&#20026;&#36755;&#20837;&#24182;&#36755;&#20986;&#23569;&#37327;&#27604;&#29305;&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#23376;&#37319;&#26679;&#20013;&#22266;&#26377;&#30340;&#22122;&#38899;&#36275;&#20197;&#20445;&#35777;&#26597;&#35810;&#30340;&#21709;&#24212;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;&#36825;&#31181;&#22522;&#20110;&#23376;&#37319;&#26679;&#30340;&#26694;&#26550;&#30340;&#31616;&#21333;&#24615;&#20351;&#20854;&#33021;&#22815;&#27169;&#25311;&#20043;&#21069;&#30740;&#31350;&#25152;&#26410;&#28085;&#30422;&#30340;&#21508;&#31181;&#23454;&#38469;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring that analyses performed on a dataset are representative of the entire population is one of the central problems in statistics. Most classical techniques assume that the dataset is independent of the analyst's query and break down in the common setting where a dataset is reused for multiple, adaptively chosen, queries. This problem of \emph{adaptive data analysis} was formalized in the seminal works of Dwork et al. (STOC, 2015) and Hardt and Ullman (FOCS, 2014).  We identify a remarkably simple set of assumptions under which the queries will continue to be representative even when chosen adaptively: The only requirements are that each query takes as input a random subsample and outputs few bits. This result shows that the noise inherent in subsampling is sufficient to guarantee that query responses generalize. The simplicity of this subsampling-based framework allows it to model a variety of real-world scenarios not covered by prior work.  In addition to its simplicity, we demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#19987;&#23478;&#34892;&#20026;&#65292;&#25913;&#21892;&#20102;&#26426;&#22120;&#20154;&#25805;&#20316;&#31574;&#30053;&#30340;&#34892;&#20026;&#20811;&#38534;&#12290;&#22312;NeurIPS 2022&#31454;&#36187;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#31616;&#21333;&#30340;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#34892;&#20026;&#20811;&#38534;&#65292;&#22312;&#19987;&#23478;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#28151;&#21512;&#25968;&#25454;&#38598;&#26102;&#65292;&#34892;&#20026;&#20811;&#38534;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#34920;&#29616;&#20063;&#19981;&#29702;&#24819;&#12290;&#36890;&#36807;&#23545;&#28151;&#21512;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#20013;&#21253;&#21547;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#19987;&#23478;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#35782;&#21035;&#28151;&#21512;&#25968;&#25454;&#20013;&#30340;&#19987;&#23478;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2301.13019</link><description>&lt;p&gt;
&#22312;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#19987;&#23478;&#34892;&#20026;&#25913;&#21892;&#20102;&#26426;&#22120;&#20154;&#25805;&#20316;&#31574;&#30053;&#30340;&#34892;&#20026;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
Identifying Expert Behavior in Offline Training Datasets Improves Behavioral Cloning of Robotic Manipulation Policies. (arXiv:2301.13019v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#19987;&#23478;&#34892;&#20026;&#65292;&#25913;&#21892;&#20102;&#26426;&#22120;&#20154;&#25805;&#20316;&#31574;&#30053;&#30340;&#34892;&#20026;&#20811;&#38534;&#12290;&#22312;NeurIPS 2022&#31454;&#36187;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#31616;&#21333;&#30340;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#34892;&#20026;&#20811;&#38534;&#65292;&#22312;&#19987;&#23478;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#28151;&#21512;&#25968;&#25454;&#38598;&#26102;&#65292;&#34892;&#20026;&#20811;&#38534;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#34920;&#29616;&#20063;&#19981;&#29702;&#24819;&#12290;&#36890;&#36807;&#23545;&#28151;&#21512;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#20013;&#21253;&#21547;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#19987;&#23478;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#35782;&#21035;&#28151;&#21512;&#25968;&#25454;&#20013;&#30340;&#19987;&#23478;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;NeurIPS 2022&#31454;&#36187;&#20013;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#31454;&#36187;&#26088;&#22312;&#36890;&#36807;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#20219;&#21153;&#25552;&#20379;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#65306;&#19987;&#23478;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#25216;&#33021;&#27700;&#24179;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;&#22312;&#19987;&#23478;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26368;&#31616;&#21333;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65288;&#34892;&#20026;&#20811;&#38534;&#65289;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#23427;&#22312;&#24212;&#29992;&#20110;&#28151;&#21512;&#25968;&#25454;&#38598;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#34920;&#29616;&#20063;&#20196;&#20154;&#19981;&#28385;&#24847;&#12290;&#32463;&#36807;&#26816;&#26597;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#20013;&#21253;&#21547;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#19987;&#23478;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#28151;&#21512;&#25968;&#25454;&#20013;&#30340;&#19987;&#23478;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our solution for the Real Robot Challenge (RRC) III, a competition featured in the NeurIPS 2022 Competition Track, aimed at addressing dexterous robotic manipulation tasks through learning from pre-collected offline data. Participants were provided with two types of datasets for each task: expert and mixed datasets with varying skill levels. While the simplest offline policy learning algorithm, Behavioral Cloning (BC), performed remarkably well when trained on expert datasets, it outperformed even the most advanced offline reinforcement learning (RL) algorithms. However, BC's performance deteriorated when applied to mixed datasets, and the performance of offline RL algorithms was also unsatisfactory. Upon examining the mixed datasets, we observed that they contained a significant amount of expert data, although this data was unlabeled. To address this issue, we proposed a semi-supervised learning-based classifier to identify the underlying expert behavior within mix
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#20013;&#19987;&#23478;&#36712;&#36857;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#34892;&#20026;&#20811;&#38534;&#65292;&#34920;&#29616;&#20986;&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11734</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#26410;&#26631;&#35760;&#23398;&#20064;&#25913;&#36827;&#34892;&#20026;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
Improving Behavioural Cloning with Positive Unlabeled Learning. (arXiv:2301.11734v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#20013;&#19987;&#23478;&#36712;&#36857;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#34892;&#20026;&#20811;&#38534;&#65292;&#34920;&#29616;&#20986;&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#39044;&#20808;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#20013;&#31163;&#32447;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#26159;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#20855;&#26377;&#28151;&#21512;&#36136;&#37327;&#65292;&#20854;&#20013;&#39640;&#36136;&#37327;&#31034;&#33539;&#65288;&#21363;&#39640;&#36136;&#37327;&#30340;&#36712;&#36857;&#65289;&#30340;&#25968;&#37327;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#32473;&#23450;&#26368;&#23567;&#25968;&#37327;&#30340;&#27491;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#20110;&#35782;&#21035;&#26080;&#26631;&#35760;&#28151;&#21512;&#36136;&#37327;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#20013;&#30340;&#19987;&#23478;&#36712;&#36857;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#19978;&#36229;&#36234;&#29616;&#26377;&#31639;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;&#34892;&#20026;&#20811;&#38534;&#24212;&#29992;&#20110;&#32467;&#26524;&#36807;&#28388;&#21518;&#30340;&#25968;&#25454;&#38598;&#20248;&#20110;&#22810;&#20010;&#31454;&#20105;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#22522;&#32447;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#27169;&#25311;&#36816;&#21160;&#20219;&#21153;&#21644;&#19968;&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#31995;&#32479;&#19978;&#36827;&#34892;&#23454;&#39564;&#65307;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning control policies offline from pre-recorded datasets is a promising avenue for solving challenging real-world problems. However, available datasets are typically of mixed quality, with a limited number of the trajectories that we would consider as positive examples; i.e., high-quality demonstrations. Therefore, we propose a novel iterative learning algorithm for identifying expert trajectories in unlabeled mixed-quality robotics datasets given a minimal set of positive examples, surpassing existing algorithms in terms of accuracy. We show that applying behavioral cloning to the resulting filtered dataset outperforms several competitive offline reinforcement learning and imitation learning baselines. We perform experiments on a range of simulated locomotion tasks and on two challenging manipulation tasks on a real robotic system; in these experiments, our method showcases state-of-the-art performance. Our website: \url{https://sites.google.com/view/offline-policy-learning-pubc}.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20351;&#29992;Transformers&#30340;&#21160;&#26426;&#12289;&#36827;&#23637;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2301.03044</link><description>&lt;p&gt;
&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;Transformers&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Transformers in Reinforcement Learning. (arXiv:2301.03044v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03044
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20351;&#29992;Transformers&#30340;&#21160;&#26426;&#12289;&#36827;&#23637;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#34987;&#35748;&#20026;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#20013;&#30340;&#20027;&#23548;&#31070;&#32463;&#26550;&#26500;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39046;&#22495;&#20013;&#20063;&#20986;&#29616;&#20102;&#31867;&#20284;&#30340;&#20351;&#29992;Transformers&#30340;&#28526;&#27969;&#65292;&#20294;&#38754;&#20020;&#30528;RL&#30340;&#29305;&#27530;&#35774;&#35745;&#36873;&#25321;&#21644;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;Transformers&#22312;RL&#20013;&#30340;&#21457;&#23637;&#23578;&#26410;&#34987;&#20805;&#20998;&#25581;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#22312;RL&#20013;&#20351;&#29992;Transformers&#30340;&#21160;&#26426;&#21644;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29616;&#26377;&#24037;&#20316;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#35752;&#35770;&#20102;&#27599;&#20010;&#23376;&#39046;&#22495;&#65292;&#24182;&#24635;&#32467;&#20102;&#26410;&#26469;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#20855;&#22791;&#20915;&#31574;&#21644;&#25511;&#21046;&#33021;&#21147;&#30340;&#34893;&#23556;&#20809;&#23398;&#32593;&#32476;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#30340;&#20915;&#31574;&#21644;&#25511;&#21046;&#33021;&#21147;&#12290;&#36825;&#31181;&#32593;&#32476;&#21033;&#29992;&#27531;&#24046;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#25214;&#21040;&#26368;&#20248;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#20855;&#22791;&#39640;&#36895;&#21644;&#20302;&#21151;&#32791;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2212.11278</link><description>&lt;p&gt;
&#20351;&#29992;&#34893;&#23556;&#20809;&#23398;&#32593;&#32476;&#30340;&#20915;&#31574;&#21644;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Decision-making and control with diffractive optical networks. (arXiv:2212.11278v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#20855;&#22791;&#20915;&#31574;&#21644;&#25511;&#21046;&#33021;&#21147;&#30340;&#34893;&#23556;&#20809;&#23398;&#32593;&#32476;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#30340;&#20915;&#31574;&#21644;&#25511;&#21046;&#33021;&#21147;&#12290;&#36825;&#31181;&#32593;&#32476;&#21033;&#29992;&#27531;&#24046;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#25214;&#21040;&#26368;&#20248;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#20855;&#22791;&#39640;&#36895;&#21644;&#20302;&#21151;&#32791;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#32456;&#26497;&#30446;&#26631;&#26159;&#27169;&#25311;&#20154;&#33041;&#65292;&#20174;&#39640;&#32500;&#24863;&#30693;&#36755;&#20837;&#20013;&#30452;&#25509;&#36827;&#34892;&#20915;&#31574;&#21644;&#25511;&#21046;&#12290;&#34893;&#23556;&#20809;&#23398;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36895;&#21644;&#20302;&#21151;&#32791;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#22823;&#22810;&#25968;&#25253;&#36947;&#30340;&#34893;&#23556;&#20809;&#23398;&#32593;&#32476;&#37117;&#38598;&#20013;&#22312;&#21333;&#20010;&#25110;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#36825;&#20123;&#20219;&#21153;&#19981;&#28041;&#21450;&#29615;&#22659;&#20132;&#20114;&#65292;&#27604;&#22914;&#29289;&#20307;&#35782;&#21035;&#21644;&#22270;&#20687;&#20998;&#31867;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30446;&#21069;&#36824;&#27809;&#26377;&#24320;&#21457;&#20986;&#33021;&#22815;&#36827;&#34892;&#20915;&#31574;&#21644;&#25511;&#21046;&#30340;&#32593;&#32476;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#27169;&#25311;&#20154;&#31867;&#32423;&#21035;&#30340;&#20915;&#31574;&#21644;&#25511;&#21046;&#33021;&#21147;&#30340;&#34893;&#23556;&#20809;&#23398;&#32593;&#32476;&#12290;&#36825;&#20123;&#32593;&#32476;&#21033;&#29992;&#27531;&#24046;&#26550;&#26500;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#25214;&#21040;&#26368;&#20248;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#29616;&#26377;&#30340;&#20809;&#23398;&#35774;&#22791;&#36731;&#26494;&#23454;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ultimate goal of artificial intelligence is to mimic the human brain to perform decision-making and control directly from high-dimensional sensory input. Diffractive optical networks provide a promising solution for implementing artificial intelligence with high-speed and low-power consumption. Most of the reported diffractive optical networks focus on single or multiple tasks that do not involve environmental interaction, such as object recognition and image classification. In contrast, the networks capable of performing decision-making and control have not yet been developed to our knowledge. Here, we propose using deep reinforcement learning to implement diffractive optical networks that imitate human-level decision-making and control capability. Such networks taking advantage of a residual architecture, allow for finding optimal control policies through interaction with the environment and can be readily implemented with existing optical devices. The superior performance of the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DREAM&#30340;&#21160;&#24577;&#35843;&#24230;&#22120;&#65292;&#38024;&#23545;&#23454;&#26102;&#22810;&#27169;&#22411;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#20013;&#30340;&#21508;&#31181;&#21160;&#24577;&#34892;&#20026;&#35774;&#35745;&#12290;DREAM&#36890;&#36807;&#37327;&#21270;&#24471;&#20998;&#26469;&#39537;&#21160;&#35843;&#24230;&#20915;&#31574;&#65292;&#32771;&#34385;&#24403;&#21069;&#31995;&#32479;&#36127;&#36733;&#21644;&#20854;&#20182;&#25512;&#29702;&#20219;&#21153;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;&#20110;&#26356;&#22909;&#22320;&#21033;&#29992;&#24213;&#23618;&#30828;&#20214;&#65292;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2212.03414</link><description>&lt;p&gt;
DREAM: &#19968;&#31181;&#21160;&#24577;&#35843;&#24230;&#22120;&#65292;&#29992;&#20110;&#21160;&#24577;&#23454;&#26102;&#22810;&#27169;&#22411;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;
&lt;/p&gt;
&lt;p&gt;
DREAM: A Dynamic Scheduler for Dynamic Real-time Multi-model ML Workloads. (arXiv:2212.03414v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03414
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DREAM&#30340;&#21160;&#24577;&#35843;&#24230;&#22120;&#65292;&#38024;&#23545;&#23454;&#26102;&#22810;&#27169;&#22411;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#20013;&#30340;&#21508;&#31181;&#21160;&#24577;&#34892;&#20026;&#35774;&#35745;&#12290;DREAM&#36890;&#36807;&#37327;&#21270;&#24471;&#20998;&#26469;&#39537;&#21160;&#35843;&#24230;&#20915;&#31574;&#65292;&#32771;&#34385;&#24403;&#21069;&#31995;&#32479;&#36127;&#36733;&#21644;&#20854;&#20182;&#25512;&#29702;&#20219;&#21153;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;&#20110;&#26356;&#22909;&#22320;&#21033;&#29992;&#24213;&#23618;&#30828;&#20214;&#65292;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#23454;&#26102;&#22810;&#27169;&#22411;&#26426;&#22120;&#23398;&#20064;&#65288;RTMM&#65289;&#24037;&#20316;&#36127;&#36733;&#65292;&#22914;AR/VR&#21644;&#26080;&#20154;&#26426;&#25511;&#21046;&#65292;&#28041;&#21450;&#21040;&#19981;&#21516;&#31890;&#24230;&#30340;&#21160;&#24577;&#34892;&#20026;&#65307;&#20219;&#21153;&#12289;&#27169;&#22411;&#20197;&#21450;&#27169;&#22411;&#20869;&#30340;&#23618;&#27425;&#12290;&#36825;&#20123;&#21160;&#24577;&#34892;&#20026;&#32473;ML&#31995;&#32479;&#20013;&#30340;&#31995;&#32479;&#36719;&#20214;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25972;&#20307;&#31995;&#32479;&#36127;&#36733;&#19981;&#23436;&#20840;&#21487;&#39044;&#27979;&#65292;&#19981;&#20687;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#12290;&#27492;&#22806;&#65292;RTMM&#24037;&#20316;&#36127;&#36733;&#38656;&#35201;&#23454;&#26102;&#22788;&#29702;&#65292;&#28041;&#21450;&#21040;&#39640;&#24230;&#24322;&#26500;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#30446;&#26631;&#26159;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#35843;&#24230;&#22120;&#23545;&#20110;&#26356;&#22909;&#22320;&#21033;&#29992;&#24213;&#23618;&#30828;&#20214;&#26469;&#35828;&#26356;&#21152;&#37325;&#35201;&#65292;&#32771;&#34385;&#21040;RTMM&#24037;&#20316;&#36127;&#36733;&#30340;&#29420;&#29305;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#22120;&#65292;&#21517;&#20026;DREAM&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;RTMM&#24037;&#20316;&#36127;&#36733;&#20013;&#30340;&#21508;&#31181;&#21160;&#24577;&#24615;&#65292;&#38024;&#23545;&#22810;&#21152;&#36895;&#22120;&#31995;&#32479;&#36827;&#34892;&#35843;&#24230;&#12290;DREAM&#37327;&#21270;&#20102;RTMM&#24037;&#20316;&#36127;&#36733;&#30340;&#29420;&#29305;&#38656;&#27714;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#37327;&#21270;&#24471;&#20998;&#26469;&#39537;&#21160;&#35843;&#24230;&#20915;&#31574;&#65292;&#32771;&#34385;&#24403;&#21069;&#31995;&#32479;&#36127;&#36733;&#21644;&#20854;&#20182;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging real-time multi-model ML (RTMM) workloads such as AR/VR and drone control involve dynamic behaviors in various granularity; task, model, and layers within a model. Such dynamic behaviors introduce new challenges to the system software in an ML system since the overall system load is not completely predictable, unlike traditional ML workloads. In addition, RTMM workloads require real-time processing, involve highly heterogeneous models, and target resource-constrained devices. Under such circumstances, developing an effective scheduler gains more importance to better utilize underlying hardware considering the unique characteristics of RTMM workloads. Therefore, we propose a new scheduler, DREAM, which effectively handles various dynamicity in RTMM workloads targeting multi-accelerator systems. DREAM quantifies the unique requirements for RTMM workloads and utilizes the quantified scores to drive scheduling decisions, considering the current system load and other inference jobs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Grassmann&#27969;&#24418;&#23398;&#20064;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#31283;&#23450;&#30340;&#24418;&#29366;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.02900</link><description>&lt;p&gt;
Grassmann&#27969;&#24418;&#27969;&#29992;&#20110;&#31283;&#23450;&#24418;&#29366;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Grassmann Manifold Flows for Stable Shape Generation. (arXiv:2211.02900v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Grassmann&#27969;&#24418;&#23398;&#20064;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#31283;&#23450;&#30340;&#24418;&#29366;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#21033;&#29992;&#29305;&#23450;&#27969;&#24418;&#20013;&#30340;&#23545;&#31216;&#24615;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#30340;&#26041;&#27861;&#19978;&#12290;Grassmann&#27969;&#24418;&#25552;&#20379;&#20102;&#22788;&#29702;&#20197;&#24418;&#29366;&#31354;&#38388;&#34920;&#31034;&#30340;&#22522;&#26412;&#24418;&#29366;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#24418;&#29366;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#24402;&#19968;&#21270;&#27969;&#22312;Grassmann&#27969;&#24418;&#19978;&#24314;&#31435;&#23398;&#20064;&#20998;&#24067;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#26126;&#30830;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#31283;&#23450;&#30340;&#24418;&#29366;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;Grassmann&#27969;&#24418;&#20869;&#23398;&#20064;&#21644;&#29983;&#25104;&#65292;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#26059;&#36716;&#21644;&#32763;&#36716;&#31561;&#22806;&#37096;&#21464;&#25442;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#29983;&#25104;&#65292;&#20197;&#36866;&#24212;&#23545;&#35937;&#30340;&#22522;&#26412;&#24418;&#29366;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#25429;&#25417;&#25968;&#25454;&#32467;&#26500;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;t&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, studies on machine learning have focused on methods that use symmetry implicit in a specific manifold as an inductive bias. Grassmann manifolds provide the ability to handle fundamental shapes represented as shape spaces, enabling stable shape analysis. In this paper, we present a novel approach in which we establish the theoretical foundations for learning distributions on the Grassmann manifold via continuous normalization flows, with the explicit goal of generating stable shapes. Our approach facilitates more robust generation by effectively eliminating the influence of extraneous transformations, such as rotations and inversions, through learning and generating within a Grassmann manifolds designed to accommodate the essential shape information of the object. The experimental results indicated that the proposed method can generate high-quality samples by capturing the data structure. Furthermore, the proposed method significantly outperformed state-of-the-art methods in t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21512;&#20316;&#36873;&#39033;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22810;&#20010;&#26234;&#33021;&#20307;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#39044;&#26399;&#35206;&#30422;&#26102;&#38388;&#26469;&#26500;&#24314;&#22810;&#26234;&#33021;&#20307;&#36873;&#39033;&#65292;&#24182;&#25552;&#20986;&#20102;&#37319;&#29992;&#36825;&#20123;&#36873;&#39033;&#30340;&#26032;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2210.03269</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#35206;&#30422;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Deep Covering Skill Discovery. (arXiv:2210.03269v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03269
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21512;&#20316;&#36873;&#39033;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22810;&#20010;&#26234;&#33021;&#20307;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#39044;&#26399;&#35206;&#30422;&#26102;&#38388;&#26469;&#26500;&#24314;&#22810;&#26234;&#33021;&#20307;&#36873;&#39033;&#65292;&#24182;&#25552;&#20986;&#20102;&#37319;&#29992;&#36825;&#20123;&#36873;&#39033;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#25216;&#33021;&#65288;&#21363;&#36873;&#39033;&#65289;&#21487;&#20197;&#22823;&#22823;&#21152;&#24555;&#25506;&#32034;&#36895;&#24230;&#65292;&#23588;&#20854;&#26159;&#24403;&#21482;&#26377;&#31232;&#30095;&#30340;&#22870;&#21169;&#20449;&#21495;&#21487;&#29992;&#26102;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#38024;&#23545;&#20010;&#20307;&#26234;&#33021;&#20307;&#30340;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#23578;&#26410;&#32771;&#34385;&#22914;&#20309;&#21457;&#29616;&#21327;&#20316;&#36873;&#39033;&#65292;&#20197;&#21327;&#35843;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#24182;&#40723;&#21169;&#23427;&#20204;&#35775;&#38382;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#20013;&#26410;&#24320;&#21457;&#30340;&#21306;&#22495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#35206;&#30422;&#36873;&#39033;&#21457;&#29616;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22810;&#20010;&#26234;&#33021;&#20307;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#39044;&#26399;&#35206;&#30422;&#26102;&#38388;&#26469;&#26500;&#24314;&#22810;&#26234;&#33021;&#20307;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#36873;&#39033;&#12290;&#23454;&#38469;&#19978;&#65292;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#36890;&#24120;&#21487;&#20197;&#20998;&#20026;&#19968;&#20123;&#23376;&#20219;&#21153;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#21487;&#20197;&#30001;&#19968;&#20010;&#23376;&#22242;&#20307;&#30340;&#26234;&#33021;&#20307;&#23436;&#25104;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#39318;&#20808;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25214;&#21040;&#21327;&#20316;&#26234;&#33021;&#20307;&#23376;&#22242;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of skills (a.k.a., options) can greatly accelerate exploration in reinforcement learning, especially when only sparse reward signals are available. While option discovery methods have been proposed for individual agents, in multi-agent reinforcement learning settings, discovering collaborative options that can coordinate the behavior of multiple agents and encourage them to visit the under-explored regions of their joint state space has not been considered. In this case, we propose Multi-agent Deep Covering Option Discovery, which constructs the multi-agent options through minimizing the expected cover time of the multiple agents' joint state space. Also, we propose a novel framework to adopt the multi-agent options in the MARL process. In practice, a multi-agent task can usually be divided into some sub-tasks, each of which can be completed by a sub-group of the agents. Therefore, our algorithm framework first leverages an attention mechanism to find collaborative agent sub-gr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#36880;&#27493;&#35266;&#27979;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#21160;&#24577;Wasserstein&#23186;&#20171;&#20013;&#24515;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#24207;&#21015;&#35266;&#27979;&#20013;&#30340;&#30636;&#24577;&#34892;&#20026;&#65292;&#24182;&#25918;&#23485;&#20102;&#32431;&#24577;&#30340;&#21442;&#25968;&#21270;&#12290;</title><link>http://arxiv.org/abs/2210.01918</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#21270;&#21644;&#27491;&#21017;&#21270;&#30340;&#21160;&#24577;Wasserstein&#23186;&#20171;&#20013;&#24515;&#23545;&#20110;&#24207;&#21015;&#35266;&#27979;
&lt;/p&gt;
&lt;p&gt;
Nonparametric and Regularized Dynamical Wasserstein Barycenters for Sequential Observations. (arXiv:2210.01918v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#36880;&#27493;&#35266;&#27979;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#21160;&#24577;Wasserstein&#23186;&#20171;&#20013;&#24515;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#24207;&#21015;&#35266;&#27979;&#20013;&#30340;&#30636;&#24577;&#34892;&#20026;&#65292;&#24182;&#25918;&#23485;&#20102;&#32431;&#24577;&#30340;&#21442;&#25968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#36880;&#27493;&#35266;&#27979;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#20854;&#20013;&#26377;&#38480;&#25968;&#37327;&#30340;&#29366;&#24577;&#20043;&#38388;&#23384;&#22312;&#28176;&#36827;&#36716;&#25442;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#20154;&#31867;&#27963;&#21160;&#20998;&#26512;&#31561;&#24212;&#29992;&#65292;&#20854;&#20013;&#35266;&#27979;&#21040;&#30340;&#21152;&#36895;&#24230;&#35745;&#26102;&#38388;&#24207;&#21015;&#21253;&#21547;&#34920;&#31034;&#19981;&#21516;&#27963;&#21160;&#30340;&#29255;&#27573;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32431;&#24577;&#65292;&#20197;&#21450;&#30001;&#36825;&#20123;&#32431;&#24577;&#20043;&#38388;&#25345;&#32493;&#36716;&#25442;&#29305;&#24449;&#30340;&#26102;&#26399;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#31181;&#30636;&#24577;&#34892;&#20026;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#31243;&#24335;&#31561;&#20154;&#22312;2021&#24180;&#25552;&#20986;&#30340;&#21160;&#24577;Wasserstein&#23186;&#20171;&#20013;&#24515;(DWB)&#27169;&#22411;[1]&#65292;&#23558;&#27599;&#20010;&#32431;&#24577;&#19982;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30456;&#20851;&#32852;&#65292;&#24182;&#23558;&#36825;&#20123;&#20998;&#24067;&#30340;&#36830;&#32493;&#36716;&#25442;&#24314;&#27169;&#20026;&#20855;&#26377;&#21160;&#24577;&#28436;&#21270;&#26435;&#37325;&#30340;Wasserstein&#23186;&#20171;&#20013;&#24515;&#12290;&#38024;&#23545;&#22312;&#19968;&#20803;&#24773;&#20917;&#19979;&#21487;&#20197;&#36890;&#36807;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;Wasserstein&#36317;&#31163;&#21644;&#23186;&#20171;&#20013;&#24515;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#20855;&#20307;&#25918;&#23485;&#20102;&#32431;&#24577;&#30340;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#24378;&#35843;&#19982;&#30830;&#23450;&#21807;&#19968;&#24615;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider probabilistic models for sequential observations which exhibit gradual transitions among a finite number of states. We are particularly motivated by applications such as human activity analysis where observed accelerometer time series contains segments representing distinct activities, which we call pure states, as well as periods characterized by continuous transition among these pure states. To capture this transitory behavior, the dynamical Wasserstein barycenter (DWB) model of Cheng et al. in 2021 [1] associates with each pure state a data-generating distribution and models the continuous transitions among these states as a Wasserstein barycenter of these distributions with dynamically evolving weights. Focusing on the univariate case where Wasserstein distances and barycenters can be computed in closed form, we extend [1] specifically relaxing the parameterization of the pure states as Gaussian distributions. We highlight issues related to the uniqueness in identifying
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#33258;&#21327;&#35843;&#19988;&#30456;&#23545;&#24179;&#28369;&#30340;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#22312;&#20984;&#20989;&#25968;&#19978;&#30340;&#36951;&#25022;&#65292;&#25913;&#36827;&#20102;&#22312;&#32447;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22312;&#32447;&#23398;&#20064;&#37327;&#23376;&#24577;&#38382;&#39064;&#20013;&#36798;&#21040;&#20102;&#19982;Soft-Bayes&#31639;&#27861;&#30456;&#24403;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.00997</link><description>&lt;p&gt;
&#22312;&#32447;&#33258;&#21327;&#35843;&#19988;&#30456;&#23545;&#24179;&#28369;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#21450;&#20854;&#22312;&#22312;&#32447;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#21644;&#23398;&#20064;&#37327;&#23376;&#24577;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Online Self-Concordant and Relatively Smooth Minimization, With Applications to Online Portfolio Selection and Learning Quantum States. (arXiv:2210.00997v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#33258;&#21327;&#35843;&#19988;&#30456;&#23545;&#24179;&#28369;&#30340;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#22312;&#20984;&#20989;&#25968;&#19978;&#30340;&#36951;&#25022;&#65292;&#25913;&#36827;&#20102;&#22312;&#32447;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22312;&#32447;&#23398;&#20064;&#37327;&#23376;&#24577;&#38382;&#39064;&#20013;&#36798;&#21040;&#20102;&#19982;Soft-Bayes&#31639;&#27861;&#30456;&#24403;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#19968;&#31867;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#25439;&#22833;&#20989;&#25968;&#26159;&#33258;&#21327;&#35843;&#38556;&#30861;&#20989;&#25968;&#65292;&#22312;&#26576;&#20010;&#20984;&#20989;&#25968;h&#30340;&#30456;&#23545;&#24179;&#28369;&#65292;&#21487;&#33021;&#19981;&#26159;Lipschitz&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#22312;h&#19978;&#30340;&#36951;&#25022;&#65292;&#24182;&#22522;&#20110;&#32467;&#26524;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#35777;&#26126;&#20102;&#20197;&#19979;&#32467;&#35770;&#12290;&#23545;&#20110;&#22312;&#32447;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#38382;&#39064;&#65292;&#24403;T&gt;4d/logd&#26102;&#65292;&#25913;&#36827;&#20102;Helmbold&#31561;&#20154;&#25552;&#20986;&#30340;&#25351;&#25968;&#21270;&#26799;&#24230;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#20026;O(T^{2/3} d^{1/3})&#65292;&#21407;&#26377;&#30028;&#26159;O(T^{3/4} d^{1/2})&#12290;&#23545;&#20110;&#22312;&#32447;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#38382;&#39064;&#65292;&#20351;&#29992;&#23545;&#25968;&#38556;&#30861;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#20026;O(sqrt(Td))&#65292;&#19982;Orseau&#31561;&#20154;&#30340;Soft-Bayes&#31639;&#27861;&#20855;&#26377;&#30456;&#21516;&#30340;&#36951;&#25022;&#30028;&#65292;&#38500;&#21435;&#23545;&#25968;&#22240;&#23376;&#12290;&#23545;&#20110;&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#30340;&#22312;&#32447;&#23398;&#20064;&#37327;&#23376;&#24577;&#38382;&#39064;&#65292;&#20351;&#29992;&#23545;&#25968;&#38556;&#30861;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Consider an online convex optimization problem where the loss functions are self-concordant barriers, smooth relative to a convex function $h$, and possibly non-Lipschitz. We analyze the regret of online mirror descent with $h$. Then, based on the result, we prove the following in a unified manner. Denote by $T$ the time horizon and $d$ the parameter dimension. 1. For online portfolio selection, the regret of $\widetilde{\text{EG}}$, a variant of exponentiated gradient due to Helmbold et al., is $\tilde{O} ( T^{2/3} d^{1/3} )$ when $T &gt; 4 d / \log d$. This improves on the original $\tilde{O} ( T^{3/4} d^{1/2} )$ regret bound for $\widetilde{\text{EG}}$. 2. For online portfolio selection, the regret of online mirror descent with the logarithmic barrier is $\tilde{O}(\sqrt{T d})$. The regret bound is the same as that of Soft-Bayes due to Orseau et al. up to logarithmic terms. 3. For online learning quantum states with the logarithmic loss, the regret of online mirror descent with the log
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#24182;&#27604;&#36739;&#20102;&#24120;&#29992;&#20110;&#24230;&#37327;&#20998;&#31867;&#31995;&#32479;&#34920;&#29616;&#30340;&#21508;&#31181;&#25351;&#26631;&#65292;&#21457;&#29616;&#26399;&#26395;&#25104;&#26412;&#25351;&#26631;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#30452;&#35266;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#20174;&#36830;&#32493;&#24471;&#20998;&#29983;&#25104;&#20998;&#31867;&#20915;&#31574;&#30340;&#23454;&#36341;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.05355</link><description>&lt;p&gt;
&#20998;&#31867;&#25351;&#26631;&#30340;&#20998;&#26512;&#19982;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Analysis and Comparison of Classification Metrics. (arXiv:2209.05355v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#24182;&#27604;&#36739;&#20102;&#24120;&#29992;&#20110;&#24230;&#37327;&#20998;&#31867;&#31995;&#32479;&#34920;&#29616;&#30340;&#21508;&#31181;&#25351;&#26631;&#65292;&#21457;&#29616;&#26399;&#26395;&#25104;&#26412;&#25351;&#26631;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#30452;&#35266;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#20174;&#36830;&#32493;&#24471;&#20998;&#29983;&#25104;&#20998;&#31867;&#20915;&#31574;&#30340;&#23454;&#36341;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#24120;&#29992;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#26469;&#35780;&#20272;&#20998;&#31867;&#31995;&#32479;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20123;&#26368;&#24120;&#29992;&#30340;&#29992;&#20110;&#34913;&#37327;&#30828;&#20915;&#31574;&#36136;&#37327;&#30340;&#26631;&#20934;&#21644;&#24179;&#34913;&#20934;&#30830;&#29575;&#12289;&#26631;&#20934;&#21644;&#24179;&#34913;&#38169;&#35823;&#29575;&#12289;F-beta&#20998;&#25968;&#21644;Matthews&#30456;&#20851;&#31995;&#25968;&#65288;MCC&#65289;&#31561;&#25351;&#26631;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#36825;&#20123;&#21644;&#20854;&#20182;&#25351;&#26631;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#26399;&#26395;&#25104;&#26412;&#65288;EC&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#21518;&#32773;&#26159;&#27599;&#20010;&#32479;&#35745;&#23398;&#20064;&#35838;&#31243;&#20013;&#37117;&#20171;&#32461;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24456;&#23569;&#20351;&#29992;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#34920;&#26126;&#26631;&#20934;&#21644;&#24179;&#34913;&#38169;&#35823;&#29575;&#37117;&#26159;EC&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;EC&#19982;F&#20998;&#25968;&#21644;MCC&#30340;&#20851;&#31995;&#65292;&#24182;&#35748;&#20026;EC&#25351;&#26631;&#20248;&#20110;&#20256;&#32479;&#25351;&#26631;&#65292;&#22240;&#20854;&#26356;&#20855;&#26377;&#20248;&#38597;&#24615;&#12289;&#36890;&#29992;&#24615;&#21644;&#30452;&#35266;&#24615;&#65292;&#19988;&#22522;&#20110;&#32479;&#35745;&#23398;&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#26412;&#25991;&#20013;&#20171;&#32461;&#30340;&#25351;&#26631;&#22343;&#29992;&#20110;&#24230;&#37327;&#30828;&#20915;&#31574;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#20195;&#20998;&#31867;&#31995;&#32479;&#36755;&#20986;&#36830;&#32493;&#24471;&#20998;&#65292;&#32780;&#26377;&#19968;&#20010;&#37325;&#35201;&#30340;&#23454;&#36341;&#38382;&#39064;&#26159;&#22914;&#20309;&#20174;&#36825;&#20123;&#36830;&#32493;&#24471;&#20998;&#20013;&#29983;&#25104;&#20998;&#31867;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
A variety of different performance metrics are commonly used in the machine learning literature for the evaluation of classification systems. Some of the most common ones for measuring quality of hard decisions are standard and balanced accuracy, standard and balanced error rate, F-beta score, and Matthews correlation coefficient (MCC). In this document, we review the definition of these and other metrics and compare them with the expected cost (EC), a metric introduced in every statistical learning course but rarely used in the machine learning literature. We show that both the standard and balanced error rates are special cases of the EC. Further, we show its relation with F-score and MCC and argue that EC is superior to these traditional metrics, being more elegant, general, and intuitive, as well as being based on basic principles from statistics.  The metrics above measure the quality of hard decisions. Yet, most modern classification systems output continuous scores for the class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#20998;&#31867;&#12289;&#24403;&#21069;&#36235;&#21183;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#35813;&#35843;&#26597;&#24378;&#35843;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.03392</link><description>&lt;p&gt;
&#29992;&#20110;&#21307;&#30103;&#24212;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#20998;&#31867;&#12289;&#24403;&#21069;&#36235;&#21183;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for Medical Applications: A Taxonomy, Current Trends, Challenges, and Future Research Directions. (arXiv:2208.03392v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#20998;&#31867;&#12289;&#24403;&#21069;&#36235;&#21183;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#35813;&#35843;&#26597;&#24378;&#35843;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20986;&#29616;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#21307;&#23398;&#24212;&#29992;&#39046;&#22495;&#24050;&#25104;&#20026;&#35774;&#35745;&#24378;&#22823;&#19988;&#21487;&#25193;&#23637;&#30340;&#35786;&#26029;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#26395;&#36884;&#24452;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#21307;&#23398;&#24212;&#29992;&#39046;&#22495;&#21463;&#21040;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22312;&#25913;&#21892;&#21307;&#30103;&#26381;&#21153;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#20154;&#20204;&#26222;&#36941;&#38754;&#20020;&#30528;&#37319;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21307;&#30103;&#24212;&#29992;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#28385;&#36275;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#26381;&#21153;&#36136;&#37327;&#26631;&#20934;&#30340;&#33392;&#24040;&#20219;&#21153;&#12290;&#32852;&#37030;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#20351;&#24471;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#35757;&#32451;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#24050;&#25104;&#20026;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#25955;&#30340;&#36793;&#32536;&#32593;&#32476;&#20013;&#22788;&#29702;&#21307;&#23398;&#25968;&#25454;&#20197;&#20445;&#25252;&#38544;&#31169;&#21644;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of the IoT, AI and ML/DL algorithms, the landscape of data-driven medical applications has emerged as a promising avenue for designing robust and scalable diagnostic and prognostic models from medical data. Consequently, the realm of data-driven medical applications has garnered significant attention spanning academia and industry, ushering in marked enhancements in healthcare delivery quality. Despite these strides, the adoption of AI-driven medical applications remains hindered by formidable challenges, including the arduous task of meeting security, privacy, and quality of service (QoS) standards. Recent developments in federated learning have made it possible to train complex machine-learned models in a distributed manner and has become an active research domain, particularly processing the medical data at the edge of the network in a decentralized way to preserve privacy and address security concerns. To this end, this survey paper highlights the current and future
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20010;&#24615;&#21270;PageRank&#20256;&#25773;&#30697;&#38453;&#21644;&#19979;&#28216;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65292;&#26469;&#23398;&#20064;&#26368;&#20248;&#30340;&#22270;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#23637;&#29616;&#20102;&#20248;&#36234;&#30340;&#21151;&#25928;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.02998</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#20256;&#25773;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Optimal Propagation for Graph Neural Networks. (arXiv:2205.02998v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20010;&#24615;&#21270;PageRank&#20256;&#25773;&#30697;&#38453;&#21644;&#19979;&#28216;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65292;&#26469;&#23398;&#20064;&#26368;&#20248;&#30340;&#22270;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#23637;&#29616;&#20102;&#20248;&#36234;&#30340;&#21151;&#25928;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#20351;&#29992;&#22266;&#23450;&#30340;&#22270;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20449;&#24687;&#31232;&#32570;&#12289;&#22122;&#22768;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#22270;&#25299;&#25169;&#12289;&#29305;&#24449;&#21644;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21021;&#22987;&#36755;&#20837;&#22270;&#21487;&#33021;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#19978;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#20010;&#24615;&#21270;PageRank&#20256;&#25773;&#30697;&#38453;&#20197;&#21450;&#19979;&#28216;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#26368;&#20248;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19968;&#31181;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#20943;&#23569;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#22522;&#20934;&#26041;&#27861;&#19978;&#20855;&#26377;&#20248;&#36234;&#30340;&#21151;&#25928;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved tremendous success in a variety of real-world applications by relying on the fixed graph data as input. However, the initial input graph might not be optimal in terms of specific downstream tasks, because of information scarcity, noise, adversarial attacks, or discrepancies between the distribution in graph topology, features, and groundtruth labels. In this paper, we propose a bi-level optimization approach for learning the optimal graph structure via directly learning the Personalized PageRank propagation matrix as well as the downstream semi-supervised node classification simultaneously. We also explore a low-rank approximation model for further reducing the time complexity. Empirical evaluations show the superior efficacy and robustness of the proposed model over all baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#30340;&#22686;&#38271;&#33258;&#32452;&#32455;&#32858;&#31867;&#31639;&#27861;&#30340;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.09879</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#30340;&#25299;&#25169;&#32858;&#31867;&#33021;&#21147;&#30340;&#31867;&#21035;&#20998;&#31867;&#22120;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Class-wise Classifier Design Capable of Continual Learning using Adaptive Resonance Theory-based Topological Clustering. (arXiv:2203.09879v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#30340;&#22686;&#38271;&#33258;&#32452;&#32455;&#32858;&#31867;&#31639;&#27861;&#30340;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#65288;ART&#65289;&#30340;&#22686;&#38271;&#33258;&#32452;&#32455;&#32858;&#31867;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#29420;&#31435;&#22320;&#23558;ART&#32858;&#31867;&#31639;&#27861;&#24212;&#29992;&#20110;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#31867;&#21035;&#65292;&#29983;&#25104;&#20998;&#31867;&#22120;&#12290;&#24403;&#32473;&#23450;&#26469;&#33258;&#26032;&#31867;&#21035;&#30340;&#39069;&#22806;&#35757;&#32451;&#25968;&#25454;&#38598;&#26102;&#65292;&#23558;&#22312;&#19981;&#21516;&#30340;&#23398;&#20064;&#31354;&#38388;&#20013;&#23450;&#20041;&#19968;&#20010;&#26032;&#30340;ART&#32858;&#31867;&#12290;&#30001;&#20110;&#19978;&#36848;&#29305;&#24615;&#65292;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#20223;&#30495;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#33021;&#22815;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#32858;&#31867;&#30340;&#20998;&#31867;&#31639;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#20248;&#31168;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a supervised classification algorithm capable of continual learning by utilizing an Adaptive Resonance Theory (ART)-based growing self-organizing clustering algorithm. The ART-based clustering algorithm is theoretically capable of continual learning, and the proposed algorithm independently applies it to each class of training data for generating classifiers. Whenever an additional training data set from a new class is given, a new ART-based clustering will be defined in a different learning space. Thanks to the above-mentioned features, the proposed algorithm realizes continual learning capability. Simulation experiments showed that the proposed algorithm has superior classification performance compared with state-of-the-art clustering-based classification algorithms capable of continual learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#38480;&#21046;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#29366;&#24577;&#30340;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#24182;&#37325;&#26032;&#35299;&#37322;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#24471;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2102.11941</link><description>&lt;p&gt;
&#22686;&#24378;&#38480;&#21046;&#24615;&#24378;&#21270;&#23398;&#20064;&#65306;&#20811;&#26381;&#23398;&#20064;&#22870;&#21169;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
State Augmented Constrained Reinforcement Learning: Overcoming the Limitations of Learning with Rewards. (arXiv:2102.11941v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.11941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#38480;&#21046;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#29366;&#24577;&#30340;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#24182;&#37325;&#26032;&#35299;&#37322;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#24471;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#38480;&#21046;&#24615;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#20102;&#22810;&#20010;&#22870;&#21169;&#65292;&#36825;&#20123;&#22870;&#21169;&#24517;&#39035;&#20998;&#21035;&#32047;&#31215;&#21040;&#32473;&#23450;&#30340;&#38408;&#20540;&#12290;&#22312;&#36825;&#31867;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20363;&#23376;&#65292;&#20854;&#20013;&#26080;&#27861;&#36890;&#36807;&#20219;&#20309;&#21152;&#26435;&#32447;&#24615;&#32452;&#21512;&#30340;&#22870;&#21169;&#26469;&#35825;&#23548;&#20986;&#29702;&#24819;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#24615;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24120;&#35268;&#21270;&#21644;&#32463;&#20856;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#37117;&#26080;&#27861;&#24471;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#26412;&#25991;&#36890;&#36807;&#22686;&#21152;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#65292;&#24182;&#23558;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#37325;&#26032;&#35299;&#37322;&#20026;&#39537;&#21160;&#20056;&#23376;&#28436;&#21270;&#30340;&#21160;&#21147;&#23398;&#37096;&#20998;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#29366;&#24577;&#22686;&#24378;&#31243;&#24207;&#65292;&#33021;&#22815;&#20445;&#35777;&#35299;&#20915;&#24102;&#26377;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#27491;&#22914;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20363;&#23376;&#25152;&#31034;&#65292;&#23613;&#31649;&#20043;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#20294;&#22312;&#25191;&#34892;&#22686;&#24378;&#31574;&#30053;&#26102;&#36816;&#34892;&#23545;&#20598;&#21160;&#21147;&#23398;&#21487;&#20197;&#20174;&#20013;&#33719;&#24471;&#19968;&#31181;&#21487;&#20197;&#35777;&#26126;&#37319;&#26679;&#21160;&#20316;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common formulation of constrained reinforcement learning involves multiple rewards that must individually accumulate to given thresholds. In this class of problems, we show a simple example in which the desired optimal policy cannot be induced by any weighted linear combination of rewards. Hence, there exist constrained reinforcement learning problems for which neither regularized nor classical primal-dual methods yield optimal policies. This work addresses this shortcoming by augmenting the state with Lagrange multipliers and reinterpreting primal-dual methods as the portion of the dynamics that drives the multipliers evolution. This approach provides a systematic state augmentation procedure that is guaranteed to solve reinforcement learning problems with constraints. Thus, as we illustrate by an example, while previous methods can fail at finding optimal policies, running the dual dynamics while executing the augmented policy yields an algorithm that provably samples actions from 
&lt;/p&gt;</description></item></channel></rss>