<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#24182;&#34892;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#39044;&#27979;&#21457;&#30005;&#21378;&#20013;&#30340;&#33976;&#27773;&#36136;&#37327;&#27969;&#37327;&#65292;&#30456;&#27604;&#32431;&#32463;&#20856;&#21644;&#32431;&#37327;&#23376;&#27169;&#22411;&#65292;&#35813;&#28151;&#21512;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#24179;&#26041;&#35823;&#24046;&#38477;&#20302;&#20102;5.7&#20493;&#21644;4.9&#20493;&#65292;&#24182;&#19988;&#30456;&#23545;&#35823;&#24046;&#36739;&#23567;&#65292;&#26368;&#22810;&#25552;&#21319;&#20102;2&#20493;&#12290;</title><link>http://arxiv.org/abs/2307.09483</link><description>&lt;p&gt;
&#20351;&#29992;&#24182;&#34892;&#28151;&#21512;&#32593;&#32476;&#39044;&#27979;&#21457;&#30005;&#21378;&#20013;&#30340;&#33976;&#27773;&#36136;&#37327;&#27969;&#37327;
&lt;/p&gt;
&lt;p&gt;
Forecasting the steam mass flow in a powerplant using the parallel hybrid network. (arXiv:2307.09483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09483
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#24182;&#34892;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#39044;&#27979;&#21457;&#30005;&#21378;&#20013;&#30340;&#33976;&#27773;&#36136;&#37327;&#27969;&#37327;&#65292;&#30456;&#27604;&#32431;&#32463;&#20856;&#21644;&#32431;&#37327;&#23376;&#27169;&#22411;&#65292;&#35813;&#28151;&#21512;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#24179;&#26041;&#35823;&#24046;&#38477;&#20302;&#20102;5.7&#20493;&#21644;4.9&#20493;&#65292;&#24182;&#19988;&#30456;&#23545;&#35823;&#24046;&#36739;&#23567;&#65292;&#26368;&#22810;&#25552;&#21319;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21487;&#25345;&#32493;&#30340;&#21457;&#30005;&#26159;&#33021;&#28304;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#23588;&#20854;&#26159;&#28909;&#30005;&#21378;&#22312;&#20934;&#30830;&#39044;&#27979;&#33976;&#27773;&#36136;&#37327;&#27969;&#37327;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#65292;&#36825;&#23545;&#20110;&#36816;&#33829;&#25928;&#29575;&#21644;&#25104;&#26412;&#38477;&#20302;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24182;&#34892;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#23558;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#21644;&#20256;&#32479;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#24037;&#19994;&#29615;&#22659;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#23545;&#26410;&#26469;15&#20998;&#38047;&#20869;&#33976;&#27773;&#36136;&#37327;&#27969;&#37327;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24182;&#34892;&#28151;&#21512;&#27169;&#22411;&#20248;&#20110;&#29420;&#31435;&#30340;&#32463;&#20856;&#21644;&#37327;&#23376;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#21518;&#30340;&#27979;&#35797;&#38598;&#19978;&#30456;&#23545;&#20110;&#32431;&#32463;&#20856;&#27169;&#22411;&#21644;&#32431;&#37327;&#23376;&#32593;&#32476;&#65292;&#24179;&#22343;&#24179;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#20998;&#21035;&#38477;&#20302;&#20102;5.7&#20493;&#21644;4.9&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#28151;&#21512;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#30456;&#23545;&#35823;&#24046;&#36739;&#23567;&#65292;&#27604;&#32431;&#32463;&#20856;&#27169;&#22411;&#26356;&#22909;&#65292;&#26368;&#22810;&#25552;&#21319;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient and sustainable power generation is a crucial concern in the energy sector. In particular, thermal power plants grapple with accurately predicting steam mass flow, which is crucial for operational efficiency and cost reduction. In this study, we use a parallel hybrid neural network architecture that combines a parametrized quantum circuit and a conventional feed-forward neural network specifically designed for time-series prediction in industrial settings to enhance predictions of steam mass flow 15 minutes into the future. Our results show that the parallel hybrid model outperforms standalone classical and quantum models, achieving more than 5.7 and 4.9 times lower mean squared error (MSE) loss on the test set after training compared to pure classical and pure quantum networks, respectively. Furthermore, the hybrid model demonstrates smaller relative errors between the ground truth and the model predictions on the test set, up to 2 times better than the pure classical model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#27425;&#24615;&#20986;&#20215;&#25293;&#21334;&#20013;&#36879;&#26126;&#24230;&#23545;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#25293;&#21334;&#36879;&#26126;&#24230;&#19982;&#29615;&#22659;&#29305;&#24615;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#22312;&#25293;&#21334;&#20013;&#23398;&#20064;&#26368;&#20248;&#20986;&#20215;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.09478</link><description>&lt;p&gt;
&#36879;&#26126;&#24230;&#22312;&#26410;&#30693;&#20272;&#20540;&#30340;&#37325;&#22797;&#19968;&#27425;&#24615;&#20986;&#20215;&#25293;&#21334;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Transparency in Repeated First-Price Auctions with Unknown Valuations. (arXiv:2307.09478v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#27425;&#24615;&#20986;&#20215;&#25293;&#21334;&#20013;&#36879;&#26126;&#24230;&#23545;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#25293;&#21334;&#36879;&#26126;&#24230;&#19982;&#29615;&#22659;&#29305;&#24615;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#22312;&#25293;&#21334;&#20013;&#23398;&#20064;&#26368;&#20248;&#20986;&#20215;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#31995;&#21015;&#19968;&#27425;&#24615;&#20986;&#20215;&#25293;&#21334;&#20013;&#65292;&#21333;&#20010;&#31454;&#26631;&#20154;&#22312;&#21482;&#26377;&#22312;&#36194;&#24471;&#25293;&#21334;&#26102;&#25165;&#30693;&#36947;&#29289;&#21697;&#20215;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23436;&#25972;&#22320;&#21051;&#30011;&#20102;&#25293;&#21334;&#30340;&#36879;&#26126;&#24230;&#23545;&#26368;&#23567;&#21270;&#24863;&#21040;&#36951;&#25022;&#30340;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#36879;&#26126;&#24230;&#35843;&#25511;&#20102;&#25293;&#21334;&#24072;&#22312;&#27599;&#27425;&#25293;&#21334;&#32467;&#26463;&#26102;&#20844;&#24320;&#31454;&#20105;&#20986;&#20215;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19981;&#21516;&#20551;&#35774;&#65288;&#38543;&#26426;&#30340;&#12289;&#23545;&#25239;&#24615;&#30340;&#21644;&#24179;&#28369;&#21464;&#20307;&#65289;&#29983;&#25104;&#31454;&#26631;&#20154;&#20272;&#20540;&#21644;&#31454;&#20105;&#20986;&#20215;&#30340;&#29615;&#22659;&#12290;&#36825;&#20123;&#26497;&#23567;&#26497;&#22823;&#29575;&#25581;&#31034;&#20102;&#36879;&#26126;&#24230;&#21644;&#29615;&#22659;&#24615;&#36136;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#24433;&#21709;&#19968;&#20010;&#20154;&#23398;&#20064;&#22312;&#19968;&#27425;&#24615;&#20986;&#20215;&#25293;&#21334;&#20013;&#22914;&#20309;&#26368;&#20248;&#20986;&#20215;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of regret minimization for a single bidder in a sequence of first-price auctions where the bidder knows the item's value only if the auction is won. Our main contribution is a complete characterization, up to logarithmic factors, of the minimax regret in terms of the auction's transparency, which regulates the amount of information on competing bids disclosed by the auctioneer at the end of each auction. Our results hold under different assumptions (stochastic, adversarial, and their smoothed variants) on the environment generating the bidder's valuations and competing bids. These minimax rates reveal how the interplay between transparency and the nature of the environment affects how fast one can learn to bid optimally in first-price auctions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;&#30340;&#21457;&#23637;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65306;&#20351;&#29992;&#24207;&#21015;&#32467;&#26500;&#26469;&#34913;&#37327;&#21644;&#35745;&#31639;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20174;&#20013;&#25512;&#26029;&#30693;&#35782;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#23398;&#31185;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.09477</link><description>&lt;p&gt;
&#36827;&#23637;&#20013;&#30340;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
Towards Ordinal Data Science. (arXiv:2307.09477v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;&#30340;&#21457;&#23637;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65306;&#20351;&#29992;&#24207;&#21015;&#32467;&#26500;&#26469;&#34913;&#37327;&#21644;&#35745;&#31639;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20174;&#20013;&#25512;&#26029;&#30693;&#35782;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#23398;&#31185;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#24207;&#26159;&#34913;&#37327;&#65288;&#32463;&#39564;&#65289;&#25968;&#25454;&#20013;&#23545;&#35937;&#20043;&#38388;&#20851;&#31995;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19982;&#20351;&#29992;&#23545;&#35937;&#30340;&#25968;&#23383;&#23646;&#24615;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21457;&#23637;&#20986;&#30340;&#24207;&#21015;&#26041;&#27861;&#25968;&#37327;&#30456;&#23545;&#36739;&#23569;&#12290;&#36896;&#25104;&#36825;&#19968;&#24773;&#20917;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#22312;&#19978;&#20010;&#19990;&#32426;&#65292;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#65292;&#26080;&#27861;&#28385;&#36275;&#24207;&#21015;&#35745;&#31639;&#25152;&#38656;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#26469;&#35828;&#65292;&#21478;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;&#22522;&#20110;&#39034;&#24207;&#30340;&#26041;&#27861;&#36890;&#24120;&#34987;&#35270;&#20026;&#23545;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#36807;&#20110;&#25968;&#23398;&#20005;&#35880;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23558;&#35752;&#35770;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#21644;&#8220;&#35745;&#31639;&#8221;&#24207;&#21015;&#32467;&#26500;&#65288;&#19968;&#31867;&#29305;&#23450;&#30340;&#26377;&#21521;&#22270;&#65289;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#20174;&#20854;&#20013;&#25512;&#26029;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;&#24314;&#31435;&#20026;&#19968;&#39033;&#20840;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#12290;&#38500;&#20102;&#19982;&#20854;&#20182;&#37325;&#35201;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#30340;&#20132;&#21449;&#20114;&#34917;&#22806;&#65292;&#24191;&#27867;&#30340;&#23398;&#31185;&#39046;&#22495;&#20063;&#23558;&#21463;&#30410;&#20110;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Order is one of the main instruments to measure the relationship between objects in (empirical) data. However, compared to methods that use numerical properties of objects, the amount of ordinal methods developed is rather small. One reason for this is the limited availability of computational resources in the last century that would have been required for ordinal computations. Another reason -- particularly important for this line of research -- is that order-based methods are often seen as too mathematically rigorous for applying them to real-world data. In this paper, we will therefore discuss different means for measuring and 'calculating' with ordinal structures -- a specific class of directed graphs -- and show how to infer knowledge from them. Our aim is to establish Ordinal Data Science as a fundamentally new research agenda. Besides cross-fertilization with other cornerstone machine learning and knowledge representation methods, a broad range of disciplines will benefit from t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.09476</link><description>&lt;p&gt;
&#36807;&#24230;&#24605;&#32771;&#30495;&#30456;&#65306;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36827;&#34892;&#22797;&#26434;&#27169;&#24335;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27169;&#20223;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#37325;&#29616;&#19981;&#20934;&#30830;&#25110;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#30740;&#31350;&#26377;&#23475;&#30340;&#27169;&#20223;&#65292;&#24182;&#30830;&#23450;&#20102;&#20004;&#20010;&#30456;&#20851;&#29616;&#35937;&#65306;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#12290;&#31532;&#19968;&#20010;&#29616;&#35937;&#65292;&#36807;&#24230;&#24605;&#32771;&#65292;&#22312;&#32473;&#20986;&#27491;&#30830;&#19982;&#38169;&#35823;&#30340;&#23569;&#37327;&#31034;&#33539;&#26102;&#65292;&#25105;&#20204;&#20174;&#20013;&#38388;&#23618;&#35299;&#30721;&#39044;&#27979;&#12290;&#22312;&#26089;&#26399;&#23618;&#20013;&#65292;&#20004;&#31181;&#31034;&#33539;&#24341;&#36215;&#20102;&#30456;&#20284;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#20294;&#22312;&#26576;&#20010;&#8220;&#20851;&#38190;&#23618;&#8221;&#20043;&#21518;&#65292;&#32473;&#20986;&#38169;&#35823;&#31034;&#33539;&#30340;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#12290;&#31532;&#20108;&#20010;&#29616;&#35937;&#65292;&#38169;&#35823;&#24402;&#32435;&#22836;&#65292;&#21487;&#33021;&#26159;&#36807;&#24230;&#24605;&#32771;&#30340;&#19968;&#31181;&#26426;&#21046;&#24615;&#21407;&#22240;&#65306;&#36825;&#20123;&#26159;&#20301;&#20110;&#36739;&#26202;&#23618;&#30340;&#22836;&#37096;&#65292;&#23427;&#20204;&#20851;&#27880;&#24182;&#22797;&#21046;&#20808;&#21069;&#31034;&#33539;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20854;&#21066;&#24369;&#20250;&#20943;&#23569;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#31867;&#21035;&#65292;&#36890;&#36807;&#24102;&#26377;&#32593;&#32476;&#21487;&#20998;&#31163;&#20132;&#20114;&#30340;&#22810;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#27169;&#22411;&#65288;MZNMGs&#65289;&#26469;&#27169;&#25311;&#38750;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#23616;&#37096;&#20132;&#20114;&#32467;&#26500;&#12290;&#20316;&#32773;&#30830;&#23450;&#20102;MG&#21487;&#34987;&#34920;&#31034;&#20026;MZNMG&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#20854;Markov CCE&#38598;&#21512;&#19982;Markov NE&#38598;&#21512;&#30456;&#31561;&#65307;&#27492;&#22806;&#65292;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;MZNMG&#20013;&#25214;&#21040;&#36817;&#20284;&#30340;Markov&#31283;&#23450;CCE&#26159;PPAD&#38590;&#39064;&#65292;&#38500;&#38750;&#32593;&#32476;&#20855;&#26377;&#8220;&#26143;&#29366;&#32467;&#26500;&#8221;&#12290;</title><link>http://arxiv.org/abs/2307.09470</link><description>&lt;p&gt;
&#24102;&#26377;&#32593;&#32476;&#21487;&#20998;&#31163;&#20132;&#20114;&#30340;&#22810;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Multi-Player Zero-Sum Markov Games with Networked Separable Interactions. (arXiv:2307.09470v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#31867;&#21035;&#65292;&#36890;&#36807;&#24102;&#26377;&#32593;&#32476;&#21487;&#20998;&#31163;&#20132;&#20114;&#30340;&#22810;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#27169;&#22411;&#65288;MZNMGs&#65289;&#26469;&#27169;&#25311;&#38750;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#23616;&#37096;&#20132;&#20114;&#32467;&#26500;&#12290;&#20316;&#32773;&#30830;&#23450;&#20102;MG&#21487;&#34987;&#34920;&#31034;&#20026;MZNMG&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#20854;Markov CCE&#38598;&#21512;&#19982;Markov NE&#38598;&#21512;&#30456;&#31561;&#65307;&#27492;&#22806;&#65292;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;MZNMG&#20013;&#25214;&#21040;&#36817;&#20284;&#30340;Markov&#31283;&#23450;CCE&#26159;PPAD&#38590;&#39064;&#65292;&#38500;&#38750;&#32593;&#32476;&#20855;&#26377;&#8220;&#26143;&#29366;&#32467;&#26500;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#31867;&#21035;&#65292;&#21363;&#24102;&#26377;&#32593;&#32476;&#21487;&#20998;&#31163;&#20132;&#20114;&#30340;&#22810;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#65288;MZNMGs&#65289;&#65292;&#20197;&#27169;&#25311;&#38750;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#23616;&#37096;&#20132;&#20114;&#32467;&#26500;&#12290;&#25105;&#20204;&#23558;MZNMG&#23450;&#20041;&#20026;&#19968;&#20010;&#27169;&#22411;&#65292;&#20854;&#20013;&#19982;&#27599;&#20010;&#29366;&#24577;&#30456;&#20851;&#30340;&#36741;&#21161;&#28216;&#25103;&#30340;&#25910;&#30410;&#26159;&#38646;&#21644;&#30340;&#65292;&#24182;&#19988;&#22312;&#26576;&#20010;&#20132;&#20114;&#32593;&#32476;&#19978;&#30340;&#37051;&#23621;&#20043;&#38388;&#20855;&#26377;&#19968;&#20123;&#21487;&#20998;&#31163;&#65288;&#21363;&#32858;&#21512;&#30697;&#38453;&#65289;&#32467;&#26500;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#33021;&#22815;&#34987;&#34920;&#31034;&#20026;MZNMG&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#65292;&#39532;&#23572;&#21487;&#22827;&#31895;&#31961;&#30456;&#20851;&#22343;&#34913;&#65288;CCE&#65289;&#30340;&#38598;&#21512;&#32553;&#20943;&#20026;&#39532;&#23572;&#21487;&#22827;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#30340;&#38598;&#21512;&#65292;&#21363;&#21069;&#32773;&#23545;&#25152;&#26377;&#29609;&#23478;&#30340;&#27599;&#20010;&#29366;&#24577;&#30340;&#36793;&#38469;&#21270;&#20056;&#31215;&#32467;&#26524;&#24471;&#21040;&#21518;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;MZNMGs&#20013;&#25214;&#21040;&#36817;&#20284;&#39532;&#23572;&#21487;&#22827;\emph{&#31283;&#23450;}CCE&#26159;PPAD&#38590;&#39064;&#65292;&#38500;&#38750;&#24213;&#23618;&#32593;&#32476;&#20855;&#26377;``&#26143;&#29366;&#32467;&#26500;''&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a new class of Markov games (MGs), \textit{Multi-player Zero-sum Markov Games} with {\it Networked separable interactions} (MZNMGs), to model the local interaction structure in non-cooperative multi-agent sequential decision-making. We define an MZNMG as a model where {the payoffs of the auxiliary games associated with each state are zero-sum and} have some separable (i.e., polymatrix) structure across the neighbors over some interaction network. We first identify the necessary and sufficient conditions under which an MG can be presented as an MZNMG, and show that the set of Markov coarse correlated equilibrium (CCE) collapses to the set of Markov Nash equilibrium (NE) in these games, in that the {product of} per-state marginalization of the former for all players yields the latter. Furthermore, we show that finding approximate Markov \emph{stationary} CCE in infinite-horizon discounted MZNMGs is \texttt{PPAD}-hard, unless the underlying network has a ``star topology''. Then, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39640;&#20445;&#30495;&#24230;&#31561;&#31163;&#23376;&#20307;&#27169;&#25311;&#20013;&#30340;&#30913;&#22330;&#25299;&#25169;&#36827;&#34892;&#22270;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#22320;&#29699;&#30913;&#23618;&#27169;&#25311;&#20013;&#65292;&#26088;&#22312;&#25361;&#25112;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#36890;&#36807;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#24191;&#27867;&#28508;&#21147;&#24433;&#21709;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.09469</link><description>&lt;p&gt;
&#23545;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#39640;&#20445;&#30495;&#24230;&#31561;&#31163;&#23376;&#20307;&#27169;&#25311;&#20013;&#30913;&#22330;&#25299;&#25169;&#30340;&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Graph Representation of the Magnetic Field Topology in High-Fidelity Plasma Simulations for Machine Learning Applications. (arXiv:2307.09469v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39640;&#20445;&#30495;&#24230;&#31561;&#31163;&#23376;&#20307;&#27169;&#25311;&#20013;&#30340;&#30913;&#22330;&#25299;&#25169;&#36827;&#34892;&#22270;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#22320;&#29699;&#30913;&#23618;&#27169;&#25311;&#20013;&#65292;&#26088;&#22312;&#25361;&#25112;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#36890;&#36807;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#24191;&#27867;&#28508;&#21147;&#24433;&#21709;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27169;&#25311;&#31561;&#31163;&#23376;&#20307;&#20013;&#30340;&#30913;&#22330;&#36827;&#34892;&#25299;&#25169;&#20998;&#26512;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#19979;&#30740;&#31350;&#22810;&#31181;&#29289;&#29702;&#29616;&#35937;&#12290;&#20854;&#20013;&#19968;&#20010;&#24212;&#29992;&#26159;&#30913;&#37325;&#26032;&#36830;&#25509;&#65292;&#36825;&#26159;&#19982;&#30913;&#22330;&#25299;&#25169;&#21160;&#21147;&#23398;&#30456;&#20851;&#30340;&#19968;&#31181;&#29616;&#35937;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#24456;&#38590;&#26816;&#27979;&#21644;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#23545;&#19977;&#32500;&#30913;&#22330;&#30690;&#37327;&#22330;&#36827;&#34892;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#21644;&#26102;&#31354;&#22270;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#22320;&#29699;&#30913;&#23618;&#30340;&#27169;&#25311;&#20013;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#30001;Vlasiator&#20135;&#29983;&#30340;&#65292;Vlasiator&#26159;&#19968;&#20010;&#22522;&#20110;Vlasov&#29702;&#35770;&#30340;&#36229;&#32423;&#35745;&#31639;&#26426;&#35268;&#27169;&#30340;&#36817;&#22320;&#31354;&#38388;&#27169;&#25311;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#30340;&#26159;&#25361;&#25112;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#37319;&#29992;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#19968;&#20010;&#22312;&#31185;&#23398;&#19978;&#20855;&#26377;&#24191;&#27867;&#28508;&#21147;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological analysis of the magnetic field in simulated plasmas allows the study of various physical phenomena in a wide range of settings. One such application is magnetic reconnection, a phenomenon related to the dynamics of the magnetic field topology, which is difficult to detect and characterize in three dimensions. We propose a scalable pipeline for topological data analysis and spatiotemporal graph representation of three-dimensional magnetic vector fields. We demonstrate our methods on simulations of the Earth's magnetosphere produced by Vlasiator, a supercomputer-scale Vlasov theory-based simulation for near-Earth space. The purpose of this work is to challenge the machine learning community to explore graph-based machine learning approaches to address a largely open scientific problem with wide-ranging potential impact.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25253;&#21578;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23384;&#35745;&#31639;&#30340;&#31070;&#32463;&#35299;&#30721;&#22120;&#25512;&#29702;&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#21644;&#24615;&#33021;&#20998;&#26512;&#65292;&#35813;&#35299;&#30721;&#22120;&#29992;&#20110;&#23481;&#38169;&#37327;&#23376;&#38169;&#35823;&#32416;&#27491;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#35299;&#30721;&#26102;&#38388;&#24182;&#30830;&#20445;&#35299;&#30721;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09463</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23481;&#38169;&#37327;&#23376;&#38169;&#35823;&#32416;&#27491;&#30340;&#20302;&#28201;&#23384;&#20648;&#25935;&#21270;&#31070;&#32463;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction. (arXiv:2307.09463v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25253;&#21578;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23384;&#35745;&#31639;&#30340;&#31070;&#32463;&#35299;&#30721;&#22120;&#25512;&#29702;&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#21644;&#24615;&#33021;&#20998;&#26512;&#65292;&#35813;&#35299;&#30721;&#22120;&#29992;&#20110;&#23481;&#38169;&#37327;&#23376;&#38169;&#35823;&#32416;&#27491;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#35299;&#30721;&#26102;&#38388;&#24182;&#30830;&#20445;&#35299;&#30721;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#38169;&#35823;&#32416;&#27491;(QEC)&#30340;&#31070;&#32463;&#35299;&#30721;&#22120;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#26469;&#20998;&#31867;&#20174;&#38169;&#35823;&#32416;&#27491;&#32534;&#30721;&#20013;&#25552;&#21462;&#30340;&#32508;&#21512;&#24449;&#65292;&#24182;&#25214;&#21040;&#21512;&#36866;&#30340;&#24674;&#22797;&#25805;&#20316;&#21592;&#26469;&#20445;&#25252;&#36923;&#36753;&#20449;&#24687;&#20813;&#21463;&#38169;&#35823;&#24433;&#21709;&#12290;&#23613;&#31649;&#31070;&#32463;&#35299;&#30721;&#22120;&#24615;&#33021;&#33391;&#22909;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#30340;&#23454;&#38469;&#35201;&#27714;&#65292;&#22914;&#23558;&#35299;&#30721;&#26102;&#38388;&#26368;&#23567;&#21270;&#20197;&#28385;&#36275;&#37325;&#22797;&#38169;&#35823;&#32416;&#27491;&#26041;&#26696;&#20013;&#30340;&#32508;&#21512;&#24449;&#29983;&#25104;&#36895;&#24230;&#65292;&#20197;&#21450;&#30830;&#20445;&#35299;&#30721;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#38543;&#30528;&#32534;&#30721;&#36317;&#31163;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#35774;&#35745;&#19968;&#20010;&#19987;&#29992;&#30340;&#38598;&#25104;&#30005;&#36335;&#20197;&#19982;&#37327;&#23376;&#22788;&#29702;&#22120;&#20849;&#21516;&#23436;&#25104;&#35299;&#30721;&#20219;&#21153;&#20284;&#20046;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#23558;&#20449;&#21495;&#20174;&#20302;&#28201;&#29615;&#22659;&#20013;&#24341;&#20986;&#24182;&#36827;&#34892;&#22806;&#37096;&#22788;&#29702;&#20250;&#23548;&#33268;&#19981;&#24517;&#35201;&#30340;&#24310;&#36831;&#21644;&#26368;&#32456;&#30340;&#24067;&#32447;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23384;&#35745;&#31639;&#30340;&#31070;&#32463;&#35299;&#30721;&#22120;&#25512;&#29702;&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#21644;&#24615;&#33021;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural decoders for quantum error correction (QEC) rely on neural networks to classify syndromes extracted from error correction codes and find appropriate recovery operators to protect logical information against errors. Despite the good performance of neural decoders, important practical requirements remain to be achieved, such as minimizing the decoding time to meet typical rates of syndrome generation in repeated error correction schemes, and ensuring the scalability of the decoding approach as the code distance increases. Designing a dedicated integrated circuit to perform the decoding task in co-integration with a quantum processor appears necessary to reach these decoding time and scalability requirements, as routing signals in and out of a cryogenic environment to be processed externally leads to unnecessary delays and an eventual wiring bottleneck. In this work, we report the design and performance analysis of a neural decoder inference accelerator based on an in-memory comput
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#36335;&#20998;&#26512;&#22312;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#23545;70B&#27611;&#20011;&#40736;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#36923;&#36753;&#23618;&#24402;&#22240;&#21644;&#28608;&#27963;&#20462;&#34917;&#25216;&#26415;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.09458</link><description>&lt;p&gt;
&#30005;&#36335;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#21542;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65311;&#26469;&#33258;&#27611;&#20011;&#40736;&#20013;&#22810;&#39033;&#36873;&#25321;&#33021;&#21147;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla. (arXiv:2307.09458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#36335;&#20998;&#26512;&#22312;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#23545;70B&#27611;&#20011;&#40736;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#36923;&#36753;&#23618;&#24402;&#22240;&#21644;&#28608;&#27963;&#20462;&#34917;&#25216;&#26415;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#36335;&#20998;&#26512;&#26159;&#19968;&#31181;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#37117;&#26159;&#22312;&#36828;&#31163;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#23567;&#22411;&#27169;&#22411;&#20013;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;70B&#27611;&#20011;&#40736;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#26088;&#22312;&#27979;&#35797;&#30005;&#36335;&#20998;&#26512;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#35843;&#26597;&#20102;&#27611;&#20011;&#40736;&#22312;&#30693;&#36947;&#27491;&#30830;&#31572;&#26696;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#33021;&#22815;&#35782;&#21035;&#20986;&#27491;&#30830;&#31572;&#26696;&#26631;&#31614;&#12290;&#25105;&#20204;&#21457;&#29616;&#24050;&#26377;&#30340;&#36923;&#36753;&#23618;&#24402;&#22240;&#12289;&#27880;&#24847;&#21147;&#27169;&#24335;&#21487;&#35270;&#21270;&#21644;&#28608;&#27963;&#20462;&#34917;&#25216;&#26415;&#22312;&#27611;&#20011;&#40736;&#27169;&#22411;&#20013;&#20855;&#26377;&#33258;&#28982;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#21644;&#20998;&#31867;&#19968;&#23567;&#32452;&#8220;&#36755;&#20986;&#33410;&#28857;&#8221;&#65288;&#27880;&#24847;&#21147;&#22836;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#8220;&#27491;&#30830;&#23383;&#27597;&#8221;&#31867;&#21035;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#29305;&#24449;&#30340;&#35821;&#20041;&#65292;&#32467;&#26524;&#26377;&#25152;&#19981;&#21516;&#12290;&#23545;&#20110;&#27491;&#24120;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#25105;&#20204;&#26174;&#33879;&#21387;&#32553;&#20102;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
\emph{Circuit analysis} is a promising technique for understanding the internal mechanisms of language models. However, existing analyses are done in small models far from the state of the art. To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis. In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer \emph{label} given knowledge of the correct answer \emph{text}. We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs).  We further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results. For normal multiple-choice question answers, we significantly compress the query, ke
&lt;/p&gt;</description></item><item><title>&#20809;&#28369;&#27880;&#24847;&#21147;&#28145;&#24230;&#22810;&#31034;&#20363;&#23398;&#20064;&#27169;&#22411;&#65288;SA-DMIL&#65289;&#29992;&#20110;CT&#39045;&#20869;&#20986;&#34880;&#26816;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#24179;&#28369;&#24615;&#32422;&#26463;&#26469;&#23398;&#20064;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#21462;&#24471;&#20102;&#27604;&#20256;&#32479;MIL&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09457</link><description>&lt;p&gt;
&#20809;&#28369;&#27880;&#24847;&#21147;&#29992;&#20110;&#28145;&#24230;&#22810;&#31034;&#20363;&#23398;&#20064;&#65306;&#24212;&#29992;&#20110;CT&#39045;&#20869;&#20986;&#34880;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection. (arXiv:2307.09457v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09457
&lt;/p&gt;
&lt;p&gt;
&#20809;&#28369;&#27880;&#24847;&#21147;&#28145;&#24230;&#22810;&#31034;&#20363;&#23398;&#20064;&#27169;&#22411;&#65288;SA-DMIL&#65289;&#29992;&#20110;CT&#39045;&#20869;&#20986;&#34880;&#26816;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#24179;&#28369;&#24615;&#32422;&#26463;&#26469;&#23398;&#20064;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#21462;&#24471;&#20102;&#27604;&#20256;&#32479;MIL&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31034;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#23398;&#25104;&#20687;&#35786;&#26029;&#65292;&#22312;&#20854;&#20013;&#21253;&#34955;&#26631;&#31614;&#24050;&#30693;&#19988;&#21253;&#34955;&#20869;&#30340;&#23454;&#20363;&#26631;&#31614;&#26410;&#30693;&#12290;&#20256;&#32479;MIL&#20551;&#35774;&#27599;&#20010;&#21253;&#34955;&#20869;&#30340;&#23454;&#20363;&#26159;&#26469;&#33258;&#32473;&#23450;&#20998;&#24067;&#30340;&#29420;&#31435;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#23454;&#20363;&#36890;&#24120;&#26159;&#31354;&#38388;&#19978;&#25110;&#39034;&#24207;&#19978;&#26377;&#24207;&#30340;&#65292;&#24182;&#19988;&#20154;&#20204;&#20250;&#26399;&#26395;&#30456;&#37051;&#23454;&#20363;&#20855;&#26377;&#30456;&#20284;&#30340;&#35786;&#26029;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#28369;&#27880;&#24847;&#21147;&#28145;&#24230;MIL&#65288;SA-DMIL&#65289;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#32534;&#30721;&#23545;&#27599;&#20010;&#21253;&#34955;&#20013;&#27599;&#20010;&#23454;&#20363;&#30340;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#20989;&#25968;&#24341;&#20837;&#19968;&#38454;&#21644;&#20108;&#38454;&#32422;&#26463;&#26469;&#23454;&#29616;&#24179;&#28369;&#24615;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#22836;&#37096;CT&#25195;&#25551;&#20013;&#39045;&#20869;&#20986;&#34880;&#65288;ICH&#65289;&#30340;&#26816;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;SA-DMIL&#27169;&#22411;&#65306;&#65288;a&#65289;&#22312;&#25195;&#25551;&#65288;&#21253;&#34955;&#65289;&#21644;&#20999;&#29255;&#65288;&#23454;&#20363;&#65289;&#23618;&#38754;&#19978;&#27604;&#38750;&#20809;&#28369;&#27880;&#24847;&#21147;MIL&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65307;&#65288;b&#65289;&#23398;&#20064;&#20102;&#20999;&#29255;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65307;&#24182;&#19988;&#65288;c&#65289;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;MIL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning (MIL) has been widely applied to medical imaging diagnosis, where bag labels are known and instance labels inside bags are unknown. Traditional MIL assumes that instances in each bag are independent samples from a given distribution. However, instances are often spatially or sequentially ordered, and one would expect similar diagnostic importance for neighboring instances. To address this, in this study, we propose a smooth attention deep MIL (SA-DMIL) model. Smoothness is achieved by the introduction of first and second order constraints on the latent function encoding the attention paid to each instance in a bag. The method is applied to the detection of intracranial hemorrhage (ICH) on head CT scans. The results show that this novel SA-DMIL: (a) achieves better performance than the non-smooth attention MIL at both scan (bag) and slice (instance) levels; (b) learns spatial dependencies between slices; and (c) outperforms current state-of-the-art MIL methods
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#36870;&#38382;&#39064;&#20013;&#30340;&#32463;&#20856;&#27491;&#21017;&#21270;&#29702;&#35770;&#21644;&#36817;&#26399;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20197;&#21450;&#23545;PnP&#31639;&#27861;&#21450;&#20854;&#25910;&#25947;&#24615;&#30340;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.09441</link><description>&lt;p&gt;
&#36870;&#38382;&#39064;&#21644;&#32447;&#24615;&#25554;&#25300;&#21435;&#22122;&#22120;&#20013;&#30340;&#25910;&#25947;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Convergent regularization in inverse problems and linear plug-and-play denoisers. (arXiv:2307.09441v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#36870;&#38382;&#39064;&#20013;&#30340;&#32463;&#20856;&#27491;&#21017;&#21270;&#29702;&#35770;&#21644;&#36817;&#26399;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20197;&#21450;&#23545;PnP&#31639;&#27861;&#21450;&#20854;&#25910;&#25947;&#24615;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25554;&#25300;&#65288;PnP&#65289;&#21435;&#22122;&#26159;&#19968;&#31181;&#20351;&#29992;&#29616;&#25104;&#22270;&#20687;&#21435;&#22122;&#22120;&#27714;&#35299;&#22270;&#20687;&#36870;&#38382;&#39064;&#30340;&#27969;&#34892;&#36845;&#20195;&#26694;&#26550;&#12290;&#20182;&#20204;&#30340;&#23454;&#35777;&#25104;&#21151;&#25512;&#21160;&#20102;&#19968;&#31995;&#21015;&#30740;&#31350;&#65292;&#26088;&#22312;&#20102;&#35299;&#22312;&#23545;&#21435;&#22122;&#22120;&#36827;&#34892;&#21508;&#31181;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;PnP&#36845;&#20195;&#30340;&#25910;&#25947;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#26469;&#24314;&#31435;PnP&#36845;&#20195;&#22312;&#21435;&#22122;&#22120;&#30340;&#19981;&#21516;&#27491;&#21017;&#26465;&#20214;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#20851;&#20110;&#25910;&#25947;&#35299;&#22312;&#27979;&#37327;&#22122;&#22768;&#27700;&#24179;&#36235;&#36817;&#20110;&#38646;&#26102;&#30340;&#28176;&#36817;&#24615;&#36136;&#65292;&#21363;PnP&#26041;&#27861;&#22312;&#23545;&#21435;&#22122;&#22120;&#36827;&#34892;&#21512;&#29702;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#21487;&#20197;&#34987;&#35777;&#26126;&#20026;&#25910;&#25947;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#30446;&#21069;&#25152;&#30693;&#29978;&#23569;&#12290;&#26412;&#25991;&#26377;&#20004;&#20010;&#30446;&#30340;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#36870;&#38382;&#39064;&#20013;&#30340;&#32463;&#20856;&#27491;&#21017;&#21270;&#29702;&#35770;&#65292;&#24182;&#32508;&#36848;&#20102;&#19968;&#20123;&#36817;&#26399;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#34987;&#35777;&#26126;&#20026;&#25910;&#25947;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32487;&#32493;&#35752;&#35770;PnP&#31639;&#27861;&#21450;&#20854;&#24050;&#24314;&#31435;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plug-and-play (PnP) denoising is a popular iterative framework for solving imaging inverse problems using off-the-shelf image denoisers. Their empirical success has motivated a line of research that seeks to understand the convergence of PnP iterates under various assumptions on the denoiser. While a significant amount of research has gone into establishing the convergence of the PnP iteration for different regularity conditions on the denoisers, not much is known about the asymptotic properties of the converged solution as the noise level in the measurement tends to zero, i.e., whether PnP methods are provably convergent regularization schemes under reasonable assumptions on the denoiser. This paper serves two purposes: first, we provide an overview of the classical regularization theory in inverse problems and survey a few notable recent data-driven methods that are provably convergent regularization schemes. We then continue to discuss PnP algorithms and their established convergenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#24615;&#27133;&#23383;&#20856;&#65288;PSD&#65289;&#23454;&#29616;&#20102;&#19987;&#38376;&#30340;&#27133;&#20301;&#32465;&#23450;&#21644;&#29289;&#20307;&#23618;&#27425;&#35843;&#33410;&#20998;&#24067;&#65292;&#22312;&#22810;&#20010;&#21518;&#32493;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.09437</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#29992;&#20110;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Conditional Slot Attention for Object Centric Learning. (arXiv:2307.09437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#24615;&#27133;&#23383;&#20856;&#65288;PSD&#65289;&#23454;&#29616;&#20102;&#19987;&#38376;&#30340;&#27133;&#20301;&#32465;&#23450;&#21644;&#29289;&#20307;&#23618;&#27425;&#35843;&#33410;&#20998;&#24067;&#65292;&#22312;&#22810;&#20010;&#21518;&#32493;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21462;&#29289;&#20307;&#23618;&#27425;&#30340;&#34920;&#31034;&#20197;&#36827;&#34892;&#21518;&#32493;&#30340;&#25512;&#29702;&#20219;&#21153;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#28044;&#29616;&#30340;&#19968;&#20010;&#39046;&#22495;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#35774;&#32622;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23558;&#20219;&#24847;&#25968;&#37327;&#30340;&#29289;&#20307;&#23454;&#20363;&#32465;&#23450;&#21040;&#19987;&#38376;&#30340;&#29289;&#20307;&#27133;&#20301;&#12290;&#26368;&#36817;&#30340;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#26041;&#27861;&#22914;&#27133;&#20301;&#27880;&#24847;&#21147;&#21033;&#29992;&#36845;&#20195;&#24335;&#27880;&#24847;&#21147;&#23398;&#20064;&#20855;&#26377;&#21160;&#24577;&#25512;&#29702;&#23618;&#32423;&#32465;&#23450;&#30340;&#21487;&#32452;&#21512;&#34920;&#31034;&#65292;&#20294;&#26410;&#33021;&#36798;&#21040;&#19987;&#38376;&#30340;&#27133;&#20301;&#32465;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26032;&#39062;&#30340;&#27010;&#29575;&#24615;&#27133;&#23383;&#20856;&#65288;PSD&#65289;&#30340;&#26080;&#30417;&#30563;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#23558;PSD&#23450;&#20041;&#20026;&#65288;i&#65289;&#25277;&#35937;&#30340;&#29289;&#20307;&#23618;&#27425;&#23646;&#24615;&#21521;&#37327;&#20316;&#20026;&#38190;&#65292;&#65288;ii&#65289;&#21442;&#25968;&#21270;&#39640;&#26031;&#20998;&#24067;&#20316;&#20026;&#30456;&#24212;&#30340;&#20540;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#21518;&#32493;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#20855;&#20307;&#29289;&#20307;&#23618;&#27425;&#35843;&#33410;&#20998;&#24067;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#29289;&#20307;&#21457;&#29616;&#12289;&#32452;&#21512;&#24335;&#22330;&#26223;&#29983;&#25104;&#21644;&#32452;&#21512;&#24335;&#35270;&#35273;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting object-level representations for downstream reasoning tasks is an emerging area in AI. Learning object-centric representations in an unsupervised setting presents multiple challenges, a key one being binding an arbitrary number of object instances to a specialized object slot. Recent object-centric representation methods like Slot Attention utilize iterative attention to learn composable representations with dynamic inference level binding but fail to achieve specialized slot level binding. To address this, in this paper we propose Unsupervised Conditional Slot Attention using a novel Probabilistic Slot Dictionary (PSD). We define PSD with (i) abstract object-level property vectors as key and (ii) parametric Gaussian distribution as its corresponding value. We demonstrate the benefits of the learnt specific object-level conditioning distributions in multiple downstream tasks, namely object discovery, compositional scene generation, and compositional visual reasoning. We show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;NetHack&#28216;&#25103;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#21457;&#29616;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#21487;&#20197;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#24314;&#31435;&#20102;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;IL&#20195;&#29702;&#20154;&#30340;&#24130;&#24459;&#12290;</title><link>http://arxiv.org/abs/2307.09423</link><description>&lt;p&gt;
&#22312;NetHack&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#30340;&#35268;&#27169;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Imitation Learning in NetHack. (arXiv:2307.09423v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;NetHack&#28216;&#25103;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#21457;&#29616;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#21487;&#20197;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#24314;&#31435;&#20102;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;IL&#20195;&#29702;&#20154;&#30340;&#24130;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064; (IL) &#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#24378;&#22823;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#21457;&#29616;&#23427;&#24448;&#24448;&#19981;&#33021;&#23436;&#20840;&#24674;&#22797;&#20986;&#28508;&#22312;&#30340;&#19987;&#23478;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#27809;&#26377;&#28145;&#20837;&#25506;&#31350;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#25193;&#22823;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#21463;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#39046;&#22495;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#22312;&#37027;&#37324;&#8220;&#25193;&#22823;&#35268;&#27169;&#8221;&#24050;&#32463;&#23548;&#33268;&#20102;&#36234;&#26469;&#36234;&#26377;&#33021;&#21147;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411; (LLMs)&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20180;&#32454;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#26159;&#21542;&#21487;&#20197;&#22312;&#27169;&#20223;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#24102;&#26469;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312; NetHack &#28216;&#25103;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#31243;&#24207;&#29983;&#25104;&#12289;&#38543;&#26426;&#24615;&#12289;&#38271;&#26399;&#20381;&#36182;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#21457;&#29616; IL &#30340;&#25439;&#22833;&#21644;&#24179;&#22343;&#22238;&#25253;&#38543;&#30528;&#35745;&#31639;&#39044;&#31639;&#30340;&#21464;&#21270;&#32780;&#24179;&#28369;&#21464;&#21270;&#19988;&#24378;&#30456;&#20851;&#65292;&#20174;&#32780;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#26679;&#26412;&#25968;&#37327;&#26041;&#38754;&#20026;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;&#30340; IL &#20195;&#29702;&#20154;&#30340;&#35745;&#31639;&#39044;&#31639;&#24314;&#31435;&#20102;&#24130;&#24459;&#12290;&#25105;&#20204;&#39044;&#27979;&#24182;&#35757;&#32451;&#20102;&#20960;&#20010;&#20855;&#26377; IL &#30340;NetHack&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, while powerful, many works find it is often not able to fully recover the underlying expert behavior. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where "scaling up" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting. To demonstrate our findings, we focus on the game of NetHack, a challenging environment featuring procedural generation, stochasticity, long-term dependencies, and partial observability. We find IL loss and mean return scale smoothly with the compute budget and are strongly correlated, resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples. We forecast and train several NetHack agents with IL an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25910;&#38598;&#26377;&#30410;&#20449;&#24687;&#26159;&#26114;&#36149;&#30340;&#12290;&#30740;&#31350;&#25193;&#23637;&#20102;&#19978;&#19979;&#25991;&#20048;&#35266;&#35774;&#32622;&#65292;&#20801;&#35768;&#20195;&#29702;&#35266;&#23519;&#29305;&#24449;&#29366;&#24577;&#30340;&#23376;&#38598;&#65292;&#20197;&#22312;&#26368;&#23567;&#21270;&#20449;&#24687;&#25104;&#26412;&#21644;&#26368;&#22823;&#21270;&#25910;&#30410;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2307.09388</link><description>&lt;p&gt;
&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#20855;&#26377;&#39640;&#25104;&#26412;&#29305;&#24449;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning with Costly Features in Non-stationary Environments. (arXiv:2307.09388v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25910;&#38598;&#26377;&#30410;&#20449;&#24687;&#26159;&#26114;&#36149;&#30340;&#12290;&#30740;&#31350;&#25193;&#23637;&#20102;&#19978;&#19979;&#25991;&#20048;&#35266;&#35774;&#32622;&#65292;&#20801;&#35768;&#20195;&#29702;&#35266;&#23519;&#29305;&#24449;&#29366;&#24577;&#30340;&#23376;&#38598;&#65292;&#20197;&#22312;&#26368;&#23567;&#21270;&#20449;&#24687;&#25104;&#26412;&#21644;&#26368;&#22823;&#21270;&#25910;&#30410;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#26368;&#22823;&#21270;&#38271;&#26399;&#22238;&#25253;&#26159;&#20027;&#35201;&#30446;&#26631;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#38468;&#21152;&#20449;&#24687;&#26159;&#20813;&#36153;&#25552;&#20379;&#30340;&#65292;&#20351;&#24471;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#22312;&#20570;&#20915;&#31574;&#20043;&#21069;&#35266;&#23519;&#21040;&#25152;&#26377;&#29305;&#24449;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#20013;&#65292;&#25910;&#38598;&#26377;&#30410;&#20449;&#24687;&#24448;&#24448;&#26159;&#26114;&#36149;&#30340;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#38500;&#20102;&#20010;&#20307;&#22870;&#21169;&#20043;&#22806;&#65292;&#23398;&#20064;&#29305;&#24449;&#29366;&#24577;&#30340;&#35266;&#23519;&#23545;&#20110;&#25913;&#21892;&#20915;&#31574;&#31574;&#30053;&#20063;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#65292;&#22870;&#21169;&#21644;&#25104;&#26412;&#20998;&#24067;&#38543;&#26102;&#38388;&#21457;&#29983;&#31361;&#21464;&#65292;&#36825;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#21452;&#37325;&#23398;&#20064;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#21452;&#37325;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19978;&#19979;&#25991;&#20048;&#35266;&#35774;&#32622;&#65292;&#24182;&#20801;&#35768;&#20195;&#29702;&#35266;&#23519;&#29305;&#24449;&#29366;&#24577;&#30340;&#23376;&#38598;&#12290;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#38271;&#26399;&#24179;&#22343;&#25910;&#30410;&#65292;&#21363;&#24179;&#22343;&#32047;&#31215;&#22238;&#25253;&#21644;&#24179;&#22343;&#25903;&#20184;&#25104;&#26412;&#20043;&#24046;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#38754;&#20020;&#30528;&#22312;&#26368;&#23567;&#21270;&#20449;&#24687;&#25104;&#26412;&#21644;&#26368;&#22823;&#21270;&#25910;&#30410;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximizing long-term rewards is the primary goal in sequential decision-making problems. The majority of existing methods assume that side information is freely available, enabling the learning agent to observe all features' states before making a decision. In real-world problems, however, collecting beneficial information is often costly. That implies that, besides individual arms' reward, learning the observations of the features' states is essential to improve the decision-making strategy. The problem is aggravated in a non-stationary environment where reward and cost distributions undergo abrupt changes over time. To address the aforementioned dual learning problem, we extend the contextual bandit setting and allow the agent to observe subsets of features' states. The objective is to maximize the long-term average gain, which is the difference between the accumulated rewards and the paid costs on average. Therefore, the agent faces a trade-off between minimizing the cost of informa
&lt;/p&gt;</description></item><item><title>&#25209;&#37327;&#39044;&#27979;&#22120;&#25552;&#20379;&#20102;&#25351;&#25968;&#32423;&#26356;&#24378;&#30340;&#27867;&#21270;&#20445;&#35777;&#65292;&#21487;&#24212;&#29992;&#20110;&#31163;&#32447;&#27979;&#35797;&#21069;&#21270;&#21512;&#29289;&#36136;&#37327;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.09379</link><description>&lt;p&gt;
&#25209;&#37327;&#39044;&#27979;&#22120;&#22312;&#20998;&#24067;&#20869;&#20855;&#26377;&#24191;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batched Predictors Generalize within Distribution. (arXiv:2307.09379v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09379
&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#39044;&#27979;&#22120;&#25552;&#20379;&#20102;&#25351;&#25968;&#32423;&#26356;&#24378;&#30340;&#27867;&#21270;&#20445;&#35777;&#65292;&#21487;&#24212;&#29992;&#20110;&#31163;&#32447;&#27979;&#35797;&#21069;&#21270;&#21512;&#29289;&#36136;&#37327;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25209;&#37327;&#39044;&#27979;&#22120;&#30340;&#24191;&#20041;&#24615;&#36136;&#65292;&#21363;&#20219;&#21153;&#26159;&#39044;&#27979;&#19968;&#23567;&#32452;&#65288;&#25110;&#25209;&#37327;&#65289;&#31034;&#20363;&#30340;&#22343;&#20540;&#26631;&#31614;&#30340;&#27169;&#22411;&#12290;&#25209;&#37327;&#39044;&#27979;&#33539;&#24335;&#23545;&#20110;&#37096;&#32626;&#22312;&#31163;&#32447;&#27979;&#35797;&#21069;&#30830;&#23450;&#19968;&#32452;&#21270;&#21512;&#29289;&#30340;&#36136;&#37327;&#30340;&#27169;&#22411;&#23588;&#20026;&#30456;&#20851;&#12290;&#36890;&#36807;&#21033;&#29992;&#36866;&#24403;&#30340;Rademacher&#22797;&#26434;&#24615;&#30340;&#24191;&#20041;&#21270;&#65292;&#25105;&#20204;&#35777;&#26126;&#25209;&#37327;&#39044;&#27979;&#22120;&#20855;&#26377;&#25351;&#25968;&#32423;&#26356;&#24378;&#30340;&#27867;&#21270;&#20445;&#35777;&#65292;&#19982;&#26631;&#20934;&#30340;&#36880;&#20010;&#26679;&#26412;&#26041;&#27861;&#30456;&#27604;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#35813;&#25552;&#35758;&#30340;&#19978;&#30028;&#29420;&#31435;&#20110;&#36807;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#27934;&#23519;&#21147;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#26550;&#26500;&#21644;&#24212;&#29992;&#20013;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the generalization properties of batched predictors, i.e., models tasked with predicting the mean label of a small set (or batch) of examples. The batched prediction paradigm is particularly relevant for models deployed to determine the quality of a group of compounds in preparation for offline testing. By utilizing a suitable generalization of the Rademacher complexity, we prove that batched predictors come with exponentially stronger generalization guarantees as compared to the standard per-sample approach. Surprisingly, the proposed bound holds independently of overparametrization. Our theoretical insights are validated experimentally for various tasks, architectures, and applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20132;&#26131;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#20449;&#21495;&#36827;&#34892;&#20132;&#26131;&#65292;&#20197;&#24212;&#23545;&#22312;&#20132;&#26131;&#36739;&#23569;&#30340;&#37329;&#34701;&#24066;&#22330;&#21644;&#19981;&#21516;&#31867;&#36164;&#20135;&#24066;&#22330;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.09377</link><description>&lt;p&gt;
&#25968;&#25454;&#20132;&#21449;&#20998;&#21106;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20132;&#26131;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Data Cross-Segmentation for Improved Generalization in Reinforcement Learning Based Algorithmic Trading. (arXiv:2307.09377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20132;&#26131;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#20449;&#21495;&#36827;&#34892;&#20132;&#26131;&#65292;&#20197;&#24212;&#23545;&#22312;&#20132;&#26131;&#36739;&#23569;&#30340;&#37329;&#34701;&#24066;&#22330;&#21644;&#19981;&#21516;&#31867;&#36164;&#20135;&#24066;&#22330;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31639;&#27861;&#20132;&#26131;&#31995;&#32479;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#22312;&#19968;&#20010;&#20856;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26469;&#39044;&#27979;&#36164;&#20135;&#30340;&#26410;&#26469;&#20215;&#26684;&#65292;&#36825;&#20123;&#39044;&#27979;&#39537;&#21160;&#30528;&#31616;&#21333;&#30340;&#20132;&#26131;&#21644;&#25191;&#34892;&#31574;&#30053;&#12290;&#24403;&#39044;&#27979;&#20855;&#26377;&#36275;&#22815;&#30340;&#20449;&#21495;&#12289;&#24066;&#22330;&#27969;&#21160;&#24615;&#36275;&#22815;&#22823;&#19988;&#20132;&#26131;&#25104;&#26412;&#36739;&#20302;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#20132;&#26131;&#36739;&#23569;&#30340;&#37329;&#34701;&#24066;&#22330;&#21644;&#25151;&#22320;&#20135;&#25110;&#27773;&#36710;&#31561;&#19981;&#21516;&#31867;&#36164;&#20135;&#24066;&#22330;&#20013;&#65292;&#36825;&#20123;&#26465;&#20214;&#36890;&#24120;&#19981;&#25104;&#31435;&#12290;&#22312;&#36825;&#20123;&#24066;&#22330;&#20013;&#65292;&#20132;&#26131;&#31574;&#30053;&#24517;&#39035;&#32771;&#34385;&#21040;&#37319;&#21462;&#30456;&#23545;&#36739;&#38590;&#25913;&#21464;&#30340;&#20179;&#20301;&#23545;&#38271;&#26399;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20132;&#26131;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26681;&#25454;&#23398;&#20064;&#21040;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#20449;&#21495;&#36827;&#34892;&#20132;&#26131;&#65292;&#24182;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#39532;&#26469;&#35199;&#20122;&#20132;&#26131;&#25152;&#30340;20&#22810;&#24180;&#32929;&#31080;&#25968;&#25454;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of machine learning in algorithmic trading systems is increasingly common. In a typical set-up, supervised learning is used to predict the future prices of assets, and those predictions drive a simple trading and execution strategy. This is quite effective when the predictions have sufficient signal, markets are liquid, and transaction costs are low. However, those conditions often do not hold in thinly traded financial markets and markets for differentiated assets such as real estate or vehicles. In these markets, the trading strategy must consider the long-term effects of taking positions that are relatively more difficult to change. In this work, we propose a Reinforcement Learning (RL) algorithm that trades based on signals from a learned predictive model and addresses these challenges. We test our algorithm on 20+ years of equity data from Bursa Malaysia.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30697;&#38453;&#34920;&#36848;&#26469;&#35299;&#20915;&#22810;&#31867;&#21035;&#21644;&#22810;&#26631;&#31614;&#35774;&#32622;&#19979;&#30340;&#28789;&#27963;&#24615;&#21644;&#25972;&#21512;&#39069;&#22806;&#39033;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#22312;&#23545;&#20598;&#38382;&#39064;&#20013;&#37319;&#29992;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#30697;&#38453;SVM&#30340;&#35299;&#20915;&#25928;&#29575;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#26102;&#38388;&#25928;&#29575;&#21644;&#24615;&#33021;&#19978;&#19982;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#30456;&#20851;&#24615;SVM&#26041;&#27861;&#30456;&#24403;&#12290;&#36825;&#31181;&#30697;&#38453;&#34920;&#36848;&#25581;&#31034;&#20986;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#38590;&#20197;&#23519;&#35273;&#30340;&#20851;&#38190;&#27934;&#35265;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.09372</link><description>&lt;p&gt;
&#36890;&#36807;&#30697;&#38453;&#34920;&#36848;&#22686;&#24378;&#25903;&#25345;&#21521;&#37327;&#26426;&#20013;&#30340;&#27169;&#24335;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Enhancing Pattern Classification in Support Vector Machines through Matrix Formulation. (arXiv:2307.09372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30697;&#38453;&#34920;&#36848;&#26469;&#35299;&#20915;&#22810;&#31867;&#21035;&#21644;&#22810;&#26631;&#31614;&#35774;&#32622;&#19979;&#30340;&#28789;&#27963;&#24615;&#21644;&#25972;&#21512;&#39069;&#22806;&#39033;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#22312;&#23545;&#20598;&#38382;&#39064;&#20013;&#37319;&#29992;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#30697;&#38453;SVM&#30340;&#35299;&#20915;&#25928;&#29575;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#26102;&#38388;&#25928;&#29575;&#21644;&#24615;&#33021;&#19978;&#19982;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#30456;&#20851;&#24615;SVM&#26041;&#27861;&#30456;&#24403;&#12290;&#36825;&#31181;&#30697;&#38453;&#34920;&#36848;&#25581;&#31034;&#20986;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#38590;&#20197;&#23519;&#35273;&#30340;&#20851;&#38190;&#27934;&#35265;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#30001;&#20110;&#20854;&#25104;&#21151;&#23454;&#29616;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#32780;&#25104;&#20026;&#20998;&#31867;&#22120;&#39046;&#22495;&#20869;&#22791;&#21463;&#36190;&#35465;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#31867;&#21035;&#21644;&#22810;&#26631;&#31614;&#35774;&#32622;&#19979;&#65292;&#29616;&#26377;SVM&#27169;&#22411;&#20013;&#22522;&#20110;&#21521;&#37327;&#30340;&#34920;&#36848;&#23384;&#22312;&#28789;&#27963;&#24615;&#21644;&#25972;&#21512;&#39069;&#22806;&#39033;&#26469;&#24212;&#23545;&#29305;&#23450;&#25361;&#25112;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35770;&#25991;&#19987;&#27880;&#20110;&#24341;&#20837;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#32422;&#26463;&#30340;&#30697;&#38453;&#34920;&#36848;&#26469;&#20316;&#20026;SVM&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#23545;&#20598;&#38382;&#39064;&#20013;&#37319;&#29992;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#20915;Matrix-SVM&#38382;&#39064;&#30340;&#25928;&#29575;&#12290;&#23545;&#22810;&#26631;&#31614;&#21644;&#22810;&#31867;&#21035;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#30697;&#38453;SVM&#22312;&#26102;&#38388;&#25928;&#29575;&#19978;&#36798;&#21040;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19982;&#20108;&#36827;&#21046;&#30456;&#20851;&#24615;SVM&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30697;&#38453;&#34920;&#36848;&#25581;&#31034;&#20102;&#20256;&#32479;&#22522;&#20110;&#21521;&#37327;&#30340;&#26041;&#27861;&#20013;&#38590;&#20197;&#23519;&#35273;&#30340;&#20851;&#38190;&#27934;&#35265;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support Vector Machines (SVM) have gathered significant acclaim as classifiers due to their successful implementation of Statistical Learning Theory. However, in the context of multiclass and multilabel settings, the reliance on vector-based formulations in existing SVM-based models poses limitations regarding flexibility and ease of incorporating additional terms to handle specific challenges. To overcome these limitations, our research paper focuses on introducing a matrix formulation for SVM that effectively addresses these constraints. By employing the Accelerated Gradient Descent method in the dual, we notably enhance the efficiency of solving the Matrix-SVM problem. Experimental evaluations on multilabel and multiclass datasets demonstrate that Matrix SVM achieves superior time efficacy while delivering similar results to Binary Relevance SVM.  Moreover, our matrix formulation unveils crucial insights and advantages that may not be readily apparent in traditional vector-based not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31163;&#25955;&#20248;&#21270;&#30340;&#31232;&#30095;&#39640;&#26031;&#22270;&#27169;&#22411;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#27714;&#35299;&#22120;&#26469;&#33719;&#21462;&#33391;&#22909;&#30340;&#21407;&#22987;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.09366</link><description>&lt;p&gt;
&#31232;&#30095;&#39640;&#26031;&#22270;&#27169;&#22411;&#30340;&#31163;&#25955;&#20248;&#21270;&#65306;&#35745;&#31639;&#21644;&#32479;&#35745;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sparse Gaussian Graphical Models with Discrete Optimization: Computational and Statistical Perspectives. (arXiv:2307.09366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31163;&#25955;&#20248;&#21270;&#30340;&#31232;&#30095;&#39640;&#26031;&#22270;&#27169;&#22411;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#27714;&#35299;&#22120;&#26469;&#33719;&#21462;&#33391;&#22909;&#30340;&#21407;&#22987;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#23398;&#20064;&#22522;&#20110;&#26080;&#21521;&#39640;&#26031;&#22270;&#27169;&#22411;&#30340;&#31232;&#30095;&#22270;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#32473;&#23450;&#26469;&#33258;&#20855;&#26377;p&#20010;&#21464;&#37327;&#30340;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#30340;n&#20010;&#26679;&#26412;&#65292;&#30446;&#26631;&#26159;&#20272;&#35745;p&#215;p&#30340;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#65288;&#20063;&#31216;&#20026;&#31934;&#24230;&#30697;&#38453;&#65289;&#65292;&#20551;&#35774;&#23427;&#26159;&#31232;&#30095;&#30340;&#65288;&#21363;&#20855;&#26377;&#23569;&#25968;&#38750;&#38646;&#26465;&#30446;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GraphL0BnB&#36825;&#19968;&#26032;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#20266;&#20284;&#28982;&#20989;&#25968;&#30340;l0&#24809;&#32602;&#29256;&#26412;&#65292;&#32780;&#22823;&#22810;&#25968;&#26089;&#26399;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;l1&#26494;&#24347;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#26041;&#27861;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20984;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#65292;&#20351;&#29992;&#29616;&#25104;&#30340;&#21830;&#29992;&#27714;&#35299;&#22120;&#22312;&#22823;&#35268;&#27169;&#35745;&#31639;&#26102;&#21487;&#33021;&#24456;&#38590;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;MIP&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#38750;&#32447;&#24615;&#20998;&#25903;&#23450;&#30028;&#65288;BnB&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#23450;&#21046;&#30340;&#19968;&#38454;&#26041;&#27861;&#26469;&#35299;&#20915;&#33410;&#28857;&#25918;&#26494;&#38382;&#39064;&#12290;&#20316;&#20026;&#25105;&#20204;BnB&#26694;&#26550;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#33719;&#24471;&#29420;&#31435;&#20852;&#36259;&#30340;&#33391;&#22909;&#21407;&#22987;&#35299;&#30340;&#22823;&#35268;&#27169;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning a sparse graph underlying an undirected Gaussian graphical model, a key problem in statistical machine learning. Given $n$ samples from a multivariate Gaussian distribution with $p$ variables, the goal is to estimate the $p \times p$ inverse covariance matrix (aka precision matrix), assuming it is sparse (i.e., has a few nonzero entries). We propose GraphL0BnB, a new estimator based on an $\ell_0$-penalized version of the pseudolikelihood function, while most earlier approaches are based on the $\ell_1$-relaxation. Our estimator can be formulated as a convex mixed integer program (MIP) which can be difficult to compute at scale using off-the-shelf commercial solvers. To solve the MIP, we propose a custom nonlinear branch-and-bound (BnB) framework that solves node relaxations with tailored first-order methods. As a by-product of our BnB framework, we propose large-scale solvers for obtaining good primal solutions that are of independent interest. We d
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#24120;&#35265;&#30340;&#38646;&#25104;&#26412;&#20195;&#29702;&#22312;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#20013;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#33021;&#39044;&#27979;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#39044;&#27979;&#40065;&#26834;&#24615;&#20351;&#24471;&#20174;&#29616;&#26377;&#30340;&#38646;&#25104;&#26412;&#20195;&#29702;&#36827;&#34892;&#39044;&#27979;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09365</link><description>&lt;p&gt;
&#38646;&#25104;&#26412;&#20195;&#29702;&#30340;&#35780;&#20272;&#8212;&#8212;&#20174;&#31070;&#32463;&#32467;&#26500;&#24615;&#33021;&#21040;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
An Evaluation of Zero-Cost Proxies -- from Neural Architecture Performance to Model Robustness. (arXiv:2307.09365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09365
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#24120;&#35265;&#30340;&#38646;&#25104;&#26412;&#20195;&#29702;&#22312;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#20013;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#33021;&#39044;&#27979;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#39044;&#27979;&#40065;&#26834;&#24615;&#20351;&#24471;&#20174;&#29616;&#26377;&#30340;&#38646;&#25104;&#26412;&#20195;&#29702;&#36827;&#34892;&#39044;&#27979;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#25104;&#26412;&#20195;&#29702;&#29616;&#22914;&#20170;&#34987;&#24191;&#27867;&#30740;&#31350;&#21644;&#24212;&#29992;&#20110;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#20013;&#12290;&#23427;&#20204;&#36890;&#36807;&#21033;&#29992;&#26410;&#32463;&#35757;&#32451;&#30340;&#21442;&#25968;&#26469;&#39044;&#27979;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#25216;&#26415;&#22823;&#22823;&#21152;&#24555;&#20102;&#25628;&#32034;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#33267;&#20170;&#22312;NAS&#39046;&#22495;&#20013;&#65292;&#24456;&#23569;&#26377;&#20851;&#20110;&#21516;&#26102;&#25628;&#32034;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#38646;&#25104;&#26412;&#20195;&#29702;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#32467;&#26500;&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24212;&#35813;&#21516;&#26679;&#37325;&#35201;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#24120;&#35265;&#30340;&#38646;&#25104;&#26412;&#20195;&#29702;&#22312;&#27969;&#34892;&#30340;NAS-Bench-201&#25628;&#32034;&#31354;&#38388;&#20013;&#20316;&#20026;&#24615;&#33021;&#39044;&#27979;&#22120;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#40065;&#26834;&#24615;&#30340;&#21333;&#19968;&#39044;&#27979;&#20219;&#21153;&#21644;&#24178;&#20928;&#20934;&#30830;&#29575;&#19982;&#40065;&#26834;&#20934;&#30830;&#29575;&#30340;&#32508;&#21512;&#22810;&#30446;&#26631;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#20195;&#29702;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#39044;&#27979;&#40065;&#26834;&#24615;&#20351;&#24471;&#20174;&#29616;&#26377;&#30340;&#38646;&#25104;&#26412;&#20195;&#29702;&#36827;&#34892;&#39044;&#27979;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-cost proxies are nowadays frequently studied and used to search for neural architectures. They show an impressive ability to predict the performance of architectures by making use of their untrained weights. These techniques allow for immense search speed-ups. So far the joint search for well-performing and robust architectures has received much less attention in the field of NAS. Therefore, the main focus of zero-cost proxies is the clean accuracy of architectures, whereas the model robustness should play an evenly important part. In this paper, we analyze the ability of common zero-cost proxies to serve as performance predictors for robustness in the popular NAS-Bench-201 search space. We are interested in the single prediction task for robustness and the joint multi-objective of clean and robust accuracy. We further analyze the feature importance of the proxies and show that predicting the robustness makes the prediction task from existing zero-cost proxies more challenging. As
&lt;/p&gt;</description></item><item><title>MOCA&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#26469;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#12290;&#23427;&#21516;&#26102;&#20855;&#22791;&#33391;&#22909;&#30340;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#21644;&#23545;&#22270;&#20687;&#25200;&#21160;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#21644;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#35757;&#32451;&#36895;&#24230;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.09361</link><description>&lt;p&gt;
MOCA: &#33258;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments. (arXiv:2307.09361v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09361
&lt;/p&gt;
&lt;p&gt;
MOCA&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#26469;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#12290;&#23427;&#21516;&#26102;&#20855;&#22791;&#33391;&#22909;&#30340;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#21644;&#23545;&#22270;&#20687;&#25200;&#21160;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#21644;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#35757;&#32451;&#36895;&#24230;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#32531;&#35299;Vision Transformer&#32593;&#32476;&#23545;&#22823;&#22411;&#20840;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#36138;&#23146;&#38656;&#27714;&#12290;&#19981;&#21516;&#31867;&#21035;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#20855;&#26377;&#33391;&#22909;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;&#20351;&#29992;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#31574;&#30053;&#65292;&#25110;&#32773;&#23545;&#22270;&#20687;&#25200;&#21160;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;&#20351;&#29992;&#23545;&#27604;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#38454;&#27573;&#12289;&#29420;&#31435;&#30340;&#26041;&#27861;MOCA&#65292;&#20351;&#29992;&#22522;&#20110;&#39640;&#32423;&#29305;&#24449;&#65288;&#32780;&#19981;&#26159;&#20687;&#32032;&#32423;&#32454;&#33410;&#65289;&#23450;&#20041;&#30340;&#26032;&#22411;&#25513;&#30721;&#21644;&#39044;&#27979;&#30446;&#26631;&#26469;&#32479;&#19968;&#36825;&#20004;&#31181;&#26399;&#26395;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20197;&#21327;&#21516;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#24212;&#29992;&#36825;&#20004;&#31181;&#23398;&#20064;&#33539;&#24335;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20854;&#35757;&#32451;&#36895;&#24230;&#33267;&#23569;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#20171;&#32461;&#20102;&#20351;&#29992;IBM Analog Hardware Acceleration Kit (AIHWKit)&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#24037;&#20855;&#21253;&#27169;&#25311;&#20102;&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#65288;AIMC&#65289;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20339;&#23454;&#36341;&#21644;&#20113;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.09357</link><description>&lt;p&gt;
&#20351;&#29992;IBM&#27169;&#25311;&#20869;&#23384;&#30828;&#20214;&#21152;&#36895;&#22871;&#20214;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Using the IBM Analog In-Memory Hardware Acceleration Kit for Neural Network Training and Inference. (arXiv:2307.09357v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#20171;&#32461;&#20102;&#20351;&#29992;IBM Analog Hardware Acceleration Kit (AIHWKit)&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#24037;&#20855;&#21253;&#27169;&#25311;&#20102;&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#65288;AIMC&#65289;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20339;&#23454;&#36341;&#21644;&#20113;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#65288;AIMC&#65289;&#26159;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#26029;&#21644;&#35757;&#32451;&#30340;&#24310;&#36831;&#21644;&#33021;&#28304;&#28040;&#32791;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;AIMC&#33455;&#29255;&#20013;&#30340;&#22122;&#22768;&#21644;&#38750;&#32447;&#24615;&#22120;&#20214;&#29305;&#24615;&#20197;&#21450;&#38750;&#29702;&#24819;&#30340;&#22806;&#22260;&#30005;&#36335;&#35201;&#27714;&#35843;&#25972;DNN&#20197;&#22312;&#27492;&#31867;&#30828;&#20214;&#19978;&#23454;&#29616;&#19982;&#25968;&#23383;&#35745;&#31639;&#31561;&#25928;&#30340;&#31934;&#24230;&#12290;&#22312;&#36825;&#20010;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;IBM&#27169;&#25311;&#30828;&#20214;&#21152;&#36895;&#22871;&#20214;&#65288;AIHWKit&#65289;&#36827;&#34892;&#36825;&#26679;&#30340;&#35843;&#25972;&#21644;&#35780;&#20272;&#65292;&#35813;&#22871;&#20214;&#21487;&#22312;https://github.com/IBM/aihwkit&#19978;&#20813;&#36153;&#33719;&#24471;&#12290;AIHWKit&#26159;&#19968;&#20010;Python&#24211;&#65292;&#21487;&#20197;&#20351;&#29992;AIMC&#27169;&#25311;&#25512;&#26029;&#21644;&#35757;&#32451;DNN&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;AIHWKit&#35774;&#35745;&#12289;&#21151;&#33021;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20197;&#27491;&#30830;&#36827;&#34892;&#25512;&#26029;&#21644;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#27169;&#25311;AI&#20113;&#32452;&#21512;&#22120;&#30340;&#27010;&#36848;&#65292;&#35813;&#32452;&#21512;&#25552;&#20379;&#20102;&#22312;&#23436;&#20840;&#25176;&#31649;&#30340;&#20113;&#29615;&#22659;&#20013;&#20351;&#29992;AIHWKit&#27169;&#25311;&#24179;&#21488;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analog In-Memory Computing (AIMC) is a promising approach to reduce the latency and energy consumption of Deep Neural Network (DNN) inference and training. However, the noisy and non-linear device characteristics, and the non-ideal peripheral circuitry in AIMC chips, require adapting DNNs to be deployed on such hardware to achieve equivalent accuracy to digital computing. In this tutorial, we provide a deep dive into how such adaptations can be achieved and evaluated using the recently released IBM Analog Hardware Acceleration Kit (AIHWKit), freely available at https://github.com/IBM/aihwkit. The AIHWKit is a Python library that simulates inference and training of DNNs using AIMC. We present an in-depth description of the AIHWKit design, functionality, and best practices to properly perform inference and training. We also present an overview of the Analog AI Cloud Composer, that provides the benefits of using the AIHWKit simulation platform in a fully managed cloud setting. Finally, we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#25972;&#25968;&#32422;&#26463;&#30340;SAT&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#19968;&#32452;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#65292;&#24182;&#19988;&#19987;&#38376;&#20026;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#35774;&#35745;&#30340;&#26032;&#29305;&#24449;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09342</link><description>&lt;p&gt;
&#23398;&#20064;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#25972;&#25968;&#32422;&#26463;&#30340;SAT&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints. (arXiv:2307.09342v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#25972;&#25968;&#32422;&#26463;&#30340;SAT&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#19968;&#32452;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#65292;&#24182;&#19988;&#19987;&#38376;&#20026;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#35774;&#35745;&#30340;&#26032;&#29305;&#24449;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23558;&#23427;&#20204;&#32534;&#30721;&#20026;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65288;SAT&#65289;&#30340;&#23454;&#20363;&#26469;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#32422;&#26463;&#31867;&#22411;&#22312;&#25991;&#29486;&#20013;&#20063;&#26377;&#24456;&#22810;&#32534;&#30721;&#26041;&#24335;&#65292;&#24615;&#33021;&#24046;&#24322;&#24456;&#22823;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#32534;&#30721;&#26041;&#24335;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#23454;&#20363;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#12290;&#25105;&#20204;&#37319;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#30340;&#32534;&#30721;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#32422;&#26463;&#38382;&#39064;&#30340;&#29305;&#24449;&#38598;&#26469;&#26377;&#25928;&#22320;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#38376;&#20026;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#35774;&#35745;&#30340;&#19968;&#32452;&#26032;&#29305;&#24449;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#36873;&#25321;&#26410;&#35265;&#36807;&#30340;&#38382;&#39064;&#31867;&#21035;&#30340;&#32534;&#30721;&#26041;&#24335;&#26102;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;&#24403;&#20351;&#29992;&#30456;&#21516;&#30340;&#29305;&#24449;&#38598;&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;AutoFolio&#30456;&#27604;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23454;&#20363;&#29305;&#24449;&#23545;&#20110;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#20219;&#21153;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many constraint satisfaction and optimisation problems can be solved effectively by encoding them as instances of the Boolean Satisfiability problem (SAT). However, even the simplest types of constraints have many encodings in the literature with widely varying performance, and the problem of selecting suitable encodings for a given problem instance is not trivial. We explore the problem of selecting encodings for pseudo-Boolean and linear constraints using a supervised machine learning approach. We show that it is possible to select encodings effectively using a standard set of features for constraint problems; however we obtain better performance with a new set of features specifically designed for the pseudo-Boolean and linear constraints. In fact, we achieve good results when selecting encodings for unseen problem classes. Our results compare favourably to AutoFolio when using the same feature set. We discuss the relative importance of instance features to the task of selecting the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#27874;&#20989;&#25968;&#27169;&#22411;&#36827;&#34892;&#31934;&#30830;&#30340;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#27599;&#27425;&#36827;&#34892;&#20840;&#20248;&#21270;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#36890;&#36807;&#23569;&#37327;&#24494;&#35843;&#21363;&#21487;&#33719;&#24471;&#20934;&#30830;&#30340;&#30456;&#23545;&#33021;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.09337</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#30340;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#27861;&#8212;&#8212;&#20248;&#21270;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#27874;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Variational Monte Carlo on a Budget -- Fine-tuning pre-trained Neural Wavefunctions. (arXiv:2307.09337v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09337
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#27874;&#20989;&#25968;&#27169;&#22411;&#36827;&#34892;&#31934;&#30830;&#30340;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#27599;&#27425;&#36827;&#34892;&#20840;&#20248;&#21270;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#36890;&#36807;&#23569;&#37327;&#24494;&#35843;&#21363;&#21487;&#33719;&#24471;&#20934;&#30830;&#30340;&#30456;&#23545;&#33021;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;&#20013;&#65292;&#31934;&#30830;&#35299;&#20915;&#34203;&#23450;&#35860;&#26041;&#31243;&#26159;&#20851;&#38190;&#25361;&#25112;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#27861;&#65288;DL-VMC&#65289;&#26368;&#36817;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#65292;&#20294;&#21482;&#33021;&#20197;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#20026;&#20195;&#20215;&#12290;&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#27169;&#22411;&#21482;&#38656;&#36827;&#34892;&#19968;&#27425;&#35757;&#32451;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#25512;&#29702;&#65292;&#32780;&#20934;&#30830;&#30340;DL-VMC&#36804;&#20170;&#20026;&#27490;&#38656;&#35201;&#20026;&#27599;&#20010;&#26032;&#30340;&#38382;&#39064;&#23454;&#20363;&#36827;&#34892;&#23436;&#25972;&#20248;&#21270;&#65292;&#21363;&#20351;&#26159;&#23545;&#20110;&#23567;&#20998;&#23376;&#20063;&#38656;&#35201;&#28040;&#32791;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#36807;&#33258;&#30417;&#30563;&#24335;&#27874;&#20989;&#25968;&#20248;&#21270;&#30340;&#22823;&#35268;&#27169;&#21270;&#23398;&#22810;&#26679;&#24615;&#20998;&#23376;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;DL-VMC&#27169;&#22411;&#12290;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#27809;&#26377;&#20219;&#20309;&#20248;&#21270;&#30340;&#26032;&#20998;&#23376;&#65292;&#21487;&#20197;&#24471;&#21040;&#32988;&#36807;CCSD&#65288;T&#65289;-2Z&#31561;&#24050;&#24314;&#31435;&#26041;&#27861;&#30340;&#27874;&#20989;&#25968;&#21644;&#32477;&#23545;&#33021;&#37327;&#12290;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#30456;&#23545;&#33021;&#37327;&#65292;&#21482;&#38656;&#23545;&#36825;&#20010;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#23569;&#37327;&#24494;&#35843;&#27493;&#39588;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining accurate solutions to the Schr\"odinger equation is the key challenge in computational quantum chemistry. Deep-learning-based Variational Monte Carlo (DL-VMC) has recently outperformed conventional approaches in terms of accuracy, but only at large computational cost. Whereas in many domains models are trained once and subsequently applied for inference, accurate DL-VMC so far requires a full optimization for every new problem instance, consuming thousands of GPUhs even for small molecules. We instead propose a DL-VMC model which has been pre-trained using self-supervised wavefunction optimization on a large and chemically diverse set of molecules. Applying this model to new molecules without any optimization, yields wavefunctions and absolute energies that outperform established methods such as CCSD(T)-2Z. To obtain accurate relative energies, only few fine-tuning steps of this base model are required. We accomplish this with a fully end-to-end machine-learned model, consist
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#20998;&#31867;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#24573;&#35270;&#20102;&#23383;&#27573;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23383;&#27573;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20840;&#23616;&#23383;&#27573;&#20381;&#36182;&#30697;&#38453;&#24182;&#22312;&#23454;&#20363;&#32423;&#21035;&#19978;&#32454;&#21270;&#20381;&#36182;&#30697;&#38453;&#65292;&#25913;&#36827;&#20102;&#23383;&#27573;&#20043;&#38388;&#30340;&#24314;&#27169;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09321</link><description>&lt;p&gt;
&#21033;&#29992;&#23383;&#27573;&#20381;&#36182;&#24615;&#36827;&#34892;&#20998;&#31867;&#25968;&#25454;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Field Dependencies for Learning on Categorical Data. (arXiv:2307.09321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09321
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20998;&#31867;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#24573;&#35270;&#20102;&#23383;&#27573;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23383;&#27573;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20840;&#23616;&#23383;&#27573;&#20381;&#36182;&#30697;&#38453;&#24182;&#22312;&#23454;&#20363;&#32423;&#21035;&#19978;&#32454;&#21270;&#20381;&#36182;&#30697;&#38453;&#65292;&#25913;&#36827;&#20102;&#23383;&#27573;&#20043;&#38388;&#30340;&#24314;&#27169;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20998;&#31867;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#25968;&#25454;&#38598;&#20013;&#23383;&#27573;&#65288;&#20063;&#31216;&#20026;&#29305;&#24449;&#65289;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#28857;&#23884;&#20837;&#30340;&#20998;&#31867;/&#22238;&#24402;&#25439;&#22833;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#23383;&#27573;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#19981;&#26159;&#20840;&#23616;&#24314;&#27169;&#29305;&#24449;&#30340;&#32479;&#35745;&#20449;&#24687;&#65288;&#22914;&#29305;&#24449;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#65289;&#65292;&#32780;&#26159;&#23398;&#20064;&#19968;&#20010;&#25429;&#25417;&#23383;&#27573;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#20840;&#23616;&#23383;&#27573;&#20381;&#36182;&#30697;&#38453;&#65292;&#24182;&#21033;&#29992;&#19981;&#21516;&#26435;&#37325;&#65288;&#31216;&#20026;&#23616;&#37096;&#20381;&#36182;&#24314;&#27169;&#65289;&#22312;&#23454;&#20363;&#32423;&#21035;&#19978;&#25913;&#36827;&#20840;&#23616;&#23383;&#27573;&#20381;&#36182;&#30697;&#38453;&#30340;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#20803;&#23398;&#20064;&#33539;&#24335;&#65292;&#21363;&#22312;&#20803;&#23398;&#20064;&#31639;&#27861;&#30340;&#20869;&#37096;&#24490;&#29615;&#20013;&#26080;&#38656;&#20351;&#29992;&#26631;&#31614;&#23601;&#21487;&#20197;&#32454;&#21270;&#20381;&#36182;&#30697;&#38453;&#65292;&#32780;&#22806;&#37096;&#24490;&#29615;&#21017;&#20132;&#32455;&#20102;&#23884;&#20837;&#30697;&#38453;&#21644;&#20381;&#36182;&#30697;&#38453;&#30340;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional approaches for learning on categorical data underexploit the dependencies between columns (\aka fields) in a dataset because they rely on the embedding of data points driven alone by the classification/regression loss. In contrast, we propose a novel method for learning on categorical data with the goal of exploiting dependencies between fields. Instead of modelling statistics of features globally (i.e., by the covariance matrix of features), we learn a global field dependency matrix that captures dependencies between fields and then we refine the global field dependency matrix at the instance-wise level with different weights (so-called local dependency modelling) w.r.t. each field to improve the modelling of the field dependencies. Our algorithm exploits the meta-learning paradigm, i.e., the dependency matrices are refined in the inner loop of the meta-learning algorithm without the use of labels, whereas the outer loop intertwines the updates of the embedding matrix (the
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#29983;&#29289;&#21046;&#36896;&#32773;CA&#39033;&#30446;&#65292;&#21033;&#29992;&#32454;&#32990;&#33258;&#21160;&#26426;&#27169;&#25311;&#22797;&#26434;&#30340;&#29983;&#29289;&#32676;&#31995;&#65292;&#24182;&#22312;GPU&#19978;&#36890;&#36807;Python JAX&#26694;&#26550;&#36827;&#34892;&#39640;&#24615;&#33021;&#35745;&#31639;&#12290;&#23637;&#31034;&#20102;&#26893;&#29289;&#20195;&#29702;&#22914;&#20309;&#29983;&#38271;&#12289;&#23384;&#27963;&#12289;&#32321;&#27542;&#21644;&#36827;&#21270;&#65292;&#24418;&#25104;&#31283;&#23450;&#21644;&#19981;&#31283;&#23450;&#30340;&#29983;&#29289;&#32676;&#31995;&#12290;&#20171;&#32461;&#20102;&#31471;&#21040;&#31471;&#20803;&#36827;&#21270;&#21644;&#22521;&#20859;&#30399;&#20803;&#36827;&#21270;&#30340;&#26041;&#27861;&#26469;&#20351;&#27169;&#22411;&#22312;&#24694;&#21155;&#29615;&#22659;&#20013;&#29983;&#23384;&#12290;</title><link>http://arxiv.org/abs/2307.09320</link><description>&lt;p&gt;
&#29983;&#29289;&#21046;&#36896;&#32773;CA&#65306;&#20351;&#29992;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#29983;&#29289;&#21046;&#36896;&#32773;&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
Biomaker CA: a Biome Maker project using Cellular Automata. (arXiv:2307.09320v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09320
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#29983;&#29289;&#21046;&#36896;&#32773;CA&#39033;&#30446;&#65292;&#21033;&#29992;&#32454;&#32990;&#33258;&#21160;&#26426;&#27169;&#25311;&#22797;&#26434;&#30340;&#29983;&#29289;&#32676;&#31995;&#65292;&#24182;&#22312;GPU&#19978;&#36890;&#36807;Python JAX&#26694;&#26550;&#36827;&#34892;&#39640;&#24615;&#33021;&#35745;&#31639;&#12290;&#23637;&#31034;&#20102;&#26893;&#29289;&#20195;&#29702;&#22914;&#20309;&#29983;&#38271;&#12289;&#23384;&#27963;&#12289;&#32321;&#27542;&#21644;&#36827;&#21270;&#65292;&#24418;&#25104;&#31283;&#23450;&#21644;&#19981;&#31283;&#23450;&#30340;&#29983;&#29289;&#32676;&#31995;&#12290;&#20171;&#32461;&#20102;&#31471;&#21040;&#31471;&#20803;&#36827;&#21270;&#21644;&#22521;&#20859;&#30399;&#20803;&#36827;&#21270;&#30340;&#26041;&#27861;&#26469;&#20351;&#27169;&#22411;&#22312;&#24694;&#21155;&#29615;&#22659;&#20013;&#29983;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#29289;&#21046;&#36896;&#32773;CA&#65306;&#20351;&#29992;&#32454;&#32990;&#33258;&#21160;&#26426;&#65288;CA&#65289;&#30340;&#29983;&#29289;&#21046;&#36896;&#32773;&#39033;&#30446;&#12290;&#22312;&#29983;&#29289;&#21046;&#36896;&#32773;CA&#20013;&#65292;&#24418;&#24577;&#21457;&#29983;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20027;&#39064;&#65292;&#23567;&#31181;&#23376;&#38656;&#35201;&#22312;&#20859;&#20998;&#21294;&#20047;&#30340;&#29615;&#22659;&#20013;&#25104;&#38271;&#20026;&#26893;&#29289;&#29366;&#30340;&#29983;&#29289;&#20307;&#65292;&#26368;&#32456;&#36890;&#36807;&#21464;&#24322;&#26469;&#32321;&#27542;&#65292;&#20197;&#20351;&#29983;&#29289;&#32676;&#31995;&#24471;&#20197;&#38271;&#26399;&#23384;&#27963;&#12290;&#25105;&#20204;&#36890;&#36807;2D&#32593;&#26684;&#19978;&#30340;CA&#35268;&#21017;&#27169;&#25311;&#22797;&#26434;&#30340;&#29983;&#29289;&#32676;&#31995;&#65292;&#24182;&#36890;&#36807;Python JAX&#26694;&#26550;&#22312;GPU&#19978;&#24182;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#39033;&#30446;&#20801;&#35768;&#22810;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#29615;&#22659;&#21644;&#8220;&#29289;&#29702;&#8221;&#23450;&#24459;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#31361;&#21464;&#31574;&#30053;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#19968;&#20123;&#37197;&#32622;&#65292;&#23637;&#31034;&#20102;&#26893;&#29289;&#20195;&#29702;&#22914;&#20309;&#29983;&#38271;&#12289;&#23384;&#27963;&#12289;&#32321;&#27542;&#21644;&#36827;&#21270;&#65292;&#24418;&#25104;&#31283;&#23450;&#21644;&#19981;&#31283;&#23450;&#30340;&#29983;&#29289;&#32676;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#31471;&#21040;&#31471;&#20803;&#36827;&#21270;&#25110;&#26356;&#31934;&#30830;&#39640;&#25928;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;&#22521;&#20859;&#30399;&#20803;&#36827;&#21270;&#65289;&#26469;&#20351;&#27169;&#22411;&#22312;&#24694;&#21155;&#29615;&#22659;&#20013;&#29983;&#23384;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36827;&#34892;&#39640;&#24615;&#33021;&#35745;&#31639;&#20197;&#35780;&#20272;&#29983;&#29289;&#32676;&#31995;&#30340;&#31283;&#23450;&#24615;&#21644;&#36827;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Biomaker CA: a Biome Maker project using Cellular Automata (CA). In Biomaker CA, morphogenesis is a first class citizen and small seeds need to grow into plant-like organisms to survive in a nutrient starved environment and eventually reproduce with variation so that a biome survives for long timelines. We simulate complex biomes by means of CA rules in 2D grids and parallelize all of its computation on GPUs through the Python JAX framework. We show how this project allows for several different kinds of environments and laws of 'physics', alongside different model architectures and mutation strategies. We further analyze some configurations to show how plant agents can grow, survive, reproduce, and evolve, forming stable and unstable biomes. We then demonstrate how one can meta-evolve models to survive in a harsh environment either through end-to-end meta-evolution or by a more surgical and efficient approach, called Petri dish meta-evolution. Finally, we show how to perfo
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09312</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65306;&#25972;&#21512;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#21464;&#25442;&#22120;&#20197;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09312
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#20110;&#22270;&#30340;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21517;&#20026;&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65288;mDT&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#26631;&#35760;&#35780;&#35770;&#20026;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#22260;&#32469;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25972;&#20307;&#20998;&#26512;&#23637;&#24320;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#22270;&#21464;&#25442;&#22120;&#26469;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#20013;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#37319;&#29992;&#20132;&#32455;&#34701;&#21512;&#23618;&#26469;&#32452;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#22788;&#29702;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#65292;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25552;&#20379;&#31038;&#20250;&#20215;&#20540;&#30340;&#26410;&#26469;&#24037;&#20316;&#65292;&#24182;&#35748;&#20026;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#27714;&#35299;&#22120;&#21644;&#21487;&#23548;&#20223;&#30495;&#25216;&#26415;&#65292;&#23454;&#29616;&#23545;&#21453;&#21521;&#37327;&#23376;&#20256;&#36755;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#21487;&#20197;&#35774;&#35745;&#36830;&#32493;&#20256;&#36755;&#29305;&#24615;&#21644;&#30005;&#27969;-&#30005;&#21387;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09311</link><description>&lt;p&gt;
&#24102;&#26377;&#24212;&#29992;&#20110;&#37327;&#23376;&#20256;&#36755;&#30340;&#21453;&#38382;&#39064;&#30340;&#33258;&#21160;&#24494;&#20998;
&lt;/p&gt;
&lt;p&gt;
Automatic Differentiation for Inverse Problems with Applications in Quantum Transport. (arXiv:2307.09311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#27714;&#35299;&#22120;&#21644;&#21487;&#23548;&#20223;&#30495;&#25216;&#26415;&#65292;&#23454;&#29616;&#23545;&#21453;&#21521;&#37327;&#23376;&#20256;&#36755;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#21487;&#20197;&#35774;&#35745;&#36830;&#32493;&#20256;&#36755;&#29305;&#24615;&#21644;&#30005;&#27969;-&#30005;&#21387;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#27714;&#35299;&#22120;&#21644;&#21487;&#23548;&#20223;&#30495;&#30340;&#37327;&#23376;&#20256;&#36755;&#36793;&#30028;&#27169;&#22411;&#65292;&#29992;&#20110;&#21453;&#21521;&#37327;&#23376;&#20256;&#36755;&#38382;&#39064;&#12290;&#31070;&#32463;&#27714;&#35299;&#22120;&#29992;&#20110;&#35774;&#35745;&#36830;&#32493;&#20256;&#36755;&#29305;&#24615;&#65292;&#21487;&#23548;&#20223;&#30495;&#29992;&#20110;&#35774;&#35745;&#30005;&#27969;-&#30005;&#21387;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A neural solver and differentiable simulation of the quantum transmitting boundary model is presented for the inverse quantum transport problem. The neural solver is used to engineer continuous transmission properties and the differentiable simulation is used to engineer current-voltage characteristics.
&lt;/p&gt;</description></item><item><title>EigenTrajectory&#26159;&#19968;&#31181;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#36712;&#36857;&#25551;&#36848;&#31526;&#26469;&#24418;&#25104;&#19968;&#20010;&#32039;&#20945;&#30340;&#31354;&#38388;&#65292;&#26377;&#25928;&#25429;&#25417;&#39640;&#32500;&#31038;&#20132;&#20114;&#21160;&#21644;&#21487;&#34892;&#30340;&#26410;&#26469;&#65292;&#20197;&#20195;&#26367;&#20256;&#32479;&#30340;&#21442;&#25968;&#26354;&#32447;&#25311;&#21512;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#20302;&#31209;&#36924;&#36817;&#38477;&#20302;&#20102;&#36712;&#36857;&#25551;&#36848;&#31526;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#23558;&#34892;&#20154;&#30340;&#21382;&#21490;&#36335;&#24452;&#36716;&#21270;&#20026;&#20197;&#31354;&#38388;-&#26102;&#38388;&#20027;&#25104;&#20998;&#34920;&#31034;&#30340;&#26032;&#30340;&#36712;&#36857;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.09306</link><description>&lt;p&gt;
EigenTrajectory&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#30340;&#20302;&#31209;&#25551;&#36848;&#31526;
&lt;/p&gt;
&lt;p&gt;
EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting. (arXiv:2307.09306v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09306
&lt;/p&gt;
&lt;p&gt;
EigenTrajectory&#26159;&#19968;&#31181;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#36712;&#36857;&#25551;&#36848;&#31526;&#26469;&#24418;&#25104;&#19968;&#20010;&#32039;&#20945;&#30340;&#31354;&#38388;&#65292;&#26377;&#25928;&#25429;&#25417;&#39640;&#32500;&#31038;&#20132;&#20114;&#21160;&#21644;&#21487;&#34892;&#30340;&#26410;&#26469;&#65292;&#20197;&#20195;&#26367;&#20256;&#32479;&#30340;&#21442;&#25968;&#26354;&#32447;&#25311;&#21512;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#20302;&#31209;&#36924;&#36817;&#38477;&#20302;&#20102;&#36712;&#36857;&#25551;&#36848;&#31526;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#23558;&#34892;&#20154;&#30340;&#21382;&#21490;&#36335;&#24452;&#36716;&#21270;&#20026;&#20197;&#31354;&#38388;-&#26102;&#38388;&#20027;&#25104;&#20998;&#34920;&#31034;&#30340;&#26032;&#30340;&#36712;&#36857;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25429;&#25417;&#39640;&#32500;&#31038;&#20132;&#20114;&#21160;&#21644;&#21487;&#34892;&#30340;&#26410;&#26469;&#23545;&#20110;&#39044;&#27979;&#36712;&#36857;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#24050;&#32463;&#33268;&#21147;&#20110;&#36890;&#36807;&#21442;&#25968;&#26354;&#32447;&#25311;&#21512;&#26469;&#20943;&#23569;&#36755;&#20986;&#21464;&#37327;&#30340;&#32500;&#24230;&#65292;&#20363;&#22914;&#36125;&#22622;&#23572;&#26354;&#32447;&#21644;B&#26679;&#26465;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20989;&#25968;&#24182;&#19981;&#36866;&#21512;&#32771;&#34385;&#21040;&#31038;&#20250;&#21487;&#25509;&#21463;&#30340;&#20154;&#20307;&#21160;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EigenTrajectory&#65288;$\mathbb{ET}$&#65289;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#36712;&#36857;&#25551;&#36848;&#31526;&#26469;&#24418;&#25104;&#19968;&#20010;&#32039;&#20945;&#30340;&#31354;&#38388;&#65292;&#21363;$\mathbb{ET}$&#31354;&#38388;&#65292;&#20197;&#20195;&#26367;&#27431;&#27663;&#31354;&#38388;&#26469;&#34920;&#31034;&#34892;&#20154;&#30340;&#31227;&#21160;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20302;&#31209;&#36924;&#36817;&#38477;&#20302;&#36712;&#36857;&#25551;&#36848;&#31526;&#30340;&#22797;&#26434;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#34892;&#20154;&#30340;&#21382;&#21490;&#36335;&#24452;&#36716;&#21270;&#20026;&#20197;&#31354;&#38388;-&#26102;&#38388;&#20027;&#25104;&#20998;&#34920;&#31034;&#30340;$\mathbb{ET}$&#31354;&#38388;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#29616;&#25104;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capturing high-dimensional social interactions and feasible futures is essential for predicting trajectories. To address this complex nature, several attempts have been devoted to reducing the dimensionality of the output variables via parametric curve fitting such as the B\'ezier curve and B-spline function. However, these functions, which originate in computer graphics fields, are not suitable to account for socially acceptable human dynamics. In this paper, we present EigenTrajectory ($\mathbb{ET}$), a trajectory prediction approach that uses a novel trajectory descriptor to form a compact space, known here as $\mathbb{ET}$ space, in place of Euclidean space, for representing pedestrian movements. We first reduce the complexity of the trajectory descriptor via a low-rank approximation. We transform the pedestrians' history paths into our $\mathbb{ET}$ space represented by spatio-temporal principle components, and feed them into off-the-shelf trajectory forecasting models. The inputs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21547;&#26377;&#27169;&#31946;&#22320;&#38754;&#30495;&#30456;&#30340;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#32570;&#20047;&#26126;&#30830;&#22320;&#38754;&#30495;&#30456;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20302;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.09302</link><description>&lt;p&gt;
&#21547;&#26377;&#19981;&#30830;&#23450;&#22320;&#38754;&#30495;&#30456;&#30340;&#31526;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction under ambiguous ground truth. (arXiv:2307.09302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21547;&#26377;&#27169;&#31946;&#22320;&#38754;&#30495;&#30456;&#30340;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#32570;&#20047;&#26126;&#30830;&#22320;&#38754;&#30495;&#30456;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20302;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#31526;&#21512;&#39044;&#27979;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#32622;&#20449;&#21306;&#38388;&#26469;&#36827;&#34892;&#20005;&#26684;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#30495;&#27491;&#31867;&#21035;&#30340;&#29992;&#25143;&#25351;&#23450;&#30340;&#27010;&#29575;&#12290;&#36825;&#36890;&#24120;&#20551;&#35774;&#26377;&#19968;&#20010;&#29420;&#31435;&#30340;&#26657;&#20934;&#38598;&#21512;&#65292;&#24182;&#19988;&#33021;&#22815;&#35775;&#38382;&#22320;&#38754;&#30495;&#30456;&#26631;&#31614;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#26631;&#31614;&#24456;&#38590;&#33719;&#24471;&#65292;&#24182;&#19988;&#36890;&#24120;&#36890;&#36807;&#32858;&#21512;&#19987;&#23478;&#24847;&#35265;&#26469;&#36817;&#20284;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#36866;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30693;&#21517;&#30340;&#25968;&#25454;&#38598;&#22914;CIFAR&#21644;ImageNet&#12290;&#20351;&#29992;&#36825;&#26679;&#30340;&#26631;&#31614;&#24212;&#29992;&#31526;&#21512;&#39044;&#27979;&#20250;&#20302;&#20272;&#19981;&#30830;&#23450;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#24403;&#19987;&#23478;&#24847;&#35265;&#26080;&#27861;&#35299;&#20915;&#26102;&#65292;&#26631;&#31614;&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#27169;&#31946;&#24615;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#27809;&#26377;&#8220;&#28165;&#26224;&#8221;&#12289;&#26126;&#30830;&#30340;&#22320;&#38754;&#30495;&#30456;&#26631;&#31614;&#65292;&#32780;&#22312;&#26657;&#20934;&#36807;&#31243;&#20013;&#24212;&#35813;&#32771;&#34385;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#36825;&#31181;&#27169;&#31946;&#22320;&#38754;&#30495;&#30456;&#24773;&#26223;&#24320;&#21457;&#20102;&#19968;&#20010;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20381;&#36182;&#20110;&#23545;&#28508;&#22312;&#27169;&#31946;&#24615;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
In safety-critical classification tasks, conformal prediction allows to perform rigorous uncertainty quantification by providing confidence sets including the true class with a user-specified probability. This generally assumes the availability of a held-out calibration set with access to ground truth labels. Unfortunately, in many domains, such labels are difficult to obtain and usually approximated by aggregating expert opinions. In fact, this holds true for almost all datasets, including well-known ones such as CIFAR and ImageNet. Applying conformal prediction using such labels underestimates uncertainty. Indeed, when expert opinions are not resolvable, there is inherent ambiguity present in the labels. That is, we do not have ``crisp'', definitive ground truth labels and this uncertainty should be taken into account during calibration. In this paper, we develop a conformal prediction framework for such ambiguous ground truth settings which relies on an approximation of the underlyi
&lt;/p&gt;</description></item><item><title>&#23884;&#22871;&#28040;&#38500;&#26159;&#19968;&#31181;&#31616;&#21333;&#26131;&#23454;&#29616;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21019;&#26032;&#30340;&#28040;&#38500;&#20934;&#21017;&#21644;&#23884;&#22871;&#32467;&#26500;&#65292;&#33021;&#22815;&#20197;&#26368;&#23569;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#39640;&#32622;&#20449;&#27700;&#24179;&#35782;&#21035;&#20986;&#26368;&#21463;&#27426;&#36814;&#30340;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2307.09295</link><description>&lt;p&gt;
&#23884;&#22871;&#28040;&#38500;&#65306;&#19968;&#31181;&#20174;&#22522;&#20110;&#36873;&#25321;&#30340;&#21453;&#39304;&#20013;&#35782;&#21035;&#26368;&#20339;&#39033;&#30446;&#30340;&#31616;&#21333;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nested Elimination: A Simple Algorithm for Best-Item Identification from Choice-Based Feedback. (arXiv:2307.09295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09295
&lt;/p&gt;
&lt;p&gt;
&#23884;&#22871;&#28040;&#38500;&#26159;&#19968;&#31181;&#31616;&#21333;&#26131;&#23454;&#29616;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21019;&#26032;&#30340;&#28040;&#38500;&#20934;&#21017;&#21644;&#23884;&#22871;&#32467;&#26500;&#65292;&#33021;&#22815;&#20197;&#26368;&#23569;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#39640;&#32622;&#20449;&#27700;&#24179;&#35782;&#21035;&#20986;&#26368;&#21463;&#27426;&#36814;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#36873;&#25321;&#30340;&#21453;&#39304;&#20013;&#35782;&#21035;&#26368;&#20339;&#39033;&#30446;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#20844;&#21496;&#20381;&#27425;&#21521;&#19968;&#32676;&#39038;&#23458;&#23637;&#31034;&#26174;&#31034;&#38598;&#65292;&#24182;&#25910;&#38598;&#20182;&#20204;&#30340;&#36873;&#25321;&#12290;&#30446;&#26631;&#26159;&#20197;&#26368;&#23569;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#39640;&#32622;&#20449;&#27700;&#24179;&#35782;&#21035;&#20986;&#26368;&#21463;&#27426;&#36814;&#30340;&#39033;&#30446;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#38500;&#30340;&#31639;&#27861;&#65292;&#21363;&#23884;&#22871;&#28040;&#38500;(Nested Elimination&#65292;NE)&#65292;&#23427;&#21463;&#21040;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#25152;&#26263;&#31034;&#30340;&#23884;&#22871;&#32467;&#26500;&#30340;&#21551;&#21457;&#12290;NE&#30340;&#32467;&#26500;&#31616;&#21333;&#65292;&#26131;&#20110;&#23454;&#26045;&#65292;&#20855;&#26377;&#23545;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24378;&#22823;&#29702;&#35770;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NE&#21033;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28040;&#38500;&#20934;&#21017;&#65292;&#24182;&#36991;&#20813;&#20102;&#35299;&#20915;&#20219;&#20309;&#22797;&#26434;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;NE&#30340;&#29305;&#23450;&#23454;&#20363;&#21644;&#38750;&#28176;&#36817;&#24615;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;NE&#23454;&#29616;&#20102;&#39640;&#38454;&#26368;&#22351;&#24773;&#20917;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;&#26368;&#21518;&#65292;&#26469;&#33258;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of best-item identification from choice-based feedback. In this problem, a company sequentially and adaptively shows display sets to a population of customers and collects their choices. The objective is to identify the most preferred item with the least number of samples and at a high confidence level. We propose an elimination-based algorithm, namely Nested Elimination (NE), which is inspired by the nested structure implied by the information-theoretic lower bound. NE is simple in structure, easy to implement, and has a strong theoretical guarantee for sample complexity. Specifically, NE utilizes an innovative elimination criterion and circumvents the need to solve any complex combinatorial optimization problem. We provide an instance-specific and non-asymptotic bound on the expected sample complexity of NE. We also show NE achieves high-order worst-case asymptotic optimality. Finally, numerical experiments from both synthetic and real data corroborate our theore
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexiAST&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#26631;&#20934;AST&#27169;&#22411;&#25552;&#20379;&#34917;&#19969;&#22823;&#23567;&#30340;&#28789;&#27963;&#24615;&#65292;&#20351;&#20854;&#22312;&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#19982;&#21508;&#31181;&#19981;&#21516;&#30340;&#34917;&#19969;&#22823;&#23567;&#19968;&#36215;&#24037;&#20316;&#65292;&#32780;&#26080;&#38656;&#25913;&#21464;&#26550;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FlexiAST&#22312;&#20445;&#25345;&#35780;&#20272;&#33021;&#21147;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#19982;&#26631;&#20934;AST&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09286</link><description>&lt;p&gt;
FlexiAST: AST&#25152;&#38656;&#30340;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
FlexiAST: Flexibility is What AST Needs. (arXiv:2307.09286v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09286
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexiAST&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#26631;&#20934;AST&#27169;&#22411;&#25552;&#20379;&#34917;&#19969;&#22823;&#23567;&#30340;&#28789;&#27963;&#24615;&#65292;&#20351;&#20854;&#22312;&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#19982;&#21508;&#31181;&#19981;&#21516;&#30340;&#34917;&#19969;&#22823;&#23567;&#19968;&#36215;&#24037;&#20316;&#65292;&#32780;&#26080;&#38656;&#25913;&#21464;&#26550;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FlexiAST&#22312;&#20445;&#25345;&#35780;&#20272;&#33021;&#21147;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#19982;&#26631;&#20934;AST&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20026;&#38899;&#39057;&#20809;&#35889;&#22270;&#21464;&#25442;&#22120;&#65288;AST&#65289;&#25552;&#20379;&#34917;&#19969;&#22823;&#23567;&#30340;&#28789;&#27963;&#24615;&#12290;&#26368;&#36817;&#30340;AST&#25216;&#26415;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#22522;&#20110;&#38899;&#39057;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;AST&#30340;&#24615;&#33021;&#22312;&#20351;&#29992;&#19981;&#21516;&#30340;&#34917;&#19969;&#22823;&#23567;&#36827;&#34892;&#35780;&#20272;&#26102;&#20250;&#24613;&#21095;&#19979;&#38477;&#65292;&#32780;&#38750;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#34917;&#19969;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;AST&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#20197;&#36866;&#24212;&#34917;&#19969;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#22312;&#19981;&#25913;&#21464;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#20026;&#26631;&#20934;AST&#27169;&#22411;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#20351;&#20854;&#22312;&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#19982;&#21508;&#31181;&#19981;&#21516;&#30340;&#34917;&#19969;&#22823;&#23567;&#19968;&#36215;&#24037;&#20316;- FlexiAST&#12290;&#36825;&#31181;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#21482;&#26159;&#21033;&#29992;&#38543;&#26426;&#34917;&#19969;&#22823;&#23567;&#36873;&#25321;&#21644;&#34917;&#19969;&#22823;&#23567;&#35843;&#25972;&#21644;&#20301;&#32622;&#23884;&#20837;&#26435;&#37325;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FlexiAST&#22312;&#20445;&#25345;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#21508;&#31181;&#34917;&#19969;&#22823;&#23567;&#30340;&#35780;&#20272;&#33021;&#21147;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#19982;&#26631;&#20934;AST&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this work is to give patch-size flexibility to Audio Spectrogram Transformers (AST). Recent advancements in ASTs have shown superior performance in various audio-based tasks. However, the performance of standard ASTs degrades drastically when evaluated using different patch sizes from that used during training. As a result, AST models are typically re-trained to accommodate changes in patch sizes. To overcome this limitation, this paper proposes a training procedure to provide flexibility to standard AST models without architectural changes, allowing them to work with various patch sizes at the inference stage - FlexiAST. This proposed training approach simply utilizes random patch size selection and resizing of patch and positional embedding weights. Our experiments show that FlexiAST gives similar performance to standard AST models while maintaining its evaluation ability at various patch sizes on different datasets for audio classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#26469;&#22788;&#29702;&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35757;&#32451;&#36229;&#31435;&#26041;&#20307;&#27169;&#22411;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#33719;&#24471;&#26356;&#20248;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09269</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#30340;&#20998;&#31867;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
End-to-End Neural Network Training for Hyperbox-Based Classification. (arXiv:2307.09269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#26469;&#22788;&#29702;&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35757;&#32451;&#36229;&#31435;&#26041;&#20307;&#27169;&#22411;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#33719;&#24471;&#26356;&#20248;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#30340;&#20998;&#31867;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#23545;&#25968;&#25454;&#30340;&#20915;&#31574;&#34987;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#27491;&#20132;&#30340;&#22810;&#32500;&#31435;&#26041;&#20307;&#65288;&#21363;&#36229;&#31435;&#26041;&#20307;&#65289;&#65292;&#36825;&#20123;&#31435;&#26041;&#20307;&#36890;&#24120;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#26131;&#35835;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#19981;&#20877;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#29616;&#20170;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#38754;&#20020;&#30340;&#19981;&#26029;&#22686;&#21152;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#20998;&#31867;&#30340;&#23436;&#20840;&#21487;&#24494;&#20998;&#26694;&#26550;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#36229;&#31435;&#26041;&#20307;&#27169;&#22411;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#39640;&#25928;&#35757;&#32451;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#33719;&#24471;&#26356;&#20248;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbox-based classification has been seen as a promising technique in which decisions on the data are represented as a series of orthogonal, multidimensional boxes (i.e., hyperboxes) that are often interpretable and human-readable. However, existing methods are no longer capable of efficiently handling the increasing volume of data many application domains face nowadays. We address this gap by proposing a novel, fully differentiable framework for hyperbox-based classification via neural networks. In contrast to previous work, our hyperbox models can be efficiently trained in an end-to-end fashion, which leads to significantly reduced training times and superior classification results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31227;&#21160;&#29992;&#25143;&#22312;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#36890;&#20449;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#35843;&#24230;&#21644;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#35757;&#32451;&#24310;&#36831;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24314;&#31435;&#23454;&#38469;&#30340;&#29992;&#25143;&#31227;&#21160;&#27169;&#22411;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;&#32508;&#21512;&#32771;&#34385;&#29992;&#25143;&#36873;&#25321;&#12289;&#22522;&#31449;&#20998;&#37197;&#21644;&#24102;&#23485;&#20998;&#37197;&#31561;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2307.09263</link><description>&lt;p&gt;
&#20302;&#24310;&#36831;&#32852;&#21512;&#29992;&#25143;&#35843;&#24230;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#31227;&#21160;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mobility-Aware Joint User Scheduling and Resource Allocation for Low Latency Federated Learning. (arXiv:2307.09263v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31227;&#21160;&#29992;&#25143;&#22312;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#36890;&#20449;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#35843;&#24230;&#21644;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#35757;&#32451;&#24310;&#36831;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24314;&#31435;&#23454;&#38469;&#30340;&#29992;&#25143;&#31227;&#21160;&#27169;&#22411;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;&#32508;&#21512;&#32771;&#34385;&#29992;&#25143;&#36873;&#25321;&#12289;&#22522;&#31449;&#20998;&#37197;&#21644;&#24102;&#23485;&#20998;&#37197;&#31561;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21487;&#20197;&#36890;&#36807;&#22312;&#29992;&#25143;&#31471;&#36827;&#34892;&#36845;&#20195;&#30340;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#21644;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#31471;&#36827;&#34892;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#26469;&#33719;&#24471;&#20849;&#20139;&#27169;&#22411;&#65292;&#20174;&#32780;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#22312;FL&#31995;&#32479;&#20013;&#65292;&#31227;&#21160;&#29992;&#25143;&#36890;&#36807;&#26080;&#32447;&#20449;&#36947;&#19982;&#22522;&#31449;&#65288;BSs&#65289;&#36890;&#20449;&#65292;&#29992;&#25143;&#30340;&#31227;&#21160;&#24615;&#21487;&#33021;&#23548;&#33268;&#21487;&#38752;&#24615;&#19979;&#38477;&#65292;&#20174;&#32780;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#21482;&#30740;&#31350;&#20102;&#38745;&#24577;&#22330;&#26223;&#25110;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#29992;&#25143;&#20301;&#32622;&#65292;&#26410;&#33021;&#25429;&#25417;&#21040;&#29616;&#23454;&#32593;&#32476;&#20013;&#30340;&#31227;&#21160;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#29992;&#25143;&#31227;&#21160;&#27169;&#22411;&#65292;&#36328;&#22810;&#20010;&#22522;&#31449;&#36827;&#34892;FL&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#25143;&#35843;&#24230;&#21644;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#22312;&#36890;&#20449;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#35757;&#32451;&#24310;&#36831;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#20855;&#26377;&#29992;&#25143;&#31227;&#21160;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#29992;&#25143;&#36873;&#25321;&#12289;&#32473;&#29992;&#25143;&#20998;&#37197;&#30340;BS&#21644;&#24102;&#23485;&#20998;&#37197;&#65292;&#20197;&#26368;&#23567;&#21270;&#35757;&#32451;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an efficient distributed machine learning approach, Federated learning (FL) can obtain a shared model by iterative local model training at the user side and global model aggregating at the central server side, thereby protecting privacy of users. Mobile users in FL systems typically communicate with base stations (BSs) via wireless channels, where training performance could be degraded due to unreliable access caused by user mobility. However, existing work only investigates a static scenario or random initialization of user locations, which fail to capture mobility in real-world networks. To tackle this issue, we propose a practical model for user mobility in FL across multiple BSs, and develop a user scheduling and resource allocation method to minimize the training delay with constrained communication resources. Specifically, we first formulate an optimization problem with user mobility that jointly considers user selection, BS assignment to users, and bandwidth allocation to min
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#33258;&#36866;&#24212;&#23398;&#20064;&#28388;&#27874;&#22120;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#28857;&#20113;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#20351;&#24471;&#25345;&#20037;&#21516;&#35843;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09259</link><description>&lt;p&gt;
&#22522;&#20110;&#25345;&#20037;&#21516;&#35843;&#30340;&#33258;&#36866;&#24212;&#25299;&#25169;&#29305;&#24449;&#65306;&#28857;&#20113;&#30340;&#28388;&#27874;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds. (arXiv:2307.09259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#33258;&#36866;&#24212;&#23398;&#20064;&#28388;&#27874;&#22120;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#28857;&#20113;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#20351;&#24471;&#25345;&#20037;&#21516;&#35843;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#24418;&#29366;&#35782;&#21035;&#21644;&#26448;&#26009;&#31185;&#23398;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#25552;&#39640;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#24120;&#20250;&#24341;&#20837;&#20840;&#23616;&#25299;&#25169;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#25552;&#21462;&#12290;&#22312;&#23545;&#28857;&#20113;&#36827;&#34892;&#25345;&#20037;&#21516;&#35843;&#35745;&#31639;&#26102;&#65292;&#25105;&#20204;&#38656;&#35201;&#36873;&#25321;&#19968;&#20010;&#28857;&#20113;&#30340;&#28388;&#27874;&#22120;&#65292;&#21363;&#19968;&#20010;&#36880;&#28176;&#22686;&#21152;&#30340;&#31354;&#38388;&#24207;&#21015;&#12290;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19982;&#25345;&#20037;&#21516;&#35843;&#30340;&#32467;&#21512;&#21463;&#21040;&#28388;&#27874;&#22120;&#36873;&#25321;&#30340;&#24433;&#21709;&#24456;&#22823;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#26681;&#25454;&#25968;&#25454;&#21644;&#20219;&#21153;&#36827;&#34892;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#33258;&#36866;&#24212;&#23398;&#20064;&#28388;&#27874;&#22120;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#20351;&#24471;&#24471;&#21040;&#30340;&#25345;&#20037;&#21516;&#35843;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#26377;&#38480;&#32500;&#36817;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning for point clouds has been attracting much attention, with many applications in various fields, such as shape recognition and material science. To enhance the accuracy of such machine learning methods, it is known to be effective to incorporate global topological features, which are typically extracted by persistent homology. In the calculation of persistent homology for a point cloud, we need to choose a filtration for the point clouds, an increasing sequence of spaces. Because the performance of machine learning methods combined with persistent homology is highly affected by the choice of a filtration, we need to tune it depending on data and tasks. In this paper, we propose a framework that learns a filtration adaptively with the use of neural networks. In order to make the resulting persistent homology isometry-invariant, we develop a neural network architecture with such invariance. Additionally, we theoretically show a finite-dimensional approximation result that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09254</link><description>&lt;p&gt;
&#29992;&#20110;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models. (arXiv:2307.09254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#22686;&#24378;&#27169;&#22411;&#21487;&#20449;&#24230;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#30001;&#20110;&#23545;&#29983;&#25104;&#34394;&#26500;&#20107;&#23454;&#30340;&#25285;&#24551;&#65292;&#26368;&#36817;&#20852;&#36215;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#29305;&#21035;&#24378;&#35843;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#39044;&#27979;&#38598;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#30340;&#26041;&#24335;&#37327;&#21270;GLM&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#39044;&#27979;&#38598;&#27169;&#22411;&#36890;&#36807;&#26631;&#37327;&#20540;&#21442;&#25968;&#21270;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#39044;&#27979;&#38598;&#65292;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20294;&#20173;&#28385;&#36275;PAC&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;&#22235;&#31181;&#31867;&#22411;&#30340;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#20845;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#19978;&#23637;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty learning and quantification of models are crucial tasks to enhance the trustworthiness of the models. Importantly, the recent surge of generative language models (GLMs) emphasizes the need for reliable uncertainty quantification due to the concerns on generating hallucinated facts. In this paper, we propose to learn neural prediction set models that comes with the probably approximately correct (PAC) guarantee for quantifying the uncertainty of GLMs. Unlike existing prediction set models, which are parameterized by a scalar value, we propose to parameterize prediction sets via neural networks, which achieves more precise uncertainty quantification but still satisfies the PAC guarantee. We demonstrate the efficacy of our method on four types of language datasets and six types of models by showing that our method improves the quantified uncertainty by $63\%$ on average, compared to a standard baseline method.
&lt;/p&gt;</description></item><item><title>UniTabE&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09249</link><description>&lt;p&gt;
UniTabE: &#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data. (arXiv:2307.09249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09249
&lt;/p&gt;
&lt;p&gt;
UniTabE&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26126;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31361;&#30772;&#24615;&#24433;&#21709;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#23041;&#21147;&#25193;&#23637;&#21040;&#20256;&#32479;&#34987;&#24573;&#35270;&#30340;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#65292;&#35813;&#39046;&#22495;&#30001;&#20110;&#19981;&#21516;&#20219;&#21153;&#22266;&#26377;&#30340;&#20247;&#22810;&#34920;&#26684;&#27169;&#24335;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#24037;&#20316;&#30340;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#22260;&#32469;&#24322;&#26500;&#34920;&#26684;&#32467;&#26500;&#30340;&#36866;&#24212;&#24615;&#12289;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#21327;&#35758;&#30340;&#24314;&#31435;&#12289;&#23398;&#21040;&#30340;&#30693;&#35782;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#27867;&#21270;&#21644;&#21487;&#20256;&#36882;&#24615;&#12289;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#22686;&#37327;&#21015;&#30340;&#32435;&#20837;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UniTabE&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20197;&#19968;&#33268;&#30340;&#26041;&#24335;&#22788;&#29702;&#34920;&#26684;&#65292;&#25670;&#33073;&#20102;&#29305;&#23450;&#34920;&#26684;&#32467;&#26500;&#24378;&#21152;&#30340;&#32422;&#26463;&#12290;UniTabE&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#23545;&#27599;&#20010;&#22522;&#26412;&#34920;&#26684;&#36827;&#34892;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic tab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#20013;&#24212;&#29992;BERT&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#21518;&#22788;&#29702;&#28155;&#21152;&#26085;&#27874;&#21160;&#24615;&#20351;&#39044;&#27979;&#32467;&#26524;&#26356;&#31526;&#21512;&#27599;&#26085;&#21608;&#26399;&#24615;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#22312;&#30334;&#24230;KDD Cup 2022&#20013;&#21462;&#24471;&#20102;&#31532;&#19977;&#21517;&#12290;</title><link>http://arxiv.org/abs/2307.09248</link><description>&lt;p&gt;
BERT&#22312;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;-&#30334;&#24230;KDD Cup 2022&#20013;Teletraan&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Application of BERT in Wind Power Forecasting-Teletraan's Solution in Baidu KDD Cup 2022. (arXiv:2307.09248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09248
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#20013;&#24212;&#29992;BERT&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#21518;&#22788;&#29702;&#28155;&#21152;&#26085;&#27874;&#21160;&#24615;&#20351;&#39044;&#27979;&#32467;&#26524;&#26356;&#31526;&#21512;&#27599;&#26085;&#21608;&#26399;&#24615;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#22312;&#30334;&#24230;KDD Cup 2022&#20013;&#21462;&#24471;&#20102;&#31532;&#19977;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#39118;&#33021;&#22240;&#20854;&#22312;&#30899;&#20013;&#21644;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#24403;&#39118;&#33021;&#38598;&#25104;&#21040;&#30005;&#21147;&#32593;&#32476;&#20013;&#26102;&#65292;&#20934;&#30830;&#30340;&#39044;&#27979;&#23545;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19981;&#21487;&#39044;&#27979;&#30340;&#24615;&#36136;&#21644;&#38271;&#24207;&#21015;&#39044;&#27979;&#20351;&#20854;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BERT&#27169;&#22411;&#22312;&#30334;&#24230;KDD Cup 2022&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#21518;&#22788;&#29702;&#28155;&#21152;&#20102;&#26085;&#27874;&#21160;&#24615;&#65292;&#20351;&#39044;&#27979;&#32467;&#26524;&#31526;&#21512;&#27599;&#26085;&#21608;&#26399;&#24615;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;2490&#25903;&#38431;&#20237;&#20013;&#33719;&#24471;&#20102;&#31532;&#19977;&#21517;&#12290;&#20195;&#30721;&#24050;&#32463;&#22312;https://github.com/LongxingTan/KDD2022-Baidu&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, wind energy has drawn increasing attention as its important role in carbon neutrality and sustainable development. When wind power is integrated into the power grid, precise forecasting is necessary for the sustainability and security of the system. However, the unpredictable nature and long sequence prediction make it especially challenging. In this technical report, we introduce the BERT model applied for Baidu KDD Cup 2022, and the daily fluctuation is added by post-processing to make the predicted results in line with daily periodicity. Our solution achieves 3rd place of 2490 teams. The code is released athttps://github.com/LongxingTan/KDD2022-Baidu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;NILM&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#35745;&#31639;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#22686;&#24378;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#22312;&#34394;&#25311;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09244</link><description>&lt;p&gt;
&#38754;&#21521;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#21487;&#25345;&#32493;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Sustainable Deep Learning for Multi-Label Classification on NILM. (arXiv:2307.09244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;NILM&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#35745;&#31639;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#22686;&#24378;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#22312;&#34394;&#25311;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#65288;NILM&#65289;&#26159;&#20174;&#21333;&#20010;&#35745;&#37327;&#28857;&#33719;&#21462;&#23478;&#24237;&#25110;&#20225;&#19994;&#24635;&#30005;&#21147;&#28040;&#32791;&#30340;&#30005;&#22120;&#32423;&#25968;&#25454;&#30340;&#36807;&#31243;&#12290;&#30005;&#22120;&#32423;&#25968;&#25454;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#38656;&#27714;&#21709;&#24212;&#24212;&#29992;&#12289;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20197;&#21450;&#25552;&#39640;&#33021;&#25928;&#21644;&#20943;&#23569;&#30899;&#36275;&#36857;&#30340;&#24847;&#35782;&#25552;&#39640;&#21644;&#28608;&#21169;&#12290;&#26368;&#36817;&#65292;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#22312;NILM&#20998;&#31867;&#20013;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#65292;&#24182;&#35777;&#26126;&#22312;&#22686;&#38271;&#30340;&#22797;&#26434;&#24615;&#19979;&#23545;NILM&#20998;&#31867;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#38543;&#30528;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#25805;&#20316;&#36807;&#31243;&#20013;&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;DL&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#35745;&#31639;&#21644;&#33021;&#28304;&#25928;&#29575;&#26469;&#22686;&#24378;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20351;&#29992;&#20174;&#27979;&#37327;&#25968;&#25454;&#38598;&#21512;&#25104;&#30340;&#25968;&#25454;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-intrusive load monitoring (NILM) is the process of obtaining appliance-level data from a single metering point, measuring total electricity consumption of a household or a business. Appliance-level data can be directly used for demand response applications and energy management systems as well as for awareness raising and motivation for improvements in energy efficiency and reduction in the carbon footprint. Recently, classical machine learning and deep learning (DL) techniques became very popular and proved as highly effective for NILM classification, but with the growing complexity these methods are faced with significant computational and energy demands during both their training and operation. In this paper, we introduce a novel DL model aimed at enhanced multi-label classification of NILM with improved computation and energy efficiency. We also propose a testing methodology for comparison of different models using data synthesized from the measurement datasets so as to better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36739;&#23569;&#35814;&#32454;&#30340;&#36523;&#20307;&#39592;&#26550;&#19982;&#39640;&#24230;&#35814;&#32454;&#30340;&#25163;&#37096;&#39592;&#26550;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;CNN&#21644;transformers&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#32452;&#35013;&#22330;&#26223;&#20013;&#30340;&#21160;&#20316;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09238</link><description>&lt;p&gt;
&#32467;&#21512;&#25163;&#37096;&#21644;&#36523;&#20307;&#39592;&#26550;&#36827;&#34892;&#35013;&#37197;&#20013;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Fusing Hand and Body Skeletons for Human Action Recognition in Assembly. (arXiv:2307.09238v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36739;&#23569;&#35814;&#32454;&#30340;&#36523;&#20307;&#39592;&#26550;&#19982;&#39640;&#24230;&#35814;&#32454;&#30340;&#25163;&#37096;&#39592;&#26550;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;CNN&#21644;transformers&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#32452;&#35013;&#22330;&#26223;&#20013;&#30340;&#21160;&#20316;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21327;&#20316;&#26426;&#22120;&#20154;&#22312;&#24037;&#19994;&#21046;&#36896;&#39046;&#22495;&#26085;&#30410;&#21463;&#27426;&#36814;&#65292;&#26377;&#25928;&#30340;&#20154;&#26426;&#21327;&#20316;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#21327;&#20316;&#26426;&#22120;&#20154;&#24212;&#35813;&#33021;&#22815;&#35782;&#21035;&#20154;&#20307;&#21160;&#20316;&#65292;&#20197;&#22312;&#32452;&#35013;&#20219;&#21153;&#20013;&#25552;&#20379;&#24110;&#21161;&#24182;&#20570;&#20986;&#33258;&#20027;&#20915;&#31574;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#39592;&#26550;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#36866;&#29992;&#20110;&#19981;&#21516;&#20154;&#21592;&#21644;&#29615;&#22659;&#12290;&#23613;&#31649;&#36523;&#20307;&#39592;&#26550;&#26041;&#27861;&#24191;&#27867;&#29992;&#20110;&#21160;&#20316;&#35782;&#21035;&#65292;&#20294;&#22312;&#35013;&#37197;&#21160;&#20316;&#20013;&#65292;&#24037;&#20154;&#30340;&#25163;&#25351;&#21644;&#25163;&#37096;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#27492;&#21487;&#33021;&#19981;&#22815;&#20934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#36739;&#23569;&#35814;&#32454;&#30340;&#36523;&#20307;&#39592;&#26550;&#19982;&#39640;&#24230;&#35814;&#32454;&#30340;&#25163;&#37096;&#39592;&#26550;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;CNN&#21644;transformers&#65292;&#21518;&#32773;&#29305;&#21035;&#25797;&#38271;&#20351;&#29992;&#27880;&#24847;&#21147;&#20174;&#20004;&#31181;&#39592;&#26550;&#31867;&#22411;&#20013;&#25552;&#21462;&#21644;&#32452;&#21512;&#37325;&#35201;&#20449;&#24687;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#32452;&#35013;&#22330;&#26223;&#20013;&#30340;&#21160;&#20316;&#35782;&#21035;&#25928;&#26524;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As collaborative robots (cobots) continue to gain popularity in industrial manufacturing, effective human-robot collaboration becomes crucial. Cobots should be able to recognize human actions to assist with assembly tasks and act autonomously. To achieve this, skeleton-based approaches are often used due to their ability to generalize across various people and environments. Although body skeleton approaches are widely used for action recognition, they may not be accurate enough for assembly actions where the worker's fingers and hands play a significant role. To address this limitation, we propose a method in which less detailed body skeletons are combined with highly detailed hand skeletons. We investigate CNNs and transformers, the latter of which are particularly adept at extracting and combining important information from both skeleton types using attention. This paper demonstrates the effectiveness of our proposed approach in enhancing action recognition in assembly scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20174;&#35821;&#38899;&#35760;&#24405;&#20013;&#26816;&#27979;&#21897;&#30284;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#21457;&#29616;&#20102;22&#31687;&#30456;&#20851;&#35770;&#25991;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#25552;&#21462;&#38899;&#39057;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.09230</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#26816;&#27979;&#21897;&#30284;&#65306;&#21487;&#22797;&#29616;&#30340;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Detecting Throat Cancer from Speech Signals Using Machine Learning: A Reproducible Literature Review. (arXiv:2307.09230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20174;&#35821;&#38899;&#35760;&#24405;&#20013;&#26816;&#27979;&#21897;&#30284;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#21457;&#29616;&#20102;22&#31687;&#30456;&#20851;&#35770;&#25991;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#25552;&#21462;&#38899;&#39057;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20174;&#35821;&#38899;&#35760;&#24405;&#20013;&#26816;&#27979;&#21897;&#30284;&#30340;&#24403;&#21069;&#25991;&#29486;&#36827;&#34892;&#20102;&#33539;&#22260;&#35780;&#20272;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;22&#31687;&#30456;&#20851;&#35770;&#25991;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#35770;&#25991;&#20998;&#20026;&#20004;&#32452; - &#20061;&#31687;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;13&#31687;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#31867;&#12290;&#36825;&#20123;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;&#26159;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#20998;&#31867;&#20043;&#21069;&#36824;&#20174;&#38899;&#39057;&#20013;&#25552;&#21462;&#20102;&#35768;&#22810;&#29305;&#24449;&#65292;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;&#26159;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#12290;&#22312;&#36825;&#27425;&#25628;&#32034;&#20013;&#26410;&#25214;&#21040;&#20219;&#20309;&#24102;&#26377;&#20195;&#30721;&#24211;&#30340;&#35770;&#25991;&#65292;&#22240;&#27492;&#26080;&#27861;&#22797;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20195;&#30721;&#24211;&#26469;&#35757;&#32451;&#33258;&#24049;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22810;&#31867;&#21035;&#38382;&#39064;&#19978;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#65292;&#23558;&#19977;&#31181;&#30149;&#29702;&#21644;&#20581;&#24247;&#23545;&#29031;&#36827;&#34892;&#20998;&#31867;&#12290;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;53.54%&#30340;&#21152;&#26435;&#24179;&#22343;&#21484;&#22238;&#29575;&#12289;83.14%&#30340;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we perform a scoping review of the current literature on the detection of throat cancer from speech recordings using machine learning and artificial intelligence. We find 22 papers within this area and discuss their methods and results. We split these papers into two groups - nine performing binary classification, and 13 performing multi-class classification. The papers present a range of methods with neural networks being most commonly implemented. Many features are also extracted from the audio before classification, with the most common bring mel-frequency cepstral coefficients. None of the papers found in this search have associated code repositories and as such are not reproducible. Therefore, we create a publicly available code repository of our own classifiers. We use transfer learning on a multi-class problem, classifying three pathologies and healthy controls. Using this technique we achieve an unweighted average recall of 53.54%, sensitivity of 83.14%, and specif
&lt;/p&gt;</description></item><item><title>&#36951;&#24536;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#19981;&#20165;&#38480;&#20110;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#19982;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#36951;&#24536;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.09218</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36951;&#24536;&#29616;&#35937;&#30340;&#20840;&#38754;&#35843;&#26597;&#65306;&#36229;&#36234;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09218
&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#19981;&#20165;&#38480;&#20110;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#19982;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#36951;&#24536;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#25351;&#30340;&#26159;&#20808;&#21069;&#33719;&#21462;&#30340;&#20449;&#24687;&#25110;&#30693;&#35782;&#30340;&#20007;&#22833;&#25110;&#24694;&#21270;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20851;&#20110;&#36951;&#24536;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;&#36830;&#32493;&#23398;&#20064;&#26041;&#38754;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#36951;&#24536;&#26159;&#19968;&#31181;&#26222;&#36941;&#29616;&#35937;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#20013;&#35266;&#23519;&#21040;&#12290;&#36951;&#24536;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#20363;&#22914;&#30001;&#20110;&#29983;&#25104;&#22120;&#28418;&#31227;&#32780;&#22312;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#20197;&#21450;&#30001;&#20110;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#32780;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26469;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#28041;&#21450;&#21040;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21516;&#26102;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#35843;&#26597;&#37117;&#40664;&#35748;&#35748;&#20026;&#36951;&#24536;&#24635;&#26159;&#26377;&#23475;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#35748;&#20026;&#36951;&#24536;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#20363;&#22914;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#36951;&#24536;&#29616;&#35937;&#65292;
&lt;/p&gt;
&lt;p&gt;
Forgetting refers to the loss or deterioration of previously acquired information or knowledge. While the existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#35745;&#31639;&#36830;&#32493;&#20998;&#24067;&#19979;$d$&#20010;&#36755;&#20837;&#30340;&#26368;&#22823;&#20540;&#25152;&#38656;&#30340;&#32593;&#32476;&#22823;&#23567;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#30028;&#38480;&#21644;&#20998;&#31163;&#32467;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#20026;2&#30340;&#32593;&#32476;&#36817;&#20284;&#26368;&#22823;&#20540;&#20989;&#25968;&#30340;&#26032;&#19979;&#30028;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#28145;&#24230;&#20026;$\mathcal{O}(\log(\log(d)))$&#65292;&#23485;&#24230;&#20026;$\mathcal{O}(d)$&#30340;&#26500;&#36896;&#26469;&#25913;&#21892;&#28145;&#24230;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.09212</link><description>&lt;p&gt;
&#38656;&#35201;&#22810;&#23569;&#20010;&#31070;&#32463;&#20803;&#25165;&#33021;&#36817;&#20284;&#35745;&#31639;&#26368;&#22823;&#20540;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Many Neurons Does it Take to Approximate the Maximum?. (arXiv:2307.09212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#35745;&#31639;&#36830;&#32493;&#20998;&#24067;&#19979;$d$&#20010;&#36755;&#20837;&#30340;&#26368;&#22823;&#20540;&#25152;&#38656;&#30340;&#32593;&#32476;&#22823;&#23567;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#30028;&#38480;&#21644;&#20998;&#31163;&#32467;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#20026;2&#30340;&#32593;&#32476;&#36817;&#20284;&#26368;&#22823;&#20540;&#20989;&#25968;&#30340;&#26032;&#19979;&#30028;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#28145;&#24230;&#20026;$\mathcal{O}(\log(\log(d)))$&#65292;&#23485;&#24230;&#20026;$\mathcal{O}(d)$&#30340;&#26500;&#36896;&#26469;&#25913;&#21892;&#28145;&#24230;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#32593;&#32476;&#20013;&#65292;&#36817;&#20284;&#35745;&#31639;$L_2$&#33539;&#25968;&#19979;&#36830;&#32493;&#20998;&#24067;$d$&#20010;&#36755;&#20837;&#30340;&#26368;&#22823;&#20540;&#25152;&#38656;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36817;&#20284;&#25152;&#38656;&#23485;&#24230;&#30340;&#26032;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#65292;&#20197;&#21450;&#19981;&#21516;&#28145;&#24230;&#20043;&#38388;&#30340;&#28145;&#24230;&#20998;&#31163;&#65292;&#21253;&#25324;&#28145;&#24230;2&#21644;3&#20197;&#21450;&#28145;&#24230;3&#21644;5&#32593;&#32476;&#20043;&#38388;&#30340;&#20998;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#28145;&#24230;&#20026;$\mathcal{O}(\log(\log(d)))$&#65292;&#23485;&#24230;&#20026;$\mathcal{O}(d)$&#30340;&#26500;&#36896;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#26368;&#22823;&#20540;&#20989;&#25968;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#32447;&#24615;&#23485;&#24230;&#38480;&#21046;&#26465;&#20214;&#19979;&#24050;&#30693;&#26368;&#20248;&#30340;&#28145;&#24230;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#28145;&#24230;&#20998;&#31163;&#32467;&#26524;&#36890;&#36807;&#23545;&#22343;&#21248;&#20998;&#24067;&#19979;&#28145;&#24230;&#20026;2&#30340;&#32593;&#32476;&#36817;&#20284;&#26368;&#22823;&#20540;&#20989;&#25968;&#30340;&#26032;&#30340;&#19979;&#30028;&#24471;&#21040;&#65292;&#24182;&#20551;&#35774;&#26435;&#37325;&#22823;&#23567;&#20855;&#26377;&#25351;&#25968;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#36825;&#20010;&#28145;&#24230;&#20026;2&#30340;&#19979;&#30028;&#25552;&#20379;&#32039;&#33268;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the size of a neural network needed to approximate the maximum function over $d$ inputs, in the most basic setting of approximating with respect to the $L_2$ norm, for continuous distributions, for a network that uses ReLU activations. We provide new lower and upper bounds on the width required for approximation across various depths. Our results establish new depth separations between depth 2 and 3, and depth 3 and 5 networks, as well as providing a depth $\mathcal{O}(\log(\log(d)))$ and width $\mathcal{O}(d)$ construction which approximates the maximum function, significantly improving upon the depth requirements of the best previously known bounds for networks with linearly-bounded width. Our depth separation results are facilitated by a new lower bound for depth 2 networks approximating the maximum function over the uniform distribution, assuming an exponential upper bound on the size of the weights. Furthermore, we are able to use this depth 2 lower bound to provide tight
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#65292;&#20197;&#25506;&#27979;&#23545;&#27531;&#38556;&#20154;&#22763;&#30340;&#26126;&#26174;&#20559;&#35265;&#12290;&#30740;&#31350;&#20351;&#29992;&#20559;&#35265;&#35782;&#21035;&#26694;&#26550;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#23545;&#35805;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#27979;&#35797;&#35821;&#26009;&#24211;&#26469;&#37327;&#21270;&#26126;&#26174;&#30340;&#27531;&#38556;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25152;&#30740;&#31350;&#30340;&#27169;&#22411;&#22343;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2307.09209</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#27531;&#38556;&#20027;&#20041;&#65306;&#25506;&#32034;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#26126;&#26174;&#27531;&#38556;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models. (arXiv:2307.09209v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#65292;&#20197;&#25506;&#27979;&#23545;&#27531;&#38556;&#20154;&#22763;&#30340;&#26126;&#26174;&#20559;&#35265;&#12290;&#30740;&#31350;&#20351;&#29992;&#20559;&#35265;&#35782;&#21035;&#26694;&#26550;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#23545;&#35805;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#27979;&#35797;&#35821;&#26009;&#24211;&#26469;&#37327;&#21270;&#26126;&#26174;&#30340;&#27531;&#38556;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25152;&#30740;&#31350;&#30340;&#27169;&#22411;&#22343;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#23545;&#27531;&#38556;&#20154;&#22763;&#30340;&#26126;&#26174;&#20559;&#35265;&#12290;&#25105;&#20204;&#37319;&#29992;&#25200;&#21160;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#20559;&#35265;&#35782;&#21035;&#26694;&#26550;&#65292;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#19982;&#27531;&#38556;&#20154;&#22763;&#30456;&#20851;&#30340;&#23545;&#35805;&#65292;&#29305;&#21035;&#26159;Twitter&#21644;Reddit&#65292;&#22312;&#30495;&#23454;&#31038;&#20132;&#29615;&#22659;&#20013;&#20102;&#35299;&#27531;&#38556;&#20559;&#35265;&#26159;&#22914;&#20309;&#20256;&#25773;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#8220;&#24773;&#24863;&#20013;&#30340;&#20559;&#35265;&#35782;&#21035;&#27979;&#35797;&#8221;&#65288;BITS&#65289;&#35821;&#26009;&#24211;&#65292;&#20197;&#37327;&#21270;&#20219;&#20309;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#26126;&#26174;&#27531;&#38556;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20351;&#29992;BITS&#25581;&#31034;&#20102;&#22235;&#31181;&#24320;&#25918;&#30340;AIaaS&#65288;AI&#21363;&#26381;&#21153;&#65289;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#65288;TextBlob&#65292;VADER&#65292;Google Cloud Natural Language API&#65292;DistilBERT&#65289;&#21644;&#20004;&#31181;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#65288;&#20004;&#20010;&#29256;&#26412;&#30340;Toxic-BERT&#65289;&#20013;&#23384;&#22312;&#26174;&#30528;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD). We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. We then create the \textit{Bias Identification Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT. Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26816;&#27979;&#21464;&#21387;&#22120;&#23558;&#24494;&#38663;&#20107;&#20214;&#26816;&#27979;&#21644;&#28304;&#23450;&#20301;&#32479;&#19968;&#20026;&#19968;&#20010;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23454;&#26102;&#24494;&#38663;&#30417;&#27979;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2307.09207</link><description>&lt;p&gt;
&#20351;&#29992;&#26816;&#27979;&#21464;&#21387;&#22120;&#36827;&#34892;&#32852;&#21512;&#24494;&#38663;&#20107;&#20214;&#26816;&#27979;&#21644;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Joint Microseismic Event Detection and Location with a Detection Transformer. (arXiv:2307.09207v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26816;&#27979;&#21464;&#21387;&#22120;&#23558;&#24494;&#38663;&#20107;&#20214;&#26816;&#27979;&#21644;&#28304;&#23450;&#20301;&#32479;&#19968;&#20026;&#19968;&#20010;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23454;&#26102;&#24494;&#38663;&#30417;&#27979;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#38663;&#20107;&#20214;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#26159;&#24494;&#38663;&#30417;&#27979;&#30340;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#27833;&#34255;&#21050;&#28608;&#21644;&#28436;&#21270;&#36807;&#31243;&#20013;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#22320;&#19979;&#20449;&#24687;&#12290;&#20256;&#32479;&#30340;&#20107;&#20214;&#26816;&#27979;&#21644;&#23450;&#20301;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#25163;&#21160;&#24178;&#39044;&#21644;/&#25110;&#22823;&#37327;&#35745;&#31639;&#65292;&#32780;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26041;&#27861;&#36890;&#24120;&#20998;&#21035;&#35299;&#20915;&#26816;&#27979;&#21644;&#23450;&#20301;&#38382;&#39064;&#65307;&#36825;&#20123;&#38480;&#21046;&#38459;&#30861;&#20102;&#23454;&#26102;&#24494;&#38663;&#30417;&#27979;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20027;&#24178;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#65292;&#23558;&#20107;&#20214;&#26816;&#27979;&#21644;&#28304;&#23450;&#20301;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#26694;&#26550;&#20013;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#20110;&#35760;&#24405;&#30340;&#27874;&#24418;&#19978;&#12290;&#35813;&#32593;&#32476;&#20197;&#27169;&#25311;&#22810;&#20010;&#24494;&#38663;&#20107;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#20107;&#20214;&#23545;&#24212;&#20110;&#30097;&#20284;&#24494;&#38663;&#27963;&#21160;&#21306;&#22495;&#20013;&#30340;&#38543;&#26426;&#28304;&#20301;&#32622;&#12290;&#22312;SEAM Time Lapse&#27169;&#22411;&#30340;&#20108;&#32500;&#21078;&#38754;&#19978;&#36827;&#34892;&#20102;&#21512;&#25104;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Microseismic event detection and location are two primary components in microseismic monitoring, which offers us invaluable insights into the subsurface during reservoir stimulation and evolution. Conventional approaches for event detection and location often suffer from manual intervention and/or heavy computation, while current machine learning-assisted approaches typically address detection and location separately; such limitations hinder the potential for real-time microseismic monitoring. We propose an approach to unify event detection and source location into a single framework by adapting a Convolutional Neural Network backbone and an encoder-decoder Transformer with a set-based Hungarian loss, which is applied directly to recorded waveforms. The proposed network is trained on synthetic data simulating multiple microseismic events corresponding to random source locations in the area of suspected microseismic activities. A synthetic test on a 2D profile of the SEAM Time Lapse mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TRADYN&#30340;&#27010;&#29575;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#22312;&#33258;&#20027;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#30340;&#21464;&#21270;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#30340;&#20108;&#32500;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38271;&#35270;&#31243;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.09206</link><description>&lt;p&gt;
&#24102;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21160;&#21147;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#26465;&#20214;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model. (arXiv:2307.09206v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TRADYN&#30340;&#27010;&#29575;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#22312;&#33258;&#20027;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#30340;&#21464;&#21270;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#30340;&#20108;&#32500;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38271;&#35270;&#31243;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#23548;&#33322;&#29615;&#22659;&#20013;&#65292;&#22810;&#20010;&#21442;&#25968;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#22320;&#24418;&#29305;&#24615;&#22914;&#25705;&#25830;&#31995;&#25968;&#21487;&#33021;&#20250;&#26681;&#25454;&#26426;&#22120;&#20154;&#30340;&#20301;&#32622;&#32780;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#20154;&#30340;&#21160;&#21147;&#23398;&#21487;&#33021;&#20250;&#22240;&#19981;&#21516;&#36127;&#36733;&#12289;&#31995;&#32479;&#36136;&#37327;&#21464;&#21270;&#12289;&#30952;&#25439;&#31561;&#21407;&#22240;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#20174;&#32780;&#25913;&#21464;&#25191;&#34892;&#22120;&#22686;&#30410;&#25110;&#20851;&#33410;&#25705;&#25830;&#21147;&#12290;&#33258;&#20027;&#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36866;&#24212;&#36825;&#20123;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#31216;&#20026;TRADYN&#65292;&#23427;&#33021;&#22815;&#36866;&#24212;&#19978;&#36848;&#21464;&#21270;&#12290;&#23427;&#22522;&#20110;&#22522;&#20110;&#31070;&#32463;&#36807;&#31243;&#30340;&#20803;&#23398;&#20064;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#30340;&#20108;&#32500;&#23548;&#33322;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#31867;&#20284;&#33258;&#34892;&#36710;&#30340;&#26426;&#22120;&#20154;&#21644;&#20855;&#26377;&#31354;&#38388;&#21464;&#21270;&#25705;&#25830;&#31995;&#25968;&#30340;&#19981;&#21516;&#22320;&#24418;&#24067;&#23616;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#19982;&#38750;&#33258;&#36866;&#24212;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38271;&#35270;&#31243;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous navigation settings, several quantities can be subject to variations. Terrain properties such as friction coefficients may vary over time depending on the location of the robot. Also, the dynamics of the robot may change due to, e.g., different payloads, changing the system's mass, or wear and tear, changing actuator gains or joint friction. An autonomous agent should thus be able to adapt to such variations. In this paper, we develop a novel probabilistic, terrain- and robot-aware forward dynamics model, termed TRADYN, which is able to adapt to the above-mentioned variations. It builds on recent advances in meta-learning forward dynamics models based on Neural Processes. We evaluate our method in a simulated 2D navigation setting with a unicycle-like robot and different terrain layouts with spatially varying friction coefficients. In our experiments, the proposed model exhibits lower prediction error for the task of long-horizon trajectory prediction, compared to non-ada
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DAFT-RL&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#23398;&#20064;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#25552;&#21462;&#23545;&#35937;&#65292;&#24182;&#23398;&#20064;&#23545;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#21644;&#25512;&#26029;&#20854;&#38544;&#21547;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#23545;&#35937;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.09205</link><description>&lt;p&gt;
&#23398;&#20064;&#21160;&#24577;&#23646;&#24615;&#22240;&#23376;&#19990;&#30028;&#27169;&#22411;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#23545;&#35937;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Dynamic Attribute-factored World Models for Efficient Multi-object Reinforcement Learning. (arXiv:2307.09205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DAFT-RL&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#23398;&#20064;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#25552;&#21462;&#23545;&#35937;&#65292;&#24182;&#23398;&#20064;&#23545;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#21644;&#25512;&#26029;&#20854;&#38544;&#21547;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#23545;&#35937;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#26234;&#33021;&#20307;&#38656;&#35201;&#23398;&#20064;&#19982;&#35768;&#22810;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35937;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#32452;&#21512;&#21644;&#25968;&#37327;&#12290;&#36890;&#24120;&#65292;&#19968;&#20010;&#20219;&#21153;&#26159;&#20197;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#30340;&#32452;&#21512;&#65288;&#20363;&#22914;&#22534;&#21472;&#26041;&#22359;&#65289;&#12290;&#36825;&#20123;&#37117;&#26159;&#32452;&#25104;&#27867;&#21270;&#30340;&#20363;&#23376;&#65292;&#36890;&#36807;&#32452;&#21512;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#21033;&#29992;&#23545;&#35937;&#20998;&#35299;&#34920;&#31034;&#21644;&#23618;&#27425;&#25277;&#35937;&#21487;&#20197;&#25913;&#21892;&#26679;&#26412;&#25928;&#29575;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23545;&#35937;&#23646;&#24615;&#26041;&#38754;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20998;&#35299;&#30340;&#22909;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;Dynamic Attribute FacTored RL (DAFT-RL)&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;DAFT-RL&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#23398;&#20064;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#25552;&#21462;&#23545;&#35937;&#12290;&#25105;&#20204;&#23398;&#20064;&#23545;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25512;&#26029;&#23427;&#20204;&#30340;&#38544;&#21547;&#21442;&#25968;&#12290;&#23545;&#20110;&#27599;&#20010;&#23545;&#35937;&#31867;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#31867;&#27169;&#26495;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many reinforcement learning tasks, the agent has to learn to interact with many objects of different types and generalize to unseen combinations and numbers of objects. Often a task is a composition of previously learned tasks (e.g. block stacking). These are examples of compositional generalization, in which we compose object-centric representations to solve complex tasks. Recent works have shown the benefits of object-factored representations and hierarchical abstractions for improving sample efficiency in these settings. On the other hand, these methods do not fully exploit the benefits of factorization in terms of object attributes. In this paper, we address this opportunity and introduce the Dynamic Attribute FacTored RL (DAFT-RL) framework. In DAFT-RL, we leverage object-centric representation learning to extract objects from visual inputs. We learn to classify them in classes and infer their latent parameters. For each class of object, we learn a class template graph that des
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#20998;&#31867;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;&#19981;&#21516;&#23478;&#26063;&#30340;32&#31181;&#32534;&#30721;&#22120;&#37197;&#32622;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#20197;&#21450;36&#31181;&#23454;&#39564;&#22240;&#32032;&#21644;50&#20010;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#23454;&#39564;&#22240;&#32032;&#21644;&#32858;&#21512;&#31574;&#30053;&#23545;&#22522;&#20934;&#30740;&#31350;&#32467;&#35770;&#30340;&#28145;&#36828;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09191</link><description>&lt;p&gt;
&#29992;&#20110;&#20108;&#20998;&#31867;&#30340;&#20998;&#31867;&#32534;&#30721;&#22120;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A benchmark of categorical encoders for binary classification. (arXiv:2307.09191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#20998;&#31867;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;&#19981;&#21516;&#23478;&#26063;&#30340;32&#31181;&#32534;&#30721;&#22120;&#37197;&#32622;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#20197;&#21450;36&#31181;&#23454;&#39564;&#22240;&#32032;&#21644;50&#20010;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#23454;&#39564;&#22240;&#32032;&#21644;&#32858;&#21512;&#31574;&#30053;&#23545;&#22522;&#20934;&#30740;&#31350;&#32467;&#35770;&#30340;&#28145;&#36828;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#32534;&#30721;&#22120;&#23558;&#20998;&#31867;&#29305;&#24449;&#36716;&#21270;&#20026;&#25968;&#23383;&#34920;&#31034;&#65292;&#23545;&#20110;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#30001;&#20110;&#36873;&#25321;&#26377;&#38480;&#30340;&#32534;&#30721;&#22120;&#12289;&#23454;&#39564;&#22240;&#32032;&#21644;&#25968;&#25454;&#38598;&#65292;&#32570;&#20047;&#26222;&#36866;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#32467;&#26524;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#20998;&#31867;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;&#26469;&#33258;&#19981;&#21516;&#23478;&#26063;&#30340;32&#31181;&#32534;&#30721;&#22120;&#37197;&#32622;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#20197;&#21450;36&#31181;&#23454;&#39564;&#22240;&#32032;&#21644;50&#20010;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#12290;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#23454;&#39564;&#22240;&#32032;&#21644;&#32858;&#21512;&#31574;&#30053;&#23545;&#22522;&#20934;&#30740;&#31350;&#32467;&#35770;&#30340;&#28145;&#36828;&#24433;&#21709;&#65292;&#36825;&#26159;&#20197;&#21069;&#30340;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#24573;&#35270;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Categorical encoders transform categorical features into numerical representations that are indispensable for a wide range of machine learning models. Existing encoder benchmark studies lack generalizability because of their limited choice of (1) encoders, (2) experimental factors, and (3) datasets. Additionally, inconsistencies arise from the adoption of varying aggregation strategies. This paper is the most comprehensive benchmark of categorical encoders to date, including an extensive evaluation of 32 configurations of encoders from diverse families, with 36 combinations of experimental factors, and on 50 datasets. The study shows the profound influence of dataset selection, experimental factors, and aggregation strategies on the benchmark's conclusions -- aspects disregarded in previous encoder benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#23545;&#20110;&#22312;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;&#24322;&#26500;&#35774;&#22791;&#19978;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#35745;&#31639;&#24322;&#26500;&#24615;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#35774;&#22791;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#20294;&#19981;&#20844;&#24320;&#31169;&#26377;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.09182</link><description>&lt;p&gt;
&#38754;&#21521;&#35745;&#31639;&#21463;&#38480;&#24322;&#26500;&#35774;&#22791;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for Computationally-Constrained Heterogeneous Devices: A Survey. (arXiv:2307.09182v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#23545;&#20110;&#22312;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;&#24322;&#26500;&#35774;&#22791;&#19978;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#35745;&#31639;&#24322;&#26500;&#24615;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#35774;&#22791;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#20294;&#19981;&#20844;&#24320;&#31169;&#26377;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#26234;&#33021;&#35774;&#22791;&#65292;&#22914;&#29289;&#32852;&#32593;&#35774;&#22791;&#65292;&#37096;&#32626;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20219;&#21153;&#36716;&#31227;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#25552;&#39640;&#29992;&#25143;&#38544;&#31169;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20165;&#20351;&#29992;&#21333;&#20010;&#35774;&#22791;&#19978;&#30340;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#24456;&#38590;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#12290;&#32852;&#37030;&#23398;&#20064;&#34987;&#24341;&#20837;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#20294;&#19981;&#20844;&#24320;&#35774;&#22791;&#30340;&#31169;&#26377;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#22312;&#36890;&#20449;&#24320;&#38144;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#38544;&#31169;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#30456;&#20851;&#30340;&#20351;&#29992;&#26696;&#20363;&#20013;&#65292;&#24212;&#29992;&#22522;&#32447;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;&#21644;&#22909;&#22788;&#21463;&#21040;&#38480;&#21046;&#65292;&#21407;&#22240;&#26159;&#29615;&#22659;&#20013;&#23384;&#22312;&#24322;&#26500;&#24615;&#12290;&#26412;&#35843;&#30740;&#27010;&#36848;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#38656;&#35201;&#20811;&#26381;&#30340;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21442;&#19982;&#26041;&#20013;&#30340;&#35745;&#31639;&#24322;&#26500;&#24615;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
With an increasing number of smart devices like internet of things (IoT) devices deployed in the field, offloadingtraining of neural networks (NNs) to a central server becomes more and more infeasible. Recent efforts toimprove users' privacy have led to on-device learning emerging as an alternative. However, a model trainedonly on a single device, using only local data, is unlikely to reach a high accuracy. Federated learning (FL)has been introduced as a solution, offering a privacy-preserving trade-off between communication overheadand model accuracy by sharing knowledge between devices but disclosing the devices' private data. Theapplicability and the benefit of applying baseline FL are, however, limited in many relevant use cases dueto the heterogeneity present in such environments. In this survey, we outline the heterogeneity challengesFL has to overcome to be widely applicable in real-world applications. We especially focus on the aspect ofcomputation heterogeneity among the parti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32957;&#33258;&#32452;&#35013;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#22823;&#37327;&#32957;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24207;&#21015;&#21644;&#22270;&#24418;&#30340;&#32534;&#30721;&#26041;&#24335;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#23545;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.09169</link><description>&lt;p&gt;
&#39640;&#25928;&#39044;&#27979;&#32957;&#33258;&#32452;&#35013;&#30340;&#24207;&#21015;&#21644;&#22270;&#24418;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Efficient Prediction of Peptide Self-assembly through Sequential and Graphical Encoding. (arXiv:2307.09169v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32957;&#33258;&#32452;&#35013;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#22823;&#37327;&#32957;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24207;&#21015;&#21644;&#22270;&#24418;&#30340;&#32534;&#30721;&#26041;&#24335;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#23545;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#32957;&#30340;&#37325;&#35201;&#21457;&#23637;&#21644;&#24066;&#22330;&#28508;&#21147;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#39044;&#27979;&#21508;&#31181;&#32957;&#23646;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#30740;&#31350;&#29190;&#28856;&#24615;&#22686;&#38271;&#12290;&#20998;&#23376;&#21160;&#21147;&#23398;&#20351;&#24471;&#25910;&#38598;&#22823;&#37327;&#32957;&#25968;&#25454;&#38598;&#25104;&#20026;&#21487;&#33021;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#21487;&#38752;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;AI&#36741;&#21161;&#32957;&#30456;&#20851;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#30340;&#32957;&#32534;&#30721;&#32570;&#20047;&#31995;&#32479;&#20998;&#26512;&#65292;&#36825;&#25104;&#20026;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#32039;&#36843;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#24040;&#22823;&#27169;&#25311;&#32957;&#33258;&#32452;&#35013;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;62,000&#20010;&#30001;&#31895;&#31890;&#24230;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;CGMD&#65289;&#29983;&#25104;&#30340;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#65288;RNN&#65292;LSTM&#21644;Transformer&#65289;&#21644;&#32467;&#26500;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;GCN&#65292;GAT&#21644;GraphSA&#65289;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23558;&#27688;&#22522;&#37240;&#32534;&#30721;&#20026;&#24207;&#21015;&#21644;&#20998;&#23376;&#22270;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an explosion of research on the application of deep learning to the prediction of various peptide properties, due to the significant development and market potential of peptides. Molecular dynamics has enabled the efficient collection of large peptide datasets, providing reliable training data for deep learning. However, the lack of systematic analysis of the peptide encoding, which is essential for AI-assisted peptide-related tasks, makes it an urgent problem to be solved for the improvement of prediction accuracy. To address this issue, we first collect a high-quality, colossal simulation dataset of peptide self-assembly containing over 62,000 samples generated by coarse-grained molecular dynamics (CGMD). Then, we systematically investigate the effect of peptide encoding of amino acids into sequences and molecular graphs using state-of-the-art sequential (i.e., RNN, LSTM, and Transformer) and structural deep learning models (i.e., GCN, GAT, and GraphSA
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#20449;&#36182;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;TrustDD&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#20869;&#37096;&#20998;&#24067;&#65288;InD&#65289;&#20998;&#31867;&#21644;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#31934;&#28860;&#20026;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#20449;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09165</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#20449;&#36182;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Dataset Distillation. (arXiv:2307.09165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#20449;&#36182;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;TrustDD&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#20869;&#37096;&#20998;&#24067;&#65288;InD&#65289;&#20998;&#31867;&#21644;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#31934;&#28860;&#20026;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#20449;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#26102;&#65292;&#25928;&#29575;&#21644;&#21487;&#20449;&#36182;&#24615;&#26159;&#20004;&#20010;&#27704;&#24658;&#30340;&#36861;&#27714;&#12290;&#23601;&#25928;&#29575;&#32780;&#35328;&#65292;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#33268;&#21147;&#20110;&#36890;&#36807;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#31934;&#28860;&#20026;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#38598;&#20013;&#20110;&#22312;&#23553;&#38381;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#20869;&#37096;&#20998;&#24067;&#65288;InD&#65289;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#26679;&#26412;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;OOD&#26816;&#27979;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#20449;&#36182;&#24615;&#65292;&#22312;&#23436;&#25972;&#25968;&#25454;&#35774;&#32622;&#19979;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#39318;&#27425;&#21516;&#26102;&#32771;&#34385;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;&#21487;&#20449;&#36182;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;TrustDD&#65289;&#12290;&#36890;&#36807;&#31934;&#28860;InD&#26679;&#26412;&#21644;&#24322;&#24120;&#20540;&#65292;&#36825;&#20123;&#34987;&#31579;&#36873;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#35757;&#32451;&#20986;&#26082;&#25797;&#38271;InD&#20998;&#31867;&#21448;&#33021;&#36827;&#34892;OOD&#26816;&#27979;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#32531;&#35299;&#23545;&#30495;&#23454;&#24322;&#24120;&#20540;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#24182;&#20351;OOD&#26816;&#27979;&#26356;&#21152;&#23454;&#29992;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#23545;InD&#26679;&#26412;&#25439;&#22351;&#20197;&#29983;&#25104;&#20266;&#26679;&#26412;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiency and trustworthiness are two eternal pursuits when applying deep learning in real-world applications. With regard to efficiency, dataset distillation (DD) endeavors to reduce training costs by distilling the large dataset into a tiny synthetic dataset. However, existing methods merely concentrate on in-distribution (InD) classification in a closed-world setting, disregarding out-of-distribution (OOD) samples. On the other hand, OOD detection aims to enhance models' trustworthiness, which is always inefficiently achieved in full-data settings. For the first time, we simultaneously consider both issues and propose a novel paradigm called Trustworthy Dataset Distillation (TrustDD). By distilling both InD samples and outliers, the condensed datasets are capable to train models competent in both InD classification and OOD detection. To alleviate the requirement of real outlier data and make OOD detection more practical, we further propose to corrupt InD samples to generate pseudo-
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23567;&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;SOD4SB&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22823;&#37327;&#30340;&#40479;&#31867;&#23454;&#20363;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#32454;&#33410;&#65292;&#24182;&#27010;&#36848;&#20102;&#33719;&#22870;&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#20197;&#21450;&#35780;&#20272;&#32593;&#31449;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.09143</link><description>&lt;p&gt;
MVA2023&#23567;&#29289;&#20307;&#26816;&#27979;&#25361;&#25112;&#65306;&#29992;&#20110;&#40479;&#31867;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#21644;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
MVA2023 Small Object Detection Challenge for Spotting Birds: Dataset, Methods, and Results. (arXiv:2307.09143v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23567;&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;SOD4SB&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22823;&#37327;&#30340;&#40479;&#31867;&#23454;&#20363;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#32454;&#33410;&#65292;&#24182;&#27010;&#36848;&#20102;&#33719;&#22870;&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#20197;&#21450;&#35780;&#20272;&#32593;&#31449;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#29289;&#20307;&#26816;&#27979;(SOD)&#26159;&#37325;&#35201;&#30340;&#26426;&#22120;&#35270;&#35273;&#20027;&#39064;&#65292;&#22240;&#20026;(i)&#21508;&#31181;&#29616;&#23454;&#24212;&#29992;&#38656;&#35201;&#23545;&#36828;&#22788;&#29289;&#20307;&#36827;&#34892;&#26816;&#27979;&#65292;(ii)&#30001;&#20110;&#23567;&#29289;&#20307;&#30340;&#22122;&#22768;&#12289;&#27169;&#31946;&#21644;&#20449;&#24687;&#36739;&#23569;&#30340;&#22270;&#20687;&#34920;&#29616;&#24418;&#24335;&#65292;SOD&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;SOD&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;39,070&#24352;&#22270;&#20687;&#65292;&#20849;&#21253;&#21547;137,121&#20010;&#40479;&#31867;&#23454;&#20363;&#65292;&#31216;&#20026;&#29992;&#20110;&#40479;&#31867;&#35782;&#21035;&#30340;&#23567;&#29289;&#20307;&#26816;&#27979;(SOD4SB)&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SOD4SB&#25968;&#25454;&#38598;&#25361;&#25112;&#30340;&#32454;&#33410;&#65292;&#24182;&#31616;&#35201;&#20171;&#32461;&#20102;&#33719;&#22870;&#26041;&#27861;&#12290;&#35813;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#20195;&#30721;&#20197;&#21450;&#29992;&#20110;&#20844;&#20849;&#27979;&#35797;&#38598;&#35780;&#20272;&#30340;&#32593;&#31449;&#22343;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Small Object Detection (SOD) is an important machine vision topic because (i) a variety of real-world applications require object detection for distant objects and (ii) SOD is a challenging task due to the noisy, blurred, and less-informative image appearances of small objects. This paper proposes a new SOD dataset consisting of 39,070 images including 137,121 bird instances, which is called the Small Object Detection for Spotting Birds (SOD4SB) dataset. The detail of the challenge with the SOD4SB dataset is introduced in this paper. In total, 223 participants joined this challenge. This paper briefly introduces the award-winning methods. The dataset, the baseline code, and the website for evaluation on the public testset are publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22810;&#30456;&#22810;&#20307;&#32791;&#25955;&#31890;&#23376;&#21160;&#21147;&#23398;&#21644;&#22522;&#20110;PINNs&#30340;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#26041;&#27861;&#65292;&#34920;&#24449;&#20102;&#39640;&#40655;&#24230;&#29076;&#34701;CMAS&#28082;&#28404;&#30340;&#37096;&#20998;&#28070;&#28287;&#21160;&#21147;&#23398;&#65292;&#20026;&#35299;&#20915;&#39640;&#28201;&#24212;&#29992;&#20013;CMAS&#31215;&#32858;&#21644;&#23545;&#35774;&#22791;&#30340;&#25439;&#23475;&#38382;&#39064;&#25552;&#20379;&#20102;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.09142</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#30456;&#22810;&#20307;&#32791;&#25955;&#31890;&#23376;&#21160;&#21147;&#23398;&#21644;&#22522;&#20110;PINNs&#30340;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#26469;&#34920;&#24449;CMAS&#28082;&#28404;&#30340;&#37096;&#20998;&#28070;&#28287;
&lt;/p&gt;
&lt;p&gt;
Characterization of partial wetting by CMAS droplets using multiphase many-body dissipative particle dynamics and data-driven discovery based on PINNs. (arXiv:2307.09142v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22810;&#30456;&#22810;&#20307;&#32791;&#25955;&#31890;&#23376;&#21160;&#21147;&#23398;&#21644;&#22522;&#20110;PINNs&#30340;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#26041;&#27861;&#65292;&#34920;&#24449;&#20102;&#39640;&#40655;&#24230;&#29076;&#34701;CMAS&#28082;&#28404;&#30340;&#37096;&#20998;&#28070;&#28287;&#21160;&#21147;&#23398;&#65292;&#20026;&#35299;&#20915;&#39640;&#28201;&#24212;&#29992;&#20013;CMAS&#31215;&#32858;&#21644;&#23545;&#35774;&#22791;&#30340;&#25439;&#23475;&#38382;&#39064;&#25552;&#20379;&#20102;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29076;&#34701;&#30722;&#26159;&#30707;&#28784;&#12289;&#38209;&#12289;&#38109;&#21644;&#30789;&#37240;&#30416;&#30340;&#28151;&#21512;&#29289;&#65292;&#34987;&#31216;&#20026;CMAS&#65292;&#20854;&#20855;&#26377;&#39640;&#40655;&#24230;&#12289;&#23494;&#24230;&#21644;&#34920;&#38754;&#24352;&#21147;&#12290;CMAS&#30340;&#29420;&#29305;&#24615;&#20351;&#24471;&#22312;&#39640;&#28201;&#24212;&#29992;&#20013;&#22788;&#29702;&#23427;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#26448;&#26009;&#26469;&#38450;&#27490;&#20854;&#31215;&#32858;&#24182;&#23545;&#20851;&#38190;&#35774;&#22791;&#36896;&#25104;&#25439;&#23475;&#12290;&#26412;&#25991;&#20351;&#29992;&#22810;&#30456;&#22810;&#20307;&#32791;&#25955;&#31890;&#23376;&#21160;&#21147;&#23398;&#65288;mDPD&#65289;&#27169;&#25311;&#30740;&#31350;&#39640;&#40655;&#24230;&#29076;&#34701;CMAS&#28082;&#28404;&#30340;&#28070;&#28287;&#21160;&#21147;&#23398;&#12290;&#27169;&#25311;&#22312;&#19977;&#32500;&#20013;&#36827;&#34892;&#65292;&#21021;&#22987;&#28082;&#28404;&#23610;&#23544;&#21644;&#24179;&#34913;&#25509;&#35302;&#35282;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25429;&#25417;CMAS&#28082;&#28404;&#25193;&#23637;&#21322;&#24452;&#34892;&#20026;&#30340;&#31895;&#31890;&#21442;&#25968;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26694;&#26550;&#30830;&#23450;ODE&#21442;&#25968;&#12290;&#38543;&#21518;&#65292;&#32473;&#20986;&#20102;&#21442;&#25968;&#20540;&#22312;&#21021;&#22987;&#21322;&#24452;&#21644;&#25509;&#35302;&#35282;&#19978;&#30340;&#38381;&#24335;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The molten sand, a mixture of calcia, magnesia, alumina, and silicate, known as CMAS, is characterized by its high viscosity, density, and surface tension. The unique properties of CMAS make it a challenging material to deal with in high-temperature applications, requiring innovative solutions and materials to prevent its buildup and damage to critical equipment. Here, we use multiphase many-body dissipative particle dynamics (mDPD) simulations to study the wetting dynamics of highly viscous molten CMAS droplets. The simulations are performed in three dimensions, with varying initial droplet sizes and equilibrium contact angles. We propose a coarse parametric ordinary differential equation (ODE) that captures the spreading radius behavior of the CMAS droplets. The ODE parameters are then identified based on the Physics-Informed Neural Network (PINN) framework. Subsequently, the closed form dependency of parameter values found by PINN on the initial radii and contact angles are given us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"MiSiCAL"&#30340;&#21333;&#31867;&#21035;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#31574;&#30053;&#65292;&#21033;&#29992;&#25968;&#37327;-&#20934;&#30830;&#24615;&#30456;&#20851;&#24615;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#39640;&#24615;&#33021;&#27169;&#22411;&#35757;&#32451;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;MiSiCAL&#33021;&#22312;&#35768;&#22810;&#31867;&#21035;&#19978;&#20248;&#20110;&#38543;&#26426;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.09109</link><description>&lt;p&gt;
&#21333;&#31867;&#21035;&#20027;&#21160;&#23398;&#20064;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Mining of Single-Class by Active Learning for Semantic Segmentation. (arXiv:2307.09109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"MiSiCAL"&#30340;&#21333;&#31867;&#21035;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#31574;&#30053;&#65292;&#21033;&#29992;&#25968;&#37327;-&#20934;&#30830;&#24615;&#30456;&#20851;&#24615;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#39640;&#24615;&#33021;&#27169;&#22411;&#35757;&#32451;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;MiSiCAL&#33021;&#22312;&#35768;&#22810;&#31867;&#21035;&#19978;&#20248;&#20110;&#38543;&#26426;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#31181;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#38656;&#35201;&#22810;&#27425;&#37325;&#26032;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#65292;&#20197;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#30340;&#26679;&#26412;&#65292;&#24182;&#24456;&#23569;&#25552;&#20379;&#20174;&#23569;&#25968;&#31867;&#21035;&#20013;&#33719;&#21462;&#26679;&#26412;&#30340;&#36873;&#39033;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Mining of Single-Class by Active Learning (MiSiCAL)&#8221;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;&#25968;&#37327;-&#20934;&#30830;&#24615;&#30456;&#20851;&#24615;&#26469;&#24314;&#31435;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#12290;MiSiCAL&#22312;&#29305;&#21035;&#22823;&#30340;&#25209;&#37327;&#22823;&#23567;&#19979;&#23588;&#20854;&#26377;&#24110;&#21161;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#20687;&#20854;&#20182;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#37027;&#26679;&#36827;&#34892;&#37325;&#22797;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#35201;&#24402;&#21151;&#20110;&#23427;&#33021;&#22815;&#21033;&#29992;&#20505;&#36873;&#25968;&#25454;&#28857;&#30340;&#22266;&#23450;&#34920;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;171&#20010;COCO10k&#31867;&#21035;&#20013;&#65292;MiSiCAL&#33021;&#22815;&#22312;150&#20010;&#31867;&#21035;&#19978;&#32988;&#36807;&#38543;&#26426;&#31574;&#30053;&#65292;&#32780;&#26368;&#24378;&#30340;&#22522;&#20934;&#21482;&#22312;101&#20010;&#31867;&#21035;&#19978;&#32988;&#36807;&#38543;&#26426;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several Active Learning (AL) policies require retraining a target model several times in order to identify the most informative samples and rarely offer the option to focus on the acquisition of samples from underrepresented classes. Here the Mining of Single-Class by Active Learning (MiSiCAL) paradigm is introduced where an AL policy is constructed through deep reinforcement learning and exploits quantity-accuracy correlations to build datasets on which high-performance models can be trained with regards to specific classes. MiSiCAL is especially helpful in the case of very large batch sizes since it does not require repeated model training sessions as is common in other AL methods. This is thanks to its ability to exploit fixed representations of the candidate data points. We find that MiSiCAL is able to outperform a random policy on 150 out of 171 COCO10k classes, while the strongest baseline only outperforms random on 101 classes.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#20855;&#26377;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#30340;&#32452;&#21512;&#21322;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24310;&#36831;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#24182;&#20570;&#20986;&#20915;&#31574;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.09093</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#24310;&#36831;&#32452;&#21512;&#21322;&#24378;&#21270;&#23398;&#20064;&#22312;&#22240;&#26524;&#30456;&#20851;&#22238;&#25253;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Non-stationary Delayed Combinatorial Semi-Bandit with Causally Related Rewards. (arXiv:2307.09093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09093
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#20855;&#26377;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#30340;&#32452;&#21512;&#21322;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24310;&#36831;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#24182;&#20570;&#20986;&#20915;&#31574;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#20013;&#65292;&#24120;&#24120;&#23384;&#22312;&#38271;&#26102;&#38388;&#30340;&#21453;&#39304;&#24310;&#36831;&#12290;&#36825;&#31181;&#24310;&#36831;&#20250;&#38477;&#20302;&#23398;&#20064;&#20195;&#29702;&#22312;&#38271;&#26399;&#20013;&#35782;&#21035;&#20986;&#19968;&#32452;&#20855;&#26377;&#26368;&#20248;&#24635;&#22238;&#25253;&#30340;&#33218;&#30340;&#24615;&#33021;&#12290;&#22312;&#20855;&#26377;&#32467;&#26500;&#20381;&#36182;&#30340;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#38500;&#20102;&#36866;&#24212;&#24310;&#36831;&#21644;&#29615;&#22659;&#21464;&#21270;&#22806;&#65292;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#20943;&#36731;&#21453;&#39304;&#24310;&#36831;&#23545;&#20915;&#31574;&#36807;&#31243;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#25152;&#25551;&#36848;&#30340;&#24773;&#26223;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20855;&#26377;&#22240;&#26524;&#30456;&#20851;&#22238;&#25253;&#30340;&#38750;&#24179;&#31283;&#21644;&#24310;&#36831;&#30340;&#32452;&#21512;&#21322;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#23450;&#21521;&#22270;&#22312;&#19968;&#20010;&#22266;&#23450;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#20013;&#24314;&#27169;&#22240;&#26524;&#20851;&#31995;&#12290;&#23398;&#20064;&#20195;&#29702;&#26368;&#22823;&#21270;&#38271;&#26399;&#24179;&#22343;&#22238;&#25253;&#65292;&#35813;&#22238;&#25253;&#23450;&#20041;&#20026;&#22522;&#30784;&#33218;&#30340;&#22238;&#25253;&#30340;&#32447;&#24615;&#20989;&#25968;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20174;&#24310;&#36831;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential decision-making under uncertainty is often associated with long feedback delays. Such delays degrade the performance of the learning agent in identifying a subset of arms with the optimal collective reward in the long run. This problem becomes significantly challenging in a non-stationary environment with structural dependencies amongst the reward distributions associated with the arms. Therefore, besides adapting to delays and environmental changes, learning the causal relations alleviates the adverse effects of feedback delay on the decision-making process. We formalize the described setting as a non-stationary and delayed combinatorial semi-bandit problem with causally related rewards. We model the causal relations by a directed graph in a stationary structural equation model. The agent maximizes the long-term average payoff, defined as a linear function of the base arms' rewards. We develop a policy that learns the structural dependencies from delayed feedback and utiliz
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#21457;&#23637;&#20013;&#22269;&#23478;&#38754;&#20020;&#30340;&#33021;&#28304;&#30701;&#32570;&#21644;&#30005;&#21147;&#36127;&#33655;&#25925;&#38556;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#21487;&#20877;&#29983;&#33021;&#28304;&#24182;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#33021;&#28304;&#38656;&#27714;&#24182;&#28385;&#36275;&#28040;&#36153;&#32773;&#30340;&#38656;&#27714;&#12290;&#35813;&#30740;&#31350;&#36824;&#20351;&#29992;&#21306;&#22359;&#38142;&#30830;&#20445;&#25968;&#25454;&#20132;&#26131;&#30340;&#36879;&#26126;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09080</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#30005;&#33021;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Federated learning model for Electric Energy management using Blockchain Technology. (arXiv:2307.09080v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09080
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#21457;&#23637;&#20013;&#22269;&#23478;&#38754;&#20020;&#30340;&#33021;&#28304;&#30701;&#32570;&#21644;&#30005;&#21147;&#36127;&#33655;&#25925;&#38556;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#21487;&#20877;&#29983;&#33021;&#28304;&#24182;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#33021;&#28304;&#38656;&#27714;&#24182;&#28385;&#36275;&#28040;&#36153;&#32773;&#30340;&#38656;&#27714;&#12290;&#35813;&#30740;&#31350;&#36824;&#20351;&#29992;&#21306;&#22359;&#38142;&#30830;&#20445;&#25968;&#25454;&#20132;&#26131;&#30340;&#36879;&#26126;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#30701;&#32570;&#21644;&#30005;&#21147;&#36127;&#33655;&#25925;&#38556;&#26159;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#33021;&#28304;&#37096;&#38376;&#31649;&#29702;&#19981;&#36275;&#21644;&#38750;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#20351;&#29992;&#12290;&#25913;&#21892;&#33021;&#28304;&#31649;&#29702;&#21644;&#21033;&#29992;&#21487;&#20877;&#29983;&#36164;&#28304;&#23545;&#35299;&#20915;&#33021;&#28304;&#21361;&#26426;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#28385;&#36275;&#30001;&#20110;&#29123;&#26009;&#20215;&#26684;&#39640;&#23548;&#33268;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#33021;&#28304;&#38656;&#27714;&#65292;&#26377;&#24517;&#35201;&#22686;&#21152;&#21487;&#20877;&#29983;&#33021;&#28304;&#65288;RESs&#65289;&#30340;&#20351;&#29992;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#26368;&#26032;&#20852;&#30340;&#25216;&#26415;&#12290;&#32852;&#37030;&#23398;&#20064;&#24110;&#21161;&#22312;&#36828;&#31243;&#36793;&#32536;&#31449;&#28857;&#38598;&#21512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#29983;&#25104;&#26381;&#21153;&#22120;&#31471;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#20840;&#23616;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#33021;&#28304;&#38656;&#27714;&#65292;&#28385;&#36275;&#28040;&#36153;&#32773;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#20998;&#24067;&#24335;&#36134;&#26412;&#25216;&#26415;&#65292;&#29992;&#20110;&#22788;&#29702;&#29983;&#20135;&#32773;&#21644;&#28040;&#36153;&#32773;&#20043;&#38388;&#30340;&#25968;&#25454;&#20132;&#26131;&#65292;&#20197;&#30830;&#20445;&#20854;&#36879;&#26126;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy shortfall and electricity load shedding are the main problems for developing countries. The main causes are lack of management in the energy sector and the use of non-renewable energy sources. The improved energy management and use of renewable sources can be significant to resolve energy crisis. It is necessary to increase the use of renewable energy sources (RESs) to meet the increasing energy demand due to high prices of fossil-fuel based energy. Federated learning (FL) is the most emerging technique in the field of artificial intelligence. Federated learning helps to generate global model at server side by ensemble locally trained models at remote edges sites while preserving data privacy. The global model used to predict energy demand to satisfy the needs of consumers. In this article, we have proposed Blockchain based safe distributed ledger technology for transaction of data between prosumer and consumer to ensure their transparency, traceability and security. Furthermore
&lt;/p&gt;</description></item><item><title>DiTTO&#26159;&#19968;&#31181;&#25193;&#25955;&#21551;&#21457;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;Transformer&#26550;&#26500;&#30340;&#20803;&#32032;&#65292;&#26080;&#38656;&#26102;&#38388;&#31163;&#25955;&#21270;&#36830;&#32493;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;PDEs&#65292;&#24182;&#22312;&#22810;&#32500;&#24230;&#30340;&#21508;&#31181;PDE&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09072</link><description>&lt;p&gt;
DiTTO&#65306;&#21463;&#25193;&#25955;&#21551;&#21457;&#30340;&#26102;&#31354;&#36716;&#25442;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
DiTTO: Diffusion-inspired Temporal Transformer Operator. (arXiv:2307.09072v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09072
&lt;/p&gt;
&lt;p&gt;
DiTTO&#26159;&#19968;&#31181;&#25193;&#25955;&#21551;&#21457;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;Transformer&#26550;&#26500;&#30340;&#20803;&#32032;&#65292;&#26080;&#38656;&#26102;&#38388;&#31163;&#25955;&#21270;&#36830;&#32493;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;PDEs&#65292;&#24182;&#22312;&#22810;&#32500;&#24230;&#30340;&#21508;&#31181;PDE&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#24050;&#32463;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#26368;&#36817;&#30340;&#31639;&#23376;&#23398;&#20064;&#33539;&#24335;&#30340;&#21457;&#23637;&#20351;&#24471;&#35299;&#20915;&#26356;&#24191;&#27867;PDE&#30456;&#20851;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36830;&#32493;&#22320;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;&#30340;PDEs&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26102;&#38388;&#31163;&#25955;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;DiTTO&#65292;&#21463;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#65292;&#20294;&#20854;&#26102;&#38388;&#26465;&#20214;&#26426;&#21046;&#23545;PDEs&#38750;&#24120;&#26377;&#29992;&#12290;&#25193;&#25955;&#21551;&#21457;&#30340;&#26694;&#26550;&#19982;Transformer&#26550;&#26500;&#30340;&#20803;&#32032;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26032;&#26041;&#27861;&#22312;&#22810;&#32500;&#24230;&#30340;&#24191;&#27867;PDE&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;1&#32500;Burgers&#26041;&#31243;&#65292;2&#32500;Navier-Stokes&#26041;&#31243;&#21644;2&#32500;&#21644;3&#32500;&#22768;&#27874;&#26041;&#31243;&#12290;DiTTO&#22312;&#36825;&#20123;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving partial differential equations (PDEs) using a data-driven approach has become increasingly common. The recent development of the operator learning paradigm has enabled the solution of a broader range of PDE-related problems. We propose an operator learning method to solve time-dependent PDEs continuously in time without needing any temporal discretization. The proposed approach, named DiTTO, is inspired by latent diffusion models. While diffusion models are usually used in generative artificial intelligence tasks, their time-conditioning mechanism is extremely useful for PDEs. The diffusion-inspired framework is combined with elements from the Transformer architecture to improve its capabilities.  We demonstrate the effectiveness of the new approach on a wide variety of PDEs in multiple dimensions, namely the 1-D Burgers' equation, 2-D Navier-Stokes equations, and the acoustic wave equation in 2-D and 3-D. DiTTO achieves state-of-the-art results in terms of accuracy for these p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;U-Net&#36827;&#34892;&#32974;&#20799;&#22836;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;MobileNet&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#23545;&#26377;&#38480;&#30340;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#23218;&#32654;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#19988;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.09067</link><description>&lt;p&gt;
&#35780;&#20272;&#20351;&#29992;U-Net&#36827;&#34892;&#32974;&#20799;&#22836;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24494;&#35843;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net. (arXiv:2307.09067v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;U-Net&#36827;&#34892;&#32974;&#20799;&#22836;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;MobileNet&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#23545;&#26377;&#38480;&#30340;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#23218;&#32654;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#19988;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32974;&#20799;&#22836;&#20998;&#21106;&#26159;&#27979;&#37327;&#22922;&#23072;&#26399;&#38388;&#32974;&#20799;&#22836;&#22260;(HC)&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#26159;&#30417;&#27979;&#32974;&#20799;&#29983;&#38271;&#30340;&#37325;&#35201;&#29983;&#29289;&#27979;&#23450;&#23398;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#29983;&#25104;&#29983;&#29289;&#23398;&#27979;&#23450;&#26159;&#32791;&#26102;&#19988;&#32467;&#26524;&#19981;&#19968;&#33268;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32454;&#35843;(U-Net&#32593;&#32476;&#21644;&#36731;&#37327;&#32423;&#30340;MobileNet&#20316;&#20026;&#32534;&#30721;&#22120;)&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#32974;&#20799;&#22836;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;CNN&#32593;&#32476;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32454;&#35843;&#31574;&#30053;&#22312;&#35757;&#32451;&#21442;&#25968;&#20943;&#23569;85.8%&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;&#24182;&#19988;&#65292;&#25105;&#20204;&#30340;&#32454;&#35843;&#31574;&#30053;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fetal head segmentation is a crucial step in measuring the fetal head circumference (HC) during gestation, an important biometric in obstetrics for monitoring fetal growth. However, manual biometry generation is time-consuming and results in inconsistent accuracy. To address this issue, convolutional neural network (CNN) models have been utilized to improve the efficiency of medical biometry. But training a CNN network from scratch is a challenging task, we proposed a Transfer Learning (TL) method. Our approach involves fine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to perform segmentation on a set of fetal head ultrasound (US) images with limited effort. This method addresses the challenges associated with training a CNN network from scratch. It suggests that our proposed FT strategy yields segmentation performance that is comparable when trained with a reduced number of parameters by 85.8%. And our proposed FT strategy outperforms other strategies with s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#33258;&#36866;&#24212;&#37051;&#22495;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#22270;&#29983;&#25104;&#22120;&#65292;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#36873;&#25321;&#27599;&#20010;&#33410;&#28857;&#30340;&#37051;&#22495;&#21644;&#22823;&#23567;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#29616;&#26377;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#30456;&#27604;&#20854;&#20182;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#33021;&#33719;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09065</link><description>&lt;p&gt;
&#23398;&#20064;&#33258;&#36866;&#24212;&#37051;&#22495;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Adaptive Neighborhoods for Graph Neural Networks. (arXiv:2307.09065v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#33258;&#36866;&#24212;&#37051;&#22495;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#22270;&#29983;&#25104;&#22120;&#65292;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#36873;&#25321;&#27599;&#20010;&#33410;&#28857;&#30340;&#37051;&#22495;&#21644;&#22823;&#23567;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#29616;&#26377;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#30456;&#27604;&#20854;&#20182;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#33021;&#33719;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#21487;&#20197;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#20551;&#35774;&#32473;&#23450;&#22270;&#32467;&#26500;&#12290;&#24403;&#36755;&#20837;&#30340;&#22270;&#20855;&#26377;&#22122;&#22768;&#25110;&#19981;&#21487;&#29992;&#26102;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#26500;&#24314;&#25110;&#23398;&#20064;&#19968;&#20010;&#28508;&#22312;&#30340;&#22270;&#32467;&#26500;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22266;&#23450;&#25972;&#20010;&#22270;&#30340;&#33410;&#28857;&#24230;&#30340;&#36873;&#25321;&#65292;&#36825;&#26159;&#27425;&#20248;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#22270;&#29983;&#25104;&#22120;&#65292;&#35813;&#29983;&#25104;&#22120;&#24314;&#31435;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#27599;&#20010;&#33410;&#28857;&#37117;&#36873;&#25321;&#20854;&#37051;&#22495;&#21644;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#28041;&#21450;&#22270;&#21367;&#31215;&#25805;&#20316;&#30340;&#29616;&#26377;&#27969;&#31243;&#20013;&#65292;&#23558;&#39044;&#23450;&#25110;&#29616;&#26377;&#30340;&#37051;&#25509;&#30697;&#38453;&#26367;&#25442;&#20026;&#20316;&#20026;&#25972;&#20307;&#30446;&#26631;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#23398;&#20064;&#21644;&#20248;&#21270;&#30340;&#37051;&#25509;&#30697;&#38453;&#12290;&#22240;&#27492;&#65292;&#23427;&#36866;&#29992;&#20110;&#20219;&#20309;GCN&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22359;&#38598;&#25104;&#21040;&#36712;&#36857;&#39044;&#27979;&#12289;&#28857;&#20113;&#20998;&#31867;&#21644;&#33410;&#28857;&#20998;&#31867;&#30340;&#27969;&#31243;&#20013;&#65292;&#22312;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#21644;GCN&#19978;&#30456;&#27604;&#20854;&#20182;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) enable end-to-end learning on graph structured data. However, many works assume a given graph structure. When the input graph is noisy or unavailable, one approach is to construct or learn a latent graph structure. These methods typically fix the choice of node degree for the entire graph, which is suboptimal. Instead, we propose a novel end-to-end differentiable graph generator which builds graph topologies where each node selects both its neighborhood and its size. Our module can be readily integrated into existing pipelines involving graph convolution operations, replacing the predetermined or existing adjacency matrix with one that is learned, and optimized, as part of the general objective. As such it is applicable to any GCN. We integrate our module into trajectory prediction, point cloud classification and node classification pipelines resulting in improved accuracy over other structure-learning methods across a wide range of datasets and GCN 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#39532;&#23572;&#21487;&#22827;&#38142;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#25311;&#22120;&#65292;&#36866;&#29992;&#20110;&#39044;&#27979;&#27861;&#22269;&#21644;&#26031;&#22570;&#30340;&#32435;&#32500;&#20122;&#30340;&#38271;&#26102;&#38388;&#28909;&#28010;&#12290;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#27169;&#25311;&#22120;&#22312;&#27010;&#29575;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#32463;&#36807;&#36866;&#24403;&#35780;&#20272;&#21644;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.09060</link><description>&lt;p&gt;
&#29992;&#31867;&#27604;&#39532;&#23572;&#21487;&#22827;&#38142;&#21644;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#26497;&#31471;&#28909;&#28010;&#30340;&#37319;&#26679;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Extreme heatwave sampling and prediction with analog Markov chain and comparisons with deep learning. (arXiv:2307.09060v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#39532;&#23572;&#21487;&#22827;&#38142;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#25311;&#22120;&#65292;&#36866;&#29992;&#20110;&#39044;&#27979;&#27861;&#22269;&#21644;&#26031;&#22570;&#30340;&#32435;&#32500;&#20122;&#30340;&#38271;&#26102;&#38388;&#28909;&#28010;&#12290;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#27169;&#25311;&#22120;&#22312;&#27010;&#29575;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#32463;&#36807;&#36866;&#24403;&#35780;&#20272;&#21644;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#25311;&#22120;&#65292;&#38543;&#26426;&#22825;&#27668;&#29983;&#25104;&#22120;&#65288;SWG&#65289;&#65292;&#36866;&#29992;&#20110;&#20272;&#35745;&#27861;&#22269;&#21644;&#26031;&#22570;&#30340;&#32435;&#32500;&#20122;&#38271;&#26102;&#38388;&#28909;&#28010;&#30340;&#27010;&#29575;&#12290;&#36825;&#20010;&#27169;&#25311;&#22120;&#22522;&#20110;&#29615;&#27969;&#30340;&#31867;&#27604;&#26041;&#27861;&#65292;&#25105;&#20204;&#21152;&#20837;&#28201;&#24230;&#21644;&#22303;&#22756;&#28287;&#24230;&#20316;&#20026;&#39044;&#27979;&#23383;&#27573;&#12290;&#25105;&#20204;&#23558;&#27169;&#25311;&#22120;&#35757;&#32451;&#22312;&#19968;&#20010;&#20013;&#31561;&#22797;&#26434;&#24230;&#27668;&#20505;&#27169;&#22411;&#30340;&#36816;&#34892;&#19978;&#65292;&#24182;&#23637;&#31034;&#23427;&#33021;&#22815;&#39044;&#27979;&#26679;&#26412;&#22806;&#28909;&#28010;&#30340;&#26465;&#20214;&#27010;&#29575;&#65288;&#39044;&#27979;&#65289;&#12290;&#25105;&#20204;&#29305;&#21035;&#27880;&#24847;&#65292;&#20351;&#29992;&#36866;&#29992;&#20110;&#32597;&#35265;&#20107;&#20214;&#30340;&#21512;&#36866;&#35780;&#20998;&#26469;&#35780;&#20272;&#36825;&#20010;&#39044;&#27979;&#12290;&#20026;&#20102;&#21152;&#36895;&#31867;&#27604;&#30340;&#35745;&#31639;&#65292;&#38477;&#32500;&#25216;&#26415;&#34987;&#24212;&#29992;&#65292;&#24182;&#19988;&#24615;&#33021;&#24471;&#21040;&#35780;&#20272;&#12290;&#36890;&#36807;SWG&#23454;&#29616;&#30340;&#27010;&#29575;&#39044;&#27979;&#19982;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23454;&#29616;&#30340;&#27010;&#29575;&#39044;&#27979;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#38543;&#30528;&#25968;&#30334;&#24180;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;CNN&#22312;&#27010;&#29575;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#32463;&#36807;8&#20010;&#35757;&#32451;&#23454;&#20363;&#35757;&#32451;&#30340;SWG&#27169;&#25311;&#22120;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a data-driven emulator, stochastic weather generator (SWG), suitable for estimating probabilities of prolonged heatwaves in France and Scandinavia. This emulator is based on the method of analogs of circulation to which we add temperature and soil moisture as predictor fields. We train the emulator on an intermediate complexity climate model run and show that it is capable of predicting conditional probabilities (forecasting) of heatwaves out of sample. Special attention is payed that this prediction is evaluated using proper score appropriate for rare events. To accelerate the computation of analogs dimensionality reduction techniques are applied and the performance is evaluated. The probabilistic prediction achieved with SWG is compared with the one achieved with  Convolutional Neural Network (CNN). With the availability of hundreds of years of training data CNNs perform better at the task of probabilistic prediction. In addition, we show that the SWG emulator trained on 8
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#20302;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#35745;&#31639;&#20004;&#32452;&#28857;&#20043;&#38388;Gromov-Wasserstein&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#20302;&#32500;&#20248;&#21270;&#38382;&#39064;&#26469;&#35299;&#20915;&#35745;&#31639;&#22256;&#38590;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#25214;&#21040;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.09057</link><description>&lt;p&gt;
&#22312;&#20302;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#20840;&#23616;&#27714;&#35299;&#28857;&#20113;&#30340;Gromov-Wasserstein&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces. (arXiv:2307.09057v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#20302;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#35745;&#31639;&#20004;&#32452;&#28857;&#20043;&#38388;Gromov-Wasserstein&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#20302;&#32500;&#20248;&#21270;&#38382;&#39064;&#26469;&#35299;&#20915;&#35745;&#31639;&#22256;&#38590;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#25214;&#21040;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#35745;&#31639;&#20004;&#32452;&#28857;&#20043;&#38388;Gromov-Wasserstein&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#24046;&#24322;&#26159;&#27431;&#20960;&#37324;&#24471;&#33539;&#25968;&#30340;&#24179;&#26041;&#12290;Gromov-Wasserstein&#38382;&#39064;&#26159;&#20248;&#21270;&#36816;&#36755;&#38382;&#39064;&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#23427;&#23547;&#25214;&#20445;&#25345;&#23613;&#21487;&#33021;&#22810;&#30340;&#25104;&#23545;&#36317;&#31163;&#30340;&#20004;&#32452;&#28857;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#36825;&#21487;&#20197;&#29992;&#20110;&#37327;&#21270;AI&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#20004;&#20010;&#24418;&#24577;&#25110;&#24418;&#29366;&#30340;&#30456;&#20284;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#38382;&#39064;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;Quadratic Assignment Problem&#65288;QAP&#65289;&#65292;&#21363;&#20351;&#23545;&#20110;&#23567;&#38382;&#39064;&#26469;&#35828;&#65292;QAP&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#20063;&#26159;&#38590;&#20197;&#35745;&#31639;&#30340;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23558;QAP&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#22312;&#20302;&#32500;&#22495;&#19978;&#30340;&#20248;&#21270;&#38382;&#39064;&#26469;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#21033;&#29992;&#20102;&#38382;&#39064;&#21487;&#20197;&#34920;&#31034;&#20026;&#20302;&#31209;&#30340;&#20985;&#20108;&#27425;&#20248;&#21270;&#38382;&#39064;&#30340;&#20107;&#23454;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#28857;&#30340;&#25968;&#37327;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#22312;&#25104;&#21315;&#19978;&#19975;&#30340;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#25214;&#21040;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a framework for computing the Gromov-Wasserstein problem between two sets of points in low dimensional spaces, where the discrepancy is the squared Euclidean norm. The Gromov-Wasserstein problem is a generalization of the optimal transport problem that finds the assignment between two sets preserving pairwise distances as much as possible. This can be used to quantify the similarity between two formations or shapes, a common problem in AI and machine learning. The problem can be formulated as a Quadratic Assignment Problem (QAP), which is in general computationally intractable even for small problems. Our framework addresses this challenge by reformulating the QAP as an optimization problem with a low-dimensional domain, leveraging the fact that the problem can be expressed as a concave quadratic optimization problem with low rank. The method scales well with the number of points, and it can be used to find the global solution for large-scale problems with thousands
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#24182;&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#20855;&#26377;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#21644;&#26816;&#27979;&#24322;&#24120;&#20540;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#25193;&#23637;&#26041;&#27861;&#20197;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.09055</link><description>&lt;p&gt;
&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#29992;&#20110;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Outlier-Robust Tensor Low-Rank Representation for Data Clustering. (arXiv:2307.09055v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#24182;&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#20855;&#26377;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#21644;&#26816;&#27979;&#24322;&#24120;&#20540;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#25193;&#23637;&#26041;&#27861;&#20197;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#24352;&#37327;&#20998;&#26512;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24352;&#37327;&#25968;&#25454;&#32463;&#24120;&#21463;&#21040;&#24322;&#24120;&#20540;&#25110;&#26679;&#26412;&#29305;&#23450;&#30340;&#27745;&#26579;&#12290;&#22914;&#20309;&#24674;&#22797;&#34987;&#24322;&#24120;&#20540;&#25439;&#22351;&#30340;&#24352;&#37327;&#25968;&#25454;&#24182;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#24352;&#37327;&#25968;&#25454;&#32858;&#31867;&#30340;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#65288;OR-TLRR&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21463;&#21040;&#26368;&#36817;&#25552;&#20986;&#30340;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#21487;&#36870;&#32447;&#24615;&#21464;&#25442;&#24341;&#36215;&#30340;&#24352;&#37327;&#24352;&#37327;&#31215;&#30340;&#21551;&#21457;&#12290;&#23545;&#20110;&#24102;&#26377;&#20219;&#24847;&#24322;&#24120;&#20540;&#27745;&#26579;&#30340;&#24352;&#37327;&#35266;&#27979;&#65292;OR-TLRR&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#33021;&#22815;&#30830;&#20999;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#24182;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;OR-TLRR&#30340;&#25193;&#23637;&#26469;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank tensor analysis has received widespread attention with many practical applications. However, the tensor data are often contaminated by outliers or sample-specific corruptions. How to recover the tensor data that are corrupted by outliers and perform data clustering remains a challenging problem. This paper develops an outlier-robust tensor low-rank representation (OR-TLRR) method for simultaneous outlier detection and tensor data clustering based on the tensor singular value decomposition (t-SVD) algebraic framework. It is motivated by the recently proposed tensor-tensor product induced by invertible linear transforms that satisfy certain conditions. For tensor observations with arbitrary outlier corruptions, OR-TLRR has provable performance guarantee for exactly recovering the row space of clean data and detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is also proposed to handle the case when parts of the data are missing. Finally, extensive experim
&lt;/p&gt;</description></item><item><title>qecGPT&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#29992;&#29983;&#25104;&#27169;&#22411;&#35299;&#30721;&#37327;&#23376;&#32416;&#38169;&#30721;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;Transformers&#23398;&#20064;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#32508;&#21512;&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#22312;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21518;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#21644;&#29983;&#25104;&#26368;&#21487;&#33021;&#30340;&#36923;&#36753;&#36816;&#31639;&#31526;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2307.09025</link><description>&lt;p&gt;
qecGPT&#65306;&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#23545;&#37327;&#23376;&#32416;&#38169;&#30721;&#36827;&#34892;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
qecGPT: decoding Quantum Error-correcting Codes with Generative Pre-trained Transformers. (arXiv:2307.09025v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09025
&lt;/p&gt;
&lt;p&gt;
qecGPT&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#29992;&#29983;&#25104;&#27169;&#22411;&#35299;&#30721;&#37327;&#23376;&#32416;&#38169;&#30721;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;Transformers&#23398;&#20064;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#32508;&#21512;&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#22312;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21518;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#21644;&#29983;&#25104;&#26368;&#21487;&#33021;&#30340;&#36923;&#36753;&#36816;&#31639;&#31526;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#29983;&#25104;&#24314;&#27169;&#35299;&#30721;&#37327;&#23376;&#32416;&#38169;&#30721;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;Transformer&#23398;&#20064;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#32508;&#21512;&#30340;&#32852;&#21512;&#27010;&#29575;&#12290;&#35813;&#35757;&#32451;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#19981;&#38656;&#35201;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#34987;&#31216;&#20026;&#39044;&#35757;&#32451;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#27169;&#22411;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#32473;&#23450;&#32508;&#21512;&#30340;&#36923;&#36753;&#36816;&#31639;&#31526;&#30340;&#21487;&#33021;&#24615;&#65292;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#35299;&#30721;&#12290;&#23427;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#26368;&#21487;&#33021;&#30340;&#36923;&#36753;&#36816;&#31639;&#31526;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;$\mathcal O(2k)$&#65292;&#20854;&#20013;$k$&#20026;&#36923;&#36753;&#37327;&#23376;&#27604;&#29305;&#30340;&#25968;&#37327;&#65292;&#36825;&#27604;&#24120;&#35268;&#30340;&#26368;&#22823;&#20284;&#28982;&#35299;&#30721;&#31639;&#27861;$\mathcal O(4^k)$&#26356;&#20248;&#12290;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#36890;&#36807;&#30452;&#25509;&#37319;&#26679;&#31283;&#23450;&#23376;&#31639;&#31526;&#26469;&#26356;&#20934;&#30830;&#22320;&#33719;&#24471;&#32473;&#23450;&#32508;&#21512;&#30340;&#36923;&#36753;&#36816;&#31639;&#31526;&#21487;&#33021;&#24615;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a general framework for decoding quantum error-correcting codes with generative modeling. The model utilizes autoregressive neural networks, specifically Transformers, to learn the joint probability of logical operators and syndromes. This training is in an unsupervised way, without the need for labeled training data, and is thus referred to as pre-training. After the pre-training, the model can efficiently compute the likelihood of logical operators for any given syndrome, using maximum likelihood decoding. It can directly generate the most-likely logical operators with computational complexity $\mathcal O(2k)$ in the number of logical qubits $k$, which is significantly better than the conventional maximum likelihood decoding algorithms that require $\mathcal O(4^k)$ computation. Based on the pre-trained model, we further propose refinement to achieve more accurately the likelihood of logical operators for a given syndrome by directly sampling the stabilizer operators. We p
&lt;/p&gt;</description></item><item><title>&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#20256;&#32479;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#20302;&#31209;&#29305;&#24615;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;U&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#36339;&#36291;&#23618;&#36830;&#25509;&#21644;&#34917;&#19969;&#21512;&#24182;&#19982;&#20998;&#21106;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#20445;&#30041;&#39640;&#39057;&#19978;&#19979;&#25991;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.09019</link><description>&lt;p&gt;
U&#22411;&#21464;&#21387;&#22120;&#65306;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#20445;&#25345;&#39640;&#39057;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
U-shaped Transformer: Retain High Frequency Context in Time Series Analysis. (arXiv:2307.09019v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09019
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#20256;&#32479;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#20302;&#31209;&#29305;&#24615;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;U&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#36339;&#36291;&#23618;&#36830;&#25509;&#21644;&#34917;&#19969;&#21512;&#24182;&#19982;&#20998;&#21106;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#20445;&#30041;&#39640;&#39057;&#19978;&#19979;&#25991;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#24037;&#19994;&#39046;&#22495;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#19978;&#20063;&#32988;&#36807;&#39640;&#32423;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#26102;&#38388;&#24207;&#21015;&#24207;&#21015;&#23384;&#22312;&#20302;&#31209;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21464;&#21387;&#22120;&#30340;&#20302;&#36890;&#29305;&#24615;&#65292;&#24182;&#23581;&#35797;&#32467;&#21512;MLP&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23558;&#21463;Unet&#21551;&#21457;&#30340;&#36339;&#36291;&#23618;&#36830;&#25509;&#24212;&#29992;&#21040;&#20256;&#32479;&#30340;&#21464;&#21387;&#22120;&#32972;&#39592;&#32467;&#26500;&#20013;&#65292;&#20174;&#32780;&#23558;&#36755;&#20837;&#21040;&#36755;&#20986;&#30340;&#39640;&#39057;&#19978;&#19979;&#25991;&#20445;&#30041;&#19979;&#26469;&#65292;&#21363;U&#22411;&#21464;&#21387;&#22120;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#34917;&#19969;&#21512;&#24182;&#21644;&#20998;&#21106;&#25805;&#20316;&#26469;&#25552;&#21462;&#19981;&#21516;&#23610;&#24230;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#20805;&#20998;&#21033;&#29992;&#21464;&#21387;&#22120;&#30340;&#32972;&#39592;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series prediction plays a crucial role in various industrial fields. In recent years, neural networks with a transformer backbone have achieved remarkable success in many domains, including computer vision and NLP. In time series analysis domain, some studies have suggested that even the simplest MLP networks outperform advanced transformer-based networks on time series forecast tasks. However, we believe these findings indicate there to be low-rank properties in time series sequences. In this paper, we consider the low-pass characteristics of transformers and try to incorporate the advantages of MLP. We adopt skip-layer connections inspired by Unet into traditional transformer backbone, thus preserving high-frequency context from input to output, namely U-shaped Transformer. We introduce patch merge and split operation to extract features with different scales and use larger datasets to fully make use of the transformer backbone. Our experiments demonstrate that the model perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20581;&#24247;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;HeLM&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#22797;&#26434;&#25968;&#25454;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#25903;&#25345;&#31616;&#21333;&#27169;&#24577;&#25968;&#25454;&#30340;&#25991;&#26412;&#24207;&#21015;&#21270;&#65292;HeLM&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#20010;&#20307;&#19987;&#23646;&#25968;&#25454;&#20272;&#35745;&#30142;&#30149;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.09018</link><description>&lt;p&gt;
&#22522;&#20110;&#20010;&#20307;&#19987;&#23646;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#20581;&#24247;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multimodal LLMs for health grounded in individual-specific data. (arXiv:2307.09018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20581;&#24247;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;HeLM&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#22797;&#26434;&#25968;&#25454;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#25903;&#25345;&#31616;&#21333;&#27169;&#24577;&#25968;&#25454;&#30340;&#25991;&#26412;&#24207;&#21015;&#21270;&#65292;HeLM&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#20010;&#20307;&#19987;&#23646;&#25968;&#25454;&#20272;&#35745;&#30142;&#30149;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20010;&#20307;&#19987;&#23646;&#25968;&#25454;&#30340;&#20581;&#24247;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20010;&#24615;&#21270;&#20581;&#24247;&#38382;&#39064;&#65292;&#20294;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;LLMs&#38656;&#35201;&#20855;&#22791;&#25668;&#20837;&#19982;&#20010;&#20307;&#20581;&#24247;&#29366;&#20917;&#30456;&#20851;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#27169;&#24577;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65288;HeLM&#65306;&#20581;&#24247;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#27169;&#24577;&#29702;&#35299;&#65289;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#23558;&#22797;&#26434;&#30340;&#25968;&#25454;&#27169;&#24577;&#26144;&#23556;&#21040;LLMs&#30340;&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#23558;&#31616;&#21333;&#30340;&#27169;&#24577;&#22914;&#34920;&#26684;&#25968;&#25454;&#24207;&#21015;&#21270;&#20026;&#25991;&#26412;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#20010;&#20307;&#19987;&#23646;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;LLMs&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#33521;&#22269;&#29983;&#29289;&#24211;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;HeLM&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#20020;&#24202;&#29305;&#24449;&#65292;&#20197;&#21450;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#20272;&#35745;&#30142;&#30149;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation large language models (LLMs) have shown an impressive ability to solve tasks across a wide range of fields including health. To effectively solve personalized health tasks, LLMs need the ability to ingest a diversity of data modalities that are relevant to an individual's health status. In this paper, we take a step towards creating multimodal LLMs for health that are grounded in individual-specific data by developing a framework (HeLM: Health Large Language Model for Multimodal Understanding) that enables LLMs to use high-dimensional clinical modalities to estimate underlying disease risk. HeLM encodes complex data modalities by learning an encoder that maps them into the LLM's token embedding space and for simple modalities like tabular data by serializing the data into text. Using data from the UK Biobank, we show that HeLM can effectively use demographic and clinical features in addition to high-dimensional time-series data to estimate disease risk. For example, HeLM ach
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09009</link><description>&lt;p&gt;
ChatGPT&#30340;&#34892;&#20026;&#38543;&#26102;&#38388;&#21464;&#21270;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3.5&#21644;GPT-4&#26159;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#26356;&#26032;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#30340;2023&#24180;3&#26376;&#21644;2023&#24180;6&#26376;&#29256;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28041;&#21450;&#22235;&#39033;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;1&#65289;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;2&#65289;&#22238;&#31572;&#25935;&#24863;/&#21361;&#38505;&#38382;&#39064;&#65292;3&#65289;&#29983;&#25104;&#20195;&#30721;&#21644;4&#65289;&#35270;&#35273;&#25512;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3.5&#21644;GPT-4&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#22312;&#26102;&#38388;&#19978;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;GPT-4&#65288;2023&#24180;3&#26376;&#65289;&#22312;&#35782;&#21035;&#36136;&#25968;&#26041;&#38754;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65288;&#20934;&#30830;&#29575;&#20026;97.6%&#65289;&#65292;&#20294;GPT-4&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#30456;&#21516;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#38750;&#24120;&#24046;&#65288;&#20934;&#30830;&#29575;&#20026;2.4%&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;GPT-3.5&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#27604;GPT-3.5&#65288;2023&#24180;3&#26376;&#65289;&#35201;&#22909;&#24471;&#22810;&#12290;GPT-4&#22312;6&#26376;&#20221;&#23545;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#30340;&#24847;&#24895;&#36739;3&#26376;&#20221;&#35201;&#20302;&#65292;&#32780;&#26080;&#35770;&#26159;GPT-4&#36824;&#26159;GPT-3.5&#22312;6&#26376;&#20221;&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#37117;&#26377;&#26356;&#22810;&#30340;&#26684;&#24335;&#38169;&#35823;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#30456;&#21516;LLM&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#36739;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
&lt;/p&gt;</description></item><item><title>OxfordVGG&#22242;&#38431;&#30340;WhisperX&#31995;&#32479;&#22312;EGO4D AV&#36716;&#24405;&#25361;&#25112;&#20013;&#20197;56.0&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#25490;&#21517;&#31532;1&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2307.09006</link><description>&lt;p&gt;
OxfordVGG&#23545;EGO4D AV&#36716;&#24405;&#25361;&#25112;&#30340;&#25552;&#20132;
&lt;/p&gt;
&lt;p&gt;
OxfordVGG Submission to the EGO4D AV Transcription Challenge. (arXiv:2307.09006v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09006
&lt;/p&gt;
&lt;p&gt;
OxfordVGG&#22242;&#38431;&#30340;WhisperX&#31995;&#32479;&#22312;EGO4D AV&#36716;&#24405;&#25361;&#25112;&#20013;&#20197;56.0&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#25490;&#21517;&#31532;1&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;OxfordVGG&#22242;&#38431;&#23545;EGO4D&#38899;&#39057;-&#35270;&#35273;&#65288;AV&#65289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#25361;&#25112;2023&#24180;&#19978;&#30340;&#25552;&#20132;&#30340;&#25216;&#26415;&#32454;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;WhisperX&#65292;&#19968;&#20010;&#29992;&#20110;&#38271;&#24418;&#38899;&#39057;&#30340;&#39640;&#25928;&#35821;&#38899;&#36716;&#24405;&#31995;&#32479;&#65292;&#20855;&#26377;&#21333;&#35789;&#32423;&#26102;&#38388;&#23545;&#40784;&#65292;&#20197;&#21450;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#25552;&#20132;&#22312;&#25361;&#25112;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#20102;56.0&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#22312;&#25490;&#34892;&#27036;&#19978;&#25490;&#21517;&#31532;1&#12290;&#25152;&#26377;&#22522;&#20934;&#20195;&#30721;&#21644;&#27169;&#22411;&#37117;&#21487;&#20197;&#22312;https://github.com/m-bain/whisperX&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report presents the technical details of our submission on the EGO4D Audio-Visual (AV) Automatic Speech Recognition Challenge 2023 from the OxfordVGG team. We present WhisperX, a system for efficient speech transcription of long-form audio with word-level time alignment, along with two text normalisers which are publicly available. Our final submission obtained 56.0% of the Word Error Rate (WER) on the challenge test set, ranked 1st on the leaderboard. All baseline codes and models are available on https://github.com/m-bain/whisperX.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23545;&#25239;&#32972;&#26223;&#19979;&#30340;&#20840;&#38754;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#22810;&#26657;&#20934;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#26080;&#38480;&#30340;&#22522;&#20934;&#20989;&#25968;&#31867;&#65292;&#24182;&#19988;&#26159;Oracle&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.08999</link><description>&lt;p&gt;
Oracle&#39640;&#25928;&#30340;&#22312;&#32447;&#22810;&#26657;&#20934;&#21644;&#20840;&#38754;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Oracle Efficient Online Multicalibration and Omniprediction. (arXiv:2307.08999v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23545;&#25239;&#32972;&#26223;&#19979;&#30340;&#20840;&#38754;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#22810;&#26657;&#20934;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#26080;&#38480;&#30340;&#22522;&#20934;&#20989;&#25968;&#31867;&#65292;&#24182;&#19988;&#26159;Oracle&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#34920;&#26126;&#65292;&#22810;&#26657;&#20934;&#65288;multicalibration&#65289;&#36825;&#19968;&#22810;&#32452;&#20844;&#24179;&#24615;&#27010;&#24565;&#19982;&#20840;&#38754;&#39044;&#27979;&#65288;omniprediction&#65289;&#36825;&#19968;&#20026;&#22823;&#37327;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#21516;&#26102;&#25439;&#22833;&#26368;&#23567;&#21270;&#20445;&#35777;&#30340;&#23398;&#20064;&#33539;&#24335;&#20043;&#38388;&#23384;&#22312;&#24847;&#24819;&#19981;&#21040;&#30340;&#32852;&#31995;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25209;&#22788;&#29702;&#35774;&#32622;&#19979;&#30340;&#20840;&#38754;&#39044;&#27979;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;&#22312;&#32447;&#23545;&#25239;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#20840;&#38754;&#39044;&#27979;&#12290;&#23613;&#31649;&#24050;&#32463;&#23384;&#22312;&#29992;&#20110;&#22312;&#32447;&#23545;&#25239;&#35774;&#32622;&#19979;&#33719;&#21462;&#22810;&#26657;&#20934;&#27010;&#24565;&#30340;&#31639;&#27861;&#65292;&#20294;&#19982;&#25209;&#22788;&#29702;&#31639;&#27861;&#19981;&#21516;&#65292;&#23427;&#20204;&#21482;&#36866;&#29992;&#20110;&#26377;&#38480;&#30340;&#22522;&#20934;&#20989;&#25968;&#31867;$F$&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;&#27599;&#19968;&#36718;&#26522;&#20030;&#27599;&#20010;&#20989;&#25968;$f \in F$&#12290;&#30456;&#21453;&#65292;&#20840;&#38754;&#39044;&#27979;&#23545;&#20110;&#23398;&#20064;&#29702;&#35770;&#30340;&#20551;&#35774;&#31867;$F$&#26368;&#26377;&#36259;&#65292;&#32780;&#36825;&#20123;&#31867;&#36890;&#24120;&#26159;&#36830;&#32493;&#22823;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#22810;&#26657;&#20934;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#26080;&#38480;&#30340;&#22522;&#20934;&#20989;&#25968;&#31867;$F$&#65292;&#24182;&#19988;&#26159;Oracle&#39640;&#25928;&#30340;&#65288;&#21363;&#23545;&#20110;&#20219;&#20309;&#31867;$F$&#65292;&#31639;&#27861;&#37117;&#21487;&#20197;&#36716;&#21270;&#20026;&#39640;&#25928;&#30340;&#32422;&#31616;&#24418;&#24335;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent line of work has shown a surprising connection between multicalibration, a multi-group fairness notion, and omniprediction, a learning paradigm that provides simultaneous loss minimization guarantees for a large family of loss functions. Prior work studies omniprediction in the batch setting. We initiate the study of omniprediction in the online adversarial setting. Although there exist algorithms for obtaining notions of multicalibration in the online adversarial setting, unlike batch algorithms, they work only for small finite classes of benchmark functions $F$, because they require enumerating every function $f \in F$ at every round. In contrast, omniprediction is most interesting for learning theoretic hypothesis classes $F$, which are generally continuously large.  We develop a new online multicalibration algorithm that is well defined for infinite benchmark classes $F$, and is oracle efficient (i.e. for any class $F$, the algorithm has the form of an efficient reduction 
&lt;/p&gt;</description></item><item><title>GraphCL-DTA&#26159;&#19968;&#31181;&#20351;&#29992;&#20998;&#23376;&#35821;&#20041;&#36827;&#34892;&#33647;&#29289;&#38774;&#26631;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#26412;&#36136;&#21644;&#26377;&#25928;&#30340;&#33647;&#29289;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.08989</link><description>&lt;p&gt;
GraphCL-DTA: &#19968;&#31181;&#22522;&#20110;&#20998;&#23376;&#35821;&#20041;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#33647;&#29289;&#38774;&#26631;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GraphCL-DTA: a graph contrastive learning with molecular semantics for drug-target binding affinity prediction. (arXiv:2307.08989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08989
&lt;/p&gt;
&lt;p&gt;
GraphCL-DTA&#26159;&#19968;&#31181;&#20351;&#29992;&#20998;&#23376;&#35821;&#20041;&#36827;&#34892;&#33647;&#29289;&#38774;&#26631;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#26412;&#36136;&#21644;&#26377;&#25928;&#30340;&#33647;&#29289;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#38774;&#26631;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#22312;&#33647;&#29289;&#30740;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21487;&#20197;&#25512;&#26029;&#26032;&#33647;&#21644;&#26032;&#38774;&#26631;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#24378;&#24230;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#35745;&#31639;&#27169;&#22411;&#22312;&#20197;&#19979;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65306;&#33647;&#29289;&#34920;&#31034;&#30340;&#23398;&#20064;&#20165;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#25968;&#25454;&#65292;&#27809;&#26377;&#32771;&#34385;&#20998;&#23376;&#22270;&#20013;&#25152;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20542;&#21521;&#20110;&#35774;&#35745;&#22797;&#26434;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#65292;&#24573;&#30053;&#20102;&#29992;&#20110;&#34913;&#37327;&#34920;&#31034;&#36136;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCL-DTA&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33647;&#29289;&#38774;&#26631;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#12290;&#22312;GraphCL-DTA&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#33647;&#29289;&#34920;&#31034;&#65292;&#20197;&#20445;&#30041;&#20998;&#23376;&#22270;&#30340;&#35821;&#20041;&#12290;&#36890;&#36807;&#36825;&#20010;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#26412;&#36136;&#21644;&#26377;&#25928;&#30340;&#33647;&#29289;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug-target binding affinity prediction plays an important role in the early stages of drug discovery, which can infer the strength of interactions between new drugs and new targets. However, the performance of previous computational models is limited by the following drawbacks. The learning of drug representation relies only on supervised data, without taking into account the information contained in the molecular graph itself. Moreover, most previous studies tended to design complicated representation learning module, while uniformity, which is used to measure representation quality, is ignored. In this study, we propose GraphCL-DTA, a graph contrastive learning with molecular semantics for drug-target binding affinity prediction. In GraphCL-DTA, we design a graph contrastive learning framework for molecular graphs to learn drug representations, so that the semantics of molecular graphs are preserved. Through this graph contrastive framework, a more essential and effective drug repre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#30697;&#38453;&#35889;&#23398;&#20064;&#19982;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#65292;&#36890;&#36807;&#30697;&#38453;&#31232;&#30095;&#21270;&#36807;&#31243;&#26469;&#20445;&#25345;&#39057;&#35889;&#65292;&#20174;&#32780;&#24471;&#21040;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#29256;&#26412;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#21098;&#26525;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08982</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#20316;&#20026;&#20445;&#25345;&#39057;&#35889;&#30340;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Network Pruning as Spectrum Preserving Process. (arXiv:2307.08982v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#30697;&#38453;&#35889;&#23398;&#20064;&#19982;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#65292;&#36890;&#36807;&#30697;&#38453;&#31232;&#30095;&#21270;&#36807;&#31243;&#26469;&#20445;&#25345;&#39057;&#35889;&#65292;&#20174;&#32780;&#24471;&#21040;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#29256;&#26412;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#21098;&#26525;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22823;&#37327;&#26435;&#37325;&#20351;&#20854;&#26080;&#27861;&#22312;&#26234;&#33021;&#25163;&#26426;&#21644;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#37096;&#32626;&#12290;&#33719;&#21462;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#29256;&#26412;&#20197;&#36827;&#34892;&#36793;&#32536;&#35774;&#22791;&#25512;&#29702;&#26159;&#38750;&#24120;&#29702;&#24819;&#30340;&#12290;&#35768;&#22810;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#29992;&#20110;&#20462;&#21098;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#35265;&#19988;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#31264;&#23494;&#21644;&#21367;&#31215;&#23618;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#32479;&#19968;&#29702;&#35770;&#22522;&#30784;&#22823;&#22810;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#30697;&#38453;&#35889;&#23398;&#20064;&#19982;&#31264;&#23494;&#21644;&#21367;&#31215;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20043;&#38388;&#23384;&#22312;&#23494;&#20999;&#32852;&#31995;&#65292;&#24182;&#35748;&#20026;&#26435;&#37325;&#20462;&#21098;&#26412;&#36136;&#19978;&#26159;&#19968;&#31181;&#20445;&#25345;&#39057;&#35889;&#30340;&#30697;&#38453;&#31232;&#30095;&#21270;&#36807;&#31243;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#30340;&#30697;&#38453;&#31232;&#30095;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#20462;&#21098;&#32467;&#26524;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#21644;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have achieved remarkable performance in various application domains. Nevertheless, a large number of weights in pre-trained deep neural networks prohibit them from being deployed on smartphones and embedded systems. It is highly desirable to obtain lightweight versions of neural networks for inference in edge devices. Many cost-effective approaches were proposed to prune dense and convolutional layers that are common in deep neural networks and dominant in the parameter space. However, a unified theoretical foundation for the problem mostly is missing. In this paper, we identify the close connection between matrix spectrum learning and neural network training for dense and convolutional layers and argue that weight pruning is essentially a matrix sparsification process to preserve the spectrum. Based on the analysis, we also propose a matrix sparsification algorithm tailored for neural network pruning that yields better pruning result. We carefully design and conduct ex
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#21644;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#26029;&#35266;&#23519;&#19979;&#32500;&#25252;&#24046;&#20998;&#31169;&#26377;&#36882;&#20943;&#21644;&#12290;&#31639;&#27861;&#25913;&#36827;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#36882;&#20943;&#26435;&#37325;&#19979;&#27809;&#26377;&#20056;&#27861;&#35823;&#24046;&#30340;&#24046;&#20998;&#31169;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08970</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#29992;&#20110;&#22312;&#19981;&#26029;&#35266;&#23519;&#19979;&#30340;&#24046;&#20998;&#31169;&#26377;&#21644;&#20043;&#21644;
&lt;/p&gt;
&lt;p&gt;
A Unifying Framework for Differentially Private Sums under Continual Observation. (arXiv:2307.08970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08970
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#21644;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#26029;&#35266;&#23519;&#19979;&#32500;&#25252;&#24046;&#20998;&#31169;&#26377;&#36882;&#20943;&#21644;&#12290;&#31639;&#27861;&#25913;&#36827;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#36882;&#20943;&#26435;&#37325;&#19979;&#27809;&#26377;&#20056;&#27861;&#35823;&#24046;&#30340;&#24046;&#20998;&#31169;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#19981;&#26029;&#35266;&#23519;&#19979;&#32500;&#25252;&#24046;&#20998;&#31169;&#26377;&#36882;&#20943;&#21644;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#21644;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23545;&#20110;\emph{&#20219;&#20309;&#36275;&#22815;&#24179;&#28369;}&#30340;&#20989;&#25968;&#37117;&#36866;&#29992;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#23545;&#20110;&#22810;&#39033;&#24335;&#36882;&#20943;&#26435;&#37325;&#27809;&#26377;&#20056;&#27861;&#35823;&#24046;&#30340;&#24046;&#20998;&#31169;&#26377;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#25913;&#36827;&#20102;&#20043;&#21069;&#25152;&#26377;&#22312;&#19981;&#26029;&#35266;&#23519;&#19979;&#30340;&#24046;&#20998;&#31169;&#26377;&#36882;&#20943;&#21644;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#20316;&#20026;&#19968;&#20010;&#25512;&#35770;&#65292;&#24674;&#22797;&#20102;Henzinger&#31561;&#20154;&#65288;SODA 2023&#65289;&#22312;&#36830;&#32493;&#35745;&#25968;&#29305;&#20363;&#20013;&#30340;&#21152;&#27861;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#22522;&#20110;&#22240;&#23376;&#21270;&#26426;&#21046;&#30340;&#21464;&#20307;&#65292;&#20854;&#35823;&#24046;&#21462;&#20915;&#20110;&#24213;&#23618;&#30697;&#38453;&#30340;$\gamma_2$&#21644;$\gamma_F$&#33539;&#25968;&#12290;&#25105;&#20204;&#20026;&#19968;&#22823;&#31867;&#19979;&#19977;&#35282;&#30697;&#38453;&#32473;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#31934;&#30830;&#30340;$\gamma_2$&#21644;$\gamma_F$&#33539;&#25968;&#30340;&#19978;&#30028;&#30340;&#26500;&#36896;&#24615;&#35777;&#26126;&#65292;&#24182;&#19988;&#20026;$\gamma_2$&#33539;&#25968;&#32473;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#32039;&#30830;&#30340;&#19979;&#30028;&#12290;&#36825;&#26159;&#23545;&#20110;&#19979;&#19977;&#35282;&#30697;&#38453;&#30340;&#31532;&#19968;&#20010;&#38750;&#24179;&#20961;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of maintaining a differentially private decaying sum under continual observation. We give a unifying framework and an efficient algorithm for this problem for \emph{any sufficiently smooth} function. Our algorithm is the first differentially private algorithm that does not have a multiplicative error for polynomially-decaying weights. Our algorithm improves on all prior works on differentially private decaying sums under continual observation and recovers exactly the additive error for the special case of continual counting from Henzinger et al. (SODA 2023) as a corollary.  Our algorithm is a variant of the factorization mechanism whose error depends on the $\gamma_2$ and $\gamma_F$ norm of the underlying matrix. We give a constructive proof for an almost exact upper bound on the $\gamma_2$ and $\gamma_F$ norm and an almost tight lower bound on the $\gamma_2$ norm for a large class of lower-triangular matrices. This is the first non-trivial lower bound for lower-tr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26223;&#35266;&#26367;&#20195;&#21697;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#37096;&#20998;&#20449;&#24687;&#19979;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20248;&#21270;&#22120;&#26469;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08964</link><description>&lt;p&gt;
&#26223;&#35266;&#26367;&#20195;&#21697;&#65306;&#22312;&#37096;&#20998;&#20449;&#24687;&#19979;&#23398;&#20064;&#25968;&#23398;&#20248;&#21270;&#30340;&#20915;&#31574;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information. (arXiv:2307.08964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26223;&#35266;&#26367;&#20195;&#21697;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#37096;&#20998;&#20449;&#24687;&#19979;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20248;&#21270;&#22120;&#26469;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23398;&#20064;&#38598;&#25104;&#20248;&#21270;&#24037;&#20316;&#22312;&#20248;&#21270;&#38382;&#39064;&#21482;&#26377;&#37096;&#20998;&#21487;&#35266;&#27979;&#25110;&#36890;&#29992;&#20248;&#21270;&#22120;&#22312;&#26080;&#19987;&#23478;&#35843;&#20248;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#30340;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20248;&#21270;&#22120;$ \mathbf{g} $&#26469;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#12290;&#20248;&#21270;&#22120;&#21487;&#20197;&#36890;&#36807;&#24050;&#30693;&#26368;&#20248;&#35299;&#30340;&#30417;&#30563;&#25110;&#36890;&#36807;&#20248;&#21270;&#22797;&#21512;&#20989;&#25968;$ f\circ \mathbf{g} $&#30340;&#38544;&#24335;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#38544;&#24335;&#26041;&#27861;&#21487;&#33021;&#19981;&#38656;&#35201;&#26368;&#20248;&#35299;&#20316;&#20026;&#26631;&#31614;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#39057;&#32321;&#35843;&#29992;&#20248;&#21270;&#22120;$ \mathbf{g} $&#65292;&#22240;&#27492;&#35757;&#32451;&#21644;&#37096;&#32626;&#32531;&#24930;&#12290;&#23545;&#20110;&#32452;&#21512;&#27714;&#35299;&#22120;&#65292;&#30001;&#20110;$ \mathbf{g} $&#30340;&#31232;&#30095;&#26799;&#24230;&#65292;&#35757;&#32451;&#36827;&#19968;&#27493;&#21463;&#21040;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24179;&#28369;&#21487;&#23398;&#20064;&#30340;&#26223;&#35266;&#26367;&#20195;&#21697;$ M $&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer $\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\circ \mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\mathbf{g}$ during both training and testing. The training is further challenged by sparse gradients of $\mathbf{g}$, especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as a replace
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;AI&#20195;&#29702;&#26041;&#27861;REX&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#22870;&#21169;&#23618;&#21644;&#31867;&#20284;&#20110;UCB&#20998;&#25968;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;AI&#20195;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#31163;&#32447;&#34892;&#20026;&#21033;&#29992;&#21644;&#19982;&#22522;&#30784;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.08962</link><description>&lt;p&gt;
REX: &#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#22686;&#24378;&#22411;AI&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
REX: Rapid Exploration and eXploitation for AI Agents. (arXiv:2307.08962v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;AI&#20195;&#29702;&#26041;&#27861;REX&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#22870;&#21169;&#23618;&#21644;&#31867;&#20284;&#20110;UCB&#20998;&#25968;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;AI&#20195;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#31163;&#32447;&#34892;&#20026;&#21033;&#29992;&#21644;&#19982;&#22522;&#30784;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;AI&#20195;&#29702;&#26041;&#27861;&#65292;&#31216;&#20026;REX&#12290;&#29616;&#26377;&#30340;AutoGPT&#39118;&#26684;&#25216;&#26415;&#23384;&#22312;&#19968;&#20123;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#22914;&#23545;&#20110;&#20915;&#31574;&#30340;&#31934;&#30830;&#25551;&#36848;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#20197;&#21450;&#32570;&#20047;&#31867;&#20284;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;(Reinforcement Learning&#65292;RL)&#20013;&#30340;&#23581;&#35797;&#21644;&#22833;&#36133;&#31243;&#24207;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#12290;REX&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22870;&#21169;&#23618;&#65292;&#24182;&#38598;&#25104;&#20102;&#31867;&#20284;&#20110;&#19978;&#38480;&#32622;&#20449;&#30028;&#38480;(UCB)&#20998;&#25968;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;AI&#20195;&#29702;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#26159;&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;&#26085;&#24535;&#30340;&#31163;&#32447;&#34892;&#20026;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#24494;&#35843;&#12290;&#36890;&#36807;&#19982;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#24605;&#32500;&#38142;(CoT)&#21644;&#35268;&#21010;&#25512;&#29702;(RAP)&#65289;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#22522;&#20110;REX&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#36229;&#36807;&#20102;&#36825;&#20123;&#29616;&#26377;&#25216;&#26415;&#25152;&#21462;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniqu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#21270;&#25216;&#26415;&#21644;&#38598;&#25104;&#26041;&#27861;&#22312;&#25552;&#39640;IoT&#35774;&#22791;&#35782;&#21035;&#27169;&#22411;&#31283;&#20581;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#25239;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#21644;&#20943;&#23569;&#22122;&#22768;&#25110;&#38169;&#35823;&#30340;&#24433;&#21709;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08955</link><description>&lt;p&gt;
&#22522;&#20110;&#31163;&#25955;&#21270;&#30340;&#38598;&#25104;&#27169;&#22411;&#29992;&#20110;IoT&#20013;&#30340;&#20581;&#22766;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Discretization-based ensemble model for robust learning in IoT. (arXiv:2307.08955v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#21270;&#25216;&#26415;&#21644;&#38598;&#25104;&#26041;&#27861;&#22312;&#25552;&#39640;IoT&#35774;&#22791;&#35782;&#21035;&#27169;&#22411;&#31283;&#20581;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#25239;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#21644;&#20943;&#23569;&#22122;&#22768;&#25110;&#38169;&#35823;&#30340;&#24433;&#21709;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
IoT&#35774;&#22791;&#35782;&#21035;&#26159;&#35782;&#21035;&#21644;&#39564;&#35777;&#36830;&#25509;&#21040;&#32593;&#32476;&#30340;IoT&#35774;&#22791;&#30340;&#36807;&#31243;&#12290;&#36825;&#26159;&#30830;&#20445;&#21482;&#26377;&#25480;&#26435;&#35774;&#22791;&#21487;&#20197;&#35775;&#38382;&#32593;&#32476;&#30340;&#37325;&#35201;&#36807;&#31243;&#65292;&#23545;&#20110;&#32593;&#32476;&#31649;&#29702;&#21644;&#32500;&#25252;&#26469;&#35828;&#26159;&#24517;&#35201;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24191;&#27867;&#29992;&#20110;&#33258;&#21160;&#21270;&#32593;&#32476;&#20013;&#30340;&#35774;&#22791;&#35782;&#21035;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#24433;&#21709;&#65292;&#20174;&#32780;&#24433;&#21709;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20445;&#25252;&#35774;&#22791;&#35782;&#21035;&#27169;&#22411;&#65292;&#31163;&#25955;&#21270;&#25216;&#26415;&#33021;&#22815;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38598;&#25104;&#26041;&#27861;&#23558;&#22810;&#20010;&#24322;&#36136;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20943;&#23569;&#27169;&#22411;&#20013;&#21097;&#20313;&#22122;&#22768;&#25110;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#31163;&#25955;&#21270;&#25216;&#26415;&#21644;&#38598;&#25104;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#30740;&#31350;&#20854;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#27169;&#22411;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoT device identification is the process of recognizing and verifying connected IoT devices to the network. This is an essential process for ensuring that only authorized devices can access the network, and it is necessary for network management and maintenance. In recent years, machine learning models have been used widely for automating the process of identifying devices in the network. However, these models are vulnerable to adversarial attacks that can compromise their accuracy and effectiveness. To better secure device identification models, discretization techniques enable reduction in the sensitivity of machine learning models to adversarial attacks contributing to the stability and reliability of the model. On the other hand, Ensemble methods combine multiple heterogeneous models to reduce the impact of remaining noise or errors in the model. Therefore, in this paper, we integrate discretization techniques and ensemble methods and examine it on model robustness against adversar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#65292;&#20351;&#29992;&#36716;&#25442;&#22120;&#32593;&#32476;LFIT&#20174;&#20808;&#21069;&#30340;&#30693;&#35782;&#21644;&#22810;&#28304;&#25968;&#25454;&#20013;&#23398;&#20064;&#28369;&#22369;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#39044;&#27979;&#28369;&#22369;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#39640;&#20102;&#28369;&#22369;&#39044;&#27979;&#30340;&#32508;&#21512;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08951</link><description>&lt;p&gt;
&#30693;&#35782;&#27880;&#20837;&#30340;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#28369;&#22369;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge-infused Deep Learning Enables Interpretable Landslide Forecasting. (arXiv:2307.08951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08951
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#65292;&#20351;&#29992;&#36716;&#25442;&#22120;&#32593;&#32476;LFIT&#20174;&#20808;&#21069;&#30340;&#30693;&#35782;&#21644;&#22810;&#28304;&#25968;&#25454;&#20013;&#23398;&#20064;&#28369;&#22369;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#39044;&#27979;&#28369;&#22369;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#39640;&#20102;&#28369;&#22369;&#39044;&#27979;&#30340;&#32508;&#21512;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#28369;&#22369;&#22312;&#26102;&#38388;&#19978;&#22914;&#20309;&#28436;&#21464;&#25110;&#32773;&#23427;&#20204;&#26159;&#21542;&#20250;&#21457;&#29983;&#25925;&#38556;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20869;&#37096;&#21644;&#22806;&#37096;&#30340;&#22810;&#31181;&#22240;&#32032;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#21066;&#24369;&#20102;&#23427;&#20204;&#20135;&#29983;&#30340;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;&#26368;&#36817;&#21457;&#23637;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#20026;&#29992;&#20110;&#39044;&#27979;&#28369;&#22369;&#25552;&#20379;&#20102;&#26410;&#34987;&#24320;&#21457;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#65292;&#33021;&#22815;&#20840;&#38754;&#39044;&#27979;&#28369;&#22369;&#30340;&#34892;&#20026;&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;LFIT&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#32593;&#32476;&#20174;&#20808;&#21069;&#30340;&#30693;&#35782;&#21644;&#22810;&#28304;&#25968;&#25454;&#20013;&#23398;&#20064;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#35782;&#21035;&#26368;&#30456;&#20851;&#30340;&#21464;&#37327;&#65292;&#24182;&#23637;&#31034;&#23545;&#28369;&#22369;&#28436;&#21270;&#21644;&#26102;&#38388;&#27169;&#24335;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#32508;&#21512;&#28369;&#22369;&#39044;&#27979;&#30340;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting how landslides will evolve over time or whether they will fail is a challenging task due to a variety of factors, both internal and external. Despite their considerable potential to address these challenges, deep learning techniques lack interpretability, undermining the credibility of the forecasts they produce. The recent development of transformer-based deep learning offers untapped possibilities for forecasting landslides with unprecedented interpretability and nonlinear feature learning capabilities. Here, we present a deep learning pipeline that is capable of predicting landslide behavior holistically, which employs a transformer-based network called LFIT to learn complex nonlinear relationships from prior knowledge and multiple source data, identifying the most relevant variables, and demonstrating a comprehensive understanding of landslide evolution and temporal patterns. By integrating prior knowledge, we provide improvement in holistic landslide forecasting, enabl
&lt;/p&gt;</description></item><item><title>Alioth&#26159;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#30417;&#35270;&#22120;&#65292;&#29992;&#20110;&#30417;&#27979;&#20844;&#20849;&#20113;&#20013;&#22810;&#31199;&#25143;&#24212;&#29992;&#30340;&#24178;&#25200;&#24341;&#36215;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#26500;&#24314;&#21453;&#26144;&#30495;&#23454;&#22330;&#26223;&#20013;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;Alioth&#33021;&#22815;&#25552;&#20379;&#23545;&#20110;&#30456;&#20851;&#24178;&#25200;&#20107;&#20214;&#30340;&#24863;&#30693;&#21644;&#20998;&#26512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.08949</link><description>&lt;p&gt;
Alioth: &#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20844;&#20849;&#20113;&#22810;&#31199;&#25143;&#24212;&#29992;&#24178;&#25200;&#24863;&#30693;&#24615;&#33021;&#30417;&#35270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Alioth: A Machine Learning Based Interference-Aware Performance Monitor for Multi-Tenancy Applications in Public Cloud. (arXiv:2307.08949v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08949
&lt;/p&gt;
&lt;p&gt;
Alioth&#26159;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#30417;&#35270;&#22120;&#65292;&#29992;&#20110;&#30417;&#27979;&#20844;&#20849;&#20113;&#20013;&#22810;&#31199;&#25143;&#24212;&#29992;&#30340;&#24178;&#25200;&#24341;&#36215;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#26500;&#24314;&#21453;&#26144;&#30495;&#23454;&#22330;&#26223;&#20013;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;Alioth&#33021;&#22815;&#25552;&#20379;&#23545;&#20110;&#30456;&#20851;&#24178;&#25200;&#20107;&#20214;&#30340;&#24863;&#30693;&#21644;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#20113;&#20013;&#30340;&#22810;&#31199;&#25143;&#21487;&#33021;&#23548;&#33268;&#20849;&#20139;&#36164;&#28304;&#30340;&#20849;&#23384;&#24178;&#25200;&#65292;&#21487;&#33021;&#23548;&#33268;&#20113;&#24212;&#29992;&#24615;&#33021;&#19979;&#38477;&#12290;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#24076;&#26395;&#22312;&#21457;&#29983;&#27492;&#31867;&#20107;&#20214;&#26102;&#30693;&#36947;&#21457;&#29983;&#30340;&#26102;&#38388;&#20197;&#21450;&#24615;&#33021;&#19979;&#38477;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#20197;&#25191;&#34892;&#24178;&#25200;&#24863;&#30693;&#36801;&#31227;&#24182;&#32531;&#35299;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22522;&#30784;&#35774;&#26045;&#21363;&#26381;&#21153;&#65288;IaaS&#65289;&#20844;&#20849;&#20113;&#20013;&#30340;&#34394;&#25311;&#26426;&#65288;VM&#65289;&#23545;&#20110;&#25552;&#20379;&#21830;&#26469;&#35828;&#26159;&#40657;&#30418;&#23376;&#65292;&#26080;&#27861;&#33719;&#21462;&#24212;&#29992;&#32423;&#24615;&#33021;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#24615;&#33021;&#30417;&#35270;&#21464;&#24471;&#24322;&#24120;&#22256;&#38590;&#65292;&#22240;&#20026;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#21482;&#33021;&#20381;&#36182;&#20110;&#20302;&#32423;&#25351;&#26631;&#65292;&#22914;CPU&#20351;&#29992;&#29575;&#21644;&#30828;&#20214;&#35745;&#25968;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;Alioth&#65292;&#29992;&#20110;&#30417;&#35270;&#20113;&#24212;&#29992;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#20026;&#25968;&#25454;&#23494;&#38598;&#22411;&#27169;&#22411;&#25552;&#20379;&#25968;&#25454;&#65292;&#25105;&#20204;&#39318;&#20808;&#38416;&#36848;&#24178;&#25200;&#29983;&#25104;&#22120;&#65292;&#24182;&#22312;&#23454;&#39564;&#24179;&#21488;&#19978;&#36827;&#34892;&#20840;&#38754;&#30340;&#20849;&#23384;&#23454;&#39564;&#65292;&#26500;&#24314;&#21453;&#26144;&#29616;&#23454;&#22330;&#26223;&#20013;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#24615;&#30340;Alioth&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-tenancy in public clouds may lead to co-location interference on shared resources, which possibly results in performance degradation of cloud applications. Cloud providers want to know when such events happen and how serious the degradation is, to perform interference-aware migrations and alleviate the problem. However, virtual machines (VM) in Infrastructure-as-a-Service public clouds are black-boxes to providers, where application-level performance information cannot be acquired. This makes performance monitoring intensely challenging as cloud providers can only rely on low-level metrics such as CPU usage and hardware counters.  We propose a novel machine learning framework, Alioth, to monitor the performance degradation of cloud applications. To feed the data-hungry models, we first elaborate interference generators and conduct comprehensive co-location experiments on a testbed to build Alioth-dataset which reflects the complexity and dynamicity in real-world scenarios. Then w
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCoLe&#30340;&#20462;&#21098;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#26631;&#31614;&#20559;&#35265;&#38382;&#39064;&#12290;&#30740;&#31350;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08945</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#32622;&#20449;&#23398;&#20064;&#26469;&#20943;&#32531;&#26631;&#31614;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Bias via Decoupled Confident Learning. (arXiv:2307.08945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08945
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCoLe&#30340;&#20462;&#21098;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#26631;&#31614;&#20559;&#35265;&#38382;&#39064;&#12290;&#30740;&#31350;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#25285;&#24551;&#22686;&#21152;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#20943;&#36731;&#31639;&#27861;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#26631;&#31614;&#20559;&#35265;&#22312;&#21253;&#25324;&#21307;&#30103;&#12289;&#25307;&#32856;&#21644;&#20869;&#23481;&#23457;&#26680;&#22312;&#20869;&#30340;&#37325;&#35201;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#29305;&#21035;&#26159;&#65292;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#23481;&#26131;&#24102;&#26377;&#31038;&#20250;&#20559;&#35265;&#12290;&#34429;&#28982;&#26631;&#31614;&#20559;&#35265;&#30340;&#23384;&#22312;&#24050;&#32463;&#22312;&#27010;&#24565;&#19978;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#20294;&#32570;&#20047;&#24212;&#23545;&#27492;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#21098;&#26041;&#27861;&#8212;&#8212;&#35299;&#32806;&#32622;&#20449;&#23398;&#20064;&#65288;DeCoLe&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#26469;&#20943;&#36731;&#26631;&#31614;&#20559;&#35265;&#12290;&#22312;&#28436;&#31034;&#20102;&#20854;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#21518;&#65292;&#25105;&#20204;&#23558;DeCoLe&#24212;&#29992;&#20110;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#34987;&#35748;&#20026;&#26159;&#37325;&#35201;&#25361;&#25112;&#30340;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20854;&#25104;&#21151;&#35782;&#21035;&#20986;&#20559;&#35265;&#26631;&#31614;&#24182;&#36229;&#36234;&#31454;&#20105;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing concerns regarding algorithmic fairness have led to a surge in methodologies to mitigate algorithmic bias. However, such methodologies largely assume that observed labels in training data are correct. This is problematic because bias in labels is pervasive across important domains, including healthcare, hiring, and content moderation. In particular, human-generated labels are prone to encoding societal biases. While the presence of labeling bias has been discussed conceptually, there is a lack of methodologies to address this problem. We propose a pruning method -- Decoupled Confident Learning (DeCoLe) -- specifically designed to mitigate label bias. After illustrating its performance on a synthetic dataset, we apply DeCoLe in the context of hate speech detection, where label bias has been recognized as an important challenge, and show that it successfully identifies biased labels and outperforms competing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Siamese&#32593;&#32476;&#36827;&#34892;&#24369;&#30417;&#30563;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20165;&#21033;&#29992;&#25968;&#25454;&#26679;&#26412;&#23545;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#19981;&#21516;&#32858;&#31867;&#31639;&#27861;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08944</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;Siamese&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Siamese Networks for Weakly Supervised Human Activity Recognition. (arXiv:2307.08944v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Siamese&#32593;&#32476;&#36827;&#34892;&#24369;&#30417;&#30563;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20165;&#21033;&#29992;&#25968;&#25454;&#26679;&#26412;&#23545;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#19981;&#21516;&#32858;&#31867;&#31639;&#27861;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#26126;&#30830;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#22810;&#20010;Siamese&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#20165;&#21033;&#29992;&#25968;&#25454;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#30693;&#36947;&#26126;&#30830;&#30340;&#26631;&#31614;&#12290;&#35757;&#32451;&#21518;&#30340;&#27169;&#22411;&#23558;&#27963;&#21160;&#25968;&#25454;&#26679;&#26412;&#26144;&#23556;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#34920;&#31034;&#21521;&#37327;&#65292;&#20351;&#24471;&#34920;&#31034;&#31354;&#38388;&#20013;&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#36817;&#20284;&#20110;&#36755;&#20837;&#31354;&#38388;&#20013;&#25968;&#25454;&#26679;&#26412;&#30340;&#30456;&#20284;&#24615;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#21518;&#30340;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24191;&#27867;&#30340;&#19981;&#21516;&#32858;&#31867;&#31639;&#27861;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#35757;&#32451;&#36807;&#31243;&#36890;&#36807;&#26368;&#23567;&#21270;&#30456;&#20284;&#24615;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#24471;&#21516;&#19968;&#31181;&#27963;&#21160;&#30340;&#26679;&#26412;&#23545;&#30340;&#36317;&#31163;&#24230;&#37327;&#23567;&#65292;&#24182;&#20351;&#24471;&#19981;&#21516;&#31181;&#27963;&#21160;&#30340;&#26679;&#26412;&#23545;&#30340;&#36317;&#31163;&#24230;&#37327;&#22823;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#39564;&#35777;&#20854;&#22312;&#20998;&#21106;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been successfully applied to human activity recognition. However, training deep neural networks requires explicitly labeled data which is difficult to acquire. In this paper, we present a model with multiple siamese networks that are trained by using only the information about the similarity between pairs of data samples without knowing the explicit labels. The trained model maps the activity data samples into fixed size representation vectors such that the distance between the vectors in the representation space approximates the similarity of the data samples in the input space. Thus, the trained model can work as a metric for a wide range of different clustering algorithms. The training process minimizes a similarity loss function that forces the distance metric to be small for pairs of samples from the same kind of activity, and large for pairs of samples from different kinds of activities. We evaluate the model on three datasets to verify its effectiveness in segm
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#36817;&#20284;MLP&#34701;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08941</link><description>&lt;p&gt;
NTK-&#36817;&#20284;MLP&#34701;&#21512;&#29992;&#20110;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. (arXiv:2307.08941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#36817;&#20284;MLP&#34701;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#24050;&#25104;&#20026;&#20027;&#35201;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#24494;&#35843;PLM&#21644;&#36827;&#34892;&#25512;&#29702;&#20063;&#26159;&#26114;&#36149;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#33021;&#21147;&#36739;&#20302;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#19968;&#20123;&#36890;&#29992;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#37327;&#21270;&#21644;&#33976;&#39311;&#65289;&#26469;&#20943;&#23569;PLM&#24494;&#35843;&#30340;&#35745;&#31639;/&#23384;&#20648;&#24320;&#38144;&#65292;&#20294;&#24456;&#23569;&#26377;&#19968;&#27425;&#24615;&#21387;&#32553;&#25216;&#26415;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#27169;&#22359;&#20013;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;NTK&#36817;&#20284;MLP&#34701;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;PLM&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;MLP&#37325;&#26032;&#35270;&#20026;&#19968;&#26463;&#23376;MLP&#65292;&#24182;&#23558;&#23427;&#20204;&#32858;&#31867;&#20026;&#32473;&#23450;&#25968;&#37327;&#30340;&#36136;&#24515;&#65292;&#28982;&#21518;&#23558;&#20854;&#24674;&#22797;&#20026;&#21387;&#32553;&#30340;MLP&#65292;&#24182;&#24847;&#22806;&#22320;&#26174;&#31034;&#20986;&#23545;&#21407;&#22987;PLM&#30340;NTK&#36827;&#34892;&#33391;&#22909;&#36817;&#20284;&#30340;&#25928;&#26524;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#20197;&#39564;&#35777;PLM&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural l
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#22270;&#20687;&#25200;&#21160;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08939</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks. (arXiv:2307.08939v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08939
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#22270;&#20687;&#25200;&#21160;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39550;&#39542;&#21592;&#36741;&#21161;&#21151;&#33021;&#65292;&#29992;&#20110;&#20445;&#25345;&#26399;&#26395;&#36895;&#24230;&#21644;&#19982;&#21069;&#26041;&#36710;&#36742;&#30340;&#23433;&#20840;&#36317;&#31163;&#12290;&#26412;&#25991;&#35780;&#20272;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;ACC&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#35813;&#25915;&#20987;&#20250;&#23545;&#25668;&#20687;&#26426;&#25968;&#25454;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#25200;&#21160;&#65292;&#20197;&#23548;&#33268;&#21069;&#26041;&#30896;&#25758;&#20107;&#25925;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#65292;&#29992;&#20110;&#36873;&#25321;&#35302;&#21457;&#25915;&#20987;&#26368;&#20851;&#38190;&#30340;&#26102;&#38388;&#28857;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#36816;&#34892;&#26102;&#29983;&#25104;&#36866;&#24212;&#24615;&#22270;&#20687;&#25200;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#39550;&#39542;&#25968;&#25454;&#38598;&#21644;&#36924;&#30495;&#30340;&#20223;&#30495;&#24179;&#21488;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#20223;&#30495;&#24179;&#21488;&#20351;&#29992;&#20102;&#26469;&#33258;&#29983;&#20135;ACC&#31995;&#32479;&#30340;&#25511;&#21046;&#36719;&#20214;&#21644;&#29289;&#29702;&#19990;&#30028;&#39550;&#39542;&#27169;&#25311;&#22120;&#65292;&#24182;&#32771;&#34385;&#20102;&#39550;&#39542;&#21592;&#30340;&#24178;&#39044;&#20197;&#21450;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#21644;&#21069;&#21521;&#30896;&#25758;&#35686;&#31034;&#65288;FCW&#65289;&#31561;&#23433;&#20840;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive Cruise Control (ACC) is a widely used driver assistance feature for maintaining desired speed and safe distance to the leading vehicles. This paper evaluates the security of the deep neural network (DNN) based ACC systems under stealthy perception attacks that strategically inject perturbations into camera data to cause forward collisions. We present a combined knowledge-and-data-driven approach to design a context-aware strategy for the selection of the most critical times for triggering the attacks and a novel optimization-based method for the adaptive generation of image perturbations at run-time. We evaluate the effectiveness of the proposed attack using an actual driving dataset and a realistic simulation platform with the control software from a production ACC system and a physical-world driving simulator while considering interventions by the driver and safety features such as Automatic Emergency Braking (AEB) and Forward Collision Warning (FCW). Experimental results sh
&lt;/p&gt;</description></item><item><title>&#22810;&#38454;&#27573;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#35757;&#32451;&#36807;&#31243;&#20998;&#20026;&#19981;&#21516;&#38454;&#27573;&#65292;&#24182;&#20248;&#21270;&#25311;&#21512;&#27531;&#24046;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35889;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.08934</link><description>&lt;p&gt;
&#22810;&#38454;&#27573;&#31070;&#32463;&#32593;&#32476;&#65306;&#26426;&#22120;&#31934;&#24230;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multi-stage Neural Networks: Function Approximator of Machine Precision. (arXiv:2307.08934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08934
&lt;/p&gt;
&lt;p&gt;
&#22810;&#38454;&#27573;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#35757;&#32451;&#36807;&#31243;&#20998;&#20026;&#19981;&#21516;&#38454;&#27573;&#65292;&#24182;&#20248;&#21270;&#25311;&#21512;&#27531;&#24046;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35889;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#31185;&#23398;&#38382;&#39064;&#65292;&#32593;&#32476;&#30340;&#31934;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#34987;&#35748;&#20026;&#26159;&#36890;&#29992;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#21363;&#20351;&#20351;&#29992;&#22823;&#22411;&#32593;&#32476;&#21644;&#25193;&#23637;&#30340;&#35757;&#32451;&#36845;&#20195;&#65292;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#23558;&#39044;&#27979;&#35823;&#24046;&#38477;&#20302;&#21040;10&#30340;&#36127;5&#27425;&#26041;&#20197;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#38454;&#27573;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#35757;&#32451;&#36807;&#31243;&#20998;&#20026;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#27599;&#20010;&#38454;&#27573;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#65292;&#38024;&#23545;&#21069;&#19968;&#38454;&#27573;&#30340;&#27531;&#24046;&#36827;&#34892;&#20248;&#21270;&#25311;&#21512;&#12290;&#22312;&#36830;&#32493;&#30340;&#38454;&#27573;&#20013;&#65292;&#27531;&#24046;&#24133;&#24230;&#26174;&#33879;&#20943;&#23567;&#65292;&#24182;&#19988;&#19982;&#27531;&#24046;&#39057;&#29575;&#21576;&#21453;&#24130;&#24459;&#20851;&#31995;&#12290;&#22810;&#38454;&#27573;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#20943;&#23567;&#20102;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35889;&#20559;&#24046;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#25417;&#30446;&#26631;&#20989;&#25968;&#30340;&#39640;&#39057;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#38454;&#27573;&#35757;&#32451;&#22312;&#22238;&#24402;&#38382;&#39064;&#21644;&#29289;&#29702;&#38382;&#39064;&#30340;&#39044;&#27979;&#35823;&#24046;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning techniques are increasingly applied to scientific problems, where the precision of networks is crucial. Despite being deemed as universal function approximators, neural networks, in practice, struggle to reduce the prediction errors below $O(10^{-5})$ even with large network size and extended training iterations. To address this issue, we developed the multi-stage neural networks that divides the training process into different stages, with each stage using a new network that is optimized to fit the residue from the previous stage. Across successive stages, the residue magnitudes decreases substantially and follows an inverse power-law relationship with the residue frequencies. The multi-stage neural networks effectively mitigate the spectral biases associated with regular neural networks, enabling them to capture the high frequency feature of target functions. We demonstrate that the prediction error from the multi-stage training for both regression problems and physics-
&lt;/p&gt;</description></item><item><title>IxDRL&#26159;&#19968;&#31181;&#22522;&#20110;&#26377;&#36259;&#20998;&#26512;&#30340;&#26032;&#22411;&#21487;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24037;&#20855;&#21253;&#65292;&#20855;&#22791;&#33021;&#21147;&#24863;&#30693;&#26426;&#21046;&#65292;&#33021;&#22815;&#25552;&#20379;&#20154;&#31867;&#25805;&#20316;&#21592;&#23545;RL&#20195;&#29702;&#33021;&#21147;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;</title><link>http://arxiv.org/abs/2307.08933</link><description>&lt;p&gt;
IxDRL:&#19968;&#31181;&#22522;&#20110;&#26377;&#36259;&#20998;&#26512;&#30340;&#26032;&#22411;&#21487;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness. (arXiv:2307.08933v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08933
&lt;/p&gt;
&lt;p&gt;
IxDRL&#26159;&#19968;&#31181;&#22522;&#20110;&#26377;&#36259;&#20998;&#26512;&#30340;&#26032;&#22411;&#21487;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24037;&#20855;&#21253;&#65292;&#20855;&#22791;&#33021;&#21147;&#24863;&#30693;&#26426;&#21046;&#65292;&#33021;&#22815;&#25552;&#20379;&#20154;&#31867;&#25805;&#20316;&#21592;&#23545;RL&#20195;&#29702;&#33021;&#21147;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#22312;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#20855;&#26377;&#39640;&#32500;&#36755;&#20837;&#30340;&#22797;&#26434;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#20247;&#22810;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#32570;&#20047;&#24517;&#35201;&#30340;&#26426;&#21046;&#26469;&#25552;&#20379;&#20154;&#31867;&#23545;&#20854;&#33021;&#21147;&#30340;&#25972;&#20307;&#35270;&#22270;&#65292;&#36825;&#22312;&#20851;&#38190;&#24212;&#29992;&#20013;&#25104;&#20026;&#37319;&#29992;RL&#30340;&#38556;&#30861;&#65292;&#22240;&#20026;&#20195;&#29702;&#31243;&#24207;&#25152;&#20570;&#30340;&#20915;&#31574;&#21487;&#33021;&#20855;&#26377;&#37325;&#22823;&#21518;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RL&#31995;&#32479;&#26412;&#36136;&#19978;&#19981;&#20855;&#22791;&#33021;&#21147;&#24863;&#30693;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#24517;&#35201;&#30340;&#35299;&#37322;&#26426;&#21046;&#65292;&#20351;&#20154;&#31867;&#25805;&#20316;&#21592;&#33021;&#22815;&#23545;&#20854;&#33021;&#21147;&#26377;&#28145;&#20837;&#12289;&#25972;&#20307;&#30340;&#20102;&#35299;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;RL&#65288;xDRL&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26377;&#36259;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#22522;&#20110;&#26377;&#36259;&#20998;&#26512;&#25552;&#20379;&#22810;&#31181;RL&#20195;&#29702;&#30340;&#33021;&#21147;&#24230;&#37327;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;RL&#31639;&#27861;&#65292;&#24182;&#21407;&#29983;&#25903;&#25345;&#27969;&#34892;&#30340;RLLib&#24037;&#20855;&#21253;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#24037;&#20855;&#21253;&#22312;...&#65288;&#21407;&#25991;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
In recent years, advances in deep learning have resulted in a plethora of successes in the use of reinforcement learning (RL) to solve complex sequential decision tasks with high-dimensional inputs. However, existing systems lack the necessary mechanisms to provide humans with a holistic view of their competence, presenting an impediment to their adoption, particularly in critical applications where the decisions an agent makes can have significant consequences. Yet, existing RL-based systems are essentially competency-unaware in that they lack the necessary interpretation mechanisms to allow human operators to have an insightful, holistic view of their competency. Towards more explainable Deep RL (xDRL), we propose a new framework based on analyses of interestingness. Our tool provides various measures of RL agent competence stemming from interestingness analysis and is applicable to a wide range of RL algorithms, natively supporting the popular RLLib toolkit. We showcase the use of o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#30340;&#21363;&#26102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21442;&#25968;&#21270;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#31995;&#32479;&#20013;&#37117;&#21487;&#20197;&#24471;&#21040;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2307.08929</link><description>&lt;p&gt;
&#29992;&#20110;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#21442;&#25968;&#21270;&#30340;&#21363;&#26102;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On-the-fly machine learning for parametrization of the effective Hamiltonian. (arXiv:2307.08929v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#30340;&#21363;&#26102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21442;&#25968;&#21270;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#31995;&#32479;&#20013;&#37117;&#21487;&#20197;&#24471;&#21040;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#34987;&#24191;&#27867;&#29992;&#20110;&#39044;&#27979;&#21644;&#27169;&#25311;&#38081;&#30005;&#21644;&#24347;&#35947;&#38081;&#30005;&#20307;&#30340;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#22797;&#26434;&#65292;&#24456;&#38590;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#21644;/&#25110;&#22797;&#26434;&#32452;&#20998;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#30340;&#21363;&#26102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#12290;&#21442;&#25968;&#21270;&#26159;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#23436;&#25104;&#30340;&#65292;&#27599;&#19968;&#27493;&#39044;&#27979;&#33021;&#37327;&#12289;&#21147;&#21644;&#24212;&#21147;&#20197;&#21450;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#24403;&#19981;&#30830;&#23450;&#24615;&#36739;&#22823;&#26102;&#65292;&#25191;&#34892;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#20197;&#37325;&#26032;&#35757;&#32451;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#35745;&#31639;&#20219;&#20309;&#25152;&#32771;&#34385;&#31995;&#32479;&#30340;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#21442;&#25968;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#21644;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#20197;BaTiO3&#21644;Pb(Sc,Ta)O3&#20026;&#20363;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The first-principles-based effective Hamiltonian is widely used to predict and simulate the properties of ferroelectrics and relaxor ferroelectrics. However, the parametrization method of the effective Hamiltonian is complicated and hardly can resolve the systems with complex interactions and/or complex components. Here, we developed an on-the-fly machine learning approach to parametrize the effective Hamiltonian based on Bayesian linear regression. The parametrization is completed in molecular dynamics simulations, with the energy, forces and stress predicted at each step along with their uncertainties. First-principles calculations are executed when the uncertainties are large to retrain the parameters. This approach provides a universal and automatic way to compute the effective Hamiltonian parameters for any considered systems including complex systems which previous methods can not handle. BaTiO3 and Pb(Sc,Ta)O3 are taken as examples to show the accurateness of this approach compa
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#30340;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#31169;&#26377;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#36825;&#19977;&#20010;&#32452;&#20214;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#26045;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21644;&#28508;&#22312;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2307.08925</link><description>&lt;p&gt;
&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#20010;&#31435;&#22330;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Federated Large Language Model: A Position Paper. (arXiv:2307.08925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08925
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#30340;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#31169;&#26377;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#36825;&#19977;&#20010;&#32452;&#20214;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#26045;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21644;&#28508;&#22312;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#33719;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#24182;&#25214;&#21040;&#20102;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#65292;&#20294;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#24320;&#21457;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#20844;&#20849;&#39046;&#22495;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#21294;&#20047;&#20197;&#21450;&#23545;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#39033;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#20986;&#29616;&#20102;&#65292;&#23427;&#33021;&#22815;&#22312;&#20445;&#25345;&#20998;&#25955;&#25968;&#25454;&#30340;&#21516;&#26102;&#23454;&#29616;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;LLM&#30340;&#27010;&#24565;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#32852;&#37030;&#24335;LLM&#39044;&#35757;&#32451;&#12289;&#32852;&#37030;&#24335;LLM&#24494;&#35843;&#21644;&#32852;&#37030;&#24335;LLM&#25552;&#31034;&#24037;&#31243;&#12290;&#23545;&#20110;&#27599;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#30456;&#23545;&#20110;&#20256;&#32479;LLM&#35757;&#32451;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#24037;&#31243;&#31574;&#30053;&#26469;&#23454;&#26045;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#20998;&#26512;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#30830;&#23450;&#21487;&#33021;&#30340;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles fac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#39564;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#65306;&#27809;&#26377;&#36890;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#31574;&#30053;&#33021;&#20445;&#35777;&#20803;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65307;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#27424;&#25311;&#21512;&#25110;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65307;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;ASr&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#26469;&#37319;&#26679;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21644;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;ASr&#12290;&#22823;&#37327;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;ASr&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08924</link><description>&lt;p&gt;
&#23398;&#20064;&#37319;&#26679;&#20219;&#21153;&#29992;&#20110;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Sample Tasks for Meta Learning. (arXiv:2307.08924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08924
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#65306;&#27809;&#26377;&#36890;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#31574;&#30053;&#33021;&#20445;&#35777;&#20803;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65307;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#27424;&#25311;&#21512;&#25110;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65307;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;ASr&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#26469;&#37319;&#26679;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21644;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;ASr&#12290;&#22823;&#37327;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;ASr&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#21508;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#12289;&#20219;&#21153;&#37319;&#26679;&#22120;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#26412;&#25991;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#12290;&#39318;&#20808;&#65292;&#27809;&#26377;&#36890;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#31574;&#30053;&#33021;&#20445;&#35777;&#20803;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#27424;&#25311;&#21512;&#25110;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#37319;&#26679;&#22120;&#65288;ASr&#65289;&#12290;ASr&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#26469;&#37319;&#26679;&#20219;&#21153;&#12290;&#20026;&#20102;&#20248;&#21270;ASr&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#22823;&#37327;&#30340;&#23454;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;ASr&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through experiments on various meta-learning methods, task samplers, and few-shot learning tasks, this paper arrives at three conclusions. Firstly, there are no universal task sampling strategies to guarantee the performance of meta-learning models. Secondly, task diversity can cause the models to either underfit or overfit during training. Lastly, the generalization performance of the models are influenced by task divergence, task entropy, and task difficulty. In response to these findings, we propose a novel task sampler called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes task divergence, task entropy, and task difficulty to sample tasks. To optimize ASr, we rethink and propose a simple and general meta-learning algorithm. Finally, a large number of empirical experiments demonstrate the effectiveness of the proposed ASr.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20048;&#35266;&#20272;&#35745;&#26041;&#27861;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;&#38750;&#32447;&#24615;&#27169;&#22411;&#22312;&#25311;&#21512;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;DNN&#30340;&#26550;&#26500;&#35774;&#35745;&#21407;&#21017;&#12290;</title><link>http://arxiv.org/abs/2307.08921</link><description>&lt;p&gt;
&#20048;&#35266;&#20272;&#35745;&#25581;&#31034;&#20102;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Optimistic Estimate Uncovers the Potential of Nonlinear Models. (arXiv:2307.08921v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08921
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20048;&#35266;&#20272;&#35745;&#26041;&#27861;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;&#38750;&#32447;&#24615;&#27169;&#22411;&#22312;&#25311;&#21512;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;DNN&#30340;&#26550;&#26500;&#35774;&#35745;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#26368;&#20339;&#25311;&#21512;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#20048;&#35266;&#26679;&#26412;&#22823;&#23567;&#65292;&#29992;&#20110;&#30830;&#23450;&#20351;&#29992;&#38750;&#32447;&#24615;&#27169;&#22411;&#26469;&#25311;&#21512;&#25110;&#24674;&#22797;&#30446;&#26631;&#20989;&#25968;&#25152;&#38656;&#30340;&#26368;&#23567;&#26679;&#26412;&#22823;&#23567;&#12290;&#25105;&#20204;&#20272;&#35745;&#20102;&#30697;&#38453;&#22240;&#24335;&#20998;&#35299;&#27169;&#22411;&#12289;&#28145;&#24230;&#27169;&#22411;&#21644;&#20855;&#26377;&#20840;&#36830;&#25509;&#25110;&#21367;&#31215;&#32467;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#20048;&#35266;&#26679;&#26412;&#22823;&#23567;&#12290;&#23545;&#20110;&#27599;&#20010;&#38750;&#32447;&#24615;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#39044;&#27979;&#20102;&#21487;&#20197;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#25311;&#21512;&#30340;&#29305;&#23450;&#30446;&#26631;&#23376;&#38598;&#65292;&#36825;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#12290;&#25105;&#20204;&#30340;&#20048;&#35266;&#20272;&#35745;&#25581;&#31034;&#20102;DNN&#27169;&#22411;&#30340;&#20004;&#20010;&#29305;&#27530;&#23646;&#24615;--&#23485;&#24230;&#19978;&#30340;&#33258;&#30001;&#34920;&#36798;&#21644;&#36830;&#25509;&#19978;&#30340;&#26114;&#36149;&#34920;&#36798;&#12290;&#36825;&#20123;&#23646;&#24615;&#25552;&#31034;&#20102;DNN&#30340;&#20197;&#19979;&#26550;&#26500;&#35774;&#35745;&#21407;&#21017;&#65306;(i)&#38543;&#24847;&#22686;&#21152;&#31070;&#32463;&#20803;/&#26680;&#65307;(ii)&#36991;&#20813;&#36830;&#25509;&#31070;&#32463;&#20803;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#30340;&#20048;&#35266;&#20272;&#35745;&#22312;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;&#38750;&#32447;&#24615;&#27169;&#22411;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#25311;&#21512;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an optimistic estimate to evaluate the best possible fitting performance of nonlinear models. It yields an optimistic sample size that quantifies the smallest possible sample size to fit/recover a target function using a nonlinear model. We estimate the optimistic sample sizes for matrix factorization models, deep models, and deep neural networks (DNNs) with fully-connected or convolutional architecture. For each nonlinear model, our estimates predict a specific subset of targets that can be fitted at overparameterization, which are confirmed by our experiments. Our optimistic estimate reveals two special properties of the DNN models -- free expressiveness in width and costly expressiveness in connection. These properties suggest the following architecture design principles of DNNs: (i) feel free to add neurons/kernels; (ii) restrain from connecting neurons. Overall, our optimistic estimate theoretically unveils the vast potential of nonlinear models in fitting at overparame
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#26032;&#30340;&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#36825;&#20123;&#31639;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#30340;&#22797;&#26434;&#24615;&#12289;&#25968;&#20540;&#26465;&#20214;&#21644;&#32500;&#24230;&#25193;&#23637;&#31561;&#35774;&#35745;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.08920</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;: &#20855;&#26377;&#29702;&#35770;&#27934;&#23519;&#21147;&#21644;&#24615;&#33021;&#20445;&#35777;&#30340;&#26032;&#35774;&#35745;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continuous-Time Reinforcement Learning: New Design Algorithms with Theoretical Insights and Performance Guarantees. (arXiv:2307.08920v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#26032;&#30340;&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#36825;&#20123;&#31639;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#30340;&#22797;&#26434;&#24615;&#12289;&#25968;&#20540;&#26465;&#20214;&#21644;&#32500;&#24230;&#25193;&#23637;&#31561;&#35774;&#35745;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#38750;&#32447;&#24615;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#32463;&#21382;&#20102;&#20960;&#21313;&#24180;&#30340;&#21457;&#23637;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#35774;&#35745;&#26041;&#27861;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#23545;&#29616;&#26377;&#30340;&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#65288;CT-RL&#65289;&#26041;&#27861;&#36827;&#34892;&#30340;&#20840;&#38754;&#20998;&#26512;&#25581;&#31034;&#20102;&#23427;&#20204;&#38754;&#20020;&#37325;&#22823;&#30340;&#35774;&#35745;&#25361;&#25112;&#65292;&#21253;&#25324;&#22797;&#26434;&#24615;&#12289;&#25968;&#20540;&#26465;&#20214;&#21644;&#32500;&#24230;&#25193;&#23637;&#38382;&#39064;&#12290;&#23613;&#31649;&#26377;&#20808;&#36827;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#30340;ADP CT-RL&#32508;&#21512;&#26041;&#27861;&#22312;&#35299;&#20915;&#21363;&#20351;&#26159;&#23567;&#30340;&#23398;&#26415;&#38382;&#39064;&#26102;&#37117;&#19981;&#36275;&#22815;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20171;&#32461;&#19968;&#22871;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26032;&#30340;CT-RL&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#26041;&#27861;&#20381;&#36182;&#20110;&#20004;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21487;&#20197;&#21010;&#20998;&#20026;&#36739;&#23567;&#23376;&#38382;&#39064;&#30340;&#29289;&#29702;&#31995;&#32479;&#12290;&#36825;&#31181;&#26500;&#36896;&#24615;&#30340;&#32771;&#34385;&#20351;&#24471;&#38382;&#39064;&#30340;&#32500;&#24230;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous-time nonlinear optimal control problems hold great promise in real-world applications. After decades of development, reinforcement learning (RL) has achieved some of the greatest successes as a general nonlinear control design method. However, a recent comprehensive analysis of state-of-the-art continuous-time RL (CT-RL) methods, namely, adaptive dynamic programming (ADP)-based CT-RL algorithms, reveals they face significant design challenges due to their complexity, numerical conditioning, and dimensional scaling issues. Despite advanced theoretical results, existing ADP CT-RL synthesis methods are inadequate in solving even small, academic problems. The goal of this work is thus to introduce a suite of new CT-RL algorithms for control of affine nonlinear systems. Our design approach relies on two important factors. First, our methods are applicable to physical systems that can be partitioned into smaller subproblems. This constructive consideration results in reduced dimen
&lt;/p&gt;</description></item><item><title>&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#20934;&#30830;&#24615;&#19982;&#26102;&#38388;&#21069;&#27839;&#36827;&#34892;&#27604;&#36739;&#65292;&#36890;&#36807;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#30740;&#31350;&#26469;&#22238;&#31572;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.08919</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#20934;&#30830;&#24615;&#19982;&#26102;&#38388;&#21069;&#27839;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images. (arXiv:2307.08919v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08919
&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#20934;&#30830;&#24615;&#19982;&#26102;&#38388;&#21069;&#27839;&#36827;&#34892;&#27604;&#36739;&#65292;&#36890;&#36807;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#30740;&#31350;&#26469;&#22238;&#31572;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24212;&#29992;&#26469;&#35828;&#65292;&#27599;&#20010;&#22270;&#20687;&#37117;&#24456;&#38590;&#25110;&#26114;&#36149;&#22320;&#33719;&#24471;&#19968;&#20010;&#21487;&#20449;&#30340;&#26631;&#31614;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#27809;&#26377;&#26631;&#31614;&#30340;&#22270;&#20687;&#26356;&#23481;&#26131;&#33719;&#21462;&#12290;&#20004;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#37117;&#25215;&#35834;&#39069;&#22806;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#20165;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#26377;&#29992;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#26631;&#35760;&#38598;&#23545;&#36825;&#20123;&#34920;&#31034;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#20998;&#31867;&#22120;&#65307;&#21322;&#30417;&#30563;&#23398;&#20064;&#21516;&#26102;&#22312;&#26631;&#35760;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#30452;&#25509;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20174;&#20004;&#20010;&#26041;&#21521;&#19978;&#37117;&#22768;&#31216;&#22312;&#38750;&#21307;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#20294;&#27809;&#26377;&#31995;&#32479;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#65292;&#24182;&#19988;&#22823;&#22810;&#21482;&#19982;&#21516;&#19968;&#26041;&#21521;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#26469;&#22238;&#31572;&#20174;&#19994;&#32773;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22312;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#26377;&#38480;&#30340;&#22521;&#35757;&#26102;&#38388;&#39044;&#31639;&#19979;&#65292;&#39069;&#22806;&#30340;&#26080;&#26631;&#31614;&#22270;&#20687;&#33021;&#22815;&#20135;&#29983;&#22810;&#22823;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many applications of classifiers to medical images, a trustworthy label for each image can be difficult or expensive to obtain. In contrast, images without labels are more readily available. Two major research directions both promise that additional unlabeled data can improve classifier performance: self-supervised learning pretrains useful representations on unlabeled data only, then fine-tunes a classifier on these representations via the labeled set; semi-supervised learning directly trains a classifier on labeled and unlabeled data simultaneously. Recent methods from both directions have claimed significant gains on non-medical tasks, but do not systematically assess medical images and mostly compare only to methods in the same direction. This study contributes a carefully-designed benchmark to help answer a practitioner's key question: given a small labeled dataset and a limited budget of hours to spend on training, what gains from additional unlabeled images are possible and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25237;&#24433;&#22836;&#30340;&#31232;&#30095;&#24615;&#65292;&#21457;&#29616;&#36890;&#36807;&#22312;&#25237;&#24433;&#23376;&#31354;&#38388;&#20013;&#25191;&#34892;&#23545;&#27604;&#25439;&#22833;&#21487;&#20197;&#25552;&#21319;&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#24314;&#35758;&#21482;&#26377;&#19968;&#37096;&#20998;&#29305;&#24449;&#26159;&#24517;&#35201;&#30340;&#65292;&#32780;&#31232;&#30095;&#30340;&#25237;&#24433;&#22836;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08913</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25237;&#24433;&#22836;&#30340;&#31232;&#30095;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards the Sparseness of Projection Head in Self-Supervised Learning. (arXiv:2307.08913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08913
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25237;&#24433;&#22836;&#30340;&#31232;&#30095;&#24615;&#65292;&#21457;&#29616;&#36890;&#36807;&#22312;&#25237;&#24433;&#23376;&#31354;&#38388;&#20013;&#25191;&#34892;&#23545;&#27604;&#25439;&#22833;&#21487;&#20197;&#25552;&#21319;&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#24314;&#35758;&#21482;&#26377;&#19968;&#37096;&#20998;&#29305;&#24449;&#26159;&#24517;&#35201;&#30340;&#65292;&#32780;&#31232;&#30095;&#30340;&#25237;&#24433;&#22836;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#25104;&#20026;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#34920;&#31034;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#31181;&#25104;&#21151;&#30340;SSL&#26041;&#27861;&#26159;&#23545;&#27604;&#23398;&#20064;&#65292;&#20854;&#26088;&#22312;&#23558;&#27491;&#26679;&#26412;&#32858;&#38598;&#22312;&#19968;&#36215;&#65292;&#23558;&#36127;&#26679;&#26412;&#25512;&#24320;&#12290;&#35768;&#22810;&#24403;&#21069;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#37117;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#25237;&#24433;&#22836;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#21644;&#29702;&#35770;&#25506;&#32034;&#65292;&#25105;&#20204;&#23545;&#25237;&#24433;&#22836;&#30340;&#20869;&#37096;&#26426;&#21046;&#21450;&#20854;&#19982;&#32500;&#24230;&#25240;&#21472;&#29616;&#35937;&#30340;&#20851;&#31995;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25237;&#24433;&#22836;&#36890;&#36807;&#22312;&#19968;&#20010;&#25237;&#24433;&#23376;&#31354;&#38388;&#20013;&#25191;&#34892;&#23545;&#27604;&#25439;&#22833;&#65292;&#25552;&#21319;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#22312;&#26368;&#23567;&#21270;&#19968;&#20010;&#23567;&#25209;&#37327;&#25968;&#25454;&#30340;&#23545;&#27604;&#25439;&#22833;&#26102;&#65292;&#21482;&#26377;&#19968;&#37096;&#20998;&#29305;&#24449;&#26159;&#24517;&#35201;&#30340;&#12290;&#29702;&#35770;&#20998;&#26512;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#31232;&#30095;&#30340;&#25237;&#24433;&#22836;&#21487;&#20197;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#65292;&#22240;&#27492;&#25105;&#20204;&#24341;&#20837;SparseHead&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, self-supervised learning (SSL) has emerged as a promising approach for extracting valuable representations from unlabeled data. One successful SSL method is contrastive learning, which aims to bring positive examples closer while pushing negative examples apart. Many current contrastive learning approaches utilize a parameterized projection head. Through a combination of empirical analysis and theoretical investigation, we provide insights into the internal mechanisms of the projection head and its relationship with the phenomenon of dimensional collapse. Our findings demonstrate that the projection head enhances the quality of representations by performing contrastive loss in a projected subspace. Therefore, we propose an assumption that only a subset of features is necessary when minimizing the contrastive loss of a mini-batch of data. Theoretical analysis further suggests that a sparse projection head can enhance generalization, leading us to introduce SparseHead - 
&lt;/p&gt;</description></item><item><title>&#38160;&#24230;&#24863;&#30693;&#30340;&#22270;&#21327;&#21516;&#36807;&#28388;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;gSAM&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#26435;&#37325;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#22374;&#31243;&#24230;&#65292;&#36873;&#25321;&#24179;&#22374;&#26368;&#23567;&#20540;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#21327;&#21516;&#36807;&#28388;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08910</link><description>&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#30340;&#22270;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Graph Collaborative Filtering. (arXiv:2307.08910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08910
&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#30340;&#22270;&#21327;&#21516;&#36807;&#28388;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;gSAM&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#26435;&#37325;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#22374;&#31243;&#24230;&#65292;&#36873;&#25321;&#24179;&#22374;&#26368;&#23567;&#20540;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#21327;&#21516;&#36807;&#28388;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22312;&#21327;&#21516;&#36807;&#28388;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#22826;&#19968;&#33268;&#26102;&#65292;GNNs&#24448;&#24448;&#20250;&#20135;&#29983;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;GNNs&#38656;&#35201;&#20248;&#21270;&#38750;&#20984;&#31070;&#32463;&#32593;&#32476;&#65292;&#26377;&#22823;&#37327;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#36825;&#20123;&#22312;&#27979;&#35797;&#26102;&#21487;&#33021;&#24615;&#33021;&#24046;&#24322;&#24456;&#22823;&#12290;&#22240;&#27492;&#65292;&#36873;&#25321;&#26368;&#23567;&#20540;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#31216;&#20026;{gSAM}&#65292;&#26681;&#25454;&#8220;&#24179;&#22374;&#8221;&#26368;&#23567;&#20540;&#27604;&#8220;&#38160;&#21033;&#8221;&#26368;&#23567;&#20540;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#21407;&#21017;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;gSAM&#36890;&#36807;&#24418;&#25104;&#21452;&#23618;&#20248;&#21270;&#26469;&#27491;&#21017;&#21270;&#26435;&#37325;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#22374;&#31243;&#24230;&#65306;&#22806;&#37096;&#38382;&#39064;&#36827;&#34892;&#26631;&#20934;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#20869;&#37096;&#38382;&#39064;&#21017;&#24110;&#21161;&#27169;&#22411;&#36339;&#20986;&#38160;&#21033;&#30340;&#26368;&#23567;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;gSAM&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved impressive performance in collaborative filtering. However, GNNs tend to yield inferior performance when the distributions of training and test data are not aligned well. Also, training GNNs requires optimizing non-convex neural networks with an abundance of local and global minima, which may differ widely in their performance at test time. Thus, it is essential to choose the minima carefully. Here we propose an effective training schema, called {gSAM}, under the principle that the \textit{flatter} minima has a better generalization ability than the \textit{sharper} ones. To achieve this goal, gSAM regularizes the flatness of the weight loss landscape by forming a bi-level optimization: the outer problem conducts the standard model training while the inner problem helps the model jump out of the sharp minima. Experimental results show the superiority of our gSAM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#20010;&#24615;&#21270;&#34880;&#31958;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#26174;&#33879;&#25913;&#21892;&#34880;&#31958;&#25511;&#21046;&#12289;&#20943;&#23569;&#34880;&#31958;&#21464;&#24322;&#24615;&#20197;&#21450;&#39044;&#38450;&#20302;&#34880;&#31958;&#20107;&#20214;&#31561;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08897</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#22411;&#31958;&#23615;&#30149;&#65288;T1D&#65289;&#24739;&#32773;&#22522;&#30784;-&#25512;&#33616;&#21058;&#37327;&#39038;&#38382;
&lt;/p&gt;
&lt;p&gt;
Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology. (arXiv:2307.08897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#20010;&#24615;&#21270;&#34880;&#31958;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#26174;&#33879;&#25913;&#21892;&#34880;&#31958;&#25511;&#21046;&#12289;&#20943;&#23569;&#34880;&#31958;&#21464;&#24322;&#24615;&#20197;&#21450;&#39044;&#38450;&#20302;&#34880;&#31958;&#20107;&#20214;&#31561;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#19968;&#22411;&#31958;&#23615;&#30149;&#65288;T1D&#65289;&#24739;&#32773;&#20010;&#24615;&#21270;&#34880;&#31958;&#25511;&#21046;&#30340;&#26032;&#22411;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#38381;&#29615;&#31995;&#32479;&#65292;&#21253;&#25324;&#34880;&#31958;&#20195;&#35874;&#27169;&#22411;&#21644;&#20316;&#20026;&#22522;&#30784;-&#25512;&#33616;&#21058;&#37327;&#39038;&#38382;&#30340;&#22810;&#26234;&#33021;&#20307;&#36719;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;RL&#27169;&#22411;&#12290;&#36890;&#36807;&#27604;&#36739;RL&#20195;&#29702;&#19982;&#20256;&#32479;&#27835;&#30103;&#22312;&#19977;&#20010;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#35780;&#20272;&#25351;&#26631;&#21253;&#25324;&#34880;&#31958;&#27700;&#24179;&#65288;&#26368;&#20302;&#12289;&#26368;&#39640;&#21644;&#24179;&#22343;&#65289;&#65292;&#22312;&#19981;&#21516;&#34880;&#31958;&#33539;&#22260;&#20869;&#30340;&#26102;&#38388;&#65292;&#20197;&#21450;&#24179;&#22343;&#27599;&#26085;&#25512;&#33616;&#21058;&#37327;&#21644;&#22522;&#30784;&#33008;&#23707;&#32032;&#21058;&#37327;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;RL&#30340;&#22522;&#30784;-&#25512;&#33616;&#21058;&#37327;&#39038;&#38382;&#26174;&#33879;&#25913;&#21892;&#20102;&#34880;&#31958;&#25511;&#21046;&#65292;&#20943;&#23567;&#20102;&#34880;&#31958;&#21464;&#24322;&#24615;&#65292;&#22686;&#21152;&#20102;&#22312;&#30446;&#26631;&#33539;&#22260;&#65288;70-180 mg/dL&#65289;&#20869;&#30340;&#26102;&#38388;&#12290;&#20302;&#34880;&#31958;&#20107;&#20214;&#24471;&#21040;&#26377;&#25928;&#39044;&#38450;&#65292;&#20005;&#37325;&#39640;&#34880;&#31958;&#20107;&#20214;&#24471;&#21040;&#20943;&#23569;&#12290;RL&#26041;&#27861;&#36824;&#23548;&#33268;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#24179;&#22343;&#27599;&#26085;&#22522;&#30784;&#33008;&#23707;&#32032;&#21058;&#37327;&#26377;&#32479;&#35745;&#23398;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel multi-agent reinforcement learning (RL) approach for personalized glucose control in individuals with type 1 diabetes (T1D). The method employs a closed-loop system consisting of a blood glucose (BG) metabolic model and a multi-agent soft actor-critic RL model acting as the basal-bolus advisor. Performance evaluation is conducted in three scenarios, comparing the RL agents to conventional therapy. Evaluation metrics include glucose levels (minimum, maximum, and mean), time spent in different BG ranges, and average daily bolus and basal insulin dosages. Results demonstrate that the RL-based basal-bolus advisor significantly improves glucose control, reducing glycemic variability and increasing time spent within the target range (70-180 mg/dL). Hypoglycemia events are effectively prevented, and severe hyperglycemia events are reduced. The RL approach also leads to a statistically significant reduction in average daily basal insulin dosage compared to conventio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26080;&#30417;&#30563;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#22240;&#25506;&#32034;&#21644;&#30142;&#30149;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20351;&#29992;FactorVAE&#25110;beta-VAE&#30456;&#27604;&#26631;&#20934;VAE&#25110;&#38750;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#21742;&#21912;&#21644;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#30340;&#20840;&#22522;&#22240;&#32452;&#26174;&#33879;&#20301;&#28857;&#25968;&#37327;&#12289;&#36951;&#20256;&#21147;&#21644;&#22810;&#22522;&#22240;&#39118;&#38505;&#35780;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08893</link><description>&lt;p&gt;
&#35780;&#20272;&#26080;&#30417;&#30563;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#22312;&#22522;&#22240;&#25506;&#32034;&#21644;&#30142;&#30149;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating unsupervised disentangled representation learning for genomic discovery and disease risk prediction. (arXiv:2307.08893v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26080;&#30417;&#30563;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#22240;&#25506;&#32034;&#21644;&#30142;&#30149;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20351;&#29992;FactorVAE&#25110;beta-VAE&#30456;&#27604;&#26631;&#20934;VAE&#25110;&#38750;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#21742;&#21912;&#21644;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#30340;&#20840;&#22522;&#22240;&#32452;&#26174;&#33879;&#20301;&#28857;&#25968;&#37327;&#12289;&#36951;&#20256;&#21147;&#21644;&#22810;&#22522;&#22240;&#39118;&#38505;&#35780;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#20020;&#24202;&#25968;&#25454;&#30001;&#20110;&#20854;&#22312;&#29983;&#29289;&#24211;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#30340;&#21487;&#35775;&#38382;&#24615;&#21644;&#39640;&#24615;&#33021;&#24314;&#27169;&#25216;&#26415;&#65288;&#23588;&#20854;&#26159;&#28145;&#24230;&#23398;&#20064;&#65289;&#30340;&#21457;&#23637;&#65292;&#24050;&#32463;&#25104;&#20026;&#36951;&#20256;&#30740;&#31350;&#20013;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#23398;&#20064;&#30340;&#36825;&#20123;&#20020;&#24202;&#25968;&#25454;&#30340;&#20302;&#32500;&#23884;&#20837;&#21487;&#20197;&#29992;&#20110;&#20840;&#22522;&#22240;&#32452;&#20851;&#32852;&#30740;&#31350;&#21644;&#22810;&#22522;&#22240;&#39118;&#38505;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36951;&#20256;&#30456;&#20851;&#30740;&#31350;&#20013;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#65292;&#21253;&#25324;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;autoencoders&#65289;&#12289;VAE&#12289;beta-VAE&#21644;FactorVAE&#12290;&#20197;&#33521;&#22269;&#29983;&#29289;&#24211;&#30340;&#25903;&#27668;&#31649;&#22270;&#35889;&#20026;&#20363;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;FactorVAE&#25110;beta-VAE&#30456;&#27604;&#26631;&#20934;VAE&#25110;&#38750;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#25913;&#21892;&#21742;&#21912;&#21644;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#30340;&#20840;&#22522;&#22240;&#32452;&#26174;&#33879;&#20301;&#28857;&#25968;&#37327;&#12289;&#36951;&#20256;&#21147;&#21644;&#22810;&#22522;&#22240;&#39118;&#38505;&#35780;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional clinical data have become invaluable resources for genetic studies, due to their accessibility in biobank-scale datasets and the development of high performance modeling techniques especially using deep learning. Recent work has shown that low dimensional embeddings of these clinical data learned by variational autoencoders (VAE) can be used for genome-wide association studies and polygenic risk prediction. In this work, we consider multiple unsupervised learning methods for learning disentangled representations, namely autoencoders, VAE, beta-VAE, and FactorVAE, in the context of genetic association studies. Using spirograms from UK Biobank as a running example, we observed improvements in the number of genome-wide significant loci, heritability, and performance of polygenic risk scores for asthma and chronic obstructive pulmonary disease by using FactorVAE or beta-VAE, compared to standard VAE or non-variational autoencoders. FactorVAEs performed effectively across m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39044;&#27979;&#21024;&#38500;&#21160;&#24577;&#27169;&#22411;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20102;&#21160;&#24577;&#22270;&#20013;&#36793;&#30340;&#26356;&#26032;&#65292;&#35299;&#20915;&#20102;&#35774;&#35745;&#21160;&#24577;&#31639;&#27861;&#20013;&#30340;&#26410;&#30693;&#26356;&#26032;&#24207;&#21015;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36825;&#19968;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#65292;&#22312;&#29702;&#35770;&#19978;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.08890</link><description>&lt;p&gt;
&#39044;&#27979;&#21024;&#38500;&#21160;&#24577;&#27169;&#22411;&#65306;&#20805;&#20998;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#65292;&#23454;&#29616;&#38646;&#25104;&#26412;&#65288;arXiv:2307.08890v1 [cs.DS]&#65289;
&lt;/p&gt;
&lt;p&gt;
The Predicted-Deletion Dynamic Model: Taking Advantage of ML Predictions, for Free. (arXiv:2307.08890v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39044;&#27979;&#21024;&#38500;&#21160;&#24577;&#27169;&#22411;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20102;&#21160;&#24577;&#22270;&#20013;&#36793;&#30340;&#26356;&#26032;&#65292;&#35299;&#20915;&#20102;&#35774;&#35745;&#21160;&#24577;&#31639;&#27861;&#20013;&#30340;&#26410;&#30693;&#26356;&#26032;&#24207;&#21015;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36825;&#19968;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#65292;&#22312;&#29702;&#35770;&#19978;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#39640;&#25928;&#30340;&#21160;&#24577;&#31639;&#27861;&#30340;&#20027;&#35201;&#29942;&#39048;&#26159;&#26356;&#26032;&#24207;&#21015;&#30340;&#26410;&#30693;&#24615;&#12290;&#29305;&#21035;&#26159;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;3-&#39030;&#28857;&#36830;&#36890;&#24615;&#12289;&#24179;&#38754;&#26377;&#21521;&#22270;&#25152;&#26377;&#28857;&#23545;&#26368;&#30701;&#36335;&#24452;&#31561;&#65292;&#26368;&#20339;&#30340;&#37096;&#20998;&#21160;&#24577;&#35299;&#21644;&#26368;&#20339;&#30340;&#20840;&#21160;&#24577;&#35299;&#20043;&#38388;&#30340;&#36816;&#34892;&#26102;&#38388;&#24046;&#24322;&#26159;&#22810;&#39033;&#24335;&#30340;&#65292;&#29978;&#33267;&#26159;&#25351;&#25968;&#32423;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39044;&#27979;&#21024;&#38500;&#21160;&#24577;&#27169;&#22411;&#65292;&#21463;&#21040;&#36817;&#26399;&#20851;&#20110;&#39044;&#27979;&#21160;&#24577;&#22270;&#20013;&#36793;&#26356;&#26032;&#30340;&#32463;&#39564;&#24615;&#30740;&#31350;&#30340;&#21551;&#21457;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#36793;&#22312;&#32447;&#19978;&#34987;&#25554;&#20837;&#21644;&#21024;&#38500;&#65292;&#24182;&#19988;&#24403;&#19968;&#26465;&#36793;&#34987;&#25554;&#20837;&#26102;&#65292;&#23427;&#24102;&#26377;&#19968;&#20010;&#23427;&#21024;&#38500;&#26102;&#38388;&#30340;&#8220;&#39044;&#27979;&#8221;&#12290;&#35813;&#27169;&#22411;&#21453;&#26144;&#20102;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#19968;&#20123;&#24773;&#26223;&#65292;&#20854;&#20013;&#26381;&#21153;&#21487;&#20197;&#35775;&#38382;&#21382;&#21490;&#25968;&#25454;&#25110;&#20854;&#20182;&#20851;&#20110;&#36755;&#20837;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#25454;&#27492;&#23545;&#29992;&#25143;&#34892;&#20026;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#22312;&#29702;&#35770;&#19978;&#20063;&#26377;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#25554;&#20540;&#20102;&#37096;&#20998;&#21160;&#24577;&#35299;&#21644;&#20840;&#21160;&#24577;&#35299;&#20043;&#38388;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main bottleneck in designing efficient dynamic algorithms is the unknown nature of the update sequence. In particular, there are some problems, like 3-vertex connectivity, planar digraph all pairs shortest paths, and others, where the separation in runtime between the best partially dynamic solutions and the best fully dynamic solutions is polynomial, sometimes even exponential.  In this paper, we formulate the predicted-deletion dynamic model, motivated by a recent line of empirical work about predicting edge updates in dynamic graphs. In this model, edges are inserted and deleted online, and when an edge is inserted, it is accompanied by a "prediction" of its deletion time. This models real world settings where services may have access to historical data or other information about an input and can subsequently use such information make predictions about user behavior. The model is also of theoretical interest, as it interpolates between the partially dynamic and fully dynamic set
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#20004;&#31181;&#39069;&#22806;&#30340;&#21512;&#25104;&#22270;&#29983;&#25104;&#22120;&#22914;&#20309;&#25913;&#36827;GraphWorld&#30340;&#35780;&#20272;&#65292;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22270;&#31354;&#38388;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30495;&#23454;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#22270;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08881</link><description>&lt;p&gt;
&#30740;&#31350;&#24230;&#20998;&#24067;&#21644;&#21516;&#36136;&#24615;&#22312;&#22270;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Examining the Effects of Degree Distribution and Homophily in Graph Learning Models. (arXiv:2307.08881v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#20004;&#31181;&#39069;&#22806;&#30340;&#21512;&#25104;&#22270;&#29983;&#25104;&#22120;&#22914;&#20309;&#25913;&#36827;GraphWorld&#30340;&#35780;&#20272;&#65292;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22270;&#31354;&#38388;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30495;&#23454;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#22270;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#20204;&#23545;GNN&#30340;&#21457;&#23637;&#20852;&#36259;&#39640;&#28072;&#65292;&#20294;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#21516;&#36136;&#21270;&#20173;&#28982;&#26159;GNN&#30740;&#31350;&#38754;&#20020;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;GraphWorld&#26159;&#19968;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#20351;&#29992;&#38543;&#26426;&#22359;&#27169;&#22411;(SBM)&#29983;&#25104;&#22810;&#26679;&#30340;&#21512;&#25104;&#22270;&#65292;&#29992;&#20110;&#23545;&#20219;&#20309;GNN&#20219;&#21153;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#23613;&#31649;GraphWorld&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38543;&#26426;&#22359;&#27169;&#22411;&#23545;GraphWorld&#33021;&#22815;&#21019;&#24314;&#30340;&#22270;&#32467;&#26500;&#31181;&#31867;&#26045;&#21152;&#20102;&#22522;&#26412;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#21512;&#25104;&#22270;&#29983;&#25104;&#22120;&#22914;&#20309;&#25913;&#36827;GraphWorld&#30340;&#35780;&#20272;&#65307;LFR&#26159;&#22270;&#32858;&#31867;&#25991;&#29486;&#20013;&#19968;&#20010;&#25104;&#29087;&#30340;&#27169;&#22411;&#65292;&#32780;CABAM&#26159;&#19968;&#20010;&#38024;&#23545;GNN&#22522;&#20934;&#27979;&#35797;&#37327;&#36523;&#23450;&#21046;&#30340;Barabasi-Albert&#27169;&#22411;&#30340;&#26368;&#26032;&#25913;&#36827;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#22312;GraphWorld&#26694;&#26550;&#20869;&#26174;&#33879;&#25193;&#23637;&#20102;&#22270;&#31354;&#38388;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30495;&#23454;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#22270;&#23646;&#24615;&#12290;&#20026;&#20102;&#35777;&#26126;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;30&#19975;&#20010;&#22270;&#26469;&#23545;11&#20010;GNN&#27169;&#22411;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite a surge in interest in GNN development, homogeneity in benchmarking datasets still presents a fundamental issue to GNN research. GraphWorld is a recent solution which uses the Stochastic Block Model (SBM) to generate diverse populations of synthetic graphs for benchmarking any GNN task. Despite its success, the SBM imposed fundamental limitations on the kinds of graph structure GraphWorld could create.  In this work we examine how two additional synthetic graph generators can improve GraphWorld's evaluation; LFR, a well-established model in the graph clustering literature and CABAM, a recent adaptation of the Barabasi-Albert model tailored for GNN benchmarking. By integrating these generators, we significantly expand the coverage of graph space within the GraphWorld framework while preserving key graph properties observed in real-world networks. To demonstrate their effectiveness, we generate 300,000 graphs to benchmark 11 GNN models on a node classification task. We find GNN p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#35843;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25163;&#26415;&#22270;&#20687;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#36275;&#21644;&#26631;&#27880;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#23398;&#20064;&#31574;&#30053;&#38477;&#20302;&#20102;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.08880</link><description>&lt;p&gt;
&#21487;&#35843;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25163;&#26415;&#22270;&#20687;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Modular Neural Network Approaches for Surgical Image Recognition. (arXiv:2307.08880v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#35843;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25163;&#26415;&#22270;&#20687;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#36275;&#21644;&#26631;&#27880;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#23398;&#20064;&#31574;&#30053;&#38477;&#20302;&#20102;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#37117;&#24050;&#32463;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#24212;&#29992;&#20135;&#29983;&#20102;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#21462;&#24471;&#36825;&#20123;&#32467;&#26524;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#24182;&#19981;&#24635;&#26159;&#21487;&#20197;&#33719;&#21462;&#30340;&#12290;&#27492;&#22806;&#65292;&#23545;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#19988;&#32791;&#26102;&#12290;&#33258;&#25105;&#35757;&#32451;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#29702;&#35770;&#20998;&#26512;&#29978;&#33267;&#35777;&#26126;&#65292;&#23427;&#21487;&#33021;&#23548;&#33268;&#27604;&#26222;&#36890;&#20998;&#31867;&#22120;&#26356;&#22909;&#30340;&#27867;&#21270;&#34920;&#29616;&#12290;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#38754;&#20020;&#30340;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#29616;&#20195;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#19968;&#31181;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#37319;&#29992;&#27169;&#22359;&#21270;&#23398;&#20064;&#65292;&#36825;&#26159;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#30340;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#30340;&#21407;&#29702;&#26159;&#23558;&#24212;&#29992;&#25286;&#20998;&#20026;&#22810;&#20010;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#36127;&#36131;&#22788;&#29702;&#29305;&#23450;&#30340;&#37096;&#20998;&#12290;&#27599;&#20010;&#27169;&#22359;&#21487;&#20197;&#34987;&#21333;&#29420;&#35757;&#32451;&#21644;&#26356;&#26032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#25972;&#20010;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based applications have seen a lot of success in recent years. Text, audio, image, and video have all been explored with great success using deep learning approaches. The use of convolutional neural networks (CNN) in computer vision, in particular, has yielded reliable results. In order to achieve these results, a large amount of data is required. However, the dataset cannot always be accessible. Moreover, annotating data can be difficult and time-consuming. Self-training is a semi-supervised approach that managed to alleviate this problem and achieve state-of-the-art performances. Theoretical analysis even proved that it may result in a better generalization than a normal classifier. Another problem neural networks can face is the increasing complexity of modern problems, requiring a high computational and storage cost. One way to mitigate this issue, a strategy that has been inspired by human cognition known as modular learning, can be employed. The principle of the app
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#25299;&#25169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;UPNA&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#33410;&#28857;&#23646;&#24615;&#32435;&#20837;&#27169;&#22411;&#26469;&#25913;&#21892;&#38142;&#25509;&#39044;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.08877</link><description>&lt;p&gt;
&#22312;&#22270;&#38142;&#25509;&#39044;&#27979;&#20013;&#21306;&#20998;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#25299;&#25169;&#20197;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Disentangling Node Attributes from Graph Topology for Improved Generalizability in Link Prediction. (arXiv:2307.08877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#25299;&#25169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;UPNA&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#33410;&#28857;&#23646;&#24615;&#32435;&#20837;&#27169;&#22411;&#26469;&#25913;&#21892;&#38142;&#25509;&#39044;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#26159;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#25299;&#25169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#33410;&#28857;&#23646;&#24615;&#32435;&#20837;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;UPNA&#65288;&#33410;&#28857;&#23646;&#24615;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65289;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#25509;&#21463;&#19968;&#23545;&#33410;&#28857;&#23646;&#24615;&#24182;&#39044;&#27979;&#36793;&#30340;&#27010;&#29575;&#65292;&#26469;&#35299;&#20915;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;GNN&#22312;&#20855;&#26377;&#24130;&#24459;&#24230;&#20998;&#24067;&#30340;&#22270;&#20013;&#23481;&#26131;&#21463;&#21040;&#25299;&#25169;&#24555;&#25463;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;UPNA&#23398;&#20064;&#21040;&#20102;&#28508;&#22312;&#30340;&#22270;&#29983;&#25104;&#26426;&#21046;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#22240;&#20026;&#25152;&#23398;&#20989;&#25968;&#21487;&#20197;&#29992;&#20110;&#23558;&#26032;&#33410;&#28857;&#28155;&#21152;&#21040;&#27491;&#22312;&#22686;&#38271;&#30340;&#22270;&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33410;&#28857;&#23646;&#24615;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#35266;&#27979;&#20559;&#24046;&#65292;&#24182;&#23545;&#26410;&#35266;&#27979;&#33410;&#28857;&#20570;&#20986;&#26377;&#24847;&#20041;&#30340;&#39044;&#27979;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#65288;&#22312;&#22522;&#20934;&#19978;&#25552;&#21319;&#20102;3&#20493;&#21040;34&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction is a crucial task in graph machine learning with diverse applications. We explore the interplay between node attributes and graph topology and demonstrate that incorporating pre-trained node attributes improves the generalization power of link prediction models. Our proposed method, UPNA (Unsupervised Pre-training of Node Attributes), solves the inductive link prediction problem by learning a function that takes a pair of node attributes and predicts the probability of an edge, as opposed to Graph Neural Networks (GNN), which can be prone to topological shortcuts in graphs with power-law degree distribution. In this manner, UPNA learns a significant part of the latent graph generation mechanism since the learned function can be used to add incoming nodes to a growing graph. By leveraging pre-trained node attributes, we overcome observational bias and make meaningful predictions about unobserved nodes, surpassing state-of-the-art performance (3X to 34X improvement on ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#24418;&#24335;&#65292;&#20351;&#24471;&#22823;&#35268;&#27169;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#21464;&#24471;&#21487;&#34892;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#33258;&#28982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#20989;&#25968;&#36924;&#36817;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#26368;&#20339;&#40065;&#26834;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.08875</link><description>&lt;p&gt;
&#33258;&#28982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#29992;&#20110;&#24102;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation. (arXiv:2307.08875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#24418;&#24335;&#65292;&#20351;&#24471;&#22823;&#35268;&#27169;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#21464;&#24471;&#21487;&#34892;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#33258;&#28982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#20989;&#25968;&#36924;&#36817;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#26368;&#20339;&#40065;&#26834;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#30830;&#23450;&#19968;&#20010;&#23545;&#35757;&#32451;&#27169;&#25311;&#22120;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#27169;&#22411;&#19981;&#21305;&#37197;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#30340;&#31574;&#30053;&#30340;&#30446;&#26631;&#19979;&#12290;&#20197;&#21069;&#22522;&#20110;&#31574;&#30053;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20027;&#35201;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#19979;&#30340;&#34920;&#26684;&#35774;&#32622;&#65292;&#35813;&#38598;&#21512;&#20415;&#20110;&#40065;&#26834;&#31574;&#30053;&#35780;&#20272;&#65292;&#20294;&#22312;&#29366;&#24577;&#25968;&#37327;&#22686;&#21152;&#26102;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#24418;&#24335;&#65292;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#25277;&#26679;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#12290;&#20004;&#32773;&#37117;&#20351;&#24471;&#21363;&#20351;&#21482;&#33021;&#35775;&#38382;&#27169;&#25311;&#22120;&#65292;&#20063;&#33021;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#33258;&#28982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;RNAC&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#24182;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20110;&#36825;&#20010;RNAC&#31639;&#27861;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#26368;&#20339;&#40065;&#26834;&#31574;&#30053;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#32771;&#34385;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study robust reinforcement learning (RL) with the goal of determining a well-performing policy that is robust against model mismatch between the training simulator and the testing environment. Previous policy-based robust RL algorithms mainly focus on the tabular setting under uncertainty sets that facilitate robust policy evaluation, but are no longer tractable when the number of states scales up. To this end, we propose two novel uncertainty set formulations, one based on double sampling and the other on an integral probability metric. Both make large-scale robust RL tractable even when one only has access to a simulator. We propose a robust natural actor-critic (RNAC) approach that incorporates the new uncertainty sets and employs function approximation. We provide finite-time convergence guarantees for the proposed RNAC algorithm to the optimal robust policy within the function approximation error. Finally, we demonstrate the robust performance of the policy learned by our propo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#23545;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#20013;&#25191;&#34892;&#31639;&#27861;&#26102;&#20135;&#29983;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#20004;&#31181;&#25925;&#38556;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;softmax&#32858;&#21512;&#22120;&#35299;&#20915;&#20998;&#36776;&#29575;&#20007;&#22833;&#38382;&#39064;&#65292;&#20197;&#21450;&#34928;&#20943;&#28508;&#22312;&#31354;&#38388;&#26469;&#22788;&#29702;&#36229;&#20986;&#33539;&#22260;&#30340;&#20540;&#65292;&#36825;&#20123;&#25913;&#21464;&#22312;&#26631;&#20934;CLRS-30&#22522;&#20934;&#27979;&#35797;&#20013;&#22823;&#22810;&#25968;&#31639;&#27861;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.08874</link><description>&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Latent Space Representations of Neural Algorithmic Reasoners. (arXiv:2307.08874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08874
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23545;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#20013;&#25191;&#34892;&#31639;&#27861;&#26102;&#20135;&#29983;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#20004;&#31181;&#25925;&#38556;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;softmax&#32858;&#21512;&#22120;&#35299;&#20915;&#20998;&#36776;&#29575;&#20007;&#22833;&#38382;&#39064;&#65292;&#20197;&#21450;&#34928;&#20943;&#28508;&#22312;&#31354;&#38388;&#26469;&#22788;&#29702;&#36229;&#20986;&#33539;&#22260;&#30340;&#20540;&#65292;&#36825;&#20123;&#25913;&#21464;&#22312;&#26631;&#20934;CLRS-30&#22522;&#20934;&#27979;&#35797;&#20013;&#22823;&#22810;&#25968;&#31639;&#27861;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#65288;NAR&#65289;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#22320;&#25429;&#25417;&#32463;&#20856;&#35745;&#31639;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#36890;&#24120;&#36890;&#36807;&#23398;&#20064;&#25191;&#34892;&#31639;&#27861;&#26469;&#23454;&#29616;&#12290;&#20856;&#22411;&#30340;&#26041;&#27861;&#26159;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#65292;&#23427;&#20204;&#23558;&#36755;&#20837;&#32534;&#30721;&#20026;&#39640;&#32500;&#28508;&#22312;&#31354;&#38388;&#65292;&#22312;&#31639;&#27861;&#25191;&#34892;&#26399;&#38388;&#21453;&#22797;&#36716;&#25442;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;GNN&#22312;&#25191;&#34892;&#31639;&#27861;&#26102;&#23548;&#33268;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#31181;&#21487;&#33021;&#30340;&#25925;&#38556;&#27169;&#24335;&#65306;&#65288;i&#65289;&#20998;&#36776;&#29575;&#20007;&#22833;&#65292;&#20351;&#24471;&#38590;&#20197;&#21306;&#20998;&#30456;&#20284;&#30340;&#20540;&#65307;&#65288;ii&#65289;&#26080;&#27861;&#22788;&#29702;&#35757;&#32451;&#26399;&#38388;&#26410;&#35266;&#23519;&#21040;&#30340;&#20540;&#33539;&#22260;&#20043;&#22806;&#30340;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20381;&#36182;softmax&#32858;&#21512;&#22120;&#26469;&#35299;&#20915;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#24182;&#24314;&#35758;&#34928;&#20943;&#28508;&#22312;&#31354;&#38388;&#20197;&#22788;&#29702;&#36229;&#20986;&#33539;&#22260;&#30340;&#20540;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21464;&#21270;&#22312;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26102;&#65292;&#22312;&#26631;&#20934;CLRS-30&#22522;&#20934;&#27979;&#35797;&#20013;&#22823;&#22810;&#25968;&#31639;&#27861;&#19978;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Algorithmic Reasoning (NAR) is a research area focused on designing neural architectures that can reliably capture classical computation, usually by learning to execute algorithms. A typical approach is to rely on Graph Neural Network (GNN) architectures, which encode inputs in high-dimensional latent spaces that are repeatedly transformed during the execution of the algorithm. In this work we perform a detailed analysis of the structure of the latent space induced by the GNN when executing algorithms. We identify two possible failure modes: (i) loss of resolution, making it hard to distinguish similar values; (ii) inability to deal with values outside the range observed during training. We propose to solve the first issue by relying on a softmax aggregator, and propose to decay the latent space in order to deal with out-of-range values. We show that these changes lead to improvements on the majority of algorithms in the standard CLRS-30 benchmark when using the state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#23612;&#31163;&#24046;&#26469;&#26367;&#20195;&#26041;&#24046;&#65292;&#32531;&#35299;&#20102;&#26041;&#24046;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08873</link><description>&lt;p&gt;
&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#24046;&#26367;&#20195;&#65306;&#22522;&#23612;&#31163;&#24046;
&lt;/p&gt;
&lt;p&gt;
An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#23612;&#31163;&#24046;&#26469;&#26367;&#20195;&#26041;&#24046;&#65292;&#32531;&#35299;&#20102;&#26041;&#24046;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39118;&#38505;&#21388;&#24694;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#38480;&#21046;&#31574;&#30053;&#22238;&#25253;&#30340;&#26041;&#24046;&#26159;&#19968;&#31181;&#24120;&#35265;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26126;&#30830;&#30340;&#25968;&#23398;&#23450;&#20041;&#21644;&#26131;&#20110;&#35299;&#37322;&#12290;&#20256;&#32479;&#26041;&#27861;&#30452;&#25509;&#38480;&#21046;&#24635;&#22238;&#25253;&#26041;&#24046;&#65292;&#32780;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#27599;&#27493;&#22870;&#21169;&#26041;&#24046;&#20316;&#20026;&#20195;&#29702;&#12290;&#26412;&#25991;&#24443;&#24213;&#30740;&#31350;&#20102;&#36825;&#20123;&#22522;&#20110;&#26041;&#24046;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#25968;&#23383;&#23610;&#24230;&#30340;&#25935;&#24863;&#24615;&#21644;&#38459;&#30861;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#26367;&#20195;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#8212;&#8212;&#22522;&#23612;&#31163;&#24046;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26032;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#30340;&#21508;&#31181;&#23646;&#24615;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#23567;&#21270;&#22522;&#23612;&#31163;&#24046;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#22312;&#39118;&#38505;&#21388;&#24694;&#21487;&#20197;&#26126;&#30830;&#23450;&#20041;&#30340;&#39046;&#22495;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#32531;&#35299;&#22522;&#20110;&#26041;&#24046;&#30340;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#20854;&#20182;&#31574;&#30053;&#26080;&#27861;&#23398;&#21040;&#21512;&#29702;&#31574;&#30053;&#26102;&#23454;&#29616;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#65292;&#20197;&#26041;&#24046;&#21644;&#22522;&#23612;&#31163;&#24046;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.
&lt;/p&gt;</description></item><item><title>&#20803;&#20215;&#20540;&#23398;&#20064;&#26159;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#24847;&#35782;&#30340;&#23398;&#20064;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#26234;&#33021;&#20307;&#23398;&#20064;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20351;&#29992;&#20803;&#20215;&#20540;&#20989;&#25968;&#26469;&#25351;&#23548;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36924;&#36817;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.08863</link><description>&lt;p&gt;
&#20803;&#20215;&#20540;&#23398;&#20064;&#65306;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#24847;&#35782;&#30340;&#23398;&#20064;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Meta-Value Learning: a General Framework for Learning with Learning Awareness. (arXiv:2307.08863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08863
&lt;/p&gt;
&lt;p&gt;
&#20803;&#20215;&#20540;&#23398;&#20064;&#26159;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#24847;&#35782;&#30340;&#23398;&#20064;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#26234;&#33021;&#20307;&#23398;&#20064;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20351;&#29992;&#20803;&#20215;&#20540;&#20989;&#25968;&#26469;&#25351;&#23548;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36924;&#36817;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#26799;&#24230;&#23398;&#20064;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#26799;&#24230;&#26469;&#33258;&#20110;&#19968;&#20010;&#19968;&#38454;&#27169;&#22411;&#65292;&#19981;&#32771;&#34385;&#26234;&#33021;&#20307;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;LOLA&#30340;&#24605;&#24819;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#23436;&#20840;&#36890;&#29992;&#30340;&#22522;&#20110;&#20215;&#20540;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#19968;&#20010;&#31216;&#20026;&#20803;&#20215;&#20540;&#30340;&#20989;&#25968;&#65292;&#23427;&#22312;&#32852;&#21512;&#31574;&#30053;&#31354;&#38388;&#30340;&#27599;&#20010;&#28857;&#19978;&#65292;&#20026;&#27599;&#20010;&#26234;&#33021;&#20307;&#32473;&#20986;&#20854;&#26410;&#26469;&#20248;&#21270;&#27493;&#39588;&#20013;&#30446;&#26631;&#30340;&#25240;&#25187;&#24635;&#21644;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20803;&#20215;&#20540;&#30340;&#26799;&#24230;&#27604;&#21407;&#22987;&#30446;&#26631;&#30340;&#26799;&#24230;&#26356;&#21487;&#38752;&#30340;&#25913;&#36827;&#26041;&#21521;&#65292;&#22240;&#20026;&#20803;&#20215;&#20540;&#26469;&#33258;&#23545;&#20248;&#21270;&#25928;&#26524;&#30340;&#32463;&#39564;&#35266;&#23519;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#20803;&#20215;&#20540;&#65292;&#20197;&#27839;&#30528;&#26234;&#33021;&#20307;&#27839;&#30528;&#20803;&#20215;&#20540;&#26799;&#24230;&#30340;&#20248;&#21270;&#36712;&#36857;&#36827;&#34892;TD&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based learning in multi-agent systems is difficult because the gradient derives from a first-order model which does not account for the interaction between agents' learning processes. LOLA (arXiv:1709.04326) accounts for this by differentiating through one step of optimization. We extend the ideas of LOLA and develop a fully-general value-based approach to optimization. At the core is a function we call the meta-value, which at each point in joint-policy space gives for each agent a discounted sum of its objective over future optimization steps. We argue that the gradient of the meta-value gives a more reliable improvement direction than the gradient of the original objective, because the meta-value derives from empirical observations of the effects of optimization. We show how the meta-value can be approximated by training a neural network to minimize TD error along optimization trajectories in which agents follow the gradient of the meta-value. We analyze the behavior of our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#22797;&#26434;&#24615;&#24418;&#24335;&#21270;&#21644;&#27169;&#22411;&#33021;&#21147;&#20316;&#20026;&#22256;&#38590;&#24230;&#26631;&#20934;&#65292;&#20197;&#21450;&#32771;&#34385;&#26679;&#26412;&#22256;&#38590;&#24230;&#21644;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#21516;&#35270;&#35282;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#32454;&#31890;&#24230;&#22270;&#22256;&#38590;&#24230;&#26631;&#20934;&#30340;&#32435;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.08859</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35838;&#31243;&#23398;&#20064;&#65306;&#19968;&#31181;&#22522;&#20110;&#22810;&#35270;&#35282;&#21644;&#33021;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach. (arXiv:2307.08859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#22797;&#26434;&#24615;&#24418;&#24335;&#21270;&#21644;&#27169;&#22411;&#33021;&#21147;&#20316;&#20026;&#22256;&#38590;&#24230;&#26631;&#20934;&#65292;&#20197;&#21450;&#32771;&#34385;&#26679;&#26412;&#22256;&#38590;&#24230;&#21644;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#21516;&#35270;&#35282;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#32454;&#31890;&#24230;&#22270;&#22256;&#38590;&#24230;&#26631;&#20934;&#30340;&#32435;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#23398;&#20064;&#26159;&#19968;&#31181;&#35745;&#21010;&#22909;&#30340;&#23398;&#20064;&#26448;&#26009;&#24207;&#21015;&#65292;&#26377;&#25928;&#30340;&#35838;&#31243;&#23398;&#20064;&#21487;&#20197;&#20351;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#23398;&#20064;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#35821;&#35328;&#24212;&#29992;&#20013;&#20026;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#26377;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#33539;&#24335;&#20013;&#36890;&#24120;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#22256;&#38590;&#24230;&#26631;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35838;&#31243;&#23398;&#20064;&#35270;&#35282;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#24314;&#31435;&#22312;&#22270;&#22797;&#26434;&#24615;&#24418;&#24335;&#21270;&#65288;&#20316;&#20026;&#22256;&#38590;&#24230;&#26631;&#20934;&#65289;&#21644;&#27169;&#22411;&#33021;&#21147;&#20043;&#19978;&#30340;&#26032;&#26041;&#27861;&#26469;&#36827;&#34892;&#35838;&#31243;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#35843;&#24230;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#26679;&#26412;&#22256;&#38590;&#24230;&#21644;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#21516;&#35270;&#35282;&#22312;&#35757;&#32451;&#26399;&#38388;&#25512;&#23548;&#20986;&#26377;&#25928;&#30340;&#35838;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35838;&#31243;&#23398;&#20064;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#33539;&#24335;&#20013;&#32435;&#20837;&#32454;&#31890;&#24230;&#30340;&#22270;&#22256;&#38590;&#24230;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. E
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31227;&#21160;&#19968;&#33268;&#24615;&#32422;&#26463;&#35299;&#20915;&#20102;&#30697;&#38453;/&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#65292;&#24182;&#28385;&#36275;&#21487;&#25509;&#21463;&#24615;&#20934;&#21017;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#21516;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08857</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#25509;&#21463;&#30340;&#31227;&#21160;&#19968;&#33268;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Admissible Shift-Consistent Method for Recommender Systems. (arXiv:2307.08857v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31227;&#21160;&#19968;&#33268;&#24615;&#32422;&#26463;&#35299;&#20915;&#20102;&#30697;&#38453;/&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#65292;&#24182;&#28385;&#36275;&#21487;&#25509;&#21463;&#24615;&#20934;&#21017;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#21516;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32422;&#26463;&#65292;&#31216;&#20026;&#31227;&#21160;&#19968;&#33268;&#24615;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#30697;&#38453;/&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#35777;&#26126;&#20445;&#35777;&#19968;&#20123;&#20851;&#38190;&#30340;&#25968;&#23398;&#23646;&#24615;&#65306;&#65288;1&#65289;&#28385;&#36275;&#26368;&#36817;&#24314;&#31435;&#30340;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#25509;&#21463;&#24615;&#20934;&#21017;&#65307;&#65288;2&#65289;&#28385;&#36275;&#19968;&#20010;&#23450;&#20041;&#20844;&#24179;&#24615;&#30340;&#26465;&#20214;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#23545;&#31995;&#32479;&#25512;&#33616;&#30340;&#24694;&#24847;&#24433;&#21709;&#30340;&#28508;&#22312;&#26426;&#20250;&#65307;&#65288;3&#65289;&#36890;&#36807;&#21033;&#29992;&#21487;&#35777;&#26126;&#30340;&#32570;&#22833;&#20540;&#25554;&#34917;&#30340;&#21807;&#19968;&#24615;&#65292;&#25552;&#20379;&#20102;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#26041;&#27861;&#30340;&#20005;&#26684;&#25968;&#23398;&#25551;&#36848;&#65292;&#21253;&#25324;&#20174;&#30697;&#38453;&#21040;&#24352;&#37327;&#24418;&#24335;&#30340;&#27867;&#21270;&#65292;&#20197;&#20415;&#34920;&#31034;&#21644;&#21033;&#29992;&#29992;&#25143;&#21644;&#20135;&#21697;&#23646;&#24615;&#38598;&#20043;&#38388;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#23450;&#20041;&#28508;&#22312;&#31354;&#38388;&#25237;&#24433;&#65292;&#20197;&#20415;&#20026;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new constraint, called shift-consistency, for solving matrix/tensor completion problems in the context of recommender systems. Our method provably guarantees several key mathematical properties: (1) satisfies a recently established admissibility criterion for recommender systems; (2) satisfies a definition of fairness that eliminates a specific class of potential opportunities for users to maliciously influence system recommendations; and (3) offers robustness by exploiting provable uniqueness of missing-value imputation. We provide a rigorous mathematical description of the method, including its generalization from matrix to tensor form to permit representation and exploitation of complex structural relationships among sets of user and product attributes. We argue that our analysis suggests a structured means for defining latent-space projections that can permit provable performance properties to be established for machine learning methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#23450;&#20041;&#33410;&#28857;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#21644;&#35774;&#35745;&#25193;&#25955;&#25490;&#24207;&#32593;&#32476;&#20197;&#21450;&#21435;&#22122;&#32593;&#32476;&#65292;&#33021;&#22312;&#31163;&#25955;&#22270;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#22270;&#24418;&#12290;</title><link>http://arxiv.org/abs/2307.08849</link><description>&lt;p&gt;
&#22270;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Diffusion Model for Graph Generation. (arXiv:2307.08849v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08849
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#23450;&#20041;&#33410;&#28857;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#21644;&#35774;&#35745;&#25193;&#25955;&#25490;&#24207;&#32593;&#32476;&#20197;&#21450;&#21435;&#22122;&#32593;&#32476;&#65292;&#33021;&#22312;&#31163;&#25955;&#22270;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#22823;&#22810;&#26159;&#19968;&#27425;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20204;&#22312;&#21435;&#37327;&#21270;&#30340;&#37051;&#25509;&#30697;&#38453;&#31354;&#38388;&#20013;&#24212;&#29992;&#39640;&#26031;&#25193;&#25955;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#33021;&#22312;&#27169;&#22411;&#35757;&#32451;&#22256;&#38590;&#12289;&#37319;&#26679;&#36895;&#24230;&#24930;&#21644;&#26080;&#27861;&#38598;&#25104;&#32422;&#26463;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#22270;&#31354;&#38388;&#20013;&#30452;&#25509;&#36827;&#34892;&#30340;&#33410;&#28857;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#12290;&#23545;&#20110;&#21069;&#21521;&#25193;&#25955;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#25193;&#25955;&#25490;&#24207;&#32593;&#32476;&#8221;&#30340;&#32593;&#32476;&#65292;&#23427;&#20174;&#22270;&#30340;&#25299;&#25169;&#20013;&#23398;&#20064;&#21040;&#20102;&#25968;&#25454;&#30456;&#20851;&#30340;&#33410;&#28857;&#21560;&#25910;&#39034;&#24207;&#12290;&#23545;&#20110;&#36870;&#21521;&#29983;&#25104;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#21435;&#22122;&#32593;&#32476;&#8221;&#30340;&#32593;&#32476;&#65292;&#23427;&#21033;&#29992;&#36870;&#21521;&#33410;&#28857;&#25490;&#24207;&#20197;&#21450;&#20043;&#21069;&#21435;&#22122;&#30340;&#33410;&#28857;&#26469;&#39640;&#25928;&#22320;&#37325;&#26500;&#22270;&#65292;&#36890;&#36807;&#39044;&#27979;&#26032;&#33410;&#28857;&#30340;&#31867;&#22411;&#21644;&#36793;&#19982;&#20043;&#21069;&#21435;&#22122;&#33410;&#28857;&#30340;&#26102;&#38388;&#12290;&#22522;&#20110;&#32622;&#25442;&#30340;&#22270;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based graph generative models have recently obtained promising results for graph generation. However, existing diffusion-based graph generative models are mostly one-shot generative models that apply Gaussian diffusion in the dequantized adjacency matrix space. Such a strategy can suffer from difficulty in model training, slow sampling speed, and incapability of incorporating constraints. We propose an \emph{autoregressive diffusion} model for graph generation. Unlike existing methods, we define a node-absorbing diffusion process that operates directly in the discrete graph space. For forward diffusion, we design a \emph{diffusion ordering network}, which learns a data-dependent node absorbing ordering from graph topology. For reverse generation, we design a \emph{denoising network} that uses the reverse node ordering to efficiently reconstruct the graph by predicting the node type of the new node and its edges with previously denoised nodes at a time. Based on the permutatio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#24739;&#32773;&#32858;&#31867;&#26041;&#27861;&#26469;&#35299;&#20915;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;IID&#25968;&#25454;&#38382;&#39064;</title><link>http://arxiv.org/abs/2307.08847</link><description>&lt;p&gt;
&#38754;&#21521;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#24739;&#32773;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving patient clustering for personalized federated learning. (arXiv:2307.08847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08847
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#24739;&#32773;&#32858;&#31867;&#26041;&#27861;&#26469;&#35299;&#20915;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;IID&#25968;&#25454;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#22810;&#20010;&#32452;&#32455;&#22312;&#19981;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25968;&#25454;&#26159;&#38750;&#29420;&#31435;&#20998;&#24067;&#30340;&#65288;&#38750;IID&#65289;&#65292;&#20854;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#65292;&#24739;&#32773;&#32676;&#20307;&#30340;&#21464;&#21270;&#20250;&#23548;&#33268;&#21307;&#38498;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#32771;&#34385;&#29305;&#23450;&#31449;&#28857;&#30340;&#20998;&#24067;&#24046;&#24322;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#31181;&#21464;&#20307;&#8212;&#8212;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#65292;&#36890;&#36807;&#23558;&#24739;&#32773;&#20998;&#25104;&#19981;&#21516;&#30340;&#32676;&#32452;&#22312;&#21307;&#38498;&#20043;&#38388;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#32858;&#31867;&#36807;&#31243;&#38656;&#35201;&#20132;&#25442;&#24739;&#32773;&#32423;&#21035;&#30340;&#20449;&#24687;&#12290;&#20043;&#21069;&#36890;&#36807;&#20351;&#29992;&#32858;&#21512;&#25968;&#25454;&#24418;&#25104;&#32676;&#32452;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20250;&#23548;&#33268;&#32676;&#32452;&#19981;&#20934;&#30830;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#24739;&#32773;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning framework that enables multiple organizations to train a model without sharing their data with a central server. However, it experiences significant performance degradation if the data is non-identically independently distributed (non-IID). This is a problem in medical settings, where variations in the patient population contribute significantly to distribution differences across hospitals. Personalized FL addresses this issue by accounting for site-specific distribution differences. Clustered FL, a Personalized FL variant, was used to address this problem by clustering patients into groups across hospitals and training separate models on each group. However, privacy concerns remained as a challenge as the clustering process requires exchange of patient-level information. This was previously solved by forming clusters using aggregated data, which led to inaccurate groups and performance degradation. In this study, we propose Privacy-preserv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#23433;&#20840;&#31574;&#30053;&#23398;&#20064;&#30340;&#26426;&#20250;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#36234;&#21335;&#25112;&#20105;&#26399;&#38388;&#30340;&#20891;&#20107;&#23433;&#20840;&#35780;&#20272;&#20013;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#35299;&#20915;&#39640;&#39118;&#38505;&#31639;&#27861;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.08840</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#23433;&#20840;&#31574;&#30053;&#23398;&#20064;&#30340;&#26426;&#20250;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65306;&#22312;&#36234;&#21335;&#25112;&#20105;&#26399;&#38388;&#30340;&#20891;&#20107;&#23433;&#20840;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War. (arXiv:2307.08840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#23433;&#20840;&#31574;&#30053;&#23398;&#20064;&#30340;&#26426;&#20250;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#36234;&#21335;&#25112;&#20105;&#26399;&#38388;&#30340;&#20891;&#20107;&#23433;&#20840;&#35780;&#20272;&#20013;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#35299;&#20915;&#39640;&#39118;&#38505;&#31639;&#27861;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#22330;&#26223;&#65292;&#22914;&#21009;&#20107;&#21496;&#27861;&#12289;&#21307;&#23398;&#21644;&#20844;&#20849;&#25919;&#31574;&#20013;&#65292;&#24120;&#24120;&#20351;&#29992;&#31639;&#27861;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#21644;&#24314;&#35758;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#36234;&#21335;&#25112;&#20105;&#26399;&#38388;&#65292;&#26159;&#21542;&#26377;&#21487;&#33021;&#25913;&#36827;&#19968;&#31181;&#23433;&#20840;&#35780;&#20272;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#22312;&#20854;&#24341;&#20837;&#21518;&#31435;&#21363;&#27979;&#37327;&#21040;&#30340;&#32467;&#26524;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#20010;&#23454;&#35777;&#24212;&#29992;&#25552;&#20986;&#20102;&#22312;&#39640;&#39118;&#38505;&#31639;&#27861;&#20915;&#31574;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#20960;&#20010;&#26041;&#27861;&#23398;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#22312;&#23454;&#26045;&#26032;&#31639;&#27861;&#20043;&#21069;&#65292;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#23545;&#26356;&#31967;&#31957;&#30340;&#32467;&#26524;&#39118;&#38505;&#36827;&#34892;&#34920;&#24449;&#21644;&#25511;&#21046;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#31639;&#27861;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#23398;&#20064;&#26032;&#31639;&#27861;&#38656;&#35201;&#36879;&#26126;&#30340;&#22806;&#25512;&#12290;&#31532;&#19977;&#65292;&#29616;&#26377;&#31639;&#27861;&#28041;&#21450;&#24120;&#35265;&#20294;&#38590;&#20197;&#20248;&#21270;&#30340;&#31163;&#25955;&#20915;&#31574;&#34920;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24179;&#22343;&#26465;&#20214;&#39118;&#38505;&#65288;ACRisk&#65289;&#65292;&#39318;&#20808;&#37327;&#21270;&#20102;&#20135;&#29983;&#36739;&#24046;&#32467;&#26524;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic and data-driven decisions and recommendations are commonly used in high-stakes decision-making settings such as criminal justice, medicine, and public policy. We investigate whether it would have been possible to improve a security assessment algorithm employed during the Vietnam War, using outcomes measured immediately after its introduction in late 1969. This empirical application raises several methodological challenges that frequently arise in high-stakes algorithmic decision-making. First, before implementing a new algorithm, it is essential to characterize and control the risk of yielding worse outcomes than the existing algorithm. Second, the existing algorithm is deterministic, and learning a new algorithm requires transparent extrapolation. Third, the existing algorithm involves discrete decision tables that are common but difficult to optimize over.  To address these challenges, we introduce the Average Conditional Risk (ACRisk), which first quantifies the risk th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#39044;&#32534;&#30721;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#32039;&#20945;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#26469;&#20248;&#21270;Rate-Splitting Multiple Access (RSMA)&#39044;&#32534;&#30721;&#65292;&#32467;&#21512;&#37096;&#20998;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#32469;&#36807;&#20102;&#20854;&#20182;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#22312;&#20013;&#31561;&#35268;&#27169;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#31867;&#20284;&#20256;&#32479;&#26041;&#27861;&#30340;&#24179;&#22343;&#21644;&#36895;&#29575;&#24615;&#33021;&#65292;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#26174;&#33879;&#20248;&#20110;&#20302;&#22797;&#26434;&#24230;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08822</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#36895;&#29575;&#20998;&#21106;&#22810;&#22336;(Multiple Access)&#30340;&#39044;&#32534;&#30721;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Meta-Learning Based Precoder Optimization Framework for Rate-Splitting Multiple Access. (arXiv:2307.08822v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#39044;&#32534;&#30721;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#32039;&#20945;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#26469;&#20248;&#21270;Rate-Splitting Multiple Access (RSMA)&#39044;&#32534;&#30721;&#65292;&#32467;&#21512;&#37096;&#20998;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#32469;&#36807;&#20102;&#20854;&#20182;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#22312;&#20013;&#31561;&#35268;&#27169;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#31867;&#20284;&#20256;&#32479;&#26041;&#27861;&#30340;&#24179;&#22343;&#21644;&#36895;&#29575;&#24615;&#33021;&#65292;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#26174;&#33879;&#20248;&#20110;&#20302;&#22797;&#26434;&#24230;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#39044;&#32534;&#30721;&#20248;&#21270;&#26694;&#26550;&#65292;&#26469;&#30452;&#25509;&#20248;&#21270;&#20855;&#26377;&#37096;&#20998;&#21457;&#23556;&#26426;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;Rate-Splitting Multiple Access (RSMA)&#39044;&#32534;&#30721;&#12290;&#36890;&#36807;&#21033;&#29992;&#32039;&#20945;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#26469;&#26368;&#22823;&#21270;&#26126;&#30830;&#30340;&#24179;&#22343;&#21644;&#36895;&#29575;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#32469;&#36807;&#20102;&#23545;&#20219;&#20309;&#20854;&#20182;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#24635;&#36816;&#34892;&#26102;&#38388;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#20013;&#31561;&#35268;&#27169;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#31867;&#20284;&#20256;&#32479;&#39044;&#32534;&#30721;&#20248;&#21270;&#30340;&#24179;&#22343;&#21644;&#36895;&#29575;&#24615;&#33021;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#26174;&#33879;&#20248;&#20110;&#27425;&#20248;&#30340;&#20302;&#22797;&#26434;&#24230;&#39044;&#32534;&#30721;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this letter, we propose the use of a meta-learning based precoder optimization framework to directly optimize the Rate-Splitting Multiple Access (RSMA) precoders with partial Channel State Information at the Transmitter (CSIT). By exploiting the overfitting of the compact neural network to maximize the explicit Average Sum-Rate (ASR) expression, we effectively bypass the need for any other training data while minimizing the total running time. Numerical results reveal that the meta-learning based solution achieves similar ASR performance to conventional precoder optimization in medium-scale scenarios, and significantly outperforms sub-optimal low complexity precoder algorithms in the large-scale regime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#21152;&#36895;Benders&#20998;&#35299;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#21152;&#36895;&#26041;&#26696;&#30340;30%&#26356;&#24555;&#30340;&#24179;&#22343;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.08816</link><description>&lt;p&gt;
&#21152;&#36895;Benders&#20998;&#35299;&#26041;&#27861;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models. (arXiv:2307.08816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#21152;&#36895;Benders&#20998;&#35299;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#21152;&#36895;&#26041;&#26696;&#30340;30%&#26356;&#24555;&#30340;&#24179;&#22343;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20248;&#21270;&#35797;&#22270;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26368;&#20248;&#20915;&#31574;&#12290;&#36890;&#24120;&#65292;&#30001;&#20110;&#38656;&#35201;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#26223;&#25968;&#37327;&#20197;&#21450;&#29616;&#23454;&#35268;&#21010;&#38382;&#39064;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#36825;&#20123;&#38382;&#39064;&#30340;&#32463;&#20856;&#24418;&#24335;&#21464;&#24471;&#38590;&#20197;&#22788;&#29702;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#23454;&#36341;&#32773;&#20204;&#36716;&#21521;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#26356;&#26131;&#22788;&#29702;&#30340;&#23376;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#20998;&#35299;&#26041;&#27861;&#26159;Benders&#20998;&#35299;&#65288;BD&#65289;&#65292;&#23427;&#26681;&#25454;&#24773;&#26223;&#29420;&#31435;&#24615;&#23545;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#21152;&#36895;BD&#30340;&#26041;&#27861;&#65292;&#35813;&#20195;&#29702;&#27169;&#22411;&#21462;&#20195;&#20102;NP&#38590;&#30340;&#25972;&#25968;&#20027;&#38382;&#39064;&#12290;&#36890;&#36807;&#21152;&#36895;&#26041;&#27861;&#65292;&#19982;&#20854;&#20182;&#21152;&#36895;&#30340;BD&#23454;&#29616;&#30456;&#27604;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24179;&#22343;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;&#20102;30%&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20316;&#20026;&#26367;&#20195;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#35299;&#20915;&#38543;&#26426;&#24211;&#23384;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic optimization (SO) attempts to offer optimal decisions in the presence of uncertainty. Often, the classical formulation of these problems becomes intractable due to (a) the number of scenarios required to capture the uncertainty and (b) the discrete nature of real-world planning problems. To overcome these tractability issues, practitioners turn to decomposition methods that divide the problem into smaller, more tractable sub-problems. The focal decomposition method of this paper is Benders decomposition (BD), which decomposes stochastic optimization problems on the basis of scenario independence. In this paper we propose a method of accelerating BD with the aid of a surrogate model in place of an NP-hard integer master problem. Through the acceleration method we observe 30% faster average convergence when compared to other accelerated BD implementations. We introduce a reinforcement learning agent as a surrogate and demonstrate how it can be used to solve a stochastic invent
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.08813</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#27604;&#36739;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#23545;&#20110;&#25581;&#31034;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#30740;&#31350;&#29983;&#29289;&#21151;&#33021;&#21644;&#22797;&#26434;&#30142;&#30149;&#30340;&#22522;&#26412;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#26469;&#33258;&#25991;&#29486;&#21644;&#20854;&#20182;&#28304;&#30340;&#31574;&#21010;&#29983;&#29289;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#19981;&#23436;&#25972;&#19988;&#32500;&#25252;&#24037;&#20316;&#32321;&#37325;&#65292;&#22240;&#27492;&#38656;&#35201;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#33258;&#21160;&#20174;&#30456;&#20851;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#36825;&#20123;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#12289;&#36890;&#36335;&#21644;&#22522;&#22240;&#35843;&#25511;&#20851;&#31995;&#31561;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#37325;&#35201;&#30340;&#21457;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#38142;&#25509;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding protein interactions and pathway knowledge is crucial for unraveling the complexities of living systems and investigating the underlying mechanisms of biological functions and complex diseases. While existing databases provide curated biological data from literature and other sources, they are often incomplete and their maintenance is labor-intensive, necessitating alternative approaches. In this study, we propose to harness the capabilities of large language models to address these issues by automatically extracting such knowledge from the relevant scientific literature. Toward this goal, in this work, we investigate the effectiveness of different large language models in tasks that involve recognizing protein interactions, pathways, and gene regulatory relations. We thoroughly evaluate the performance of various models, highlight the significant findings, and discuss both the future opportunities and the remaining challenges associated with this approach. The code and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#23384;&#20648;&#36890;&#36947;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#36807;&#24230;&#21442;&#25968;&#21270;&#26469;&#22686;&#21152;&#36890;&#36947;&#23481;&#37327;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26102;&#23884;&#20837;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#40657;&#30418;&#35775;&#38382;&#23454;&#29616;&#20449;&#24687;&#30340;&#23384;&#20648;&#21644;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2307.08811</link><description>&lt;p&gt;
DeepMem: &#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20316;&#23384;&#20648;&#36890;&#36947;&#21450;&#20854;&#65288;&#35823;&#29992;&#65289;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DeepMem: ML Models as storage channels and their (mis-)applications. (arXiv:2307.08811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#23384;&#20648;&#36890;&#36947;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#36807;&#24230;&#21442;&#25968;&#21270;&#26469;&#22686;&#21152;&#36890;&#36947;&#23481;&#37327;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26102;&#23884;&#20837;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#40657;&#30418;&#35775;&#38382;&#23454;&#29616;&#20449;&#24687;&#30340;&#23384;&#20648;&#21644;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20026;&#20102;&#25903;&#25345;&#36890;&#29992;&#24615;&#21644;&#36991;&#20813;&#36807;&#25311;&#21512;&#32780;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#21442;&#25968;&#26082;&#21487;&#20197;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#38544;&#34255;&#19968;&#20010;&#27169;&#22411;&#65289;&#65292;&#20063;&#21487;&#20197;&#29992;&#20110;&#26377;&#30410;&#30446;&#30340;&#65288;&#20363;&#22914;&#65292;&#32473;&#27169;&#22411;&#21152;&#19978;&#27700;&#21360;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20449;&#24687;&#35770;&#35270;&#35282;&#65307;&#25105;&#20204;&#23558;ML&#27169;&#22411;&#35270;&#20026;&#19968;&#20010;&#23384;&#20648;&#36890;&#36947;&#65292;&#20854;&#23481;&#37327;&#38543;&#30528;&#36807;&#24230;&#21442;&#25968;&#21270;&#32780;&#22686;&#21152;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21457;&#36865;&#26041;&#65292;&#22312;&#35757;&#32451;&#26102;&#23558;&#20219;&#24847;&#20449;&#24687;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#25509;&#25910;&#26041;&#21487;&#20197;&#36890;&#36807;&#23545;&#37096;&#32626;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#25105;&#20204;&#26681;&#25454;&#21487;&#29992;&#21442;&#25968;&#30340;&#25968;&#37327;&#25512;&#23548;&#20986;&#36890;&#36947;&#23481;&#37327;&#30340;&#19978;&#30028;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#40657;&#30418;&#20889;&#20837;&#21644;&#35835;&#21462;&#21407;&#35821;&#65292;&#20801;&#35768;&#25915;&#20987;&#32773;&#65306;&#65288;i&#65289;&#36890;&#36807;&#22312;&#21457;&#23556;&#26426;&#31471;&#25193;&#20805;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#24335;&#20197;&#20248;&#21270;&#22320;&#23558;&#25968;&#25454;&#23384;&#20648;&#22312;&#27169;&#22411;&#20013;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#36890;&#36807;&#26597;&#35810;&#27169;&#22411;&#26469;&#35835;&#21462;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models are overparameterized to support generality and avoid overfitting. Prior works have shown that these additional parameters can be used for both malicious (e.g., hiding a model covertly within a trained model) and beneficial purposes (e.g., watermarking a model). In this paper, we propose a novel information theoretic perspective of the problem; we consider the ML model as a storage channel with a capacity that increases with overparameterization. Specifically, we consider a sender that embeds arbitrary information in the model at training time, which can be extracted by a receiver with a black-box access to the deployed model. We derive an upper bound on the capacity of the channel based on the number of available parameters. We then explore black-box write and read primitives that allow the attacker to: (i) store data in an optimized way within the model by augmenting the training data at the transmitter side, and (ii) to read it by querying the model afte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#22686;&#24378;&#27169;&#25311;&#25351;&#23548;&#30340;&#25805;&#20316;&#21592;&#25351;&#23548;&#26041;&#27861;&#65292;&#21033;&#29992;LSTM&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#21452;&#23792;&#21452;&#21521;&#28023;&#20917;&#19979;&#33337;&#33334;&#21709;&#24212;&#32479;&#35745;&#37327;&#12290;&#36890;&#36807;&#23545;&#27604;&#20302;&#20445;&#30495;&#24230;&#21644;&#39640;&#20445;&#30495;&#24230;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08810</link><description>&lt;p&gt;
AI&#22686;&#24378;&#27169;&#25311;&#25351;&#23548;&#30340;&#25805;&#20316;&#21592;&#25351;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Operator Guidance Informed by AI-Augmented Simulations. (arXiv:2307.08810v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#22686;&#24378;&#27169;&#25311;&#25351;&#23548;&#30340;&#25805;&#20316;&#21592;&#25351;&#23548;&#26041;&#27861;&#65292;&#21033;&#29992;LSTM&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#21452;&#23792;&#21452;&#21521;&#28023;&#20917;&#19979;&#33337;&#33334;&#21709;&#24212;&#32479;&#35745;&#37327;&#12290;&#36890;&#36807;&#23545;&#27604;&#20302;&#20445;&#30495;&#24230;&#21644;&#39640;&#20445;&#30495;&#24230;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#22810;&#20445;&#30495;&#24230;&#21644;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;(LSTM)&#26469;&#20272;&#35745;&#21452;&#23792;&#21452;&#21521;&#28023;&#20917;&#19979;&#33337;&#33334;&#21709;&#24212;&#32479;&#35745;&#37327;&#12290;&#30740;&#31350;&#23558;&#37319;&#29992;&#24555;&#36895;&#20302;&#20445;&#30495;&#24230;&#30340;&#22522;&#20110;&#20307;&#31215;&#30340;&#24037;&#20855;SimpleCode&#21644;&#26356;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#20855;Large Amplitude Motion Program (LAMP)&#12290;&#35757;&#32451;&#25968;&#25454;&#26159;&#36890;&#36807;&#24120;&#35265;&#30340;&#21452;&#23792;&#21452;&#21521;&#28023;&#20917;&#22312;&#21271;&#22823;&#35199;&#27915;&#29983;&#25104;&#30340;SimpleCode&#21644;LAMP&#25968;&#25454;&#12290;&#22312;&#29992;LAMP&#33337;&#33334;&#36816;&#21160;&#21709;&#24212;&#25968;&#25454;&#35757;&#32451;LSTM&#32593;&#32476;&#21518;&#65292;&#26679;&#26412;&#36335;&#32447;&#34987;&#31359;&#36807;&#65292;&#38543;&#26426;&#36873;&#21462;&#21382;&#21490;&#22825;&#27668;&#36755;&#20837;SimpleCode&#21644;LSTM&#32593;&#32476;&#65292;&#24182;&#19982;&#26356;&#39640;&#20445;&#30495;&#24230;&#30340;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper will present a multi-fidelity, data-adaptive approach with a Long Short-Term Memory (LSTM) neural network to estimate ship response statistics in bimodal, bidirectional seas. The study will employ a fast low-fidelity, volume-based tool SimpleCode and a higher-fidelity tool known as the Large Amplitude Motion Program (LAMP). SimpleCode and LAMP data were generated by common bi-modal, bi-directional sea conditions in the North Atlantic as training data. After training an LSTM network with LAMP ship motion response data, a sample route was traversed and randomly sampled historical weather was input into SimpleCode and the LSTM network, and compared against the higher fidelity results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FedLabel&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#23458;&#25143;&#31471;&#26681;&#25454;&#25968;&#25454;&#30340;&#19987;&#19994;&#24615;&#36873;&#25321;&#26412;&#22320;&#25110;&#20840;&#23616;&#27169;&#22411;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#20266;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;-&#26412;&#22320;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26469;&#21033;&#29992;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.08809</link><description>&lt;p&gt;
&#26412;&#22320;&#25110;&#20840;&#23616;&#65306;&#22522;&#20110;&#26377;&#38480;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36873;&#25321;&#24615;&#30693;&#35782;&#21516;&#21270;
&lt;/p&gt;
&lt;p&gt;
Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels. (arXiv:2307.08809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FedLabel&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#23458;&#25143;&#31471;&#26681;&#25454;&#25968;&#25454;&#30340;&#19987;&#19994;&#24615;&#36873;&#25321;&#26412;&#22320;&#25110;&#20840;&#23616;&#27169;&#22411;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#20266;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;-&#26412;&#22320;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26469;&#21033;&#29992;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#23458;&#25143;&#31471;&#20855;&#26377;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#32780;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#26631;&#35760;&#36807;&#31243;&#30340;&#26114;&#36149;&#21644;&#36153;&#21147;&#65292;&#23458;&#25143;&#31471;&#21482;&#26377;&#26377;&#38480;&#30340;&#26631;&#31614;&#12290;&#23458;&#25143;&#31471;&#26377;&#38480;&#30340;&#26631;&#35760;&#26412;&#22320;&#25968;&#25454;&#24120;&#24120;&#23548;&#33268;&#23427;&#20204;&#30340;&#26412;&#22320;&#27169;&#22411;&#23545;&#20854;&#26356;&#22823;&#30340;&#26080;&#26631;&#31614;&#26412;&#22320;&#25968;&#25454;&#20855;&#26377;&#36739;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20363;&#22914;&#19982;&#26080;&#26631;&#31614;&#25968;&#25454;&#23384;&#22312;&#31867;&#20998;&#24067;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#20250;&#36873;&#25321;&#20174;&#36328;&#23458;&#25143;&#31471;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#20013;&#21463;&#30410;&#65292;&#20197;&#21033;&#29992;&#20182;&#20204;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#20294;&#30001;&#20110;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#36825;&#20063;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedLabel&#65292;&#23458;&#25143;&#31471;&#26681;&#25454;&#21738;&#20010;&#27169;&#22411;&#23545;&#25968;&#25454;&#26356;&#20855;&#19987;&#19994;&#30693;&#35782;&#36873;&#25321;&#26412;&#22320;&#25110;&#20840;&#23616;&#27169;&#22411;&#26469;&#20266;&#26631;&#35760;&#20854;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#20840;&#23616;-&#26412;&#22320;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26469;&#21033;&#29992;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#24403;&#23427;&#20204;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#20855;&#26377;&#30456;&#21516;&#30340;&#20266;&#26631;&#31614;&#26102;&#65292;&#26368;&#23567;&#21270;&#20004;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many existing FL methods assume clients with fully-labeled data, while in realistic settings, clients have limited labels due to the expensive and laborious process of labeling. Limited labeled local data of the clients often leads to their local model having poor generalization abilities to their larger unlabeled local data, such as having class-distribution mismatch with the unlabeled data. As a result, clients may instead look to benefit from the global model trained across clients to leverage their unlabeled data, but this also becomes difficult due to data heterogeneity across clients. In our work, we propose FedLabel where clients selectively choose the local or global model to pseudo-label their unlabeled data depending on which is more of an expert of the data. We further utilize both the local and global models' knowledge via global-local consistency regularization which minimizes the divergence between the two models' outputs when they have identical pseudo-labels for the unl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36873;&#25321;&#24615;&#23383;&#20856;&#23398;&#20064;&#30340;&#26032;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23558;&#23383;&#20856;&#23398;&#20064;&#31639;&#27861;&#21644;&#26680;&#23383;&#20856;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#20026;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#38382;&#39064;&#30340;&#38477;&#32500;&#26680;&#23383;&#20856;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#20449;&#21495;&#30340;&#26041;&#27861;&#25913;&#36827;&#31639;&#27861;&#20197;&#28040;&#38500;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#25152;&#26377;&#31639;&#27861;&#22343;&#34987;&#25972;&#21512;&#22312;&#19968;&#20010;&#24322;&#24120;&#26816;&#27979;&#24037;&#20855;&#31665;&#20013;&#65292;&#24182;&#19982;&#26631;&#20934;&#22522;&#20934;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.08807</link><description>&lt;p&gt;
&#22522;&#20110;&#36873;&#25321;&#24615;&#23383;&#20856;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection with Selective Dictionary Learning. (arXiv:2307.08807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08807
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36873;&#25321;&#24615;&#23383;&#20856;&#23398;&#20064;&#30340;&#26032;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23558;&#23383;&#20856;&#23398;&#20064;&#31639;&#27861;&#21644;&#26680;&#23383;&#20856;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#20026;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#38382;&#39064;&#30340;&#38477;&#32500;&#26680;&#23383;&#20856;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#20449;&#21495;&#30340;&#26041;&#27861;&#25913;&#36827;&#31639;&#27861;&#20197;&#28040;&#38500;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#25152;&#26377;&#31639;&#27861;&#22343;&#34987;&#25972;&#21512;&#22312;&#19968;&#20010;&#24322;&#24120;&#26816;&#27979;&#24037;&#20855;&#31665;&#20013;&#65292;&#24182;&#19982;&#26631;&#20934;&#22522;&#20934;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#26680;&#23383;&#20856;&#23398;&#20064;&#65288;KDL&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#26032;&#26041;&#27861;&#12290;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23558;&#24050;&#30693;&#30340;DL&#21644;KDL&#31639;&#27861;&#25913;&#36827;&#20026;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#30340;&#26680;&#23383;&#20856;&#23398;&#20064;&#26041;&#27861;&#65288;RKDL&#65289;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#22823;&#26680;&#30697;&#38453;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#20449;&#21495;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;DL&#21644;RKDL&#26041;&#27861;&#65292;&#26088;&#22312;&#28040;&#38500;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#25152;&#26377;&#31639;&#27861;&#22343;&#34987;&#38598;&#25104;&#22312;&#19968;&#20010;&#24322;&#24120;&#26816;&#27979;&#24037;&#20855;&#31665;&#20013;&#65292;&#24182;&#19982;&#26631;&#20934;&#22522;&#20934;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present new methods of anomaly detection based on Dictionary Learning (DL) and Kernel Dictionary Learning (KDL). The main contribution consists in the adaption of known DL and KDL algorithms in the form of unsupervised methods, used for outlier detection. We propose a reduced kernel version (RKDL), which is useful for problems with large data sets, due to the large kernel matrix. We also improve the DL and RKDL methods by the use of a random selection of signals, which aims to eliminate the outliers from the training procedure. All our algorithms are introduced in an anomaly detection toolbox and are compared to standard benchmark results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;libLEARNA&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#26680;&#37240;&#24320;&#20851;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#20840;&#23616;&#29305;&#24615;&#21644;&#24207;&#21015;&#32467;&#26500;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#20379;&#22810;&#26679;&#24615;&#30340;&#21487;&#21464;&#38271;&#24230;&#21512;&#26684;&#20505;&#36873;&#29289;&#30340;RNA&#20851;&#27880;&#24211;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#33719;&#24471;30%&#26356;&#22810;&#29420;&#29305;&#39640;&#36136;&#37327;&#30340;&#20505;&#36873;&#29289;&#12290;</title><link>http://arxiv.org/abs/2307.08801</link><description>&lt;p&gt;
&#33258;&#21160;&#35774;&#35745;&#26680;&#37240;&#24320;&#20851;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Automated Design of Riboswitches. (arXiv:2307.08801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;libLEARNA&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#26680;&#37240;&#24320;&#20851;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#20840;&#23616;&#29305;&#24615;&#21644;&#24207;&#21015;&#32467;&#26500;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#20379;&#22810;&#26679;&#24615;&#30340;&#21487;&#21464;&#38271;&#24230;&#21512;&#26684;&#20505;&#36873;&#29289;&#30340;RNA&#20851;&#27880;&#24211;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#33719;&#24471;30%&#26356;&#22810;&#29420;&#29305;&#39640;&#36136;&#37327;&#30340;&#20505;&#36873;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#31579;&#36873;&#21644;&#36873;&#25321;&#26032;&#22411;&#26680;&#37240;&#24320;&#20851;&#30340;&#27969;&#31243;&#25104;&#26412;&#39640;&#12289;&#32791;&#26102;&#38271;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#20351;&#29992;&#35745;&#31639;&#26041;&#27861;&#26469;&#20943;&#23569;&#31579;&#36873;&#20505;&#36873;&#29289;&#30340;&#25968;&#37327;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#36825;&#20123;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35745;&#31639;&#26041;&#27861;&#24182;&#19981;&#33021;&#23436;&#20840;&#28385;&#36275;&#35774;&#35745;&#27492;&#31867;&#21021;&#22987;&#31579;&#36873;&#24211;&#30340;&#25152;&#26377;&#35201;&#27714;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;libLEARNA&#65292;&#33021;&#22815;&#25552;&#20379;&#22810;&#26679;&#24615;&#30340;&#21487;&#21464;&#38271;&#24230;&#21512;&#26684;&#20505;&#36873;&#29289;&#30340;RNA&#20851;&#27880;&#24211;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#22522;&#20110;&#32467;&#26500;&#30340;&#35774;&#35745;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#20840;&#23616;&#29305;&#24615;&#65292;&#36824;&#32771;&#34385;&#25152;&#38656;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#25353;&#29031;&#20808;&#21069;&#21457;&#34920;&#30340;&#21327;&#35758;&#35774;&#35745;&#33590;&#30897;&#26680;&#37240;&#24320;&#20851;&#24211;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#33719;&#24471;&#20102;30%&#26356;&#22810;&#29420;&#29305;&#30340;&#39640;&#36136;&#37327;&#20505;&#36873;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimental screening and selection pipelines for the discovery of novel riboswitches are expensive, time-consuming, and inefficient. Using computational methods to reduce the number of candidates for the screen could drastically decrease these costs. However, existing computational approaches do not fully satisfy all requirements for the design of such initial screening libraries. In this work, we present a new method, libLEARNA, capable of providing RNA focus libraries of diverse variable-length qualified candidates. Our novel structure-based design approach considers global properties as well as desired sequence and structure features. We demonstrate the benefits of our method by designing theophylline riboswitch libraries, following a previously published protocol, and yielding 30% more unique high-quality candidates.
&lt;/p&gt;</description></item><item><title>regulAS&#26159;&#19968;&#20010;&#29992;&#20110;&#20174;RNA-Seq&#25968;&#25454;&#20013;&#30740;&#31350;&#36873;&#25321;&#24615;&#21098;&#20999;&#35843;&#25511;&#26426;&#21046;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#21270;&#35745;&#31639;&#23454;&#39564;&#12289;&#39640;&#25928;&#22788;&#29702;&#32467;&#26524;&#12289;&#20248;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#20855;&#26377;&#22522;&#20110;&#20844;&#20849;&#25968;&#25454;&#20179;&#24211;&#30340;&#25968;&#25454;&#26816;&#32034;&#12289;&#39044;&#27979;&#24314;&#27169;&#21644;&#28789;&#27963;&#25253;&#21578;&#29983;&#25104;&#31561;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08800</link><description>&lt;p&gt;
regulAS:&#19968;&#31181;&#29992;&#20110;&#20351;&#29992;RNA-Seq&#25968;&#25454;&#36827;&#34892;&#36873;&#25321;&#24615;&#21098;&#20999;&#35843;&#25511;&#32452;&#23398;&#32508;&#21512;&#20998;&#26512;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
regulAS: A Bioinformatics Tool for the Integrative Analysis of Alternative Splicing Regulome using RNA-Seq data. (arXiv:2307.08800v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08800
&lt;/p&gt;
&lt;p&gt;
regulAS&#26159;&#19968;&#20010;&#29992;&#20110;&#20174;RNA-Seq&#25968;&#25454;&#20013;&#30740;&#31350;&#36873;&#25321;&#24615;&#21098;&#20999;&#35843;&#25511;&#26426;&#21046;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#21270;&#35745;&#31639;&#23454;&#39564;&#12289;&#39640;&#25928;&#22788;&#29702;&#32467;&#26524;&#12289;&#20248;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#20855;&#26377;&#22522;&#20110;&#20844;&#20849;&#25968;&#25454;&#20179;&#24211;&#30340;&#25968;&#25454;&#26816;&#32034;&#12289;&#39044;&#27979;&#24314;&#27169;&#21644;&#28789;&#27963;&#25253;&#21578;&#29983;&#25104;&#31561;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
regulAS&#36719;&#20214;&#21253;&#26159;&#19968;&#20010;&#29983;&#29289;&#20449;&#24687;&#23398;&#24037;&#20855;&#65292;&#26088;&#22312;&#25903;&#25345;&#35745;&#31639;&#29983;&#29289;&#23398;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#26469;&#33258;TCGA&#21644;GTEx&#39033;&#30446;&#30340;&#30284;&#30151;&#21644;&#20581;&#24247;&#20154;&#31867;&#20379;&#20307;&#30340;&#22823;&#35268;&#27169;RNA-Seq&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#21098;&#20999;&#25913;&#21464;&#30340;&#35843;&#25511;&#26426;&#21046;&#12290;&#36825;&#31687;&#25216;&#26415;&#25253;&#21578;&#20840;&#38754;&#20171;&#32461;&#20102;regulAS&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#20854;&#26680;&#24515;&#21151;&#33021;&#12289;&#22522;&#26412;&#27169;&#22359;&#12289;&#23454;&#39564;&#37197;&#32622;&#12289;&#36827;&#19968;&#27493;&#21487;&#25193;&#23637;&#24615;&#21644;&#33258;&#23450;&#20041;&#24615;&#12290;regulAS&#30340;&#26680;&#24515;&#21151;&#33021;&#21253;&#25324;&#33258;&#21160;&#21270;&#35745;&#31639;&#23454;&#39564;&#12289;&#39640;&#25928;&#32467;&#26524;&#23384;&#20648;&#21644;&#22788;&#29702;&#20197;&#21450;&#27969;&#31243;&#31649;&#29702;&#12290;&#38598;&#25104;&#30340;&#22522;&#26412;&#27169;&#22359;&#36890;&#36807;&#20351;&#29992;scikit-learn&#21253;&#20174;&#20844;&#20849;&#30340;&#22810;&#32452;&#23398;UCSC Xena&#25968;&#25454;&#20179;&#24211;&#26816;&#32034;RNA-Seq&#25968;&#25454;&#65292;&#20855;&#26377;&#39044;&#27979;&#24314;&#27169;&#21644;&#29305;&#24449;&#25490;&#24207;&#33021;&#21147;&#65292;&#24182;&#21487;&#29983;&#25104;&#28789;&#27963;&#30340;&#25253;&#21578;&#65292;&#29992;&#20110;&#20998;&#26512;&#22522;&#22240;&#34920;&#36798;&#35889;&#21644;&#30456;&#20851;&#30340;&#36873;&#25321;&#24615;&#21098;&#20999;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
The regulAS software package is a bioinformatics tool designed to support computational biology researchers in investigating regulatory mechanisms of splicing alterations through integrative analysis of large-scale RNA-Seq data from cancer and healthy human donors, characterized by TCGA and GTEx projects. This technical report provides a comprehensive overview of regulAS, focusing on its core functionality, basic modules, experiment configuration, further extensibility and customisation.  The core functionality of regulAS enables the automation of computational experiments, efficient results storage and processing, and streamlined workflow management. Integrated basic modules extend regulAS with features such as RNA-Seq data retrieval from the public multi-omics UCSC Xena data repository, predictive modeling and feature ranking capabilities using the scikit-learn package, and flexible reporting generation for analysing gene expression profiles and relevant modulations of alternative sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#26680;&#23383;&#20856;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36755;&#20837;&#20449;&#21495;&#30340;&#35757;&#32451;&#31232;&#30095;&#34920;&#31034;&#26469;&#33719;&#24471;&#26680;&#21521;&#37327;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#30452;&#25509;&#20248;&#21270;&#26680;&#21521;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20351;&#29992;&#23569;&#37327;&#26680;&#21521;&#37327;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#20943;&#23569;&#25191;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.08798</link><description>&lt;p&gt;
&#38477;&#32500;&#26680;&#23383;&#20856;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reduced Kernel Dictionary Learning. (arXiv:2307.08798v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#26680;&#23383;&#20856;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36755;&#20837;&#20449;&#21495;&#30340;&#35757;&#32451;&#31232;&#30095;&#34920;&#31034;&#26469;&#33719;&#24471;&#26680;&#21521;&#37327;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#30452;&#25509;&#20248;&#21270;&#26680;&#21521;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20351;&#29992;&#23569;&#37327;&#26680;&#21521;&#37327;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#20943;&#23569;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#26680;&#23383;&#20856;&#23398;&#20064;(KDL)&#38382;&#39064;&#20013;&#30340;&#38477;&#32500;&#38750;&#32447;&#24615;&#34920;&#31034;&#30340;&#26032;&#31639;&#27861;&#12290;&#26631;&#20934;&#30340;KDL&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#37027;&#23601;&#26159;&#24403;&#25968;&#25454;&#38598;&#24456;&#22823;&#26102;&#65292;&#26680;&#30697;&#38453;&#30340;&#22823;&#23567;&#24456;&#22823;&#12290;&#26377;&#20960;&#31181;&#20943;&#23567;&#26680;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;Nystr&#246;m&#25277;&#26679;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31526;&#21512;&#23383;&#20856;&#23398;&#20064;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#26680;&#21521;&#37327;&#26159;&#36890;&#36807;&#36755;&#20837;&#20449;&#21495;&#30340;&#35757;&#32451;&#31232;&#30095;&#34920;&#31034;&#26469;&#33719;&#24471;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30452;&#25509;&#20248;&#21270;&#20102;KDL&#36807;&#31243;&#20013;&#30340;&#26680;&#21521;&#37327;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#31034;&#65292;&#23613;&#31649;&#20351;&#29992;&#20102;&#23569;&#37327;&#30340;&#26680;&#21521;&#37327;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;KDL&#36824;&#20943;&#23569;&#20102;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present new algorithms for training reduced-size nonlinear representations in the Kernel Dictionary Learning (KDL) problem. Standard KDL has the drawback of a large size of the kernel matrix when the data set is large. There are several ways of reducing the kernel size, notably Nystr\"om sampling. We propose here a method more in the spirit of dictionary learning, where the kernel vectors are obtained with a trained sparse representation of the input signals. Moreover, we optimize directly the kernel vectors in the KDL process, using gradient descent steps. We show with three data sets that our algorithms are able to provide better representations, despite using a small number of kernel vectors, and also decrease the execution time with respect to KDL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#30456;&#24178;&#26680;&#23383;&#20856;&#23398;&#20064;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#26631;&#20934;&#32447;&#24615;&#23545;&#24212;&#29289;&#65292;&#23454;&#29616;&#20102;&#38750;&#30456;&#24178;&#23383;&#20856;&#23398;&#20064;&#30340;&#26680;&#29256;&#26412;&#65292;&#24182;&#23545;&#34920;&#31034;&#26356;&#26032;&#31639;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.08796</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#30456;&#24178;&#26680;&#23383;&#20856;&#23398;&#20064;&#30340;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Classification with Incoherent Kernel Dictionary Learning. (arXiv:2307.08796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#30456;&#24178;&#26680;&#23383;&#20856;&#23398;&#20064;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#26631;&#20934;&#32447;&#24615;&#23545;&#24212;&#29289;&#65292;&#23454;&#29616;&#20102;&#38750;&#30456;&#24178;&#23383;&#20856;&#23398;&#20064;&#30340;&#26680;&#29256;&#26412;&#65292;&#24182;&#23545;&#34920;&#31034;&#26356;&#26032;&#31639;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#30340;&#26032;&#20998;&#31867;&#26041;&#27861;&#12290;&#20027;&#35201;&#36129;&#29486;&#26159;&#36890;&#36807;&#23545;&#20854;&#26631;&#20934;&#32447;&#24615;&#23545;&#24212;&#29289;&#36827;&#34892;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#38750;&#30456;&#24178;&#23383;&#20856;&#23398;&#20064;&#30340;&#26680;&#29256;&#26412;&#12290;&#25105;&#20204;&#36824;&#23545;AK-SVD&#31639;&#27861;&#30340;&#34920;&#31034;&#26356;&#26032;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#24120;&#29992;&#30340;&#20998;&#31867;&#38382;&#39064;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a new classification method based on Dictionary Learning (DL). The main contribution consists of a kernel version of incoherent DL, derived from its standard linear counterpart. We also propose an improvement of the AK-SVD algorithm concerning the representation update. Our algorithms are tested on several popular databases of classification problems.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22810;&#26102;&#38388;&#23610;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#38750;&#24179;&#31283;&#31574;&#30053;&#12290;&#20182;&#20204;&#21033;&#29992;&#26234;&#33021;&#20307;&#26102;&#38388;&#23610;&#24230;&#30340;&#20449;&#24687;&#23450;&#20041;&#20102;&#21608;&#26399;&#24615;&#26102;&#38388;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#21608;&#26399;&#24615;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#23398;&#20064;&#22810;&#26102;&#38388;&#23610;&#24230;&#24341;&#20837;&#30340;&#38750;&#24179;&#31283;&#24615;&#30340;&#25928;&#26524;&#12290;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#26469;&#23398;&#20064;&#36825;&#20123;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.08794</link><description>&lt;p&gt;
&#22810;&#26102;&#38388;&#23610;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#24179;&#31283;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning. (arXiv:2307.08794v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08794
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22810;&#26102;&#38388;&#23610;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#38750;&#24179;&#31283;&#31574;&#30053;&#12290;&#20182;&#20204;&#21033;&#29992;&#26234;&#33021;&#20307;&#26102;&#38388;&#23610;&#24230;&#30340;&#20449;&#24687;&#23450;&#20041;&#20102;&#21608;&#26399;&#24615;&#26102;&#38388;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#21608;&#26399;&#24615;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#23398;&#20064;&#22810;&#26102;&#38388;&#23610;&#24230;&#24341;&#20837;&#30340;&#38750;&#24179;&#31283;&#24615;&#30340;&#25928;&#26524;&#12290;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#26469;&#23398;&#20064;&#36825;&#20123;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26102;&#38388;&#23610;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#20132;&#20114;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#23545;&#20110;&#21463;&#22810;&#26102;&#38388;&#23610;&#24230;&#24341;&#36215;&#30340;&#26102;&#38388;&#20381;&#36182;&#34892;&#20026;&#65292;&#31574;&#30053;&#26159;&#38750;&#24179;&#31283;&#30340;&#12290;&#23398;&#20064;&#38750;&#24179;&#31283;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24448;&#24448;&#38656;&#35201;&#22797;&#26434;&#25110;&#20302;&#25928;&#30340;&#31639;&#27861;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#31995;&#32479;&#20013;&#25511;&#21046;&#38382;&#39064;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#26469;&#23398;&#20064;&#22810;&#26102;&#38388;&#23610;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#24179;&#31283;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20851;&#20110;&#26234;&#33021;&#20307;&#26102;&#38388;&#23610;&#24230;&#30340;&#21487;&#29992;&#20449;&#24687;&#26469;&#23450;&#20041;&#21608;&#26399;&#24615;&#26102;&#38388;&#32534;&#30721;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36890;&#36807;&#21608;&#26399;&#24615;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#21487;&#20197;&#23398;&#20064;&#22810;&#26102;&#38388;&#23610;&#24230;&#24341;&#20837;&#30340;&#38750;&#24179;&#31283;&#24615;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#23398;&#20064;&#36825;&#26679;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#30456;&#20301;&#20989;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#28436;&#21592;&#21644;&#35780;&#35770;&#23478;&#65292;&#20026;&#21608;&#26399;&#24615;&#25552;&#20379;&#24402;&#32435;&#20559;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-timescale multi-agent reinforcement learning (MARL), agents interact across different timescales. In general, policies for time-dependent behaviors, such as those induced by multiple timescales, are non-stationary. Learning non-stationary policies is challenging and typically requires sophisticated or inefficient algorithms. Motivated by the prevalence of this control problem in real-world complex systems, we introduce a simple framework for learning non-stationary policies for multi-timescale MARL. Our approach uses available information about agent timescales to define a periodic time encoding. In detail, we theoretically demonstrate that the effects of non-stationarity introduced by multiple timescales can be learned by a periodic multi-agent policy. To learn such policies, we propose a policy gradient algorithm that parameterizes the actor and critic with phase-functioned neural networks, which provide an inductive bias for periodicity. The framework's ability to effective
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#24067;&#29305;&#24615;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#29575;&#65292;&#24182;&#36741;&#21161;&#20154;&#24037;&#26631;&#27880;&#26469;&#20943;&#23569;&#34394;&#35686;&#25968;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#32597;&#35265;&#26696;&#20363;&#30340;&#26816;&#27979;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.08782</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#24067;&#29305;&#24615;&#21487;&#20197;&#36741;&#21161;&#20154;&#24037;&#26631;&#27880;&#24182;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning of Distributional Properties can Supplement Human Labeling and Increase Active Learning Efficiency in Anomaly Detection. (arXiv:2307.08782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08782
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#24067;&#29305;&#24615;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#29575;&#65292;&#24182;&#36741;&#21161;&#20154;&#24037;&#26631;&#27880;&#26469;&#20943;&#23569;&#34394;&#35686;&#25968;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#32597;&#35265;&#26696;&#20363;&#30340;&#26816;&#27979;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#36890;&#36807;&#30005;&#23376;&#37038;&#20214;&#30340;&#27844;&#38706;&#23545;&#35768;&#22810;&#32452;&#32455;&#26469;&#35828;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#12290;&#26816;&#27979;&#25968;&#25454;&#27844;&#38706;&#65288;&#24322;&#24120;&#65289;&#27169;&#24335;&#36890;&#24120;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#26469;&#20943;&#23569;&#34394;&#35686;&#30340;&#25968;&#37327;&#12290;&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#26631;&#27880;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#38656;&#35201;&#36873;&#25321;&#19968;&#20010;&#26377;&#25928;&#30340;&#39034;&#24207;&#26469;&#26631;&#27880;&#26696;&#20363;&#65292;&#24182;&#19988;&#22312;&#30830;&#23450;&#29992;&#20110;&#20248;&#20808;&#26631;&#27880;&#26696;&#20363;&#30340;&#35780;&#20998;&#36807;&#31243;&#26102;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20851;&#38190;&#26102;&#21051;&#26816;&#27979;&#32597;&#35265;&#30340;&#26696;&#20363;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20027;&#21160;&#23398;&#20064;&#37319;&#26679;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#20808;&#21069;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20135;&#29983;&#25209;&#27425;&#30340;&#26696;&#20363;&#20197;&#36827;&#34892;&#26631;&#27880;&#65292;&#20854;&#20013;&#21253;&#21547;&#32597;&#35265;&#24322;&#24120;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65306;&#65288;1&#65289;&#20998;&#31867;&#22120;&#20174;&#20855;&#26377;&#20195;&#34920;&#24615;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#27491;&#24120;&#19982;&#24322;&#24120;&#23454;&#20363;&#30340;&#25209;&#27425;&#20013;&#21463;&#30410;&#65292;&#65288;2&#65289;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#22312;&#26500;&#24314;&#20998;&#31867;&#22120;&#20013;&#21457;&#25381;&#20102;&#26377;&#29992;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exfiltration of data via email is a serious cybersecurity threat for many organizations. Detecting data exfiltration (anomaly) patterns typically requires labeling, most often done by a human annotator, to reduce the high number of false alarms. Active Learning (AL) is a promising approach for labeling data efficiently, but it needs to choose an efficient order in which cases are to be labeled, and there are uncertainties as to what scoring procedure should be used to prioritize cases for labeling, especially when detecting rare cases of interest is crucial. We propose an adaptive AL sampling strategy that leverages the underlying prior data distribution, as well as model uncertainty, to produce batches of cases to be labeled that contain instances of rare anomalies. We show that (1) the classifier benefits from a batch of representative and informative instances of both normal and anomalous examples, (2) unsupervised anomaly detection plays a useful role in building the classifier in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;&#25277;&#35937;&#23618;&#21644;&#31532;&#20108;&#23618;&#37319;&#29992;&#19981;&#21516;&#30340;&#25506;&#32034;&#26041;&#24335;&#65292;&#21462;&#24471;&#20102;&#36229;&#36807;2%&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.08767</link><description>&lt;p&gt;
&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#30340;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A mixed policy to improve performance of language models on math problems. (arXiv:2307.08767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;&#25277;&#35937;&#23618;&#21644;&#31532;&#20108;&#23618;&#37319;&#29992;&#19981;&#21516;&#30340;&#25506;&#32034;&#26041;&#24335;&#65292;&#21462;&#24471;&#20102;&#36229;&#36807;2%&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#37319;&#29992;&#37319;&#26679;&#31574;&#30053;&#26681;&#25454;&#26465;&#20214;&#27010;&#29575;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#12290;&#22312;&#25968;&#23398;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#21487;&#33021;&#20250;&#29983;&#25104;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#32771;&#34385;&#21040;&#25968;&#23398;&#38382;&#39064;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#30340;&#25506;&#32034;&#26041;&#27861;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#32423;&#26631;&#35760;&#25506;&#32034;&#31574;&#30053;&#65306;&#25277;&#35937;&#23618;&#20197;&#27010;&#29575;&#37319;&#26679;&#26469;&#20915;&#23450;&#19979;&#19968;&#20010;&#26631;&#35760;&#26159;&#36816;&#31639;&#31526;&#36824;&#26159;&#25805;&#20316;&#25968;&#65292;&#32780;&#31532;&#20108;&#23618;&#21017;&#20197;&#36138;&#23146;&#26041;&#24335;&#36873;&#25321;&#24471;&#20998;&#26368;&#39640;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-2&#27169;&#22411;&#22312;GSM8K&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36229;&#36807;2&#65285;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#20195;&#30721;&#21487;&#22312;https://github.com/vividitytech/math_lm_rl&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
When to solve math problems, most language models take a sampling strategy to predict next word according conditional probabilities. In the math reasoning step, it may generate wrong answer. Considering math problems are deterministic, we propose a mixed policy exploration approach to solve math problems with reinforcement learning. In peculiar, we propose a two level token exploration policy: the abstract level explores next token with probability and the second level is deterministic. Specifically, the abstract level policy will decide whether the token is operator or operand with probability sampling, while the second level is deterministic to select next token with the highest score in a greedy way. We test our method on GSM8K dataset with GPT-2 model, and demonstrate more than $2\%$ performance gain. Our implementation is available at https://github.com/vividitytech/math_lm_rl.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20809;&#30005;&#23481;&#25239;&#65288;PPG&#65289;&#20449;&#21495;&#36827;&#34892;&#35757;&#32451;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#36827;&#34892;&#36830;&#32493;&#30417;&#27979;&#26102;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24178;&#25200;&#22240;&#32032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08766</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#30417;&#27979;&#24515;&#34880;&#31649;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#20809;&#30005;&#23481;&#25239;&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices. (arXiv:2307.08766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20809;&#30005;&#23481;&#25239;&#65288;PPG&#65289;&#20449;&#21495;&#36827;&#34892;&#35757;&#32451;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#36827;&#34892;&#36830;&#32493;&#30417;&#27979;&#26102;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24178;&#25200;&#22240;&#32032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#25239;&#65288;PPG&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#25216;&#26415;&#65292;&#29992;&#20110;&#27979;&#37327;&#24494;&#34880;&#31649;&#32452;&#32455;&#20013;&#30340;&#34880;&#23481;&#37327;&#21464;&#21270;&#12290;&#23427;&#24120;&#34987;&#29992;&#20110;&#21307;&#30103;&#35774;&#22791;&#65292;&#22914;&#33033;&#25615;&#34880;&#27687;&#20202;&#21644;&#25163;&#33109;&#24335;&#24515;&#29575;&#30417;&#27979;&#22120;&#65292;&#29992;&#20110;&#30417;&#27979;&#24515;&#34880;&#31649;&#34880;&#28082;&#21160;&#21147;&#23398;&#12290;PPG&#21487;&#20197;&#35780;&#20272;&#24515;&#29575;&#12289;&#33033;&#25615;&#27874;&#24418;&#21644;&#22806;&#21608;&#28748;&#27880;&#31561;&#21442;&#25968;&#65292;&#20197;&#25351;&#31034;&#34880;&#31649;&#25910;&#32553;&#25110;&#25193;&#24352;&#31561;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#24494;&#34880;&#31649;&#34880;&#27969;&#30340;&#20449;&#24687;&#65292;&#26159;&#30417;&#27979;&#24515;&#34880;&#31649;&#20581;&#24247;&#30340;&#23453;&#36149;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;PPG&#21463;&#21040;&#22810;&#31181;&#21464;&#21270;&#28304;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#22312;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#36827;&#34892;&#36830;&#32493;&#30417;&#27979;&#26102;&#65292;&#22914;&#36816;&#21160;&#20266;&#24433;&#12289;&#30382;&#32932;&#33394;&#32032;&#21644;&#34880;&#31649;&#36816;&#21160;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;PPG&#20449;&#21495;&#20013;&#25552;&#21462;&#20102;27&#20010;&#32479;&#35745;&#29305;&#24449;&#65292;&#29992;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#21644;CatBoost&#65289;&#21644;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#31639;&#27861;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#35780;&#20272;&#20854;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) is a non-invasive technology that measures changes in blood volume in the microvascular bed of tissue. It is commonly used in medical devices such as pulse oximeters and wrist worn heart rate monitors to monitor cardiovascular hemodynamics. PPG allows for the assessment of parameters (e.g., heart rate, pulse waveform, and peripheral perfusion) that can indicate conditions such as vasoconstriction or vasodilation, and provides information about microvascular blood flow, making it a valuable tool for monitoring cardiovascular health. However, PPG is subject to a number of sources of variations that can impact its accuracy and reliability, especially when using a wearable device for continuous monitoring, such as motion artifacts, skin pigmentation, and vasomotion. In this study, we extracted 27 statistical features from the PPG signal for training machine-learning models based on gradient boosting (XGBoost and CatBoost) and Random Forest (RF) algorithms to asse
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#23558;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#24212;&#29992;&#20110;&#20809;&#24230;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#32422;&#26463;&#38464;&#34746;&#24180;&#40836;&#27861;&#25512;&#26029;&#24658;&#26143;&#24180;&#40836;&#30340;&#31934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#24471;&#21040;&#20102;&#25991;&#29486;&#20540;&#30340;&#33391;&#22909;&#24674;&#22797;&#12290;&#36825;&#19968;&#27010;&#29575;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#26377;&#26395;&#25193;&#22823;&#38464;&#34746;&#24180;&#40836;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2307.08753</link><description>&lt;p&gt;
&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#30340;&#26032;&#39062;&#24212;&#29992;&#65306;&#20351;&#29992;&#38464;&#34746;&#24180;&#40836;&#27861;&#25512;&#26029;&#24658;&#26143;&#24180;&#40836;
&lt;/p&gt;
&lt;p&gt;
A Novel Application of Conditional Normalizing Flows: Stellar Age Inference with Gyrochronology. (arXiv:2307.08753v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08753
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#23558;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#24212;&#29992;&#20110;&#20809;&#24230;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#32422;&#26463;&#38464;&#34746;&#24180;&#40836;&#27861;&#25512;&#26029;&#24658;&#26143;&#24180;&#40836;&#30340;&#31934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#24471;&#21040;&#20102;&#25991;&#29486;&#20540;&#30340;&#33391;&#22909;&#24674;&#22797;&#12290;&#36825;&#19968;&#27010;&#29575;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#26377;&#26395;&#25193;&#22823;&#38464;&#34746;&#24180;&#40836;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24658;&#26143;&#24180;&#40836;&#26159;&#36827;&#21270;&#27169;&#22411;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;&#20302;&#36136;&#37327;&#20027;&#24207;&#26143;&#32780;&#35328;&#65292;&#27979;&#37327;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#20010;&#33539;&#22260;&#20869;&#65292;&#23578;&#26410;&#25506;&#32034;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#38464;&#34746;&#24180;&#40836;&#23398;&#65292;&#36825;&#26159;&#19968;&#31181;&#38750;&#24120;&#36866;&#21512;&#36825;&#20123;&#24658;&#26143;&#30340;&#24658;&#26143;&#32422;&#20250;&#25216;&#26415;&#12290;&#34429;&#28982;&#31934;&#30830;&#30340;&#20998;&#26512;&#38464;&#34746;&#24180;&#40836;&#27169;&#22411;&#30340;&#24320;&#21457;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;&#36825;&#37324;&#23558;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#24212;&#29992;&#20110;&#24320;&#25918;&#26143;&#22242;&#30340;&#20809;&#24230;&#25968;&#25454;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#21487;&#20197;&#19982;&#20854;&#20182;&#26631;&#20934;&#25216;&#26415;&#30456;&#27604;&#32422;&#26463;&#38464;&#34746;&#24180;&#40836;&#12290;&#25105;&#20204;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#35780;&#20272;&#27969;&#21160;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#25512;&#26029;&#30340;&#24180;&#40836;&#19982;&#25991;&#29486;&#20540;&#36739;&#20026;&#21563;&#21512;&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#27010;&#29575;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#25193;&#22823;&#38464;&#34746;&#24180;&#40836;&#24658;&#26143;&#32422;&#20250;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stellar ages are critical building blocks of evolutionary models, but challenging to measure for low mass main sequence stars. An unexplored solution in this regime is the application of probabilistic machine learning methods to gyrochronology, a stellar dating technique that is uniquely well suited for these stars. While accurate analytical gyrochronological models have proven challenging to develop, here we apply conditional normalizing flows to photometric data from open star clusters, and demonstrate that a data-driven approach can constrain gyrochronological ages with a precision comparable to other standard techniques. We evaluate the flow results in the context of a Bayesian framework, and show that our inferred ages recover literature values well. This work demonstrates the potential of a probabilistic data-driven solution to widen the applicability of gyrochronological stellar dating.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#23545;KNN&#30340;&#20844;&#24179;&#24615;&#36827;&#34892;&#35748;&#35777;&#12290;&#36890;&#36807;&#36817;&#20284;&#35745;&#31639;&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#25277;&#35937;&#22495;&#20013;&#35745;&#31639;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08722</link><description>&lt;p&gt;
&#22312;&#23384;&#22312;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;KNN&#30340;&#20844;&#24179;&#24615;&#36827;&#34892;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Certifying the Fairness of KNN in the Presence of Dataset Bias. (arXiv:2307.08722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#23545;KNN&#30340;&#20844;&#24179;&#24615;&#36827;&#34892;&#35748;&#35777;&#12290;&#36890;&#36807;&#36817;&#20284;&#35745;&#31639;&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#25277;&#35937;&#22495;&#20013;&#35745;&#31639;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#23384;&#22312;&#21382;&#21490;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;KNN&#30340;&#20998;&#31867;&#32467;&#26524;&#30340;&#20844;&#24179;&#24615;&#36827;&#34892;&#35748;&#35777;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#19977;&#31181;&#20844;&#24179;&#24615;&#23450;&#20041;&#30340;&#21464;&#20307;&#65306;&#20010;&#20307;&#20844;&#24179;&#24615;&#12289;&#949;-&#20844;&#24179;&#24615;&#21644;&#26631;&#31614;&#32763;&#36716;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;KNN&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#38382;&#39064;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#22312;&#26368;&#20808;&#36827;&#30340;KNN&#31639;&#27861;&#20013;&#20351;&#29992;&#30340;&#22797;&#26434;&#31639;&#26415;&#35745;&#31639;&#30340;&#21487;&#38752;&#36817;&#20284;&#12290;&#36825;&#26088;&#22312;&#23558;&#35745;&#31639;&#32467;&#26524;&#20174;&#20855;&#20307;&#22495;&#25552;&#21319;&#21040;&#25277;&#35937;&#22495;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#22312;&#20844;&#24179;&#30740;&#31350;&#25991;&#29486;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#25216;&#26415;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method for certifying the fairness of the classification result of a widely used supervised learning algorithm, the k-nearest neighbors (KNN), under the assumption that the training data may have historical bias caused by systematic mislabeling of samples from a protected minority group. To the best of our knowledge, this is the first certification method for KNN based on three variants of the fairness definition: individual fairness, $\epsilon$-fairness, and label-flipping fairness. We first define the fairness certification problem for KNN and then propose sound approximations of the complex arithmetic computations used in the state-of-the-art KNN algorithm. This is meant to lift the computation results from the concrete domain to an abstract domain, to reduce the computational cost. We show effectiveness of this abstract interpretation based technique through experimental evaluation on six datasets widely used in the fairness research literature. We also show that the m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35273;&#27169;&#31946;&#24191;&#27867;&#23398;&#20064;&#31995;&#32479;&#65288;IF-BLS&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#23545;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#35757;&#32451;&#28857;&#20998;&#37197;&#27169;&#31946;&#38582;&#23646;&#24230;&#26469;&#20943;&#23567;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.08713</link><description>&lt;p&gt;
&#30452;&#35273;&#27169;&#31946;&#24191;&#27867;&#23398;&#20064;&#31995;&#32479;&#65306;&#22686;&#24378;&#23545;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intuitionistic Fuzzy Broad Learning System: Enhancing Robustness Against Noise and Outliers. (arXiv:2307.08713v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35273;&#27169;&#31946;&#24191;&#27867;&#23398;&#20064;&#31995;&#32479;&#65288;IF-BLS&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#23545;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#35757;&#32451;&#28857;&#20998;&#37197;&#27169;&#31946;&#38582;&#23646;&#24230;&#26469;&#20943;&#23567;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#20998;&#31867;&#39046;&#22495;&#65292;&#24191;&#27867;&#23398;&#20064;&#31995;&#32479;&#65288;BLS&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#23427;&#21033;&#29992;&#36880;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#21253;&#25324;&#29305;&#24449;&#23398;&#20064;&#21644;&#22686;&#24378;&#37096;&#20998;&#65292;&#20849;&#21516;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#25552;&#21462;&#22797;&#26434;&#30340;&#29305;&#24449;&#12290;&#20256;&#32479;&#30340;BLS&#23558;&#25152;&#26377;&#26679;&#26412;&#35270;&#20026;&#21516;&#31561;&#37325;&#35201;&#65292;&#36825;&#20351;&#24471;&#23427;&#22312;&#22788;&#29702;&#24102;&#26377;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#26102;&#19981;&#22815;&#31283;&#20581;&#21644;&#26377;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#31946;BLS&#65288;F-BLS&#65289;&#27169;&#22411;&#65292;&#23427;&#20026;&#27599;&#20010;&#35757;&#32451;&#28857;&#20998;&#37197;&#27169;&#31946;&#38582;&#23646;&#24230;&#65292;&#20197;&#20943;&#23567;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;&#22312;&#20998;&#37197;&#38582;&#23646;&#24230;&#30340;&#36807;&#31243;&#20013;&#65292;F-BLS&#27169;&#22411;&#20165;&#32771;&#34385;&#26679;&#26412;&#21040;&#31867;&#21035;&#20013;&#24515;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#32780;&#19981;&#32771;&#34385;&#19981;&#23646;&#20110;&#31867;&#21035;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#35273;&#27169;&#31946;&#29702;&#35770;&#30340;&#26032;&#22411;BLS&#65288;IF-BLS&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;IF-BLS&#21033;&#29992;&#22522;&#20110;&#27169;&#31946;&#38582;&#23646;&#24230;&#21644;&#38750;&#38582;&#23646;&#24230;&#30340;&#30452;&#35273;&#27169;&#31946;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of data classification, broad learning system (BLS) has proven to be a potent tool that utilizes a layer-by-layer feed-forward neural network. It consists of feature learning and enhancement segments, working together to extract intricate features from input data. The traditional BLS treats all samples as equally significant, which makes it less robust and less effective for real-world datasets with noises and outliers. To address this issue, we propose the fuzzy BLS (F-BLS) model, which assigns a fuzzy membership value to each training point to reduce the influence of noises and outliers. In assigning the membership value, the F-BLS model solely considers the distance from samples to the class center in the original feature space without incorporating the extent of non-belongingness to a class. We further propose a novel BLS based on intuitionistic fuzzy theory (IF-BLS). The proposed IF-BLS utilizes intuitionistic fuzzy numbers based on fuzzy membership and non-membership
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#35760;&#24518;&#36816;&#21160;&#30340;&#24515;&#29702;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#26088;&#22312;&#25512;&#21160;&#36825;&#20010;&#30475;&#20284;&#34987;&#20302;&#20272;&#30340;&#36816;&#21160;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.08712</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36935;&#19978;&#24515;&#29702;&#35757;&#32451; - &#24212;&#29992;&#20110;&#35760;&#24518;&#36816;&#21160;&#30340;&#27010;&#24565;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Meets Mental Training -- A Proof of Concept Applied to Memory Sports. (arXiv:2307.08712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#35760;&#24518;&#36816;&#21160;&#30340;&#24515;&#29702;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#26088;&#22312;&#25512;&#21160;&#36825;&#20010;&#30475;&#20284;&#34987;&#20302;&#20272;&#30340;&#36816;&#21160;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23637;&#31034;&#26426;&#22120;&#23398;&#20064;&#22312;&#35760;&#24518;&#33402;&#26415;&#30340;&#29305;&#27530;&#24418;&#24335;&#65288;&#34987;&#31216;&#20026;&#8220;&#35760;&#24518;&#36816;&#21160;&#8221;&#65289;&#30340;&#24515;&#29702;&#35757;&#32451;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#23558;&#36825;&#20004;&#20010;&#39046;&#22495;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#36825;&#31181;&#34701;&#21512;&#19968;&#26041;&#38754;&#26088;&#22312;&#25552;&#39640;&#23545;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#35748;&#35782;&#65292;&#21478;&#19968;&#26041;&#38754;&#20063;&#21147;&#22270;&#36890;&#36807;&#36825;&#19968;&#28151;&#21512;&#39046;&#22495;&#30340;&#30740;&#31350;&#26469;&#25512;&#21160;&#36825;&#20010;&#30475;&#20284;&#34987;&#20302;&#20272;&#30340;&#36816;&#21160;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to combine these two fields together by presenting a practical implementation of machine learning to the particular form of mental training that is the art of memory, taken in its competitive version called "Memory Sports". Such a fusion, on the one hand, strives to raise awareness about both realms, while on the other it seeks to encourage research in this mixed field as a way to, ultimately, drive forward the development of this seemingly underestimated sport.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37327;&#23376;&#22238;&#24402;&#30340;&#39640;&#25928;&#24378;&#22810;&#39033;&#24335;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#24369;&#22810;&#39033;&#24335;&#31639;&#27861;&#30340;&#31354;&#30333;&#12290;&#23545;&#20110;&#20108;&#32500;QR&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#30830;&#23450;&#24615;&#26368;&#22351;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^{4/3} polylog(n))&#21644;&#26399;&#26395;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^{4/3})&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08706</link><description>&lt;p&gt;
&#37327;&#23376;&#22238;&#24402;&#30340;&#39640;&#25928;&#24378;&#22810;&#39033;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Strongly Polynomial Algorithms for Quantile Regression. (arXiv:2307.08706v1 [cs.CG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37327;&#23376;&#22238;&#24402;&#30340;&#39640;&#25928;&#24378;&#22810;&#39033;&#24335;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#24369;&#22810;&#39033;&#24335;&#31639;&#27861;&#30340;&#31354;&#30333;&#12290;&#23545;&#20110;&#20108;&#32500;QR&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#30830;&#23450;&#24615;&#26368;&#22351;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^{4/3} polylog(n))&#21644;&#26399;&#26395;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^{4/3})&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#22238;&#24402;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#65292;&#20854;&#30446;&#26631;&#26159;&#24314;&#31435;&#21709;&#24212;&#21464;&#37327;&#65288;&#21363;&#20381;&#36182;&#21464;&#37327;&#65289;&#21644;&#19968;&#20010;&#25110;&#22810;&#20010;&#39044;&#27979;&#21464;&#37327;&#65288;&#21363;&#33258;&#21464;&#37327;&#65289;&#20043;&#38388;&#30340;&#32447;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#32463;&#20856;&#30340;&#20998;&#20301;&#25968;&#22238;&#24402;&#65288;QR&#65289;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#32479;&#35745;&#23398;&#20013;&#30456;&#23545;&#20110;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#65288;OLS&#65289;&#26356;&#40065;&#26834;&#30340;&#26367;&#20195;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#23384;&#22312;&#26377;&#25928;&#30340;OLS&#31639;&#27861;&#65292;&#20294;&#20960;&#20046;&#25152;&#26377;&#24050;&#30693;&#30340;QR&#32467;&#26524;&#37117;&#21482;&#26159;&#24369;&#22810;&#39033;&#24335;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#38024;&#23545;&#19981;&#21516;&#30340;&#35774;&#32622;&#25552;&#20986;&#20102;&#20960;&#31181;&#39640;&#25928;&#30340;&#24378;&#22810;&#39033;&#24335;QR&#31639;&#27861;&#12290;&#23545;&#20110;&#20108;&#32500;QR&#65292;&#36890;&#36807;&#19982;&#20960;&#20309;&#27010;&#24565;&#20013;&#30340;k-&#38598;&#21512;&#24314;&#31435;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#30830;&#23450;&#24615;&#26368;&#22351;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^{4/3} polylog(n))&#21644;&#26399;&#26395;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^{4/3})&#30340;&#31639;&#27861;&#65288;&#38543;&#26426;&#21270;&#29256;&#26412;&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#21010;&#20998;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Regression is a seminal technique in statistics and machine learning, where the objective is to build linear predictive models between a response (i.e., dependent) variable and one or more predictor (i.e., independent) variables. In this paper, we revisit the classical technique of Quantile Regression (QR), which is statistically a more robust alternative to the other classical technique of Ordinary Least Square Regression (OLS). However, while there exist efficient algorithms for OLS, almost all of the known results for QR are only weakly polynomial. Towards filling this gap, this paper proposes several efficient strongly polynomial algorithms for QR for various settings. For two dimensional QR, making a connection to the geometric concept of $k$-set, we propose an algorithm with a deterministic worst-case time complexity of $\mathcal{O}(n^{4/3} polylog(n))$ and an expected time complexity of $\mathcal{O}(n^{4/3})$ for the randomized version. We also propose a randomized divide
&lt;/p&gt;</description></item><item><title>CausalModels&#26159;&#19968;&#20010;&#29992;&#20110;&#21442;&#25968;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;R&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#25026;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#21333;&#29420;&#30340;&#36719;&#20214;&#21253;&#20013;&#20351;&#29992;&#22810;&#31181;&#32479;&#35745;&#26041;&#27861;&#26469;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.08686</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#21442;&#25968;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;R&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
An R package for parametric estimation of causal effects. (arXiv:2307.08686v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08686
&lt;/p&gt;
&lt;p&gt;
CausalModels&#26159;&#19968;&#20010;&#29992;&#20110;&#21442;&#25968;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;R&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#25026;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#21333;&#29420;&#30340;&#36719;&#20214;&#21253;&#20013;&#20351;&#29992;&#22810;&#31181;&#32479;&#35745;&#26041;&#27861;&#26469;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;R&#36719;&#20214;&#21253;CausalModels&#30340;&#29992;&#27861;&#65292;&#35813;&#36719;&#20214;&#21253;&#21487;&#20197;&#22312;Comprehensive R Archive Network&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;&#34429;&#28982;&#26377;&#19968;&#20123;&#36719;&#20214;&#21253;&#21487;&#20197;&#36275;&#22815;&#20934;&#30830;&#22320;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#20351;&#29992;Hernan&#21644;Robins&#65288;2020&#65289;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#24320;&#21457;&#30340;&#32467;&#26500;&#27169;&#22411;&#38598;&#21512;&#26469;&#25552;&#20379;&#12290;CausalModels&#36890;&#36807;&#25552;&#20379;&#26041;&#27861;&#24037;&#20855;&#26469;&#35299;&#20915;R&#20013;&#26377;&#20851;&#22240;&#26524;&#25512;&#26029;&#30340;&#36719;&#20214;&#19981;&#36275;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#32479;&#35745;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#32771;&#34385;&#35266;&#23519;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#23481;&#24573;&#35270;&#65292;&#21487;&#33021;&#22312;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#26102;&#26356;&#21512;&#36866;&#25110;&#26356;&#39640;&#25928;&#12290;&#34429;&#28982;&#36825;&#20123;&#32479;&#35745;&#27169;&#22411;&#30340;&#23454;&#29616;&#20998;&#24067;&#22312;&#35768;&#22810;&#22240;&#26524;&#36719;&#20214;&#21253;&#20013;&#65292;&#20294;CausalModels&#22312;&#19968;&#20010;&#21333;&#29420;&#30340;R&#36719;&#20214;&#21253;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#26131;&#25026;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32479;&#19968;&#24314;&#27169;&#27969;&#31243;&#65292;&#20197;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#21508;&#31181;&#32479;&#35745;&#26041;&#27861;&#12290;&#23427;&#21253;&#21547;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
This article explains the usage of R package CausalModels, which is publicly available on the Comprehensive R Archive Network. While packages are available for sufficiently estimating causal effects, there lacks a package that provides a collection of structural models using the conventional statistical approach developed by Hernan and Robins (2020). CausalModels addresses this deficiency of software in R concerning causal inference by offering tools for methods that account for biases in observational data without requiring extensive statistical knowledge. These methods should not be ignored and may be more appropriate or efficient in solving particular problems. While implementations of these statistical models are distributed among a number of causal packages, CausalModels introduces a simple and accessible framework for a consistent modeling pipeline among a variety of statistical methods for estimating causal effects in a single R package. It consists of common methods including s
&lt;/p&gt;</description></item><item><title>TableGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22806;&#37096;&#21151;&#33021;&#21629;&#20196;&#20351;LLMs&#33021;&#22815;&#26080;&#32541;&#22320;&#19982;&#34920;&#26684;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20415;&#21033;&#21644;&#21487;&#35775;&#38382;&#24615;&#32473;&#29992;&#25143;&#12290;&#20854;&#20013;&#30340;&#21019;&#26032;&#26159;&#20840;&#23616;&#34920;&#26684;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#20351;LLMs&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2307.08674</link><description>&lt;p&gt;
TableGPT&#65306;&#23558;&#34920;&#26684;&#65292;&#33258;&#28982;&#35821;&#35328;&#21644;&#21629;&#20196;&#32479;&#19968;&#21040;&#19968;&#20010;GPT&#20013;
&lt;/p&gt;
&lt;p&gt;
TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT. (arXiv:2307.08674v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08674
&lt;/p&gt;
&lt;p&gt;
TableGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22806;&#37096;&#21151;&#33021;&#21629;&#20196;&#20351;LLMs&#33021;&#22815;&#26080;&#32541;&#22320;&#19982;&#34920;&#26684;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20415;&#21033;&#21644;&#21487;&#35775;&#38382;&#24615;&#32473;&#29992;&#25143;&#12290;&#20854;&#20013;&#30340;&#21019;&#26032;&#26159;&#20840;&#23616;&#34920;&#26684;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#20351;LLMs&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24211;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#38656;&#35201;&#20154;&#20204;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#36827;&#34892;&#20998;&#26512;&#21644;&#25805;&#20316;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#20351;&#24471;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#19982;&#34920;&#26684;&#20132;&#20114;&#25104;&#20026;&#21487;&#33021;&#65292;&#20351;&#24471;&#36825;&#31181;&#33021;&#21147;&#26356;&#21152;&#25509;&#36817;&#29616;&#23454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TableGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#31934;&#35843;&#26694;&#26550;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#21033;&#29992;&#22806;&#37096;&#21151;&#33021;&#21629;&#20196;&#29702;&#35299;&#21644;&#25805;&#20316;&#34920;&#26684;&#12290;&#23427;&#24341;&#20837;&#20102;&#19982;&#34920;&#26684;&#26080;&#32541;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#22914;&#38382;&#31572;&#12289;&#25968;&#25454;&#25805;&#20316;&#65288;&#20363;&#22914;&#25554;&#20837;&#12289;&#21024;&#38500;&#12289;&#26597;&#35810;&#21644;&#20462;&#25913;&#25805;&#20316;&#65289;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#12289;&#20998;&#26512;&#25253;&#21578;&#29983;&#25104;&#21644;&#33258;&#21160;&#39044;&#27979;&#12290;TableGPT&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#20379;&#20415;&#21033;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;TableGPT&#30340;&#26680;&#24515;&#26159;&#20840;&#23616;&#34920;&#26684;&#34920;&#31034;&#30340;&#26032;&#27010;&#24565;&#65292;&#23427;&#20351;LLMs&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#65292;&#24182;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#21629;&#20196;&#25805;&#20316;&#23545;&#34920;&#26684;&#23454;&#29616;&#26080;&#32541;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the ent
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#23567;&#38169;&#35823;&#29109;&#20934;&#21017;&#22312;&#22788;&#29702;&#38750;&#39640;&#26031;&#22122;&#22768;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#23454;&#38469;&#36716;&#31227;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22522;&#26412;&#36716;&#31227;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#36890;&#36807;&#29992;&#26368;&#23567;&#38169;&#35823;&#29109;&#20195;&#26367;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#65292;&#21487;&#20197;&#21462;&#24471;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.08572</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26368;&#23567;&#38169;&#35823;&#29109;&#20934;&#21017;&#30340;&#40065;&#26834;&#24615;&#65306;&#36716;&#31227;&#23398;&#20064;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study. (arXiv:2307.08572v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#23567;&#38169;&#35823;&#29109;&#20934;&#21017;&#22312;&#22788;&#29702;&#38750;&#39640;&#26031;&#22122;&#22768;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#23454;&#38469;&#36716;&#31227;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22522;&#26412;&#36716;&#31227;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#36890;&#36807;&#29992;&#26368;&#23567;&#38169;&#35823;&#29109;&#20195;&#26367;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#65292;&#21487;&#20197;&#21462;&#24471;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#26159;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#20415;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#20551;&#35774;&#25968;&#25454;&#19981;&#21253;&#21547;&#22122;&#22768;&#65292;&#35201;&#20040;&#37319;&#29992;&#22797;&#26434;&#30340;&#35757;&#32451;&#33539;&#24335;&#25110;&#27169;&#22411;&#35774;&#35745;&#26469;&#22788;&#29702;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#32479;&#35745;&#20449;&#21495;&#22788;&#29702;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#23567;&#38169;&#35823;&#29109;&#65288;MEE&#65289;&#20934;&#21017;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#30740;&#31350;&#20854;&#22312;&#23454;&#38469;&#36716;&#31227;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#65292;&#20854;&#20013;&#20998;&#24067;&#36716;&#31227;&#26159;&#24120;&#35265;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;MEE&#23545;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#26367;&#25442;&#20026;MEE&#65292;&#22312;&#22522;&#26412;&#30340;&#36716;&#31227;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#24494;&#35843;&#21644;&#32447;&#24615;&#25506;&#27979;&#65289;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#19982;&#29616;&#26377;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coping with distributional shifts is an important part of transfer learning methods in order to perform well in real-life tasks. However, most of the existing approaches in this area either focus on an ideal scenario in which the data does not contain noises or employ a complicated training paradigm or model design to deal with distributional shifts. In this paper, we revisit the robustness of the minimum error entropy (MEE) criterion, a widely used objective in statistical signal processing to deal with non-Gaussian noises, and investigate its feasibility and usefulness in real-life transfer learning regression tasks, where distributional shifts are common. Specifically, we put forward a new theoretical result showing the robustness of MEE against covariate shift. We also show that by simply replacing the mean squared error (MSE) loss with the MEE on basic transfer learning algorithms such as fine-tuning and linear probing, we can achieve competitive performance with respect to state-
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21453;&#23556;&#33108;&#20013;&#30340;&#22810;&#27425;&#25955;&#23556;&#36890;&#36807;&#34987;&#21160;&#35825;&#23548;&#20809;&#23398;&#38750;&#32447;&#24615;&#26144;&#23556;&#30340;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#20809;&#23398;&#25968;&#25454;&#21387;&#32553;&#21644;&#39640;&#25928;&#22788;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.08558</link><description>&lt;p&gt;
&#20855;&#26377;&#34987;&#21160;&#20809;&#23398;&#38750;&#32447;&#24615;&#26144;&#23556;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning with Passive Optical Nonlinear Mapping. (arXiv:2307.08558v2 [physics.optics] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08558
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21453;&#23556;&#33108;&#20013;&#30340;&#22810;&#27425;&#25955;&#23556;&#36890;&#36807;&#34987;&#21160;&#35825;&#23548;&#20809;&#23398;&#38750;&#32447;&#24615;&#26144;&#23556;&#30340;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#20809;&#23398;&#25968;&#25454;&#21387;&#32553;&#21644;&#39640;&#25928;&#22788;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#20294;&#26159;&#26085;&#30410;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#19987;&#38376;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#20809;&#23398;&#21152;&#36895;&#22120;&#21487;&#20197;&#25552;&#20379;&#22686;&#24378;&#30340;&#24615;&#33021;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#20809;&#23398;&#20013;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#65292;&#36825;&#26159;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#35774;&#35745;&#65292;&#21033;&#29992;&#19968;&#20010;&#21453;&#23556;&#33108;&#20013;&#30340;&#22810;&#27425;&#25955;&#23556;&#26469;&#34987;&#21160;&#35825;&#23548;&#20809;&#23398;&#38750;&#32447;&#24615;&#38543;&#26426;&#26144;&#23556;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#28608;&#20809;&#21151;&#29575;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#19968;&#20010;&#37325;&#35201;&#20248;&#21183;&#26159;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#21453;&#23556;&#33108;&#20013;&#30340;&#22810;&#27425;&#25955;&#23556;&#26469;&#36827;&#34892;&#20809;&#23398;&#25968;&#25454;&#21387;&#32553;&#65292;&#20197;&#39640;&#25928;&#22320;&#21387;&#32553;&#21644;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#65292;&#21516;&#26102;&#38477;&#20302;&#25968;&#25454;&#30340;&#32500;&#24230;&#12290;&#36825;&#20351;&#24471;&#24555;&#36895;&#30340;&#20809;&#23398;&#20449;&#24687;&#22788;&#29702;&#21644;&#29983;&#25104;&#39640;&#24230;&#38750;&#32447;&#24615;&#29305;&#24449;&#30340;&#20302;&#32500;&#28151;&#21512;&#25104;&#20998;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#23545;&#20110;&#38656;&#35201;&#39640;&#25928;&#22788;&#29702;&#30340;&#24212;&#29992;&#29305;&#21035;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has fundamentally transformed artificial intelligence, but the ever-increasing complexity in deep learning models calls for specialized hardware accelerators. Optical accelerators can potentially offer enhanced performance, scalability, and energy efficiency. However, achieving nonlinear mapping, a critical component of neural networks, remains challenging optically. Here, we introduce a design that leverages multiple scattering in a reverberating cavity to passively induce optical nonlinear random mapping, without the need for additional laser power. A key advantage emerging from our work is that we show we can perform optical data compression, facilitated by multiple scattering in the cavity, to efficiently compress and retain vital information while also decreasing data dimensionality. This allows rapid optical information processing and generation of low dimensional mixtures of highly nonlinear features. These are particularly useful for applications demanding high-sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31867;&#21035;&#28857;&#20113;&#34917;&#20840;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;cine&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#37325;&#24314;&#22810;&#31867;&#21035;&#30340;3D&#24515;&#33039;&#35299;&#21078;&#32467;&#26500;&#12290;&#35813;&#32593;&#32476;&#33021;&#22815;&#35299;&#20915;&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#31232;&#30095;&#21644;&#38169;&#20301;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08535</link><description>&lt;p&gt;
&#22810;&#31867;&#21035;&#28857;&#20113;&#34917;&#20840;&#32593;&#32476;&#29992;&#20110;&#20174;cine&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#37325;&#24314;3D&#24515;&#33039;&#35299;&#21078;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images. (arXiv:2307.08535v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31867;&#21035;&#28857;&#20113;&#34917;&#20840;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;cine&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#37325;&#24314;&#22810;&#31867;&#21035;&#30340;3D&#24515;&#33039;&#35299;&#21078;&#32467;&#26500;&#12290;&#35813;&#32593;&#32476;&#33021;&#22815;&#35299;&#20915;&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#31232;&#30095;&#21644;&#38169;&#20301;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cine&#30913;&#20849;&#25391;&#25104;&#20687;&#26159;&#35780;&#20272;&#24515;&#33039;&#35299;&#21078;&#21644;&#21151;&#33021;&#30340;&#24403;&#21069;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#23427;&#36890;&#24120;&#21482;&#33719;&#21462;&#19968;&#32452;&#20108;&#32500;&#20999;&#29255;&#65292;&#38480;&#21046;&#20102;&#23545;&#20581;&#24247;&#21644;&#30149;&#29702;&#24515;&#33039;&#24418;&#24577;&#21644;&#29983;&#29702;&#30340;&#29702;&#35299;&#21644;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23436;&#20840;&#33258;&#21160;&#34920;&#38754;&#37325;&#24314;&#27969;&#31243;&#65292;&#33021;&#22815;&#20174;&#21407;&#22987;cine&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#37325;&#24314;&#22810;&#31867;&#21035;&#30340;3D&#24515;&#33039;&#35299;&#21078;&#32593;&#26684;&#12290;&#20854;&#20851;&#38190;&#32452;&#20214;&#26159;&#19968;&#20010;&#22810;&#31867;&#21035;&#28857;&#20113;&#34917;&#20840;&#32593;&#32476;(PCCN)&#65292;&#33021;&#22815;&#22312;&#32479;&#19968;&#27169;&#22411;&#20013;&#32416;&#27491;3D&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#31232;&#30095;&#21644;&#38169;&#20301;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#22823;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;PCCN&#65292;&#35266;&#23519;&#21040;&#37325;&#24314;&#21644;&#40644;&#37329;&#26631;&#20934;&#35299;&#21078;&#20043;&#38388;&#30340;Chamfer&#36317;&#31163;&#23567;&#20110;&#25110;&#31561;&#20110;&#22522;&#30784;&#22270;&#20687;&#20998;&#36776;&#29575;&#30340;&#22810;&#20010;&#23618;&#27425;&#30340;&#20999;&#29255;&#38169;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cine magnetic resonance imaging (MRI) is the current gold standard for the assessment of cardiac anatomy and function. However, it typically only acquires a set of two-dimensional (2D) slices of the underlying three-dimensional (3D) anatomy of the heart, thus limiting the understanding and analysis of both healthy and pathological cardiac morphology and physiology. In this paper, we propose a novel fully automatic surface reconstruction pipeline capable of reconstructing multi-class 3D cardiac anatomy meshes from raw cine MRI acquisitions. Its key component is a multi-class point cloud completion network (PCCN) capable of correcting both the sparsity and misalignment issues of the 3D reconstruction task in a unified model. We first evaluate the PCCN on a large synthetic dataset of biventricular anatomies and observe Chamfer distances between reconstructed and gold standard anatomies below or similar to the underlying image resolution for multiple levels of slice misalignment. Furthermo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27425;&#25955;&#23556;&#23454;&#29616;&#22810;&#23618;&#20809;&#23398;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#20302;&#20809;&#21151;&#29575;&#21516;&#26102;&#21512;&#25104;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#23454;&#29616;&#33021;&#37327;&#39640;&#25928;&#21644;&#39640;&#36895;&#30340;&#20809;&#23398;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2307.08533</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#22788;&#29702;&#19982;&#32447;&#24615;&#20809;&#23398;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Processing with Linear Optics. (arXiv:2307.08533v2 [physics.optics] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27425;&#25955;&#23556;&#23454;&#29616;&#22810;&#23618;&#20809;&#23398;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#20302;&#20809;&#21151;&#29575;&#21516;&#26102;&#21512;&#25104;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#23454;&#29616;&#33021;&#37327;&#39640;&#25928;&#21644;&#39640;&#36895;&#30340;&#20809;&#23398;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#25968;&#25454;&#22788;&#29702;&#26469;&#25552;&#21462;&#38544;&#34255;&#30340;&#34920;&#24449;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#31361;&#30772;&#65292;&#20294;&#21364;&#20197;&#22823;&#30005;&#23376;&#35745;&#31639;&#33021;&#21147;&#20026;&#20195;&#20215;&#12290;&#20026;&#20102;&#25552;&#39640;&#33021;&#37327;&#25928;&#29575;&#21644;&#36895;&#24230;&#65292;&#20809;&#23398;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#20809;&#23398;&#24102;&#23485;&#30340;&#20248;&#21183;&#21644;&#20809;&#23398;&#20114;&#36830;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;&#22312;&#32570;&#20047;&#20302;&#21151;&#29575;&#20809;&#23398;&#38750;&#32447;&#24615;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#23454;&#29616;&#22810;&#23618;&#20809;&#23398;&#32593;&#32476;&#20013;&#30340;&#25361;&#25112;&#22312;&#20110;&#23454;&#29616;&#22810;&#20010;&#20809;&#23398;&#23618;&#65292;&#32780;&#19981;&#20381;&#36182;&#30005;&#23376;&#20803;&#20214;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27425;&#25955;&#23556;&#21487;&#20197;&#21516;&#26102;&#20197;&#20302;&#20809;&#21151;&#29575;&#21512;&#25104;&#21487;&#32534;&#31243;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#21033;&#29992;&#25955;&#23556;&#21183;&#33021;&#65288;&#30001;&#25968;&#25454;&#34920;&#31034;&#65289;&#19982;&#25955;&#23556;&#22330;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#22810;&#27425;&#25955;&#23556;&#36827;&#34892;&#25968;&#25454;&#37325;&#22797;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#20809;&#23398;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have achieved remarkable breakthroughs by leveraging multiple layers of data processing to extract hidden representations, albeit at the cost of large electronic computing power. To enhance energy efficiency and speed, the optical implementation of neural networks aims to harness the advantages of optical bandwidth and the energy efficiency of optical interconnections. In the absence of low-power optical nonlinearities, the challenge in the implementation of multilayer optical networks lies in realizing multiple optical layers without resorting to electronic components. In this study, we present a novel framework that uses multiple scattering that is capable of synthesizing programmable linear and nonlinear transformations concurrently at low optical power by leveraging the nonlinear relationship between the scattering potential, represented by data, and the scattered field. Theoretical and experimental investigations show that repeating the data by multiple scatte
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27668;&#20307;&#32477;&#32536;&#39640;&#21387;&#30452;&#27969;&#31995;&#32479;&#20013;&#26222;&#36941;&#20998;&#31867;&#36229;&#39640;&#39057;&#23616;&#37096;&#25918;&#30005;&#20449;&#21495;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#30340;PD&#28304;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#25805;&#20316;&#30005;&#21387;&#20493;&#25968;&#12290;&#21516;&#26102;&#65292;&#27604;&#36739;&#20102;&#26102;&#22495;&#21644;&#39057;&#22495;&#36755;&#20837;&#20449;&#21495;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#24402;&#19968;&#21270;&#26041;&#26696;&#23545;&#20943;&#36731;&#33258;&#30001;&#31354;&#38388;&#24433;&#21709;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.08466</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#27668;&#20307;&#32477;&#32536;&#39640;&#21387;&#30452;&#27969;&#31995;&#32479;&#20013;&#23545;&#36229;&#39640;&#39057;&#23616;&#37096;&#25918;&#30005;&#20449;&#21495;&#36827;&#34892;&#26222;&#36866;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Generalizable Classification of UHF Partial Discharge Signals in Gas-Insulated HVDC Systems Using Neural Networks. (arXiv:2307.08466v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27668;&#20307;&#32477;&#32536;&#39640;&#21387;&#30452;&#27969;&#31995;&#32479;&#20013;&#26222;&#36941;&#20998;&#31867;&#36229;&#39640;&#39057;&#23616;&#37096;&#25918;&#30005;&#20449;&#21495;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#30340;PD&#28304;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#25805;&#20316;&#30005;&#21387;&#20493;&#25968;&#12290;&#21516;&#26102;&#65292;&#27604;&#36739;&#20102;&#26102;&#22495;&#21644;&#39057;&#22495;&#36755;&#20837;&#20449;&#21495;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#24402;&#19968;&#21270;&#26041;&#26696;&#23545;&#20943;&#36731;&#33258;&#30001;&#31354;&#38388;&#24433;&#21709;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#34987;&#21457;&#29616;&#30340;&#23616;&#37096;&#25918;&#30005;(PD)&#26159;&#39640;&#21387;(HV)&#27668;&#20307;&#32477;&#32536;&#31995;&#32479;(GIS)&#20013;&#30340;&#19968;&#20010;&#23433;&#20840;&#20851;&#38190;&#38382;&#39064;&#12290;&#34429;&#28982;&#22312;&#20132;&#27969;&#30005;&#21387;&#19979;&#23545;PD&#30340;&#35786;&#26029;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#22788;&#29702;&#65292;&#20294;&#22312;&#30452;&#27969;&#30005;&#21387;&#19979;&#30340;PD&#20998;&#26512;&#20173;&#28982;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36825;&#20123;&#30740;&#31350;&#30340;&#37325;&#28857;&#20043;&#19968;&#26159;&#23558;&#19981;&#21516;&#30340;PD&#28304;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#20415;&#36827;&#34892;&#21518;&#32493;&#30340;&#22797;&#26434;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;HVDC GIS&#19978;&#32477;&#32536;&#20307;&#19978;&#30340;&#37329;&#23646;&#31361;&#36215;&#21644;&#23548;&#30005;&#39063;&#31890;&#24341;&#36215;&#30340;PD&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#33033;&#20914;&#24207;&#21015;&#20998;&#26512;&#29305;&#24449;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#21306;&#20998;&#36127;&#30005;&#20301;&#21644;&#27491;&#30005;&#20301;&#19979;&#33719;&#24471;&#30340;&#30740;&#31350;PD&#20449;&#21495;&#65292;&#24182;&#19988;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#25805;&#20316;&#30005;&#21387;&#20493;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#26102;&#22495;&#21644;&#39057;&#22495;&#36755;&#20837;&#20449;&#21495;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#24402;&#19968;&#21270;&#26041;&#26696;&#23545;&#20943;&#36731;&#33258;&#30001;&#31354;&#38388;&#24433;&#21709;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Undetected partial discharges (PDs) are a safety critical issue in high voltage (HV) gas insulated systems (GIS). While the diagnosis of PDs under AC voltage is well-established, the analysis of PDs under DC voltage remains an active research field. A key focus of these investigations is the classification of different PD sources to enable subsequent sophisticated analysis.  In this paper, we propose and analyze a neural network-based approach for classifying PD signals caused by metallic protrusions and conductive particles on the insulator of HVDC GIS, without relying on pulse sequence analysis features. In contrast to previous approaches, our proposed model can discriminate the studied PD signals obtained at negative and positive potentials, while also generalizing to unseen operating voltage multiples. Additionally, we compare the performance of time- and frequency-domain input signals and explore the impact of different normalization schemes to mitigate the influence of free-space
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#20855;&#26377;&#20302;&#24310;&#36831;&#30340;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20986;&#27969;&#24335;&#20302;&#24310;&#36831;&#30340;&#36817;&#20284;&#38543;&#26426;&#28216;&#36208;&#29305;&#24449;&#65292;&#35745;&#31639;&#26102;&#38388;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#20197;&#24635;&#32467;&#22810;&#36339;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.08433</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#28216;&#36208;&#21040;&#22270;&#24418;&#24555;&#36305;&#65306;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#20855;&#26377;&#20302;&#24310;&#36831;&#30340;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs. (arXiv:2307.08433v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08433
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#20855;&#26377;&#20302;&#24310;&#36831;&#30340;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20986;&#27969;&#24335;&#20302;&#24310;&#36831;&#30340;&#36817;&#20284;&#38543;&#26426;&#28216;&#36208;&#29305;&#24449;&#65292;&#35745;&#31639;&#26102;&#38388;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#20197;&#24635;&#32467;&#22810;&#36339;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#22522;&#30784;&#30340;&#21160;&#24577;&#22270;&#32467;&#26500;&#65292;&#20854;&#20013;&#23454;&#20307;&#21644;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#38543;&#26102;&#38388;&#28436;&#21464;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#32771;&#34385;&#36825;&#20123;&#21160;&#24577;&#22240;&#32032;&#65292;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20805;&#20998;&#21457;&#25381;&#20854;&#28508;&#21147;&#12290;&#20197;&#21069;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#35201;&#20040;&#20391;&#37325;&#20110;&#25277;&#26679;k-&#36339;&#37051;&#22495;&#65292;&#31867;&#20284;&#20110;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65292;&#35201;&#20040;&#20391;&#37325;&#20110;&#38543;&#26426;&#28216;&#36208;&#65292;&#31867;&#20284;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#26102;&#21160;&#24577;&#22270;&#19978;&#36827;&#34892;&#20302;&#24310;&#36831;&#25512;&#26029;&#26159;&#35745;&#31639;&#19978;&#26114;&#36149;&#19988;&#19981;&#36866;&#29992;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#24418;&#24555;&#36305;&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#65288;CTDGs&#65289;&#30340;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#20855;&#26377;&#20302;&#24310;&#36831;&#65292;&#24182;&#19988;&#19982;&#39640;&#24310;&#36831;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;&#12289;&#20302;&#24310;&#36831;&#30340;&#36817;&#20284;&#38543;&#26426;&#28216;&#36208;&#29305;&#24449;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20351;&#29992;&#20165;&#21333;&#36339;&#25805;&#20316;&#35745;&#31639;&#24635;&#32467;&#22810;&#36339;&#20449;&#24687;&#30340;&#26102;&#38388;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks. Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop ope
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#20998;&#35010;&#23398;&#20064;&#23545;&#25239;&#25932;&#23545;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#19981;&#21463;&#20449;&#20219;&#26381;&#21153;&#22120;&#21482;&#33021;&#35775;&#38382;&#27169;&#22411;&#20013;&#38388;&#23618;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#20165;&#21521;&#26381;&#21153;&#22120;&#20844;&#24320;&#37096;&#20998;&#27169;&#22411;&#65292;&#20998;&#35010;&#23398;&#20064;&#21487;&#20197;&#32531;&#35299;&#25932;&#23545;&#25915;&#20987;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2307.07916</link><description>&lt;p&gt;
&#35770;&#20998;&#35010;&#23398;&#20064;&#23545;&#25239;&#25932;&#23545;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Split Learning against Adversarial Attacks. (arXiv:2307.07916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07916
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#20998;&#35010;&#23398;&#20064;&#23545;&#25239;&#25932;&#23545;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#19981;&#21463;&#20449;&#20219;&#26381;&#21153;&#22120;&#21482;&#33021;&#35775;&#38382;&#27169;&#22411;&#20013;&#38388;&#23618;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#20165;&#21521;&#26381;&#21153;&#22120;&#20844;&#24320;&#37096;&#20998;&#27169;&#22411;&#65292;&#20998;&#35010;&#23398;&#20064;&#21487;&#20197;&#32531;&#35299;&#25932;&#23545;&#25915;&#20987;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#35010;&#23398;&#20064;&#36890;&#36807;&#36991;&#20813;&#30452;&#25509;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#21644;&#27169;&#22411;&#32454;&#33410;&#65288;&#21363;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20165;&#25345;&#26377;&#37096;&#20998;&#23376;&#32593;&#32476;&#24182;&#20132;&#25442;&#20013;&#38388;&#35745;&#31639;&#65289;&#26469;&#23454;&#29616;&#21327;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#27169;&#22411;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26816;&#39564;&#20854;&#23545;&#38544;&#31169;&#20445;&#25252;&#30340;&#21487;&#38752;&#24615;&#65292;&#23545;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#30740;&#31350;&#29978;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#25506;&#32034;&#23436;&#25972;&#27169;&#22411;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21457;&#36215;&#25932;&#23545;&#25915;&#20987;&#65292;&#32780;&#20998;&#35010;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#20165;&#21521;&#19981;&#21463;&#20449;&#20219;&#30340;&#26381;&#21153;&#22120;&#20844;&#24320;&#37096;&#20998;&#27169;&#22411;&#26469;&#32531;&#35299;&#36825;&#31181;&#20005;&#37325;&#23041;&#32961;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#20998;&#35010;&#23398;&#20064;&#23545;&#25239;&#25932;&#23545;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21463;&#20449;&#20219;&#30340;&#26381;&#21153;&#22120;&#21482;&#33021;&#35775;&#38382;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#30340;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#12290;&#29616;&#26377;&#30340;&#25932;&#23545;&#25915;&#20987;&#22823;&#22810;&#38598;&#20013;&#22312;&#38598;&#20013;&#24335;&#29615;&#22659;&#32780;&#38750;&#21327;&#21516;&#24335;&#29615;&#22659;&#65292;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#20998;&#35010;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split learning enables collaborative deep learning model training while preserving data privacy and model security by avoiding direct sharing of raw data and model details (i.e., sever and clients only hold partial sub-networks and exchange intermediate computations). However, existing research has mainly focused on examining its reliability for privacy protection, with little investigation into model security. Specifically, by exploring full models, attackers can launch adversarial attacks, and split learning can mitigate this severe threat by only disclosing part of models to untrusted servers.This paper aims to evaluate the robustness of split learning against adversarial attacks, particularly in the most challenging setting where untrusted servers only have access to the intermediate layers of the model.Existing adversarial attacks mostly focus on the centralized setting instead of the collaborative setting, thus, to better evaluate the robustness of split learning, we develop a ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;GFlowNets&#20013;&#20351;&#29992;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#26377;&#25928;&#24615;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#22238;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#25216;&#26415;&#23545;&#27169;&#24335;&#21457;&#29616;&#36895;&#24230;&#21644;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.07674</link><description>&lt;p&gt;
&#22312;GFlowNets&#20013;&#20351;&#29992;&#22238;&#25918;&#32531;&#20914;&#21306;&#23545;&#27169;&#24335;&#21457;&#29616;&#30340;&#26377;&#25928;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of the Effectiveness of Using a Replay Buffer on Mode Discovery in GFlowNets. (arXiv:2307.07674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;GFlowNets&#20013;&#20351;&#29992;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#26377;&#25928;&#24615;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#22238;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#25216;&#26415;&#23545;&#27169;&#24335;&#21457;&#29616;&#36895;&#24230;&#21644;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#37319;&#26679;&#21160;&#20316;&#20174;&#32780;&#23398;&#20064;&#22914;&#20309;&#26368;&#22823;&#21270;&#24635;&#26399;&#26395;&#22238;&#25253;$R&#65288;x&#65289;$&#26469;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;GFlowNets&#26159;&#19968;&#31867;&#29305;&#27530;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#36817;&#20284;&#20110;$R&#65288;x&#65289;$&#30340;&#27010;&#29575;&#37319;&#26679;&#31574;&#30053;&#65292;&#20174;&#31163;&#25955;&#38598;&#21512;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#20505;&#36873;&#26679;&#26412;$x$&#12290;&#19982;&#20256;&#32479;&#30340;RL&#31639;&#27861;&#30456;&#27604;&#65292;GFlowNets&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27169;&#24335;&#21457;&#29616;&#33021;&#21147;&#65292;&#23545;&#20110;&#33647;&#29289;&#21457;&#29616;&#21644;&#32452;&#21512;&#25628;&#32034;&#31561;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;GFlowNets&#26159;&#19968;&#20010;&#30456;&#23545;&#36739;&#26032;&#30340;&#31639;&#27861;&#31867;&#21035;&#65292;&#35768;&#22810;&#22312;RL&#20013;&#26377;&#29992;&#30340;&#25216;&#26415;&#23578;&#26410;&#19982;&#20854;&#20851;&#32852;&#36215;&#26469;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;GFlowNets&#20013;&#21033;&#29992;&#22238;&#25918;&#32531;&#20914;&#21306;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#21508;&#31181;&#22238;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#25216;&#26415;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#20102;&#27169;&#24335;&#21457;&#29616;&#36895;&#24230;&#21644;&#21457;&#29616;&#30340;&#27169;&#24335;&#36136;&#37327;&#12290;&#22312;Hypergrid&#27169;&#25311;&#29615;&#22659;&#21644;&#20998;&#23376;&#21512;&#25104;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) algorithms aim to learn an optimal policy by iteratively sampling actions to learn how to maximize the total expected return, $R(x)$. GFlowNets are a special class of algorithms designed to generate diverse candidates, $x$, from a discrete set, by learning a policy that approximates the proportional sampling of $R(x)$. GFlowNets exhibit improved mode discovery compared to conventional RL algorithms, which is very useful for applications such as drug discovery and combinatorial search. However, since GFlowNets are a relatively recent class of algorithms, many techniques which are useful in RL have not yet been associated with them. In this paper, we study the utilization of a replay buffer for GFlowNets. We explore empirically various replay buffer sampling techniques and assess the impact on the speed of mode discovery and the quality of the modes discovered. Our experimental results in the Hypergrid toy domain and a molecule synthesis environment demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#21521;&#26368;&#20248;&#21270;&#65288;IO&#65289;&#23398;&#20064;&#36335;&#30001;&#38382;&#39064;&#20915;&#31574;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20122;&#39532;&#36874;&#26411;&#31471;&#36335;&#30001;&#30740;&#31350;&#25361;&#25112;&#20013;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#22312;&#22797;&#21046;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#36335;&#30001;&#20559;&#22909;&#26041;&#38754;&#21462;&#24471;&#20102;&#31532;2&#21517;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2307.07357</link><description>&lt;p&gt;
&#21453;&#21521;&#26368;&#20248;&#21270;&#29992;&#20110;&#36335;&#30001;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Inverse Optimization for Routing Problems. (arXiv:2307.07357v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#21521;&#26368;&#20248;&#21270;&#65288;IO&#65289;&#23398;&#20064;&#36335;&#30001;&#38382;&#39064;&#20915;&#31574;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20122;&#39532;&#36874;&#26411;&#31471;&#36335;&#30001;&#30740;&#31350;&#25361;&#25112;&#20013;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#22312;&#22797;&#21046;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#36335;&#30001;&#20559;&#22909;&#26041;&#38754;&#21462;&#24471;&#20102;&#31532;2&#21517;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#21521;&#26368;&#20248;&#21270;&#65288;IO&#65289;&#23398;&#20064;&#36335;&#30001;&#38382;&#39064;&#20915;&#31574;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;IO&#26694;&#26550;&#23646;&#20110;&#30417;&#30563;&#23398;&#20064;&#31867;&#21035;&#65292;&#24182;&#24314;&#31435;&#22312;&#30446;&#26631;&#34892;&#20026;&#26159;&#26410;&#30693;&#25104;&#26412;&#20989;&#25968;&#30340;&#20248;&#21270;&#22120;&#30340;&#21069;&#25552;&#19979;&#12290;&#36825;&#20010;&#25104;&#26412;&#20989;&#25968;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#36335;&#30001;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#21487;&#20197;&#29702;&#35299;&#20026;&#20915;&#31574;&#32773;&#30340;&#36335;&#30001;&#20559;&#22909;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36335;&#30001;&#38382;&#39064;&#30340;IO&#26041;&#27861;&#65292;&#21253;&#25324;&#20551;&#35774;&#20989;&#25968;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#22522;&#20110;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20122;&#39532;&#36874;&#26411;&#31471;&#36335;&#30001;&#30740;&#31350;&#25361;&#25112;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;IO&#26041;&#27861;&#65292;&#35813;&#25361;&#25112;&#30340;&#30446;&#26631;&#26159;&#20351;&#29992;&#25104;&#21315;&#19978;&#19975;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36335;&#30001;&#26696;&#20363;&#23398;&#20064;&#27169;&#22411;&#20197;&#22797;&#21046;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#36335;&#30001;&#20559;&#22909;&#12290;&#25105;&#20204;&#26368;&#32456;&#23398;&#20064;&#21040;&#30340;IO&#36335;&#30001;&#27169;&#22411;&#22312;48&#20010;&#26187;&#32423;&#21040;&#20915;&#36187;&#30340;&#27169;&#22411;&#20013;&#25490;&#21517;&#31532;2&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method for learning decision-makers' behavior in routing problems using Inverse Optimization (IO). The IO framework falls into the supervised learning category and builds on the premise that the target behavior is an optimizer of an unknown cost function. This cost function is to be learned through historical data, and in the context of routing problems, can be interpreted as the routing preferences of the decision-makers. In this view, the main contributions of this study are to propose an IO methodology with a hypothesis function, loss function, and stochastic first-order algorithm tailored to routing problems. We further test our IO approach in the Amazon Last Mile Routing Research Challenge, where the goal is to learn models that replicate the routing preferences of human drivers, using thousands of real-world routing examples. Our final IO-learned routing model achieves a score that ranks 2nd compared with the 48 models that qualified for the final round of the challe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#37327;&#21270;&#21644;&#32531;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#25932;&#23545;&#36755;&#20837;&#26102;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07250</link><description>&lt;p&gt;
&#36890;&#36807;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#30340;&#22240;&#26524;&#21442;&#25968;&#20272;&#35745;&#26469;&#32531;&#35299;&#25932;&#23545;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (arXiv:2307.07250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07250
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#37327;&#21270;&#21644;&#32531;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#25932;&#23545;&#36755;&#20837;&#26102;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#35270;&#35273;&#36755;&#20837;&#20013;&#34893;&#29983;&#20986;&#30340;&#25932;&#23545;&#20363;&#23376;&#21487;&#20197;&#36731;&#26494;&#22320;&#25439;&#23475;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#38450;&#27490;&#28508;&#22312;&#30340;&#23041;&#32961;&#65292;&#21508;&#31181;&#22522;&#20110;&#25932;&#23545;&#35757;&#32451;&#30340;&#38450;&#24481;&#26041;&#27861;&#36805;&#36895;&#22686;&#38271;&#65292;&#24182;&#25104;&#20026;&#31283;&#20581;&#24615;&#30340;&#20107;&#23454;&#19978;&#26631;&#20934;&#26041;&#27861;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#25104;&#23601;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25932;&#23545;&#33030;&#24369;&#24615;&#22312;&#19981;&#21516;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#26576;&#20123;&#33030;&#24369;&#24615;&#20173;&#28982;&#26222;&#36941;&#23384;&#22312;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21363;&#20351;&#20351;&#29992;&#26356;&#28145;&#23618;&#27425;&#30340;&#26550;&#26500;&#21644;&#20808;&#36827;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36825;&#31181;&#22855;&#29305;&#30340;&#29616;&#35937;&#20173;&#28982;&#26080;&#27861;&#32531;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;ADML&#65289;&#30340;&#22240;&#26524;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#37327;&#21270;&#32593;&#32476;&#39044;&#27979;&#30340;&#25932;&#23545;&#33030;&#24369;&#24615;&#31243;&#24230;&#65292;&#24182;&#25429;&#25417;&#23545;&#32467;&#26524;&#30340;&#22788;&#29702;&#25928;&#26524;&#12290;ADML&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#25932;&#23545;&#25200;&#21160;&#26412;&#36523;&#30340;&#22240;&#26524;&#21442;&#25968;&#65292;&#24182;&#20943;&#36731;&#21487;&#33021;&#25439;&#23475;&#31283;&#20581;&#24615;&#30340;&#36127;&#38754;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples derived from deliberately crafted perturbations on visual inputs can easily harm decision process of deep neural networks. To prevent potential threats, various adversarial training-based defense methods have grown rapidly and become a de facto standard approach for robustness. Despite recent competitive achievements, we observe that adversarial vulnerability varies across targets and certain vulnerabilities remain prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with deeper architectures and advanced defense methods. To address this issue, in this paper, we introduce a causal approach called Adversarial Double Machine Learning (ADML), which allows us to quantify the degree of adversarial vulnerability for network predictions and capture the effect of treatments on outcome of interests. ADML can directly estimate causal parameter of adversarial perturbations per se and mitigate negative effects that can potentially damage robustness, bridgi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#21152;&#26435;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#26696;&#65292;&#24182;&#24314;&#31435;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#25552;&#20379;&#20102;&#28176;&#36817;&#26377;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20248;&#30340;&#32479;&#35745;&#36895;&#24230;&#21644;&#26377;&#21033;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06915</link><description>&lt;p&gt;
&#21152;&#26435;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;: &#28176;&#36817;&#27491;&#24577;&#24615;&#21644;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Weighted Averaged Stochastic Gradient Descent: Asymptotic Normality and Optimality. (arXiv:2307.06915v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#21152;&#26435;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#26696;&#65292;&#24182;&#24314;&#31435;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#25552;&#20379;&#20102;&#28176;&#36817;&#26377;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20248;&#30340;&#32479;&#35745;&#36895;&#24230;&#21644;&#26377;&#21033;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#29616;&#20195;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#31616;&#21333;&#21644;&#26368;&#27969;&#34892;&#30340;&#31639;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#32780;&#21463;&#21040;&#38738;&#30544;&#12290;&#22312;&#19981;&#21516;&#30340;&#24773;&#22659;&#19979;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#24179;&#22343;&#26041;&#26696;&#26469;&#21152;&#36895;SGD&#30340;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#29992;&#20110;SGD&#30340;&#36890;&#29992;&#24179;&#22343;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31867;&#21152;&#26435;&#24179;&#22343;SGD&#35299;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#28176;&#36817;&#26377;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#26041;&#26696;&#65292;&#23637;&#29616;&#20986;&#26368;&#20248;&#30340;&#32479;&#35745;&#36895;&#24230;&#21644;&#26377;&#21033;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#20511;&#37492;&#20102;&#32447;&#24615;&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#26368;&#20248;&#26435;&#37325;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent (SGD) is one of the simplest and most popular algorithms in modern statistical and machine learning due to its computational and memory efficiency. Various averaging schemes have been proposed to accelerate the convergence of SGD in different settings. In this paper, we explore a general averaging scheme for SGD. Specifically, we establish the asymptotic normality of a broad range of weighted averaged SGD solutions and provide asymptotically valid online inference approaches. Furthermore, we propose an adaptive averaging scheme that exhibits both optimal statistical rate and favorable non-asymptotic convergence, drawing insights from the optimal weight for the linear model in terms of non-asymptotic mean squared error (MSE).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06887</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks. (arXiv:2307.06887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06887
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23398;&#20064;&#26159;&#31070;&#32463;&#32593;&#32476;&#23454;&#38469;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#28982;&#32780;&#22914;&#20309;&#20197;&#21450;&#20026;&#20309;&#21457;&#29983;&#29305;&#24449;&#23398;&#20064;&#20173;&#28982;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20248;&#21270;&#30340;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#21487;&#20197;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#25193;&#23637;&#20102;&#25105;&#20204;&#23545;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#25110;&#38543;&#26426;&#29305;&#24449;&#33539;&#20363;&#20013;&#24494;&#19981;&#36275;&#36947;&#30340;&#29305;&#24449;&#23398;&#20064;&#30340;&#20102;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#32463;&#24120;&#22320;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#20123;&#20808;&#21069;&#30340;&#20998;&#26512;&#24182;&#19981;&#36866;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#21508;&#31181;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#31616;&#21333;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#26368;&#24120;&#35265;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#26410;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature learning, i.e. extracting meaningful representations of data, is quintessential to the practical success of neural networks trained with gradient descent, yet it is notoriously difficult to explain how and why it occurs. Recent theoretical studies have shown that shallow neural networks optimized on a single task with gradient-based methods can learn meaningful features, extending our understanding beyond the neural tangent kernel or random feature regime in which negligible feature learning occurs. But in practice, neural networks are increasingly often trained on {\em many} tasks simultaneously with differing loss functions, and these prior analyses do not generalize to such settings. In the multi-task learning setting, a variety of studies have shown effective feature learning by simple linear models. However, multi-task learning via {\em nonlinear} models, arguably the most common learning paradigm in practice, remains largely mysterious. In this work, we present the first 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#35757;&#32451;&#26356;&#29615;&#20445;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25214;&#20986;&#20102;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.05520</link><description>&lt;p&gt;
DL&#27169;&#22411;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#33021;&#28304;&#28040;&#32791;&#26377;&#24433;&#21709;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do DL models and training environments have an impact on energy consumption?. (arXiv:2307.05520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#35757;&#32451;&#26356;&#29615;&#20445;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25214;&#20986;&#20102;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#27491;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#24615;&#33021;&#19978;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#20851;&#20110;&#35757;&#32451;DL&#27169;&#22411;&#24102;&#26469;&#24040;&#22823;&#30899;&#36275;&#36857;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#35757;&#32451;&#26356;&#29615;&#20445;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30446;&#26631;&#20998;&#20026;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#27169;&#22411;&#26550;&#26500;&#23545;&#23454;&#29616;&#26356;&#29615;&#20445;&#27169;&#22411;&#21516;&#26102;&#20445;&#25345;&#27491;&#30830;&#24615;&#22312;&#26368;&#20339;&#27700;&#24179;&#30340;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#35757;&#32451;&#29615;&#22659;&#23545;&#29983;&#25104;&#26356;&#29615;&#20445;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20123;&#20851;&#31995;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#25910;&#38598;&#20102;&#19982;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#30456;&#20851;&#30340;&#22810;&#20010;&#25351;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#27169;&#22411;&#26550;&#26500;&#22312;&#27979;&#37327;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#26041;&#38754;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#35757;&#32451;&#29615;&#22659;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#39564;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#36825;&#39033;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current research in the computer vision field mainly focuses on improving Deep Learning (DL) correctness and inference time performance. However, there is still little work on the huge carbon footprint that has training DL models. This study aims to analyze the impact of the model architecture and training environment when training greener computer vision models. We divide this goal into two research questions. First, we analyze the effects of model architecture on achieving greener models while keeping correctness at optimal levels. Second, we study the influence of the training environment on producing greener models. To investigate these relationships, we collect multiple metrics related to energy efficiency and model correctness during the models' training. Then, we outline the trade-offs between the measured energy efficiency and the models' correctness regarding model architecture, and their relationship with the training environment. We conduct this research in the context of a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22870;&#21169;&#27169;&#22411;&#12289;PPO&#21644;&#36827;&#31243;&#30417;&#30563;&#31561;&#25216;&#26415;&#36335;&#24452;&#65292;&#25506;&#32034;&#22914;&#20309;&#35299;&#20915;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04964</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494; &#31532;&#19968;&#37096;&#20998;&#65306;PPO
&lt;/p&gt;
&lt;p&gt;
Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22870;&#21169;&#27169;&#22411;&#12289;PPO&#21644;&#36827;&#31243;&#30417;&#30563;&#31561;&#25216;&#26415;&#36335;&#24452;&#65292;&#25506;&#32034;&#22914;&#20309;&#35299;&#20915;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#25512;&#21160;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#34013;&#22270;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#25104;&#20026;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#65288;&#26377;&#30410;&#12289;&#35802;&#23454;&#21644;&#26080;&#23475;&#65289;&#21161;&#25163;&#12290;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#24847;&#20041;&#65292;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#25104;&#20026;&#25903;&#25745;&#36825;&#19968;&#36861;&#27714;&#30340;&#20851;&#38190;&#25216;&#26415;&#33539;&#24335;&#12290;&#24403;&#21069;&#30340;&#25216;&#26415;&#36335;&#32447;&#36890;&#24120;&#21253;&#25324;&#29992;&#20110;&#34913;&#37327;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#12289;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#27169;&#22411;&#36755;&#20986;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#20197;&#21450;&#29992;&#20110;&#25913;&#21892;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#36827;&#31243;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22870;&#21169;&#35774;&#35745;&#12289;&#29615;&#22659;&#20132;&#20114;&#21644;&#20195;&#29702;&#35757;&#32451;&#30340;&#25361;&#25112;&#65292;&#20877;&#21152;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35797;&#39564;&#25104;&#26412;&#24040;&#22823;&#65292;&#23545;&#20110;AI&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#28608;&#21169;&#25216;&#26415;&#23545;&#40784;&#21644;LLMs&#30340;&#23433;&#20840;&#30528;&#38470;&#23384;&#22312;&#37325;&#22823;&#38556;&#30861;&#12290;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbf{reward models} to measure human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20998;&#26512;&#30340;&#25112;&#30053;&#35745;&#21010;&#65292;&#36890;&#36807;&#30740;&#31350;&#37329;&#34701;&#20844;&#21496;LendingClub&#30340;&#23454;&#38469;&#26696;&#20363;&#65292;&#25506;&#32034;&#24341;&#20837;&#22823;&#25968;&#25454;&#24179;&#21488;&#21644;&#20808;&#36827;&#29305;&#24449;&#36873;&#25321;&#33021;&#21147;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#22686;&#21152;&#25910;&#20837;&#24182;&#38477;&#20302;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.04778</link><description>&lt;p&gt;
&#22522;&#20110;&#32479;&#35745;&#20998;&#26512;&#21644;&#24212;&#29992;&#31243;&#24207;&#22312;&#37329;&#34701;&#20844;&#21496;&#20013;&#21046;&#23450;&#25112;&#30053;&#35745;&#21010;&#30340;&#19968;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Formulating A Strategic Plan Based On Statistical Analyses And Applications For Financial Companies Through A Real-World Use Case. (arXiv:2307.04778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20998;&#26512;&#30340;&#25112;&#30053;&#35745;&#21010;&#65292;&#36890;&#36807;&#30740;&#31350;&#37329;&#34701;&#20844;&#21496;LendingClub&#30340;&#23454;&#38469;&#26696;&#20363;&#65292;&#25506;&#32034;&#24341;&#20837;&#22823;&#25968;&#25454;&#24179;&#21488;&#21644;&#20808;&#36827;&#29305;&#24449;&#36873;&#25321;&#33021;&#21147;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#22686;&#21152;&#25910;&#20837;&#24182;&#38477;&#20302;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#32479;&#35745;&#22312;&#20225;&#19994;&#32423;&#21035;&#23454;&#26045;&#25968;&#25454;&#39537;&#21160;&#30340;&#25112;&#30053;&#35745;&#21010;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20197;&#36816;&#29992;&#21508;&#31181;&#20998;&#26512;&#25163;&#27573;&#65292;&#36825;&#26679;&#30340;&#35745;&#21010;&#30340;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#20225;&#19994;&#25913;&#36827;&#20915;&#31574;&#36807;&#31243;&#25110;&#38477;&#20302;&#32452;&#32455;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20998;&#26512;&#30340;&#25112;&#30053;&#35745;&#21010;&#65292;&#38024;&#23545;&#19968;&#23478;&#37329;&#34701;&#20844;&#21496;LendingClub&#65292;&#35745;&#21010;&#21253;&#25324;&#25506;&#32034;&#24341;&#20837;&#22823;&#25968;&#25454;&#24179;&#21488;&#21644;&#20808;&#36827;&#30340;&#29305;&#24449;&#36873;&#25321;&#33021;&#21147;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#20010;&#35745;&#21010;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22686;&#21152;&#20844;&#21496;&#30340;&#25910;&#20837;&#65292;&#21516;&#26102;&#20943;&#23569;&#21521;&#26080;&#27861;&#24402;&#36824;&#36151;&#27454;&#30340;&#20511;&#27454;&#20154;&#25480;&#20104;&#36151;&#27454;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#23545;&#20844;&#21496;&#25285;&#24551;&#36827;&#34892;&#20102;&#19981;&#21516;&#20551;&#35774;&#30340;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;&#36151;&#27454;&#37329;&#39069;&#23545;&#26080;&#27861;&#24402;&#36824;&#36151;&#27454;&#30340;&#20511;&#27454;&#20154;&#20154;&#25968;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#25552;&#20986;&#30340;&#25112;&#30053;&#35745;&#21010;&#36824;&#21253;&#25324;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#31561;&#20808;&#36827;&#20998;&#26512;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business statistics play a crucial role in implementing a data-driven strategic plan at the enterprise level to employ various analytics where the outcomes of such a plan enable an enterprise to enhance the decision-making process or to mitigate risks to the organization. In this work, a strategic plan informed by the statistical analysis is introduced for a financial company called LendingClub, where the plan is comprised of exploring the possibility of onboarding a big data platform along with advanced feature selection capacities. The main objectives of such a plan are to increase the company's revenue while reducing the risks of granting loans to borrowers who cannot return their loans. In this study, different hypotheses formulated to address the company's concerns are studied, where the results reveal that the amount of loans profoundly impacts the number of borrowers charging off their loans. Also, the proposed strategic plan includes onboarding advanced analytics such as machin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#23398;&#20064;&#26799;&#24230;&#25163;&#26415;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;&#26799;&#24230;&#26469;&#28040;&#38500;&#25968;&#25454;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.04550</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#23398;&#20064;&#26799;&#24230;&#25163;&#26415;&#22312;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Gradient Surgery for One-shot Unlearning on Generative Model. (arXiv:2307.04550v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#23398;&#20064;&#26799;&#24230;&#25163;&#26415;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;&#26799;&#24230;&#26469;&#28040;&#38500;&#25968;&#25454;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#8220;&#36951;&#24536;&#26435;&#8221;&#30340;&#30417;&#31649;&#24341;&#21457;&#20102;&#23545;&#21462;&#28040;&#39044;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20852;&#36259;&#12290;&#36817;&#26399;&#30340;&#26426;&#22120;&#23398;&#20064;&#21462;&#28040;&#26041;&#27861;&#36890;&#36807;&#26356;&#26032;&#26435;&#37325;&#26469;&#28040;&#38500;&#26679;&#26412;&#23545;&#26435;&#37325;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#20197;&#36924;&#36817;&#19968;&#31181;&#30452;&#25509;&#20294;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#28040;&#38500;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#21463;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#25805;&#20316;&#26469;&#35843;&#25972;&#26679;&#26412;&#20043;&#38388;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20445;&#30041;&#26799;&#24230;&#30340;&#27861;&#24179;&#38754;&#19978;&#36827;&#34892;&#35268;&#33539;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#31227;&#38500;&#26679;&#26412;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#24182;&#39318;&#27425;&#22312;&#21462;&#28040;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent regulation on right-to-be-forgotten emerges tons of interest in unlearning pre-trained machine learning models. While approximating a straightforward yet expensive approach of retrain-from-scratch, recent machine unlearning methods unlearn a sample by updating weights to remove its influence on the weight parameters. In this paper, we introduce a simple yet effective approach to remove a data influence on the deep generative model. Inspired by works in multi-task learning, we propose to manipulate gradients to regularize the interplay of influence among samples by projecting gradients onto the normal plane of the gradients to be retained. Our work is agnostic to statistics of the removal samples, outperforming existing baselines while providing theoretical analysis for the first time in unlearning a generative model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#20351;&#29992;&#35760;&#24518;&#30340;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#36807;&#21435;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#32531;&#35299;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.00497</link><description>&lt;p&gt;
&#19981;&#35201;&#32972;&#35829;&#65292;&#27169;&#20223;&#36807;&#21435;&#65306;&#26080;&#38656;&#20351;&#29992;&#35760;&#24518;&#30340;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Don't Memorize; Mimic The Past: Federated Class Incremental Learning Without Episodic Memory. (arXiv:2307.00497v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#20351;&#29992;&#35760;&#24518;&#30340;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#36807;&#21435;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#32531;&#35299;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#26032;&#25968;&#25454;&#26102;&#23481;&#26131;&#24536;&#35760;&#36807;&#21435;&#23398;&#20064;&#30340;&#20449;&#24687;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#65292;&#22240;&#20026;&#25968;&#25454;&#26159;&#20998;&#25955;&#30340;&#65292;&#27599;&#20010;&#29992;&#25143;&#37117;&#20250;&#29420;&#31435;&#22320;&#36827;&#34892;&#26356;&#25913;&#12290;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#20027;&#35201;&#22312;&#20013;&#24515;&#21270;&#30340;&#29615;&#22659;&#20013;&#30740;&#31350;&#36825;&#31181;&#25152;&#35859;&#30340;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#29616;&#35937;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#21487;&#20197;&#30452;&#25509;&#35775;&#38382;&#23436;&#25972;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#23558;CL&#25216;&#26415;&#24212;&#29992;&#20110;FL&#24182;&#19981;&#30452;&#25509;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#38544;&#31169;&#38382;&#39064;&#21644;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20174;&#36807;&#21435;&#30340;&#20998;&#24067;&#20013;&#21512;&#25104;&#26679;&#26412;&#65292;&#32780;&#19981;&#26159;&#23384;&#20648;&#37096;&#20998;&#36807;&#21435;&#30340;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#23458;&#25143;&#31471;&#21487;&#20197;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#22312;&#26412;&#22320;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#22312;&#27599;&#20010;&#20219;&#21153;&#32467;&#26463;&#26102;&#20351;&#29992;&#26080;&#25968;&#25454;&#26041;&#27861;&#22312;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#35831;&#27714;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models are prone to forgetting information learned in the past when trained on new data. This problem becomes even more pronounced in the context of federated learning (FL), where data is decentralized and subject to independent changes for each user. Continual Learning (CL) studies this so-called \textit{catastrophic forgetting} phenomenon primarily in centralized settings, where the learner has direct access to the complete training dataset. However, applying CL techniques to FL is not straightforward due to privacy concerns and resource limitations. This paper presents a framework for federated class incremental learning that utilizes a generative model to synthesize samples from past distributions instead of storing part of past data. Then, clients can leverage the generative model to mitigate catastrophic forgetting locally. The generative model is trained on the server using data-free methods at the end of each task without requesting data from clients. Therefore, i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WireMask-BBO&#30340;&#40657;&#30418;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#25513;&#27169;&#24341;&#23548;&#30340;&#36138;&#24515;&#36807;&#31243;&#36827;&#34892;&#23439;&#21333;&#20803;&#24067;&#23616;&#65292;&#22312;&#26377;&#25928;&#38477;&#20302;HPWL&#30340;&#21516;&#26102;&#33410;&#30465;&#22823;&#37327;&#26102;&#38388;&#12290;&#27492;&#26041;&#27861;&#36824;&#21487;&#20197;&#23545;&#29616;&#26377;&#24067;&#23616;&#36827;&#34892;&#24494;&#35843;&#65292;&#25913;&#21892;50%&#30340;HPWL&#12290;</title><link>http://arxiv.org/abs/2306.16844</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#25513;&#27169;&#24341;&#23548;&#30340;&#40657;&#30418;&#20248;&#21270;&#23454;&#29616;&#23439;&#21333;&#20803;&#24067;&#23616;
&lt;/p&gt;
&lt;p&gt;
Macro Placement by Wire-Mask-Guided Black-Box Optimization. (arXiv:2306.16844v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WireMask-BBO&#30340;&#40657;&#30418;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#25513;&#27169;&#24341;&#23548;&#30340;&#36138;&#24515;&#36807;&#31243;&#36827;&#34892;&#23439;&#21333;&#20803;&#24067;&#23616;&#65292;&#22312;&#26377;&#25928;&#38477;&#20302;HPWL&#30340;&#21516;&#26102;&#33410;&#30465;&#22823;&#37327;&#26102;&#38388;&#12290;&#27492;&#26041;&#27861;&#36824;&#21487;&#20197;&#23545;&#29616;&#26377;&#24067;&#23616;&#36827;&#34892;&#24494;&#35843;&#65292;&#25913;&#21892;50%&#30340;HPWL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#22823;&#35268;&#27169;&#38598;&#25104;&#65288;VLSI&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#33455;&#29255;&#24067;&#23616;&#20013;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#25216;&#26415;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#12290;&#23439;&#21333;&#20803;&#24067;&#23616;&#20316;&#20026;&#35813;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#23376;&#38382;&#39064;&#65292;&#35797;&#22270;&#30830;&#23450;&#25152;&#26377;&#23439;&#21333;&#20803;&#30340;&#20301;&#32622;&#65292;&#20197;&#26368;&#23567;&#21270;&#21322;&#21608;&#38271;&#32447;&#38271;&#65288;HPWL&#65289;&#24182;&#36991;&#20813;&#37325;&#21472;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#21253;&#25324;&#22522;&#20110;&#25171;&#21253;&#12289;&#20998;&#26512;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40657;&#30418;&#20248;&#21270;&#65288;BBO&#65289;&#26694;&#26550;&#65288;&#31216;&#20026;WireMask-BBO&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#25513;&#27169;&#24341;&#23548;&#30340;&#36138;&#24515;&#36807;&#31243;&#36827;&#34892;&#30446;&#26631;&#35780;&#20272;&#26469;&#36827;&#34892;&#23439;&#21333;&#20803;&#24067;&#23616;&#12290;&#37197;&#22791;&#19981;&#21516;&#30340;BBO&#31639;&#27861;&#65292;WireMask-BBO&#22312;&#23454;&#36341;&#20013;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#26102;&#38388;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#30701;&#30340;HPWL&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#23558;&#20854;&#35270;&#20026;&#21021;&#22987;&#35299;&#26469;&#24494;&#35843;&#29616;&#26377;&#30340;&#24067;&#23616;&#65292;&#20174;&#32780;&#20351;HPWL&#25913;&#21892;&#22810;&#36798;50%&#12290;WireMask-BBO&#20855;&#26377;&#24341;&#39046;&#33455;&#29255;&#24067;&#23616;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of very large-scale integration (VLSI) technology has posed new challenges for electronic design automation (EDA) techniques in chip floorplanning. During this process, macro placement is an important subproblem, which tries to determine the positions of all macros with the aim of minimizing half-perimeter wirelength (HPWL) and avoiding overlapping. Previous methods include packing-based, analytical and reinforcement learning methods. In this paper, we propose a new black-box optimization (BBO) framework (called WireMask-BBO) for macro placement, by using a wire-mask-guided greedy procedure for objective evaluation. Equipped with different BBO algorithms, WireMask-BBO empirically achieves significant improvements over previous methods, i.e., achieves significantly shorter HPWL by using much less time. Furthermore, it can fine-tune existing placements by treating them as initial solutions, which can bring up to 50% improvement in HPWL. WireMask-BBO has the potential to s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#21453;&#20107;&#23454;&#25512;&#29702;&#65292;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#21644;&#29983;&#25104;&#24314;&#27169;&#30340;&#24605;&#24819;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#28145;&#24230;&#22240;&#26524;&#26426;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15764</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#22240;&#26524;&#27169;&#22411;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
High Fidelity Image Counterfactuals with Probabilistic Causal Models. (arXiv:2306.15764v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15764
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#21453;&#20107;&#23454;&#25512;&#29702;&#65292;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#21644;&#29983;&#25104;&#24314;&#27169;&#30340;&#24605;&#24819;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#28145;&#24230;&#22240;&#26524;&#26426;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#20855;&#26377;&#28145;&#24230;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;&#23545;&#20110;&#39640;&#32500;&#32467;&#26500;&#21464;&#37327;&#65288;&#22914;&#22270;&#20687;&#65289;&#30340;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#26597;&#35810;&#30340;&#20272;&#35745;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#30340;&#24605;&#24819;&#21644;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#23637;&#65292;&#20026;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#21464;&#37327;&#35774;&#35745;&#20102;&#26032;&#30340;&#28145;&#24230;&#22240;&#26524;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26426;&#21046;&#33021;&#22815;&#20934;&#30830;&#22320;&#25512;&#26029;&#21644;&#20272;&#35745;&#30452;&#25509;&#12289;&#38388;&#25509;&#21644;&#24635;&#25928;&#24212;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#21453;&#20107;&#23454;&#30340;&#20844;&#29702;&#20005;&#23494;&#24615;&#26469;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general causal generative modelling framework for accurate estimation of high fidelity image counterfactuals with deep structural causal models. Estimation of interventional and counterfactual queries for high-dimensional structured variables, such as images, remains a challenging task. We leverage ideas from causal mediation analysis and advances in generative modelling to design new deep causal mechanisms for structured variables in causal models. Our experiments demonstrate that our proposed mechanisms are capable of accurate abduction and estimation of direct, indirect and total effects as measured by axiomatic soundness of counterfactuals.
&lt;/p&gt;</description></item><item><title>SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15656</link><description>&lt;p&gt;
SparseOptimizer: &#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#26469;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#32534;&#35793;&#22120;&#20849;&#21516;&#35774;&#35745;&#26469;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15656
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SparseOptimizer&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65292;ALBERT&#21644;GPT&#65289;&#20013;&#33258;&#28982;&#22320;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;SparseOptimizer&#35774;&#35745;&#30340;&#20851;&#38190;&#26159;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30452;&#25509;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#36825;&#20010;&#25805;&#20316;&#31526;&#36890;&#36807;&#22362;&#23454;&#30340;&#29702;&#35770;&#26694;&#26550;&#25903;&#25345;&#65292;&#24182;&#21253;&#21547;&#20102;&#19968;&#20010;&#20998;&#26512;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20248;&#21270;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;SparseOptimizer&#30340;&#21363;&#25554;&#21363;&#29992;&#21151;&#33021;&#28040;&#38500;&#20102;&#23545;&#20195;&#30721;&#20462;&#25913;&#30340;&#38656;&#27714;&#65292;&#20351;&#20854;&#25104;&#20026;&#36866;&#29992;&#20110;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#36866;&#24212;&#24037;&#20855;&#12290;&#22312;GLUE&#12289;RACE&#12289;SQuAD1&#21644;SQuAD2&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;SparseOptimizer&#31232;&#30095;&#21270;&#21518;&#30340;SparseBERT&#21644;SparseALBERT&#22312;&#24615;&#33021;&#19978;&#19982;&#23494;&#38598;&#22411;&#30340;BERT&#21644;ALBERT&#30456;&#24403;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#26694;&#26550;&#30340;&#36229;&#22768;&#23450;&#20301;&#26174;&#24494;&#38236;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20381;&#36182;&#21040;&#36798;&#26102;&#38388;&#24046;&#20449;&#24687;&#23454;&#29616;&#24494;&#27668;&#27873;&#30340;&#23450;&#20301;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15548</link><description>&lt;p&gt;
&#20960;&#20309;&#36229;&#22768;&#23450;&#20301;&#26174;&#24494;&#38236;
&lt;/p&gt;
&lt;p&gt;
Geometric Ultrasound Localization Microscopy. (arXiv:2306.15548v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#26694;&#26550;&#30340;&#36229;&#22768;&#23450;&#20301;&#26174;&#24494;&#38236;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20381;&#36182;&#21040;&#36798;&#26102;&#38388;&#24046;&#20449;&#24687;&#23454;&#29616;&#24494;&#27668;&#27873;&#30340;&#23450;&#20301;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#22686;&#24378;&#36229;&#22768;&#65288;CEUS&#65289;&#24050;&#25104;&#20026;&#26080;&#21019;&#21160;&#24577;&#21487;&#35270;&#21270;&#21307;&#23398;&#35786;&#26029;&#26041;&#27861;&#65292;&#28982;&#32780;&#36229;&#22768;&#23450;&#20301;&#26174;&#24494;&#38236;&#65288;ULM&#65289;&#36890;&#36807;&#25552;&#20379;&#21313;&#20493;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#23454;&#29616;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#24310;&#36831;&#21644;&#27714;&#21644;&#65288;DAS&#65289;&#27874;&#26463;&#24418;&#25104;&#22120;&#34987;&#29992;&#20110;&#28210;&#26579;ULM&#24103;&#65292;&#26368;&#32456;&#30830;&#23450;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#33021;&#21147;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;ULM&#65292;&#26412;&#30740;&#31350;&#36136;&#30097;&#27874;&#26463;&#24418;&#25104;&#26159;&#21542;&#26159;ULM&#26368;&#26377;&#25928;&#30340;&#22788;&#29702;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;&#21040;&#36798;&#26102;&#38388;&#24046;&#65288;TDoA&#65289;&#20449;&#24687;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#26925;&#22278;&#20132;&#28857;&#23450;&#20301;&#24494;&#27668;&#27873;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#20197;&#20811;&#26381;&#29616;&#26377;&#27874;&#26463;&#24418;&#25104;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#20960;&#20309;ULM&#22312;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#20165;&#21033;&#29992;&#20102;&#37096;&#20998;&#21487;&#29992;&#30340;&#25442;&#33021;&#22120;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound Localization Microscopy (ULM) has enabled a revolutionary breakthrough by offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers are used to render ULM frames, ultimately determining the image resolution capability. To take full advantage of ULM, this study questions whether beamforming is the most effective processing step for ULM, suggesting an alternative approach that relies solely on Time-Difference-of-Arrival (TDoA) information. To this end, a novel geometric framework for micro bubble localization via ellipse intersections is proposed to overcome existing beamforming limitations. We present a benchmark comparison based on a public dataset for which our geometric ULM outperforms existing baseline methods in terms of accuracy and reliability while only utilizing a portion of the available transducer data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#30340;&#22810;&#30446;&#26631;&#26550;&#26500;&#65292;&#31216;&#20026;MOMA-DDPG&#65292;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#30899;&#20943;&#25490;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20004;&#31181;&#31867;&#22411;&#30340;&#26234;&#33021;&#20307;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20248;&#21270;&#27599;&#20010;&#36335;&#21475;&#30340;&#26412;&#22320;&#20132;&#36890;&#65292;&#32780;&#21478;&#19968;&#20010;&#26088;&#22312;&#20248;&#21270;&#20840;&#23616;&#20132;&#36890;&#21534;&#21520;&#37327;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#31561;&#24453;&#26102;&#38388;&#21644;&#30899;&#25490;&#25918;&#37327;&#20004;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09662</link><description>&lt;p&gt;
&#21512;&#20316;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#30899;&#20943;&#25490;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction. (arXiv:2306.09662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#30340;&#22810;&#30446;&#26631;&#26550;&#26500;&#65292;&#31216;&#20026;MOMA-DDPG&#65292;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#30899;&#20943;&#25490;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20004;&#31181;&#31867;&#22411;&#30340;&#26234;&#33021;&#20307;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20248;&#21270;&#27599;&#20010;&#36335;&#21475;&#30340;&#26412;&#22320;&#20132;&#36890;&#65292;&#32780;&#21478;&#19968;&#20010;&#26088;&#22312;&#20248;&#21270;&#20840;&#23616;&#20132;&#36890;&#21534;&#21520;&#37327;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#31561;&#24453;&#26102;&#38388;&#21644;&#30899;&#25490;&#25918;&#37327;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#20381;&#36182;&#20110;&#36807;&#20110;&#31616;&#21270;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#29978;&#33267;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#20063;&#32463;&#24120;&#26159;&#27425;&#20248;&#30340;&#21644;&#19981;&#31283;&#23450;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#20316;&#30340;&#22810;&#30446;&#26631;&#26550;&#26500;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;MOMA-DDPG&#65289;&#65292;&#20351;&#29992;&#34928;&#20943;&#26435;&#37325;&#26469;&#20272;&#35745;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20248;&#21270;&#30340;&#22810;&#20010;&#22870;&#21169;&#39033;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20004;&#31181;&#31867;&#22411;&#30340;&#26234;&#33021;&#20307;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20248;&#21270;&#27599;&#20010;&#36335;&#21475;&#30340;&#26412;&#22320;&#20132;&#36890;&#65292;&#32780;&#21478;&#19968;&#20010;&#26088;&#22312;&#20248;&#21270;&#20840;&#23616;&#20132;&#36890;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#19968;&#20010;&#20122;&#27954;&#22269;&#23478;&#30340;&#20132;&#36890;&#25668;&#20687;&#22836;&#25910;&#38598;&#21040;&#30340;&#30495;&#23454;&#19990;&#30028;&#20132;&#36890;&#25968;&#25454;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#21253;&#21547;&#20102;&#19968;&#20010;&#20840;&#23616;&#26234;&#33021;&#20307;&#65292;&#20294;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#26159;&#20998;&#25955;&#30340;&#65292;&#22240;&#20026;&#36825;&#20010;&#26234;&#33021;&#20307;&#22312;&#25512;&#29702;&#38454;&#27573;&#19981;&#20877;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;MOMA-DDPG&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#25152;&#26377;&#24615;&#33021;&#25351;&#26631;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#26368;&#23567;&#21270;&#20102;&#31561;&#24453;&#26102;&#38388;&#21644;&#30899;&#25490;&#25918;&#37327;&#20004;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing traffic signal control systems rely on oversimplified rule-based methods, and even RL-based methods are often suboptimal and unstable. To address this, we propose a cooperative multi-objective architecture called Multi-Objective Multi-Agent Deep Deterministic Policy Gradient (MOMA-DDPG), which estimates multiple reward terms for traffic signal control optimization using age-decaying weights. Our approach involves two types of agents: one focuses on optimizing local traffic at each intersection, while the other aims to optimize global traffic throughput. We evaluate our method using real-world traffic data collected from an Asian country's traffic cameras. Despite the inclusion of a global agent, our solution remains decentralized as this agent is no longer necessary during the inference stage. Our results demonstrate the effectiveness of MOMA-DDPG, outperforming state-of-the-art methods across all performance metrics. Additionally, our proposed system minimizes both waiting ti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07528</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65306;&#24378;&#21270;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective. (arXiv:2306.07528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#26088;&#22312;&#36890;&#36807;&#24050;&#37096;&#32626;&#30340;&#35760;&#24405;&#31574;&#30053;&#25910;&#38598;&#30340;&#25968;&#25454;&#20248;&#21270;&#25490;&#21517;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#32463;&#24120;&#23545;&#29992;&#25143;&#22914;&#20309;&#29983;&#25104;&#28857;&#20987;&#25968;&#25454;&#21363;&#28857;&#20987;&#27169;&#22411;&#36827;&#34892;&#20551;&#35774;&#65292;&#22240;&#27492;&#38656;&#35201;&#26681;&#25454;&#19981;&#21516;&#30340;&#28857;&#20987;&#27169;&#22411;&#19987;&#38376;&#35843;&#25972;&#20182;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25490;&#21517;&#36807;&#31243;&#22312;&#19968;&#33324;&#38543;&#26426;&#28857;&#20987;&#27169;&#22411;&#19979;&#32479;&#19968;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#31163;&#32447;RL&#25216;&#26415;&#36827;&#34892;&#38750;&#21516;&#31574;&#30053;LTR&#65292;&#24182;&#25552;&#20986;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MDP&#30340;&#19987;&#38376;&#21046;&#23450;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;RL&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#21435;&#20559;&#20506;&#25216;&#26415;&#21644;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy Learning to Rank (LTR) aims to optimize a ranker from data collected by a deployed logging policy. However, existing off-policy learning to rank methods often make strong assumptions about how users generate the click data, i.e., the click model, and hence need to tailor their methods specifically under different click models. In this paper, we unified the ranking process under general stochastic click models as a Markov Decision Process (MDP), and the optimal ranking could be learned with offline reinforcement learning (RL) directly. Building upon this, we leverage offline RL techniques for off-policy LTR and propose the Click Model-Agnostic Unified Off-policy Learning to Rank (CUOLR) method, which could be easily applied to a wide range of click models. Through a dedicated formulation of the MDP, we show that offline RL algorithms can adapt to various click models without complex debiasing techniques and prior knowledge of the model. Results on various large-scale datasets
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;Lipschitz&#27491;&#21017;&#21270;Transformer (LRFormer)&#26469;&#20943;&#36731;Transformer&#22312;&#39044;&#27979;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#22522;&#20934;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06849</link><description>&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#26469;&#20943;&#36731;Transformer&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Transformer Overconfidence via Lipschitz Regularization. (arXiv:2306.06849v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06849
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;Lipschitz&#27491;&#21017;&#21270;Transformer (LRFormer)&#26469;&#20943;&#36731;Transformer&#22312;&#39044;&#27979;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#22522;&#20934;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#22312;&#39044;&#27979;&#20013;&#36807;&#20110;&#33258;&#20449;&#65292;&#22240;&#20026;&#26631;&#20934;&#30340;&#28857;&#31215;&#33258;&#27880;&#24847;&#21147;(DPSA)&#22312;&#26080;&#30028;&#36755;&#20837;&#22495;&#20013;&#20960;&#20046;&#26080;&#27861;&#20445;&#25345;&#36317;&#31163;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;Lipschitz&#27491;&#21017;&#21270;Transformer(LRFormer)&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#23427;&#22312;Banach&#31354;&#38388;&#20869;&#20445;&#35777;&#20102;Lipschitz&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#25910;&#32553;&#30340;Lipschitz&#36793;&#30028;&#26469;&#27491;&#21017;&#21270;&#35813;&#39033;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32463;&#36807;&#29702;&#35770;&#20445;&#35777;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#20026;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#25552;&#20379;&#20102;&#20005;&#35880;&#30340;&#22522;&#30784;&#12290;&#22312;&#26631;&#20934;&#30340;&#35270;&#35273;&#22522;&#20934;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#12289;&#26657;&#20934;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though Transformers have achieved promising results in many computer vision tasks, they tend to be over-confident in predictions, as the standard Dot Product Self-Attention (DPSA) can barely preserve distance for the unbounded input domain. In this work, we fill this gap by proposing a novel Lipschitz Regularized Transformer (LRFormer). Specifically, we present a new similarity function with the distance within Banach Space to ensure the Lipschitzness and also regularize the term by a contractive Lipschitz Bound. The proposed method is analyzed with a theoretical guarantee, providing a rigorous basis for its effectiveness and reliability. Extensive experiments conducted on standard vision benchmarks demonstrate that our method outperforms the state-of-the-art single forward pass approaches in prediction, calibration, and uncertainty estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27491;&#21322;&#23450;&#30697;&#38453;&#30340;&#33258;&#19968;&#33268;&#24615;&#32858;&#31867;&#31639;&#27861;&#65288;K-&#24352;&#37327;&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#29305;&#24449;&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2306.06534</link><description>&lt;p&gt;
K-Tensors&#65306;&#23545;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
K-Tensors: Clustering Positive Semi-Definite Matrices. (arXiv:2306.06534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27491;&#21322;&#23450;&#30697;&#38453;&#30340;&#33258;&#19968;&#33268;&#24615;&#32858;&#31867;&#31639;&#27861;&#65288;K-&#24352;&#37327;&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#29305;&#24449;&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#19968;&#33268;&#24615;&#32858;&#31867;&#31639;&#27861;&#65288;K-Tensors&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#23427;&#20204;&#30340;&#29305;&#24449;&#32467;&#26500;&#23558;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#20998;&#21306;&#12290;&#30001;&#20110;&#27491;&#21322;&#23450;&#30697;&#38453;&#21487;&#20197;&#22312; p&#8805;2 &#30340;&#31354;&#38388;&#20013;&#34920;&#31034;&#20026;&#26925;&#29699;&#20307;&#65292;&#22240;&#27492;&#20445;&#25345;&#23427;&#20204;&#30340;&#32467;&#26500;&#20449;&#24687;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#32858;&#31867;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#30697;&#38453;&#32858;&#31867;&#31639;&#27861;&#24120;&#24120;&#28041;&#21450;&#23558;&#30697;&#38453;&#21521;&#37327;&#21270;&#65292;&#23548;&#33268;&#20851;&#38190;&#32467;&#26500;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21322;&#23450;&#30697;&#38453;&#32467;&#26500;&#20449;&#24687;&#30340;&#36317;&#31163;&#24230;&#37327;&#26469;&#36827;&#34892;&#32858;&#31867;&#12290;&#36825;&#31181;&#36317;&#31163;&#24230;&#37327;&#20351;&#24471;&#32858;&#31867;&#31639;&#27861;&#33021;&#22815;&#32771;&#34385;&#27491;&#21322;&#23450;&#30697;&#38453;&#19982;&#23427;&#20204;&#22312;&#30001;&#19968;&#32452;&#27491;&#21322;&#23450;&#30697;&#38453;&#23450;&#20041;&#30340;&#27491;&#20132;&#21521;&#37327;&#24352;&#25104;&#30340;&#20849;&#21516;&#31354;&#38388;&#19978;&#30340;&#25237;&#24433;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel self-consistency clustering algorithm ($K$-Tensors) designed for {partitioning a distribution of} positive-semidefinite matrices based on their eigenstructures. As positive semi-definite matrices can be represented as ellipsoids in $\Re^p$, $p \ge 2$, it is critical to maintain their structural information to perform effective clustering. However, traditional clustering algorithms {applied to matrices} often {involve vectorization of} the matrices, resulting in a loss of essential structural information. To address this issue, we propose a distance metric {for clustering} that is specifically based on the structural information of positive semi-definite matrices. This distance metric enables the clustering algorithm to consider the differences between positive semi-definite matrices and their projections onto {a} common space spanned by \thadJulyTen{orthonormal vectors defined from a set of} positive semi-definite matrices. This innovative approach to clus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05439</link><description>&lt;p&gt;
CLC: &#22522;&#20110;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#30340;&#32858;&#31867;&#20998;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLC: Cluster Assignment via Contrastive Representation Learning. (arXiv:2306.05439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#26679;&#26412;&#20998;&#32452;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21253;&#21547;&#22823;&#37327;&#32858;&#31867;&#30340;&#25968;&#25454;&#38598;&#65292;&#22914;ImageNet&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20173;&#28982;&#26080;&#27861;&#23454;&#29616;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#12290;&#25105;&#20204;&#23558;&#34920;&#31034;&#20998;&#35299;&#20026;&#20004;&#37096;&#20998;&#65306;&#19968;&#37096;&#20998;&#23545;&#31867;&#21035;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#37319;&#29992;&#31561;&#20998;&#32422;&#26463;&#65292;&#21478;&#19968;&#37096;&#20998;&#25429;&#25417;&#23454;&#20363;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#25439;&#22833;&#65292;&#20351;&#29992;&#34920;&#31034;&#30340;&#20004;&#20010;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#24182;&#25581;&#31034;&#20102;CLC&#22312;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#26102;&#20026;&#36127;&#26679;&#26412;&#35774;&#32622;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;&#36827;&#19968;&#27493;&#30340;&#26799;&#24230;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;CLC&#26102;&#65292;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering remains an important and challenging task of grouping samples into clusters without manual annotations. Recent works have achieved excellent results on small datasets by performing clustering on feature representations learned from self-supervised learning. However, for datasets with a large number of clusters, such as ImageNet, current methods still can not achieve high clustering performance. In this paper, we propose Contrastive Learning-based Clustering (CLC), which uses contrastive learning to directly learn cluster assignment. We decompose the representation into two parts: one encodes the categorical information under an equipartition constraint, and the other captures the instance-wise factors. We propose a contrastive loss using both parts of the representation. We theoretically analyze the proposed contrastive loss and reveal that CLC sets different weights for the negative samples while learning cluster assignments. Further gradient analysis shows that the larger 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#35780;&#20272;&#23884;&#20837;&#36136;&#37327;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#20851;&#27880;&#22914;&#20309;&#20197;&#31283;&#23450;&#30340;&#26041;&#24335;&#36827;&#34892;&#32447;&#24615;&#20998;&#31163;&#12290;&#20174;&#35843;&#26597;&#30340;&#25991;&#29486;&#21644;&#24341;&#20837;&#30340;&#26032;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#35780;&#20272;&#23884;&#20837;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;(This work proposes new methods to evaluate the quality of embeddings, focusing on stable linear separation. From the surveyed literature and introduced novel methods, we can evaluate the quality of embeddings and improve the performance of unsupervised learning.)</title><link>http://arxiv.org/abs/2305.16562</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23884;&#20837;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Embedding Quality Evaluation. (arXiv:2305.16562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16562
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#35780;&#20272;&#23884;&#20837;&#36136;&#37327;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#20851;&#27880;&#22914;&#20309;&#20197;&#31283;&#23450;&#30340;&#26041;&#24335;&#36827;&#34892;&#32447;&#24615;&#20998;&#31163;&#12290;&#20174;&#35843;&#26597;&#30340;&#25991;&#29486;&#21644;&#24341;&#20837;&#30340;&#26032;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#35780;&#20272;&#23884;&#20837;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;(This work proposes new methods to evaluate the quality of embeddings, focusing on stable linear separation. From the surveyed literature and introduced novel methods, we can evaluate the quality of embeddings and improve the performance of unsupervised learning.)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26368;&#36817;&#22312;&#23398;&#26415;&#30028;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#34429;&#28982;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25509;&#36817;&#30417;&#30563;&#23398;&#20064;&#27700;&#24179;&#30340;&#25104;&#26524;&#65292;&#20294;&#30001;&#20110;&#26080;&#30417;&#30563;&#38382;&#39064;&#30340;&#26412;&#36136;&#65292;&#23454;&#36341;&#20013;&#35757;&#32451;&#21644;&#35780;&#20272; SSL &#27169;&#22411;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#21363;&#20351;&#26159;&#20197;&#26377;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#22312;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#26102;&#26159;&#21542;&#33021;&#22815;&#33391;&#22909;&#22320;&#34920;&#29616;&#65292;&#20063;&#24448;&#24448;&#19981;&#28165;&#26970;&#12290;&#36807;&#21435;&#30340;&#24037;&#20316;&#36890;&#24120;&#20165;&#38480;&#20110;&#35780;&#20272;&#23884;&#20837;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#65292;&#36825;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26368;&#20026;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#24037;&#20316;&#36873;&#25321;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#25105;&#20204;&#33021;&#21542;&#37327;&#21270;&#25968;&#25454;&#20013;&#22914;&#20309;&#20197;&#31283;&#23450;&#30340;&#26041;&#24335;&#36827;&#34892;&#32447;&#24615;&#20998;&#31163;&#65311;&#25105;&#20204;&#35843;&#26597;&#20102;&#30456;&#20851;&#30340;&#25991;&#29486;&#65292;&#24182;&#21457;&#29616;&#19977;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#26399;&#23545;&#39640;&#32500;&#31354;&#38388;&#29702;&#35299;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning has recently significantly gained in popularity, especially with deep learning-based approaches. Despite numerous successes and approaching supervised-level performance on a variety of academic benchmarks, it is still hard to train and evaluate SSL models in practice due to the unsupervised nature of the problem. Even with networks trained in a supervised fashion, it is often unclear whether they will perform well when transferred to another domain.  Past works are generally limited to assessing the amount of information contained in embeddings, which is most relevant for self-supervised learning of deep neural networks. This works chooses to follow a different approach: can we quantify how easy it is to linearly separate the data in a stable way? We survey the literature and uncover three methods that could be potentially used for evaluating quality of representations. We also introduce one novel method based on recent advances in understanding the high-dimension
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#23610;&#24230;&#32858;&#31867;&#21644;&#28304;&#20998;&#31163;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#27874;&#25955;&#23556;&#21327;&#26041;&#24046;&#26469;&#25552;&#20379;&#38543;&#26426;&#36807;&#31243;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#38750;&#39640;&#26031;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#22312;MRO&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16189</link><description>&lt;p&gt;
&#28779;&#26143;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#65306;&#19968;&#31181;&#22810;&#23610;&#24230;&#23884;&#22871;&#26041;&#27861;&#20013;&#30340;&#22240;&#23376;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders. (arXiv:2305.16189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16189
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#23610;&#24230;&#32858;&#31867;&#21644;&#28304;&#20998;&#31163;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#27874;&#25955;&#23556;&#21327;&#26041;&#24046;&#26469;&#25552;&#20379;&#38543;&#26426;&#36807;&#31243;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#38750;&#39640;&#26031;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#22312;MRO&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#28304;&#20998;&#31163;&#28041;&#21450;&#36890;&#36807;&#28151;&#21512;&#25805;&#20316;&#35760;&#24405;&#30340;&#26410;&#30693;&#28304;&#20449;&#21495;&#30340;&#20998;&#35299;&#65292;&#20854;&#20013;&#23545;&#28304;&#30340;&#20808;&#39564;&#30693;&#35782;&#26377;&#38480;&#65292;&#20165;&#21487;&#20197;&#35775;&#38382;&#20449;&#21495;&#28151;&#21512;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#19981;&#36866;&#29992;&#30340;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#21463;&#21040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#28304;&#23637;&#29616;&#20986;&#30340;&#22810;&#31181;&#26102;&#38388;&#23610;&#24230;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#23610;&#24230;&#32858;&#31867;&#21644;&#28304;&#20998;&#31163;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#27874;&#25955;&#23556;&#21327;&#26041;&#24046;&#26469;&#25552;&#20379;&#38543;&#26426;&#36807;&#31243;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#38750;&#39640;&#26031;&#38543;&#26426;&#36807;&#31243;&#12290;&#22312;&#36825;&#20010;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22240;&#23376;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23427;&#34987;&#35757;&#32451;&#29992;&#20110;(1)&#27010;&#29575;&#22320;&#23545;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#28304;&#36827;&#34892;&#32858;&#31867;&#21644;&#36880;&#23618;&#38750;&#30417;&#30563;&#28304;&#20998;&#31163;&#65292;(2)&#22312;&#27599;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#25552;&#21462;&#20302;&#32500;&#34920;&#31034;&#65292;(3)&#23398;&#20064;&#28304;&#20449;&#21495;&#30340;&#22240;&#23376;&#34920;&#31034;&#65292;(4)&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#29983;&#25104;&#26410;&#30693;&#28304;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;MRO&#19978;&#30340;&#19977;&#20010;&#39057;&#36947;&#30340;&#21487;&#35265;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised source separation involves unraveling an unknown set of source signals recorded through a mixing operator, with limited prior knowledge about the sources, and only access to a dataset of signal mixtures. This problem is inherently ill-posed and is further challenged by the variety of time-scales exhibited by sources in time series data. Existing methods typically rely on a preselected window size that limits their capacity to handle multi-scale sources. To address this issue, instead of operating in the time domain, we propose an unsupervised multi-scale clustering and source separation framework by leveraging wavelet scattering covariances that provide a low-dimensional representation of stochastic processes, capable of distinguishing between different non-Gaussian stochastic processes. Nested within this representation space, we develop a factorial Gaussian-mixture variational autoencoder that is trained to (1) probabilistically cluster sources at different time-scales a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.16044</link><description>&lt;p&gt;
&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#23558;&#22122;&#22768;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#26159;&#22823;&#33041;&#38750;&#20961;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#30340;&#22522;&#30784;&#65292;&#24182;&#24050;&#25104;&#20026;&#31070;&#32463;&#24418;&#24577;&#26234;&#33021;&#30340;&#25903;&#26609;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#37319;&#29992;&#24102;&#26377;&#22122;&#22768;&#31070;&#32463;&#20803;&#21160;&#21147;&#23398;&#30340;&#33033;&#20914;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#26174;&#31034;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#29702;&#35770;&#19978;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;NDL&#20026;&#20195;&#29702;&#26799;&#24230;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#12290;&#36890;&#36807;&#23558;&#21508;&#31181;SNN&#26550;&#26500;&#21644;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#30830;&#23450;&#24615;SNNs&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#22823;&#35268;&#27169;&#30340;&#21333;&#27169;&#24577;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30005;&#21830;&#24212;&#29992;&#20013;&#35270;&#35273;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#28040;&#34701;&#30740;&#31350;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.13399</link><description>&lt;p&gt;
&#39640;&#25928;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Large-Scale Vision Representation Learning. (arXiv:2305.13399v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#22823;&#35268;&#27169;&#30340;&#21333;&#27169;&#24577;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30005;&#21830;&#24212;&#29992;&#20013;&#35270;&#35273;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#28040;&#34701;&#30740;&#31350;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#21333;&#27169;&#24577;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20102;&#35299;&#20135;&#21697;&#20869;&#23481;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#30005;&#21830;&#25512;&#33616;&#12289;&#25628;&#32034;&#21644;&#24191;&#21578;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#21644;&#23545;&#27604;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#26377;&#25928;&#24494;&#35843;&#22823;&#35268;&#27169;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#22810;&#31181;&#39044;&#35757;&#32451;&#30340;&#39592;&#24178;&#26550;&#26500;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#31995;&#21015;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#30340;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#25552;&#20379;&#35270;&#35273;&#34920;&#31034;&#30340;&#21162;&#21147;&#12290;&#25105;&#20204;&#20026;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#21253;&#25324;&#25105;&#20204;&#30340;&#35270;&#35273;&#30456;&#20284;&#24191;&#21578;&#25512;&#33616;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#24471;&#35270;&#35273;&#34920;&#31034;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31163;&#32447;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#31163;&#32447;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#35273;&#30456;&#20284;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we present our approach to single-modality vision representation learning. Understanding vision representations of product content is vital for recommendations, search, and advertising applications in e-commerce. We detail and contrast techniques used to fine tune large-scale vision representation learning models in an efficient manner under low-resource settings, including several pretrained backbone architectures, both in the convolutional neural network as well as the vision transformer family. We highlight the challenges for e-commerce applications at-scale and highlight the efforts to more efficiently train, evaluate, and serve visual representations. We present ablation studies for several downstream tasks, including our visually similar ad recommendations. We evaluate the offline performance of the derived visual representations in downstream tasks. To this end, we present a novel text-to-image generative offline evaluation method for visually similar recommenda
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#30417;&#30563;&#27880;&#24847;&#21147;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#20351;&#24471;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22024;&#26434;&#30340;&#22270;&#34920;&#36798;&#20013;&#26356;&#21152;&#31283;&#20581;&#21644;&#20855;&#26377;&#19968;&#33324;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13115</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30417;&#30563;&#27880;&#24847;&#21147;&#65306;&#26356;&#22909;&#21644;&#26356;&#31616;&#21333;&#30340;&#36873;&#25321;&#65292;&#23454;&#29616;&#24378;&#22823;&#30340;&#20851;&#27880;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal-Based Supervision of Attention in Graph Neural Network: A Better and Simpler Choice towards Powerful Attention. (arXiv:2305.13115v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13115
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#30417;&#30563;&#27880;&#24847;&#21147;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#20351;&#24471;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22024;&#26434;&#30340;&#22270;&#34920;&#36798;&#20013;&#26356;&#21152;&#31283;&#20581;&#21644;&#20855;&#26377;&#19968;&#33324;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20307;&#27491;&#22312;&#20026;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#35774;&#23450;&#26032;&#30340;&#22522;&#20934;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#30001;&#20110;&#32570;&#20047;&#30452;&#25509;&#30417;&#30563;&#65292;&#23427;&#20204;&#25152;&#20135;&#29983;&#30340;&#20851;&#27880;&#21147;&#23545;&#20110;&#22024;&#26434;&#30340;&#22270;&#34920;&#36798;&#19981;&#22815;&#31283;&#20581;&#21644;&#20855;&#26377;&#19968;&#33324;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#24615;&#24037;&#20855;&#20026;&#27880;&#24847;&#21147;&#20989;&#25968;&#30340;&#23398;&#20064;&#36807;&#31243;&#25552;&#20379;&#24378;&#22823;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#27880;&#24847;&#21147;&#23545;&#20110;&#26368;&#32456;&#39044;&#27979;&#30340;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#65292;&#28982;&#21518;&#26368;&#22823;&#21270;&#35813;&#25928;&#24212;&#65292;&#24341;&#23548;&#27880;&#24847;&#21147;&#20851;&#27880;&#26356;&#26377;&#24847;&#20041;&#30340;&#37051;&#23621;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#32463;&#20856;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#20351;&#29992;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#26126;&#65292;&#36890;&#36807;&#30452;&#25509;&#30417;&#30563;&#27880;&#24847;&#21147;&#20989;&#25968;&#65292;&#27169;&#22411;&#33021;&#22815;&#26356;&#24555;&#22320;&#25910;&#25947;&#24182;&#20135;&#29983;&#26356;&#28165;&#26224;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the great potential of attention mechanism in graph representation learning. However, while variants of attention-based GNNs are setting new benchmarks for numerous real-world datasets, recent works have pointed out that their induced attentions are less robust and generalizable against noisy graphs due to lack of direct supervision. In this paper, we present a new framework which utilizes the tool of causality to provide a powerful supervision signal for the learning process of attention functions. Specifically, we estimate the direct causal effect of attention to the final prediction, and then maximize such effect to guide attention attending to more meaningful neighbors. Our method can serve as a plug-and-play module for any canonical attention-based GNNs in an end-to-end fashion. Extensive experiments on a wide range of benchmark datasets illustrated that, by directly supervising attention functions, the model is able to converge faster with a clearer de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11997</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#29575;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees. (arXiv:2305.11997v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#20559;&#31227;&#65292;&#36890;&#36807;&#20351;&#29992;&#31283;&#23450;&#24615;&#24230;&#37327;&#26469;&#37327;&#21270;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#21487;&#33021;&#30340;&#27169;&#22411;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;&#21453;&#20107;&#23454;&#35299;&#37322;&#20248;&#21270;&#20013;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#26469;&#23558;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#38752;&#36817;&#25968;&#25454;&#27969;&#24418;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#30340;&#39640;&#27010;&#29575;&#40065;&#26834;&#24615;&#12290;&#26032;&#30340;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. Towards finding robust counterfactuals, existing literature often assumes that the original model $m$ and the new model $M$ are bounded in the parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{&lt;}\Delta$. However, models can often change significantly in the parameter space with little to no change in their predictions or accuracy on the given dataset. In this work, we introduce a mathematical abstraction termed \emph{naturally-occurring} model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. Next, we propose a measure -- that we call \emph{Stability} -- to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution is to show that counter
&lt;/p&gt;</description></item><item><title>CB-HVTNet &#25552;&#20986;&#20102;&#19968;&#31181; Channel Boosted Hybrid Vision Transformer &#32593;&#32476;&#65292;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#29983;&#25104;&#22686;&#24378;&#36890;&#36947;&#65292;&#24182;&#32467;&#21512;&#20351;&#29992; Transformers &#21644; CNN&#65292;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#39640;&#25928;&#20934;&#30830;&#22320;&#35780;&#20272;&#28107;&#24052;&#32454;&#32990;&#12290;</title><link>http://arxiv.org/abs/2305.09211</link><description>&lt;p&gt;
CB-HVTNet&#65306;&#19968;&#31181;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#28107;&#24052;&#32454;&#32990;&#35780;&#20272;&#30340;&#36890;&#36947;&#22686;&#24378;&#28151;&#21512;&#35270;&#35273; Transformer &#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images. (arXiv:2305.09211v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09211
&lt;/p&gt;
&lt;p&gt;
CB-HVTNet &#25552;&#20986;&#20102;&#19968;&#31181; Channel Boosted Hybrid Vision Transformer &#32593;&#32476;&#65292;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#29983;&#25104;&#22686;&#24378;&#36890;&#36947;&#65292;&#24182;&#32467;&#21512;&#20351;&#29992; Transformers &#21644; CNN&#65292;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#39640;&#25928;&#20934;&#30830;&#22320;&#35780;&#20272;&#28107;&#24052;&#32454;&#32990;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#30001;&#20110;&#20854;&#23398;&#20064;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#24050;&#32463;&#20811;&#26381;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20840;&#23616;&#36879;&#35270;&#23398;&#20064;&#30340;&#32570;&#28857;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#20851;&#27880;&#30340;&#28966;&#28857;&#65292;&#29992;&#20110;&#22810;&#20010;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#21307;&#30103;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#22810;&#22836;&#27880;&#24847;&#27169;&#22359;&#20165;&#25429;&#33719;&#20840;&#23616;&#32423;&#21035;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#36825;&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#26469;&#35828;&#26159;&#19981;&#36275;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181; Channel Boosted Hybrid Vision Transformer&#65288;CB HVT&#65289;&#65292;&#23427;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#29983;&#25104;&#22686;&#24378;&#36890;&#36947;&#65292;&#24182;&#20351;&#29992; Transformers &#21644; CNN &#26469;&#20998;&#26512;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30340;&#28107;&#24052;&#32454;&#32990;&#12290;&#25152;&#25552;&#20986;&#30340; CB HVT &#21253;&#25324;&#20116;&#20010;&#27169;&#22359;&#65292;&#21253;&#25324;&#36890;&#36947;&#29983;&#25104;&#27169;&#22359;&#12289;&#36890;&#36947;&#21033;&#29992;&#27169;&#22359;&#12289;&#36890;&#36947;&#21512;&#24182;&#27169;&#22359;&#12289;&#21306;&#22495;&#24863;&#30693;&#27169;&#22359;&#21644;&#26816;&#27979;&#21644;&#20998;&#27573;&#22836;&#65292;&#23427;&#20204;&#20849;&#21516;&#26377;&#25928;&#22320;&#35782;&#21035;&#28107;&#24052;&#32454;&#32990;&#12290;&#36890;&#36947;&#29983;&#25104;&#27169;&#22359;&#20351;&#29992;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#36890;&#36947;&#22686;&#24378;&#30340;&#24605;&#24819;&#21019;&#24314;&#22810;&#20010;&#24378;&#22823;&#30340;&#36890;&#36947;&#65292;&#28982;&#21518;&#19982; Transformers &#21644; CNN &#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#26356;&#22909;&#22320;&#20998;&#26512;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30340;&#28107;&#24052;&#32454;&#32990;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340; CB HVT &#26159;&#21307;&#23398;&#35786;&#26029;&#20013;&#20934;&#30830;&#12289;&#39640;&#25928;&#35780;&#20272;&#28107;&#24052;&#32454;&#32990;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers, due to their ability to learn long range dependencies, have overcome the shortcomings of convolutional neural networks (CNNs) for global perspective learning. Therefore, they have gained the focus of researchers for several vision related tasks including medical diagnosis. However, their multi-head attention module only captures global level feature representations, which is insufficient for medical images. To address this issue, we propose a Channel Boosted Hybrid Vision Transformer (CB HVT) that uses transfer learning to generate boosted channels and employs both transformers and CNNs to analyse lymphocytes in histopathological images. The proposed CB HVT comprises five modules, including a channel generation module, channel exploitation module, channel merging module, region-aware module, and a detection and segmentation head, which work together to effectively identify lymphocytes. The channel generation module uses the idea of channel boosting through transfer learni
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;Meta-Polyp&#65292;&#23558;Meta-Former&#19982;UNet&#34701;&#21512;&#24182;&#24341;&#20837;&#22810;&#23610;&#24230;&#19978;&#37319;&#26679;&#22359;&#21644;Convformer&#22359;&#65292;&#35299;&#20915;&#20102;CNN&#21644;Vision Transformer&#22312;&#22788;&#29702;&#20998;&#24067;&#22806;&#25968;&#25454;&#38598;&#12289;&#32570;&#22833;&#36793;&#30028;&#21644;&#23567;&#24687;&#32905;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#20102;&#24687;&#32905;&#20998;&#21106;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.07848</link><description>&lt;p&gt;
Meta-Polyp&#65306;&#39640;&#25928;&#24687;&#32905;&#20998;&#21106;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-Polyp: a baseline for efficient Polyp segmentation. (arXiv:2305.07848v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;Meta-Polyp&#65292;&#23558;Meta-Former&#19982;UNet&#34701;&#21512;&#24182;&#24341;&#20837;&#22810;&#23610;&#24230;&#19978;&#37319;&#26679;&#22359;&#21644;Convformer&#22359;&#65292;&#35299;&#20915;&#20102;CNN&#21644;Vision Transformer&#22312;&#22788;&#29702;&#20998;&#24067;&#22806;&#25968;&#25454;&#38598;&#12289;&#32570;&#22833;&#36793;&#30028;&#21644;&#23567;&#24687;&#32905;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#20102;&#24687;&#32905;&#20998;&#21106;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24687;&#32905;&#20998;&#21106;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#24182;&#19988;&#35768;&#22810;&#26041;&#27861;&#21033;&#29992;CNN&#12289;Vision Transformer&#21644;Transformer&#25216;&#26415;&#24320;&#21457;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#22806;&#25968;&#25454;&#38598;&#12289;&#32570;&#22833;&#36793;&#30028;&#21644;&#23567;&#24687;&#32905;&#26102;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;2022&#24180;&#65292;Meta-Former&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#22522;&#20934;&#32447;&#34987;&#24341;&#20837;&#65292;&#23427;&#19981;&#20165;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#35299;&#20915;&#20102;Vision Transformer&#21644;CNN&#23478;&#26063;&#39592;&#26550;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#20998;&#21106;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-Former&#19982;UNet&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#22312;&#35299;&#30721;&#22120;&#38454;&#27573;&#24341;&#20837;&#20102;&#22810;&#23610;&#24230;&#19978;&#37319;&#26679;&#22359;&#19982;&#32423;&#32852;&#32452;&#21512;&#65292;&#20197;&#22686;&#24378;&#32441;&#29702;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Convformer&#22359;&#65292;&#22522;&#20110;Meta-Former&#30340;&#24605;&#24819;&#65292;&#20197;&#21152;&#24378;&#23616;&#37096;&#29305;&#24449;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#36825;&#20123;&#22359;&#23558;&#20840;&#23616;&#20449;&#24687;&#65288;&#20363;&#22914;&#24687;&#32905;&#30340;&#25972;&#20307;&#24418;&#29366;&#65289;&#19982;&#23616;&#37096;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#36827;&#34892;&#24687;&#32905;&#20998;&#21106;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, polyp segmentation has gained significant importance, and many methods have been developed using CNN, Vision Transformer, and Transformer techniques to achieve competitive results. However, these methods often face difficulties when dealing with out-of-distribution datasets, missing boundaries, and small polyps. In 2022, Meta-Former was introduced as a new baseline for vision, which not only improved the performance of multi-task computer vision but also addressed the limitations of the Vision Transformer and CNN family backbones. To further enhance segmentation, we propose a fusion of Meta-Former with UNet, along with the introduction of a Multi-scale Upsampling block with a level-up combination in the decoder stage to enhance the texture, also we propose the Convformer block base on the idea of the Meta-former to enhance the crucial information of the local feature. These blocks enable the combination of global information, such as the overall shape of the polyp, wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;NP-hard&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#31163;&#25955;&#22270;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#21516;&#26102;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20855;&#26377;&#23545;&#39044;&#27979;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.07617</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#25193;&#23637;&#32806;&#21512;
&lt;/p&gt;
&lt;p&gt;
Scalable Coupling of Deep Learning with Logical Reasoning. (arXiv:2305.07617v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;NP-hard&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#31163;&#25955;&#22270;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#21516;&#26102;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20855;&#26377;&#23545;&#39044;&#27979;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#31163;&#25955;&#25512;&#29702;&#19982;&#31070;&#32463;&#32593;&#32476;&#28151;&#21512;&#30340;&#19981;&#26029;&#25506;&#32034;&#20013;&#65292;&#20986;&#29616;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#23545;&#31070;&#32463;&#32467;&#26500;&#20855;&#22791;&#20174;&#33258;&#28982;&#36755;&#20837;&#20013;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;&#31163;&#25955;&#25512;&#29702;&#25110;&#20248;&#21270;&#38382;&#39064;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32467;&#26500;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#23398;&#20064;&#34987;&#34920;&#31034;&#20026;&#31163;&#25955;&#22270;&#27169;&#22411;&#30340; NP-hard &#25512;&#29702;&#38382;&#39064;&#30340;&#32422;&#26463;&#21644;&#26631;&#20934;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#35299;&#20915;&#20102; Besag &#30340;&#20266;&#23545;&#25968;&#20284;&#28982;&#30340;&#20027;&#35201;&#38480;&#21046;&#20043;&#19968;&#65292;&#33021;&#22815;&#23398;&#20064;&#39640;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#36755;&#20837;&#20013;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915; NP-hard &#25512;&#29702;&#38382;&#39064;&#65292;&#22914;&#31526;&#21495;&#12289;&#35270;&#35273;&#25110;&#22810;&#35299;&#25968;&#25968;&#29420;&#38382;&#39064;&#65292;&#20197;&#21450;&#34507;&#30333;&#36136;&#35774;&#35745;&#38382;&#39064;&#30340;&#33021;&#37327;&#20248;&#21270;&#24418;&#24335;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#23545;&#39044;&#27979;&#30340; \textit{a posteriori} &#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs. In this paper, we introduce a scalable neural architecture and loss function dedicated to learning the constraints and criteria of NP-hard reasoning problems expressed as discrete Graphical Models. Our loss function solves one of the main limitations of Besag's pseudo-loglikelihood, enabling learning of high energies. We empirically show it is able to efficiently learn how to solve NP-hard reasoning problems from natural inputs as the symbolic, visual or many-solutions Sudoku problems as well as the energy optimization formulation of the protein design problem, providing data efficiency, interpretability, and \textit{a posteriori} control over predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#25105;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#36739;&#23567;&#30340;&#28176;&#36817;&#37319;&#26679;&#26041;&#24046;&#65292;&#36866;&#29992;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#37319;&#26679;&#21644;&#37051;&#22495;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.05097</link><description>&lt;p&gt;
&#36890;&#29992;&#22270;&#19978;&#30340;&#33258;&#25105;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208; - &#36890;&#36807;&#38750;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#38142;&#23454;&#29616;&#26368;&#23567;&#37319;&#26679;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
Self-Repellent Random Walks on General Graphs - Achieving Minimal Sampling Variance via Nonlinear Markov Chains. (arXiv:2305.05097v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#25105;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#36739;&#23567;&#30340;&#28176;&#36817;&#37319;&#26679;&#26041;&#24046;&#65292;&#36866;&#29992;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#37319;&#26679;&#21644;&#37051;&#22495;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#20363;&#22914;&#19968;&#33324;&#30340;&#26080;&#21521;&#22270;&#65292;&#20854;&#20013;&#38543;&#26426;&#28216;&#36208;&#35774;&#35745;&#25104;&#36890;&#36807;&#37319;&#26679;&#21644;&#37051;&#22495;&#25506;&#32034;&#26469;&#36924;&#36817;&#32593;&#32476;&#25299;&#25169;&#19978;&#30340;&#30446;&#26631;&#37327;&#65292;&#20197;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599; (MCMC) &#31243;&#24207;&#30340;&#24418;&#24335;&#36827;&#34892;&#12290;&#23545;&#20110;&#20219;&#20309;&#30456;&#24212;&#20110;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#25105;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208; (SRRW)&#65292;&#23427;&#19981;&#22826;&#21487;&#33021;&#36716;&#31227;&#21040;&#36807;&#21435;&#34987;&#39640;&#24230;&#35775;&#38382;&#30340;&#33410;&#28857;&#65292;&#32780;&#26356;&#21487;&#33021;&#36716;&#31227;&#21040;&#24456;&#23569;&#34987;&#35775;&#38382;&#30340;&#33410;&#28857;&#12290;&#23545;&#20110;&#19968;&#31867;&#30001;&#27491;&#23454;&#25968; {\alpha} &#21442;&#25968;&#21270;&#30340; SRRW&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#36807;&#31243;&#30340;&#32463;&#39564;&#20998;&#24067;&#20960;&#20046;&#32943;&#23450;&#25910;&#25947;&#20110;&#24213;&#23618;&#39532;&#23572;&#21487;&#22827;&#38142;&#20869;&#26680;&#30340;&#30446;&#26631; (&#24179;&#31283;) &#20998;&#24067;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#24182;&#25512;&#23548;&#20986;&#25152;&#24471;&#21040;&#30340;&#28176;&#36817;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#31934;&#30830;&#24418;&#24335;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#26126;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#25490;&#26021;&#20316;&#29992; (&#36739;&#22823;&#30340; {\alpha}) &#30340; SRRW &#19968;&#23450;&#27604;&#20855;&#26377;&#36739;&#24369;&#30340;&#25490;&#26021;&#20316;&#29992; (&#36739;&#23567;&#30340; {\alpha}) &#30340; SRRW &#23454;&#29616;&#26356;&#23567;&#30340;&#28176;&#36817;&#37319;&#26679;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider random walks on discrete state spaces, such as general undirected graphs, where the random walkers are designed to approximate a target quantity over the network topology via sampling and neighborhood exploration in the form of Markov chain Monte Carlo (MCMC) procedures. Given any Markov chain corresponding to a target probability distribution, we design a self-repellent random walk (SRRW) which is less likely to transition to nodes that were highly visited in the past, and more likely to transition to seldom visited nodes. For a class of SRRWs parameterized by a positive real {\alpha}, we prove that the empirical distribution of the process converges almost surely to the the target (stationary) distribution of the underlying Markov chain kernel. We then provide a central limit theorem and derive the exact form of the arising asymptotic co-variance matrix, which allows us to show that the SRRW with a stronger repellence (larger {\alpha}) always achieves a smaller asymptotic
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20083;&#33146;&#31579;&#26597; X &#20809;&#29255;&#24322;&#24120;&#20998;&#31867;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#23588;&#20854;&#26159;&#19982;&#20154;&#21475;&#23398;&#21644;&#25104;&#20687;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26088;&#22312;&#24320;&#21457;&#20844;&#24179;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.04422</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#31579;&#26597;&#20083;&#33146; X &#20809;&#29255;&#30340;&#24615;&#33021;&#24046;&#36317; -- &#36808;&#21521;&#20844;&#24179;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Performance Gaps of Artificial Intelligence Models Screening Mammography -- Towards Fair and Interpretable Models. (arXiv:2305.04422v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04422
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20083;&#33146;&#31579;&#26597; X &#20809;&#29255;&#24322;&#24120;&#20998;&#31867;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#23588;&#20854;&#26159;&#19982;&#20154;&#21475;&#23398;&#21644;&#25104;&#20687;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26088;&#22312;&#24320;&#21457;&#20844;&#24179;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#31579;&#26597;&#20083;&#33146; X &#20809;&#29255;&#30340;&#24322;&#24120;&#20998;&#31867;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#19982;&#22686;&#21152;&#24322;&#24120;&#20998;&#31867;&#22833;&#36133;&#39118;&#38505;&#30456;&#20851;&#30340;&#20154;&#21475;&#23398;&#21644;&#25104;&#20687;&#29305;&#24449;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#22238;&#39038;&#24615;&#30740;&#31350;&#20351;&#29992; Emory BrEast Imaging Dataset&#65288;EMBED&#65289;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;2013&#24180;&#33267;2020&#24180;&#38388; Emory University Healthcare &#30340; 115,931 &#21517;&#24739;&#32773;&#30340;&#20083;&#33146; X &#20809;&#29255;&#12290;&#20020;&#24202;&#21644;&#25104;&#20687;&#25968;&#25454;&#21253;&#25324;&#20083;&#33146;&#25104;&#20687;&#25253;&#21578;&#21644;&#25968;&#25454;&#31995;&#32479;&#65288;BI-RADS&#65289;&#35780;&#20272;&#65292;&#24322;&#24120;&#21306;&#22495;&#30340;&#20852;&#36259;&#28857;&#22352;&#26631;&#65292;&#25104;&#20687;&#29305;&#24449;&#65292;&#30149;&#29702;&#32467;&#26524;&#21644;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#23398;&#12290;&#24320;&#21457;&#20102; InceptionV3&#12289;VGG16&#12289;ResNet50V2 &#21644; ResNet152V2 &#31561;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21306;&#20998;&#31579;&#26597;&#20083;&#33146; X &#20809;&#29255;&#20013;&#24322;&#24120;&#32452;&#32455;&#21306;&#22495;&#21644;&#38543;&#26426;&#36873;&#25321;&#30340;&#27491;&#24120;&#32452;&#32455;&#21306;&#22495;&#12290;&#35757;&#32451;&#38598;&#12289;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#30340;&#20998;&#24067;&#20026; 29,144&#65288;55.6%&#65289;&#20010;&#24322;&#24120;&#32452;&#32455;&#21306;&#22495;&#21644;&#26469;&#33258; 10,678&#65288;54.2%&#65289;&#20301;&#24739;&#32773;&#30340;&#33192;&#32960;&#32452;&#32455;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even though deep learning models for abnormality classification can perform well in screening mammography, the demographic and imaging characteristics associated with increased risk of failure for abnormality classification in screening mammograms remain unclear. This retrospective study used data from the Emory BrEast Imaging Dataset (EMBED) including mammograms from 115,931 patients imaged at Emory University Healthcare between 2013 to 2020. Clinical and imaging data includes Breast Imaging Reporting and Data System (BI-RADS) assessment, region of interest coordinates for abnormalities, imaging features, pathologic outcomes, and patient demographics. Deep learning models including InceptionV3, VGG16, ResNet50V2, and ResNet152V2 were developed to distinguish between patches of abnormal tissue and randomly selected patches of normal tissue from the screening mammograms. The distributions of the training, validation and test sets are 29,144 (55.6%) patches of 10,678 (54.2%) patients, 9,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#22810;&#31181;&#27835;&#30103;&#30340;&#22240;&#26524;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#22122;&#22768;&#22810;&#30340;&#21307;&#30103;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03829</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22240;&#26524;&#27169;&#22411;&#25552;&#39640;&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;
&lt;/p&gt;
&lt;p&gt;
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models. (arXiv:2305.03829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#22810;&#31181;&#27835;&#30103;&#30340;&#22240;&#26524;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#22122;&#22768;&#22810;&#30340;&#21307;&#30103;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;&#26088;&#22312;&#26681;&#25454;&#20010;&#20307;&#30340;&#29420;&#29305;&#25104;&#20687;&#29305;&#24449;&#20010;&#24615;&#21270;&#35786;&#30103;&#20915;&#31574;&#65292;&#20197;&#25913;&#21892;&#20854;&#20020;&#24202;&#32467;&#26524;&#12290;&#38598;&#25104;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20316;&#20026;&#27835;&#30103;&#24314;&#35758;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#23558;&#26356;&#21152;&#23433;&#20840;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#22312;&#31934;&#20934;&#21307;&#30103;&#20013;&#65292;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25216;&#26415;&#21644;&#39564;&#35777;&#25351;&#26631;&#12290;&#26412;&#25991;&#37319;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#22810;&#31181;&#27835;&#30103;&#30340;&#22240;&#26524;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#20272;&#35745;&#27599;&#31181;&#27835;&#30103;&#36873;&#39033;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#20219;&#24847;&#20004;&#31181;&#27835;&#30103;&#20043;&#38388;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;&#25105;&#20204;&#23545;&#24739;&#26377;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#30340;&#24739;&#32773;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20197;&#39044;&#27979;&#26032;&#30340;&#21644;&#25193;&#22823;&#30340;T2&#30149;&#21464;&#25968;&#37327;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based precision medicine aims to personalize treatment decisions based on an individual's unique imaging features so as to improve their clinical outcome. Machine learning frameworks that integrate uncertainty estimation as part of their treatment recommendations would be safer and more reliable. However, little work has been done in adapting uncertainty estimation techniques and validation metrics for precision medicine. In this paper, we use Bayesian deep learning for estimating the posterior distribution over factual and counterfactual outcomes on several treatments. This allows for estimating the uncertainty for each treatment option and for the individual treatment effects (ITE) between any two treatments. We train and evaluate this model to predict future new and enlarging T2 lesion counts on a large, multi-center dataset of MR brain images of patients with multiple sclerosis, exposed to several treatments during randomized controlled trials. We evaluate the correlation of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#24046;&#24322;&#27969;&#27169;&#22411;(SD flow)&#65292;&#23427;&#21487;&#20197;&#26368;&#20248;&#22320;&#20943;&#23569;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#21516;&#26102;&#35299;&#20915;Schr&#8203;&#8203;&#246;dinger&#26725;&#38382;&#39064;&#12290;&#19982;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#27809;&#26377;&#23545;&#20808;&#39564;&#20998;&#24067;&#26045;&#21152;&#20219;&#20309;&#38480;&#21046;&#65292;&#22312;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.12906</link><description>&lt;p&gt;
&#35780;&#20998;&#24046;&#20540;&#27969;&#27169;&#22411;&#29992;&#20110;&#38544;&#24335;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
The Score-Difference Flow for Implicit Generative Modeling. (arXiv:2304.12906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#24046;&#24322;&#27969;&#27169;&#22411;(SD flow)&#65292;&#23427;&#21487;&#20197;&#26368;&#20248;&#22320;&#20943;&#23569;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#21516;&#26102;&#35299;&#20915;Schr&#8203;&#8203;&#246;dinger&#26725;&#38382;&#39064;&#12290;&#19982;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#27809;&#26377;&#23545;&#20808;&#39564;&#20998;&#24067;&#26045;&#21152;&#20219;&#20309;&#38480;&#21046;&#65292;&#22312;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#29983;&#25104;&#24314;&#27169;(IGM)&#26088;&#22312;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#26679;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;(&#20363;&#22914;&#35780;&#20998;&#21305;&#37197;&#32593;&#32476;&#12289;&#25193;&#25955;&#27169;&#22411;)&#20174;&#36890;&#36807;&#29615;&#22659;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#25200;&#21160;&#25110;&#27969;&#23558;&#21512;&#25104;&#28304;&#25968;&#25454;&#25512;&#21521;&#30446;&#26631;&#20998;&#24067;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;IGM&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20219;&#24847;&#30446;&#26631;&#21644;&#28304;&#20998;&#24067;&#20043;&#38388;&#30340;&#35780;&#20998;&#24046;&#24322;(SD)&#20316;&#20026;&#27969;&#65292;&#23427;&#21487;&#20197;&#26368;&#20248;&#22320;&#20943;&#23569;&#23427;&#20204;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#65292;&#21516;&#26102;&#35299;&#20915;Schr&#8203;&#8203;&#246;dinger&#26725;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;SD&#27969;&#24212;&#29992;&#20110;&#26041;&#20415;&#30340;&#20195;&#29702;&#20998;&#24067;&#65292;&#24403;&#19988;&#20165;&#24403;&#21407;&#22987;&#20998;&#24067;&#23545;&#40784;&#26102;&#65292;&#23427;&#20204;&#26159;&#23545;&#40784;&#30340;&#12290;&#25105;&#20204;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#23637;&#31034;&#20102;&#36825;&#31181;&#20844;&#24335;&#19982;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#24418;&#24335;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;SD&#27969;&#27809;&#26377;&#23545;&#20808;&#39564;&#20998;&#24067;&#26045;&#21152;&#20219;&#20309;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#22312;&#26080;&#38480;&#36776;&#21035;&#22120;&#33021;&#21147;&#30340;&#26497;&#38480;&#19979;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#35757;&#32451;&#21253;&#21547;SD&#27969;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SD&#27969;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit generative modeling (IGM) aims to produce samples of synthetic data matching the characteristics of a target data distribution. Recent work (e.g. score-matching networks, diffusion models) has approached the IGM problem from the perspective of pushing synthetic source data toward the target distribution via dynamical perturbations or flows in the ambient space. We introduce the score difference (SD) between arbitrary target and source distributions as a flow that optimally reduces the Kullback-Leibler divergence between them while also solving the Schr\"odinger bridge problem. We apply the SD flow to convenient proxy distributions, which are aligned if and only if the original distributions are aligned. We demonstrate the formal equivalence of this formulation to denoising diffusion models under certain conditions. However, unlike diffusion models, SD flow places no restrictions on the prior distribution. We also show that the training of generative adversarial networks includ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORSE&#30340;&#22522;&#20110;&#38544;&#24335;&#35299;&#21078;&#28210;&#26579;&#30340;&#36890;&#29992;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24110;&#21161;&#34701;&#21512;&#39640;&#32423;&#35821;&#20041;&#30456;&#20851;&#20869;&#23481;&#21644;&#20302;&#32423;&#35299;&#21078;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.03209</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#19987;&#23478;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#38544;&#24615;&#35299;&#21078;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts. (arXiv:2304.03209v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03209
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORSE&#30340;&#22522;&#20110;&#38544;&#24335;&#35299;&#21078;&#28210;&#26579;&#30340;&#36890;&#29992;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24110;&#21161;&#34701;&#21512;&#39640;&#32423;&#35821;&#20041;&#30456;&#20851;&#20869;&#23481;&#21644;&#20302;&#32423;&#35299;&#21078;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#32423;&#35821;&#20041;&#30456;&#20851;&#20869;&#23481;&#21644;&#20302;&#32423;&#35299;&#21078;&#29305;&#24449;&#38598;&#25104;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#36817;&#26399;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#20998;&#21106;&#26041;&#27861;&#22312;&#26356;&#22909;&#22320;&#24314;&#27169;&#36825;&#20123;&#20449;&#24687;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#20998;&#21106;&#30340;&#21367;&#31215;&#25805;&#20316;&#36890;&#24120;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#36816;&#34892;&#65292;&#36825;&#22312;&#39640;&#39057;&#21306;&#22495;&#21363;&#36793;&#30028;&#21306;&#22495;&#20013;&#22825;&#29983;&#27169;&#31946;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MORSE&#30340;&#36890;&#29992;&#38544;&#24335;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#35299;&#21078;&#23618;&#38754;&#19978;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#36741;&#21161;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20107;&#23454;&#65306;&#30456;&#36739;&#20110;&#31163;&#25955;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#22312;&#25311;&#21512;&#22797;&#26434;&#20449;&#21495;&#21644;&#35299;&#20915;&#35745;&#31639;&#26426;&#22270;&#24418;&#38382;&#39064;&#26102;&#34920;&#29616;&#26356;&#20026;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23558;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#35270;&#20026;&#28210;&#26579;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25345;&#32493;&#22320;&#23545;&#40784;&#31895;&#30053;&#30340;&#20998;&#21106;p&#24182;&#21033;&#29992;&#38543;&#26426;&#19987;&#23478;&#26469;&#29983;&#25104;&#28210;&#26579;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating high-level semantically correlated contents and low-level anatomical features is of central importance in medical image segmentation. Towards this end, recent deep learning-based medical segmentation methods have shown great promise in better modeling such information. However, convolution operators for medical segmentation typically operate on regular grids, which inherently blur the high-frequency regions, i.e., boundary regions. In this work, we propose MORSE, a generic implicit neural rendering framework designed at an anatomical level to assist learning in medical image segmentation. Our method is motivated by the fact that implicit neural representation has been shown to be more effective in fitting complex signals and solving computer graphics problems than discrete grid-based representation. The core of our approach is to formulate medical image segmentation as a rendering problem in an end-to-end manner. Specifically, we continuously align the coarse segmentation p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;ACTION++&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#35299;&#21078;&#23545;&#27604;&#26469;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2304.02689</link><description>&lt;p&gt;
ACTION++&#65306;&#20351;&#29992;&#33258;&#36866;&#24212;&#35299;&#21078;&#23545;&#27604;&#24230;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. (arXiv:2304.02689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;ACTION++&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#35299;&#21078;&#23545;&#27604;&#26469;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25968;&#25454;&#36890;&#24120;&#34920;&#29616;&#20026;&#38271;&#23614;&#20998;&#24067;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#36825;&#33258;&#28982;&#23548;&#33268;&#23569;&#25968;&#31867;&#21035;&#65288;&#21363;&#36793;&#30028;&#21306;&#22495;&#25110;&#32597;&#35265;&#29289;&#20307;&#65289;&#30340;&#20998;&#31867;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#37197;&#22791;&#26080;&#30417;&#30563;&#23545;&#27604;&#26631;&#20934;&#65292;&#22312;&#38271;&#23614;&#22330;&#26223;&#20013;&#26174;&#30528;&#25913;&#36827;&#20102;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#22312;&#31867;&#21035;&#20998;&#24067;&#20063;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#26631;&#35760;&#25968;&#25454;&#37096;&#20998;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACTION++&#65292;&#19968;&#31181;&#25913;&#36827;&#30340;&#20855;&#26377;&#33258;&#36866;&#24212;&#35299;&#21078;&#23545;&#27604;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#21307;&#23398;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical data often exhibits long-tail distributions with heavy class imbalance, which naturally leads to difficulty in classifying the minority classes (i.e., boundary regions or rare objects). Recent work has significantly improved semi-supervised medical image segmentation in long-tailed scenarios by equipping them with unsupervised contrastive criteria. However, it remains unclear how well they will perform in the labeled portion of data where class distribution is also highly imbalanced. In this work, we present ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Specifically, we propose an adaptive supervised contrastive loss, where we first compute the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then perform online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed cla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#26469;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.02011</link><description>&lt;p&gt;
FakET: &#21033;&#29992;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#27169;&#25311;&#20919;&#20923;&#30005;&#23376;&#26029;&#23618;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer. (arXiv:2304.02011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#26469;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#26159;&#35745;&#31639;&#26174;&#24494;&#23398;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#36825;&#20123;&#30417;&#30563;&#24335;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#28857;&#26159;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36890;&#24120;&#26159;&#19982;&#27169;&#25311;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#29289;&#29702;&#30340;&#22797;&#26434;&#25968;&#20540;&#27491;&#21521;&#27169;&#22411;&#20013;&#30340;&#31890;&#23376;&#27169;&#22411;&#32467;&#21512;&#29983;&#25104;&#30340;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#26426;&#23454;&#29616;&#38750;&#24120;&#32791;&#36153;&#35745;&#31639;&#36164;&#28304;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24050;&#32463;&#24314;&#31435;&#30340;&#29366;&#24577;&#20043;&#19968;&#23545;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#19982;&#22522;&#20934;&#27979;&#35797;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21152;&#36895;&#20102;&#36816;&#31639;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Particle localization and -classification constitute two of the most fundamental problems in computational microscopy. In recent years, deep learning based approaches have been introduced for these tasks with great success. A key shortcoming of these supervised learning methods is their need for large training data sets, typically generated from particle models in conjunction with complex numerical forward models simulating the physics of transmission electron microscopes. Computer implementations of such forward models are computationally extremely demanding and limit the scope of their applicability. In this paper we propose a simple method for simulating the forward operator of an electron microscope based on additive noise and Neural Style Transfer techniques. We evaluate the method on localization and classification tasks using one of the established state-of-the-art architectures showing performance on par with the benchmark. In contrast to previous approaches, our method acceler
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30830;&#23450;&#24182;&#35782;&#21035;&#23545;&#20110;TBI&#31561;&#24613;&#24615;&#30142;&#30149;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;&#30740;&#31350;&#36824;&#21033;&#29992;&#20020;&#24202;&#25968;&#25454;&#39564;&#35777;&#24182;&#35299;&#37322;&#25152;&#35782;&#21035;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.13024</link><description>&lt;p&gt;
&#29992;&#20110;&#35782;&#21035;TBI&#29983;&#29702;&#29366;&#24577;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Clustering of Multivariate Time-Series Data for Identifying TBI Physiological States. (arXiv:2303.13024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13024
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30830;&#23450;&#24182;&#35782;&#21035;&#23545;&#20110;TBI&#31561;&#24613;&#24615;&#30142;&#30149;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;&#30740;&#31350;&#36824;&#21033;&#29992;&#20020;&#24202;&#25968;&#25454;&#39564;&#35777;&#24182;&#35299;&#37322;&#25152;&#35782;&#21035;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30830;&#23450;&#20020;&#24202;&#30456;&#20851;&#30340;&#29983;&#29702;&#29366;&#24577;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#23545;&#20110;&#25552;&#20379;&#24613;&#24615;&#30142;&#30149;&#65288;&#22914;&#39045;&#33041;&#25439;&#20260;&#12289;&#21628;&#21560;&#34928;&#31469;&#21644;&#24515;&#21147;&#34928;&#31469;&#65289;&#30340;&#36866;&#24403;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#21033;&#29992;&#38750;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#25110;&#25968;&#25454;&#25554;&#20540;&#21644;&#32858;&#21512;&#25216;&#26415;&#21487;&#33021;&#23548;&#33268;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#20002;&#22833;&#21644;&#20559;&#35265;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340;SLAC-Time&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#25554;&#20540;&#25110;&#32858;&#21512;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#24613;&#24615;&#24739;&#32773;&#29366;&#24577;&#12290;&#36890;&#36807;&#20351;&#29992;SLAC-Time&#26469;&#32858;&#31867;&#22823;&#22411;&#30740;&#31350;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;TBI&#29983;&#29702;&#29366;&#24577;&#21450;&#20854;&#20855;&#20307;&#29305;&#24449;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#21508;&#31181;&#32858;&#31867;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#32467;&#21512;&#20020;&#24202;&#39046;&#22495;&#19987;&#23478;&#30340;&#24847;&#35265;&#26469;&#39564;&#35777;&#21644;&#35299;&#37322;&#25152;&#35782;&#21035;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#29305;&#23450;&#20020;&#24202;&#20107;&#20214;&#21644;&#29983;&#29702;&#29366;&#24577;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining clinically relevant physiological states from multivariate time series data with missing values is essential for providing appropriate treatment for acute conditions such as Traumatic Brain Injury (TBI), respiratory failure, and heart failure. Utilizing non-temporal clustering or data imputation and aggregation techniques may lead to loss of valuable information and biased analyses. In our study, we apply the SLAC-Time algorithm, an innovative self-supervision-based approach that maintains data integrity by avoiding imputation or aggregation, offering a more useful representation of acute patient states. By using SLAC-Time to cluster data in a large research dataset, we identified three distinct TBI physiological states and their specific feature profiles. We employed various clustering evaluation metrics and incorporated input from a clinical domain expert to validate and interpret the identified physiological states. Further, we discovered how specific clinical events and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Follow-the-regularized-leader&#31639;&#27861;&#30340;&#19977;&#37325;&#19990;&#30028;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#35813;&#31639;&#27861;&#20351;&#29992;&#36127;&#29109;&#27491;&#21017;&#21270;&#22120;&#21487;&#20197;&#22312;&#32447;&#24615;bandit&#38382;&#39064;&#20013;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06825</link><description>&lt;p&gt;
&#20351;&#29992;Follow-the-regularized-leader&#31639;&#27861;&#30340;&#32447;&#24615;bandits&#38382;&#39064;&#30340;&#19977;&#37325;&#19990;&#30028;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Best-of-three-worlds Analysis for Linear Bandits with Follow-the-regularized-leader Algorithm. (arXiv:2303.06825v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Follow-the-regularized-leader&#31639;&#27861;&#30340;&#19977;&#37325;&#19990;&#30028;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#35813;&#31639;&#27861;&#20351;&#29992;&#36127;&#29109;&#27491;&#21017;&#21270;&#22120;&#21487;&#20197;&#22312;&#32447;&#24615;bandit&#38382;&#39064;&#20013;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;bandit&#38382;&#39064;&#22312;&#38543;&#26426;&#21644;&#23545;&#25239;&#29615;&#22659;&#20013;&#24050;&#32463;&#30740;&#31350;&#20102;&#24456;&#22810;&#24180;&#12290;&#35774;&#35745;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#30693;&#36947;&#25439;&#22833;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#29615;&#22659;&#30340;&#31639;&#27861;&#65292;&#24341;&#36215;&#20102;&#24456;&#22810;&#20852;&#36259;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#26816;&#27979;&#25439;&#22833;&#31867;&#22411;&#65292;&#28982;&#21518;&#22312;&#38024;&#23545;&#29305;&#23450;&#29615;&#22659;&#35774;&#35745;&#30340;&#19981;&#21516;&#31639;&#27861;&#20043;&#38388;&#20999;&#25442;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#20197;&#22312;&#25152;&#26377;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;Follow-the-regularized-leader&#65288;FTRL&#65289;&#26159;&#21478;&#19968;&#31181;&#27969;&#34892;&#30340;&#31639;&#27861;&#31867;&#22411;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#29615;&#22659;&#12290;&#19982;&#26816;&#27979;&#24182;&#20999;&#25442;&#31867;&#22411;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#35774;&#35745;&#31616;&#21333;&#65292;&#36951;&#25022;&#30028;&#38480;&#22312;&#20256;&#32479;&#30340;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#34987;&#35777;&#26126;&#26159;&#26368;&#20248;&#30340;&#12290;&#20026;&#32447;&#24615;bandit&#35774;&#35745;&#19968;&#31181;FTRL&#31867;&#22411;&#30340;&#31639;&#27861;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;&#36127;&#29109;&#27491;&#21017;&#21270;&#22120;&#30340;FTRL&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;bandit&#38382;&#39064;&#30340;&#26368;&#20339;&#19977;&#37325;&#19990;&#30028;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The linear bandit problem has been studied for many years in both stochastic and adversarial settings. Designing an algorithm that can optimize the environment without knowing the loss type attracts lots of interest. \citet{LeeLWZ021} propose an algorithm that actively detects the loss type and then switches between different algorithms specially designed for specific settings. However, such an approach requires meticulous designs to perform well in all environments. Follow-the-regularized-leader (FTRL) is another type of popular algorithm that can adapt to different environments. This algorithm is of simple design and the regret bounds are shown to be optimal in traditional multi-armed bandit problems compared with the detect-switch type. Designing an FTRL-type algorithm for linear bandits is an important question that has been open for a long time. In this paper, we prove that the FTRL algorithm with a negative entropy regularizer can achieve the best-of-three-world results for the l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;DMD&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;DLHDMD&#65292;&#21033;&#29992;Takens&#23884;&#20837;&#23450;&#29702;&#30340;&#22522;&#26412;&#35265;&#35299;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#39640;&#32500;&#21644;&#28151;&#27788;&#21160;&#21147;&#23398;&#65292;&#33021;&#22815;&#20026;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#30340;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.06289</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#30340;Hankel&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Enhanced Hankel Dynamic-Mode Decomposition. (arXiv:2303.06289v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;DMD&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;DLHDMD&#65292;&#21033;&#29992;Takens&#23884;&#20837;&#23450;&#29702;&#30340;&#22522;&#26412;&#35265;&#35299;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#39640;&#32500;&#21644;&#28151;&#27788;&#21160;&#21147;&#23398;&#65292;&#33021;&#22815;&#20026;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a deep learning DMD based method, called DLHDMD, which uses the fundamental insight of Takens' Embedding Theorem to develop an adaptive learning scheme that better captures higher dimensional and chaotic dynamics, and is able to generate accurate dynamics for chaotic time series.
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#30340;&#33719;&#21462;&#21464;&#24471;&#36234;&#26469;&#36234;&#31616;&#21333;&#21644;&#22797;&#26434;&#65292;&#20294;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#24320;&#21457;&#21160;&#24577;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#38382;&#39064;&#39046;&#22495;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#24050;&#32463;&#19982;&#25152;&#35859;&#30340;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;DMD&#65289;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#36890;&#29992;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#29305;&#21035;&#26377;&#21069;&#36884;&#30340;&#31934;&#23494;&#21644;&#20934;&#30830;&#30340;&#27169;&#22411;&#24320;&#21457;&#36884;&#24452;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;DMD&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Takens&#23884;&#20837;&#23450;&#29702;&#30340;&#22522;&#26412;&#35265;&#35299;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#39640;&#32500;&#21644;&#28151;&#27788;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;&#28145;&#24230;&#23398;&#20064;Hankel DMD&#65288;DLHDMD&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DLHDMD&#33021;&#22815;&#20026;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#30340;&#21160;&#24577;&#65292;&#24182;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#23398;&#20064;&#26144;&#23556;&#65292;&#36825;&#20123;&#26144;&#23556;&#22312;&#25104;&#21151;&#35757;&#32451;&#21518;&#24448;&#24448;&#36235;&#21521;&#20110;&#26174;&#33879;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the acquisition of time series has become increasingly more straightforward and sophisticated, developing dynamical models from time series is still a challenging and ever evolving problem domain. Within the last several years, to address this problem, there has been a merging of machine learning tools with what is called the dynamic mode decomposition (DMD). This general approach has been shown to be an especially promising avenue for sophisticated and accurate model development. Building on this prior body of work, we develop a deep learning DMD based method which makes use of the fundamental insight of Takens' Embedding Theorem to develop an adaptive learning scheme that better captures higher dimensional and chaotic dynamics. We call this method the Deep Learning Hankel DMD (DLHDMD). We show that the DLHDMD is able to generate accurate dynamics for chaotic time series, and we likewise explore how our method learns mappings which tend, after successful training, to significant
&lt;/p&gt;</description></item><item><title>SLCA&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24930;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#20915;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.05118</link><description>&lt;p&gt;
SLCA: &#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#24930;&#23398;&#20064;&#32773;&#19982;&#20998;&#31867;&#22120;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05118
&lt;/p&gt;
&lt;p&gt;
SLCA&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24930;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#20915;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#22312;&#23398;&#20064;&#39034;&#24207;&#21040;&#36798;&#30340;&#25968;&#25454;&#20013;&#25552;&#39640;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#29616;&#26377;&#24037;&#20316;&#37117;&#24314;&#31435;&#22312;&#20174;&#22836;&#23398;&#20064;&#30340;&#21069;&#25552;&#19979;&#65292;&#20294;&#36234;&#26469;&#36234;&#22810;&#30340;&#21162;&#21147;&#24050;&#32463;&#33268;&#21147;&#20110;&#34701;&#20837;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#27599;&#20010;&#22686;&#37327;&#20219;&#21153;&#20013;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#30340;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#24182;&#23558;&#20851;&#38190;&#25361;&#25112;&#24402;&#22240;&#20110;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#35266;&#23519;&#21040;&#22312;&#34920;&#24449;&#23618;&#27425;&#19978;&#36873;&#25321;&#24615;&#38477;&#20302;&#23398;&#20064;&#29575;&#20960;&#20046;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#24930;&#23398;&#20064;&#32773;&#19982;&#20998;&#31867;&#22120;&#23545;&#40784;&#65288;SLCA&#65289;&#65292;&#36890;&#36807;&#24314;&#27169;&#31867;&#21035;&#20998;&#24067;&#24182;&#22312;&#20107;&#21518;&#23545;&#40784;&#20998;&#31867;&#23618;&#27425;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20998;&#31867;&#23618;&#27425;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SLCA&#22312;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of continual learning is to improve the performance of recognition models in learning sequentially arrived data. Although most existing works are established on the premise of learning from scratch, growing efforts have been devoted to incorporating the benefits of pre-training. However, how to adaptively exploit the pre-trained knowledge for each incremental task while maintaining its generalizability remains an open question. In this work, we present an extensive analysis for continual learning on a pre-trained model (CLPM), and attribute the key challenge to a progressive overfitting problem. Observing that selectively reducing the learning rate can almost resolve this issue in the representation layer, we propose a simple but extremely effective approach named Slow Learner with Classifier Alignment (SLCA), which further improves the classification layer by modeling the class-wise distributions and aligning the classification layers in a post-hoc fashion. Across a variety o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#32534;&#36753;&#23545;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#32534;&#36753;&#36890;&#24120;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#20855;&#20307;&#31243;&#24230;&#21462;&#20915;&#20110;&#32534;&#36753;&#31639;&#27861;&#21644;&#23618;&#30340;&#36873;&#25321;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#32534;&#36753;&#31639;&#27861;&#65292;&#36890;&#36807;&#25554;&#20540;&#19981;&#21516;&#23618;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.00046</link><description>&lt;p&gt;
&#33258;&#34892;&#25215;&#25285;&#32534;&#36753;&#39118;&#38505;&#65306;&#35780;&#20272;&#32463;&#36807;&#20998;&#24067;&#36716;&#31227;&#30340;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Edit at your own risk: evaluating the robustness of edited models to distribution shifts. (arXiv:2303.00046v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#32534;&#36753;&#23545;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#32534;&#36753;&#36890;&#24120;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#20855;&#20307;&#31243;&#24230;&#21462;&#20915;&#20110;&#32534;&#36753;&#31639;&#27861;&#21644;&#23618;&#30340;&#36873;&#25321;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#32534;&#36753;&#31639;&#27861;&#65292;&#36890;&#36807;&#25554;&#20540;&#19981;&#21516;&#23618;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#22686;&#22823;&#27169;&#22411;&#30340;&#36235;&#21183;&#20351;&#24471;&#26631;&#20934;&#30340;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#33021;&#22815;&#22312;&#21518;&#26399;&#36827;&#34892;&#35299;&#37322;&#24615;&#30340;&#12289;&#35745;&#31639;&#20415;&#23452;&#30340;&#27169;&#22411;&#20462;&#25913;&#30340;&#26041;&#27861;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#34429;&#28982;&#35768;&#22810;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#23545;&#20110;&#32534;&#36753;&#27169;&#22411;&#30340;&#23646;&#24615;&#30340;&#30740;&#31350;&#20027;&#35201;&#20165;&#38480;&#20110;&#39564;&#35777;&#20934;&#30830;&#24615;&#30340;&#35780;&#20272;&#12290;&#32534;&#36753;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#24456;&#23569;&#34987;&#25506;&#32034;&#30340;&#20027;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#26368;&#36817;&#20174;&#28145;&#24230;&#23398;&#20064;&#40065;&#26834;&#24615;&#39046;&#22495;&#24320;&#21457;&#30340;&#25216;&#26415;&#65292;&#30740;&#31350;&#27169;&#22411;&#32534;&#36753;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#26222;&#36941;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#32534;&#36753;&#25152;&#38024;&#23545;&#30340;&#29305;&#23450;&#34892;&#20026;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#32534;&#36753;&#20542;&#21521;&#20110;&#38477;&#20302;&#26222;&#36941;&#40065;&#26834;&#24615;&#65292;&#20294;&#21463;&#32534;&#36753;&#31639;&#27861;&#21644;&#36873;&#25321;&#30340;&#23618;&#30340;&#24433;&#21709;&#31243;&#24230;&#19981;&#21516;&#12290;&#22312;&#36825;&#20123;&#35266;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#32534;&#36753;&#31639;&#27861;&#65292;1&#23618;&#25554;&#20540;(1-LI)&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#23618;&#30340;&#29305;&#24449;&#36827;&#34892;&#25554;&#20540;&#26469;&#32534;&#36753;&#27169;&#22411;&#65292;&#36825;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current trend toward ever-larger models makes standard retraining procedures an ever-more expensive burden. For this reason, there is growing interest in model editing, which enables computationally inexpensive, interpretable, post-hoc model modifications. While many model editing techniques are promising, research on the properties of edited models is largely limited to evaluation of validation accuracy. The robustness of edited models is an important and yet mostly unexplored topic. In this paper, we employ recently developed techniques from the field of deep learning robustness to investigate both how model editing affects the general robustness of a model, as well as the robustness of the specific behavior targeted by the edit. We find that edits tend to reduce general robustness, but that the degree of degradation depends on the editing algorithm and layers chosen. Motivated by these observations we introduce a new model editing algorithm, 1-layer interpolation (1-LI), which u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#65292;&#23427;&#20204;&#22312;&#26799;&#24230;/&#31639;&#23376;&#22122;&#22768;&#20855;&#26377;&#26377;&#30028;&#20013;&#24515;&#30340; &#945; &#38454;&#30697;&#30340;&#23485;&#26494;&#20551;&#35774;&#19979;&#20855;&#22791;&#39640;&#27010;&#29575;&#25910;&#25947;&#24615;&#12290;&#36825;&#20123;&#31639;&#27861;&#36866;&#29992;&#20110;&#20809;&#28369;&#38750;&#20984;&#12289;Polyak-Lojasiewicz&#12289;&#20984;&#12289;&#24378;&#20984;&#12289;&#25311;&#24378;&#20984;&#26368;&#23567;&#21270;&#38382;&#39064;&#20197;&#21450;Lipschitz&#12289;&#26143;&#24418;&#24378;&#21327;&#21516;&#19988;&#21333;&#35843;&#12289;&#25311;&#21327;&#21516;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00999</link><description>&lt;p&gt;
&#39640;&#27010;&#29575;&#30028;&#38480;&#30340;&#38543;&#26426;&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#65306;&#26080;&#30028;&#26041;&#24046;&#24773;&#20917;&#19979;&#30340;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance. (arXiv:2302.00999v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#65292;&#23427;&#20204;&#22312;&#26799;&#24230;/&#31639;&#23376;&#22122;&#22768;&#20855;&#26377;&#26377;&#30028;&#20013;&#24515;&#30340; &#945; &#38454;&#30697;&#30340;&#23485;&#26494;&#20551;&#35774;&#19979;&#20855;&#22791;&#39640;&#27010;&#29575;&#25910;&#25947;&#24615;&#12290;&#36825;&#20123;&#31639;&#27861;&#36866;&#29992;&#20110;&#20809;&#28369;&#38750;&#20984;&#12289;Polyak-Lojasiewicz&#12289;&#20984;&#12289;&#24378;&#20984;&#12289;&#25311;&#24378;&#20984;&#26368;&#23567;&#21270;&#38382;&#39064;&#20197;&#21450;Lipschitz&#12289;&#26143;&#24418;&#24378;&#21327;&#21516;&#19988;&#21333;&#35843;&#12289;&#25311;&#21327;&#21516;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#20248;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#30028;&#23545;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#24615;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#39640;&#27010;&#29575;&#22797;&#26434;&#24230;&#30028;&#38480;&#27604;&#26399;&#26395;&#22797;&#26434;&#24230;&#30028;&#38480;&#26356;&#20934;&#30830;&#19988;&#38656;&#27714;&#36739;&#23569;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39640;&#27010;&#29575;&#38750;&#28176;&#36817;&#25910;&#25947;&#32467;&#26524;&#37117;&#26159;&#22312;&#20551;&#35774;&#26799;&#24230;&#22122;&#22768;&#30340;&#26041;&#24046;&#25110;&#30446;&#26631;&#30340;&#26799;&#24230;&#26412;&#36523;&#26159;&#26377;&#30028;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#65292;&#23427;&#20204;&#22312;&#36739;&#23485;&#26494;&#30340;&#20551;&#35774;&#19979;&#20855;&#22791;&#39640;&#27010;&#29575;&#25910;&#25947;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#20197;&#19979;&#24773;&#20917;&#19979;&#25512;&#23548;&#20102;&#26799;&#24230;/&#31639;&#23376;&#22122;&#22768;&#20855;&#26377;&#26377;&#30028;&#20013;&#24515;&#30340;&#945;&#38454;&#30697;&#65288;&#20854;&#20013;&#945;&#8712;(1,2]&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#24615;&#65306;&#65288;i&#65289;&#20809;&#28369;&#38750;&#20984;/Polyak-Lojasiewicz/&#20984;/&#24378;&#20984;/&#25311;&#24378;&#20984;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#65288;ii&#65289;Lipschitz/&#26143;&#24418;&#24378;&#21327;&#21516;&#19988;&#21333;&#35843;/&#25311;&#21327;&#21516;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
During recent years the interest of optimization and machine learning communities in high-probability convergence of stochastic optimization methods has been growing. One of the main reasons for this is that high-probability complexity bounds are more accurate and less studied than in-expectation ones. However, SOTA high-probability non-asymptotic convergence results are derived under strong assumptions such as the boundedness of the gradient noise variance or of the objective's gradient itself. In this paper, we propose several algorithms with high-probability convergence results under less restrictive assumptions. In particular, we derive new high-probability convergence results under the assumption that the gradient/operator noise has bounded central $\alpha$-th moment for $\alpha \in (1,2]$ in the following setups: (i) smooth non-convex / Polyak-Lojasiewicz / convex / strongly convex / quasi-strongly convex minimization problems, (ii) Lipschitz / star-cocoercive and monotone / quas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.00422</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Robust online active learning. (arXiv:2302.00422v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#33719;&#24471;&#26631;&#35760;&#30340;&#35266;&#27979;&#25968;&#25454;&#24182;&#19981;&#31616;&#21333;&#65292;&#36890;&#24120;&#38656;&#35201;&#20154;&#24037;&#19987;&#23478;&#24178;&#39044;&#25110;&#20351;&#29992;&#26114;&#36149;&#30340;&#27979;&#35797;&#35774;&#22791;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#25311;&#21512;&#27169;&#22411;&#26102;&#26368;&#20449;&#24687;&#25968;&#25454;&#28857;&#30340;&#24314;&#35758;&#12290;&#20943;&#23569;&#27169;&#22411;&#24320;&#21457;&#25152;&#38656;&#30340;&#35266;&#27979;&#25968;&#25454;&#25968;&#37327;&#21487;&#20197;&#20943;&#36731;&#35757;&#32451;&#25152;&#38656;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#26631;&#35760;&#30456;&#20851;&#30340;&#25805;&#20316;&#25903;&#20986;&#12290;&#29305;&#21035;&#26159;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#22312;&#38656;&#35201;&#22312;&#26497;&#30701;&#26102;&#38388;&#20869;&#20915;&#23450;&#26159;&#21542;&#33719;&#21462;&#25968;&#25454;&#28857;&#26631;&#35760;&#30340;&#39640;&#23481;&#37327;&#29983;&#20135;&#36807;&#31243;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#33268;&#21147;&#20110;&#24320;&#21457;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20294;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#36825;&#20123;&#26041;&#27861;&#30340;&#34892;&#20026;&#20173;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#32447;&#20027;&#21160;&#32447;&#24615;&#22238;&#24402;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#35777;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many industrial applications, obtaining labeled observations is not straightforward as it often requires the intervention of human experts or the use of expensive testing equipment. In these circumstances, active learning can be highly beneficial in suggesting the most informative data points to be used when fitting a model. Reducing the number of observations needed for model development alleviates both the computational burden required for training and the operational expenses related to labeling. Online active learning, in particular, is useful in high-volume production processes where the decision about the acquisition of the label for a data point needs to be taken within an extremely short time frame. However, despite the recent efforts to develop online active learning strategies, the behavior of these methods in the presence of outliers has not been thoroughly examined. In this work, we investigate the performance of online active linear regression in contaminated data strea
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#31574;&#30053;&#30340;&#22870;&#21169;&#20449;&#21495;&#30001;&#19982;&#20043;&#30456;&#20851;&#19988;&#21516;&#26102;&#20248;&#21270;&#30340;&#21028;&#21035;&#22120;&#29983;&#25104;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#19981;&#31283;&#23450;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#21098;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.00270</link><description>&lt;p&gt;
&#20869;&#37096;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Internally Rewarded Reinforcement Learning. (arXiv:2302.00270v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00270
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#31574;&#30053;&#30340;&#22870;&#21169;&#20449;&#21495;&#30001;&#19982;&#20043;&#30456;&#20851;&#19988;&#21516;&#26102;&#20248;&#21270;&#30340;&#21028;&#21035;&#22120;&#29983;&#25104;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#19981;&#31283;&#23450;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#21098;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#29992;&#20110;&#31574;&#30053;&#23398;&#20064;&#30340;&#22870;&#21169;&#20449;&#21495;&#30001;&#19968;&#20010;&#19982;&#31574;&#30053;&#30456;&#20851;&#19988;&#19982;&#31574;&#30053;&#21516;&#26102;&#20248;&#21270;&#30340;&#21028;&#21035;&#22120;&#29983;&#25104;&#12290;&#31574;&#30053;&#21644;&#21028;&#21035;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#23548;&#33268;&#20102;&#19981;&#31283;&#23450;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#22240;&#20026;&#26469;&#33258;&#19981;&#25104;&#29087;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#20449;&#21495;&#26159;&#22024;&#26434;&#30340;&#65292;&#38459;&#30861;&#20102;&#31574;&#30053;&#30340;&#23398;&#20064;&#65307;&#21453;&#36807;&#26469;&#65292;&#26410;&#32463;&#20248;&#21270;&#30340;&#31574;&#30053;&#20063;&#20250;&#38459;&#30861;&#21028;&#21035;&#22120;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#23398;&#20064;&#35774;&#32622;&#31216;&#20026;&#8220;&#20869;&#37096;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#8221;&#65288;IRRL&#65289;&#65292;&#22240;&#20026;&#22870;&#21169;&#19981;&#26159;&#30452;&#25509;&#26469;&#33258;&#29615;&#22659;&#65292;&#32780;&#26159;&#30001;&#21028;&#21035;&#22120;&#8220;&#20869;&#37096;&#8221;&#25552;&#20379;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#22320;&#34920;&#36848;&#20102;IRRL&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31867;&#23646;&#20110;IRRL&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#24182;&#32463;&#39564;&#24615;&#22320;&#20998;&#26512;&#20102;IRRL&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#25552;&#20986;&#20102;&#20462;&#21098;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#25345;&#32493;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting \textit{Internally Rewarded Reinforcement Learning} (IRRL) as the reward is not provided directly by the environment but \textit{internally} by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the tra
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;PPOCoder&#26694;&#26550;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#21644;Proximal Policy Optimization&#25216;&#26415;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2301.13816</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13816
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;PPOCoder&#26694;&#26550;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#21644;Proximal Policy Optimization&#25216;&#26415;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22312;&#22823;&#35268;&#27169;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;PL&#65289;&#27169;&#22411;&#65292;&#20316;&#20026;&#33258;&#21160;&#21270;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#30340;&#25163;&#27573;&#65292;&#22312;&#20195;&#30721;&#23436;&#25104;&#12289;&#20195;&#30721;&#32763;&#35793;&#21644;&#31243;&#24207;&#21512;&#25104;&#31561;&#21508;&#31181;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#25991;&#26412;&#29983;&#25104;&#20013;&#20511;&#29992;&#30340;&#30417;&#30563;&#24494;&#35843;&#30446;&#26631;&#65292;&#24573;&#35270;&#20102;&#20195;&#30721;&#30340;&#29420;&#29305;&#24207;&#21015;&#32423;&#29305;&#24449;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21487;&#32534;&#35793;&#24615;&#20197;&#21450;&#35821;&#27861;&#21644;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PPOCoder&#65292;&#19968;&#31181;&#26032;&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;PL&#27169;&#22411;&#19982;Proximal Policy Optimization&#65288;PPO&#65289;&#30456;&#32467;&#21512;&#65292;PPO&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#12290;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;PPOCoder&#23558;&#22806;&#37096;&#20195;&#30721;&#29305;&#23450;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#12290;&#36825;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#38750;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#37327;&#30340;&#39640;&#32500;&#22343;&#22330;&#21338;&#24328;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#26410;&#30693;&#35299;&#21644;&#21069;&#21521;-&#21518;&#21521;&#26465;&#20214;&#65292;&#35813;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#39640;&#36798;300&#32500;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36824;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#20132;&#36890;&#27969;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#21333;&#23618;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.02877</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#22343;&#22330;&#21338;&#24328;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Mean Field Games with non-separable Hamiltonians. (arXiv:2301.02877v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#38750;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#37327;&#30340;&#39640;&#32500;&#22343;&#22330;&#21338;&#24328;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#26410;&#30693;&#35299;&#21644;&#21069;&#21521;-&#21518;&#21521;&#26465;&#20214;&#65292;&#35813;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#39640;&#36798;300&#32500;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36824;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#20132;&#36890;&#27969;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#21333;&#23618;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20285;&#36797;&#37329;&#26041;&#27861;&#65288;DGMs&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#38543;&#26426;&#22343;&#22330;&#21338;&#24328;&#65288;MFGs&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;MFG&#31995;&#32479;&#21644;&#21069;&#21521;-&#21518;&#21521;&#26465;&#20214;&#30340;&#26410;&#30693;&#35299;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39640;&#25928;&#65292;&#21363;&#20351;&#36845;&#20195;&#27425;&#25968;&#36739;&#23569;&#65292;&#20063;&#33021;&#22788;&#29702;&#39640;&#36798;300&#32500;&#30340;&#38382;&#39064;&#65292;&#36895;&#24230;&#26356;&#24555;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#20855;&#26377;&#38750;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#37327;&#30340;MFG&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20132;&#36890;&#27969;&#38382;&#39064;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21069;&#35813;&#38382;&#39064;&#20165;&#22312;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#20351;&#29992;&#29275;&#39039;&#36845;&#20195;&#26041;&#27861;&#35299;&#20915;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#32467;&#26524;&#19982;&#35299;&#26512;&#35299;&#21644;&#20808;&#21069;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#65292;&#20854;&#20013;&#20165;&#20351;&#29992;&#20102;&#21333;&#23618;&#38544;&#34255;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new method based on Deep Galerkin Methods (DGMs) for solving high-dimensional stochastic Mean Field Games (MFGs). We achieve this by using two neural networks to approximate the unknown solutions of the MFG system and forward-backward conditions. Our method is efficient, even with a small number of iterations, and is capable of handling up to 300 dimensions with a single layer, which makes it faster than other approaches. In contrast, methods based on Generative Adversarial Networks (GANs) cannot solve MFGs with non-separable Hamiltonians. We demonstrate the effectiveness of our approach by applying it to a traffic flow problem, which was previously solved using the Newton iteration method only in the deterministic case. We compare the results of our method to analytical solutions and previous approaches, showing its efficiency. We also prove the convergence of our neural network approximation with a single hidden layer using the universal approximation theorem.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#39046;&#22495;&#20013;&#65292;&#27809;&#26377;&#19968;&#20010;"&#20449;&#24687;&#35770;"&#26694;&#26550;&#33021;&#22815;&#24314;&#31435;&#26799;&#24230;&#19979;&#38477;&#30340;&#26368;&#23567;&#26368;&#22823;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20998;&#26512;&#19968;&#31181;&#24120;&#35265;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#39640;&#26031;&#22122;&#22768;&#30772;&#22351;&#36845;&#20195;&#30340;&#26041;&#27861;&#20063;&#26080;&#27861;&#24314;&#31435;&#26368;&#23567;&#26368;&#22823;&#36895;&#29575;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#38656;&#35201;&#26032;&#30340;&#24605;&#36335;&#26469;&#20998;&#26512;&#20351;&#29992;&#20449;&#24687;&#35770;&#25216;&#26415;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.13556</link><description>&lt;p&gt;
&#12298;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#30340;&#23616;&#38480;&#24615;&#12299;
&lt;/p&gt;
&lt;p&gt;
Limitations of Information-Theoretic Generalization Bounds for Gradient Descent Methods in Stochastic Convex Optimization. (arXiv:2212.13556v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#39046;&#22495;&#20013;&#65292;&#27809;&#26377;&#19968;&#20010;"&#20449;&#24687;&#35770;"&#26694;&#26550;&#33021;&#22815;&#24314;&#31435;&#26799;&#24230;&#19979;&#38477;&#30340;&#26368;&#23567;&#26368;&#22823;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20998;&#26512;&#19968;&#31181;&#24120;&#35265;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#39640;&#26031;&#22122;&#22768;&#30772;&#22351;&#36845;&#20195;&#30340;&#26041;&#27861;&#20063;&#26080;&#27861;&#24314;&#31435;&#26368;&#23567;&#26368;&#22823;&#36895;&#29575;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#38656;&#35201;&#26032;&#30340;&#24605;&#36335;&#26469;&#20998;&#26512;&#20351;&#29992;&#20449;&#24687;&#35770;&#25216;&#26415;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33267;&#20170;&#20026;&#27490;&#65292;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#35774;&#32622;&#20013;&#65292;&#27809;&#26377;&#35777;&#26126;&#8220;&#20449;&#24687;&#35770;&#8221;&#26694;&#26550;&#33021;&#22815;&#24314;&#31435;&#26799;&#24230;&#19979;&#38477;&#30340;&#26368;&#23567;&#26368;&#22823;&#36895;&#29575;&#20197;&#25512;&#26029;&#27867;&#21270;&#35823;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#20960;&#31181;&#29616;&#26377;&#30340;&#20449;&#24687;&#35770;&#26694;&#26550;&#26469;&#24314;&#31435;&#36825;&#26679;&#30340;&#36895;&#29575;&#65306;&#36755;&#20837;-&#36755;&#20986;&#20114;&#20449;&#24687;&#30028;&#38480;&#12289;&#26465;&#20214;&#20114;&#20449;&#24687;&#30028;&#38480;&#21450;&#20854;&#21464;&#20307;&#12289;PAC-Bayes&#30028;&#38480;&#21644;&#26368;&#36817;&#30340;&#26465;&#20214;&#21464;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#30028;&#38480;&#22343;&#26080;&#27861;&#24314;&#31435;&#26368;&#23567;&#26368;&#22823;&#36895;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#30740;&#31350;&#26799;&#24230;&#26041;&#27861;&#20013;&#24120;&#29992;&#30340;&#19968;&#31181;&#31574;&#30053;&#65292;&#21363;&#36890;&#36807;&#39640;&#26031;&#22122;&#22768;&#30772;&#22351;&#26368;&#32456;&#30340;&#36845;&#20195;&#65292;&#20174;&#32780;&#20135;&#29983;&#22122;&#22768;&#30340;&#8220;&#20195;&#29702;&#8221;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#26080;&#27861;&#36890;&#36807;&#23545;&#36825;&#20123;&#20195;&#29702;&#31639;&#27861;&#30340;&#20998;&#26512;&#24314;&#31435;&#26368;&#23567;&#26368;&#22823;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#38656;&#35201;&#26032;&#30340;&#24605;&#36335;&#26469;&#20998;&#26512;&#20351;&#29992;&#20449;&#24687;&#35770;&#25216;&#26415;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To date, no "information-theoretic" frameworks for reasoning about generalization error have been shown to establish minimax rates for gradient descent in the setting of stochastic convex optimization. In this work, we consider the prospect of establishing such rates via several existing information-theoretic frameworks: input-output mutual information bounds, conditional mutual information bounds and variants, PAC-Bayes bounds, and recent conditional variants thereof. We prove that none of these bounds are able to establish minimax rates. We then consider a common tactic employed in studying gradient methods, whereby the final iterate is corrupted by Gaussian noise, producing a noisy "surrogate" algorithm. We prove that minimax rates cannot be established via the analysis of such surrogates. Our results suggest that new ideas are required to analyze gradient descent using information-theoretic techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#28145;&#24230;&#40654;&#26364;&#32593;&#32476;&#23545;EEG&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#32593;&#32476;&#22823;&#23567;&#12289;&#31471;&#21040;&#31471;&#33021;&#21147;&#12289;&#27169;&#22411;&#35757;&#32451;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#22522;&#20110;&#40654;&#26364;&#20960;&#20309;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10426</link><description>&lt;p&gt;
EEG&#35299;&#30721;&#30340;&#28145;&#24230;&#40654;&#26364;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Riemannian Networks for EEG Decoding. (arXiv:2212.10426v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#28145;&#24230;&#40654;&#26364;&#32593;&#32476;&#23545;EEG&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#32593;&#32476;&#22823;&#23567;&#12289;&#31471;&#21040;&#31471;&#33021;&#21147;&#12289;&#27169;&#22411;&#35757;&#32451;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#22522;&#20110;&#40654;&#26364;&#20960;&#20309;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22312;&#30005;&#33041;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35299;&#30721;&#20219;&#21153;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#36890;&#24120;&#26159;&#30001;&#28145;&#24230;&#23398;&#20064;&#25110;&#22522;&#20110;&#40654;&#26364;&#20960;&#20309;&#30340;&#35299;&#30721;&#22120;&#23454;&#29616;&#30340;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#28145;&#24230;&#40654;&#26364;&#32593;&#32476;&#65288;DRNs&#65289;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#21487;&#33021;&#32467;&#21512;&#20102;&#20043;&#21069;&#20004;&#31867;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#36824;&#26377;&#19968;&#31995;&#21015;&#38382;&#39064;&#38656;&#35201;&#36827;&#19968;&#27493;&#27934;&#23519;&#65292;&#20197;&#38138;&#24179;DRNs&#22312;EEG&#20013;&#26356;&#24191;&#27867;&#24212;&#29992;&#30340;&#36947;&#36335;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#26550;&#26500;&#35774;&#35745;&#38382;&#39064;&#65292;&#22914;&#32593;&#32476;&#22823;&#23567;&#21644;&#31471;&#21040;&#31471;&#33021;&#21147;&#65292;&#20197;&#21450;&#27169;&#22411;&#35757;&#32451;&#38382;&#39064;&#12290;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32593;&#32476;&#20013;&#30340;&#25968;&#25454;&#22914;&#20309;&#36716;&#25442;&#65292;&#20197;&#21450;&#26159;&#21542;&#19982;&#20256;&#32479;&#30340;EEG&#35299;&#30721;&#30456;&#20851;&#20063;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#20855;&#26377;&#24191;&#27867;&#36229;&#21442;&#25968;&#30340;DRNs&#26469;&#22880;&#23450;&#36825;&#20123;&#20027;&#39064;&#39046;&#22495;&#30340;&#22522;&#30784;&#12290;&#20351;&#29992;&#20004;&#20010;&#20844;&#20849;EEG&#25968;&#25454;&#38598;&#27979;&#35797;&#20102;&#32593;&#32476;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#40654;&#26364;&#20960;&#20309;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art performance in electroencephalography (EEG) decoding tasks is currently often achieved with either Deep-Learning or Riemannian-Geometry-based decoders. Recently, there is growing interest in Deep Riemannian Networks (DRNs) possibly combining the advantages of both previous classes of methods. However, there are still a range of topics where additional insight is needed to pave the way for a more widespread application of DRNs in EEG. These include architecture design questions such as network size and end-to-end ability as well as model training questions. How these factors affect model performance has not been explored. Additionally, it is not clear how the data within these networks is transformed, and whether this would correlate with traditional EEG decoding. Our study aims to lay the groundwork in the area of these topics through the analysis of DRNs for EEG with a wide range of hyperparameters. Networks were tested on two public EEG datasets and compared with sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28431;&#26007;&#20989;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#23398;&#20064;&#40065;&#26834;&#28385;&#36275;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;&#30340;&#26102;&#38388;&#20381;&#36182;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.03181</link><description>&lt;p&gt;
&#22522;&#20110;&#28431;&#26007;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#20219;&#21153;&#30340;&#22870;&#21169;&#22609;&#24418;
&lt;/p&gt;
&lt;p&gt;
Funnel-based Reward Shaping for Signal Temporal Logic Tasks in Reinforcement Learning. (arXiv:2212.03181v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28431;&#26007;&#20989;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#23398;&#20064;&#40065;&#26834;&#28385;&#36275;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;&#30340;&#26102;&#38388;&#20381;&#36182;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#65288;STL&#65289;&#26159;&#25551;&#36848;&#21160;&#24577;&#31995;&#32479;&#22797;&#26434;&#26102;&#24577;&#21644;&#36923;&#36753;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#24378;&#21046;&#25191;&#34892;STL&#35268;&#33539;&#30340;&#25511;&#21046;&#22120;&#65292;&#28982;&#32780;&#65292;&#20182;&#20204;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30830;&#20445;&#40065;&#26834;&#28385;&#36275;&#21644;&#20445;&#25345;&#21487;&#25511;&#24615;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20511;&#21161;&#28431;&#26007;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25511;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;STL&#35268;&#33539;&#30340;&#40065;&#26834;&#28385;&#36275;&#30340;&#26102;&#38388;&#20381;&#36182;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Signal Temporal Logic (STL) is a powerful framework for describing the complex temporal and logical behaviour of the dynamical system. Numerous studies have attempted to employ reinforcement learning to learn a controller that enforces STL specifications; however, they have been unable to effectively tackle the challenges of ensuring robust satisfaction in continuous state space and maintaining tractability. In this paper, leveraging the concept of funnel functions, we propose a tractable reinforcement learning algorithm to learn a time-dependent policy for robust satisfaction of STL specification in continuous state space. We demonstrate the utility of our approach on several STL tasks using different environments.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31232;&#26377;&#20107;&#20214;&#30340;&#26032;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#22522;&#20110;&#25910;&#38598;&#21040;&#30340;&#26102;&#38388;&#19981;&#21464;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#21472;&#21152;&#25968;&#25454;&#38598;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25581;&#31034;&#22312;&#21464;&#37327;&#31532;&#19968;&#27425;&#32463;&#21382;&#20302;&#27010;&#29575;&#23454;&#29616;&#26102;&#25165;&#20250;&#26174;&#29616;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#21487;&#34892;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.16596</link><description>&lt;p&gt;
&#38754;&#21521;&#31232;&#26377;&#20107;&#20214;&#30340;&#21160;&#24577;&#22240;&#26524;&#21457;&#29616;&#65306;&#19968;&#31181;&#38750;&#21442;&#25968;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test. (arXiv:2211.16596v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16596
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31232;&#26377;&#20107;&#20214;&#30340;&#26032;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#22522;&#20110;&#25910;&#38598;&#21040;&#30340;&#26102;&#38388;&#19981;&#21464;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#21472;&#21152;&#25968;&#25454;&#38598;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25581;&#31034;&#22312;&#21464;&#37327;&#31532;&#19968;&#27425;&#32463;&#21382;&#20302;&#27010;&#29575;&#23454;&#29616;&#26102;&#25165;&#20250;&#26174;&#29616;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#21487;&#34892;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#31232;&#26377;&#20107;&#20214;&#30456;&#20851;&#32852;&#30340;&#22240;&#26524;&#29616;&#35937;&#22312;&#35768;&#22810;&#24037;&#31243;&#38382;&#39064;&#20013;&#37117;&#23384;&#22312;&#65292;&#20363;&#22914;&#38024;&#23545;&#39118;&#38505;&#30340;&#23433;&#20840;&#20998;&#26512;&#12289;&#20107;&#25925;&#20998;&#26512;&#21644;&#39044;&#38450;&#20197;&#21450;&#26497;&#20540;&#29702;&#35770;&#31561;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#21457;&#29616;&#22312;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#21407;&#22240;&#32852;&#31995;&#65292;&#29305;&#21035;&#26159;&#22312;&#21464;&#21160;&#29615;&#22659;&#19979;&#65292;&#20165;&#22312;&#21464;&#37327;&#31532;&#19968;&#27425;&#32463;&#21382;&#20302;&#27010;&#29575;&#23454;&#29616;&#26102;&#25165;&#20250;&#26174;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21457;&#29983;&#31232;&#26377;&#20294;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#26102;&#38388;&#19981;&#21464;&#21160;&#24577;&#31995;&#32479;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#25506;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#24213;&#23618;&#25968;&#25454;&#30340;&#26102;&#38388;&#19981;&#21464;&#24615;&#26469;&#26500;&#24314;&#19968;&#20010;&#21472;&#21152;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#19981;&#21516;&#26102;&#38388;&#27493;&#39588;&#20043;&#21069;&#31232;&#26377;&#20107;&#20214;&#21457;&#29983;&#21069;&#31995;&#32479;&#29366;&#24577;&#30340;&#25968;&#25454;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#37325;&#26032;&#32452;&#32455;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#26041;&#27861;&#19968;&#33268;&#24615;&#30340;&#38750;&#28176;&#36817;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#22312;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal phenomena associated with rare events occur across a wide range of engineering problems, such as risk-sensitive safety analysis, accident analysis and prevention, and extreme value theory. However, current methods for causal discovery are often unable to uncover causal links, between random variables in a dynamic setting, that manifest only when the variables first experience low-probability realizations. To address this issue, we introduce a novel statistical independence test on data collected from time-invariant dynamical systems in which rare but consequential events occur. In particular, we exploit the time-invariance of the underlying data to construct a superimposed dataset of the system state before rare events happen at different timesteps. We then design a conditional independence test on the reorganized data. We provide non-asymptotic sample complexity bounds for the consistency of our method, and validate its performance across various simulated and real-world datase
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Refoqus&#30340;&#36164;&#28304;&#33410;&#32422;&#30340;&#37327;&#23376;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#21516;&#26102;&#38543;&#26426;&#37319;&#26679;&#25968;&#25454;&#38598;&#21644;&#27979;&#37327;&#25805;&#20316;&#65292;&#33021;&#22815;&#20445;&#23384;&#22823;&#37327;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2211.04965</link><description>&lt;p&gt;
&#36164;&#28304;&#33410;&#32422;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Resource frugal optimizer for quantum machine learning. (arXiv:2211.04965v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04965
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Refoqus&#30340;&#36164;&#28304;&#33410;&#32422;&#30340;&#37327;&#23376;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#21516;&#26102;&#38543;&#26426;&#37319;&#26679;&#25968;&#25454;&#38598;&#21644;&#27979;&#37327;&#25805;&#20316;&#65292;&#33021;&#22815;&#20445;&#23384;&#22823;&#37327;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#22686;&#24378;&#30340;&#25968;&#25454;&#31185;&#23398;&#65292;&#20063;&#31216;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#65292;&#20316;&#20026;&#36817;&#26399;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#21464;&#20998;QML&#31639;&#27861;&#22312;&#28041;&#21450;&#37327;&#23376;&#25968;&#25454;&#26102;&#26377;&#33021;&#21147;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#31639;&#27861;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#38656;&#35201;&#23450;&#21046;&#30340;&#20248;&#21270;&#31243;&#24207;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;QML&#24212;&#29992;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#37319;&#26679;&#27425;&#25968;&#65292;&#22240;&#20026;&#28041;&#21450;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20513;&#23545;&#25968;&#25454;&#38598;&#21644;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#30340;&#27979;&#37327;&#25805;&#20316;&#36827;&#34892;&#21516;&#26102;&#38543;&#26426;&#37319;&#26679;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#39640;&#24230;&#36890;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#20102;&#35768;&#22810;QML&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#26500;&#24314;&#20854;&#26799;&#24230;&#30340;&#26080;&#20559;&#20272;&#35745;&#22120;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;Refoqus&#65288;&#36164;&#28304;&#33410;&#32422;&#30340;&#37327;&#23376;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#65289;&#30340;&#33410;&#32422;&#37319;&#26679;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;Refoqus&#33021;&#22815;&#33410;&#30465;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum-enhanced data science, also known as quantum machine learning (QML), is of growing interest as an application of near-term quantum computers. Variational QML algorithms have the potential to solve practical problems on real hardware, particularly when involving quantum data. However, training these algorithms can be challenging and calls for tailored optimization procedures. Specifically, QML applications can require a large shot-count overhead due to the large datasets involved. In this work, we advocate for simultaneous random sampling over both the dataset as well as the measurement operators that define the loss function. We consider a highly general loss function that encompasses many QML applications, and we show how to construct an unbiased estimator of its gradient. This allows us to propose a shot-frugal gradient descent optimizer called Refoqus (REsource Frugal Optimizer for QUantum Stochastic gradient descent). Our numerics indicate that Refoqus can save several orde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#12289;&#23454;&#26102;&#35299;&#20915;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#22810;&#20010;&#35299;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25910;&#25947;&#21040;&#36817;&#20284;&#31561;&#20215;&#35299;&#30340;&#27491;&#21017;&#21270;&#21382;&#21490;&#22534;&#26632;&#35266;&#23519;&#22120;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#25968;&#25454;&#20016;&#23500;&#24615;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#35813;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16299</link><description>&lt;p&gt;
&#22522;&#20110;&#35266;&#27979;&#22120;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#31561;&#20215;&#35299;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning. (arXiv:2210.16299v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#12289;&#23454;&#26102;&#35299;&#20915;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#22810;&#20010;&#35299;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25910;&#25947;&#21040;&#36817;&#20284;&#31561;&#20215;&#35299;&#30340;&#27491;&#21017;&#21270;&#21382;&#21490;&#22534;&#26632;&#35266;&#23519;&#22120;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#25968;&#25454;&#20016;&#23500;&#24615;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#35813;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#21644;&#23454;&#26102;&#35299;&#20915;&#30830;&#23450;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#22810;&#20010;&#35299;&#30340;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#38750;&#21807;&#19968;&#24615;&#38656;&#35201;&#30740;&#31350;&#31561;&#20215;&#35299;&#30340;&#27010;&#24565;&#65292;&#21363;&#32467;&#26524;&#22312;&#19981;&#21516;&#30340;&#20195;&#20215;&#20989;&#25968;&#20294;&#30456;&#21516;&#30340;&#21453;&#39304;&#30697;&#38453;&#65292;&#20197;&#21450;&#25910;&#25947;&#21040;&#36825;&#20123;&#35299;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#24320;&#21457;&#20102;&#31163;&#32447;&#31639;&#27861;&#20197;&#25910;&#25947;&#21040;&#31561;&#20215;&#35299;&#65292;&#20294;&#23578;&#26410;&#25552;&#20379;&#35299;&#20915;&#38750;&#21807;&#19968;&#24615;&#30340;&#22312;&#32447;&#12289;&#23454;&#26102;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25910;&#25947;&#21040;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#36817;&#20284;&#31561;&#20215;&#35299;&#30340;&#27491;&#21017;&#21270;&#21382;&#21490;&#22534;&#26632;&#35266;&#23519;&#22120;&#12290;&#21457;&#23637;&#20102;&#26032;&#30340;&#25968;&#25454;&#20016;&#23500;&#24615;&#26465;&#20214;&#20197;&#20419;&#36827;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#24320;&#21457;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in solving the deterministic inverse reinforcement learning (IRL) problem online and in real-time is the existence of multiple solutions. Nonuniqueness necessitates the study of the notion of equivalent solutions, i.e., solutions that result in a different cost functional but same feedback matrix, and convergence to such solutions. While offline algorithms that result in convergence to equivalent solutions have been developed in the literature, online, real-time techniques that address nonuniqueness are not available. In this paper, a regularized history stack observer that converges to approximately equivalent solutions of the IRL problem is developed. Novel data-richness conditions are developed to facilitate the analysis and simulation results are provided to demonstrate the effectiveness of the developed technique.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20998;&#36776;&#29575;&#24322;&#36136;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#34920;&#31034;&#30340;&#28909;&#38656;&#27714;&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#21033;&#29992;CNN&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#28909;&#36127;&#33655;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#20025;&#40614;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.13108</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20998;&#36776;&#29575;&#30340;&#24322;&#36136;&#26102;&#38388;&#31995;&#21015;&#38598;&#21512;&#34920;&#31034;&#36827;&#34892;&#28909;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Heat Demand Forecasting with Multi-Resolutional Representation of Heterogeneous Temporal Ensemble. (arXiv:2210.13108v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20998;&#36776;&#29575;&#24322;&#36136;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#34920;&#31034;&#30340;&#28909;&#38656;&#27714;&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#21033;&#29992;CNN&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#28909;&#36127;&#33655;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#20025;&#40614;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#29992;&#20107;&#19994;&#20844;&#21496;&#38754;&#20020;&#30340;&#39318;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#22312;&#30830;&#20445;&#39640;&#25928;&#20379;&#24212;&#30340;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#12290;&#26234;&#33021;&#30005;&#34920;&#21644;&#26234;&#33021;&#30005;&#32593;&#30340;&#20986;&#29616;&#20026;&#36890;&#36807;&#20027;&#21160;&#25216;&#26415;&#65288;&#22914;&#36127;&#33655;&#39044;&#27979;&#65289;&#23454;&#29616;&#28909;&#33021;&#30340;&#20248;&#21270;&#20379;&#24212;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#28909;&#38656;&#27714;&#39044;&#27979;&#26694;&#26550;&#65292;&#20854;&#20013;&#26102;&#38388;&#24207;&#21015;&#34987;&#32534;&#30721;&#20026;&#20855;&#26377;&#23884;&#20837;&#22806;&#29983;&#21464;&#37327;&#65288;&#22914;&#22825;&#27668;&#21644;&#20551;&#26399;/&#38750;&#20551;&#26399;&#65289;&#33021;&#21147;&#30340;scalogram&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;CNN&#26469;&#39044;&#27979;&#26410;&#26469;&#22810;&#27493;&#39588;&#30340;&#28909;&#36127;&#33655;&#12290;&#26368;&#21518;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#22914;SARIMAX&#21644;LSTM&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26469;&#33258;&#20025;&#40614;&#30340;&#23454;&#38469;&#25968;&#25454;&#30340;&#22238;&#39038;&#24615;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#23450;&#37327;&#32467;&#26524;&#19978;&#25345;&#32493;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;MAPE&#30340;&#26368;&#23567;&#22343;&#35823;&#24046;&#20026;7.54&#65285;&#65292;RMSE&#20026;417kW&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the primal challenges faced by utility companies is ensuring efficient supply with minimal greenhouse gas emissions. The advent of smart meters and smart grids provide an unprecedented advantage in realizing an optimised supply of thermal energies through proactive techniques such as load forecasting. In this paper, we propose a forecasting framework for heat demand based on neural networks where the time series are encoded as scalograms equipped with the capacity of embedding exogenous variables such as weather, and holiday/non-holiday. Subsequently, CNNs are utilized to predict the heat load multi-step ahead. Finally, the proposed framework is compared with other state-of-the-art methods, such as SARIMAX and LSTM. The quantitative results from retrospective experiments show that the proposed framework consistently outperforms the state-of-the-art baseline method with real-world data acquired from Denmark. A minimal mean error of 7.54% for MAPE and 417kW for RMSE is achieved wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#30446;&#26631;GFlowNets (MOGFNs) &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;Pareto&#26368;&#20248;&#35299;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;GFlowNets&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#21464;&#20307;&#65306;MOGFN-PC&#21644;MOGFN-AL&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOGFNs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.12765</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;GFlowNets
&lt;/p&gt;
&lt;p&gt;
Multi-Objective GFlowNets. (arXiv:2210.12765v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#30446;&#26631;GFlowNets (MOGFNs) &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;Pareto&#26368;&#20248;&#35299;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;GFlowNets&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#21464;&#20307;&#65306;MOGFN-PC&#21644;MOGFN-AL&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOGFNs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#32972;&#26223;&#19979;&#29983;&#25104;&#22810;&#26679;&#20505;&#36873;&#35299;&#30340;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#22914;&#33647;&#29289;&#21457;&#29616;&#21644;&#26448;&#26009;&#35774;&#35745;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21516;&#26102;&#20248;&#21270;&#19968;&#32452;&#28508;&#22312;&#20914;&#31361;&#30446;&#26631;&#30340;&#20505;&#36873;&#35299;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30446;&#26631;&#24448;&#24448;&#26159;&#23545;&#26576;&#20010;&#24863;&#20852;&#36259;&#23646;&#24615;&#30340;&#19981;&#23436;&#21892;&#35780;&#20272;&#65292;&#22240;&#27492;&#29983;&#25104;&#22810;&#26679;&#20505;&#36873;&#35299;&#23545;&#20110;&#36827;&#34892;&#26114;&#36149;&#30340;&#19979;&#28216;&#35780;&#20272;&#26469;&#35828;&#26159;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;GFlowNets&#65288;MOGFNs&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#30340;Pareto&#26368;&#20248;&#35299;&#65292;&#22522;&#20110;GFlowNets&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;MOGFNs&#30340;&#21464;&#20307;&#65306;MOGFN-PC&#65292;&#23427;&#36890;&#36807;&#29305;&#23450;&#26631;&#37327;&#21270;&#20989;&#25968;&#23450;&#20041;&#20102;&#19968;&#20010;&#29420;&#31435;&#23376;&#38382;&#39064;&#30340;&#31995;&#21015;&#65292;&#24182;&#20351;&#29992;&#22870;&#21169;&#26465;&#20214;&#30340;GFlowNets&#36827;&#34892;&#24314;&#27169;&#65307;&#20197;&#21450;MOGFN-AL&#65292;&#23427;&#22312;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#30001;&#25910;&#36141;&#20989;&#25968;&#23450;&#20041;&#30340;&#23376;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#22522;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;MOGFNs&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of generating diverse candidates in the context of Multi-Objective Optimization. In many applications of machine learning such as drug discovery and material design, the goal is to generate candidates which simultaneously optimize a set of potentially conflicting objectives. Moreover, these objectives are often imperfect evaluations of some underlying property of interest, making it important to generate diverse candidates to have multiple options for expensive downstream evaluations. We propose Multi-Objective GFlowNets (MOGFNs), a novel method for generating diverse Pareto optimal solutions, based on GFlowNets. We introduce two variants of MOGFNs: MOGFN-PC, which models a family of independent sub-problems defined by a scalarization function, with reward-conditional GFlowNets, and MOGFN-AL, which solves a sequence of sub-problems defined by an acquisition function in an active learning loop. Our experiments on wide variety of synthetic and benchmark tasks demonst
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;&#65288;CMCGS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22312;&#32447;&#35268;&#21010;&#29615;&#22659;&#12290;CMCGS&#36890;&#36807;&#23558;&#30456;&#20284;&#29366;&#24577;&#32858;&#31867;&#65292;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#22312;&#32447;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2210.01426</link><description>&lt;p&gt;
&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Continuous Monte Carlo Graph Search. (arXiv:2210.01426v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01426
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;&#65288;CMCGS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22312;&#32447;&#35268;&#21010;&#29615;&#22659;&#12290;CMCGS&#36890;&#36807;&#23558;&#30456;&#20284;&#29366;&#24577;&#32858;&#31867;&#65292;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#22312;&#32447;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22797;&#26434;&#30340;&#36830;&#32493;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#22312;&#32447;&#35268;&#21010;&#23545;&#20110;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#22312;&#32447;&#35268;&#21010;&#65292;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#37319;&#29992;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26426;&#21046;&#26469;&#26435;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;MCTS&#22312;&#35768;&#22810;&#31163;&#25955;&#20915;&#31574;&#39046;&#22495;&#65288;&#22914;&#22260;&#26827;&#12289;&#22269;&#38469;&#35937;&#26827;&#21644;&#23558;&#26827;&#65289;&#20013;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;&#32780;&#38024;&#23545;&#36830;&#32493;&#39046;&#22495;&#30340;MCTS&#25193;&#23637;&#20063;&#24050;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#39640;&#20998;&#25903;&#22240;&#23376;&#21644;&#23548;&#33268;&#25628;&#32034;&#26641;&#22823;&#23567;&#29190;&#28856;&#30340;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;&#65288;CMCGS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;MCTS&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22312;&#32447;&#35268;&#21010;&#29615;&#22659;&#12290;CMCGS&#21033;&#29992;&#20102;&#19968;&#20010;&#27934;&#23519;&#21147;&#65292;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#65292;&#23558;&#30456;&#20284;&#29366;&#24577;&#20043;&#38388;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#21487;&#20197;&#24471;&#21040;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;CMCGS&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#23558;&#30456;&#20284;&#29366;&#24577;&#32858;&#31867;&#25104;&#26377;&#38480;&#25968;&#37327;&#30340;&#38543;&#26426;&#21160;&#20316;&#36172;&#21338;&#33410;&#28857;&#65292;&#36825;&#20123;&#33410;&#28857;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many complex sequential decision-making tasks, online planning is crucial for high performance. For efficient online planning, Monte Carlo Tree Search (MCTS) employs a principled mechanism for trading off exploration for exploitation. MCTS outperforms comparison methods in many discrete decision-making domains such as Go, Chess, and Shogi. Following, extensions of MCTS to continuous domains have been proposed. However, the inherent high branching factor and the resulting explosion of search tree size are limiting existing methods. To address this problem, we propose Continuous Monte Carlo Graph Search (CMCGS), a novel extension of MCTS to online planning in environments with continuous state and action spaces. CMCGS takes advantage of the insight that, during planning, sharing the same action policy between several states can yield high performance. To implement this idea, at each time step, CMCGS clusters similar states into a limited number of stochastic action bandit nodes, which
&lt;/p&gt;</description></item><item><title>Frouros&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#21487;&#20197;&#26816;&#27979;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#27010;&#24565;&#21644;&#25968;&#25454;&#28418;&#31227;&#65292;&#26131;&#20110;&#32500;&#25252;&#21644;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2208.06868</link><description>&lt;p&gt;
Frouros: &#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#28418;&#31227;&#26816;&#27979;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
Frouros: A Python library for drift detection in machine learning systems. (arXiv:2208.06868v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06868
&lt;/p&gt;
&lt;p&gt;
Frouros&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#21487;&#20197;&#26816;&#27979;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#27010;&#24565;&#21644;&#25968;&#25454;&#28418;&#31227;&#65292;&#26131;&#20110;&#32500;&#25252;&#21644;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Frouros&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#33021;&#22815;&#26816;&#27979;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#28418;&#31227;&#12290;&#23427;&#25552;&#20379;&#20102;&#20256;&#32479;&#21644;&#26368;&#36817;&#31639;&#27861;&#30340;&#32452;&#21512;&#26469;&#26816;&#27979;&#27010;&#24565;&#21644;&#25968;&#25454;&#28418;&#31227;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#30446;&#26631;&#26159;&#20351;&#23427;&#19982;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20860;&#23481;&#65292;&#24182;&#36731;&#26494;&#36866;&#24212;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;&#35813;&#24211;&#36981;&#24490;&#19968;&#31995;&#21015;&#26368;&#20339;&#24320;&#21457;&#21644;&#25345;&#32493;&#38598;&#25104;&#23454;&#36341;&#65292;&#20197;&#30830;&#20445;&#26131;&#20110;&#32500;&#25252;&#21644;&#25193;&#23637;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/IFCA/frouros&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Frouros is an open-source Python library capable of detecting drift in machine learning systems. It provides a combination of classical and more recent algorithms for drift detection: both concept and data drift. We have designed it with the objective of making it compatible with any machine learning framework and easily adaptable to real-world use cases. The library is developed following a set of best development and continuous integration practices to ensure ease of maintenance and extensibility. The source code is available at https://github.com/IFCA/frouros.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DESCN&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#20174;&#31471;&#21040;&#31471;&#30340;&#35282;&#24230;&#24314;&#27169;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.09920</link><description>&lt;p&gt;
DESCN: &#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;&#29992;&#20110;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation. (arXiv:2207.09920v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DESCN&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#20174;&#31471;&#21040;&#31471;&#30340;&#35282;&#24230;&#24314;&#27169;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#22312;&#30005;&#23376;&#21830;&#21153;&#21644;&#31934;&#20934;&#21307;&#23398;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;(ITE)&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#20998;&#21035;&#22312;&#21508;&#33258;&#30340;&#26679;&#26412;&#31354;&#38388;&#20013;&#24314;&#27169;&#21463;&#27835;&#30103;&#32452;&#21644;&#23545;&#29031;&#32452;&#30340;&#21709;&#24212;&#20989;&#25968;&#26469;&#39044;&#27979;ITE&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#36935;&#21040;&#20004;&#20010;&#38382;&#39064;&#65292;&#21363;&#30001;&#20110;&#27835;&#30103;&#20559;&#24046;&#32780;&#23548;&#33268;&#30340;&#21463;&#27835;&#30103;&#32452;&#21644;&#23545;&#29031;&#32452;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31163;&#65292;&#20197;&#21450;&#20854;&#20154;&#21475;&#35268;&#27169;&#20043;&#38388;&#30340;&#26174;&#33879;&#26679;&#26412;&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;(DESCN)&#65292;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#24314;&#27169;&#27835;&#30103;&#25928;&#26524;&#12290;DESCN&#36890;&#36807;&#19968;&#20010;&#36328;&#32593;&#32476;&#20197;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#24335;&#25429;&#25417;&#27835;&#30103;&#20542;&#21521;&#12289;&#21709;&#24212;&#21644;&#38544;&#34255;&#27835;&#30103;&#25928;&#26524;&#30340;&#32508;&#21512;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25972;&#20010;&#26679;&#26412;&#31354;&#38388;&#20013;&#32852;&#21512;&#23398;&#20064;&#27835;&#30103;&#21644;&#21709;&#24212;&#20989;&#25968;&#65292;&#20197;&#36991;&#20813;&#27835;&#30103;&#20559;&#24046;&#65292;&#24182;&#37319;&#29992;&#20013;&#38388;&#20266;&#22788;&#29702;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal Inference has wide applications in various areas such as E-commerce and precision medicine, and its performance heavily relies on the accurate estimation of the Individual Treatment Effect (ITE). Conventionally, ITE is predicted by modeling the treated and control response functions separately in their individual sample spaces. However, such an approach usually encounters two issues in practice, i.e. divergent distribution between treated and control groups due to treatment bias, and significant sample imbalance of their population sizes. This paper proposes Deep Entire Space Cross Networks (DESCN) to model treatment effects from an end-to-end perspective. DESCN captures the integrated information of the treatment propensity, the response, and the hidden treatment effect through a cross network in a multi-task learning manner. Our method jointly learns the treatment and response functions in the entire sample space to avoid treatment bias and employs an intermediate pseudo treat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#20998;&#23376;&#36716;&#21464;&#36335;&#24452;&#37319;&#26679;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#37319;&#26679;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#38656;&#35201;&#36873;&#25321;&#38598;&#21512;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#36739;&#22823;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2207.02149</link><description>&lt;p&gt;
&#20998;&#23376;&#36716;&#21464;&#36335;&#24452;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#30340;&#38598;&#21512;&#21464;&#37327;&#33258;&#30001;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Stochastic Optimal Control for Collective Variable Free Sampling of Molecular Transition Paths. (arXiv:2207.02149v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#20998;&#23376;&#36716;&#21464;&#36335;&#24452;&#37319;&#26679;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#37319;&#26679;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#38656;&#35201;&#36873;&#25321;&#38598;&#21512;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#36739;&#22823;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#20998;&#23376;&#31995;&#32479;&#20013;&#37319;&#26679;&#20004;&#20010;&#32473;&#23450;&#20122;&#31283;&#24577;&#20043;&#38388;&#30340;&#36716;&#21464;&#36335;&#24452;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#25240;&#21472;&#21644;&#23637;&#24320;&#30340;&#34507;&#30333;&#36136;&#25110;&#21270;&#23398;&#21453;&#24212;&#30340;&#20135;&#29289;&#21644;&#21453;&#24212;&#29289;&#12290;&#30001;&#20110;&#23384;&#22312;&#39640;&#33021;&#23631;&#38556;&#65292;&#26631;&#20934;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#24456;&#38590;&#37319;&#26679;&#21040;&#36825;&#20123;&#36716;&#21464;&#36335;&#24452;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#24341;&#20837;&#20559;&#20506;&#21183;&#22330;&#26469;&#22686;&#21152;&#36716;&#21464;&#30340;&#27010;&#29575;&#65292;&#20294;&#36825;&#38656;&#35201;&#22522;&#20110;&#38598;&#21512;&#21464;&#37327;&#30340;&#38477;&#32500;&#27493;&#39588;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;&#38598;&#21512;&#21464;&#37327;&#38656;&#35201;&#21270;&#23398;&#30452;&#35273;&#65292;&#22240;&#27492;&#20256;&#32479;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36739;&#22823;&#30340;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#19981;&#27491;&#30830;&#30340;&#38598;&#21512;&#21464;&#37327;&#26102;&#65292;&#20559;&#20506;&#21183;&#22330;&#21487;&#33021;&#19981;&#26159;&#26368;&#23567;&#30340;&#65292;&#20351;&#31995;&#32479;&#22312;&#19982;&#36716;&#21464;&#26080;&#20851;&#30340;&#32500;&#24230;&#19978;&#21457;&#29983;&#20559;&#20506;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#37319;&#26679;&#20998;&#23376;&#36716;&#21464;&#36335;&#24452;&#38382;&#39064;&#12289;Schr\"odinger&#26725;&#38382;&#39064;&#21644;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#19982;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#24418;&#24335;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sampling transition paths between two given metastable states of a molecular system, e.g. a folded and unfolded protein or products and reactants of a chemical reaction. Due to the existence of high energy barriers separating the states, these transition paths are unlikely to be sampled with standard Molecular Dynamics (MD) simulation. Traditional methods to augment MD with a bias potential to increase the probability of the transition rely on a dimensionality reduction step based on Collective Variables (CVs). Unfortunately, selecting appropriate CVs requires chemical intuition and traditional methods are therefore not always applicable to larger systems. Additionally, when incorrect CVs are used, the bias potential might not be minimal and bias the system along dimensions irrelevant to the transition. Showing a formal relation between the problem of sampling molecular transition paths, the Schr\"odinger bridge problem and stochastic optimal control with neu
&lt;/p&gt;</description></item><item><title>\nu-Flows&#26159;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#21644;&#28145;&#24230;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38480;&#21046;&#20013;&#24494;&#23376;&#21160;&#21147;&#23398;&#30340;&#20284;&#28982;&#31354;&#38388;&#65292;&#24182;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#21160;&#37327;&#37325;&#24314;&#21644;&#21943;&#27880;&#20851;&#32852;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2207.00664</link><description>&lt;p&gt;
\nu-Flows&#65306;&#26465;&#20214;&#20013;&#24494;&#23376;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
\nu-Flows: Conditional Neutrino Regression. (arXiv:2207.00664v7 [hep-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00664
&lt;/p&gt;
&lt;p&gt;
\nu-Flows&#26159;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#21644;&#28145;&#24230;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38480;&#21046;&#20013;&#24494;&#23376;&#21160;&#21147;&#23398;&#30340;&#20284;&#28982;&#31354;&#38388;&#65292;&#24182;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#21160;&#37327;&#37325;&#24314;&#21644;&#21943;&#27880;&#20851;&#32852;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;$\nu$-Flows&#65292;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#21644;&#28145;&#24230;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#38480;&#21046;&#39640;&#33021;&#23545;&#25758;&#26426;&#23454;&#39564;&#20013;&#20013;&#24494;&#23376;&#21160;&#21147;&#23398;&#30340;&#20284;&#28982;&#31354;&#38388;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#24674;&#22797;&#36890;&#24120;&#20316;&#20026;&#33258;&#30001;&#21442;&#25968;&#30340;&#23436;&#25972;&#20013;&#24494;&#23376;&#21160;&#37327;&#65292;&#24182;&#22312;&#32473;&#23450;&#20107;&#20214;&#35266;&#27979;&#30340;&#26465;&#20214;&#20284;&#28982;&#19979;&#23545;&#20013;&#24494;&#23376;&#20540;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;$\nu$-Flows&#24212;&#29992;&#20110;&#27169;&#25311;&#30340;&#21322;&#36731;&#23376;$t\bar{t}$&#20107;&#20214;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;$\nu$-Flows&#30340;&#25104;&#21151;&#65292;&#24182;&#26174;&#31034;&#23427;&#21487;&#20197;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#21160;&#37327;&#37325;&#24314;&#65292;&#29305;&#21035;&#26159;&#32437;&#21521;&#22352;&#26631;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#23545;&#20110;&#21943;&#27880;&#20851;&#32852;&#30340;&#19979;&#28216;&#20219;&#21153;&#20855;&#26377;&#30452;&#25509;&#30340;&#22909;&#22788;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#39640;&#36798;1.41&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present $\nu$-Flows, a novel method for restricting the likelihood space of neutrino kinematics in high energy collider experiments using conditional normalizing flows and deep invertible neural networks. This method allows the recovery of the full neutrino momentum which is usually left as a free parameter and permits one to sample neutrino values under a learned conditional likelihood given event observations. We demonstrate the success of $\nu$-Flows in a case study by applying it to simulated semileptonic $t\bar{t}$ events and show that it can lead to more accurate momentum reconstruction, particularly of the longitudinal coordinate. We also show that this has direct benefits in a downstream task of jet association, leading to an improvement of up to a factor of 1.41 compared to conventional methods.
&lt;/p&gt;</description></item><item><title>TabText&#26159;&#19968;&#31181;&#22788;&#29702;&#21644;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#36716;&#25442;&#20869;&#23481;&#20026;&#35821;&#35328;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36890;&#36807;&#24212;&#29992;TabText&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#39640;&#24615;&#33021;&#19988;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27169;&#22411;&#65292;&#20943;&#23569;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#24037;&#20316;&#37327;&#12290;&#35813;&#26694;&#26550;&#22312;&#21307;&#30103;&#39044;&#27979;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.10381</link><description>&lt;p&gt;
TabText:&#19968;&#31181;&#28789;&#27963;&#21644;&#19978;&#19979;&#25991;&#21270;&#30340;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TabText: A Flexible and Contextual Approach to Tabular Data Representation. (arXiv:2206.10381v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10381
&lt;/p&gt;
&lt;p&gt;
TabText&#26159;&#19968;&#31181;&#22788;&#29702;&#21644;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#36716;&#25442;&#20869;&#23481;&#20026;&#35821;&#35328;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36890;&#36807;&#24212;&#29992;TabText&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#39640;&#24615;&#33021;&#19988;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27169;&#22411;&#65292;&#20943;&#23569;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#24037;&#20316;&#37327;&#12290;&#35813;&#26694;&#26550;&#22312;&#21307;&#30103;&#39044;&#27979;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#23545;&#20110;&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#34920;&#26684;&#20013;&#25152;&#26377;&#21487;&#29992;&#30340;&#20449;&#24687;&#65292;&#24573;&#35270;&#20102;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22914;&#21015;&#26631;&#39064;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23558;&#25968;&#25454;&#39044;&#22788;&#29702;&#25104;&#34920;&#26684;&#26684;&#24335;&#20173;&#28982;&#26159;&#27169;&#22411;&#24320;&#21457;&#20013;&#19968;&#39033;&#32791;&#26102;&#30340;&#29942;&#39048;&#12290;&#26412;&#24037;&#20316;&#24341;&#20837;&#20102;TabText&#65292;&#19968;&#31181;&#22788;&#29702;&#21644;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#23558;&#19978;&#19979;&#25991;&#20449;&#24687;&#20174;&#34920;&#26684;&#25968;&#25454;&#32467;&#26500;&#20013;&#25552;&#21462;&#20986;&#26469;&#12290;TabText&#36890;&#36807;&#23558;&#20869;&#23481;&#36716;&#25442;&#20026;&#35821;&#35328;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35299;&#20915;&#22788;&#29702;&#22256;&#38590;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#24739;&#32773;&#20986;&#38498;&#12289;ICU&#20837;&#38498;&#21644;&#27515;&#20129;&#31561;&#20061;&#20010;&#21307;&#30103;&#39044;&#27979;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;1) &#24212;&#29992;&#25105;&#20204;&#30340;TabText&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#24615;&#33021;&#20248;&#31168;&#19988;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27169;&#22411;&#65292;&#21482;&#38656;&#26368;&#23569;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#65307;2) &#22686;&#24378;&#39044;&#22788;&#29702;&#21518;&#30340;&#25968;&#25454;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Tabular data is essential for applying machine learning tasks across various industries. However, traditional data processing methods do not fully utilize all the information available in the tables, ignoring important contextual information such as column header descriptions. In addition, pre-processing data into a tabular format can remain a labor-intensive bottleneck in model development. This work introduces TabText, a processing and feature extraction framework that extracts contextual information from tabular data structures. TabText addresses processing difficulties by converting the content into language and utilizing pre-trained large language models (LLMs). We evaluate our framework on nine healthcare prediction tasks ranging from patient discharge, ICU admission, and mortality. We show that 1) applying our TabText framework enables the generation of high-performing and simple machine learning baseline models with minimal data pre-processing, and 2) augmenting pre-processed t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cal-PIT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#27010;&#29575;-&#27010;&#29575;&#26144;&#23556;&#65292;&#35299;&#20915;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#35786;&#26029;&#21644;&#26657;&#20934;&#38382;&#39064;&#65292;&#26469;&#23454;&#29616;&#26377;&#26465;&#20214;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2205.14568</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;-&#27010;&#29575;&#26144;&#23556;&#23454;&#29616;&#26377;&#26465;&#20214;&#26657;&#20934;&#30340;&#39044;&#27979;&#20998;&#24067;&#65306;&#22312;&#38134;&#27827;&#32418;&#31227;&#20272;&#35745;&#21644;&#27010;&#29575;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conditionally Calibrated Predictive Distributions by Probability-Probability Map: Application to Galaxy Redshift Estimation and Probabilistic Forecasting. (arXiv:2205.14568v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cal-PIT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#27010;&#29575;-&#27010;&#29575;&#26144;&#23556;&#65292;&#35299;&#20915;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#35786;&#26029;&#21644;&#26657;&#20934;&#38382;&#39064;&#65292;&#26469;&#23454;&#29616;&#26377;&#26465;&#20214;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#35780;&#20272;AI&#31639;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25551;&#36848;&#30446;&#26631;&#21464;&#37327;$y \in \mathbb{R}$&#22312;&#32473;&#23450;&#22797;&#26434;&#36755;&#20837;&#29305;&#24449;$\mathbf{x} \in \mathcal{X}$&#30340;&#26465;&#20214;&#19979;&#30340;&#39044;&#27979;&#20998;&#24067;$F(y|\mathbf{x})$&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#27979;&#20998;&#24067;&#65288;&#20363;&#22914;&#65292;&#24402;&#19968;&#21270;&#27969;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65289;&#24448;&#24448;&#32570;&#20047;&#26465;&#20214;&#26657;&#20934;&#65292;&#21363;&#32473;&#23450;&#36755;&#20837;$\mathbf{x}$&#30340;&#20107;&#20214;&#21457;&#29983;&#30340;&#27010;&#29575;&#19982;&#39044;&#27979;&#27010;&#29575;&#26174;&#33879;&#19981;&#21516;&#12290;&#24403;&#21069;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#33021;&#23436;&#20840;&#35780;&#20272;&#21644;&#23454;&#26045;&#26377;&#26465;&#20214;&#26657;&#20934;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cal-PIT&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20174;&#26657;&#20934;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#27010;&#29575;-&#27010;&#29575;&#26144;&#23556;&#26469;&#21516;&#26102;&#35299;&#20915;&#39044;&#27979;&#20998;&#24067;&#30340;&#35786;&#26029;&#21644;&#26657;&#20934;&#38382;&#39064;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23545;&#27010;&#29575;&#31215;&#20998;&#21464;&#25442;&#20998;&#25968;&#36827;&#34892;$\mathbf{x}$&#30340;&#22238;&#24402;&#12290;&#20272;&#35745;&#30340;&#22238;&#24402;&#25552;&#20379;&#20102;&#23545;&#29305;&#24449;&#31354;&#38388;&#20013;&#26465;&#20214;&#35206;&#30422;&#30340;&#21487;&#35299;&#37322;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification is crucial for assessing the predictive ability of AI algorithms. Much research has been devoted to describing the predictive distribution (PD) $F(y|\mathbf{x})$ of a target variable $y \in \mathbb{R}$ given complex input features $\mathbf{x} \in \mathcal{X}$. However, off-the-shelf PDs (from, e.g., normalizing flows and Bayesian neural networks) often lack conditional calibration with the probability of occurrence of an event given input $\mathbf{x}$ being significantly different from the predicted probability. Current calibration methods do not fully assess and enforce conditionally calibrated PDs. Here we propose \texttt{Cal-PIT}, a method that addresses both PD diagnostics and recalibration by learning a single probability-probability map from calibration data. The key idea is to regress probability integral transform scores against $\mathbf{x}$. The estimated regression provides interpretable diagnostics of conditional coverage across the feature space. 
&lt;/p&gt;</description></item><item><title>FedFormer&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#32852;&#37030;&#31574;&#30053;&#65292;&#21033;&#29992;Transformer Attention&#19978;&#19979;&#25991;&#22320;&#27719;&#24635;&#26469;&#33258;&#19981;&#21516;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23884;&#20837;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#22238;&#25253;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2205.13697</link><description>&lt;p&gt;
FedFormer&#65306;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19978;&#19979;&#25991;&#32852;&#37030;&#23398;&#20064;&#19982;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
FedFormer: Contextual Federation with Attention in Reinforcement Learning. (arXiv:2205.13697v3 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13697
&lt;/p&gt;
&lt;p&gt;
FedFormer&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#32852;&#37030;&#31574;&#30053;&#65292;&#21033;&#29992;Transformer Attention&#19978;&#19979;&#25991;&#22320;&#27719;&#24635;&#26469;&#33258;&#19981;&#21516;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23884;&#20837;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#22238;&#25253;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#27719;&#24635;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;&#35265;&#35299;&#12290;&#36890;&#24120;&#37319;&#29992;&#23558;&#27599;&#20010;&#21442;&#19982;&#26234;&#33021;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#21462;&#24179;&#22343;&#24471;&#21040;&#19968;&#20010;&#20849;&#21516;&#27169;&#22411;&#65288;FedAvg&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FedFormer&#65292;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#31574;&#30053;&#65292;&#21033;&#29992;Transformer Attention&#26469;&#19978;&#19979;&#25991;&#22320;&#27719;&#24635;&#26469;&#33258;&#19981;&#21516;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23884;&#20837;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#26681;&#25454;&#24403;&#21069;&#26234;&#33021;&#20307;&#30340;&#29615;&#22659;&#21644;&#23398;&#24471;&#20851;&#31995;&#26377;&#36873;&#25321;&#22320;&#34913;&#37327;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#36129;&#29486;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;Meta-World&#29615;&#22659;&#20013;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;FedAvg&#21644;&#38750;&#32852;&#37030;Soft Actor-Critic&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#19982;Soft Actor-Critic&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;FedFormer&#22312;&#20173;&#36981;&#23432;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#32422;&#26463;&#30340;&#21516;&#26102;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#20998;&#38598;&#22238;&#25253;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core issue in multi-agent federated reinforcement learning is defining how to aggregate insights from multiple agents. This is commonly done by taking the average of each participating agent's model weights into one common model (FedAvg). We instead propose FedFormer, a novel federation strategy that utilizes Transformer Attention to contextually aggregate embeddings from models originating from different learner agents. In so doing, we attentively weigh the contributions of other agents with respect to the current agent's environment and learned relationships, thus providing a more effective and efficient federation. We evaluate our methods on the Meta-World environment and find that our approach yields significant improvements over FedAvg and non-federated Soft Actor-Critic single-agent methods. Our results compared to Soft Actor-Critic show that FedFormer achieves higher episodic return while still abiding by the privacy constraints of federated learning. Finally, we also demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26080;&#30417;&#30563;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#24314;&#27169;&#29992;&#25143;&#30340;&#31038;&#20132;&#32972;&#26223;&#21644;&#22320;&#28857;&#65292;&#20174;&#32780;&#20174;&#25163;&#26426;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#25552;&#21462;&#39640;&#23618;&#27425;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2205.08790</link><description>&lt;p&gt;
&#36890;&#36807;&#25163;&#26426;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#23545;&#29992;&#25143;&#30340;&#31038;&#20132;&#32972;&#26223;&#21644;&#29087;&#24713;&#22320;&#28857;&#36827;&#34892;&#35774;&#22791;&#20869;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
On-device modeling of user's social context and familiar places from smartphone-embedded sensor data. (arXiv:2205.08790v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26080;&#30417;&#30563;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#24314;&#27169;&#29992;&#25143;&#30340;&#31038;&#20132;&#32972;&#26223;&#21644;&#22320;&#28857;&#65292;&#20174;&#32780;&#20174;&#25163;&#26426;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#25552;&#21462;&#39640;&#23618;&#27425;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#35782;&#21035;&#26159;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#20351;&#31227;&#21160;&#21644;&#27867;&#22312;&#35745;&#31639;&#24212;&#29992;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#30340;&#24773;&#22659;&#12290;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#19978;&#65292;&#36890;&#24120;&#22312;&#38598;&#20013;&#24335;&#26550;&#26500;&#19978;&#22788;&#29702;&#65292;&#21487;&#33021;&#20250;&#26292;&#38706;&#29992;&#25143;&#30340;&#20010;&#20154;&#25968;&#25454;&#65292;&#32780;&#19988;&#32570;&#20047;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;&#22240;&#27492;&#65292;&#35774;&#22791;&#20869;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#35782;&#21035;&#20195;&#34920;&#20102;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#30740;&#31350;&#36235;&#21183;&#12290;&#22312;&#31227;&#21160;&#29615;&#22659;&#20013;&#65292;&#29992;&#25143;&#30340;&#31038;&#20132;&#20114;&#21160;&#21644;&#35775;&#38382;&#22320;&#28857;&#26159;&#23545;&#26085;&#24120;&#29983;&#27963;&#22330;&#26223;&#36827;&#34892;&#34920;&#24449;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26080;&#30417;&#30563;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#22312;&#29992;&#25143;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#22522;&#20110;&#33258;&#25105;&#32593;&#32476;&#23545;&#29992;&#25143;&#30340;&#31038;&#20132;&#32972;&#26223;&#21644;&#22320;&#28857;&#36827;&#34892;&#24314;&#27169;&#12290;&#20381;&#38752;&#36825;&#20010;&#27169;&#22411;&#65292;&#31995;&#32479;&#33021;&#22815;&#20174;&#25163;&#26426;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#25552;&#21462;&#39640;&#23618;&#27425;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context modeling and recognition represent complex tasks that allow mobile and ubiquitous computing applications to adapt to the user's situation. Current solutions mainly focus on limited context information generally processed on centralized architectures, potentially exposing users' personal data to privacy leakage, and missing personalization features. For these reasons on-device context modeling and recognition represent the current research trend in this area. Among the different information characterizing the user's context in mobile environments, social interactions and visited locations remarkably contribute to the characterization of daily life scenarios. In this paper we propose a novel, unsupervised and lightweight approach to model the user's social context and her locations based on ego networks directly on the user mobile device. Relying on this model, the system is able to extract high-level and semantic-rich context features from smartphone-embedded sensors data. Speci
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20174;&#19981;&#24179;&#34913;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#23454;&#39564;&#26694;&#26550;&#65292;&#23545;&#22810;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#27969;&#22330;&#26223;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#27604;&#36739;&#20102;24&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2204.03719</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#35843;&#26597;&#65306;&#20998;&#31867;&#12289;&#25361;&#25112;&#12289;&#23454;&#35777;&#30740;&#31350;&#21644;&#21487;&#22797;&#21046;&#30340;&#23454;&#39564;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A survey on learning from imbalanced data streams: taxonomy, challenges, empirical study, and reproducible experimental framework. (arXiv:2204.03719v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03719
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20174;&#19981;&#24179;&#34913;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#23454;&#39564;&#26694;&#26550;&#65292;&#23545;&#22810;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#27969;&#22330;&#26223;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#27604;&#36739;&#20102;24&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#28041;&#21450;&#21040;&#23545;&#25968;&#25454;&#27969;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#31867;&#21035;&#19981;&#24179;&#34913;&#20250;&#24102;&#26469;&#26032;&#30340;&#25361;&#25112;&#12290;&#36817;&#26399;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#21508;&#31181;&#25968;&#25454;&#32423;&#21035;&#12289;&#31639;&#27861;&#32423;&#21035;&#21644;&#38598;&#25104;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#31639;&#27861;&#23384;&#22312;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#20849;&#35782;&#30340;&#31243;&#24207;&#21644;&#22522;&#20934;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#12289;&#35814;&#23613;&#21644;&#20840;&#38754;&#30340;&#23454;&#39564;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#19968;&#31995;&#21015;&#22810;&#26679;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#27969;&#22330;&#26223;&#20013;&#30340;&#31639;&#27861;&#12290;&#23454;&#35777;&#30740;&#31350;&#35780;&#20272;&#20102;24&#20010;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#27969;&#31639;&#27861;&#22312;515&#20010;&#32467;&#21512;&#20102;&#38745;&#24577;&#21644;&#21160;&#24577;&#31867;&#21035;&#19981;&#24179;&#34913;&#27604;&#20363;&#12289;&#23454;&#20363;&#32423;&#22256;&#38590;&#24230;&#12289;&#27010;&#24565;&#28418;&#31227;&#12289;&#30495;&#23454;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#20108;&#20803;&#21644;&#22810;&#31867;&#24773;&#26223;&#19979;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#27969;&#19978;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#25968;&#25454;&#27969;&#25366;&#25496;&#39046;&#22495;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance poses new challenges when it comes to classifying data streams. Many algorithms recently proposed in the literature tackle this problem using a variety of data-level, algorithm-level, and ensemble approaches. However, there is a lack of standardized and agreed-upon procedures and benchmarks on how to evaluate these algorithms. This work proposes a standardized, exhaustive, and comprehensive experimental framework to evaluate algorithms in a collection of diverse and challenging imbalanced data stream scenarios. The experimental study evaluates 24 state-of-the-art data streams algorithms on 515 imbalanced data streams that combine static and dynamic class imbalance ratios, instance-level difficulties, concept drift, real-world and semi-synthetic datasets in binary and multi-class scenarios. This leads to a large-scale experimental study comparing state-of-the-art classifiers in the data stream mining domain. We discuss the advantages and disadvantages of state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#35266;&#27979;&#22120;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20108;&#27425;&#25104;&#26412;&#20989;&#25968;&#19979;&#30340;&#36755;&#20986;&#21453;&#39304;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#36716;&#21270;&#20026;&#29366;&#24577;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#24320;&#21457;&#20004;&#31181;&#22522;&#20110;&#35266;&#27979;&#22120;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#19968;&#31181;&#21382;&#21490;&#22534;&#26632;&#37325;&#22797;&#20351;&#29992;&#20808;&#21069;&#29366;&#24577;&#20272;&#35745;&#30340;&#26032;&#39062;&#35266;&#27979;&#22120;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#25910;&#25947;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#23454;&#39564;&#39564;&#35777;&#20102;&#22312;&#26377;&#22122;&#22768;&#21644;&#26080;&#22122;&#22768;&#27979;&#37327;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2011.02057</link><description>&lt;p&gt;
&#22522;&#20110;&#22312;&#32447;&#35266;&#27979;&#22120;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Observer-Based Inverse Reinforcement Learning. (arXiv:2011.02057v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.02057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#35266;&#27979;&#22120;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20108;&#27425;&#25104;&#26412;&#20989;&#25968;&#19979;&#30340;&#36755;&#20986;&#21453;&#39304;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#36716;&#21270;&#20026;&#29366;&#24577;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#24320;&#21457;&#20004;&#31181;&#22522;&#20110;&#35266;&#27979;&#22120;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#19968;&#31181;&#21382;&#21490;&#22534;&#26632;&#37325;&#22797;&#20351;&#29992;&#20808;&#21069;&#29366;&#24577;&#20272;&#35745;&#30340;&#26032;&#39062;&#35266;&#27979;&#22120;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#25910;&#25947;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#23454;&#39564;&#39564;&#35777;&#20102;&#22312;&#26377;&#22122;&#22768;&#21644;&#26080;&#22122;&#22768;&#27979;&#37327;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32447;&#24615;&#31995;&#32479;&#20108;&#27425;&#25104;&#26412;&#20989;&#25968;&#19979;&#30340;&#36755;&#20986;&#21453;&#39304;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#38382;&#39064;&#65292;&#23558;IRL&#38382;&#39064;&#35270;&#20026;&#29366;&#24577;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#22522;&#20110;&#35266;&#27979;&#22120;&#30340;IRL&#25216;&#26415;&#65292;&#21253;&#25324;&#19968;&#31181;&#36890;&#36807;&#21382;&#21490;&#22534;&#26632;&#37325;&#22797;&#20351;&#29992;&#20808;&#21069;&#29366;&#24577;&#20272;&#35745;&#30340;&#26032;&#39062;&#35266;&#27979;&#22120;&#26041;&#27861;&#12290;&#22312;&#36866;&#24403;&#30340;&#28608;&#21169;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25910;&#25947;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#20223;&#30495;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#26377;&#22122;&#22768;&#21644;&#26080;&#22122;&#22768;&#27979;&#37327;&#19979;&#24320;&#21457;&#30340;&#35266;&#27979;&#22120;&#21644;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a novel approach to the output-feedback inverse reinforcement learning (IRL) problem is developed by casting the IRL problem, for linear systems with quadratic cost functions, as a state estimation problem. Two observer-based techniques for IRL are developed, including a novel observer method that re-uses previous state estimates via history stacks. Theoretical guarantees for convergence and robustness are established under appropriate excitation conditions. Simulations demonstrate the performance of the developed observers and filters under noisy and noise-free measurements.
&lt;/p&gt;</description></item></channel></rss>