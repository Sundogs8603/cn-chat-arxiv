<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#36890;&#36807;COgnitive REplay&#65288;CORE&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#25968;&#37327;&#20998;&#37197;&#21644;&#20197;&#36136;&#37327;&#20026;&#37325;&#28857;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#20248;&#21270;&#37325;&#25773;&#32531;&#20914;&#21306;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01348</link><description>&lt;p&gt;
CORE&#65306;&#36890;&#36807;&#35748;&#30693;&#37325;&#25773;&#26469;&#20943;&#36731;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
CORE: Mitigating Catastrophic Forgetting in Continual Learning through Cognitive Replay
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;COgnitive REplay&#65288;CORE&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#25968;&#37327;&#20998;&#37197;&#21644;&#20197;&#36136;&#37327;&#20026;&#37325;&#28857;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#20248;&#21270;&#37325;&#25773;&#32531;&#20914;&#21306;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26174;&#33879;&#20943;&#36731;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26032;&#35270;&#35282;&#65292;&#24378;&#35843;&#27169;&#22411;&#20445;&#25345;&#29616;&#26377;&#30693;&#35782;&#24182;&#34701;&#20837;&#26032;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#30340;&#37325;&#25773;&#26041;&#27861;&#21516;&#31561;&#23545;&#24453;&#27599;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#26679;&#26412;&#65292;&#22240;&#27492;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#37325;&#25773;&#32531;&#20914;&#21306;&#30340;&#28508;&#21147;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COgnitive REplay&#65288;CORE&#65289;&#65292;&#23427;&#20174;&#20154;&#31867;&#35748;&#30693;&#22797;&#20064;&#36807;&#31243;&#20013;&#24471;&#21040;&#28789;&#24863;&#12290;CORE&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#31574;&#30053;&#65306;&#33258;&#36866;&#24212;&#25968;&#37327;&#20998;&#37197;&#21644;&#20197;&#36136;&#37327;&#20026;&#37325;&#28857;&#30340;&#25968;&#25454;&#36873;&#25321;&#12290;&#21069;&#32773;&#26681;&#25454;&#27599;&#20010;&#20219;&#21153;&#30340;&#36951;&#24536;&#36895;&#29575;&#33258;&#36866;&#24212;&#22320;&#35843;&#33410;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#20998;&#37197;&#65292;&#32780;&#21518;&#32773;&#20445;&#35777;&#22312;&#32531;&#20914;&#21306;&#20013;&#21253;&#21547;&#26368;&#33021;&#27010;&#25324;&#27599;&#20010;&#20219;&#21153;&#29305;&#24449;&#30340;&#20195;&#34920;&#24615;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#21106;CIFAR10&#19978;&#23454;&#29616;&#20102;37.95%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;6.52%&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#26368;&#24046;&#34920;&#29616;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel perspective to significantly mitigate catastrophic forgetting in continuous learning (CL), which emphasizes models' capacity to preserve existing knowledge and assimilate new information. Current replay-based methods treat every task and data sample equally and thus can not fully exploit the potential of the replay buffer. In response, we propose COgnitive REplay (CORE), which draws inspiration from human cognitive review processes. CORE includes two key strategies: Adaptive Quantity Allocation and Quality-Focused Data Selection. The former adaptively modulates the replay buffer allocation for each task based on its forgetting rate, while the latter guarantees the inclusion of representative data that best encapsulates the characteristics of each task within the buffer. Our approach achieves an average accuracy of 37.95% on split-CIFAR10, surpassing the best baseline method by 6.52%. Additionally, it significantly enhances the accuracy of the poorest-perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20808;&#39564;&#39057;&#29575;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65288;PFGDM&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#31283;&#20581;&#19988;&#20445;&#25345;&#32467;&#26500;&#30340;&#26377;&#38480;&#35282;&#24230;CBCT&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2404.01448</link><description>&lt;p&gt;
&#26377;&#38480;&#35282;&#24230;(CBCT)&#37325;&#24314;&#30340;&#20808;&#39564;&#39057;&#29575;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prior Frequency Guided Diffusion Model for Limited Angle (LA)-CBCT Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20808;&#39564;&#39057;&#29575;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65288;PFGDM&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#31283;&#20581;&#19988;&#20445;&#25345;&#32467;&#26500;&#30340;&#26377;&#38480;&#35282;&#24230;CBCT&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38181;&#26463;&#35745;&#31639;&#26426;&#20307;&#23618;&#25668;&#24433;&#65288;CBCT&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#24341;&#23548;&#25918;&#30103;&#12290;&#20174;&#26377;&#38480;&#35282;&#24230;&#37319;&#38598;&#65288;LA-CBCT&#65289;&#37325;&#24314;CBCT&#23545;&#20110;&#25552;&#39640;&#25104;&#20687;&#25928;&#29575;&#12289;&#20943;&#23569;&#21058;&#37327;&#20197;&#21450;&#26356;&#22909;&#30340;&#26426;&#26800;&#38388;&#38553;&#28165;&#38500;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;LA-CBCT&#37325;&#24314;&#21463;&#21040;&#20005;&#37325;&#27424;&#37319;&#26679;&#20266;&#24433;&#30340;&#22256;&#25200;&#65292;&#20351;&#20854;&#25104;&#20026;&#39640;&#24230;&#19981;&#36866;&#23450;&#30340;&#36870;&#38382;&#39064;&#12290;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#36870;&#36716;&#25968;&#25454;&#21152;&#22122;&#36807;&#31243;&#26469;&#29983;&#25104;&#25968;&#25454;/&#22270;&#20687;&#65307;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;LA-CBCT&#37325;&#24314;&#20013;&#30340;&#21435;&#22122;&#22120;/&#27491;&#21017;&#21270;&#22120;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20026;LA-CBCT&#37325;&#24314;&#24320;&#21457;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21363;&#20808;&#39564;&#39057;&#29575;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65288;PFGDM&#65289;&#65292;&#29992;&#20110;&#31283;&#20581;&#19988;&#20445;&#25345;&#32467;&#26500;&#30340;LA-CBCT&#37325;&#24314;&#12290;PFGDM&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;LA-CBCT&#37325;&#24314;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#26465;&#20214;&#26159;&#22522;&#20110;&#20174;&#24739;&#32773;&#29305;&#23450;&#20808;&#21069;CT&#25195;&#25551;&#20013;&#25552;&#21462;&#30340;&#39640;&#39057;&#20449;&#24687;&#65292;&#36825;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01448v1 Announce Type: cross  Abstract: Cone-beam computed tomography (CBCT) is widely used in image-guided radiotherapy. Reconstructing CBCTs from limited-angle acquisitions (LA-CBCT) is highly desired for improved imaging efficiency, dose reduction, and better mechanical clearance. LA-CBCT reconstruction, however, suffers from severe under-sampling artifacts, making it a highly ill-posed inverse problem. Diffusion models can generate data/images by reversing a data-noising process through learned data distributions; and can be incorporated as a denoiser/regularizer in LA-CBCT reconstruction. In this study, we developed a diffusion model-based framework, prior frequency-guided diffusion model (PFGDM), for robust and structure-preserving LA-CBCT reconstruction. PFGDM uses a conditioned diffusion model as a regularizer for LA-CBCT reconstruction, and the condition is based on high-frequency information extracted from patient-specific prior CT scans which provides a strong ana
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#30340;&#31232;&#30095;&#26041;&#27861;&#65292;&#25506;&#32034;&#31867;&#20284;&#20110;&#29983;&#29289;&#32593;&#32476;&#30340;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181; NLP &#20219;&#21153;&#37117;&#34920;&#29616;&#20986;&#33394;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#31232;&#30095;&#24615;&#26041;&#27861;</title><link>https://arxiv.org/abs/2404.01306</link><description>&lt;p&gt;
NeuroPrune&#65306;&#19968;&#31181;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25299;&#25169;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#30340;&#31232;&#30095;&#26041;&#27861;&#65292;&#25506;&#32034;&#31867;&#20284;&#20110;&#29983;&#29289;&#32593;&#32476;&#30340;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181; NLP &#20219;&#21153;&#37117;&#34920;&#29616;&#20986;&#33394;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#31232;&#30095;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; Transformer &#30340;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#32780;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#21464;&#24471;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#26114;&#36149;&#30340;&#35757;&#32451;&#20197;&#21450;&#25512;&#29702;&#20173;&#28982;&#26159;&#23427;&#20204;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#22312;&#27169;&#22411;&#26550;&#26500;&#30340;&#21508;&#20010;&#23618;&#27425;&#24378;&#21046;&#24341;&#20837;&#31232;&#30095;&#24615;&#24050;&#34987;&#35777;&#26126;&#26377;&#21161;&#20110;&#35299;&#20915;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#20294;&#31232;&#30095;&#24615;&#23545;&#32593;&#32476;&#25299;&#25169;&#30340;&#24433;&#21709;&#20173;&#23384;&#22312;&#26029;&#35010;&#12290;&#21463;&#22823;&#33041;&#31070;&#32463;&#32593;&#32476;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#32593;&#32476;&#25299;&#25169;&#30340;&#35270;&#35282;&#25506;&#32034;&#31232;&#30095;&#24615;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#29983;&#29289;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#26426;&#21046;&#65292;&#22914;&#20248;&#20808;&#38468;&#30528;&#21644;&#20887;&#20313;&#31361;&#35302;&#20462;&#21098;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#31232;&#30095;&#24615;&#26041;&#27861;&#22312;&#36328;&#36234;&#20998;&#31867;&#65288;&#22914;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65289;&#21644;&#29983;&#25104;&#65288;&#25688;&#35201;&#12289;&#26426;&#22120;&#32763;&#35793;&#65289;&#30340;&#21508;&#31181; NLP &#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#19988;&#39640;&#25928;&#65292;&#23613;&#31649; o
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01306v1 Announce Type: cross  Abstract: Transformer-based Language Models have become ubiquitous in Natural Language Processing (NLP) due to their impressive performance on various tasks. However, expensive training as well as inference remains a significant impediment to their widespread applicability. While enforcing sparsity at various levels of the model architecture has found promise in addressing scaling and efficiency issues, there remains a disconnect between how sparsity affects network topology. Inspired by brain neuronal networks, we explore sparsity approaches through the lens of network topology. Specifically, we exploit mechanisms seen in biological networks, such as preferential attachment and redundant synapse pruning, and show that principled, model-agnostic sparsity approaches are performant and efficient across diverse NLP tasks, spanning both classification (such as natural language inference) and generation (summarization, machine translation), despite o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;</title><link>https://arxiv.org/abs/2403.19871</link><description>&lt;p&gt;
&#36890;&#36807;&#32531;&#24930;&#21464;&#21270;&#30340;&#24207;&#21015;&#23454;&#29616;&#31283;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19871
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#36138;&#23146;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#32771;&#34385;&#36890;&#36807;&#19981;&#21516;&#30340;&#37325;&#26032;&#35757;&#32451;&#28436;&#21464;&#26469;&#20445;&#25345;&#35757;&#32451;&#27169;&#22411;&#32467;&#26500;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#36890;&#36807;&#19981;&#21516;&#30340;&#25968;&#25454;&#25209;&#27425;&#26356;&#26032;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#20445;&#30041;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265; - &#36825;&#23545;&#20110;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#23454;&#26045;&#31616;&#26131;&#24615;&#21644;&#19982;&#29992;&#25143;&#24314;&#31435;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201; - &#36890;&#36807;&#20351;&#29992;&#21487;&#20197;&#30452;&#25509;&#32435;&#20837;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#23450;&#20041;&#23450;&#20041;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;&#29983;&#20135;&#26696;&#20363;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#27604;&#36138;&#23146;&#35757;&#32451;&#27169;&#22411;&#26356;&#24378;&#30340;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19871v1 Announce Type: cross  Abstract: Retraining machine learning models remains an important task for real-world machine learning model deployment. Existing methods focus largely on greedy approaches to find the best-performing model without considering the stability of trained model structures across different retraining evolutions. In this study, we develop a mixed integer optimization algorithm that holistically considers the problem of retraining machine learning models across different data batch updates. Our method focuses on retaining consistent analytical insights - which is important to model interpretability, ease of implementation, and fostering trust with users - by using custom-defined distance metrics that can be directly incorporated into the optimization problem. Importantly, our method shows stronger stability than greedily trained models with a small, controllable sacrifice in model performance in a real-world production case study. Finally, important an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.19521</link><description>&lt;p&gt;
&#35299;&#37322;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20013;&#30340;&#20851;&#38190;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#25152;&#37319;&#29992;&#30340;&#26426;&#21046;&#12290;&#22312;&#38646;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#31867;&#20284;&#8220;&#27861;&#22269;&#30340;&#39318;&#37117;&#26159;&#8221;&#30340;&#25552;&#31034;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#24847;&#21147;&#22836;&#20250;&#20174;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20027;&#39064;&#23454;&#20307;&#65292;&#22914;&#8220;&#27861;&#22269;&#8221;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#21518;&#32493;&#30340;MLP&#20197;&#22238;&#24518;&#25152;&#38656;&#30340;&#31572;&#26696;&#65292;&#22914;&#8220;&#24052;&#40654;&#8221;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;MLP&#30340;&#36755;&#20986;&#20998;&#35299;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#32452;&#20214;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36319;&#38543;&#36825;&#20123;&#29305;&#23450;&#20219;&#21153;&#22836;&#30340;MLP&#23618;&#30340;&#21151;&#33021;&#12290;&#22312;&#27531;&#24046;&#27969;&#20013;&#65292;&#23427;&#20250;&#25830;&#38500;&#25110;&#25918;&#22823;&#26469;&#33258;&#21508;&#20010;&#22836;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#20250;&#29983;&#25104;&#19968;&#20010;&#32452;&#20214;&#65292;&#23558;&#27531;&#24046;&#27969;&#37325;&#26032;&#23450;&#21521;&#21040;&#39044;&#26399;&#31572;&#26696;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#38646;&#27425;&#26426;&#21046;&#20063;&#36866;&#29992;&#20110;&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19521v1 Announce Type: cross  Abstract: In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like "The capital of France is," task-specific attention heads extract the topic entity, such as "France," from the context and pass it to subsequent MLPs to recall the required answer such as "Paris." We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#19968;&#23450;&#26435;&#37325;&#32422;&#26463;&#30340;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#65292;&#20197;&#21450;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#20570;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#20026;&#22522;&#20110;CNNs&#30340;&#23398;&#20064;&#38382;&#39064;&#25512;&#23548;&#20102;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#21644;&#20108;&#20803;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16459</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the rates of convergence for learning with convolutional neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16459
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#19968;&#23450;&#26435;&#37325;&#32422;&#26463;&#30340;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#65292;&#20197;&#21450;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#20570;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#20026;&#22522;&#20110;CNNs&#30340;&#23398;&#20064;&#38382;&#39064;&#25512;&#23548;&#20102;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#21644;&#20108;&#20803;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#36924;&#36817;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;&#31532;&#19968;&#20010;&#32467;&#26524;&#35777;&#26126;&#20102;&#22312;&#26435;&#37325;&#19978;&#26377;&#19968;&#23450;&#32422;&#26463;&#26465;&#20214;&#19979;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#12290;&#31532;&#20108;&#20010;&#32467;&#26524;&#32473;&#20986;&#20102;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#30340;&#26032;&#20998;&#26512;&#65292;&#20854;&#20013;CNNs&#26159;&#20854;&#29305;&#20363;&#12290;&#35813;&#20998;&#26512;&#35814;&#32454;&#32771;&#34385;&#20102;&#26435;&#37325;&#30340;&#22823;&#23567;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#27604;&#29616;&#26377;&#25991;&#29486;&#26356;&#22909;&#30340;&#19978;&#30028;&#12290;&#21033;&#29992;&#36825;&#20004;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#22522;&#20110;CNNs&#30340;&#20272;&#35745;&#22120;&#22312;&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#35774;&#32622;&#20013;&#20026;&#22522;&#20110;CNNs&#30340;&#26368;&#23567;&#20108;&#20056;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#24314;&#31435;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#23545;&#20110;&#20108;&#20803;&#20998;&#31867;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20855;&#26377;&#38128;&#38142;&#25439;&#22833;&#21644;&#36923;&#36753;&#25439;&#22833;&#30340;CNN&#20998;&#31867;&#22120;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#21516;&#26102;&#36824;&#34920;&#26126;&#25152;&#24471;&#21040;&#30340;&#36895;&#29575;&#22312;&#20960;&#31181;&#24773;&#20917;&#19979;&#26159;&#26497;&#23567;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16459v1 Announce Type: new  Abstract: We study the approximation and learning capacities of convolutional neural networks (CNNs). Our first result proves a new approximation bound for CNNs with certain constraint on the weights. Our second result gives a new analysis on the covering number of feed-forward neural networks, which include CNNs as special cases. The analysis carefully takes into account the size of the weights and hence gives better bounds than existing literature in some situations. Using these two results, we are able to derive rates of convergence for estimators based on CNNs in many learning problems. In particular, we establish minimax optimal convergence rates of the least squares based on CNNs for learning smooth functions in the nonparametric regression setting. For binary classification, we derive convergence rates for CNN classifiers with hinge loss and logistic loss. It is also shown that the obtained rates are minimax optimal in several settings.
&lt;/p&gt;</description></item><item><title>GeRM&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#19987;&#23478;&#32467;&#26500;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#25968;&#25454;&#21033;&#29992;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24615;&#33021;&#38382;&#39064;&#21644;&#25968;&#25454;&#25910;&#38598;&#22256;&#38590;&#30340;&#24773;&#20917;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13358</link><description>&lt;p&gt;
GeRM&#65306;&#19968;&#31181;&#29992;&#20110;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#28151;&#21512;&#19987;&#23478;&#36890;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13358
&lt;/p&gt;
&lt;p&gt;
GeRM&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#19987;&#23478;&#32467;&#26500;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#25968;&#25454;&#21033;&#29992;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24615;&#33021;&#38382;&#39064;&#21644;&#25968;&#25454;&#25910;&#38598;&#22256;&#38590;&#30340;&#24773;&#20917;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#23398;&#20064;&#22312;&#35299;&#20915;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#24773;&#26223;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#21463;&#21040;&#24615;&#33021;&#38382;&#39064;&#21644;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#38598;&#22256;&#38590;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GeRM&#65288;&#36890;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#25968;&#25454;&#21033;&#29992;&#31574;&#30053;&#65292;&#20174;&#28436;&#31034;&#21644;&#27425;&#20248;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#36229;&#36234;&#20154;&#31867;&#28436;&#31034;&#30340;&#23616;&#38480;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;VLA&#32593;&#32476;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#24182;&#36755;&#20986;&#21160;&#20316;&#12290;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#28151;&#21512;&#32467;&#26500;&#65292;GeRM&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#25972;&#20307;&#27169;&#22411;&#23481;&#37327;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;RL&#21442;&#25968;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#22312;&#25511;&#21046;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#23454;GeRM&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;, &#32780;&#19988;&#36824;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13358v1 Announce Type: cross  Abstract: Multi-task robot learning holds significant importance in tackling diverse and complex scenarios. However, current approaches are hindered by performance issues and difficulties in collecting training datasets. In this paper, we propose GeRM (Generalist Robotic Model). We utilize offline reinforcement learning to optimize data utilization strategies to learn from both demonstrations and sub-optimal data, thus surpassing the limitations of human demonstrations. Thereafter, we employ a transformer-based VLA network to process multi-modal inputs and output actions. By introducing the Mixture-of-Experts structure, GeRM allows faster inference speed with higher whole model capacity, and thus resolves the issue of limited RL parameters, enhancing model performance in multi-task learning while controlling computational costs. Through a series of experiments, we demonstrate that GeRM outperforms other methods across all tasks, while also valid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24322;&#26500;&#28304;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#32771;&#34385;&#38544;&#31169;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.11343</link><description>&lt;p&gt;
&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Transfer Learning with Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24322;&#26500;&#28304;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#32771;&#34385;&#38544;&#31169;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#38544;&#31169;&#24615;&#26159;&#20004;&#20010;&#31361;&#20986;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#20869;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#24322;&#26500;&#28304;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#36981;&#23432;&#38544;&#31169;&#32422;&#26463;&#12290;&#25105;&#20204;&#20005;&#26684;&#21046;&#23450;&#20102;\textit{&#32852;&#37030;&#24046;&#20998;&#38544;&#31169;}&#30340;&#27010;&#24565;&#65292;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#32780;&#26080;&#38656;&#20551;&#35774;&#26377;&#19968;&#20010;&#21463;&#20449;&#20219;&#30340;&#20013;&#22830;&#26381;&#21153;&#22120;&#12290;&#22312;&#36825;&#20010;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#32463;&#20856;&#30340;&#32479;&#35745;&#38382;&#39064;&#65292;&#21363;&#21333;&#21464;&#37327;&#22343;&#20540;&#20272;&#35745;&#12289;&#20302;&#32500;&#32447;&#24615;&#22238;&#24402;&#21644;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#12290;&#36890;&#36807;&#30740;&#31350;&#26497;&#23567;&#20540;&#29575;&#24182;&#30830;&#23450;&#36825;&#20123;&#38382;&#39064;&#30340;&#38544;&#31169;&#25104;&#26412;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32852;&#37030;&#24046;&#20998;&#38544;&#31169;&#26159;&#24050;&#24314;&#31435;&#30340;&#23616;&#37096;&#21644;&#20013;&#22830;&#27169;&#22411;&#20043;&#38388;&#30340;&#19968;&#31181;&#20013;&#38388;&#38544;&#31169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11343v1 Announce Type: new  Abstract: Federated learning is gaining increasing popularity, with data heterogeneity and privacy being two prominent challenges. In this paper, we address both issues within a federated transfer learning framework, aiming to enhance learning on a target data set by leveraging information from multiple heterogeneous source data sets while adhering to privacy constraints. We rigorously formulate the notion of \textit{federated differential privacy}, which offers privacy guarantees for each data set without assuming a trusted central server. Under this privacy constraint, we study three classical statistical problems, namely univariate mean estimation, low-dimensional linear regression, and high-dimensional linear regression. By investigating the minimax rates and identifying the costs of privacy for these problems, we show that federated differential privacy is an intermediate privacy model between the well-established local and central models of 
&lt;/p&gt;</description></item><item><title>PeerAiD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#21516;&#34892;&#32593;&#32476;&#23398;&#20064;&#23398;&#29983;&#32593;&#32476;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#33258;&#36523;&#30340;&#31034;&#20363;&#65292;&#26469;&#25552;&#21319;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06668</link><description>&lt;p&gt;
PeerAiD&#65306;&#25913;&#21892;&#19987;&#19994;&#21516;&#34892;&#23548;&#24072;&#30340;&#23545;&#25239;&#24615;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06668
&lt;/p&gt;
&lt;p&gt;
PeerAiD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#21516;&#34892;&#32593;&#32476;&#23398;&#20064;&#23398;&#29983;&#32593;&#32476;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#33258;&#36523;&#30340;&#31034;&#20363;&#65292;&#26469;&#25552;&#21319;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#22312;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#26102;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23545;&#25239;&#24615;&#33976;&#39311;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#65292;&#26088;&#22312;&#25552;&#28860;&#25945;&#24072;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#25913;&#36827;&#23567;&#22411;&#23398;&#29983;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PeerAiD&#65292;&#36890;&#36807;&#35753;&#21516;&#34892;&#32593;&#32476;&#23398;&#20064;&#23398;&#29983;&#32593;&#32476;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#33258;&#36523;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26469;&#25913;&#36827;&#23545;&#25239;&#24615;&#33976;&#39311;&#12290;PeerAiD&#26159;&#19968;&#31181;&#23545;&#25239;&#24615;&#33976;&#39311;&#65292;&#21516;&#26102;&#35757;&#32451;&#21516;&#34892;&#32593;&#32476;&#21644;&#23398;&#29983;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06668v1 Announce Type: new  Abstract: Adversarial robustness of the neural network is a significant concern when it is applied to security-critical domains. In this situation, adversarial distillation is a promising option which aims to distill the robustness of the teacher network to improve the robustness of a small student network. Previous works pretrain the teacher network to make it robust to the adversarial examples aimed at itself. However, the adversarial examples are dependent on the parameters of the target network. The fixed teacher network inevitably degrades its robustness against the unseen transferred adversarial examples which targets the parameters of the student network in the adversarial distillation process. We propose PeerAiD to make a peer network learn the adversarial examples of the student network instead of adversarial examples aimed at itself. PeerAiD is an adversarial distillation that trains the peer network and the student network simultaneousl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;Thompson&#25277;&#26679;&#36817;&#20284;&#30340;&#36951;&#25022;&#19982;&#19981;&#30830;&#23450;&#24615;&#27604;&#29575;&#65292;&#25104;&#21151;&#21327;&#35843;&#27599;&#20010;&#25209;&#27425;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#27010;&#29575;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#38750;&#20984;&#27979;&#35797;&#20989;&#25968;&#19978;&#34920;&#29616;&#20986;&#33394;.</title><link>https://arxiv.org/abs/2403.04764</link><description>&lt;p&gt;
&#23558;Thompson&#25277;&#26679;&#36951;&#25022;&#19982;Sigma&#27604;&#29575;&#65288;TS-RSR&#65289;&#26368;&#23567;&#21270;&#65306;&#19968;&#31181;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32463;&#36807;&#35777;&#26126;&#30340;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;Thompson&#25277;&#26679;&#36817;&#20284;&#30340;&#36951;&#25022;&#19982;&#19981;&#30830;&#23450;&#24615;&#27604;&#29575;&#65292;&#25104;&#21151;&#21327;&#35843;&#27599;&#20010;&#25209;&#27425;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#27010;&#29575;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#38750;&#20984;&#27979;&#35797;&#20989;&#25968;&#19978;&#34920;&#29616;&#20986;&#33394;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#65292;&#20854;&#20013;&#25277;&#26679;&#36890;&#36807;&#26368;&#23567;&#21270;Thompson&#25277;&#26679;&#26041;&#27861;&#30340;&#36951;&#25022;&#19982;&#19981;&#30830;&#23450;&#24615;&#27604;&#29575;&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#33021;&#22815;&#21327;&#35843;&#27599;&#20010;&#25209;&#27425;&#20013;&#36873;&#25321;&#30340;&#21160;&#20316;&#65292;&#20197;&#26368;&#23567;&#21270;&#28857;&#20043;&#38388;&#30340;&#20887;&#20313;&#65292;&#21516;&#26102;&#20851;&#27880;&#20855;&#26377;&#39640;&#39044;&#27979;&#22343;&#20540;&#25110;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#28857;&#12290;&#25105;&#20204;&#23545;&#31639;&#27861;&#30340;&#36951;&#25022;&#25552;&#20379;&#20102;&#39640;&#27010;&#29575;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#20174;&#25968;&#23383;&#19978;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#38750;&#20984;&#27979;&#35797;&#20989;&#25968;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#24179;&#22343;&#20540;&#19978;&#27604;&#20960;&#20010;&#31454;&#20105;&#23545;&#25163;&#30340;&#22522;&#20934;&#25209;&#37327;BO&#31639;&#27861;&#34920;&#29616;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04764v1 Announce Type: new  Abstract: This paper presents a new approach for batch Bayesian Optimization (BO), where the sampling takes place by minimizing a Thompson Sampling approximation of a regret to uncertainty ratio. Our objective is able to coordinate the actions chosen in each batch in a way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty. We provide high-probability theoretical guarantees on the regret of our algorithm. Finally, numerically, we demonstrate that our method attains state-of-the-art performance on a range of nonconvex test functions, where it outperforms several competitive benchmark batch BO algorithms by an order of magnitude on average.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Hamiltonian&#30340;&#26412;&#22320;&#24615;&#27979;&#35797;&#20316;&#20026;&#23646;&#24615;&#27979;&#35797;&#38382;&#39064;&#65292;&#37325;&#28857;&#22312;&#20110;&#30830;&#23450;&#26410;&#30693;&#30340;$n$&#27604;&#29305;Hamiltonian&#26159;&#21542;&#26159;$k$&#23616;&#37096;&#30340;&#65292;&#36890;&#36807;&#23545;$H$&#30340;&#26102;&#38388;&#28436;&#21270;&#36827;&#34892;&#35775;&#38382;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02968</link><description>&lt;p&gt;
Hamiltonian&#24615;&#36136;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Hamiltonian Property Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Hamiltonian&#30340;&#26412;&#22320;&#24615;&#27979;&#35797;&#20316;&#20026;&#23646;&#24615;&#27979;&#35797;&#38382;&#39064;&#65292;&#37325;&#28857;&#22312;&#20110;&#30830;&#23450;&#26410;&#30693;&#30340;$n$&#27604;&#29305;Hamiltonian&#26159;&#21542;&#26159;$k$&#23616;&#37096;&#30340;&#65292;&#36890;&#36807;&#23545;$H$&#30340;&#26102;&#38388;&#28436;&#21270;&#36827;&#34892;&#35775;&#38382;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Hamiltonian&#26412;&#22320;&#24615;&#27979;&#35797;&#20316;&#20026;&#19968;&#20010;&#23646;&#24615;&#27979;&#35797;&#38382;&#39064;&#65292;&#21363;&#30830;&#23450;&#19968;&#20010;&#26410;&#30693;&#30340;$n$&#27604;&#29305;Hamiltonian $H$&#26159;&#21542;&#26159;$k$&#23616;&#37096;&#30340;&#65292;&#25110;&#32773;&#19982;&#25152;&#26377;$k$&#23616;&#37096;Hamiltonian&#37117;&#30456;&#36317;$\varepsilon$&#65292;&#24182;&#36890;&#36807;&#23545;$H$&#30340;&#26102;&#38388;&#28436;&#21270;&#36827;&#34892;&#35775;&#38382;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02968v1 Announce Type: cross  Abstract: Locality is a fundamental feature of many physical time evolutions. Assumptions on locality and related structural properties also underlie recently proposed procedures for learning an unknown Hamiltonian from access to the induced time evolution. However, no protocols to rigorously test whether an unknown Hamiltonian is local were known. We investigate Hamiltonian locality testing as a property testing problem, where the task is to determine whether an unknown $n$-qubit Hamiltonian $H$ is $k$-local or $\varepsilon$-far from all $k$-local Hamiltonians, given access to the time evolution along $H$. First, we emphasize the importance of the chosen distance measure: With respect to the operator norm, a worst-case distance measure, incoherent quantum locality testers require $\tilde{\Omega}(2^n)$ many time evolution queries and an expected total evolution time of $\tilde{\Omega}(2^n / \varepsilon)$, and even coherent testers need $\Omega(2
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;Dynamic Dataset Generator&#65288;DDG&#65289;&#26469;&#35299;&#20915;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#32858;&#31867;&#26102;&#32570;&#20047;&#22810;&#26679;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#29616;&#23454;&#24615;&#21160;&#24577;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24110;&#21161;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15731</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#32858;&#31867;&#65306;&#20855;&#26377;&#24322;&#36136;&#24615;&#21464;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Clustering in Dynamic Environments: A Framework for Benchmark Dataset Generation With Heterogeneous Changes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;Dynamic Dataset Generator&#65288;DDG&#65289;&#26469;&#35299;&#20915;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#32858;&#31867;&#26102;&#32570;&#20047;&#22810;&#26679;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#29616;&#23454;&#24615;&#21160;&#24577;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24110;&#21161;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#32858;&#31867;&#26159;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#20174;&#23454;&#26102;&#25968;&#25454;&#20998;&#26512;&#21644;&#22312;&#32447;&#26080;&#30417;&#30563;&#23398;&#20064;&#21040;&#21160;&#24577;&#35774;&#26045;&#23450;&#20301;&#38382;&#39064;&#12290;&#23613;&#31649;&#20803;&#21551;&#21457;&#24335;&#22312;&#38745;&#24577;&#32858;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36319;&#36394;&#26368;&#20339;&#32858;&#31867;&#35299;&#20915;&#26041;&#26696;&#25110;&#22312;&#26102;&#38388;&#19978;&#31283;&#20581;&#22320;&#36827;&#34892;&#32858;&#31867;&#30340;&#24212;&#29992;&#20173;&#28982;&#24456;&#23569;&#25506;&#35752;&#12290;&#36825;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#32570;&#20047;&#20855;&#26377;&#22810;&#26679;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#29616;&#23454;&#24615;&#21160;&#24577;&#29305;&#24449;&#30340;&#21160;&#24577;&#25968;&#25454;&#38598;&#65292;&#38459;&#30861;&#20102;&#22312;&#21508;&#31181;&#21160;&#24577;&#22330;&#26223;&#20013;&#23545;&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#31995;&#32479;&#24615;&#24615;&#33021;&#35780;&#20272;&#12290;&#36825;&#31181;&#32570;&#38519;&#23548;&#33268;&#25105;&#20204;&#23545;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#35774;&#35745;&#32858;&#31867;&#31639;&#27861;&#30340;&#29702;&#35299;&#21644;&#33021;&#21147;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#21160;&#24577;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65288;DDG&#65289;&#12290;DDG&#20855;&#26377;&#22810;&#20010;&#21160;&#24577;&#39640;&#26031;&#32452;&#20214;&#65292;&#38598;&#25104;&#20102;&#19968;&#31995;&#21015;&#24322;&#36136;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15731v1 Announce Type: new  Abstract: Clustering in dynamic environments is of increasing importance, with broad applications ranging from real-time data analysis and online unsupervised learning to dynamic facility location problems. While meta-heuristics have shown promising effectiveness in static clustering tasks, their application for tracking optimal clustering solutions or robust clustering over time in dynamic environments remains largely underexplored. This is partly due to a lack of dynamic datasets with diverse, controllable, and realistic dynamic characteristics, hindering systematic performance evaluations of clustering algorithms in various dynamic scenarios. This deficiency leads to a gap in our understanding and capability to effectively design algorithms for clustering in dynamic environments. To bridge this gap, this paper introduces the Dynamic Dataset Generator (DDG). DDG features multiple dynamic Gaussian components integrated with a range of heterogeneo
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10877</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#26234;&#33021;&#20307;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust agents learn causal world models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10877
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#30452;&#26377;&#20154;&#20551;&#35774;&#22240;&#26524;&#25512;&#29702;&#22312;&#24378;&#20581;&#19988;&#20855;&#26377;&#36890;&#29992;&#26234;&#33021;&#20013;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#65292;&#28982;&#32780;&#19981;&#28165;&#26970;&#26234;&#33021;&#20307;&#26159;&#21542;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#25110;&#32773;&#20854;&#20182;&#24402;&#32435;&#20559;&#24046;&#26159;&#21542;&#36275;&#22815;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#34920;&#26126;&#20219;&#20309;&#33021;&#22815;&#22312;&#22823;&#37327;&#20998;&#24067;&#36716;&#21464;&#19979;&#28385;&#36275;&#21518;&#24724;&#30028;&#38480;&#30340;&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#36817;&#20284;&#22240;&#26524;&#27169;&#22411;&#65292;&#23545;&#20110;&#20248;&#21270;&#26234;&#33021;&#20307;&#26469;&#35828;&#65292;&#35813;&#36817;&#20284;&#27169;&#22411;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#25324;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10877v1 Announce Type: new  Abstract: It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23454;&#29616;&#30340;&#29983;&#29289;&#23398;&#21487;&#34892;&#26041;&#27861;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#20943;&#36731;&#20102;&#24378;&#21270;&#23398;&#20064;&#25152;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#22122;&#22768;&#24341;&#20837;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.10069</link><description>&lt;p&gt;
&#23398;&#20064;&#24555;&#36895;&#21464;&#21270;&#30340;&#24930;&#24615;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;
&lt;/p&gt;
&lt;p&gt;
Learning fast changing slow in spiking neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10069
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23454;&#29616;&#30340;&#29983;&#29289;&#23398;&#21487;&#34892;&#26041;&#27861;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#20943;&#36731;&#20102;&#24378;&#21270;&#23398;&#20064;&#25152;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#22122;&#22768;&#24341;&#20837;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#38382;&#39064;&#20013;&#38754;&#20020;&#30528;&#24456;&#22823;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#20110;&#19982;&#29615;&#22659;&#30340;&#26377;&#38480;&#20132;&#20114;&#23548;&#33268;&#30340;&#21487;&#29992;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290; RL&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#26469;&#36827;&#34892;&#26377;&#25928;&#30340;&#23398;&#20064;&#65292;&#36825;&#20351;&#24471;&#22797;&#26434;&#24615;&#36827;&#19968;&#27493;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#20351;&#29992;&#24490;&#29615;&#33033;&#20914;&#32593;&#32476;&#23454;&#29616;RL&#26102;&#65292;&#33033;&#20914;&#24341;&#20837;&#30340;&#22266;&#26377;&#22122;&#22768;&#22686;&#21152;&#20102;&#38590;&#24230;&#12290;&#32456;&#36523;&#23398;&#20064;&#26426;&#22120;&#22312;&#26412;&#36136;&#19978;&#24517;&#39035;&#35299;&#20915;&#21487;&#22609;&#24615;-&#31283;&#23450;&#24615;&#24726;&#35770;&#12290;&#22312;&#33719;&#24471;&#26032;&#30693;&#35782;&#21644;&#20445;&#25345;&#31283;&#23450;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20174;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29983;&#29289;&#21487;&#34892;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23454;&#29616;&#65292;&#35748;&#20026;&#23427;&#26174;&#33879;&#20943;&#36731;&#20102;&#27492;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24102;&#26469;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#36827;&#23637;&#65306;&#39318;&#20808;&#65292;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10069v1 Announce Type: cross  Abstract: Reinforcement learning (RL) faces substantial challenges when applied to real-life problems, primarily stemming from the scarcity of available data due to limited interactions with the environment. This limitation is exacerbated by the fact that RL often demands a considerable volume of data for effective learning. The complexity escalates further when implementing RL in recurrent spiking networks, where inherent noise introduced by spikes adds a layer of difficulty. Life-long learning machines must inherently resolve the plasticity-stability paradox. Striking a balance between acquiring new knowledge and maintaining stability is crucial for artificial agents. In this context, we take inspiration from machine learning technology and introduce a biologically plausible implementation of proximal policy optimization, arguing that it significantly alleviates this challenge. Our approach yields two notable advancements: first, the ability t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#24179;&#34913;&#31639;&#27861;&#25512;&#29702;&#22120;&#65292;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#25214;&#21040;&#31639;&#27861;&#30340;&#24179;&#34913;&#28857;&#26469;&#35757;&#32451;&#32593;&#32476;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06445</link><description>&lt;p&gt;
&#28145;&#24230;&#24179;&#34913;&#31639;&#27861;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
The Deep Equilibrium Algorithmic Reasoner
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#24179;&#34913;&#31639;&#27861;&#25512;&#29702;&#22120;&#65292;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#25214;&#21040;&#31639;&#27861;&#30340;&#24179;&#34913;&#28857;&#26469;&#35757;&#32451;&#32593;&#32476;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21487;&#20197;&#23398;&#20064;&#25191;&#34892;&#32463;&#20856;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#19968;&#30452;&#20351;&#29992;&#30340;&#26159;&#36882;&#24402;&#26550;&#26500;&#65292;&#20854;&#20013;&#27599;&#20010;GNN&#30340;&#36845;&#20195;&#19982;&#31639;&#27861;&#30340;&#36845;&#20195;&#19968;&#33268;&#12290;&#30001;&#20110;&#31639;&#27861;&#30340;&#35299;&#36890;&#24120;&#26159;&#19968;&#20010;&#24179;&#34913;&#28857;&#65292;&#25105;&#20204;&#29468;&#27979;&#24182;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#65292;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#25214;&#21040;&#24179;&#34913;&#28857;&#26469;&#35757;&#32451;&#32593;&#32476;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;&#12290;&#27880;&#24847;&#65292;&#36825;&#19981;&#38656;&#35201;&#23558;&#27599;&#20010;GNN&#30340;&#36845;&#20195;&#19982;&#31639;&#27861;&#30340;&#27493;&#39588;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on neural algorithmic reasoning has demonstrated that graph neural networks (GNNs) could learn to execute classical algorithms. Doing so, however, has always used a recurrent architecture, where each iteration of the GNN aligns with an algorithm's iteration. Since an algorithm's solution is often an equilibrium, we conjecture and empirically validate that one can train a network to solve algorithmic problems by directly finding the equilibrium. Note that this does not require matching each GNN iteration with a step of the algorithm.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#25511;&#21046;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01810</link><description>&lt;p&gt;
&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#30340;&#38169;&#35823;&#35268;&#33539;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Misspecification uncertainties in near-deterministic regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01810
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#25511;&#21046;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26395;&#25439;&#22833;&#26159;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#21487;&#29992;&#20110;&#23398;&#20064;&#30340;&#40065;&#26834;PAC-Bayes&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#25439;&#22833;&#26368;&#23567;&#21270;&#34987;&#35748;&#20026;&#24573;&#30053;&#20102;&#38169;&#35823;&#35268;&#33539;&#21270;&#65292;&#21363;&#27169;&#22411;&#19981;&#33021;&#23436;&#20840;&#22797;&#21046;&#35266;&#27979;&#32467;&#26524;&#12290;&#36825;&#23548;&#33268;&#22823;&#25968;&#25454;&#25110;&#27424;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#23545;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#33879;&#20302;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#36817;&#30830;&#23450;&#24615;&#12289;&#38169;&#35823;&#35268;&#33539;&#21270;&#21644;&#27424;&#21442;&#25968;&#21270;&#26367;&#20195;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36825;&#26159;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#24191;&#27867;&#30456;&#20851;&#30340;&#19968;&#20010;&#39046;&#22495;&#12290;&#25105;&#20204;&#35777;&#26126;&#21518;&#39564;&#20998;&#24067;&#24517;&#39035;&#35206;&#30422;&#27599;&#20010;&#35757;&#32451;&#28857;&#65292;&#20197;&#36991;&#20813;&#21457;&#25955;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#23548;&#20986;&#19968;&#20010;&#31526;&#21512;&#36825;&#20010;&#32422;&#26463;&#30340;&#32452;&#21512;&#27169;&#22411;&#12290;&#23545;&#20110;&#32447;&#24615;&#27169;&#22411;&#65292;&#36825;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#39069;&#22806;&#24320;&#38144;&#26368;&#23567;&#12290;&#36825;&#31181;&#39640;&#25928;&#26041;&#27861;&#22312;&#27169;&#22411;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#21407;&#23376;&#23610;&#24230;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expected loss is an upper bound to the model generalization error which admits robust PAC-Bayes bounds for learning. However, loss minimization is known to ignore misspecification, where models cannot exactly reproduce observations. This leads to significant underestimates of parameter uncertainties in the large data, or underparameterized, limit. We analyze the generalization error of near-deterministic, misspecified and underparametrized surrogate models, a regime of broad relevance in science and engineering. We show posterior distributions must cover every training point to avoid a divergent generalization error and derive an ensemble {ansatz} that respects this constraint, which for linear models incurs minimal overhead. The efficient approach is demonstrated on model problems before application to high dimensional datasets in atomistic machine learning. Parameter uncertainties from misspecification survive in the underparametrized limit, giving accurate prediction and boundin
&lt;/p&gt;</description></item><item><title>BEND&#26159;&#19968;&#20010;&#38024;&#23545;DNA&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;&#19968;&#31995;&#21015;&#22312;&#20154;&#31867;&#22522;&#22240;&#32452;&#19978;&#23450;&#20041;&#30340;&#29616;&#23454;&#19988;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2311.12570</link><description>&lt;p&gt;
BEND: &#22312;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#20219;&#21153;&#19978;&#23545;DNA&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BEND: Benchmarking DNA Language Models on biologically meaningful tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12570
&lt;/p&gt;
&lt;p&gt;
BEND&#26159;&#19968;&#20010;&#38024;&#23545;DNA&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;&#19968;&#31995;&#21015;&#22312;&#20154;&#31867;&#22522;&#22240;&#32452;&#19978;&#23450;&#20041;&#30340;&#29616;&#23454;&#19988;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#32452;&#24207;&#21015;&#21253;&#21547;&#20102;&#25351;&#23548;&#32454;&#32990;&#36807;&#31243;&#30340;&#34013;&#22270;&#12290;&#23613;&#31649;&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#22522;&#22240;&#32452;&#30340;&#21487;&#29992;&#24615;&#22823;&#22823;&#22686;&#21152;&#65292;&#20294;&#23545;DNA&#24207;&#21015;&#20013;&#32534;&#30721;&#30340;&#21508;&#31181;&#21151;&#33021;&#24615;&#12289;&#38750;&#32534;&#30721;&#21644;&#35843;&#33410;&#20803;&#32032;&#36827;&#34892;&#23454;&#39564;&#27880;&#37322;&#20173;&#26082;&#26114;&#36149;&#21448;&#20855;&#25361;&#25112;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#22522;&#22240;&#32452;DNA&#36827;&#34892;&#26080;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#30340;&#20852;&#36259;&#65292;&#36825;&#31181;&#33539;&#24335;&#22312;&#34507;&#30333;&#24207;&#21015;&#25968;&#25454;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#21508;&#31181;DNA&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#20010;&#21035;&#20316;&#21697;&#20043;&#38388;&#30340;&#35780;&#20272;&#20219;&#21153;&#24448;&#24448;&#19981;&#21516;&#65292;&#24182;&#19988;&#21487;&#33021;&#26080;&#27861;&#23436;&#20840;&#22797;&#21046;&#22522;&#22240;&#32452;&#27880;&#37322;&#30340;&#22522;&#26412;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#30340;&#38271;&#24230;&#12289;&#35268;&#27169;&#21644;&#31232;&#30095;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BEND&#65292;&#19968;&#20010;&#38024;&#23545;DNA&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#23450;&#20041;&#22312;&#20154;&#31867;&#22522;&#22240;&#32452;&#19978;&#30340;&#29616;&#23454;&#21644;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;DNA&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21487;&#20197;&#25509;&#36817;pe
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12570v3 Announce Type: replace-cross  Abstract: The genome sequence contains the blueprint for governing cellular processes. While the availability of genomes has vastly increased over the last decades, experimental annotation of the various functional, non-coding and regulatory elements encoded in the DNA sequence remains both expensive and challenging. This has sparked interest in unsupervised language modeling of genomic DNA, a paradigm that has seen great success for protein sequence data. Although various DNA language models have been proposed, evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data. In this study, we introduce BEND, a Benchmark for DNA language models, featuring a collection of realistic and biologically meaningful downstream tasks defined on the human genome. We find that embeddings from current DNA LMs can approach pe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36130;&#20135;&#35777;&#26126;&#27010;&#24565;&#65292;&#21487;&#20197;&#22312;&#19981;&#27844;&#38706;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21521;&#39564;&#35777;&#32773;&#23637;&#31034;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2308.09552</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#29305;&#24615;&#30340;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Attesting Distributional Properties of Training Data for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.09552
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36130;&#20135;&#35777;&#26126;&#27010;&#24565;&#65292;&#21487;&#20197;&#22312;&#19981;&#27844;&#38706;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21521;&#39564;&#35777;&#32773;&#23637;&#31034;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#25104;&#21151;&#20276;&#38543;&#30528;&#23545;&#20854;&#21487;&#20449;&#24230;&#30340;&#22686;&#21152;&#20851;&#27880;&#12290;&#19968;&#20123;&#21496;&#27861;&#31649;&#36758;&#21306;&#27491;&#22312;&#20934;&#22791;&#26426;&#22120;&#23398;&#20064;&#30417;&#31649;&#26694;&#26550;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#27880;&#28857;&#26159;&#30830;&#20445;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#26576;&#20123;&#29305;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#29702;&#24819;&#20998;&#24067;&#29305;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36130;&#20135;&#35777;&#26126;&#30340;&#27010;&#24565;&#65292;&#20801;&#35768;&#35777;&#26126;&#32773;&#65288;&#20363;&#22914;&#65292;&#27169;&#22411;&#35757;&#32451;&#32773;&#65289;&#21521;&#39564;&#35777;&#32773;&#65288;&#20363;&#22914;&#65292;&#23458;&#25143;&#65289;&#23637;&#31034;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20851;&#20998;&#24067;&#29305;&#24615;&#65292;&#32780;&#19981;&#27844;&#38706;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#28151;&#21512;&#36130;&#20135;&#35777;&#26126;&#65292;&#32467;&#21512;&#20102;&#36130;&#20135;&#25512;&#29702;&#19982;&#21152;&#23494;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.09552v3 Announce Type: replace-cross  Abstract: The success of machine learning (ML) has been accompanied by increased concerns about its trustworthiness. Several jurisdictions are preparing ML regulatory frameworks. One such concern is ensuring that model training data has desirable distributional properties for certain sensitive attributes. For example, draft regulations indicate that model trainers are required to show that training datasets have specific distributional properties, such as reflecting diversity of the population. We propose the notion of property attestation allowing a prover (e.g., model trainer) to demonstrate relevant distributional properties of training data to a verifier (e.g., a customer) without revealing the data. We present an effective hybrid property attestation combining property inference with cryptographic mechanisms.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;3-WL&#22270;&#21516;&#26500;&#27979;&#35797;&#20110;&#28857;&#20113;&#30340;Gram&#30697;&#38453;&#65292;&#25110;&#32773;&#24212;&#29992;&#27431;&#20960;&#37324;&#24471;2-WL&#27979;&#35797;&#65292;&#22312;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#20869;&#23454;&#29616;&#23545;&#23436;&#20840;&#27431;&#20960;&#37324;&#24471;&#22270;&#30340;&#23436;&#20840;&#30830;&#23450;&#24615;</title><link>https://arxiv.org/abs/2301.13821</link><description>&lt;p&gt;
&#23436;&#20840;&#27431;&#20960;&#37324;&#24471;&#22270;&#30340;&#23436;&#20840;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Complete Neural Networks for Complete Euclidean Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13821
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;3-WL&#22270;&#21516;&#26500;&#27979;&#35797;&#20110;&#28857;&#20113;&#30340;Gram&#30697;&#38453;&#65292;&#25110;&#32773;&#24212;&#29992;&#27431;&#20960;&#37324;&#24471;2-WL&#27979;&#35797;&#65292;&#22312;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#20869;&#23454;&#29616;&#23545;&#23436;&#20840;&#27431;&#20960;&#37324;&#24471;&#22270;&#30340;&#23436;&#20840;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#23545;&#28857;&#20113;&#24314;&#27169;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22240;&#20026;&#23427;&#20204;&#23562;&#37325;&#25490;&#21015;&#21644;&#21018;&#24615;&#36816;&#21160;&#30340;&#33258;&#28982;&#19981;&#21464;&#24615;&#65292;&#20174;&#20998;&#23376;&#21160;&#21147;&#23398;&#21040;&#25512;&#33616;&#31995;&#32479;&#65292;&#20174;&#20960;&#20309;&#29616;&#35937;&#21040;&#25512;&#33616;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#26410;&#30693;&#20855;&#26377;&#22810;&#39033;&#24335;&#22797;&#26434;&#24615;&#30340;&#27169;&#22411;&#26159;&#23436;&#22791;&#30340;&#65292;&#21363;&#33021;&#22815;&#21306;&#20998;&#20219;&#24847;&#19968;&#23545;&#38750;&#21516;&#26500;&#28857;&#20113;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#21487;&#20197;&#23558;&#28857;&#20113;&#23436;&#20840;&#30830;&#23450;&#65292;&#30452;&#21040;&#25490;&#21015;&#21644;&#21018;&#24615;&#36816;&#21160;&#65292;&#36890;&#36807;&#23558;3-WL&#22270;&#21516;&#26500;&#27979;&#35797;&#24212;&#29992;&#20110;&#28857;&#20113;&#30340;&#38598;&#20013;Gram&#30697;&#38453;&#26469;&#22635;&#34917;&#36825;&#19968;&#29702;&#35770;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;2-WL&#27979;&#35797;&#30340;&#27431;&#20960;&#37324;&#24471;&#21464;&#20307;&#65292;&#24182;&#23637;&#31034;&#23427;&#20063;&#36275;&#20197;&#23454;&#29616;&#23436;&#25972;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20013;&#31561;&#22823;&#23567;&#30340;&#27431;&#20960;&#37324;&#24471;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#25105;&#20204;&#30340;&#23436;&#20840;&#27431;&#20960;&#37324;&#24471;WL&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#39640;&#24230;&#23545;&#31216;&#30340;&#28857;&#20113;&#19978;&#30340;&#20998;&#31163;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13821v3 Announce Type: replace  Abstract: Neural networks for point clouds, which respect their natural invariance to permutation and rigid motion, have enjoyed recent success in modeling geometric phenomena, from molecular dynamics to recommender systems. Yet, to date, no model with polynomial complexity is known to be complete, that is, able to distinguish between any pair of non-isomorphic point clouds. We fill this theoretical gap by showing that point clouds can be completely determined, up to permutation and rigid motion, by applying the 3-WL graph isomorphism test to the point cloud's centralized Gram matrix. Moreover, we formulate an Euclidean variant of the 2-WL test and show that it is also sufficient to achieve completeness. We then show how our complete Euclidean WL tests can be simulated by an Euclidean graph neural network of moderate size and demonstrate their separation capability on highly symmetrical point clouds.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20026;&#20363;&#65292;&#21457;&#24067;&#22312;GitHub&#21644;Hugging Face&#19978;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.16640</link><description>&lt;p&gt;
TeenyTinyLlama&#65306;&#22522;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35757;&#32451;&#30340;&#24320;&#28304;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese. (arXiv:2401.16640v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16640
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20026;&#20363;&#65292;&#21457;&#24067;&#22312;GitHub&#21644;Hugging Face&#19978;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#30340;&#36827;&#23637;&#36824;&#19981;&#24179;&#34913;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;LLMs&#26159;&#22312;&#20687;&#33521;&#35821;&#36825;&#26679;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#65292;&#20294;&#22810;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#31245;&#24046;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#22810;&#35821;&#35328;&#22522;&#30784;&#26377;&#26102;&#20250;&#38480;&#21046;&#23427;&#20204;&#20135;&#29983;&#30340;&#21103;&#20135;&#21697;&#65292;&#22914;&#35745;&#31639;&#38656;&#27714;&#21644;&#35768;&#21487;&#21046;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#20026;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20351;&#29992;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#36807;&#31243;&#12289;&#20854;&#23616;&#38480;&#24615;&#21644;&#20248;&#21183;&#12290;&#36825;&#23601;&#26159;TeenyTinyLlama&#65306;&#20004;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#29983;&#25104;&#30340;&#32039;&#20945;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;GitHub&#21644;Hugging Face&#19978;&#20197;&#23485;&#26494;&#30340;Apache 2.0&#35768;&#21487;&#35777;&#21457;&#24067;&#23427;&#20204;&#65292;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;&#35814;&#35265;https://github.com/Nkluge-correa/TeenyTinyLlama
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced natural language processing, but their progress has yet to be equal across languages. While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes. In this study, we document the development of open-foundation models tailored for use in low-resource settings, their limitations, and their benefits. This is the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation. We release them under the permissive Apache 2.0 license on GitHub and Hugging Face for community use and further development. See https://github.com/Nkluge-correa/TeenyTinyLlama
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;TTMs&#27169;&#22411;&#26356;&#23567;&#12289;&#26356;&#24555;&#65292;&#24182;&#32771;&#34385;&#20102;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.03955</link><description>&lt;p&gt;
&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs): &#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#22686;&#24378;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#30340;&#24555;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;TTMs&#27169;&#22411;&#26356;&#23567;&#12289;&#26356;&#24555;&#65292;&#24182;&#32771;&#34385;&#20102;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015; (TS) &#20013;&#38754;&#20020;&#30528;&#22810;&#26679;&#24615;&#21644;&#20844;&#24320;&#39044;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#21508;&#31181;&#36866;&#24212;&#30340;&#36235;&#21183;&#36880;&#28176;&#22686;&#21152;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#36328;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#65292;&#20986;&#22855;&#22320;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38750;&#24120;&#32531;&#24930;&#19988;&#24222;&#22823;&#65288;&#22823;&#32422;&#21313;&#20159;&#20010;&#21442;&#25968;&#65289;&#65292;&#24182;&#19988;&#19981;&#32771;&#34385;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23618;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTM)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423; TSMixer &#32467;&#26500;&#30340;&#26174;&#33879;&#23567;&#22411;&#27169;&#22411;&#12290;TTM &#26159;&#39318;&#20010;&#25104;&#21151;&#24320;&#21457;&#30340;&#24494;&#22411;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#8804;100&#19975;&#20010;&#21442;&#25968;&#65289;&#65292;&#19987;&#38376;&#22312;&#20844;&#24320;TS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#65288;&#20165;&#38656;4-8&#23567;&#26102;&#65289;&#65292;&#20855;&#26377;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Pretrained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pretraining data. Consequently, there has been a recent surge in utilizing pretrained large language models (LLMs) with various adaptations for time series forecasting. These approaches employ cross-domain transfer learning and surprisingly yield impressive results. However, these models are typically very slow and large ($\sim$billion parameters) and do not consider cross-channel correlations. To address this, we present Multi-level Tiny Time Mixers (TTM), a significantly small model based on the lightweight TSMixer architecture. TTM marks the first success in developing tiny general-pretrained models ($\le$1 million parameters), exclusively trained on public TS datasets in a flash of just 4-8 hrs with effective transfer learning capabilities for forecasting. To tackle the complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedQV&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#20108;&#27425;&#25237;&#31080;&#26426;&#21046;&#30340;&#26032;&#22411;&#32858;&#21512;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#23481;&#26131;&#21463;&#21040;&#27745;&#26579;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;FedQV&#26159;&#19968;&#20010;&#30495;&#23454;&#26426;&#21046;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#21305;&#37197;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01168</link><description>&lt;p&gt;
FedQV: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#20108;&#27425;&#25237;&#31080;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FedQV: Leveraging Quadratic Voting in Federated Learning. (arXiv:2401.01168v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedQV&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#20108;&#27425;&#25237;&#31080;&#26426;&#21046;&#30340;&#26032;&#22411;&#32858;&#21512;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#23481;&#26131;&#21463;&#21040;&#27745;&#26579;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;FedQV&#26159;&#19968;&#20010;&#30495;&#23454;&#26426;&#21046;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#21305;&#37197;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#19981;&#21516;&#26041;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#36879;&#38706;&#21508;&#33258;&#30340;&#26412;&#22320;&#26631;&#31614;&#12290;FL&#30340;&#20851;&#38190;&#27493;&#39588;&#20043;&#19968;&#26159;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#25104;&#20840;&#23616;&#27169;&#22411;&#65292;&#36825;&#19982;&#20844;&#20849;&#20915;&#31574;&#65292;&#23588;&#20854;&#26159;&#36873;&#20030;&#65292;&#26377;&#24456;&#22810;&#30456;&#20284;&#20043;&#22788;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;FL&#30340;&#19968;&#20010;&#20027;&#35201;&#24369;&#28857;&#26159;&#20854;&#26131;&#21463;&#27745;&#26579;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#29616;&#20195;&#32858;&#21512;&#35268;&#21017;&#20013;&#8220;&#19968;&#20154;&#19968;&#31080;&#8221;&#21407;&#21017;&#65288;&#21363;1p1v&#65289;&#30340;&#21518;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedQV&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20108;&#27425;&#25237;&#31080;&#26426;&#21046;&#30340;&#26032;&#22411;&#32858;&#21512;&#31639;&#27861;&#65292;&#35813;&#26426;&#21046;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;1p1v&#36873;&#20030;&#30340;&#26356;&#22909;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;FedQV&#26159;&#19968;&#20010;&#30495;&#23454;&#26426;&#21046;&#65292;&#26681;&#25454;&#33258;&#24049;&#30340;&#30495;&#23454;&#20272;&#20540;&#36827;&#34892;&#25237;&#26631;&#26159;&#19968;&#20010;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#31574;&#30053;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#35777;&#20998;&#26512;...
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) permits different parties to collaboratively train a global model without disclosing their respective local labels. A crucial step of FL, that of aggregating local models to produce the global one, shares many similarities with public decision-making, and elections in particular. In that context, a major weakness of FL, namely its vulnerability to poisoning attacks, can be interpreted as a consequence of the one person one vote (henceforth 1p1v) principle underpinning most contemporary aggregation rules. In this paper, we propose FedQV, a novel aggregation algorithm built upon the quadratic voting scheme, recently proposed as a better alternative to 1p1v-based elections. Our theoretical analysis establishes that FedQV is a truthful mechanism in which bidding according to one's true valuation is a dominant strategy that achieves a convergence rate that matches those of state-of-the-art methods. Furthermore, our empirical analysis using multiple real-world dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#25511;&#21046;&#22120;&#31867;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20854;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#24120;&#35265;&#25511;&#21046;&#22120;&#20013;&#30340;&#21487;&#39564;&#35777;&#24615;&#21644;&#24615;&#33021;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;&#35813;&#25511;&#21046;&#22120;&#31867;&#20284;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#38382;&#39064;&#30340;&#20108;&#27425;&#35268;&#21010;&#27714;&#35299;&#22120;&#65292;&#20855;&#26377;&#21487;&#39564;&#35777;&#30340;&#23646;&#24615;&#65292;&#24182;&#19988;&#22312;&#25511;&#21046;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#19982;&#20854;&#20182;&#25511;&#21046;&#22120;&#30456;&#23218;&#32654;&#12290;&#21516;&#26102;&#65292;&#35813;&#25511;&#21046;&#22120;&#30340;&#35745;&#31639;&#25928;&#29575;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2312.05332</link><description>&lt;p&gt;
&#28040;&#38500;&#24046;&#36317;&#65306;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#21487;&#39564;&#35777;&#27169;&#22411;&#26080;&#20851;&#20108;&#27425;&#35268;&#21010;&#25511;&#21046;&#22120;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming Controllers Inspired by Model Predictive Control. (arXiv:2312.05332v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#25511;&#21046;&#22120;&#31867;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20854;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#24120;&#35265;&#25511;&#21046;&#22120;&#20013;&#30340;&#21487;&#39564;&#35777;&#24615;&#21644;&#24615;&#33021;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;&#35813;&#25511;&#21046;&#22120;&#31867;&#20284;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#38382;&#39064;&#30340;&#20108;&#27425;&#35268;&#21010;&#27714;&#35299;&#22120;&#65292;&#20855;&#26377;&#21487;&#39564;&#35777;&#30340;&#23646;&#24615;&#65292;&#24182;&#19988;&#22312;&#25511;&#21046;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#19982;&#20854;&#20182;&#25511;&#21046;&#22120;&#30456;&#23218;&#32654;&#12290;&#21516;&#26102;&#65292;&#35813;&#25511;&#21046;&#22120;&#30340;&#35745;&#31639;&#25928;&#29575;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#25511;&#21046;&#22120;&#31867;&#65292;&#21463;&#21040;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#30340;&#21551;&#21457;&#12290;&#35813;&#25511;&#21046;&#22120;&#31867;&#20284;&#20110;&#32447;&#24615;MPC&#38382;&#39064;&#30340;&#20108;&#27425;&#35268;&#21010;&#65288;QP&#65289;&#27714;&#35299;&#22120;&#65292;&#20294;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#26159;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#20174;&#31995;&#32479;&#27169;&#22411;&#20013;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#24120;&#35265;&#25511;&#21046;&#22120;&#20013;&#20351;&#29992;MLP&#25110;&#20854;&#20182;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;DRL&#30340;&#21487;&#39564;&#35777;&#24615;&#21644;&#24615;&#33021;&#20445;&#35777;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#25152;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#20855;&#26377;&#19982;MPC&#31867;&#20284;&#30340;&#25345;&#32493;&#21487;&#34892;&#24615;&#21644;&#28176;&#36817;&#31283;&#23450;&#24615;&#31561;&#21487;&#39564;&#35777;&#23646;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#22120;&#22312;&#25511;&#21046;&#24615;&#33021;&#19978;&#19982;MPC&#21644;MLP&#25511;&#21046;&#22120;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#23545;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#21644;&#22122;&#22768;&#20855;&#26377;&#26356;&#20248;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#22120;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#26126;&#26174;&#20248;&#20110;MPC&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new class of parameterized controllers, drawing inspiration from Model Predictive Control (MPC). The controller resembles a Quadratic Programming (QP) solver of a linear MPC problem, with the parameters of the controller being trained via Deep Reinforcement Learning (DRL) rather than derived from system models. This approach addresses the limitations of common controllers with Multi-Layer Perceptron (MLP) or other general neural network architecture used in DRL, in terms of verifiability and performance guarantees, and the learned controllers possess verifiable properties like persistent feasibility and asymptotic stability akin to MPC. On the other hand, numerical examples illustrate that the proposed controller empirically matches MPC and MLP controllers in terms of control performance and has superior robustness against modeling uncertainty and noises. Furthermore, the proposed controller is significantly more computationally efficient compared to MPC a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRED&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01605</link><description>&lt;p&gt;
&#23545;&#20110;&#25991;&#26412;&#39044;&#27979;&#30340;&#24544;&#23454;&#21644;&#31283;&#20581;&#30340;&#26412;&#22320;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithful and Robust Local Interpretability for Textual Predictions. (arXiv:2311.01605v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRED&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#24471;&#21040;&#20449;&#20219;&#21644;&#37096;&#32626;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#27169;&#22411;&#30340;&#26041;&#27861;&#36890;&#24120;&#22797;&#26434;&#65292;&#24182;&#19988;&#32570;&#20047;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20063;&#19981;&#33021;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;FRED&#65288;Faithful and Robust Explainer for textual Documents&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24403;&#36825;&#20123;&#35789;&#34987;&#31227;&#38500;&#26102;&#23545;&#39044;&#27979;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#27491;&#24335;&#30340;&#23450;&#20041;&#21644;&#23545;&#21487;&#35299;&#37322;&#20998;&#31867;&#22120;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#30830;&#31435;&#20102;FRED&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;FRED&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability is essential for machine learning models to be trusted and deployed in critical domains. However, existing methods for interpreting text models are often complex, lack solid mathematical foundations, and their performance is not guaranteed. In this paper, we propose FRED (Faithful and Robust Explainer for textual Documents), a novel method for interpreting predictions over text. FRED identifies key words in a document that significantly impact the prediction when removed. We establish the reliability of FRED through formal definitions and theoretical analyses on interpretable classifiers. Additionally, our empirical evaluation against state-of-the-art methods demonstrates the effectiveness of FRED in providing insights into text models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#37327;&#23376;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;QLSTM&#65289;&#21644;&#32463;&#20856;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#22312;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#39044;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;QLSTM&#20855;&#26377;&#21152;&#24555;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#21644;&#20943;&#23567;&#27979;&#35797;&#25439;&#22833;&#30340;&#20248;&#21183;&#65292;&#25317;&#26377;&#21560;&#32435;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#20851;&#31995;&#30340;&#28508;&#21147;&#65292;&#20294;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.17032</link><description>&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#37327;&#23376;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;QLSTM&#65289;&#19982;&#32463;&#20856;LSTM&#30340;&#27604;&#36739;&#30740;&#31350;&#65306;&#20197;&#22826;&#38451;&#33021;&#39044;&#27979;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Quantum Long Short-Term Memory (QLSTM) vs Classical LSTM in Time Series Forecasting: A Comparative Study in Solar Power Forecasting. (arXiv:2310.17032v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#37327;&#23376;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;QLSTM&#65289;&#21644;&#32463;&#20856;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#22312;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#39044;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;QLSTM&#20855;&#26377;&#21152;&#24555;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#21644;&#20943;&#23567;&#27979;&#35797;&#25439;&#22833;&#30340;&#20248;&#21183;&#65292;&#25317;&#26377;&#21560;&#32435;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#20851;&#31995;&#30340;&#28508;&#21147;&#65292;&#20294;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#21521;&#21487;&#25345;&#32493;&#33021;&#28304;&#31995;&#32479;&#21457;&#23637;&#30340;&#36807;&#31243;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#37327;&#23376;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;QLSTM&#65289;&#21644;&#32463;&#20856;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#22312;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;&#26041;&#38754;&#36827;&#34892;&#20102;&#20180;&#32454;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;QLSTM&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21253;&#25324;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#21152;&#24555;&#21644;&#22312;&#21021;&#22987;&#38454;&#27573;&#26126;&#26174;&#38477;&#20302;&#30340;&#27979;&#35797;&#25439;&#22833;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#32463;&#20856;LSTM&#27169;&#22411;&#12290;&#36825;&#20123;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;QLSTM&#26377;&#28508;&#21147;&#24555;&#36895;&#21560;&#32435;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#20851;&#31995;&#65292;&#36825;&#24471;&#30410;&#20110;&#37327;&#23376;&#29616;&#35937;&#65288;&#22914;&#21472;&#21152;&#65289;&#12290;&#28982;&#32780;&#65292;&#35201;&#23454;&#29616;QLSTM&#30340;&#20840;&#37096;&#33021;&#21147;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#27169;&#22411;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#39564;&#35777;&#12289;&#31995;&#32479;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#30828;&#20214;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#30456;&#20851;&#21487;&#20877;&#29983;&#33021;&#28304;&#39044;&#27979;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;&#38543;&#30528;&#19981;&#26029;&#30340;&#36827;&#23637;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#22312;&#39044;&#27979;&#21644;&#20248;&#21270;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#24102;&#26469;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately forecasting solar power generation is crucial in the global progression towards sustainable energy systems. In this study, we conduct a meticulous comparison between Quantum Long Short-Term Memory (QLSTM) and classical Long Short-Term Memory (LSTM) models for solar power production forecasting. Our controlled experiments reveal promising advantages of QLSTMs, including accelerated training convergence and substantially reduced test loss within the initial epoch compared to classical LSTMs. These empirical findings demonstrate QLSTM's potential to swiftly assimilate complex time series relationships, enabled by quantum phenomena like superposition. However, realizing QLSTM's full capabilities necessitates further research into model validation across diverse conditions, systematic hyperparameter optimization, hardware noise resilience, and applications to correlated renewable forecasting problems. With continued progress, quantum machine learning can offer a paradigm shift in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#20195;&#29702;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#24046;&#36317;&#12290;&#36825;&#20010;&#24046;&#36317;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#33021;&#21542;&#36866;&#24403;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.11211</link><description>&lt;p&gt;
&#29702;&#35299;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Understanding Fairness Surrogate Functions in Algorithmic Fairness. (arXiv:2310.11211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#20195;&#29702;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#24046;&#36317;&#12290;&#36825;&#20010;&#24046;&#36317;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#33021;&#21542;&#36866;&#24403;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#35266;&#23519;&#21040;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26576;&#20123;&#20154;&#32676;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#24182;&#23454;&#29616;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#65292;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#24341;&#20837;&#28041;&#21450;&#20844;&#24179;&#24615;&#23450;&#20041;&#30340;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#65292;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#36825;&#31181;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#28145;&#20837;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20197;&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#8212;&#8212;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#8212;&#8212;&#20026;&#20363;&#65292;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#20844;&#24179;&#24615;&#23450;&#20041;&#21644;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#20195;&#29702;-&#20844;&#24179;&#24615;&#24046;&#36317;&#12290;&#36825;&#20010;"&#24046;&#36317;"&#30452;&#25509;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#26159;&#21542;&#36866;&#21512;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#20851;&#20110;&#36825;&#20010;"&#24046;&#36317;"&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#28608;&#21457;&#20102;&#25105;&#20204;&#30340;&#20852;&#36259;&#65292;&#34920;&#26126;&#26080;&#38480;&#21046;&#30340;&#20195;&#29702;&#20989;&#25968;&#23558;&#21463;&#21040;&#20915;&#31574;&#36793;&#30028;&#36828;&#31163;&#30340;&#28857;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been observed that machine learning algorithms exhibit biased predictions against certain population groups. To mitigate such bias while achieving comparable accuracy, a promising approach is to introduce surrogate functions of the concerned fairness definition and solve a constrained optimization problem. However, an intriguing issue in previous work is that such fairness surrogate functions may yield unfair results. In this work, in order to deeply understand this issue, taking a widely used fairness definition, demographic parity as an example, we both theoretically and empirically show that there is a surrogate-fairness gap between the fairness definition and the fairness surrogate function. The "gap" directly determines whether a surrogate function is an appropriate substitute for a fairness definition. Also, the theoretical analysis and experimental results about the "gap" motivate us that the unbounded surrogate functions will be affected by the points far from the decisi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#22686;&#24378;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.10655</link><description>&lt;p&gt;
&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22686;&#24378;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Enhancing Trustworthiness in ML-Based Network Intrusion Detection with Uncertainty Quantification. (arXiv:2310.10655v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10655
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#22686;&#24378;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#21644;&#30456;&#20851;&#36890;&#20449;&#25216;&#26415;&#30340;&#21457;&#23637;&#19981;&#26029;&#22686;&#21152;&#20102;&#32593;&#32476;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#35782;&#21035;&#21644;&#32531;&#35299;&#23545;&#29616;&#20195;&#32593;&#32476;&#30340;&#25915;&#20987;&#30340;&#23433;&#20840;&#35774;&#22791;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#29992;&#20110;&#25191;&#34892;IDS&#25152;&#38656;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36866;&#24212;&#36825;&#20010;&#30446;&#30340;&#32780;&#37319;&#29992;&#30340;&#20856;&#22411;&#30340;ML&#27169;&#22411;&#27809;&#26377;&#36866;&#24403;&#22320;&#32771;&#34385;&#21040;&#19982;&#20854;&#33258;&#36523;&#39044;&#27979;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#20250;&#20026;&#35823;&#20998;&#31867;&#30340;&#36755;&#20837;&#21644;&#23646;&#20110;&#26410;&#30693;&#31867;&#21035;&#65288;&#20363;&#22914;&#26032;&#22411;&#25915;&#20987;&#65289;&#30340;&#36755;&#20837;&#20135;&#29983;&#35823;&#23548;&#24615;&#36739;&#39640;&#30340;&#20998;&#31867;&#24471;&#20998;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#22522;&#20110;ML&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;ML&#30340;IDS&#24212;&#35813;&#22987;&#32456;&#25552;&#20379;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20197;&#36991;&#20813;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#20107;&#23454;&#19978;&#65292;&#19968;&#31181;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21487;&#20197;&#20026;&#36827;&#19968;&#27493;&#30340;&#20915;&#31574;&#25552;&#20379;&#37325;&#35201;&#30340;&#21442;&#32771;&#65292;&#20174;&#32780;&#22686;&#24378;ML-based IDS&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of Internet and its related communication technologies have consistently increased the risk of cyber-attacks. In this context, a crucial role is played by Intrusion Detection Systems (IDSs), which are security devices designed to identify and mitigate attacks to modern networks. In the last decade, data-driven approaches based on Machine Learning (ML) have gained more and more popularity for executing the classification tasks required by IDSs. However, typical ML models adopted for this purpose do not properly take into account the uncertainty associated with their own prediction. This poses significant challenges, as they tend to produce misleadingly high classification scores for both misclassified inputs and inputs belonging to unknown classes (e.g. novel attacks), limiting the trustworthiness of existing ML-based solutions. In this paper we argue that ML-based IDSs should always provide accurate uncertainty quantification to avoid overconfident predictions. In fact, a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;60&#22810;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;TSC&#30340;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.10060</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#25968;&#25454;&#22686;&#24378;&#65306;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#21644;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Time-Series Classification: An Extensive Empirical Study and Comprehensive Survey. (arXiv:2310.10060v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;60&#22810;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;TSC&#30340;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#24050;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#31574;&#30053;&#65292;&#20027;&#35201;&#22240;&#20026;&#23427;&#21487;&#20197;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#20351;&#25968;&#25454;&#38598;&#22810;&#26679;&#21270;&#65292;&#24182;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;TSC&#20013;&#30340;DA&#30740;&#31350;&#23384;&#22312;&#30528;&#25991;&#29486;&#35780;&#23457;&#30340;&#29255;&#27573;&#21270;&#65292;&#26041;&#27861;&#23398;&#20998;&#31867;&#19981;&#28165;&#26224;&#65292;&#35780;&#20272;&#25351;&#26631;&#19981;&#36275;&#20197;&#21450;&#32570;&#20047;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#31561;&#38382;&#39064;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#23545;TSC&#39046;&#22495;&#20013;&#30340;DA&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#25345;&#32493;&#21313;&#24180;&#30340;&#24191;&#27867;&#25991;&#29486;&#22238;&#39038;&#65292;&#21457;&#29616;&#24403;&#20195;&#32508;&#36848;&#25991;&#31456;&#24456;&#23569;&#33021;&#22815;&#28085;&#30422;DA&#22312;TSC&#19978;&#30340;&#20840;&#37096;&#36827;&#23637;&#65292;&#22240;&#27492;&#25105;&#20204;&#20180;&#32454;&#20998;&#26512;&#20102;100&#22810;&#31687;&#23398;&#26415;&#25991;&#31456;&#65292;&#24635;&#32467;&#20986;&#20102;60&#22810;&#31181;&#29420;&#29305;&#30340;DA&#25216;&#26415;&#12290;&#36825;&#39033;&#20005;&#26684;&#30340;&#20998;&#26512;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#19987;&#38376;&#38024;&#23545;TSC&#20013;&#30340;DA&#32454;&#33410;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation (DA) has emerged as an indispensable strategy in Time Series Classification (TSC), primarily due to its capacity to amplify training samples, thereby bolstering model robustness, diversifying datasets, and curtailing overfitting. However, the current landscape of DA in TSC is plagued with fragmented literature reviews, nebulous methodological taxonomies, inadequate evaluative measures, and a dearth of accessible, user-oriented tools. In light of these challenges, this study embarks on an exhaustive dissection of DA methodologies within the TSC realm. Our initial approach involved an extensive literature review spanning a decade, revealing that contemporary surveys scarcely capture the breadth of advancements in DA for TSC, prompting us to meticulously analyze over 100 scholarly articles to distill more than 60 unique DA techniques. This rigorous analysis precipitated the formulation of a novel taxonomy, purpose-built for the intricacies of DA in TSC, categorizing tech
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#28857;&#30340;&#22810;&#35270;&#22270;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#29305;&#24449;&#19979;&#38477;&#25366;&#25496;&#21644;&#37096;&#32626;&#35270;&#22270;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#32858;&#31867;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#35270;&#22270;&#30340;&#29305;&#24449;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#27861;&#24471;&#21040;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#34987;&#31216;&#20026;&#8220;&#30456;&#20284;&#31354;&#38388;&#8221;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07166</link><description>&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22810;&#35270;&#22270;&#23376;&#31354;&#38388;&#32858;&#31867;&#19982;&#20998;&#23618;&#29305;&#24449;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent. (arXiv:2310.07166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#28857;&#30340;&#22810;&#35270;&#22270;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#29305;&#24449;&#19979;&#38477;&#25366;&#25496;&#21644;&#37096;&#32626;&#35270;&#22270;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#32858;&#31867;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#35270;&#22270;&#30340;&#29305;&#24449;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#27861;&#24471;&#21040;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#34987;&#31216;&#20026;&#8220;&#30456;&#20284;&#31354;&#38388;&#8221;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#32858;&#31867;&#22240;&#20854;&#33021;&#22815;&#20174;&#22810;&#20010;&#20449;&#24687;&#28304;&#20013;&#32858;&#21512;&#20449;&#24687;&#30340;&#33021;&#21147;&#20197;&#21450;&#22312;&#20844;&#20849;&#20107;&#21153;&#20013;&#26377;&#21069;&#26223;&#30340;&#28508;&#21147;&#32780;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#36817;&#26399;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38590;&#39064;&#38656;&#35201;&#35299;&#20915;&#12290;&#22312;&#35797;&#22270;&#23545;&#19981;&#21516;&#35270;&#22270;&#30340;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#26102;&#65292;&#20986;&#29616;&#20102;&#19968;&#20010;&#24120;&#35265;&#30340;&#22256;&#22659;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#23618;&#29305;&#24449;&#19979;&#38477;&#25366;&#25496;&#21644;&#37096;&#32626;&#35270;&#22270;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#23548;&#33268;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#65288;&#31532;&#19968;&#38454;&#27573;&#65289;&#12290;&#36825;&#20010;&#28508;&#22312;&#31354;&#38388;&#39318;&#27425;&#34987;&#35270;&#20026;&#8220;&#30456;&#20284;&#31354;&#38388;&#8221;&#65292;&#22240;&#20026;&#23427;&#25581;&#31034;&#20102;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#26576;&#20123;&#30456;&#20851;&#24615;&#21644;&#20381;&#36182;&#20851;&#31995;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#31867;&#21035;&#30340;&#29420;&#28909;&#32534;&#30721;&#20063;&#21487;&#20197;&#34987;&#31216;&#20026;&#32456;&#27490;&#38454;&#27573;&#30340;&#30456;&#20284;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#31639;&#27861;&#36215;&#28304;&#20110;k-means&#32858;&#31867;&#21644;&#35889;&#32858;&#31867;&#65292;&#36825;&#23548;&#33268;&#20102;&#31435;&#26041;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering has attracted growing attention owing to its capabilities of aggregating information from various sources and its promising horizons in public affairs. Up till now, many advanced approaches have been proposed in recent literature. However, there are several ongoing difficulties to be tackled. One common dilemma occurs while attempting to align the features of different views. We dig out as well as deploy the dependency amongst views through hierarchical feature descent, which leads to a common latent space( STAGE 1). This latent space, for the first time of its kind, is regarded as a 'resemblance space', as it reveals certain correlations and dependencies of different views. To be exact, the one-hot encoding of a category can also be referred to as a resemblance space in its terminal phase. Moreover, due to the intrinsic fact that most of the existing multi-view clustering algorithms stem from k-means clustering and spectral clustering, this results in cubic time 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;EIF+&#21644;ExIFFI&#20004;&#31181;&#25913;&#36827;&#20102;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#25512;&#24191;&#33021;&#21147;&#21644;&#35299;&#37322;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.05468</link><description>&lt;p&gt;
ExIFFI&#21644;EIF+&#65306;&#35299;&#37322;&#24615;&#21644;&#22686;&#24378;&#30340;&#25512;&#24191;&#33021;&#21147;&#20197;&#25193;&#23637;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
ExIFFI and EIF+: Interpretability and Enhanced Generalizability to Extend the Extended Isolation Forest. (arXiv:2310.05468v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;EIF+&#21644;ExIFFI&#20004;&#31181;&#25913;&#36827;&#20102;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#25512;&#24191;&#33021;&#21147;&#21644;&#35299;&#37322;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#28041;&#21450;&#22312;&#22797;&#26434;&#25968;&#25454;&#38598;&#21644;&#31995;&#32479;&#20013;&#35782;&#21035;&#24322;&#24120;&#34892;&#20026;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;DSS&#65289;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20165;&#20165;&#23450;&#20301;&#24322;&#24120;&#24448;&#24448;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#19981;&#36275;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#29992;&#25143;&#36890;&#24120;&#38656;&#35201;&#20102;&#35299;&#39044;&#27979;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#20197;&#20415;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24182;&#22686;&#24378;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#24615;&#36136;&#65292;&#21019;&#24314;&#21487;&#35299;&#37322;&#30340;&#24037;&#20855;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EIF+&#65292;&#36825;&#26159;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;&#65288;EIF&#65289;&#30340;&#22686;&#24378;&#21464;&#20307;&#65292;&#26088;&#22312;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ExIFFI&#65292;&#19968;&#31181;&#23558;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;&#19982;&#35299;&#37322;&#24615;&#21151;&#33021;&#65288;&#29305;&#24449;&#25490;&#21517;&#65289;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#25552;&#20379;&#20102;&#20197;&#23396;&#31435;&#22522;&#20110;&#26041;&#27861;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection, an essential unsupervised machine learning task, involves identifying unusual behaviors within complex datasets and systems. While Machine Learning algorithms and decision support systems (DSSs) offer effective solutions for this task, simply pinpointing anomalies often falls short in real-world applications. Users of these systems often require insight into the underlying reasons behind predictions to facilitate Root Cause Analysis and foster trust in the model. However, due to the unsupervised nature of anomaly detection, creating interpretable tools is challenging. This work introduces EIF+, an enhanced variant of Extended Isolation Forest (EIF), designed to enhance generalization capabilities. Additionally, we present ExIFFI, a novel approach that equips Extended Isolation Forest with interpretability features, specifically feature rankings. Experimental results provide a comprehensive comparative analysis of Isolation-based approaches for Anomaly Detection, incl
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#26680;&#26041;&#27861;&#26159;&#37327;&#23376;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#26368;&#33258;&#28982;&#30340;&#32852;&#31995;&#20043;&#19968;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#36890;&#36807;&#24341;&#20837;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#20219;&#20309;&#26680;&#20989;&#25968;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#21644;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#12290;</title><link>http://arxiv.org/abs/2309.14419</link><description>&lt;p&gt;
&#20851;&#20110;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the expressivity of embedding quantum kernels. (arXiv:2309.14419v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14419
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26680;&#26041;&#27861;&#26159;&#37327;&#23376;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#26368;&#33258;&#28982;&#30340;&#32852;&#31995;&#20043;&#19968;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#36890;&#36807;&#24341;&#20837;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#20219;&#20309;&#26680;&#20989;&#25968;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#21644;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26680;&#26041;&#27861;&#30340;&#32972;&#26223;&#19979;&#65292;&#37327;&#23376;&#26680;&#19982;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#24314;&#31435;&#20102;&#26368;&#33258;&#28982;&#30340;&#32852;&#31995;&#12290;&#26680;&#26041;&#27861;&#20381;&#36182;&#20110;&#20869;&#31215;&#29305;&#24449;&#21521;&#37327;&#65292;&#36825;&#20123;&#29305;&#24449;&#21521;&#37327;&#23384;&#22312;&#20110;&#22823;&#22411;&#29305;&#24449;&#31354;&#38388;&#20013;&#12290;&#37327;&#23376;&#26680;&#36890;&#24120;&#36890;&#36807;&#26174;&#24335;&#26500;&#36896;&#37327;&#23376;&#29305;&#24449;&#24577;&#24182;&#35745;&#31639;&#23427;&#20204;&#30340;&#20869;&#31215;&#26469;&#35780;&#20272;&#65292;&#36825;&#37324;&#31216;&#20026;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#12290;&#30001;&#20110;&#32463;&#20856;&#26680;&#36890;&#24120;&#22312;&#19981;&#20351;&#29992;&#29305;&#24449;&#21521;&#37327;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#24819;&#30693;&#36947;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#30340;&#34920;&#36798;&#33021;&#21147;&#22914;&#20309;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#26159;&#21542;&#25152;&#26377;&#30340;&#37327;&#23376;&#26680;&#37117;&#21487;&#20197;&#34920;&#36798;&#20026;&#37327;&#23376;&#29305;&#24449;&#24577;&#30340;&#20869;&#31215;&#65311;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#32467;&#26524;&#26159;&#32943;&#23450;&#30340;&#65306;&#36890;&#36807;&#35843;&#29992;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#20219;&#20309;&#26680;&#20989;&#25968;&#65292;&#24635;&#26159;&#23384;&#22312;&#23545;&#24212;&#30340;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#21644;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#12290;&#28982;&#32780;&#65292;&#38382;&#39064;&#26356;&#20851;&#27880;&#30340;&#26159;&#26377;&#25928;&#30340;&#26500;&#36896;&#26041;&#24335;&#12290;&#22312;&#31532;&#20108;&#37096;&#20998;&#20013;
&lt;/p&gt;
&lt;p&gt;
One of the most natural connections between quantum and classical machine learning has been established in the context of kernel methods. Kernel methods rely on kernels, which are inner products of feature vectors living in large feature spaces. Quantum kernels are typically evaluated by explicitly constructing quantum feature states and then taking their inner product, here called embedding quantum kernels. Since classical kernels are usually evaluated without using the feature vectors explicitly, we wonder how expressive embedding quantum kernels are. In this work, we raise the fundamental question: can all quantum kernels be expressed as the inner product of quantum feature states? Our first result is positive: Invoking computational universality, we find that for any kernel function there always exists a corresponding quantum feature map and an embedding quantum kernel. The more operational reading of the question is concerned with efficient constructions, however. In a second part
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#65292;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#26799;&#24230;&#19979;&#38477;&#21464;&#31181;&#65292;&#30830;&#23450;&#20102;&#19968;&#20010;&#31283;&#23450;&#24615;&#36793;&#30028;&#65292;&#35813;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.12488</link><description>&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#21644;&#31283;&#23450;&#24615;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization and the Edge of Stability. (arXiv:2309.12488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#65292;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#26799;&#24230;&#19979;&#38477;&#21464;&#31181;&#65292;&#30830;&#23450;&#20102;&#19968;&#20010;&#31283;&#23450;&#24615;&#36793;&#30028;&#65292;&#35813;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;(GD)&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#30340;&#25805;&#20316;&#31526;&#33539;&#25968;&#20250;&#22686;&#38271;&#65292;&#30452;&#21040;&#25509;&#36817;$2/\eta$&#65292;&#20043;&#21518;&#20250;&#22312;&#35813;&#20540;&#21608;&#22260;&#27874;&#21160;&#12290;&#26681;&#25454;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#20108;&#27425;&#36924;&#36817;&#65292;$2/\eta$&#34987;&#31216;&#20026;&#8220;&#31283;&#23450;&#24615;&#36793;&#30028;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#30830;&#23450;&#20102;&#19968;&#20010;&#8220;&#31283;&#23450;&#24615;&#36793;&#30028;&#8221;&#65292;SAM&#26159;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;GD&#21464;&#31181;&#12290;&#19982;GD&#19981;&#21516;&#65292;SAM&#30340;&#31283;&#23450;&#24615;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;&#36890;&#36807;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#23454;&#35777;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;SAM&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#30830;&#23450;&#30340;&#31283;&#23450;&#24615;&#36793;&#30028;&#19978;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\eta$, after which it fluctuates around this value.  The quantity $2/\eta$ has been called the "edge of stability" based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an "edge of stability" for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;PCN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#21033;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21943;&#27880;&#26631;&#35760;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.08630</link><description>&lt;p&gt;
PCN&#65306;&#19968;&#31181;&#21033;&#29992;&#26032;&#39062;&#30340;&#22270;&#26500;&#24314;&#26041;&#27861;&#21644;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#21943;&#27880;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph Construction Methods and Chebyshev Graph Convolutions. (arXiv:2309.08630v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;PCN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#21033;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21943;&#27880;&#26631;&#35760;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21943;&#27880;&#26631;&#35760;&#26159;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#30340;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#31890;&#23376;&#30896;&#25758;&#20135;&#29983;&#30340;&#38181;&#29366;&#21943;&#27880;&#65292;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#21457;&#23556;&#31890;&#23376;&#12290;&#21943;&#27880;&#26631;&#35760;&#30340;&#36827;&#23637;&#20026;&#36229;&#20986;&#26631;&#20934;&#27169;&#22411;&#30340;&#26032;&#29289;&#29702;&#25628;&#32034;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#22797;&#26434;&#30896;&#25758;&#25968;&#25454;&#20013;&#23547;&#25214;&#38544;&#34255;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#23558;&#21943;&#27880;&#34920;&#31034;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#30340;&#26041;&#27861;&#22810;&#31181;&#22810;&#26679;&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#21521;&#27169;&#22411;&#38544;&#34255;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#23613;&#21487;&#33021;&#22320;&#32534;&#30721;&#26368;&#22810;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20174;&#36825;&#31181;&#34920;&#31034;&#20013;&#26368;&#22909;&#22320;&#23398;&#20064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Particle Chebyshev Network&#65288;PCN&#65289;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#24182;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#12290;ChebConv&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;GNN&#20013;&#30340;&#19968;&#31181;&#26377;&#25928;&#26367;&#20195;&#20256;&#32479;&#22270;&#21367;&#31215;&#30340;&#26041;&#27861;&#65292;&#32780;&#22312;&#21943;&#27880;&#26631;&#35760;&#20013;&#36824;&#27809;&#26377;&#34987;&#25506;&#32034;&#36807;&#12290;PCN&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jet tagging is a classification problem in high-energy physics experiments that aims to identify the collimated sprays of subatomic particles, jets, from particle collisions and tag them to their emitter particle. Advances in jet tagging present opportunities for searches of new physics beyond the Standard Model. Current approaches use deep learning to uncover hidden patterns in complex collision data. However, the representation of jets as inputs to a deep learning model have been varied, and often, informative features are withheld from models. In this study, we propose a graph-based representation of a jet that encodes the most information possible. To learn best from this representation, we design Particle Chebyshev Network (PCN), a graph neural network (GNN) using Chebyshev graph convolutions (ChebConv). ChebConv has been demonstrated as an effective alternative to classical graph convolutions in GNNs and has yet to be explored in jet tagging. PCN achieves a substantial improvemen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#30340;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LRNN&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LRNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37197;&#22791;&#20102;&#19968;&#20010;&#22359;&#23545;&#35282;&#19988;&#36755;&#20837;&#30456;&#20851;&#30340;&#36716;&#31227;&#30697;&#38453;&#65292;&#24182;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22312;&#27491;&#21017;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#38271;&#24230;&#22806;&#25512;&#30340;LRNN&#12290;</title><link>http://arxiv.org/abs/2309.07412</link><description>&lt;p&gt;
&#22312;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#25512;&#36827;&#27491;&#21017;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Advancing Regular Language Reasoning in Linear Recurrent Neural Networks. (arXiv:2309.07412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07412
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#30340;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LRNN&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LRNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37197;&#22791;&#20102;&#19968;&#20010;&#22359;&#23545;&#35282;&#19988;&#36755;&#20837;&#30456;&#20851;&#30340;&#36716;&#31227;&#30697;&#38453;&#65292;&#24182;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22312;&#27491;&#21017;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#38271;&#24230;&#22806;&#25512;&#30340;LRNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LRNN&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#31243;&#24314;&#27169;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#24555;&#36895;&#30340;&#24182;&#34892;&#35757;&#32451;&#21644;&#24658;&#23450;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#22312;&#23545;LRNN&#37325;&#26032;&#20135;&#29983;&#20852;&#36259;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#35757;&#32451;&#24207;&#21015;&#20013;&#30340;&#38544;&#34255;&#35268;&#21017;&#65292;&#20363;&#22914;&#27491;&#21017;&#35821;&#35328;&#30340;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#23545;&#19968;&#20123;&#24050;&#26377;&#30340;LRNN&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#27491;&#21017;&#35821;&#35328;&#19978;&#23384;&#22312;&#38480;&#21046;&#12290;&#22312;&#36825;&#20010;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LRNN&#65292;&#37197;&#22791;&#20102;&#19968;&#20010;&#22359;&#23545;&#35282;&#21644;&#36755;&#20837;&#30456;&#20851;&#30340;&#36716;&#31227;&#30697;&#38453;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22312;&#27491;&#21017;&#35821;&#35328;&#20219;&#21153;&#65288;&#22914;&#27714;&#21644;&#12289;&#20598;&#25968;&#23545;&#12289;&#27169;&#36816;&#31639;&#31561;&#65289;&#19978;&#36827;&#34892;&#38271;&#24230;&#22806;&#25512;&#30340;LRNN&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language modeling and long-range modeling while offering rapid parallel training and constant inference costs. With the resurged interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations on regular language. Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BAM&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#36741;&#21161;&#21464;&#37327;&#26469;&#25918;&#22823;&#20559;&#35265;&#65292;&#25552;&#39640;&#20102;&#23569;&#25968;&#32676;&#20307;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.06717</link><description>&lt;p&gt;
&#20559;&#35265;&#25918;&#22823;&#22686;&#24378;&#20102;&#23569;&#25968;&#32676;&#20307;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Bias Amplification Enhances Minority Group Performance. (arXiv:2309.06717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BAM&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#36741;&#21161;&#21464;&#37327;&#26469;&#25918;&#22823;&#20559;&#35265;&#65292;&#25552;&#39640;&#20102;&#23569;&#25968;&#32676;&#20307;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#26631;&#20934;&#35757;&#32451;&#20135;&#29983;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#32597;&#35265;&#30340;&#23376;&#32676;&#19978;&#30340;&#20934;&#30830;&#24615;&#36739;&#24046;&#65292;&#23613;&#31649;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#20934;&#30830;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;&#26576;&#20123;&#34394;&#20551;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#20043;&#21069;&#22522;&#20110;&#26368;&#24046;&#32676;&#20307;&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;Group-DRO&#65289;&#22312;&#25913;&#21892;&#26368;&#24046;&#32676;&#20307;&#20934;&#30830;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#38656;&#35201;&#20026;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25552;&#20379;&#26114;&#36149;&#30340;&#32676;&#20307;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#24615;&#30340;&#24773;&#26223;&#65292;&#21363;&#32676;&#20307;&#27880;&#37322;&#20165;&#22312;&#19968;&#20010;&#23567;&#30340;&#39564;&#35777;&#38598;&#19978;&#21487;&#29992;&#65292;&#25110;&#32773;&#26681;&#26412;&#19981;&#21487;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BAM&#65292;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31639;&#27861;&#65306;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#36741;&#21161;&#21464;&#37327;&#20026;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#35757;&#32451;&#19968;&#20010;&#20559;&#35265;&#25918;&#22823;&#26041;&#26696;&#30340;&#27169;&#22411;&#65307;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#23545;&#20559;&#35265;&#25918;&#22823;&#30340;&#27169;&#22411;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#36827;&#34892;&#21152;&#26435;&#65292;&#28982;&#21518;&#22312;&#37325;&#26032;&#21152;&#26435;&#30340;&#25968;&#25454;&#38598;&#19978;&#32487;&#32493;&#35757;&#32451;&#21516;&#19968;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BAM&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks produced by standard training are known to suffer from poor accuracy on rare subgroups despite achieving high accuracy on average, due to the correlations between certain spurious features and labels. Previous approaches based on worst-group loss minimization (e.g. Group-DRO) are effective in improving worse-group accuracy but require expensive group annotations for all the training samples. In this paper, we focus on the more challenging and realistic setting where group annotations are only available on a small validation set or are not available at all. We propose BAM, a novel two-stage training algorithm: in the first stage, the model is trained using a bias amplification scheme via introducing a learnable auxiliary variable for each training sample; in the second stage, we upweight the samples that the bias-amplified model misclassifies, and then continue training the same model on the reweighted dataset. Empirically, BAM achieves competitive performance compared w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#36719;&#26448;&#26009;&#22312;&#22823;&#21464;&#24418;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#20855;&#26377;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#31283;&#20581;&#22320;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.15640</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#22797;&#26434;&#36229;&#24377;&#24615;&#22266;&#20307;&#30340;&#32452;&#20998;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Identifying Constitutive Parameters for Complex Hyperelastic Solids using Physics-Informed Neural Networks. (arXiv:2308.15640v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#36719;&#26448;&#26009;&#22312;&#22823;&#21464;&#24418;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#20855;&#26377;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#31283;&#20581;&#22320;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#31243;&#21644;&#29983;&#29289;&#26448;&#26009;&#20013;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#21644;&#26426;&#26800;&#34892;&#20026;&#30340;&#26448;&#26009;&#20013;&#65292;&#35782;&#21035;&#32452;&#20998;&#21442;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20026;&#27492;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24403;&#21069;&#30340;&#26694;&#26550;&#36890;&#24120;&#20165;&#38480;&#20110;&#22522;&#26412;&#30340;&#32452;&#20998;&#23450;&#24459;&#65292;&#24182;&#22312;&#19982;&#23454;&#39564;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#36935;&#21040;&#23454;&#38469;&#32422;&#26463;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;PINN&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#36719;&#26448;&#26009;&#30340;&#26448;&#26009;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#22312;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#21576;&#29616;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#12290;&#35813;&#27169;&#22411;&#24378;&#35843;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;PINN&#65292;&#20854;&#20013;&#21253;&#25324;&#20840;&#22330;&#21464;&#24418;&#21644;&#21152;&#36733;&#21382;&#21490;&#65292;&#20197;&#30830;&#20445;&#31639;&#27861;&#22312;&#22024;&#26434;&#25968;&#25454;&#20013;&#20173;&#28982;&#31283;&#20581;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying constitutive parameters in engineering and biological materials, particularly those with intricate geometries and mechanical behaviors, remains a longstanding challenge. The recent advent of Physics-Informed Neural Networks (PINNs) offers promising solutions, but current frameworks are often limited to basic constitutive laws and encounter practical constraints when combined with experimental data. In this paper, we introduce a new PINN-based framework designed to identify material parameters for soft materials, specifically those exhibiting complex constitutive behaviors, under large deformation in plane stress conditions. Distinctively, our model emphasizes training PINNs with multi-modal time-dependent experimental datasets consisting of full-field deformation and loading history, ensuring algorithm robustness even amidst noisy data. Our results reveal that our framework can accurately identify constitutive parameters of the incompressible Arruda-Boyce model for samples 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#24178;&#39044;&#25512;&#24191;&#21040;&#19981;&#21516;&#24773;&#22659;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#24178;&#39044;&#26469;&#33258;&#21160;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#24403;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#26102;&#65292;CB2M&#33021;&#22815;&#26816;&#27979;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2308.13453</link><description>&lt;p&gt;
&#23398;&#20064;&#24178;&#39044;&#27010;&#24565;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Learning to Intervene on Concept Bottlenecks. (arXiv:2308.13453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#24178;&#39044;&#25512;&#24191;&#21040;&#19981;&#21516;&#24773;&#22659;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#24178;&#39044;&#26469;&#33258;&#21160;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#24403;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#26102;&#65292;CB2M&#33021;&#22815;&#26816;&#27979;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#32780;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#36890;&#36807;&#20854;&#27010;&#24565;&#34920;&#31034;&#25552;&#20379;&#22266;&#26377;&#30340;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#20204;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#26356;&#26032;&#27010;&#24565;&#20540;&#24182;&#32416;&#27491;&#27169;&#22411;&#30340;&#39044;&#27979;&#36755;&#20986;&#26469;&#36827;&#34892;&#24178;&#39044;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#20013;&#36825;&#20123;&#24178;&#39044;&#20165;&#24212;&#29992;&#20110;&#27169;&#22411;&#19968;&#27425;&#21518;&#21363;&#34987;&#20002;&#24323;&#12290;&#20026;&#20102;&#32416;&#27491;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36825;&#26159;CBM&#30340;&#19968;&#20010;&#25193;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CB2M&#36890;&#36807;&#21452;&#25240;&#21472;&#35760;&#24518;&#23398;&#20064;&#23558;&#24178;&#39044;&#30340;&#25512;&#24191;&#21040;&#36866;&#24403;&#30340;&#26032;&#24773;&#22659;&#20013;&#65292;&#20174;&#32780;&#33021;&#22815;&#23398;&#20064;&#26816;&#27979;&#38169;&#35823;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#30340;&#24178;&#39044;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CB2M&#33021;&#22815;&#20174;&#26368;&#21021;&#33719;&#24471;&#30340;&#23569;&#37327;&#24178;&#39044;&#20013;&#33258;&#21160;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22914;&#26524;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#65292;CB2M&#21487;&#20197;&#26816;&#27979;&#21040;CBM&#29942;&#39048;&#30340;&#28508;&#22312;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
While traditional deep learning models often lack interpretability, concept bottleneck models (CBMs) provide inherent explanations via their concept representations. Specifically, they allow users to perform interventional interactions on these concepts by updating the concept values and thus correcting the predictive output of the model. Traditionally, however, these interventions are applied to the model only once and discarded afterward. To rectify this, we present concept bottleneck memory models (CB2M), an extension to CBMs. Specifically, a CB2M learns to generalize interventions to appropriate novel situations via a two-fold memory with which it can learn to detect mistakes and to reapply previous interventions. In this way, a CB2M learns to automatically improve model performance from a few initially obtained interventions. If no prior human interventions are available, a CB2M can detect potential mistakes of the CBM bottleneck and request targeted interventions. In our experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21387;&#32553;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;(SCALLION&#21644;SCAFCOM)&#65292;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#32463;&#20856;&#30340;&#38543;&#26426;&#25511;&#21046;&#24179;&#22343;&#27861;&#24182;&#25552;&#20986;&#20102;&#31561;&#20215;&#20294;&#26356;&#39640;&#25928;/&#31616;&#21270;&#30340;&#24418;&#24335;&#65292;&#20943;&#23569;&#20102;&#19978;&#34892;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.08165</link><description>&lt;p&gt;
&#24102;&#26377;&#36890;&#20449;&#21387;&#32553;&#30340;&#38543;&#26426;&#25511;&#21046;&#24179;&#22343;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stochastic Controlled Averaging for Federated Learning with Communication Compression. (arXiv:2308.08165v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21387;&#32553;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;(SCALLION&#21644;SCAFCOM)&#65292;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#32463;&#20856;&#30340;&#38543;&#26426;&#25511;&#21046;&#24179;&#22343;&#27861;&#24182;&#25552;&#20986;&#20102;&#31561;&#20215;&#20294;&#26356;&#39640;&#25928;/&#31616;&#21270;&#30340;&#24418;&#24335;&#65292;&#20943;&#23569;&#20102;&#19978;&#34892;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#21387;&#32553;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#36890;&#36807;&#26080;&#32447;&#20256;&#36755;&#30340;&#20449;&#24687;&#37327;&#30340;&#25216;&#26415;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#20943;&#36731;&#36890;&#20449;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#36890;&#20449;&#21387;&#32553;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#21387;&#32553;&#24341;&#36215;&#30340;&#20449;&#24687;&#22833;&#30495;&#20197;&#21450;&#32852;&#37030;&#23398;&#20064;&#30340;&#29305;&#24615;&#65292;&#22914;&#37096;&#20998;&#21442;&#19982;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#26377;&#25152;&#21457;&#23637;&#65292;&#21387;&#32553;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#19981;&#33021;&#36866;&#24212;&#20219;&#24847;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#25110;&#37096;&#20998;&#21442;&#19982;&#65292;&#35201;&#20040;&#35201;&#27714;&#23545;&#21387;&#32553;&#26377;&#20005;&#26684;&#30340;&#26465;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20855;&#26377;&#24320;&#38144;&#20943;&#21322;&#30340;&#19978;&#34892;&#36890;&#20449;&#25104;&#26412;&#30340;&#32463;&#20856;&#38543;&#26426;&#25511;&#21046;&#24179;&#22343;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21387;&#32553;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;SCALLION&#21644;SCAFCOM&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication compression, a technique aiming to reduce the information volume to be transmitted over the air, has gained great interests in Federated Learning (FL) for the potential of alleviating its communication overhead. However, communication compression brings forth new challenges in FL due to the interplay of compression-incurred information distortion and inherent characteristics of FL such as partial participation and data heterogeneity. Despite the recent development, the performance of compressed FL approaches has not been fully exploited. The existing approaches either cannot accommodate arbitrary data heterogeneity or partial participation, or require stringent conditions on compression.  In this paper, we revisit the seminal stochastic controlled averaging method by proposing an equivalent but more efficient/simplified formulation with halved uplink communication costs. Building upon this implementation, we propose two compressed FL algorithms, SCALLION and SCAFCOM, to s
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#32806;&#21512;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#26469;&#22788;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#26469;&#30830;&#20445;&#20195;&#29702;&#23545;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.12062</link><description>&lt;p&gt;
&#28216;&#25103;&#29702;&#35770;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#22788;&#29702;&#26102;&#38388;&#32806;&#21512;&#30340;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations. (arXiv:2307.12062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12062
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#32806;&#21512;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#26469;&#22788;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#26469;&#30830;&#20445;&#20195;&#29702;&#23545;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#35757;&#32451;&#33021;&#22815;&#22312;&#29615;&#22659;&#24178;&#25200;&#25110;&#23545;&#25239;&#25915;&#20987;&#19979;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#21487;&#33021;&#24178;&#25200;&#30340;&#31354;&#38388;&#22312;&#21508;&#20010;&#26102;&#38388;&#27493;&#39588;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#26102;&#38388;&#27493;&#39588;&#19978;&#21487;&#33021;&#24178;&#25200;&#30340;&#31354;&#38388;&#21462;&#20915;&#20110;&#36807;&#21435;&#30340;&#24178;&#25200;&#12290;&#25105;&#20204;&#27491;&#24335;&#24341;&#20837;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#65292;&#23545;&#29616;&#26377;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GRAD&#65292;&#19968;&#31181;&#26032;&#30340;&#28216;&#25103;&#29702;&#35770;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#32806;&#21512;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#12290;&#36890;&#36807;&#22312;&#36825;&#20010;&#28216;&#25103;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#22343;&#34913;&#65292;GRAD&#30830;&#20445;&#20102;&#20195;&#29702;&#30340;&#23545;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#23545;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#22312;&#26631;&#20934;&#21644;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#19979;&#20855;&#26377;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust reinforcement learning (RL) seeks to train policies that can perform well under environment perturbations or adversarial attacks. Existing approaches typically assume that the space of possible perturbations remains the same across timesteps. However, in many settings, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a novel challenge for existing robust RL methods. To tackle this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks demonstrate that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#21160;&#24577;&#22270;&#39046;&#22495;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#23398;&#20064;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#30340;&#26368;&#26032;&#20248;&#21183;&#65292;&#24182;&#23545;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#20026;&#35780;&#20272;&#26032;&#26550;&#26500;&#21644;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06104</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21160;&#24577;&#22270;&#65306;&#27169;&#22411;&#19982;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Deep learning for dynamic graphs: models and benchmarks. (arXiv:2307.06104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#21160;&#24577;&#22270;&#39046;&#22495;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#23398;&#20064;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#30340;&#26368;&#26032;&#20248;&#21183;&#65292;&#24182;&#23545;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#20026;&#35780;&#20272;&#26032;&#26550;&#26500;&#21644;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#28145;&#24230;&#22270;&#32593;&#32476;&#65288;DGNs&#65289;&#30340;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#25512;&#21160;&#20102;&#22270;&#19978;&#23398;&#20064;&#30340;&#39046;&#22495;&#25104;&#29087;&#21457;&#23637;&#12290;&#23613;&#31649;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#27491;&#22312;&#24555;&#36895;&#22686;&#38271;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#23578;&#26410;&#35299;&#20915;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#29305;&#21035;&#22320;&#65292;&#24613;&#38656;&#20351;DGNs&#36866;&#29992;&#20110;&#23454;&#26102;&#31995;&#32479;&#20013;&#38543;&#26102;&#38388;&#25512;&#31227;&#19981;&#26029;&#28436;&#21270;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#20026;&#20419;&#36827;&#21160;&#24577;&#22270;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23398;&#20064;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#30340;&#26368;&#26032;&#20248;&#21183;&#65292;&#24182;&#25552;&#20379;&#20102;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#24403;&#21069;&#26368;&#26032;&#27010;&#35272;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#27169;&#22411;&#36873;&#25321;&#21644;&#35780;&#20272;&#65292;&#20026;&#35780;&#20272;&#26032;&#26550;&#26500;&#21644;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in research on Deep Graph Networks (DGNs) has led to a maturation of the domain of learning on graphs. Despite the growth of this research field, there are still important challenges that are yet unsolved. Specifically, there is an urge of making DGNs suitable for predictive tasks on realworld systems of interconnected entities, which evolve over time. With the aim of fostering research in the domain of dynamic graphs, at first, we survey recent advantages in learning both temporal and spatial information, providing a comprehensive overview of the current state-of-the-art in the domain of representation learning for dynamic graphs. Secondly, we conduct a fair performance comparison among the most popular proposed approaches, leveraging rigorous model selection and assessment for all the methods, thus establishing a sound baseline for evaluating new architectures and approaches
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#12290;&#19982;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#21516;&#65292;&#20316;&#32773;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2307.03305</link><description>&lt;p&gt;
&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
A Vulnerability of Attribution Methods Using Pre-Softmax Scores. (arXiv:2307.03305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03305
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#12290;&#19982;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#21516;&#65292;&#20316;&#32773;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31867;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#12290;&#24050;&#30693;&#36825;&#31181;&#31867;&#22411;&#30340;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#36755;&#20837;&#30340;&#24494;&#23567;&#25200;&#21160;&#21487;&#33021;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss a vulnerability involving a category of attribution methods used to provide explanations for the outputs of convolutional neural networks working as classifiers. It is known that this type of networks are vulnerable to adversarial attacks, in which imperceptible perturbations of the input may alter the outputs of the model. In contrast, here we focus on effects that small modifications in the model may cause on the attribution method without altering the model outputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#26893;&#20837;&#27880;&#20837;&#30340;&#35760;&#24518;&#21270;&#20869;&#23481;&#65292;&#26469;&#26816;&#27979;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;&#12290;&#35813;&#26041;&#27861;&#20462;&#25913;&#20102;&#21463;&#20445;&#25252;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#28155;&#21152;&#20102;&#23545;&#20154;&#30524;&#19981;&#21487;&#23519;&#35273;&#20294;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21644;&#35760;&#24518;&#30340;&#20869;&#23481;&#65292;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#23545;&#27880;&#20837;&#20869;&#23481;&#30340;&#35760;&#24518;&#26469;&#21028;&#26029;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#29983;&#25104;&#31867;&#20284;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.03108</link><description>&lt;p&gt;
&#22914;&#20309;&#26816;&#27979;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models. (arXiv:2307.03108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#26893;&#20837;&#27880;&#20837;&#30340;&#35760;&#24518;&#21270;&#20869;&#23481;&#65292;&#26469;&#26816;&#27979;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;&#12290;&#35813;&#26041;&#27861;&#20462;&#25913;&#20102;&#21463;&#20445;&#25252;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#28155;&#21152;&#20102;&#23545;&#20154;&#30524;&#19981;&#21487;&#23519;&#35273;&#20294;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21644;&#35760;&#24518;&#30340;&#20869;&#23481;&#65292;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#23545;&#27880;&#20837;&#20869;&#23481;&#30340;&#35760;&#24518;&#26469;&#21028;&#26029;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#29983;&#25104;&#31867;&#20284;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#19968;&#20010;&#20363;&#23376;&#26159;&#24403;&#27169;&#22411;&#35757;&#32451;&#32773;&#25910;&#38598;&#20102;&#19968;&#20010;&#29305;&#23450;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#19968;&#31995;&#21015;&#22270;&#20687;&#65292;&#24182;&#35797;&#22270;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#22270;&#20687;&#30340;&#27169;&#22411;&#65292;&#32780;&#27809;&#26377;&#33719;&#24471;&#33402;&#26415;&#23478;&#30340;&#35768;&#21487;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#20837;&#30340;&#35760;&#24518;&#21270;&#20869;&#23481;&#26893;&#20837;&#20445;&#25252;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#26469;&#26816;&#27979;&#27492;&#31867;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#19978;&#28155;&#21152;&#29420;&#29305;&#30340;&#20869;&#23481;&#65292;&#20363;&#22914;&#23545;&#20154;&#31867;&#35270;&#35273;&#19981;&#21487;&#23519;&#35273;&#20294;&#33021;&#22815;&#34987;&#25193;&#25955;&#27169;&#22411;&#25429;&#25417;&#21644;&#35760;&#24518;&#30340;&#38544;&#31192;&#22270;&#20687;&#21253;&#35013;&#20989;&#25968;&#65292;&#26469;&#20462;&#25913;&#21463;&#20445;&#25252;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#26159;&#21542;&#23545;&#27880;&#20837;&#30340;&#20869;&#23481;&#36827;&#34892;&#35760;&#24518;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#21028;&#26029;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#36825;&#19968;&#35760;&#24518;&#65288;&#21363;&#26159;&#21542;&#23384;&#22312;&#29983;&#25104;&#31867;&#20284;&#22270;&#20687;&#30340;&#33021;&#21147;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-image diffusion models have shown surprising performance in generating high-quality images. However, concerns have arisen regarding the unauthorized usage of data during the training process. One example is when a model trainer collects a set of images created by a particular artist and attempts to train a model capable of generating similar images without obtaining permission from the artist. To address this issue, it becomes crucial to detect unauthorized data usage. In this paper, we propose a method for detecting such unauthorized data usage by planting injected memorization into the text-to-image diffusion models trained on the protected dataset. Specifically, we modify the protected image dataset by adding unique contents on the images such as stealthy image wrapping functions that are imperceptible to human vision but can be captured and memorized by diffusion models. By analyzing whether the model has memorization for the injected content (i.e., whether the gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#22238;&#31572;&#20102;Nesterov-1983&#21644;FISTA&#26159;&#21542;&#22312;&#24378;&#20984;&#20989;&#25968;&#19978;&#32447;&#24615;&#25910;&#25947;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#32447;&#24615;&#25910;&#25947;&#24615;&#19981;&#20381;&#36182;&#20110;&#24378;&#20984;&#24615;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2306.09694</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#20984;&#24615;&#30340; Nesterov-1983 &#30340;&#32447;&#24615;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Linear convergence of Nesterov-1983 with the strong convexity. (arXiv:2306.09694v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#22238;&#31572;&#20102;Nesterov-1983&#21644;FISTA&#26159;&#21542;&#22312;&#24378;&#20984;&#20989;&#25968;&#19978;&#32447;&#24615;&#25910;&#25947;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#32447;&#24615;&#25910;&#25947;&#24615;&#19981;&#20381;&#36182;&#20110;&#24378;&#20984;&#24615;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#20195;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;Nesterov &#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#27861;&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#37324;&#31243;&#30865;&#65292;&#35813;&#26041;&#27861;&#22312;[Nesterov&#65292;1983]&#20013;&#25552;&#20986;&#65292;&#31616;&#31216;&#20026;Nesterov-1983&#12290;&#27492;&#21518;&#65292;&#37325;&#35201;&#30340;&#36827;&#23637;&#20043;&#19968;&#26159;&#23427;&#30340;&#36817;&#31471;&#25512;&#24191;&#65292;&#21517;&#20026;&#24555;&#36895;&#36845;&#20195;&#25910;&#32553;&#38408;&#20540;&#31639;&#27861;&#65288;FISTA&#65289;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#31185;&#23398;&#21644;&#24037;&#31243;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#26410;&#30693;&#36947;Nesterov-1983&#21644;FISTA&#26159;&#21542;&#22312;&#24378;&#20984;&#20989;&#25968;&#19978;&#32447;&#24615;&#25910;&#25947;&#65292;&#32780;&#36825;&#24050;&#34987;&#21015;&#20026;&#32508;&#21512;&#35780;&#23457;[Chambolle&#21644;Pock&#65292;2016&#65292;&#38468;&#24405;B]&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#37319;&#29992;&#30340;&#30456;&#31354;&#38388;&#34920;&#31034;&#19968;&#36215;&#65292;&#26500;&#36896;Lyapunov&#20989;&#25968;&#30340;&#20851;&#38190;&#21306;&#21035;&#22312;&#20110;&#21160;&#33021;&#30340;&#31995;&#25968;&#38543;&#36845;&#20195;&#32780;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#65292;&#19978;&#36848;&#20004;&#31181;&#31639;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#27809;&#26377;&#20381;&#36182;&#20110;&#24378;&#20984;&#20989;&#25968;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
For modern gradient-based optimization, a developmental landmark is Nesterov's accelerated gradient descent method, which is proposed in [Nesterov, 1983], so shorten as Nesterov-1983. Afterward, one of the important progresses is its proximal generalization, named the fast iterative shrinkage-thresholding algorithm (FISTA), which is widely used in image science and engineering. However, it is unknown whether both Nesterov-1983 and FISTA converge linearly on the strongly convex function, which has been listed as the open problem in the comprehensive review [Chambolle and Pock, 2016, Appendix B]. In this paper, we answer this question by the use of the high-resolution differential equation framework. Along with the phase-space representation previously adopted, the key difference here in constructing the Lyapunov function is that the coefficient of the kinetic energy varies with the iteration. Furthermore, we point out that the linear convergence of both the two algorithms above has no d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#27010;&#29575;&#36317;&#31163;&#21644;&#20004;&#31181;&#20248;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#36807;&#37325;&#21644;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.18171</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved Probabilistic Image-Text Representations. (arXiv:2305.18171v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#27010;&#29575;&#36317;&#31163;&#21644;&#20004;&#31181;&#20248;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#36807;&#37325;&#21644;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#65292;&#30001;&#20110;&#22810;&#26679;&#24615;&#21644;&#19981;&#23436;&#32654;&#27880;&#37322;&#23548;&#33268;&#30340;&#22266;&#26377;&#27495;&#20041;&#20351;&#20854;&#21463;&#21040;&#22256;&#25200;&#12290;&#30830;&#23450;&#24615;&#20989;&#25968;&#26080;&#27861;&#36275;&#22815;&#24378;&#22823;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#27495;&#20041;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#27010;&#29575;&#23884;&#20837;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#32570;&#28857;&#65306;&#33945;&#29305;&#21345;&#27931;&#36924;&#36817;&#23548;&#33268;&#35745;&#31639;&#36127;&#25285;&#36739;&#37325;&#65292;&#19988;&#22312;&#22823;&#37327;&#35823;&#26816;&#24773;&#20917;&#19979;&#23481;&#26131;&#20986;&#29616;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27010;&#29575;&#36328;&#27169;&#24577;&#23884;&#20837;&#26041;&#27861;&#65288;PCME++&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#38381;&#21512;&#24418;&#24335;&#35299;&#30340;&#26032;&#30340;&#27010;&#29575;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#25216;&#26415;&#36827;&#19968;&#27493;&#22686;&#24378;PCME++&#65306;&#39318;&#20808;&#65292;&#24341;&#20837;&#20266;&#27491;&#26679;&#26412;&#20197;&#38450;&#27490;&#22823;&#37327;&#35823;&#26816;&#24773;&#20917;&#19979;&#30340;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#65307;&#20854;&#27425;&#65292;&#37319;&#29992;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#27010;&#29575;&#21305;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PCME++&#22312;ITM&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-Text Matching (ITM) task, a fundamental vision-language (VL) task, suffers from the inherent ambiguity arising from multiplicity and imperfect annotations. Deterministic functions are not sufficiently powerful to capture ambiguity, prompting the exploration of probabilistic embeddings to tackle the challenge. However, the existing probabilistic ITM approach encounters two key shortcomings; the burden of heavy computations due to the Monte Carlo approximation, and the loss saturation issue in the face of abundant false negatives. To overcome the issues, this paper presents an improved Probabilistic Cross-Modal Embeddings (named PCME++) by introducing a new probabilistic distance with a closed-form solution. In addition, two optimization techniques are proposed to enhance PCME++ further; first, the incorporation of pseudo-positives to prevent the loss saturation problem under massive false negatives; second, mixed sample data augmentation for probabilistic matching. Experimental re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#32534;&#36753;&#30340;DDPM&#22122;&#22768;&#31354;&#38388;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25163;&#27573;&#36827;&#34892;&#24191;&#27867;&#30340;&#32534;&#36753;&#25805;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#32534;&#36753;&#21451;&#22909;&#22122;&#22768;&#22270;&#30340;&#21453;&#28436;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.06140</link><description>&lt;p&gt;
&#19968;&#31181;&#26131;&#20110;&#32534;&#36753;&#30340;DDPM&#22122;&#22768;&#31354;&#38388;&#65306;&#21453;&#28436;&#19982;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
An Edit Friendly DDPM Noise Space: Inversion and Manipulations. (arXiv:2304.06140v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#32534;&#36753;&#30340;DDPM&#22122;&#22768;&#31354;&#38388;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25163;&#27573;&#36827;&#34892;&#24191;&#27867;&#30340;&#32534;&#36753;&#25805;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#32534;&#36753;&#21451;&#22909;&#22122;&#22768;&#22270;&#30340;&#21453;&#28436;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#21033;&#29992;&#19968;&#31995;&#21015;&#30333;&#22122;&#22768;&#26679;&#26412;&#29983;&#25104;&#22270;&#20687;&#12290;&#31867;&#20284;&#20110;GAN&#65292;&#36825;&#20123;&#22122;&#22768;&#22270;&#21487;&#20197;&#30475;&#20316;&#26159;&#29983;&#25104;&#22270;&#20687;&#30456;&#20851;&#30340;&#28508;&#22312;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21407;&#22987;&#22122;&#22768;&#31354;&#38388;&#27809;&#26377;&#26041;&#20415;&#30340;&#32467;&#26500;&#65292;&#22240;&#27492;&#22312;&#32534;&#36753;&#20219;&#21153;&#20013;&#24456;&#38590;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;DDPM&#30340;&#28508;&#22312;&#22122;&#22768;&#31354;&#38388;&#65292;&#21487;&#36890;&#36807;&#31616;&#21333;&#25163;&#27573;&#36827;&#34892;&#24191;&#27867;&#30340;&#32534;&#36753;&#25805;&#20316;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#20219;&#20309;&#32473;&#23450;&#22270;&#20687;&#65288;&#30495;&#23454;&#25110;&#21512;&#25104;&#29983;&#25104;&#65289;&#30340;&#26131;&#20110;&#32534;&#36753;&#22122;&#22768;&#22270;&#30340;&#21453;&#28436;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion probabilistic models (DDPMs) employ a sequence of white Gaussian noise samples to generate an image. In analogy with GANs, those noise maps could be considered as the latent code associated with the generated image. However, this native noise space does not possess a convenient structure, and is thus challenging to work with in editing tasks. Here, we propose an alternative latent noise space for DDPM that enables a wide range of editing operations via simple means, and present an inversion method for extracting these edit-friendly noise maps for any given image (real or synthetically generated). As opposed to the native DDPM noise space, the edit-friendly noise maps do not have a standard normal distribution and are not statistically independent across timesteps. However, they allow perfect reconstruction of any desired image, and simple transformations on them translate into meaningful manipulations of the output image (e.g., shifting, color edits). Moreover, in t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#31934;&#32454;&#25511;&#21046;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#23545;&#35937;&#23646;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#20256;&#25773;&#27880;&#20837;&#30340;&#22806;&#35266;&#21040;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.17546</link><description>&lt;p&gt;
PAIR-Diffusion: &#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models. (arXiv:2303.17546v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#31934;&#32454;&#25511;&#21046;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#23545;&#35937;&#23646;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#20256;&#25773;&#27880;&#20837;&#30340;&#22806;&#35266;&#21040;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#21457;&#23637;&#36805;&#36895;&#12290;&#20197;&#21069;&#30340;&#20316;&#21697;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#26041;&#24335;&#36827;&#34892;&#25511;&#21046;&#21644;&#32534;&#36753;&#22270;&#20687;&#65292;&#26576;&#20123;&#20316;&#21697;&#20351;&#29992;&#39640;&#32423;&#26465;&#20214;&#65288;&#20363;&#22914;&#25991;&#26412;&#65289;&#65292;&#32780;&#20854;&#20182;&#20316;&#21697;&#20351;&#29992;&#20302;&#32423;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20316;&#21697;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#19981;&#21516;&#23545;&#35937;&#30340;&#23646;&#24615;&#36827;&#34892;&#31934;&#32454;&#21270;&#25511;&#21046;&#65292;&#21363;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#12290;&#26412;&#25991;&#23558;&#22270;&#20687;&#35270;&#20026;&#30001;&#22810;&#20010;&#23545;&#35937;&#32452;&#25104;&#65292;&#27599;&#20010;&#23545;&#35937;&#30001;&#19981;&#21516;&#23646;&#24615;&#23450;&#20041;&#12290;&#25105;&#20204;&#21457;&#29616;&#32467;&#26500;&#21644;&#22806;&#35266;&#26159;&#26368;&#30452;&#35266;&#19988;&#26368;&#26377;&#29992;&#20110;&#32534;&#36753;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#65288;PAIR-Diffusion&#65289;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20174;&#22270;&#20687;&#20013;&#26126;&#30830;&#25552;&#21462;&#30340;&#32467;&#26500;&#21644;&#22806;&#35266;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#23545;&#35937;&#21644;&#20840;&#23616;&#32423;&#21035;&#23558;&#21442;&#32771;&#22270;&#20687;&#30340;&#22806;&#35266;&#27880;&#20837;&#36755;&#20837;&#22270;&#20687;&#20013;&#12290;&#27492;&#22806;&#65292;PAIR-Diffusion&#33258;&#21160;&#23558;&#27880;&#20837;&#30340;&#22806;&#35266;&#20256;&#25773;&#21040;&#36755;&#20837;&#22270;&#20687;&#20013;&#20855;&#26377;&#31867;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image editing using diffusion models has witnessed extremely fast-paced growth recently. There are various ways in which previous works enable controlling and editing images. Some works use high-level conditioning such as text, while others use low-level conditioning. Nevertheless, most of them lack fine-grained control over the properties of the different objects present in the image, i.e. object-level image editing. In this work, we consider an image as a composition of multiple objects, each defined by various properties. Out of these properties, we identify structure and appearance as the most intuitive to understand and useful for editing purposes. We propose Structure-and-Appearance Paired Diffusion model (PAIR-Diffusion), which is trained using structure and appearance information explicitly extracted from the images. The proposed model enables users to inject a reference image's appearance into the input image at both the object and global levels. Additionally, PAIR-Diffusion a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24230;&#24809;&#32602;&#12290;</title><link>http://arxiv.org/abs/2301.12554</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#24179;&#28369;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing. (arXiv:2301.12554v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24230;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22823;&#37327;&#22686;&#24378;&#31070;&#32463;&#20998;&#31867;&#22120;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22312;&#28165;&#26224;&#24230;&#26041;&#38754;&#23384;&#22312;&#19981;&#21487;&#25509;&#21463;&#30340;&#20005;&#37325;&#24809;&#32602;&#65292;&#23454;&#36341;&#32773;&#20173;&#28982;&#19981;&#24895;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#24378;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#20854;&#20013;&#26631;&#20934;&#32593;&#32476;&#20248;&#21270;&#28165;&#26224;&#24230;&#32780;&#19981;&#26159;&#19968;&#33324;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#26174;&#30528;&#20943;&#36731;&#36825;&#31181;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#22522;&#20110;&#40065;&#26834;&#24615;&#30340;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#31034;&#20363;&#30340;&#32622;&#20449;&#24230;&#24046;&#24322;&#26159;&#36825;&#31181;&#25913;&#21892;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#38500;&#25552;&#20379;&#30452;&#35266;&#21644;&#32463;&#39564;&#35777;&#25454;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#29616;&#23454;&#20551;&#35774;&#19979;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#19968;&#20010;&#23545;&#25239;&#24615;&#36755;&#20837;&#26816;&#27979;&#22120;&#36866;&#24212;&#20026;&#28151;&#21512;&#32593;&#32476;&#65292;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#28151;&#21512;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#20943;&#23569;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24615;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
While prior research has proposed a plethora of methods that enhance the adversarial robustness of neural classifiers, practitioners are still reluctant to adopt these techniques due to their unacceptably severe penalties in clean accuracy. This paper shows that by mixing the output probabilities of a standard classifier and a robust model, where the standard network is optimized for clean accuracy and is not robust in general, this accuracy-robustness trade-off can be significantly alleviated. We show that the robust base classifier's confidence difference for correct and incorrect examples is the key ingredient of this improvement. In addition to providing intuitive and empirical evidence, we also theoretically certify the robustness of the mixed classifier under realistic assumptions. Furthermore, we adapt an adversarial input detector into a mixing network that adaptively adjusts the mixture of the two base models, further reducing the accuracy penalty of achieving robustness. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20107;&#20214;&#20013;&#24515;&#30340;&#38598;&#21512;&#25490;&#21517;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#26102;&#38388;&#21160;&#24577;&#24615;&#65292;&#33021;&#22815;&#25512;&#33616;&#26368;&#30456;&#20851;&#30340;&#23454;&#20307;&#26041;&#38754;&#65292;&#25552;&#39640;&#25628;&#32034;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/1803.07890</link><description>&lt;p&gt;
&#25512;&#33616;&#23454;&#20307;&#30340;&#26102;&#38388;&#22240;&#32032;&#30340;&#22810;&#27169;&#22411;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multiple Models for Recommending Temporal Aspects of Entities. (arXiv:1803.07890v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1803.07890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20107;&#20214;&#20013;&#24515;&#30340;&#38598;&#21512;&#25490;&#21517;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#26102;&#38388;&#21160;&#24577;&#24615;&#65292;&#33021;&#22815;&#25512;&#33616;&#26368;&#30456;&#20851;&#30340;&#23454;&#20307;&#26041;&#38754;&#65292;&#25552;&#39640;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#26041;&#38754;&#30340;&#25512;&#33616;&#26159;&#35821;&#20041;&#25628;&#32034;&#20013;&#30340;&#26032;&#20852;&#20219;&#21153;&#65292;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#24039;&#21512;&#21644;&#31361;&#20986;&#20449;&#24687;&#65292;&#20854;&#20013;&#26174;&#30528;&#24615;&#65288;&#20363;&#22914;&#27969;&#34892;&#24230;&#65289;&#26159;&#20197;&#21069;&#24037;&#20316;&#20013;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#12290;&#20294;&#26159;&#65292;&#23454;&#20307;&#26041;&#38754;&#26159;&#20855;&#26377;&#26102;&#38388;&#21160;&#24577;&#24615;&#30340;&#65292;&#32463;&#24120;&#21463;&#21040;&#38543;&#26102;&#38388;&#21457;&#29983;&#30340;&#20107;&#20214;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20165;&#22522;&#20110;&#26174;&#30528;&#24615;&#29305;&#24449;&#30340;&#26041;&#38754;&#24314;&#35758;&#21487;&#33021;&#20250;&#32473;&#20986;&#20196;&#20154;&#19981;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#21407;&#22240;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#26174;&#30528;&#24615;&#36890;&#24120;&#22312;&#38271;&#26102;&#38388;&#27573;&#20869;&#32047;&#31215;&#65292;&#24182;&#19988;&#19981;&#32771;&#34385;&#26368;&#36817;&#24773;&#20917;&#12290;&#20854;&#27425;&#65292;&#19982;&#20107;&#20214;&#23454;&#20307;&#30456;&#20851;&#30340;&#35768;&#22810;&#26041;&#38754;&#24378;&#28872;&#20381;&#36182;&#20110;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;&#32473;&#23450;&#23454;&#20307;&#30340;&#26102;&#38388;&#26041;&#38754;&#25512;&#33616;&#20219;&#21153;&#65292;&#26088;&#22312;&#25512;&#33616;&#26368;&#30456;&#20851;&#30340;&#26041;&#38754;&#65292;&#24182;&#32771;&#34385;&#26102;&#38388;&#20197;&#25552;&#39640;&#25628;&#32034;&#20307;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20107;&#20214;&#20013;&#24515;&#30340;&#38598;&#21512;&#25490;&#21517;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#22810;&#20010;&#26102;&#38388;&#21644;&#31867;&#22411;&#20381;&#36182;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#65292;&#24182;&#21160;&#24577;&#26435;&#34913;&#26174;&#30528;&#24615;&#21644;&#26368;&#36817;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity aspect recommendation is an emerging task in semantic search that helps users discover serendipitous and prominent information with respect to an entity, of which salience (e.g., popularity) is the most important factor in previous work. However, entity aspects are temporally dynamic and often driven by events happening over time. For such cases, aspect suggestion based solely on salience features can give unsatisfactory results, for two reasons. First, salience is often accumulated over a long time period and does not account for recency. Second, many aspects related to an event entity are strongly time-dependent. In this paper, we study the task of temporal aspect recommendation for a given entity, which aims at recommending the most relevant aspects and takes into account time in order to improve search experience. We propose a novel event-centric ensemble ranking method that learns from multiple time and type-dependent models and dynamically trades off salience and recency c
&lt;/p&gt;</description></item></channel></rss>