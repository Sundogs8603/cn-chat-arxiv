<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#39134;&#26426;&#21457;&#21160;&#26426;&#32500;&#20462;&#24773;&#20917;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#21644;&#20998;&#26512;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.13310</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#39134;&#26426;&#21457;&#21160;&#26426;&#39044;&#27979;&#32500;&#20462;&#30340;&#21487;&#35299;&#37322;&#24615;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Systematic Review of Machine Learning Models for Predictive Maintenance of Aircraft Engine. (arXiv:2309.13310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#39134;&#26426;&#21457;&#21160;&#26426;&#32500;&#20462;&#24773;&#20917;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#21644;&#20998;&#26512;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#30340;&#32508;&#36848;&#65292;&#23545;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#39044;&#27979;&#39134;&#26426;&#21457;&#21160;&#26426;&#30340;&#32500;&#20462;&#24773;&#20917;&#65292;&#20197;&#36991;&#20813;&#20219;&#20309;&#28798;&#38590;&#30340;&#21457;&#29983;&#12290;&#35813;&#31574;&#30053;&#30340;&#20248;&#28857;&#20043;&#19968;&#26159;&#21487;&#20197;&#20351;&#29992;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#20351;&#29992;LSTM&#12289;Bi-LSTM&#12289;RNN&#12289;Bi-RNN&#12289;GRU&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;KNN&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;&#26799;&#24230;&#25552;&#21319;&#31561;&#26041;&#27861;&#36827;&#34892;&#39134;&#26426;&#21457;&#21160;&#26426;&#25925;&#38556;&#39044;&#27979;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#29983;&#25104;&#39044;&#27979;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#19968;&#20010;&#25968;&#25454;&#28304;&#30340;&#31616;&#21333;&#24773;&#20917;&#12290;&#25105;&#20204;&#20351;&#29992;lime&#26469;&#35299;&#37322;&#27169;&#22411;&#65292;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#20026;&#20160;&#20040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#19981;&#22914;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23545;&#22810;&#20010;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#22411;&#34892;&#20026;&#20998;&#26512;&#65292;&#20197;&#29702;&#35299;&#27169;&#22411;&#40657;&#30418;&#24773;&#20917;&#12290;GRU&#12289;Bi-LSTM&#21644;LSTM&#20998;&#21035;&#23454;&#29616;&#20102;97.8%&#12289;97.14%&#21644;96.42%&#30340;&#36739;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an interpretable review of various machine learning and deep learning models to predict the maintenance of aircraft engine to avoid any kind of disaster. One of the advantages of the strategy is that it can work with modest datasets. In this study, sensor data is utilized to predict aircraft engine failure within a predetermined number of cycles using LSTM, Bi-LSTM, RNN, Bi-RNN GRU, Random Forest, KNN, Naive Bayes, and Gradient Boosting. We explain how deep learning and machine learning can be used to generate predictions in predictive maintenance using a straightforward scenario with just one data source. We applied lime to the models to help us understand why machine learning models did not perform well than deep learning models. An extensive analysis of the model's behavior is presented for several test data to understand the black box scenario of the models. A lucrative accuracy of 97.8%, 97.14%, and 96.42% are achieved by GRU, Bi-LSTM, and LSTM respectively whi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CORE&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#36890;&#36807;&#25237;&#24433;&#21644;&#37325;&#26500;&#20449;&#24687;&#26469;&#20943;&#23569;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13307</link><description>&lt;p&gt;
CORE: &#21487;&#35777;&#26126;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#36890;&#29992;&#38543;&#26426;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
CORE: Common Random Reconstruction for Distributed Optimization with Provable Low Communication Complexity. (arXiv:2309.13307v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13307
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CORE&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#36890;&#36807;&#25237;&#24433;&#21644;&#37325;&#26500;&#20449;&#24687;&#26469;&#20943;&#23569;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25104;&#20026;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#36890;&#20449;&#22797;&#26434;&#24230;&#24050;&#25104;&#20026;&#21152;&#36895;&#35757;&#32451;&#21644;&#25193;&#23637;&#26426;&#22120;&#25968;&#37327;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CORE&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#22312;&#27809;&#26377;&#20854;&#20182;&#20005;&#26684;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#21387;&#32553;&#26426;&#22120;&#20043;&#38388;&#20256;&#36755;&#30340;&#20449;&#24687;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#23558;&#21521;&#37327;&#20540;&#20449;&#24687;&#36890;&#36807;&#20849;&#21516;&#38543;&#26426;&#21521;&#37327;&#25237;&#24433;&#21040;&#20302;&#32500;&#31354;&#38388;&#65292;&#24182;&#22312;&#36890;&#20449;&#21518;&#20351;&#29992;&#30456;&#21516;&#30340;&#38543;&#26426;&#22122;&#22768;&#23545;&#20449;&#24687;&#36827;&#34892;&#37325;&#26500;&#12290;&#25105;&#20204;&#23558;CORE&#24212;&#29992;&#20110;&#20004;&#20010;&#20998;&#24067;&#24335;&#20219;&#21153;&#65292;&#20998;&#21035;&#26159;&#32447;&#24615;&#27169;&#22411;&#19978;&#30340;&#20984;&#20248;&#21270;&#21644;&#36890;&#29992;&#38750;&#20984;&#20248;&#21270;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#26356;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#32447;&#24615;&#27169;&#22411;&#65292;&#22522;&#20110;CORE&#30340;&#31639;&#27861;&#21487;&#20197;&#23558;&#26799;&#24230;&#21521;&#37327;&#32534;&#30721;&#20026;O(1)&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
With distributed machine learning being a prominent technique for large-scale machine learning tasks, communication complexity has become a major bottleneck for speeding up training and scaling up machine numbers. In this paper, we propose a new technique named Common randOm REconstruction(CORE), which can be used to compress the information transmitted between machines in order to reduce communication complexity without other strict conditions. Especially, our technique CORE projects the vector-valued information to a low-dimensional one through common random vectors and reconstructs the information with the same random noises after communication. We apply CORE to two distributed tasks, respectively convex optimization on linear models and generic non-convex optimization, and design new distributed algorithms, which achieve provably lower communication complexities. For example, we show for linear models CORE-based algorithm can encode the gradient vector to $\mathcal{O}(1)$-bits (aga
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;C$^2$VAE&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#38750;&#32806;&#21512;&#19988;&#30456;&#20851;&#30340;&#38544;&#34255;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#20998;&#31867;&#22120;&#28040;&#38500;&#32806;&#21512;&#34920;&#31034;&#65292;&#20197;&#22686;&#24378;&#38750;&#32806;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#22312;&#19981;&#20381;&#36182;&#20808;&#39564;&#30693;&#35782;&#21644;&#24378;&#24314;&#27169;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#24635;&#30456;&#20851;&#39537;&#21160;&#20998;&#35299;&#21518;&#39564;&#26469;&#23398;&#20064;&#22240;&#23376;&#21270;&#30340;&#38750;&#32806;&#21512;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#39640;&#26031;Copula&#27169;&#22411;&#25552;&#21462;&#38544;&#34255;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#33719;&#24471;&#32806;&#21512;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.13303</link><description>&lt;p&gt;
C$^2$VAE&#65306;&#22522;&#20110;&#39640;&#26031;Copula&#30340;VAE&#19982;&#23545;&#27604;&#21518;&#39564;&#30340;&#38750;&#32806;&#21512;&#19982;&#38750;&#32806;&#21512;&#34920;&#31034;&#26377;&#24046;&#24322;&#30340;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
C$^2$VAE: Gaussian Copula-based VAE Differing Disentangled from Coupled Representations with Contrastive Posterior. (arXiv:2309.13303v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13303
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;C$^2$VAE&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#38750;&#32806;&#21512;&#19988;&#30456;&#20851;&#30340;&#38544;&#34255;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#20998;&#31867;&#22120;&#28040;&#38500;&#32806;&#21512;&#34920;&#31034;&#65292;&#20197;&#22686;&#24378;&#38750;&#32806;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#22312;&#19981;&#20381;&#36182;&#20808;&#39564;&#30693;&#35782;&#21644;&#24378;&#24314;&#27169;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#24635;&#30456;&#20851;&#39537;&#21160;&#20998;&#35299;&#21518;&#39564;&#26469;&#23398;&#20064;&#22240;&#23376;&#21270;&#30340;&#38750;&#32806;&#21512;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#39640;&#26031;Copula&#27169;&#22411;&#25552;&#21462;&#38544;&#34255;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#33719;&#24471;&#32806;&#21512;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#20197;&#32852;&#21512;&#23398;&#20064;&#38750;&#32806;&#21512;&#19988;&#30456;&#20851;&#30340;&#38544;&#34255;&#22240;&#32032;&#65292;&#28982;&#21518;&#36890;&#36807;&#33258;&#30417;&#30563;&#20998;&#31867;&#22120;&#22686;&#24378;&#38750;&#32806;&#21512;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#23545;&#27604;&#26041;&#24335;&#28040;&#38500;&#32806;&#21512;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#20381;&#36182;&#20808;&#39564;&#30693;&#35782;&#21644;&#22312;&#31070;&#32463;&#26550;&#26500;&#20013;&#28041;&#21450;&#21518;&#39564;&#30340;&#24378;&#24314;&#27169;&#20551;&#35774;&#30340;&#23545;&#27604;Copula VAE&#65288;C$^2$VAE&#65289;&#12290;C$^2$VAE&#20351;&#29992;&#24635;&#30456;&#20851;&#65288;TC&#65289;&#39537;&#21160;&#20998;&#35299;&#26469;&#22240;&#23376;&#21270;&#21518;&#39564;&#65288;ELBO&#65289;&#65292;&#20197;&#23398;&#20064;&#22240;&#23376;&#21270;&#30340;&#38750;&#32806;&#21512;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#39640;&#26031;Copula&#25552;&#21462;&#38544;&#34255;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#20197;&#33719;&#24471;&#32806;&#21512;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#33258;&#30417;&#30563;&#23545;&#27604;&#20998;&#31867;&#22120;&#21306;&#20998;&#38750;&#32806;&#21512;&#34920;&#31034;&#21644;&#32806;&#21512;&#34920;&#31034;&#65292;&#20854;&#20013;&#23545;&#27604;&#25439;&#22833;&#29992;&#20110;&#27491;&#21017;&#21270;&#35813;&#23545;&#27604;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a self-supervised variational autoencoder (VAE) to jointly learn disentangled and dependent hidden factors and then enhance disentangled representation learning by a self-supervised classifier to eliminate coupled representations in a contrastive manner. To this end, a Contrastive Copula VAE (C$^2$VAE) is introduced without relying on prior knowledge about data in the probabilistic principle and involving strong modeling assumptions on the posterior in the neural architecture. C$^2$VAE simultaneously factorizes the posterior (evidence lower bound, ELBO) with total correlation (TC)-driven decomposition for learning factorized disentangled representations and extracts the dependencies between hidden features by a neural Gaussian copula for copula coupled representations. Then, a self-supervised contrastive classifier differentiates the disentangled representations from the coupled representations, where a contrastive loss regularizes this contrastive classification together wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;GradCAM&#30340;&#29305;&#24449;&#23631;&#34109;&#32467;&#21512;&#38598;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#24180;&#40836;&#20559;&#35265;&#30340;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#65292;&#26082;&#20445;&#35777;&#20102;&#20844;&#24179;&#24615;&#21448;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13292</link><description>&lt;p&gt;
&#36229;&#36234;&#20844;&#24179;&#65306;&#36890;&#36807;&#35821;&#38899;&#26816;&#27979;&#26080;&#24180;&#40836;&#20559;&#35265;&#30340;&#24085;&#37329;&#26862;&#30149;
&lt;/p&gt;
&lt;p&gt;
Beyond Fairness: Age-Harmless Parkinson's Detection via Voice. (arXiv:2309.13292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13292
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;GradCAM&#30340;&#29305;&#24449;&#23631;&#34109;&#32467;&#21512;&#38598;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#24180;&#40836;&#20559;&#35265;&#30340;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#65292;&#26082;&#20445;&#35777;&#20102;&#20844;&#24179;&#24615;&#21448;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#26159;&#19968;&#31181;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#36890;&#24120;&#34920;&#29616;&#20026;&#35821;&#38899;&#21644;&#22768;&#38899;&#21151;&#33021;&#38556;&#30861;&#12290;&#34429;&#28982;&#21033;&#29992;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;PD&#26816;&#27979;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#26377;&#24456;&#22823;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#21516;&#24180;&#40836;&#21457;&#30149;&#38454;&#27573;&#23384;&#22312;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#36825;&#20123;&#28145;&#24230;&#27169;&#22411;&#22312;&#32769;&#24180;&#20154;&#32676;&#65288;&#24180;&#40836;&gt; 55&#23681;&#65289;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#24180;&#36731;&#20154;&#32676;&#65288;&#24180;&#40836;&#8804;55&#23681;&#65289;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#35843;&#26597;&#65292;&#32769;&#24180;&#20154;&#21644;&#24180;&#36731;&#20154;&#20043;&#38388;&#30340;&#24046;&#24322;&#26159;&#30001;&#20110;1&#65289;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#21644;2&#65289;&#26089;&#21457;&#24739;&#32773;&#24120;&#35265;&#30340;&#36739;&#36731;&#30151;&#29366;&#24341;&#36215;&#30340;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#19981;&#23454;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20250;&#38477;&#20302;&#22823;&#22810;&#25968;&#32676;&#20307;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23567;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;GradCAM&#30340;&#29305;&#24449;&#23631;&#34109;&#32467;&#21512;&#38598;&#25104;&#27169;&#22411;&#65292;&#30830;&#20445;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#22343;&#19981;&#21463;&#25439;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22522;&#20110;GradCAM&#30340;&#29305;&#24449;&#23631;&#34109;&#36873;&#25321;&#29305;&#24449;&#65292;&#21512;&#24182;&#20102;ensemble models&#65292;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD), a neurodegenerative disorder, often manifests as speech and voice dysfunction. While utilizing voice data for PD detection has great potential in clinical applications, the widely used deep learning models currently have fairness issues regarding different ages of onset. These deep models perform well for the elderly group (age $&gt;$ 55) but are less accurate for the young group (age $\leq$ 55). Through our investigation, the discrepancy between the elderly and the young arises due to 1) an imbalanced dataset and 2) the milder symptoms often seen in early-onset patients. However, traditional debiasing methods are impractical as they typically impair the prediction accuracy for the majority group while minimizing the discrepancy. To address this issue, we present a new debiasing method using GradCAM-based feature masking combined with ensemble models, ensuring that neither fairness nor accuracy is compromised. Specifically, the GradCAM-based feature masking selec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#38750;&#21516;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#30340;&#32479;&#19968;&#35823;&#24046;&#37327;&#21270;&#26694;&#26550;&#65292;&#24182;&#35299;&#20915;&#20102;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#21306;&#38388;&#20869;&#20849;&#21516;&#37327;&#21270;&#20004;&#20010;&#20272;&#35745;&#35823;&#24046;&#28304;&#65292;&#35813;&#26694;&#26550;&#25581;&#31034;&#20102;&#20043;&#21069;&#38544;&#34255;&#30340;&#35823;&#24046;&#26435;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32622;&#20449;&#21306;&#38388;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13278</link><description>&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;&#24863;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;&#38750;&#21516;&#31574;&#30053;&#21306;&#38388;&#20272;&#35745;&#26041;&#27861;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#35823;&#24046;&#37327;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Distributional Shift-Aware Off-Policy Interval Estimation: A Unified Error Quantification Framework. (arXiv:2309.13278v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#38750;&#21516;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#30340;&#32479;&#19968;&#35823;&#24046;&#37327;&#21270;&#26694;&#26550;&#65292;&#24182;&#35299;&#20915;&#20102;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#21306;&#38388;&#20869;&#20849;&#21516;&#37327;&#21270;&#20004;&#20010;&#20272;&#35745;&#35823;&#24046;&#28304;&#65292;&#35813;&#26694;&#26550;&#25581;&#31034;&#20102;&#20043;&#21069;&#38544;&#34255;&#30340;&#35823;&#24046;&#26435;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32622;&#20449;&#21306;&#38388;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#38480;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#39640;&#32622;&#20449;&#24230;&#38750;&#21516;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#20165;&#21033;&#29992;&#20174;&#26410;&#30693;&#34892;&#20026;&#31574;&#30053;&#39044;&#20808;&#25910;&#38598;&#30340;&#31163;&#32447;&#25968;&#25454;&#20026;&#30446;&#26631;&#31574;&#30053;&#30340;&#20540;&#24314;&#31435;&#19968;&#20010;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#12290;&#35813;&#20219;&#21153;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#22312;CI&#20272;&#35745;&#20013;&#25552;&#20379;&#20840;&#38754;&#19988;&#20005;&#26684;&#30340;&#35823;&#24046;&#37327;&#21270;&#65292;&#24182;&#35299;&#20915;&#30001;&#30446;&#26631;&#31574;&#30053;&#20135;&#29983;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#35813;&#20998;&#24067;&#19982;&#31163;&#32447;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#21463;&#21040;&#21019;&#26032;&#30340;&#32479;&#19968;&#35823;&#24046;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#21306;&#38388;&#20869;&#20849;&#21516;&#37327;&#21270;&#20004;&#20010;&#20272;&#35745;&#35823;&#24046;&#26469;&#28304;&#65306;&#22312;&#24314;&#27169;&#36793;&#38469;&#21270;&#37325;&#35201;&#24615;&#26435;&#37325;&#26102;&#30340;&#35268;&#33539;&#19981;&#20934;&#30830;&#35823;&#24046;&#21644;&#25277;&#26679;&#23548;&#33268;&#30340;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#19968;&#32479;&#19968;&#30340;&#26694;&#26550;&#25581;&#31034;&#20102;&#35823;&#24046;&#20043;&#38388;&#20197;&#21069;&#38544;&#34255;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;CI&#30340;&#32039;&#23494;&#24615;&#12290;&#36890;&#36807;&#20381;&#38752;&#31934;&#24515;&#35774;&#35745;&#30340;&#21028;&#21035;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20811;&#26381;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study high-confidence off-policy evaluation in the context of infinite-horizon Markov decision processes, where the objective is to establish a confidence interval (CI) for the target policy value using only offline data pre-collected from unknown behavior policies. This task faces two primary challenges: providing a comprehensive and rigorous error quantification in CI estimation, and addressing the distributional shift that results from discrepancies between the distribution induced by the target policy and the offline data-generating process. Motivated by an innovative unified error analysis, we jointly quantify the two sources of estimation errors: the misspecification error on modeling marginalized importance weights and the statistical uncertainty due to sampling, within a single interval. This unified framework reveals a previously hidden tradeoff between the errors, which undermines the tightness of the CI. Relying on a carefully designed discriminator function, the proposed
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36328;&#39046;&#22495;&#20219;&#21153;&#30340;&#26377;&#24207;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#65288;OCR&#65289;&#65292;&#36890;&#36807;&#20445;&#25345;&#39044;&#27979;&#30340;&#26377;&#24207;&#24615;&#65292;&#20351;&#27169;&#22411;&#23545;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#23646;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#36328;&#39046;&#22495;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.13258</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#36866;&#24212;&#21644;&#27867;&#21270;&#30340;&#26377;&#24207;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Order-preserving Consistency Regularization for Domain Adaptation and Generalization. (arXiv:2309.13258v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13258
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36328;&#39046;&#22495;&#20219;&#21153;&#30340;&#26377;&#24207;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#65288;OCR&#65289;&#65292;&#36890;&#36807;&#20445;&#25345;&#39044;&#27979;&#30340;&#26377;&#24207;&#24615;&#65292;&#20351;&#27169;&#22411;&#23545;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#23646;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#36328;&#39046;&#22495;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23646;&#24615;&#65288;&#22914;&#20809;&#32447;&#12289;&#32972;&#26223;&#12289;&#30456;&#26426;&#35282;&#24230;&#31561;&#65289;&#36807;&#20110;&#25935;&#24863;&#65292;&#37027;&#20040;&#22312;&#36328;&#39046;&#22495;&#25361;&#25112;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20250;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#24120;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23646;&#24615;&#19981;&#37027;&#20040;&#25935;&#24863;&#12290;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#24378;&#21046;&#27169;&#22411;&#23545;&#21516;&#19968;&#22270;&#20687;&#30340;&#20004;&#20010;&#35270;&#35282;&#36755;&#20986;&#30456;&#21516;&#30340;&#34920;&#31034;&#25110;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32422;&#26463;&#23545;&#20110;&#20998;&#31867;&#27010;&#29575;&#26469;&#35828;&#35201;&#20040;&#36807;&#20110;&#20005;&#26684;&#65292;&#35201;&#20040;&#19981;&#20855;&#26377;&#26377;&#24207;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#36328;&#39046;&#22495;&#20219;&#21153;&#30340;&#26377;&#24207;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#65288;OCR&#65289;&#12290;&#39044;&#27979;&#30340;&#26377;&#24207;&#24615;&#20351;&#27169;&#22411;&#23545;&#20110;&#20219;&#21153;&#26080;&#20851;&#30340;&#21464;&#25442;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#32467;&#26524;&#65292;&#27169;&#22411;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23646;&#24615;&#19981;&#37027;&#20040;&#25935;&#24863;&#12290;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#36328;&#39046;&#22495;&#20219;&#21153;&#19978;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models fail on cross-domain challenges if the model is oversensitive to domain-specific attributes, e.g., lightning, background, camera angle, etc. To alleviate this problem, data augmentation coupled with consistency regularization are commonly adopted to make the model less sensitive to domain-specific attributes. Consistency regularization enforces the model to output the same representation or prediction for two views of one image. These constraints, however, are either too strict or not order-preserving for the classification probabilities. In this work, we propose the Order-preserving Consistency Regularization (OCR) for cross-domain tasks. The order-preserving property for the prediction makes the model robust to task-irrelevant transformations. As a result, the model becomes less sensitive to the domain-specific attributes. The comprehensive experiments show that our method achieves clear advantages on five different cross-domain tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#30340;&#23433;&#20840;&#39118;&#38505;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20854;&#26497;&#26131;&#21463;&#21040;&#21453;&#21521;&#25915;&#20987;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDP&#30340;&#36731;&#37327;&#32423;&#12289;&#21487;&#25554;&#25300;&#19988;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#34987;&#27745;&#26579;&#26679;&#26412;&#21644;&#28165;&#27905;&#26679;&#26412;&#20043;&#38388;&#30340;&#25513;&#30721;&#25935;&#24863;&#24615;&#24046;&#36317;&#26469;&#35782;&#21035;&#27745;&#26579;&#26679;&#26412;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;MDP&#22312;&#25915;&#20987;&#25928;&#26524;&#21644;&#26816;&#27979;&#36867;&#36991;&#24615;&#20043;&#38388;&#24418;&#25104;&#20102;&#36827;&#36864;&#20004;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.13256</link><description>&lt;p&gt;
&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#30340;&#21453;&#21521;&#25915;&#20987;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks. (arXiv:2309.13256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#30340;&#23433;&#20840;&#39118;&#38505;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20854;&#26497;&#26131;&#21463;&#21040;&#21453;&#21521;&#25915;&#20987;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDP&#30340;&#36731;&#37327;&#32423;&#12289;&#21487;&#25554;&#25300;&#19988;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#34987;&#27745;&#26579;&#26679;&#26412;&#21644;&#28165;&#27905;&#26679;&#26412;&#20043;&#38388;&#30340;&#25513;&#30721;&#25935;&#24863;&#24615;&#24046;&#36317;&#26469;&#35782;&#21035;&#27745;&#26579;&#26679;&#26412;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;MDP&#22312;&#25915;&#20987;&#25928;&#26524;&#21644;&#26816;&#27979;&#36867;&#36991;&#24615;&#20043;&#38388;&#24418;&#25104;&#20102;&#36827;&#36864;&#20004;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#23427;&#20204;&#30340;&#23433;&#20840;&#39118;&#38505;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#65292;&#34920;&#26126;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#30340;PLMs&#26497;&#26131;&#21463;&#21040;&#21453;&#21521;&#25915;&#20987;&#65292;&#32780;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#30001;&#20110;&#23569;&#26679;&#26412;&#24773;&#22659;&#30340;&#29420;&#29305;&#25361;&#25112;&#32780;&#19981;&#36275;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MDP&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#36731;&#37327;&#32423;&#12289;&#21487;&#25554;&#25300;&#19988;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MDP&#21033;&#29992;&#20102;&#34987;&#27745;&#26579;&#26679;&#26412;&#21644;&#28165;&#27905;&#26679;&#26412;&#20043;&#38388;&#30340;&#25513;&#30721;&#25935;&#24863;&#24615;&#24046;&#36317;&#65306;&#21442;&#32771;&#26377;&#38480;&#30340;&#23569;&#26679;&#26412;&#25968;&#25454;&#20316;&#20026;&#20998;&#24067;&#38170;&#28857;&#65292;&#23427;&#27604;&#36739;&#19981;&#21516;&#25513;&#30721;&#19979;&#32473;&#23450;&#26679;&#26412;&#30340;&#34920;&#31034;&#65292;&#24182;&#35782;&#21035;&#20986;&#20855;&#26377;&#26174;&#33879;&#21464;&#21270;&#30340;&#34987;&#27745;&#26579;&#26679;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#34920;&#26126;&#65292;MDP&#23545;&#20110;&#25915;&#20987;&#32773;&#22312;&#25915;&#20987;&#25928;&#26524;&#21644;&#26816;&#27979;&#36867;&#36991;&#24615;&#20043;&#38388;&#20135;&#29983;&#20102;&#26377;&#36259;&#30340;&#36827;&#36864;&#20004;&#38590;&#12290;&#23454;&#35777;&#35780;&#20272;&#20351;&#29992;be
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have demonstrated remarkable performance as few-shot learners. However, their security risks under such settings are largely unexplored. In this work, we conduct a pilot study showing that PLMs as few-shot learners are highly vulnerable to backdoor attacks while existing defenses are inadequate due to the unique challenges of few-shot scenarios. To address such challenges, we advocate MDP, a novel lightweight, pluggable, and effective defense for PLMs as few-shot learners. Specifically, MDP leverages the gap between the masking-sensitivity of poisoned and clean samples: with reference to the limited few-shot data as distributional anchors, it compares the representations of given samples under varying masking and identifies poisoned samples as ones with significant variations. We show analytically that MDP creates an interesting dilemma for the attacker to choose between attack effectiveness and detection evasiveness. The empirical evaluation using be
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Zen&#65292;&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#24335;DNN&#35757;&#32451;&#20013;&#36817;&#20284;&#26368;&#20248;&#31232;&#30095;&#24352;&#37327;&#21516;&#27493;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#27969;&#34892;&#30340;DNN&#27169;&#22411;&#20013;&#31232;&#30095;&#24352;&#37327;&#30340;&#29305;&#24615;&#65292;&#24182;&#31995;&#32479;&#22320;&#25506;&#32034;&#35774;&#35745;&#31354;&#38388;&#65292;&#25214;&#21040;&#20102;&#26368;&#20339;&#30340;&#36890;&#20449;&#26041;&#26696;&#12290;&#36890;&#36807;&#20943;&#23569;&#36890;&#20449;&#27969;&#37327;&#21644;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#65292;Zen&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.13254</link><description>&lt;p&gt;
Zen&#65306;&#29992;&#20110;&#20998;&#24067;&#24335;DNN&#35757;&#32451;&#30340;&#36817;&#20284;&#26368;&#20248;&#31232;&#30095;&#24352;&#37327;&#21516;&#27493;
&lt;/p&gt;
&lt;p&gt;
Zen: Near-Optimal Sparse Tensor Synchronization for Distributed DNN Training. (arXiv:2309.13254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13254
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Zen&#65292;&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#24335;DNN&#35757;&#32451;&#20013;&#36817;&#20284;&#26368;&#20248;&#31232;&#30095;&#24352;&#37327;&#21516;&#27493;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#27969;&#34892;&#30340;DNN&#27169;&#22411;&#20013;&#31232;&#30095;&#24352;&#37327;&#30340;&#29305;&#24615;&#65292;&#24182;&#31995;&#32479;&#22320;&#25506;&#32034;&#35774;&#35745;&#31354;&#38388;&#65292;&#25214;&#21040;&#20102;&#26368;&#20339;&#30340;&#36890;&#20449;&#26041;&#26696;&#12290;&#36890;&#36807;&#20943;&#23569;&#36890;&#20449;&#27969;&#37327;&#21644;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#65292;Zen&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#35757;&#32451;&#26159;&#20351;&#29992;&#22810;&#20010;GPU&#25193;&#23637;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#35757;&#32451;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#24615;&#33021;&#29942;&#39048;&#22312;&#20110;&#28176;&#21464;&#21516;&#27493;&#30340;&#36890;&#20449;&#12290;&#26368;&#36817;&#65292;&#23454;&#36341;&#32773;&#35266;&#23519;&#21040;&#28176;&#21464;&#24352;&#37327;&#20013;&#23384;&#22312;&#31232;&#30095;&#24615;&#65292;&#34920;&#26126;&#21487;&#20197;&#20943;&#23569;&#36890;&#20449;&#30340;&#27969;&#37327;&#24182;&#25552;&#39640;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23436;&#20840;&#21457;&#25381;&#31232;&#30095;&#24615;&#30340;&#26368;&#20339;&#36890;&#20449;&#26041;&#26696;&#20173;&#28982;&#32570;&#22833;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#27969;&#34892;DNN&#27169;&#22411;&#20013;&#31232;&#30095;&#24352;&#37327;&#30340;&#29305;&#24615;&#65292;&#20197;&#20102;&#35299;&#31232;&#30095;&#24615;&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#31232;&#30095;&#24352;&#37327;&#36890;&#20449;&#26041;&#26696;&#30340;&#35774;&#35745;&#31354;&#38388;&#24182;&#25214;&#21040;&#20102;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed training is the de facto standard to scale up the training of Deep Neural Networks (DNNs) with multiple GPUs. The performance bottleneck of distributed training lies in communications for gradient synchronization. Recently, practitioners have observed sparsity in gradient tensors, suggesting the potential to reduce the traffic volume in communication and improve end-to-end training efficiency. Yet, the optimal communication scheme to fully leverage sparsity is still missing. This paper aims to address this gap. We first analyze the characteristics of sparse tensors in popular DNN models to understand the fundamentals of sparsity. We then systematically explore the design space of communication schemes for sparse tensors and find the optimal one. % We then find the optimal scheme based on the characteristics by systematically exploring the design space. We also develop a gradient synchronization system called Zen that approximately realizes it for sparse tensors. We demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#21333;&#35843;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20102;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#21644;&#27169;&#22411;&#30340;&#21333;&#35843;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.13246</link><description>&lt;p&gt;
&#25105;&#21487;&#20197;&#30456;&#20449;&#35299;&#37322;&#21527;&#65311;&#30740;&#31350;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#21333;&#35843;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Can I Trust the Explanations? Investigating Explainable Machine Learning Methods for Monotonic Models. (arXiv:2309.13246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#21333;&#35843;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20102;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#21644;&#27169;&#22411;&#30340;&#21333;&#35843;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37117;&#26159;&#24212;&#29992;&#20110;&#40657;&#30418;&#27169;&#22411;&#32780;&#27809;&#26377;&#20219;&#20309;&#39046;&#22495;&#30693;&#35782;&#12290;&#36890;&#36807;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#31185;&#23398;&#20026;&#22522;&#30784;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#35299;&#37322;&#24615;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#25105;&#20204;&#23558;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#20110;&#31185;&#23398;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#33719;&#24471;&#19968;&#33268;&#30340;&#31185;&#23398;&#35299;&#37322;&#21527;&#65311;&#36825;&#20010;&#38382;&#39064;&#22312;&#23637;&#31034;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#21333;&#35843;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#20102;&#22238;&#31572;&#12290;&#20026;&#20102;&#23637;&#31034;&#21333;&#35843;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#20844;&#29702;&#12290;&#30456;&#24212;&#22320;&#65292;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20165;&#28041;&#21450;&#20010;&#20307;&#21333;&#35843;&#24615;&#26102;&#65292;&#22522;&#20934;Shapley&#20540;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#35299;&#37322;&#65307;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#24378;&#22823;&#30340;&#25104;&#23545;&#21333;&#35843;&#24615;&#26102;&#65292;&#38598;&#25104;&#26799;&#24230;&#26041;&#27861;&#22312;&#24179;&#22343;&#19978;&#25552;&#20379;&#20102;&#21512;&#29702;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, explainable machine learning methods have been very successful. Despite their success, most explainable machine learning methods are applied to black-box models without any domain knowledge. By incorporating domain knowledge, science-informed machine learning models have demonstrated better generalization and interpretation. But do we obtain consistent scientific explanations if we apply explainable machine learning methods to science-informed machine learning models? This question is addressed in the context of monotonic models that exhibit three different types of monotonicity. To demonstrate monotonicity, we propose three axioms. Accordingly, this study shows that when only individual monotonicity is involved, the baseline Shapley value provides good explanations; however, when strong pairwise monotonicity is involved, the Integrated gradients method provides reasonable explanations on average.
&lt;/p&gt;</description></item><item><title>&#24369;&#26631;&#31614;&#23398;&#20064;&#20013;&#36873;&#25321;&#36127;&#23454;&#20363;&#30340;&#31574;&#30053;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#31181;&#34913;&#37327;&#36127;&#23454;&#20363;&#26377;&#29992;&#24615;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#24369;&#26631;&#31614;&#20998;&#31867;&#24615;&#33021;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.13227</link><description>&lt;p&gt;
&#24369;&#26631;&#31614;&#23398;&#20064;&#20013;&#36127;&#37319;&#26679;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Importance of negative sampling in weak label learning. (arXiv:2309.13227v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13227
&lt;/p&gt;
&lt;p&gt;
&#24369;&#26631;&#31614;&#23398;&#20064;&#20013;&#36873;&#25321;&#36127;&#23454;&#20363;&#30340;&#31574;&#30053;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#31181;&#34913;&#37327;&#36127;&#23454;&#20363;&#26377;&#29992;&#24615;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#24369;&#26631;&#31614;&#20998;&#31867;&#24615;&#33021;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#26631;&#31614;&#23398;&#20064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#38656;&#35201;&#20174;&#21253;&#21547;&#27491;&#23454;&#20363;&#21644;&#36127;&#23454;&#20363;&#30340;&#25968;&#25454;&#8220;&#21253;&#8221;&#20013;&#23398;&#20064;&#65292;&#20294;&#21482;&#30693;&#36947;&#21253;&#30340;&#26631;&#31614;&#12290;&#36127;&#23454;&#20363;&#30340;&#25968;&#37327;&#36890;&#24120;&#27604;&#27491;&#23454;&#20363;&#22810;&#65292;&#22240;&#27492;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#36127;&#23454;&#20363;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#36127;&#23454;&#20363;&#36873;&#25321;&#31574;&#30053;&#22312;&#24369;&#26631;&#31614;&#23398;&#20064;&#20013;&#23578;&#26410;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#31181;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#34913;&#37327;&#36127;&#23454;&#20363;&#22312;&#24369;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#36873;&#25321;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#21644;AudioSet&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#24369;&#26631;&#31614;&#20998;&#31867;&#24615;&#33021;&#19978;&#26377;&#25152;&#25552;&#21319;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#38543;&#26426;&#37319;&#26679;&#26041;&#27861;&#30456;&#27604;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#36127;&#23454;&#20363;&#24182;&#38750;&#20840;&#37117;&#26080;&#20851;&#32039;&#35201;&#65292;&#26126;&#26234;&#22320;&#36873;&#25321;&#36127;&#23454;&#20363;&#21487;&#20197;&#26377;&#30410;&#20110;&#24369;&#26631;&#31614;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weak-label learning is a challenging task that requires learning from data "bags" containing positive and negative instances, but only the bag labels are known. The pool of negative instances is usually larger than positive instances, thus making selecting the most informative negative instance critical for performance. Such a selection strategy for negative instances from each bag is an open problem that has not been well studied for weak-label learning. In this paper, we study several sampling strategies that can measure the usefulness of negative instances for weak-label learning and select them accordingly. We test our method on CIFAR-10 and AudioSet datasets and show that it improves the weak-label classification performance and reduces the computational cost compared to random sampling methods. Our work reveals that negative instances are not all equally irrelevant, and selecting them wisely can benefit weak-label learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#20844;&#21496;&#30340;Robot Induction&#65288;Robin&#65289;&#33328;&#38431;&#20013;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#65292;&#36890;&#36807;&#20351;&#29992;&#25315;&#36873;&#25104;&#21151;&#39044;&#27979;&#22120;&#20197;&#21450;&#35757;&#32451;&#30340;&#25315;&#36873;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;&#30495;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#36827;&#34892;&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.13224</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#30340;&#25315;&#36873;&#35745;&#21010;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Pick Planning Strategies for Large-Scale Package Manipulation. (arXiv:2309.13224v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#20844;&#21496;&#30340;Robot Induction&#65288;Robin&#65289;&#33328;&#38431;&#20013;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#65292;&#36890;&#36807;&#20351;&#29992;&#25315;&#36873;&#25104;&#21151;&#39044;&#27979;&#22120;&#20197;&#21450;&#35757;&#32451;&#30340;&#25315;&#36873;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;&#30495;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#36827;&#34892;&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#21487;&#20197;&#38477;&#20302;&#29289;&#27969;&#25104;&#26412;&#65292;&#26368;&#32456;&#38477;&#20302;&#28040;&#36153;&#32773;&#30340;&#20215;&#26684;&#65292;&#21152;&#24555;&#20132;&#36135;&#36895;&#24230;&#65292;&#24182;&#22686;&#24378;&#23545;&#24066;&#22330;&#27874;&#21160;&#30340;&#36866;&#24212;&#21147;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#20844;&#21496;&#30340;&#26426;&#22120;&#20154;&#24341;&#23548;&#65288;Robin&#65289;&#33328;&#38431;&#20013;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#65292;&#29992;&#20110;&#27599;&#22825;&#25315;&#36873;&#21644;&#21333;&#29420;&#22788;&#29702;600&#19975;&#20010;&#21253;&#35065;&#65292;&#24182;&#19988;&#30446;&#21069;&#24050;&#32463;&#22788;&#29702;&#20102;20&#20159;&#20010;&#21253;&#35065;&#12290;&#23427;&#25551;&#36848;&#20102;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#24320;&#21457;&#30340;&#21508;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#21450;&#20854;&#21518;&#32487;&#26041;&#27861;&#65292;&#21518;&#32487;&#26041;&#27861;&#21033;&#29992;&#20102;&#22312;&#30495;&#23454;&#29983;&#20135;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25315;&#36873;&#25104;&#21151;&#39044;&#27979;&#22120;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#22312;&#30495;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#39318;&#27425;&#22823;&#35268;&#27169;&#37096;&#32626;&#23398;&#20064;&#30340;&#25315;&#36873;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to market fluctuations.  This extended abstract showcases a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which is used for picking and singulating up to 6 million packages per day and so far has manipulated over 2 billion packages. It describes the various heuristic methods developed over time and their successor, which utilizes a pick success predictor trained on real production data.  To the best of the authors' knowledge, this work is the first large-scale deployment of learned pick quality estimation methods in a real production system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;AI&#26412;&#22320;&#21270;&#30340;&#26080;&#32447;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#8220;AI for wireless&#8221;&#33539;&#24335;&#30340;&#30701;&#26495;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;AI&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#12289;&#26354;&#32447;&#25311;&#21512;&#29305;&#24615;&#12289;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#20197;&#21450;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#25928;&#29575;&#20302;&#19979;&#31561;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#39537;&#21160;&#22411;&#12289;&#35757;&#32451;&#23494;&#38598;&#22411;AI&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13223</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#65306;&#20026;&#19979;&#19968;&#20195;AI&#26412;&#22320;&#21270;&#26080;&#32447;&#32593;&#32476;&#24320;&#36767;&#38761;&#21629;&#24615;&#36947;&#36335;
&lt;/p&gt;
&lt;p&gt;
Causal Reasoning: Charting a Revolutionary Course for Next-Generation AI-Native Wireless Networks. (arXiv:2309.13223v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;AI&#26412;&#22320;&#21270;&#30340;&#26080;&#32447;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#8220;AI for wireless&#8221;&#33539;&#24335;&#30340;&#30701;&#26495;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;AI&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#12289;&#26354;&#32447;&#25311;&#21512;&#29305;&#24615;&#12289;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#20197;&#21450;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#25928;&#29575;&#20302;&#19979;&#31561;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#39537;&#21160;&#22411;&#12289;&#35757;&#32451;&#23494;&#38598;&#22411;AI&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#26412;&#21069;&#25552;&#26159;&#19979;&#19968;&#20195;&#26080;&#32447;&#32593;&#32476;&#65288;&#20363;&#22914;6G&#65289;&#23558;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26412;&#22320;&#21270;&#30340;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#20173;&#28982;&#35201;&#20040;&#26159;&#23450;&#24615;&#30340;&#65292;&#35201;&#20040;&#26159;&#23545;&#29616;&#26377;&#8220;AI&#29992;&#20110;&#26080;&#32447;&#8221;&#33539;&#24335;&#30340;&#22686;&#37327;&#25193;&#23637;&#12290;&#23454;&#38469;&#19978;&#65292;&#21019;&#24314;AI&#26412;&#22320;&#21270;&#30340;&#26080;&#32447;&#32593;&#32476;&#38754;&#20020;&#30528;&#37325;&#35201;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#22240;&#20026;&#25968;&#25454;&#39537;&#21160;&#22411;&#12289;&#35757;&#32451;&#23494;&#38598;&#22411;&#30340;AI&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#38480;&#21046;&#21253;&#25324;AI&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#12289;&#23427;&#20204;&#30340;&#26354;&#32447;&#25311;&#21512;&#29305;&#24615;&#65288;&#36825;&#21487;&#33021;&#38480;&#21046;&#23427;&#20204;&#30340;&#25512;&#29702;&#21644;&#36866;&#24212;&#33021;&#21147;&#65289;&#12289;&#23427;&#20204;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#20197;&#21450;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#25928;&#29575;&#20302;&#19979;&#31561;&#12290;&#20316;&#20026;&#23545;&#36825;&#20123;&#38480;&#21046;&#30340;&#22238;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#20855;&#26377;&#21069;&#30651;&#24615;&#30340;&#24895;&#26223;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#22240;&#26524;&#21457;&#29616;&#12289;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the basic premise that next-generation wireless networks (e.g., 6G) will be artificial intelligence (AI)-native, to date, most existing efforts remain either qualitative or incremental extensions to existing ``AI for wireless'' paradigms. Indeed, creating AI-native wireless networks faces significant technical challenges due to the limitations of data-driven, training-intensive AI. These limitations include the black-box nature of the AI models, their curve-fitting nature, which can limit their ability to reason and adapt, their reliance on large amounts of training data, and the energy inefficiency of large neural networks. In response to these limitations, this article presents a comprehensive, forward-looking vision that addresses these shortcomings by introducing a novel framework for building AI-native wireless networks; grounded in the emerging field of causal reasoning. Causal reasoning, founded on causal discovery, causal representation learning, and causal inference, c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#24615;&#23545;&#22242;&#38431;&#21512;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#28216;&#25103;&#29609;&#23478;&#30340;&#24773;&#24863;&#34920;&#36798;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#20004;&#21608;&#20869;11&#21517;&#29609;&#23478;&#30340;&#32842;&#22825;&#35760;&#24405;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20010;&#24615;&#21464;&#37327;&#19982;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#21512;&#29702;&#30456;&#20851;&#24615;&#65292;&#20363;&#22914;&#36739;&#20302;&#30340;&#33258;&#25105;&#33021;&#21147;&#19982;&#22686;&#21152;&#30340;&#22256;&#24785;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20010;&#20154;&#28902;&#24700;&#19982;&#20869;&#22312;&#21644;&#22806;&#22312;&#24418;&#35937;&#38382;&#39064;&#30340;&#22686;&#22810;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.13214</link><description>&lt;p&gt;
&#35780;&#20272;&#20010;&#24615;&#23545;&#20110;&#35270;&#39057;&#28216;&#25103;&#20132;&#27969;&#20013;&#24773;&#24863;&#29366;&#24577;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Assessing the Impact of Personality on Affective States from Video Game Communication. (arXiv:2309.13214v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#24615;&#23545;&#22242;&#38431;&#21512;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#28216;&#25103;&#29609;&#23478;&#30340;&#24773;&#24863;&#34920;&#36798;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#20004;&#21608;&#20869;11&#21517;&#29609;&#23478;&#30340;&#32842;&#22825;&#35760;&#24405;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20010;&#24615;&#21464;&#37327;&#19982;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#21512;&#29702;&#30456;&#20851;&#24615;&#65292;&#20363;&#22914;&#36739;&#20302;&#30340;&#33258;&#25105;&#33021;&#21147;&#19982;&#22686;&#21152;&#30340;&#22256;&#24785;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20010;&#20154;&#28902;&#24700;&#19982;&#20869;&#22312;&#21644;&#22806;&#22312;&#24418;&#35937;&#38382;&#39064;&#30340;&#22686;&#22810;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#30340;&#20010;&#20307;&#24046;&#24322;&#20915;&#23450;&#20102;&#25105;&#20204;&#30340;&#21916;&#22909;&#12289;&#29305;&#24449;&#21644;&#20215;&#20540;&#35266;&#65292;&#36825;&#21516;&#26679;&#36866;&#29992;&#20110;&#25105;&#20204;&#34920;&#36798;&#33258;&#24049;&#30340;&#26041;&#24335;&#12290;&#22312;&#24403;&#21069;&#25216;&#26415;&#21644;&#31038;&#20250;&#30340;&#36827;&#27493;&#21644;&#36716;&#21464;&#20013;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#27807;&#36890;&#21464;&#24471;&#26222;&#36941;&#65292;&#24182;&#19988;&#36890;&#24120;&#29978;&#33267;&#36229;&#36807;&#20102;&#33258;&#28982;&#30340;&#35821;&#38899;&#20132;&#27969;&#65292;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#22312;&#36825;&#39033;&#25506;&#32034;&#24615;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20010;&#24615;&#23545;&#22522;&#20110;&#22242;&#38431;&#21512;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#28216;&#25103;&#29609;&#23478;&#24773;&#24863;&#34920;&#36798;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20004;&#21608;&#20869;&#25910;&#38598;&#20102;&#21313;&#19968;&#20010;&#29609;&#23478;&#30340;&#32842;&#22825;&#35760;&#24405;&#65292;&#26681;&#25454;&#20182;&#20204;&#30340;&#24773;&#24863;&#29366;&#24577;&#36827;&#34892;&#26631;&#35760;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#19982;&#20116;&#20010;&#20154;&#26684;&#39046;&#22495;&#21644;&#26041;&#38754;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#22312;&#24212;&#29992;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#20043;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31995;&#21015;&#21512;&#29702;&#30340;&#30456;&#20851;&#24615;&#65292;&#21363;&#65288;&#32452;&#21512;&#65289;&#20010;&#24615;&#21464;&#37327;&#19982;&#34920;&#36798;&#30340;&#24773;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;--&#20363;&#22914;&#65292;&#36739;&#20302;&#30340;&#33258;&#25105;&#33021;&#21147;&#65288;C1&#65289;&#21487;&#20197;&#39044;&#27979;&#22686;&#21152;&#30340;&#22256;&#24785;&#65292;&#20010;&#20154;&#28902;&#24700;&#21487;&#20197;&#39044;&#27979;&#36890;&#36807;&#20869;&#22312;&#21644;&#22806;&#22312;&#24418;&#35937;&#38382;&#39064;&#30340;&#22686;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual differences in personality determine our preferences, traits and values, which should similarly hold for the way we express ourselves. With current advancements and transformations of technology and society, text-based communication has become ordinary and often even surpasses natural voice conversations -- with distinct challenges and opportunities. In this exploratory work, we investigate the impact of personality on the tendency how players of a team-based collaborative alternate reality game express themselves affectively. We collected chat logs from eleven players over two weeks, labeled them according to their affective state, and assessed the connection between them and the five-factor personality domains and facets. After applying multi-linear regression, we found a series of reasonable correlations between (combinations of) personality variables and expressed affect -- as increased confusion could be predicted by lower self-competence (C1), personal annoyance by vul
&lt;/p&gt;</description></item><item><title>LHCb&#36229;&#24555;&#36895;&#27169;&#25311;&#36873;&#39033;Lamarr&#26159;&#19968;&#20010;&#22522;&#20110;Gaudi&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;LHCb&#25506;&#27979;&#22120;&#30340;&#27169;&#25311;&#25552;&#20379;&#26368;&#24555;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#37325;&#24314;&#31639;&#27861;&#26469;&#28385;&#36275;&#26410;&#26469;&#22823;&#35268;&#27169;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.13213</link><description>&lt;p&gt;
LHCb&#36229;&#24555;&#36895;&#27169;&#25311;&#36873;&#39033;Lamarr&#65306;&#35774;&#35745;&#19982;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
The LHCb ultra-fast simulation option, Lamarr: design and validation. (arXiv:2309.13213v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13213
&lt;/p&gt;
&lt;p&gt;
LHCb&#36229;&#24555;&#36895;&#27169;&#25311;&#36873;&#39033;Lamarr&#26159;&#19968;&#20010;&#22522;&#20110;Gaudi&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;LHCb&#25506;&#27979;&#22120;&#30340;&#27169;&#25311;&#25552;&#20379;&#26368;&#24555;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#37325;&#24314;&#31639;&#27861;&#26469;&#28385;&#36275;&#26410;&#26469;&#22823;&#35268;&#27169;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;LHCb&#20013;&#65292;&#35814;&#32454;&#30340;&#25506;&#27979;&#22120;&#27169;&#25311;&#26159;CPU&#36164;&#28304;&#30340;&#20027;&#35201;&#28040;&#32791;&#32773;&#65292;&#22312;CERN&#30340;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#31532;&#20108;&#38454;&#27573;&#36816;&#34892;&#26399;&#38388;&#65292;&#24050;&#32463;&#20351;&#29992;&#20102;&#24635;&#35745;&#36229;&#36807;90&#65285;&#30340;&#35745;&#31639;&#39044;&#31639;&#12290;&#38543;&#30528;LHCb&#25506;&#27979;&#22120;&#22312;LHC&#30340;&#31532;&#19977;&#38454;&#27573;&#36816;&#34892;&#26399;&#38388;&#25910;&#38598;&#25968;&#25454;&#65292;&#38656;&#35201;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#65292;&#24182;&#19988;&#21363;&#20351;&#23384;&#22312;&#24555;&#36895;&#27169;&#25311;&#36873;&#39033;&#65292;&#36825;&#20123;&#38656;&#27714;&#20063;&#23558;&#36828;&#36828;&#36229;&#36807;&#23454;&#39564;&#30340;&#25215;&#35834;&#36164;&#28304;&#12290;&#20026;&#20102;&#28385;&#36275;&#20998;&#26512;&#38656;&#27714;&#20197;&#35299;&#37322;&#20449;&#21495;&#19982;&#32972;&#26223;&#20197;&#21450;&#27979;&#37327;&#25928;&#29575;&#30340;&#26410;&#26469;&#38656;&#27714;&#65292;&#24517;&#39035;&#21457;&#23637;&#25216;&#26415;&#21644;&#25216;&#24039;&#26469;&#20135;&#29983;&#27169;&#25311;&#26679;&#26412;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Lamarr&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Gaudi&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#20379;&#26368;&#24555;&#36895;&#30340;LHCb&#25506;&#27979;&#22120;&#27169;&#25311;&#35299;&#20915;&#26041;&#26696;&#12290;Lamarr&#30001;&#19968;&#31995;&#21015;&#27169;&#22359;&#32452;&#25104;&#65292;&#36825;&#20123;&#27169;&#22359;&#23545;LHCb&#23454;&#39564;&#30340;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#37325;&#24314;&#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#22823;&#37096;&#20998;&#21442;&#25968;&#21270;&#37117;&#37319;&#29992;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detailed detector simulation is the major consumer of CPU resources at LHCb, having used more than 90% of the total computing budget during Run 2 of the Large Hadron Collider at CERN. As data is collected by the upgraded LHCb detector during Run 3 of the LHC, larger requests for simulated data samples are necessary, and will far exceed the pledged resources of the experiment, even with existing fast simulation options. An evolution of technologies and techniques to produce simulated samples is mandatory to meet the upcoming needs of analysis to interpret signal versus background and measure efficiencies. In this context, we propose Lamarr, a Gaudi-based framework designed to offer the fastest solution for the simulation of the LHCb detector. Lamarr consists of a pipeline of modules parameterizing both the detector response and the reconstruction algorithms of the LHCb experiment. Most of the parameterizations are made of Deep Generative Models and Gradient Boosted Decision Trees traine
&lt;/p&gt;</description></item><item><title>Evidential deep learning extends parametric deep learning to higher-order distributions, enabling the estimation of both aleatoric and epistemic uncertainty with one model. This study shows that evidential deep learning models achieve predictive accuracy comparable to standard methods while robustly quantifying both sources of uncertainty.</title><link>http://arxiv.org/abs/2309.13207</link><description>&lt;p&gt;
Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation for Earth System Science Applications. (arXiv:2309.13207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation for Earth System Science Applications. (arXiv:2309.13207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13207
&lt;/p&gt;
&lt;p&gt;
Evidential deep learning extends parametric deep learning to higher-order distributions, enabling the estimation of both aleatoric and epistemic uncertainty with one model. This study shows that evidential deep learning models achieve predictive accuracy comparable to standard methods while robustly quantifying both sources of uncertainty.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#38752;&#37327;&#21270;&#23545;&#20110;&#20102;&#35299;&#22825;&#27668;&#21644;&#27668;&#20505;&#32467;&#26524;&#30340;&#39537;&#21160;&#22240;&#32032;&#33267;&#20851;&#37325;&#35201;&#12290;&#38598;&#21512;&#25552;&#20379;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#34892;&#29289;&#29702;&#20998;&#35299;&#65292;&#20294;&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#38598;&#21512;&#37117;&#38656;&#35201;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#21442;&#25968;&#21270;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#21442;&#25968;&#26469;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#19981;&#32771;&#34385;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#31181;&#23558;&#21442;&#25968;&#21270;&#28145;&#24230;&#23398;&#20064;&#25193;&#23637;&#21040;&#39640;&#38454;&#20998;&#24067;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#27169;&#22411;&#21516;&#26102;&#32771;&#34385;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#65306;&#38543;&#26426;&#35823;&#24046;&#21644;&#35748;&#35782;&#35823;&#24046;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20174;&#35777;&#25454;&#31070;&#32463;&#32593;&#32476;&#21644;&#38598;&#21512;&#20013;&#24471;&#21040;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#23545;&#20908;&#23395;&#38477;&#27700;&#31867;&#22411;&#30340;&#20998;&#31867;&#21644;&#22320;&#34920;&#23618;&#36890;&#37327;&#30340;&#22238;&#24402;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36798;&#21040;&#20102;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#21487;&#38752;&#22320;&#37327;&#21270;&#36825;&#20004;&#31181;&#26469;&#28304;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust quantification of predictive uncertainty is critical for understanding factors that drive weather and climate outcomes. Ensembles provide predictive uncertainty estimates and can be decomposed physically, but both physics and machine learning ensembles are computationally expensive. Parametric deep learning can estimate uncertainty with one model by predicting the parameters of a probability distribution but do not account for epistemic uncertainty.. Evidential deep learning, a technique that extends parametric deep learning to higher-order distributions, can account for both aleatoric and epistemic uncertainty with one model. This study compares the uncertainty derived from evidential neural networks to those obtained from ensembles. Through applications of classification of winter precipitation type and regression of surface layer fluxes, we show evidential deep learning models attaining predictive accuracy rivaling standard methods, while robustly quantifying both sources of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#35774;&#35745;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#37325;&#28857;&#35752;&#35770;&#20102;&#20154;&#24037;&#35774;&#35745;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35780;&#20215;&#26041;&#27861;&#31561;&#22810;&#31181;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#32771;&#34385;&#22810;&#31181;&#25351;&#26631;&#21644;&#32570;&#20047;&#21333;&#19968;&#26368;&#20339;&#25552;&#31034;&#31561;&#35780;&#20272;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#25552;&#31034;&#35774;&#35745;&#22312;&#20805;&#20998;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28508;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13205</link><description>&lt;p&gt;
&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#35774;&#35745;&#30340;&#23454;&#38469;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Practical Survey on Zero-shot Prompt Design for In-context Learning. (arXiv:2309.13205v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#35774;&#35745;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#37325;&#28857;&#35752;&#35770;&#20102;&#20154;&#24037;&#35774;&#35745;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35780;&#20215;&#26041;&#27861;&#31561;&#22810;&#31181;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#32771;&#34385;&#22810;&#31181;&#25351;&#26631;&#21644;&#32570;&#20047;&#21333;&#19968;&#26368;&#20339;&#25552;&#31034;&#31561;&#35780;&#20272;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#25552;&#31034;&#35774;&#35745;&#22312;&#20805;&#20998;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28508;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26174;&#33879;&#36827;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#26412;&#25991;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#21512;&#22238;&#39038;&#65292;&#37325;&#28857;&#20851;&#27880;&#19981;&#21516;&#31867;&#22411;&#30340;&#25552;&#31034;&#65292;&#21253;&#25324;&#31163;&#25955;&#12289;&#36830;&#32493;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#65292;&#24182;&#25506;&#35752;&#23427;&#20204;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#22914;&#20154;&#24037;&#35774;&#35745;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35780;&#20215;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;LLM&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#28085;&#30422;&#20102;&#25552;&#31034;&#24037;&#31243;&#39046;&#22495;&#30340;&#20851;&#38190;&#30740;&#31350;&#65292;&#35752;&#35770;&#20102;&#20854;&#26041;&#27861;&#35770;&#21644;&#23545;&#35813;&#39046;&#22495;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#35780;&#20272;&#25552;&#31034;&#24615;&#33021;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#32570;&#20047;&#21333;&#19968;&#30340;"&#26368;&#20339;"&#25552;&#31034;&#21644;&#32771;&#34385;&#22810;&#20010;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#12290;&#24635;&#20043;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#25552;&#31034;&#35774;&#35745;&#22312;&#21457;&#25381;LLM&#30340;&#20840;&#37096;&#28508;&#21147;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20154;&#24037;&#35774;&#35745;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35780;&#20215;&#26041;&#27861;&#32467;&#21512;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable advancements in large language models (LLMs) have brought about significant improvements in Natural Language Processing(NLP) tasks. This paper presents a comprehensive review of in-context learning techniques, focusing on different types of prompts, including discrete, continuous, few-shot, and zero-shot, and their impact on LLM performance. We explore various approaches to prompt design, such as manual design, optimization algorithms, and evaluation methods, to optimize LLM performance across diverse tasks. Our review covers key research studies in prompt engineering, discussing their methodologies and contributions to the field. We also delve into the challenges faced in evaluating prompt performance, given the absence of a single "best" prompt and the importance of considering multiple metrics. In conclusion, the paper highlights the critical role of prompt design in harnessing the full potential of LLMs and provides insights into the combination of manual design, opt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#23618;&#30340;&#32852;&#37030;&#30701;&#26399;&#36127;&#36733;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#35757;&#32451;&#29305;&#23450;&#23618;&#26102;&#20165;&#20351;&#29992;&#23458;&#25143;&#30340;&#25968;&#25454;&#26469;&#32531;&#35299;&#24322;&#36136;&#24615;&#25968;&#25454;&#24102;&#26469;&#30340;&#35757;&#32451;&#27169;&#22411;&#36136;&#37327;&#19979;&#38477;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#20010;&#24615;&#21270;&#23618;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13194</link><description>&lt;p&gt;
&#22522;&#20110;&#20010;&#24615;&#21270;&#23618;&#30340;&#24322;&#36136;&#23458;&#25143;&#32852;&#37030;&#30701;&#26399;&#36127;&#36733;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Federated Short-Term Load Forecasting with Personalization Layers for Heterogeneous Clients. (arXiv:2309.13194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#23618;&#30340;&#32852;&#37030;&#30701;&#26399;&#36127;&#36733;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#35757;&#32451;&#29305;&#23450;&#23618;&#26102;&#20165;&#20351;&#29992;&#23458;&#25143;&#30340;&#25968;&#25454;&#26469;&#32531;&#35299;&#24322;&#36136;&#24615;&#25968;&#25454;&#24102;&#26469;&#30340;&#35757;&#32451;&#27169;&#22411;&#36136;&#37327;&#19979;&#38477;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#20010;&#24615;&#21270;&#23618;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30005;&#34920;&#30340;&#20986;&#29616;&#20026;&#35757;&#32451;&#30701;&#26399;&#36127;&#36733;&#39044;&#27979;(STLF)&#27169;&#22411;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#25910;&#38598;&#12290;&#20026;&#20102;&#22238;&#24212;&#38544;&#31169;&#20851;&#20999;&#65292;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;(FL)&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#28982;&#32780;&#38543;&#30528;&#23458;&#25143;&#25968;&#25454;&#21464;&#24471;&#24322;&#36136;&#21270;&#65292;&#35757;&#32451;&#27169;&#22411;&#30340;&#36136;&#37327;&#20250;&#38477;&#20302;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#23618;&#26469;&#32531;&#35299;&#36825;&#20010;&#32570;&#28857;&#65292;&#22312;FL&#26694;&#26550;&#20013;&#35757;&#32451;STLF&#27169;&#22411;&#30340;&#29305;&#23450;&#23618;&#20165;&#30001;&#23458;&#25143;&#33258;&#24049;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;(PL-FL)&#65292;&#20351;FL&#33021;&#22815;&#22788;&#29702;&#20010;&#24615;&#21270;&#23618;&#12290;PL-FL&#31639;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;Argonne&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#36719;&#20214;&#21253;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#22810;&#20010;&#21830;&#19994;&#24314;&#31569;&#24322;&#36136;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#30340;NREL ComStock&#25968;&#25454;&#38598;&#27979;&#35797;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;PL-FL&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20010;&#24615;&#21270;&#23618;&#20351;&#24471;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of smart meters has enabled pervasive collection of energy consumption data for training short-term load forecasting (STLF) models. In response to privacy concerns, federated learning (FL) has been proposed as a privacy-preserving approach for training, but the quality of trained models degrades as client data becomes heterogeneous. In this paper we alleviate this drawback using personalization layers, wherein certain layers of an STLF model in an FL framework are trained exclusively on the clients' own data. To that end, we propose a personalized FL algorithm (PL-FL) enabling FL to handle personalization layers. The PL-FL algorithm is implemented by using the Argonne Privacy-Preserving Federated Learning package. We test the forecast performance of models trained on the NREL ComStock dataset, which contains heterogeneous energy consumption data of multiple commercial buildings. Superior performance of models trained with PL-FL demonstrates that personalization layers enable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GreenTrainer&#65292;&#19968;&#31181;&#26032;&#30340;LLM&#32454;&#35843;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35780;&#20272;&#19981;&#21516;&#24352;&#37327;&#30340;&#21453;&#21521;&#20256;&#25773;&#25104;&#26412;&#21644;&#23545;&#32454;&#35843;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#65292;&#20197;&#23454;&#29616;&#32511;&#33394;AI&#12290;</title><link>http://arxiv.org/abs/2309.13192</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#21453;&#21521;&#20256;&#25773;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32511;&#33394;AI&#32454;&#35843;
&lt;/p&gt;
&lt;p&gt;
Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation. (arXiv:2309.13192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GreenTrainer&#65292;&#19968;&#31181;&#26032;&#30340;LLM&#32454;&#35843;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35780;&#20272;&#19981;&#21516;&#24352;&#37327;&#30340;&#21453;&#21521;&#20256;&#25773;&#25104;&#26412;&#21644;&#23545;&#32454;&#35843;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#65292;&#20197;&#23454;&#29616;&#32511;&#33394;AI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#26159;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#21040;&#19979;&#28216;&#24212;&#29992;&#20013;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#38543;&#30528;LLM&#39537;&#21160;&#30340;AI&#24212;&#29992;&#30340;&#24555;&#36895;&#22686;&#38271;&#20197;&#21450;&#24320;&#28304;LLM&#30340;&#27665;&#20027;&#21270;&#65292;&#38750;&#19987;&#19994;&#20154;&#21592;&#20063;&#21487;&#20197;&#36827;&#34892;&#32454;&#35843;&#65292;&#20294;&#26159;&#20840;&#29699;&#33539;&#22260;&#20869;&#23545;LLM&#30340;&#22823;&#35268;&#27169;&#32454;&#35843;&#21487;&#33021;&#23548;&#33268;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#26174;&#33879;&#22686;&#21152;&#65292;&#20174;&#32780;&#23545;&#29615;&#22659;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#23454;&#29616;&#32511;&#33394;AI&#20197;&#20943;&#23569;&#32454;&#35843;&#30340;FLOPs&#30452;&#25509;&#30456;&#20851;&#65292;&#20294;&#26159;&#29616;&#26377;&#30340;&#39640;&#25928;LLM&#32454;&#35843;&#25216;&#26415;&#21482;&#33021;&#23454;&#29616;&#26377;&#38480;&#30340;FLOPs&#38477;&#20302;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#35270;&#20102;&#32454;&#35843;&#20013;&#30340;&#21453;&#21521;&#20256;&#25773;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;GreenTrainer&#65292;&#19968;&#31181;&#26032;&#30340;LLM&#32454;&#35843;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35780;&#20272;&#19981;&#21516;&#24352;&#37327;&#30340;&#21453;&#21521;&#20256;&#25773;&#25104;&#26412;&#21644;&#23545;&#32454;&#35843;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#24352;&#37327;&#26469;&#26368;&#23567;&#21270;&#32454;&#35843;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is the most effective way of adapting pre-trained large language models (LLMs) to downstream applications. With the fast growth of LLM-enabled AI applications and democratization of open-souced LLMs, fine-tuning has become possible for non-expert individuals, but intensively performed LLM fine-tuning worldwide could result in significantly high energy consumption and carbon footprint, which may bring large environmental impact. Mitigating such environmental impact towards Green AI directly correlates to reducing the FLOPs of fine-tuning, but existing techniques on efficient LLM fine-tuning can only achieve limited reduction of such FLOPs, due to their ignorance of the backpropagation cost in fine-tuning. To address this limitation, in this paper we present GreenTrainer, a new LLM fine-tuning technique that adaptively evaluates different tensors' backpropagation costs and contributions to the fine-tuned model accuracy, to minimize the fine-tuning cost by selecting the most a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20020;&#30028;&#24102;&#23631;&#34109;&#23454;&#39564;&#25581;&#31034;&#20102;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#21644;&#31070;&#32463;&#32593;&#32476;&#29289;&#20307;&#35782;&#21035;&#22312;&#31354;&#38388;&#39057;&#29575;&#36890;&#36947;&#19978;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#35782;&#21035;&#29289;&#20307;&#25152;&#20351;&#29992;&#30340;&#36890;&#36947;&#19982;&#23383;&#27597;&#21644;&#20809;&#26629;&#35782;&#21035;&#30456;&#21516;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#19981;&#21516;&#30340;&#39057;&#29575;&#36890;&#36947;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13190</link><description>&lt;p&gt;
&#31354;&#38388;&#39057;&#29575;&#36890;&#36947;&#12289;&#24418;&#29366;&#20559;&#20506;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Spatial-frequency channels, shape bias, and adversarial robustness. (arXiv:2309.13190v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20020;&#30028;&#24102;&#23631;&#34109;&#23454;&#39564;&#25581;&#31034;&#20102;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#21644;&#31070;&#32463;&#32593;&#32476;&#29289;&#20307;&#35782;&#21035;&#22312;&#31354;&#38388;&#39057;&#29575;&#36890;&#36947;&#19978;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#35782;&#21035;&#29289;&#20307;&#25152;&#20351;&#29992;&#30340;&#36890;&#36947;&#19982;&#23383;&#27597;&#21644;&#20809;&#26629;&#35782;&#21035;&#30456;&#21516;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#19981;&#21516;&#30340;&#39057;&#29575;&#36890;&#36947;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#29289;&#20307;&#26102;&#20351;&#29992;&#20102;&#21738;&#20123;&#31354;&#38388;&#39057;&#29575;&#20449;&#24687;&#65311;&#22312;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#20020;&#30028;&#24102;&#23631;&#34109;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#25581;&#31034;&#29992;&#20110;&#29289;&#20307;&#35782;&#21035;&#30340;&#39057;&#29575;&#36873;&#25321;&#24615;&#28388;&#27874;&#22120;&#12290;&#20020;&#30028;&#24102;&#23631;&#34109;&#27979;&#37327;&#20102;&#35782;&#21035;&#24615;&#33021;&#23545;&#19981;&#21516;&#31354;&#38388;&#39057;&#29575;&#28155;&#21152;&#22122;&#22768;&#30340;&#25935;&#24863;&#24230;&#12290;&#29616;&#26377;&#30340;&#20020;&#30028;&#24102;&#23631;&#34109;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#36890;&#36807;&#31354;&#38388;&#39057;&#29575;&#28388;&#27874;&#22120;&#65288;&#25110;&#8220;&#36890;&#36947;&#8221;&#65289;&#35782;&#21035;&#21608;&#26399;&#24615;&#27169;&#24335;&#65288;&#20809;&#26629;&#65289;&#21644;&#23383;&#27597;&#65292;&#35813;&#28388;&#27874;&#22120;&#30340;&#39057;&#24102;&#23485;&#24230;&#20026;&#19968;&#20010;&#20843;&#24230;&#65288;&#39057;&#29575;&#32763;&#20493;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#20020;&#30028;&#24102;&#23631;&#34109;&#24341;&#20837;&#20026;&#32593;&#32476;&#19982;&#20154;&#31867;&#23545;&#27604;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#31364;&#24102;&#22122;&#22768;&#23384;&#22312;&#19979;&#27979;&#35797;&#20102;14&#21517;&#20154;&#31867;&#21644;76&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;16&#36335;ImageNet&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#20351;&#29992;&#19982;&#23383;&#27597;&#21644;&#20809;&#26629;&#30456;&#21516;&#30340;&#19968;&#20010;&#20843;&#24230;&#23485;&#30340;&#36890;&#36947;&#26469;&#35782;&#21035;&#33258;&#28982;&#22270;&#20687;&#20013;&#30340;&#29289;&#20307;&#65292;&#20351;&#20854;&#25104;&#20026;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#30340;&#19968;&#20010;&#20856;&#22411;&#29305;&#24449;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#39057;&#29575;&#36890;&#36947;&#65292;&#22312;&#19981;&#21516;&#30340;&#31354;&#38388;&#39057;&#29575;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
What spatial frequency information do humans and neural networks use to recognize objects? In neuroscience, critical band masking is an established tool that can reveal the frequency-selective filters used for object recognition. Critical band masking measures the sensitivity of recognition performance to noise added at each spatial frequency. Existing critical band masking studies show that humans recognize periodic patterns (gratings) and letters by means of a spatial-frequency filter (or "channel'') that has a frequency bandwidth of one octave (doubling of frequency). Here, we introduce critical band masking as a task for network-human comparison and test 14 humans and 76 neural networks on 16-way ImageNet categorization in the presence of narrowband noise. We find that humans recognize objects in natural images using the same one-octave-wide channel that they use for letters and gratings, making it a canonical feature of human object recognition. On the other hand, the neural netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#30422;&#30340;&#21028;&#21035;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#26080;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20013;&#30340;&#20869;&#23481;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#22495;&#30340;&#20840;&#23616;&#21028;&#21035;&#22120;&#19978;&#20351;&#29992;&#22522;&#20110;&#20869;&#23481;&#30340;&#36974;&#32617;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#19981;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#23616;&#37096;&#21028;&#21035;&#22120;&#21644;&#30456;&#20284;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#30001;&#20110;&#36974;&#30422;&#36807;&#31243;&#24341;&#36215;&#30340;&#20266;&#24433;&#12290;</title><link>http://arxiv.org/abs/2309.13188</link><description>&lt;p&gt;
&#22522;&#20110;&#36974;&#30422;&#30340;&#21028;&#21035;&#22120;&#29992;&#20110;&#20869;&#23481;&#19968;&#33268;&#24615;&#19981;&#37197;&#23545;&#30340;&#22270;&#20687;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Masked Discriminators for Content-Consistent Unpaired Image-to-Image Translation. (arXiv:2309.13188v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#30422;&#30340;&#21028;&#21035;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#26080;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20013;&#30340;&#20869;&#23481;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#22495;&#30340;&#20840;&#23616;&#21028;&#21035;&#22120;&#19978;&#20351;&#29992;&#22522;&#20110;&#20869;&#23481;&#30340;&#36974;&#32617;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#19981;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#23616;&#37096;&#21028;&#21035;&#22120;&#21644;&#30456;&#20284;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#30001;&#20110;&#36974;&#30422;&#36807;&#31243;&#24341;&#36215;&#30340;&#20266;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#37197;&#23545;&#30340;&#22270;&#20687;&#36716;&#25442;&#30340;&#19968;&#20010;&#20849;&#21516;&#30446;&#26631;&#26159;&#22312;&#27169;&#20223;&#30446;&#26631;&#22495;&#30340;&#39118;&#26684;&#30340;&#21516;&#26102;&#20445;&#25345;&#28304;&#22270;&#20687;&#21644;&#36716;&#25442;&#21518;&#22270;&#20687;&#20043;&#38388;&#30340;&#20869;&#23481;&#19968;&#33268;&#24615;&#12290;&#30001;&#20110;&#20004;&#20010;&#22495;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#20559;&#24046;&#65292;&#35768;&#22810;&#26041;&#27861;&#22312;&#36716;&#25442;&#36807;&#31243;&#20013;&#20250;&#20135;&#29983;&#19981;&#19968;&#33268;&#24615;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#29992;&#20110;&#32531;&#35299;&#36825;&#20123;&#19981;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#27809;&#26377;&#23545;&#21028;&#21035;&#22120;&#36827;&#34892;&#38480;&#21046;&#65292;&#23548;&#33268;&#35757;&#32451;&#35774;&#32622;&#26356;&#21152;&#26080;&#27861;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#19981;&#36866;&#29992;&#20110;&#26356;&#22823;&#30340;&#35009;&#21098;&#23610;&#23544;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20869;&#23481;&#30340;&#36974;&#32617;&#23545;&#20004;&#20010;&#22495;&#30340;&#20840;&#23616;&#21028;&#21035;&#22120;&#30340;&#36755;&#20837;&#36827;&#34892;&#36974;&#30422;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20869;&#23481;&#19981;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#21487;&#20197;&#36861;&#28335;&#21040;&#36974;&#32617;&#36807;&#31243;&#30340;&#20266;&#24433;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20123;&#20266;&#24433;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#20351;&#29992;&#30456;&#20284;&#24615;&#37319;&#26679;&#31574;&#30053;&#36873;&#25321;&#30340;&#23567;&#35009;&#21098;&#23545;&#19978;&#25805;&#20316;&#30340;&#23616;&#37096;&#21028;&#21035;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#36825;&#31181;&#37319;&#26679;&#31574;&#30053;&#24212;&#29992;&#20110;&#20840;&#23616;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common goal of unpaired image-to-image translation is to preserve content consistency between source images and translated images while mimicking the style of the target domain. Due to biases between the datasets of both domains, many methods suffer from inconsistencies caused by the translation process. Most approaches introduced to mitigate these inconsistencies do not constrain the discriminator, leading to an even more ill-posed training setup. Moreover, none of these approaches is designed for larger crop sizes. In this work, we show that masking the inputs of a global discriminator for both domains with a content-based mask is sufficient to reduce content inconsistencies significantly. However, this strategy leads to artifacts that can be traced back to the masking process. To reduce these artifacts, we introduce a local discriminator that operates on pairs of small crops selected with a similarity sampling strategy. Furthermore, we apply this sampling strategy to sample global
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#25299;&#25169;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25581;&#31034;&#19982;&#31867;&#21035;&#26631;&#31614;&#30456;&#20851;&#30340;&#27599;&#20010;&#25968;&#25454;&#38598;&#20013;&#37325;&#35201;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.13185</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#25299;&#25169;&#37325;&#35201;&#24615;&#65306;&#19968;&#31181;&#38754;&#21521;&#31867;&#21035;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Visualizing Topological Importance: A Class-Driven Approach. (arXiv:2309.13185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#25299;&#25169;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25581;&#31034;&#19982;&#31867;&#21035;&#26631;&#31614;&#30456;&#20851;&#30340;&#27599;&#20010;&#25968;&#25454;&#38598;&#20013;&#37325;&#35201;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#23450;&#20041;&#25968;&#25454;&#31867;&#21035;&#30340;&#25299;&#25169;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#12290;&#25299;&#25169;&#29305;&#24449;&#33021;&#22815;&#25277;&#35937;&#22797;&#26434;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#65292;&#22312;&#21487;&#35270;&#21270;&#21644;&#20998;&#26512;&#36807;&#31243;&#20013;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#30340;&#25299;&#25169;&#29305;&#24449;&#37117;&#20855;&#26377;&#30456;&#31561;&#30340;&#37325;&#35201;&#24615;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#23545;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#40664;&#35748;&#23450;&#20041;&#24448;&#24448;&#26159;&#22266;&#23450;&#21644;&#20551;&#35774;&#30340;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#36866;&#24212;&#25299;&#25169;&#20998;&#31867;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#31181;&#26041;&#27861;&#26469;&#25581;&#31034;&#19982;&#31867;&#21035;&#26631;&#31614;&#30456;&#20851;&#30340;&#27599;&#20010;&#25968;&#25454;&#38598;&#20013;&#37325;&#35201;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#23398;&#20064;&#30340;&#24230;&#37327;&#20998;&#31867;&#22120;&#65292;&#24182;&#20197;&#25345;&#32493;&#22270;&#30340;&#28857;&#30340;&#23494;&#24230;&#20272;&#35745;&#20316;&#20026;&#36755;&#20837;&#12290;&#36825;&#20010;&#24230;&#37327;&#23398;&#20064;&#22914;&#20309;&#37325;&#26032;&#21152;&#26435;&#23494;&#24230;&#20197;&#20351;&#20998;&#31867;&#20934;&#30830;&#29575;&#39640;&#12290;&#36890;&#36807;&#25552;&#21462;&#36825;&#20010;&#26435;&#37325;&#65292;&#24471;&#21040;&#20102;&#25345;&#32493;&#22270;&#19978;&#30340;&#19968;&#20010;&#37325;&#35201;&#24615;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first approach to visualize the importance of topological features that define classes of data. Topological features, with their ability to abstract the fundamental structure of complex data, are an integral component of visualization and analysis pipelines. Although not all topological features present in data are of equal importance. To date, the default definition of feature importance is often assumed and fixed. This work shows how proven explainable deep learning approaches can be adapted for use in topological classification. In doing so, it provides the first technique that illuminates what topological structures are important in each dataset in regards to their class label. In particular, the approach uses a learned metric classifier with a density estimator of the points of a persistence diagram as input. This metric learns how to reweigh this density such that classification accuracy is high. By extracting this weight, an importance field on persistent
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#25361;&#25112;&#35786;&#26029;&#22120;(LCD)&#26469;&#20998;&#26512;&#35270;&#39057;&#28216;&#25103;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#24182;&#22312;Procgen&#22522;&#20934;&#27979;&#35797;&#20013;&#21457;&#29616;&#26032;&#30340;&#25361;&#25112;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LCD&#30340;&#39044;&#27979;&#21487;&#38752;&#19988;&#33021;&#25351;&#23548;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.13181</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#21033;&#29992;&#35270;&#39057;&#28216;&#25103;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Diagnosing and exploiting the computational demands of videos games for deep reinforcement learning. (arXiv:2309.13181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#25361;&#25112;&#35786;&#26029;&#22120;(LCD)&#26469;&#20998;&#26512;&#35270;&#39057;&#28216;&#25103;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#24182;&#22312;Procgen&#22522;&#20934;&#27979;&#35797;&#20013;&#21457;&#29616;&#26032;&#30340;&#25361;&#25112;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LCD&#30340;&#39044;&#27979;&#21487;&#38752;&#19988;&#33021;&#25351;&#23548;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#19982;&#29615;&#22659;&#20114;&#21160;&#24182;&#24863;&#30693;&#34892;&#21160;&#32467;&#26524;&#26469;&#23398;&#20064;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35270;&#39057;&#28216;&#25103;&#20013;&#33021;&#22815;&#23454;&#29616;&#19982;&#20154;&#31867;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#36825;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26159;&#19968;&#20010;&#37324;&#31243;&#30865;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#30340;&#21407;&#22240;&#26159;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#36824;&#26159;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21457;&#29616;&#26356;&#22909;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25110;&#32773;&#20004;&#32773;&#20860;&#20855;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23398;&#20064;&#25361;&#25112;&#35786;&#26029;&#22120;&#65288;LCD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#21333;&#29420;&#27979;&#37327;&#20219;&#21153;&#20013;&#24863;&#30693;&#21644;&#24378;&#21270;&#23398;&#20064;&#38656;&#27714;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#20351;&#29992;LCD&#22312;Procgen&#22522;&#20934;&#27979;&#35797;&#20013;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#25361;&#25112;&#20998;&#31867;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#39044;&#27979;&#26082;&#39640;&#24230;&#21487;&#38752;&#65292;&#21448;&#33021;&#25351;&#23548;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#26356;&#24191;&#27867;&#22320;&#35762;&#65292;LCD&#25581;&#31034;&#20102;&#22312;&#20687;P&#36825;&#26679;&#30340;&#25972;&#20010;&#35270;&#39057;&#28216;&#25103;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#22810;&#31181;&#22833;&#36133;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans learn by interacting with their environments and perceiving the outcomes of their actions. A landmark in artificial intelligence has been the development of deep reinforcement learning (dRL) algorithms capable of doing the same in video games, on par with or better than humans. However, it remains unclear whether the successes of dRL models reflect advances in visual representation learning, the effectiveness of reinforcement learning algorithms at discovering better policies, or both. To address this question, we introduce the Learning Challenge Diagnosticator (LCD), a tool that separately measures the perceptual and reinforcement learning demands of a task. We use LCD to discover a novel taxonomy of challenges in the Procgen benchmark, and demonstrate that these predictions are both highly reliable and can instruct algorithmic development. More broadly, the LCD reveals multiple failure cases that can occur when optimizing dRL algorithms over entire video game benchmarks like P
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;&#30340;&#22810;&#29289;&#29702;&#20223;&#30495;&#21152;&#36895;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#36924;&#36817;&#21644;&#21152;&#36895;&#22797;&#26434;&#30340;&#22810;&#29289;&#29702;&#20223;&#30495;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13179</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;&#30340;&#22810;&#29289;&#29702;&#20223;&#30495;&#22686;&#24378;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-Objective Optimization through Machine Learning-Supported Multiphysics Simulation. (arXiv:2309.13179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13179
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;&#30340;&#22810;&#29289;&#29702;&#20223;&#30495;&#21152;&#36895;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#36924;&#36817;&#21644;&#21152;&#36895;&#22797;&#26434;&#30340;&#22810;&#29289;&#29702;&#20223;&#30495;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#29289;&#29702;&#20223;&#30495;&#28041;&#21450;&#22810;&#20010;&#32806;&#21512;&#29289;&#29702;&#29616;&#35937;&#65292;&#24456;&#24555;&#21464;&#24471;&#35745;&#31639;&#22797;&#26434;&#12290;&#36825;&#32473;&#23547;&#25214;&#28385;&#36275;&#22810;&#20010;&#30446;&#26631;&#30340;&#26368;&#20248;&#37197;&#32622;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#20248;&#21270;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#22810;&#27425;&#26597;&#35810;&#20223;&#30495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#12289;&#33258;&#20248;&#21270;&#21644;&#33258;&#32452;&#32455;&#20195;&#29702;&#27169;&#22411;&#26469;&#36924;&#36817;&#21644;&#21152;&#36895;&#22810;&#29289;&#29702;&#20223;&#30495;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#24182;&#20844;&#24320;&#25552;&#20379;&#65292;&#23637;&#31034;&#20102;&#20195;&#29702;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#23545;&#23569;&#37327;&#30340;&#25968;&#25454;&#19978;&#20934;&#30830;&#36924;&#36817;&#24213;&#23618;&#20223;&#30495;&#12290;&#25105;&#20204;&#32467;&#21512;&#22235;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#20004;&#31181;&#20248;&#21270;&#31639;&#27861;&#21644;&#32508;&#21512;&#35780;&#20272;&#31574;&#30053;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#39564;&#35777;&#29983;&#25104;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#38598;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#32467;&#21512;&#35757;&#32451;&#21644;&#20248;&#21270;&#27969;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiphysics simulations that involve multiple coupled physical phenomena quickly become computationally expensive. This imposes challenges for practitioners aiming to find optimal configurations for these problems satisfying multiple objectives, as optimization algorithms often require querying the simulation many times. This paper presents a methodological framework for training, self-optimizing, and self-organizing surrogate models to approximate and speed up Multiphysics simulations. We generate two real-world tabular datasets, which we make publicly available, and show that surrogate models can be trained on relatively small amounts of data to approximate the underlying simulations accurately. We conduct extensive experiments combining four machine learning and deep learning algorithms with two optimization algorithms and a comprehensive evaluation strategy. Finally, we evaluate the performance of our combined training and optimization pipeline by verifying the generated Pareto-op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#27969;&#21160;&#22240;&#24335;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#27604;&#29616;&#26377;&#26694;&#26550;&#26356;&#39640;&#25928;&#26356;&#26377;&#29992;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.13167</link><description>&lt;p&gt;
&#27969;&#21160;&#22240;&#24335;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Flow Factorized Representation Learning. (arXiv:2309.13167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#27969;&#21160;&#22240;&#24335;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#27604;&#29616;&#26377;&#26694;&#26550;&#26356;&#39640;&#25928;&#26356;&#26377;&#29992;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#23398;&#20064;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#23454;&#29616;&#19982;&#30495;&#23454;&#22240;&#32032;&#21464;&#21270;&#26377;&#20851;&#30340;&#26377;&#29992;&#22240;&#24335;&#21270;&#34920;&#31034;&#12290;&#21306;&#20998;&#23884;&#20837;&#21644;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#25509;&#36817;&#20102;&#36825;&#20010;&#29702;&#24819;&#65307;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#34987;&#35777;&#26126;&#35201;&#20040;&#35268;&#23450;&#19981;&#26126;&#30830;&#65292;&#35201;&#20040;&#28789;&#27963;&#24615;&#19981;&#36275;&#65292;&#19981;&#33021;&#26377;&#25928;&#22320;&#23558;&#25152;&#26377;&#24863;&#20852;&#36259;&#30340;&#22240;&#32032;&#22312;&#23398;&#20064;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20998;&#31163;&#24320;&#26469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#27969;&#21160;&#22240;&#24335;&#34920;&#31034;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26367;&#20195;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#27604;&#29616;&#26377;&#26694;&#26550;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#29992;&#30340;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25351;&#23450;&#20102;&#19968;&#32452;&#19981;&#21516;&#36755;&#20837;&#21464;&#25442;&#30340;&#28508;&#22312;&#27010;&#29575;&#36335;&#24452;&#12290;&#27599;&#20010;&#28508;&#22312;&#27969;&#26159;&#30001;&#19968;&#20010;&#23398;&#20064;&#30340;&#21183;&#20989;&#25968;&#30340;&#26799;&#24230;&#22330;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prominent goal of representation learning research is to achieve representations which are factorized in a useful manner with respect to the ground truth factors of variation. The fields of disentangled and equivariant representation learning have approached this ideal from a range of complimentary perspectives; however, to date, most approaches have proven to either be ill-specified or insufficiently flexible to effectively separate all realistic factors of interest in a learned latent space. In this work, we propose an alternative viewpoint on such structured representation learning which we call Flow Factorized Representation Learning, and demonstrate it to learn both more efficient and more usefully structured representations than existing frameworks. Specifically, we introduce a generative model which specifies a distinct set of latent probability paths that define different input transformations. Each latent flow is generated by the gradient field of a learned potential followi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#38544;&#24418;&#25968;&#23383;&#27700;&#21360;&#25216;&#26415;&#65292;&#22312;&#20445;&#35777;&#27491;&#24120;&#38899;&#39057;&#29983;&#25104;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#20026;&#27169;&#22411;&#39564;&#35777;&#25552;&#20379;&#20445;&#25252;&#23618;&#65292;&#29992;&#20110;&#37492;&#21035;&#27169;&#22411;&#25152;&#26377;&#26435;&#21644;&#32500;&#25252;&#20854;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13166</link><description>&lt;p&gt;
&#29992;&#20110;&#38899;&#39057;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#38544;&#24418;&#25968;&#23383;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Invisible Watermarking for Audio Generation Diffusion Models. (arXiv:2309.13166v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13166
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#38544;&#24418;&#25968;&#23383;&#27700;&#21360;&#25216;&#26415;&#65292;&#22312;&#20445;&#35777;&#27491;&#24120;&#38899;&#39057;&#29983;&#25104;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#20026;&#27169;&#22411;&#39564;&#35777;&#25552;&#20379;&#20445;&#25252;&#23618;&#65292;&#29992;&#20110;&#37492;&#21035;&#27169;&#22411;&#25152;&#26377;&#26435;&#21644;&#32500;&#25252;&#20854;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#20854;&#25968;&#25454;&#29983;&#25104;&#21644;&#36716;&#25442;&#30340;&#33021;&#21147;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#22312;&#22270;&#20687;&#21644;&#38899;&#39057;&#39046;&#22495;&#37117;&#22791;&#21463;&#37325;&#35270;&#12290;&#22312;&#36805;&#36895;&#21457;&#23637;&#30340;&#38899;&#39057;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#21644;&#30830;&#31435;&#25968;&#25454;&#30340;&#29256;&#26435;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#27425;&#24212;&#29992;&#20110;&#35757;&#32451;&#22312;mel&#39057;&#35889;&#22270;&#19978;&#30340;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#36825;&#23545;&#19978;&#36848;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#27491;&#24120;&#38899;&#39057;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#27700;&#21360;&#35302;&#21457;&#26426;&#21046;&#26469;&#36827;&#34892;&#27169;&#22411;&#39564;&#35777;&#12290;&#36825;&#20010;&#27700;&#21360;&#35302;&#21457;&#22120;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#23618;&#65292;&#33021;&#22815;&#35782;&#21035;&#27169;&#22411;&#30340;&#25152;&#26377;&#32773;&#24182;&#30830;&#20445;&#20854;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#19981;&#21487;&#35265;&#30340;&#27700;&#21360;&#35302;&#21457;&#22120;&#22312;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#20462;&#25913;&#30340;&#21516;&#26102;&#36824;&#33021;&#20445;&#25345;&#39640;&#25928;&#30340;&#21512;&#27861;&#38899;&#39057;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained prominence in the image domain for their capabilities in data generation and transformation, achieving state-of-the-art performance in various tasks in both image and audio domains. In the rapidly evolving field of audio-based machine learning, safeguarding model integrity and establishing data copyright are of paramount importance. This paper presents the first watermarking technique applied to audio diffusion models trained on mel-spectrograms. This offers a novel approach to the aforementioned challenges. Our model excels not only in benign audio generation, but also incorporates an invisible watermarking trigger mechanism for model verification. This watermark trigger serves as a protective layer, enabling the identification of model ownership and ensuring its integrity. Through extensive experiments, we demonstrate that invisible watermark triggers can effectively protect against unauthorized modifications while maintaining high utility in benign audio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE&#26041;&#27861;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;ELBO&#65292;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21644;PatchGAN&#37492;&#21035;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;</title><link>http://arxiv.org/abs/2309.13160</link><description>&lt;p&gt;
GAMIX-VAE: &#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE
&lt;/p&gt;
&lt;p&gt;
GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior. (arXiv:2309.13160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE&#26041;&#27861;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;ELBO&#65292;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21644;PatchGAN&#37492;&#21035;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#29983;&#25104;&#24314;&#27169;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#22522;&#30707;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;VAEs&#30340;&#19968;&#20010;&#32454;&#24494;&#26041;&#38754;&#65292;&#37325;&#28857;&#26159;&#35299;&#37322;KL Divergence&#65292;&#36825;&#26159;Evidence Lower Bound&#65288;ELBO&#65289;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#25511;&#21046;&#20102;&#37325;&#26500;&#20934;&#30830;&#24615;&#21644;&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#34429;&#28982;KL Divergence&#35753;&#28508;&#21464;&#37327;&#20998;&#24067;&#19982;&#20808;&#39564;&#20998;&#24067;&#23545;&#40784;&#65292;&#32473;&#25972;&#20010;&#28508;&#31354;&#38388;&#21152;&#19978;&#32467;&#26500;&#32422;&#26463;&#65292;&#20294;&#21364;&#19981;&#38480;&#21046;&#21508;&#20010;&#21464;&#37327;&#20998;&#24067;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37325;&#26032;&#23450;&#20041;&#20102;&#24102;&#26377;&#39640;&#26031;&#28151;&#21512;&#30340;&#21518;&#39564;&#27010;&#29575;&#30340;ELBO&#65292;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#39033;&#20197;&#38450;&#27490;&#26041;&#24046;&#23849;&#28291;&#65292;&#24182;&#20351;&#29992;PatchGAN&#37492;&#21035;&#22120;&#26469;&#22686;&#24378;&#32441;&#29702;&#36924;&#30495;&#24230;&#12290;&#23454;&#29616;&#32454;&#33410;&#28041;&#21450;Encoder&#21644;Decoder&#30340;ResNetV2&#26550;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#30340;&#33021;&#21147;&#65292;&#20026;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Autoencoders (VAEs) have become a cornerstone in generative modeling and representation learning within machine learning. This paper explores a nuanced aspect of VAEs, focusing on interpreting the Kullback Leibler (KL) Divergence, a critical component within the Evidence Lower Bound (ELBO) that governs the trade-off between reconstruction accuracy and regularization. While the KL Divergence enforces alignment between latent variable distributions and a prior imposing a structure on the overall latent space but leaves individual variable distributions unconstrained. The proposed method redefines the ELBO with a mixture of Gaussians for the posterior probability, introduces a regularization term to prevent variance collapse, and employs a PatchGAN discriminator to enhance texture realism. Implementation details involve ResNetV2 architectures for both the Encoder and Decoder. The experiments demonstrate the ability to generate realistic faces, offering a promising solution for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#25239;&#30456;&#26426;&#36816;&#21160;&#25200;&#21160;&#30340;&#20687;&#32032;&#32423;&#24179;&#28369;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#20108;&#32500;&#20687;&#32032;&#31354;&#38388;&#20013;&#20351;&#29992;&#24179;&#28369;&#20998;&#24067;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#25928;&#29575;&#65292;&#24182;&#23436;&#20840;&#19978;&#30028;&#25237;&#24433;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.13150</link><description>&lt;p&gt;
&#20687;&#32032;&#32423;&#24179;&#28369;&#29992;&#20110;&#23545;&#25239;&#30456;&#26426;&#36816;&#21160;&#25200;&#21160;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Pixel-wise Smoothing for Certified Robustness against Camera Motion Perturbations. (arXiv:2309.13150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#25239;&#30456;&#26426;&#36816;&#21160;&#25200;&#21160;&#30340;&#20687;&#32032;&#32423;&#24179;&#28369;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#20108;&#32500;&#20687;&#32032;&#31354;&#38388;&#20013;&#20351;&#29992;&#24179;&#28369;&#20998;&#24067;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#25928;&#29575;&#65292;&#24182;&#23436;&#20840;&#19978;&#30028;&#25237;&#24433;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#35273;&#24863;&#30693;&#27169;&#22411;&#22312;&#38754;&#23545;&#30456;&#26426;&#36816;&#21160;&#25200;&#21160;&#26102;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#40065;&#26834;&#24615;&#30340;&#35748;&#35777;&#36807;&#31243;&#32791;&#26102;&#19988;&#26114;&#36149;&#65292;&#22240;&#20026;&#38656;&#35201;&#22312;&#19977;&#32500;&#30456;&#26426;&#36816;&#21160;&#31354;&#38388;&#20013;&#36827;&#34892;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#24471;&#21040;&#22823;&#37327;&#22270;&#20687;&#25237;&#24433;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35777;&#26126;3D-2D&#25237;&#24433;&#21464;&#25442;&#23545;&#25239;&#30456;&#26426;&#36816;&#21160;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20108;&#32500;&#20687;&#32032;&#31354;&#38388;&#32780;&#38750;&#19977;&#32500;&#29289;&#29702;&#31354;&#38388;&#20013;&#20351;&#29992;&#24179;&#28369;&#20998;&#24067;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#30456;&#26426;&#36816;&#21160;&#37319;&#26679;&#65292;&#24182;&#22823;&#22823;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#20687;&#32032;&#32423;&#24179;&#28369;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#19968;&#31181;&#22343;&#21248;&#20998;&#21306;&#30340;&#25216;&#26415;&#23436;&#20840;&#19978;&#30028;&#25237;&#24433;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, computer vision has made remarkable advancements in autonomous driving and robotics. However, it has been observed that deep learning-based visual perception models lack robustness when faced with camera motion perturbations. The current certification process for assessing robustness is costly and time-consuming due to the extensive number of image projections required for Monte Carlo sampling in the 3D camera motion space. To address these challenges, we present a novel, efficient, and practical framework for certifying the robustness of 3D-2D projective transformations against camera motion perturbations. Our approach leverages a smoothing distribution over the 2D pixel space instead of in the 3D physical space, eliminating the need for costly camera motion sampling and significantly enhancing the efficiency of robustness certifications. With the pixel-wise smoothed classifier, we are able to fully upper bound the projection errors using a technique of uniform partit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#32858;&#21512;ArcFace&#21644;AdaFace&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#25442;&#22120;&#27880;&#24847;&#26426;&#21046;&#26469;&#22686;&#24378;&#25972;&#20307;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25913;&#36827;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#20197;&#26377;&#25928;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.13137</link><description>&lt;p&gt;
&#29306;&#29298;&#29305;&#24449;&#32858;&#21512;&#20013;&#30340;&#20114;&#20449;&#24687;&#29992;&#20110;&#20154;&#33080;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Trading-off Mutual Information on Feature Aggregation for Face Recognition. (arXiv:2309.13137v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#32858;&#21512;ArcFace&#21644;AdaFace&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#25442;&#22120;&#27880;&#24847;&#26426;&#21046;&#26469;&#22686;&#24378;&#25972;&#20307;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25913;&#36827;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#20197;&#26377;&#25928;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#33080;&#35782;&#21035;&#25216;&#26415;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#20934;&#30830;&#24230;&#36824;&#19981;&#22815;&#12290;&#20026;&#20102;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#32858;&#21512;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;ArcFace&#21644;AdaFace&#30340;&#36755;&#20986;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21464;&#25442;&#22120;&#27880;&#24847;&#26426;&#21046;&#26469;&#21033;&#29992;&#20004;&#20010;&#29305;&#24449;&#22270;&#30340;&#19981;&#21516;&#37096;&#20998;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22686;&#24378;&#25972;&#20010;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;&#29305;&#24449;&#32858;&#21512;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#26377;&#25928;&#24314;&#27169;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#20256;&#32479;&#30340;&#21464;&#25442;&#22120;&#20197;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#24615;&#32780;&#38395;&#21517;&#65292;&#20294;&#21364;&#24448;&#24448;&#38590;&#20197;&#20934;&#30830;&#24314;&#27169;&#23616;&#37096;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#23545;&#24212;&#20301;&#32622;&#20013;&#30340;&#37325;&#21472;&#25509;&#21463;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the advances in the field of Face Recognition (FR), the precision of these methods is not yet sufficient. To improve the FR performance, this paper proposes a technique to aggregate the outputs of two state-of-the-art (SOTA) deep FR models, namely ArcFace and AdaFace. In our approach, we leverage the transformer attention mechanism to exploit the relationship between different parts of two feature maps. By doing so, we aim to enhance the overall discriminative power of the FR system. One of the challenges in feature aggregation is the effective modeling of both local and global dependencies. Conventional transformers are known for their ability to capture long-range dependencies, but they often struggle with modeling local dependencies accurately. To address this limitation, we augment the self-attention mechanism to capture both local and global dependencies effectively. This allows our model to take advantage of the overlapping receptive fields present in corresponding locati
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#65292;&#35813;&#26041;&#27861;&#27604;&#22522;&#20934;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#32422;11%&#21644;8%&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#22914;&#21457;&#20986;&#26089;&#26399;&#35686;&#21578;&#21644;&#23450;&#37327;&#29305;&#23450;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13135</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors. (arXiv:2309.13135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#65292;&#35813;&#26041;&#27861;&#27604;&#22522;&#20934;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#32422;11%&#21644;8%&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#22914;&#21457;&#20986;&#26089;&#26399;&#35686;&#21578;&#21644;&#23450;&#37327;&#29305;&#23450;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#23545;&#20110;&#26089;&#26399;&#26816;&#27979;&#19981;&#33391;&#32467;&#26524;&#21644;&#24739;&#32773;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#22024;&#26434;&#21644;&#38388;&#27463;&#24615;&#65292;&#23454;&#38469;&#20013;&#39044;&#27979;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#36825;&#20123;&#25361;&#25112;&#36890;&#24120;&#36890;&#36807;&#22806;&#37096;&#22240;&#32032;&#35825;&#23548;&#30340;&#21464;&#21270;&#28857;&#65288;&#22914;&#33647;&#29289;&#20351;&#29992;&#65289;&#32780;&#21152;&#21095;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#21521;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#25928;&#24212;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#21463;&#27835;&#30103;&#24433;&#21709;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#20351;&#29992;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#39044;&#27979;&#34880;&#31958;&#30340;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#33647;&#21160;&#23398;&#32534;&#30721;&#22120;&#20351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36229;&#36807;&#22522;&#20934;&#32422;11&#65285;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36229;&#36807;8&#65285;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#20363;&#22914;&#21457;&#20986;&#20851;&#20110;&#24847;&#22806;&#27835;&#30103;&#21453;&#24212;&#30340;&#26089;&#26399;&#35686;&#21578;&#65292;&#25110;&#24110;&#21161;&#34920;&#24449;&#29305;&#23450;&#20110;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting healthcare time series is crucial for early detection of adverse outcomes and for patient monitoring. Forecasting, however, can be difficult in practice due to noisy and intermittent data. The challenges are often exacerbated by change points induced via extrinsic factors, such as the administration of medication. We propose a novel encoder that informs deep learning models of the pharmacokinetic effects of drugs to allow for accurate forecasting of time series affected by treatment. We showcase the effectiveness of our approach in a task to forecast blood glucose using both realistically simulated and real-world data. Our pharmacokinetic encoder helps deep learning models surpass baselines by approximately 11% on simulated data and 8% on real-world data. The proposed approach can have multiple beneficial applications in clinical practice, such as issuing early warnings about unexpected treatment responses, or helping to characterize patient-specific treatment effects in te
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;AntiBARTy&#26469;&#23548;&#21521;&#25239;&#20307;&#35774;&#35745;&#65292;&#36890;&#36807;&#35757;&#32451;&#25239;&#20307;&#29305;&#24322;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#20854;&#28508;&#22312;&#31354;&#38388;&#26469;&#35757;&#32451;&#24615;&#36136;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#28342;&#35299;&#24615;&#30340;&#26032;&#22411;&#25239;&#20307;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.13129</link><description>&lt;p&gt;
&#21453;BARTy&#25193;&#25955;&#29992;&#20110;&#22522;&#20110;&#24615;&#36136;&#23548;&#21521;&#30340;&#25239;&#20307;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
AntiBARTy Diffusion for Property Guided Antibody Design. (arXiv:2309.13129v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13129
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;AntiBARTy&#26469;&#23548;&#21521;&#25239;&#20307;&#35774;&#35745;&#65292;&#36890;&#36807;&#35757;&#32451;&#25239;&#20307;&#29305;&#24322;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#20854;&#28508;&#22312;&#31354;&#38388;&#26469;&#35757;&#32451;&#24615;&#36136;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#28342;&#35299;&#24615;&#30340;&#26032;&#22411;&#25239;&#20307;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#30001;&#20110;&#20854;&#39640;&#29305;&#24322;&#24615;&#21644;&#20302;&#19981;&#33391;&#21453;&#24212;&#39118;&#38505;&#65292;&#25239;&#20307;&#22312;&#27835;&#30103;&#20013;&#30340;&#37325;&#35201;&#24615;&#36880;&#28176;&#22686;&#21152;&#65292;&#19982;&#20854;&#20182;&#33647;&#29289;&#27169;&#24335;&#30456;&#27604;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#25239;&#20307;&#21457;&#29616;&#20027;&#35201;&#26159;&#28287;&#23454;&#39564;&#39537;&#21160;&#30340;&#65292;&#20294;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29983;&#25104;&#24314;&#27169;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#22522;&#20110;&#35745;&#31639;&#27169;&#25311;&#30340;&#26041;&#27861;&#36234;&#26469;&#36234;&#21487;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#25239;&#20307;&#29305;&#24322;&#24615;&#35821;&#35328;&#27169;&#22411;AntiBARTy&#65292;&#22522;&#20110;BART (&#21452;&#21521;&#21644;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;)&#65292;&#24182;&#20351;&#29992;&#20854;&#28508;&#22312;&#31354;&#38388;&#35757;&#32451;&#20102;&#19968;&#20010;&#24615;&#36136;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25351;&#23548;IgG&#26032;&#29983;&#25239;&#20307;&#30340;&#35774;&#35745;&#12290;&#20316;&#20026;&#19968;&#20010;&#27979;&#35797;&#26696;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#30340;&#26080;&#30149;&#27602;&#28342;&#35299;&#24615;&#30340;&#26032;&#22411;&#25239;&#20307;&#65292;&#21516;&#26102;&#20445;&#25345;&#25239;&#20307;&#30340;&#26377;&#25928;&#24615;&#21644;&#25511;&#21046;&#24207;&#21015;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, antibodies have steadily grown in therapeutic importance thanks to their high specificity and low risk of adverse effects compared to other drug modalities. While traditional antibody discovery is primarily wet lab driven, the rapid improvement of ML-based generative modeling has made in-silico approaches an increasingly viable route for discovery and engineering. To this end, we train an antibody-specific language model, AntiBARTy, based on BART (Bidirectional and Auto-Regressive Transformer) and use its latent space to train a property-conditional diffusion model for guided IgG de novo design. As a test case, we show that we can effectively generate novel antibodies with improved in-silico solubility while maintaining antibody validity and controlling sequence diversity.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#30005;&#36335;&#32534;&#35793;&#26041;&#27861;AMLET&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#37327;&#23376;&#30005;&#36335;&#21152;&#36733;&#32463;&#20856;&#25968;&#25454;&#30340;&#8220;&#36755;&#20837;&#38382;&#39064;&#8221;&#12290;&#20316;&#32773;&#22312;&#37329;&#34701;&#12289;&#22270;&#20687;&#12289;&#27969;&#20307;&#21644;&#34507;&#30333;&#36136;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13108</link><description>&lt;p&gt;
&#25968;&#25454;&#21152;&#36733;&#36890;&#24120;&#20855;&#26377;&#30701;&#28145;&#24230;&#65306;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#37329;&#34701;&#12289;&#22270;&#20687;&#12289;&#27969;&#20307;&#21644;&#34507;&#30333;&#36136;&#37327;&#23376;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Data is often loadable in short depth: Quantum circuits from tensor networks for finance, images, fluids, and proteins. (arXiv:2309.13108v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13108
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#30005;&#36335;&#32534;&#35793;&#26041;&#27861;AMLET&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#37327;&#23376;&#30005;&#36335;&#21152;&#36733;&#32463;&#20856;&#25968;&#25454;&#30340;&#8220;&#36755;&#20837;&#38382;&#39064;&#8221;&#12290;&#20316;&#32773;&#22312;&#37329;&#34701;&#12289;&#22270;&#20687;&#12289;&#27969;&#20307;&#21644;&#34507;&#30333;&#36136;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#24320;&#21457;&#29992;&#20110;&#30740;&#31350;&#32463;&#20856;&#25968;&#25454;&#38598;&#30340;&#37327;&#23376;&#31639;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#31616;&#21333;&#21152;&#36733;&#32463;&#20856;&#25968;&#25454;&#30340;&#25104;&#26412;&#26159;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#38556;&#30861;&#12290;&#24403;&#20351;&#29992;&#25391;&#24133;&#32534;&#30721;&#26102;&#65292;&#21152;&#36733;&#20219;&#24847;&#32463;&#20856;&#21521;&#37327;&#38656;&#35201;&#19982;&#27604;&#29305;&#25968;&#25104;&#25351;&#25968;&#20851;&#31995;&#30340;&#30005;&#36335;&#28145;&#24230;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#36129;&#29486;&#26469;&#35299;&#20915;&#36825;&#20010;&#8220;&#36755;&#20837;&#38382;&#39064;&#8221;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#29702;&#35770;&#30340;&#30005;&#36335;&#32534;&#35793;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;AMLET&#65288;&#33258;&#21160;&#22810;&#23618;&#21152;&#36733;&#22120;&#21033;&#29992;TNs&#65289;&#8212;&#8212;&#36890;&#36807;&#31934;&#24515;&#26500;&#24314;&#29305;&#23450;&#30340;TN&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#35843;&#25972;&#30005;&#36335;&#28145;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#37329;&#34701;&#12289;&#22270;&#20687;&#12289;&#27969;&#20307;&#21147;&#23398;&#21644;&#34507;&#30333;&#36136;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#30495;&#23454;&#32463;&#20856;&#25968;&#25454;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#20851;&#20110;&#23558;&#32463;&#20856;&#25968;&#25454;&#21152;&#36733;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#20013;&#30340;&#26368;&#24191;&#27867;&#30340;&#25968;&#20540;&#20998;&#26512;&#12290;&#19982;&#36825;&#19968;&#39046;&#22495;&#26368;&#36817;&#30340;&#20854;&#20182;&#24037;&#20316;&#19968;&#33268;&#65292;&#25152;&#38656;&#30340;
&lt;/p&gt;
&lt;p&gt;
Though there has been substantial progress in developing quantum algorithms to study classical datasets, the cost of simply loading classical data is an obstacle to quantum advantage. When the amplitude encoding is used, loading an arbitrary classical vector requires up to exponential circuit depths with respect to the number of qubits. Here, we address this ``input problem'' with two contributions. First, we introduce a circuit compilation method based on tensor network (TN) theory. Our method -- AMLET (Automatic Multi-layer Loader Exploiting TNs) -- proceeds via careful construction of a specific TN topology and can be tailored to arbitrary circuit depths. Second, we perform numerical experiments on real-world classical data from four distinct areas: finance, images, fluid mechanics, and proteins. To the best of our knowledge, this is the broadest numerical analysis to date of loading classical data into a quantum computer. Consistent with other recent work in this area, the required
&lt;/p&gt;</description></item><item><title>OpportunityFinder&#26159;&#19968;&#20010;&#26080;&#38656;&#32534;&#30721;&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#38750;&#19987;&#19994;&#29992;&#25143;&#36827;&#34892;&#21508;&#31181;&#38754;&#26495;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#65292;&#33410;&#30465;&#31185;&#23398;&#23478;&#21644;&#32463;&#27982;&#23398;&#23478;&#30340;&#24102;&#23485;&#65292;&#24182;&#25552;&#20379;&#32479;&#35745;&#21644;&#20005;&#26684;&#30340;&#25935;&#24863;&#24615;&#21644;&#31283;&#20581;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.13103</link><description>&lt;p&gt;
OpportunityFinder&#65306;&#19968;&#20010;&#33258;&#21160;&#22240;&#26524;&#25512;&#26029;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OpportunityFinder: A Framework for Automated Causal Inference. (arXiv:2309.13103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13103
&lt;/p&gt;
&lt;p&gt;
OpportunityFinder&#26159;&#19968;&#20010;&#26080;&#38656;&#32534;&#30721;&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#38750;&#19987;&#19994;&#29992;&#25143;&#36827;&#34892;&#21508;&#31181;&#38754;&#26495;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#65292;&#33410;&#30465;&#31185;&#23398;&#23478;&#21644;&#32463;&#27982;&#23398;&#23478;&#30340;&#24102;&#23485;&#65292;&#24182;&#25552;&#20379;&#32479;&#35745;&#21644;&#20005;&#26684;&#30340;&#25935;&#24863;&#24615;&#21644;&#31283;&#20581;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;OpportunityFinder&#65292;&#19968;&#20010;&#26080;&#38656;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#38750;&#19987;&#19994;&#29992;&#25143;&#36827;&#34892;&#21508;&#31181;&#38754;&#26495;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#12290;&#22312;&#24403;&#21069;&#29366;&#24577;&#19979;&#65292;OpportunityFinder&#21482;&#38656;&#35201;&#29992;&#25143;&#25552;&#20379;&#21407;&#22987;&#35266;&#23519;&#25968;&#25454;&#21644;&#37197;&#32622;&#25991;&#20214;&#12290;&#28982;&#21518;&#35302;&#21457;&#19968;&#20010;&#27969;&#27700;&#32447;&#26469;&#26816;&#26597;/&#22788;&#29702;&#25968;&#25454;&#65292;&#36873;&#25321;&#36866;&#21512;&#30340;&#31639;&#27861;&#26469;&#25191;&#34892;&#22240;&#26524;&#30740;&#31350;&#12290;&#23427;&#36820;&#22238;&#22788;&#29702;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#27835;&#30103;&#23545;&#39044;&#26399;&#32467;&#26524;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#20197;&#21450;&#25935;&#24863;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#32467;&#26524;&#12290;&#22240;&#26524;&#25512;&#26029;&#34987;&#24191;&#27867;&#30740;&#31350;&#21644;&#29992;&#20110;&#20272;&#35745;&#20010;&#20154;&#19982;&#20135;&#21697;&#21644;&#29305;&#24449;&#30456;&#20114;&#20316;&#29992;&#30340;&#19979;&#28216;&#24433;&#21709;&#12290;&#36890;&#24120;&#36825;&#20123;&#22240;&#26524;&#30740;&#31350;&#26159;&#30001;&#31185;&#23398;&#23478;&#21644;/&#25110;&#32463;&#27982;&#23398;&#23478;&#23450;&#26399;&#36827;&#34892;&#30340;&#12290;&#19994;&#21153;&#21033;&#30410;&#30456;&#20851;&#32773;&#36890;&#24120;&#21463;&#38480;&#20110;&#31185;&#23398;&#23478;&#25110;&#32463;&#27982;&#23398;&#23478;&#36827;&#34892;&#22240;&#26524;&#30740;&#31350;&#30340;&#24102;&#23485;&#12290;&#25105;&#20204;&#25552;&#20379;OpportunityFinder&#20316;&#20026;&#24120;&#35265;&#30340;&#22240;&#26524;&#30740;&#31350;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#22235;&#20010;&#20851;&#38190;&#29305;&#28857;&#65306;&#65288;1&#65289;&#26131;&#20110;&#20351;&#29992;&#65292;&#36866;&#29992;&#20110;&#19994;&#21153;&#20998;&#26512;&#24072;&#21644;&#20915;&#31574;&#32773;&#12290;(2)&#30001;&#20110;&#26080;&#32534;&#31243;&#38656;&#27714;&#65292;&#26377;&#25928;&#33410;&#30465;&#20102;&#31185;&#23398;&#23478;&#21644;&#32463;&#27982;&#23398;&#23478;&#30340;&#24102;&#23485;&#12290;(3)&#23427;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#36127;&#36131;&#25968;&#25454;&#22788;&#29702;&#12289;&#25512;&#26029;&#37197;&#32622;&#12289;&#25512;&#26029;&#35780;&#20272;&#31561;&#25152;&#26377;&#20107;&#24773;&#12290;(4)&#23427;&#25552;&#20379;&#20102;&#20851;&#20110;&#25512;&#26029;&#36136;&#37327;&#30340;&#32479;&#35745;&#21644;&#20005;&#26684;&#30340;&#25935;&#24863;&#24615;&#21644;&#31283;&#20581;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce OpportunityFinder, a code-less framework for performing a variety of causal inference studies with panel data for non-expert users. In its current state, OpportunityFinder only requires users to provide raw observational data and a configuration file. A pipeline is then triggered that inspects/processes data, chooses the suitable algorithm(s) to execute the causal study. It returns the causal impact of the treatment on the configured outcome, together with sensitivity and robustness results. Causal inference is widely studied and used to estimate the downstream impact of individual's interactions with products and features. It is common that these causal studies are performed by scientists and/or economists periodically. Business stakeholders are often bottle-necked on scientist or economist bandwidth to conduct causal studies. We offer OpportunityFinder as a solution for commonly performed causal studies with four key features: (1) easy to use for both Business Analysts a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#31471;&#21040;&#31471;ASR&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20248;&#21270;&#22120;&#23545;&#24179;&#28369;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24635;&#32467;&#20102;&#36866;&#29992;&#30340;&#31639;&#27861;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2309.13102</link><description>&lt;p&gt;
FL4ASR&#20013;&#20248;&#21270;&#22120;&#24341;&#36215;&#30340;&#24179;&#28369;&#24615;&#30340;&#37325;&#35201;&#24615;&#65306;&#29702;&#35299;&#31471;&#21040;&#31471;ASR&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Importance of Smoothness Induced by Optimizers in FL4ASR: Towards Understanding Federated Learning for End-to-End ASR. (arXiv:2309.13102v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#31471;&#21040;&#31471;ASR&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20248;&#21270;&#22120;&#23545;&#24179;&#28369;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24635;&#32467;&#20102;&#36866;&#29992;&#30340;&#31639;&#27861;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#35757;&#32451;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#26368;&#23567;&#21270;FL&#27169;&#22411;&#19982;&#20013;&#24515;&#21270;&#27169;&#22411;&#20043;&#38388;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#24615;&#33021;&#24046;&#36317;&#26041;&#38754;&#65292;&#21487;&#20197;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#30340;&#22522;&#26412;&#32771;&#34385;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25913;&#21464;&#36830;&#25509;&#20027;&#20041;&#26102;&#24207;&#20998;&#31867;&#65288;CTC&#65289;&#26435;&#37325;&#26469;&#30740;&#31350;&#25439;&#22833;&#29305;&#24449;&#65292;&#36890;&#36807;&#31181;&#23376;&#36215;&#22987;&#20540;&#23545;&#27169;&#22411;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#36890;&#36807;&#20174;&#20013;&#24515;&#21270;&#35757;&#32451;&#30340;&#32463;&#39564;&#20013;&#24310;&#32493;&#24314;&#27169;&#35774;&#32622;&#21040;FL&#20013;&#65292;&#20363;&#22914;&#39044;&#23618;&#25110;&#21518;&#23618;&#24402;&#19968;&#21270;&#65292;&#20197;&#21450;&#38024;&#23545;&#24322;&#36136;&#25968;&#25454;&#20998;&#24067;&#30340;ASR&#30340;FL&#19987;&#29992;&#36229;&#21442;&#25968;&#65292;&#22914;&#26412;&#22320;epoch&#25968;&#37327;&#12289;&#23458;&#25143;&#31471;&#37319;&#26679;&#22823;&#23567;&#21644;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#19968;&#20123;&#20248;&#21270;&#22120;&#36890;&#36807;&#24341;&#23548;&#24179;&#28369;&#24615;&#32780;&#27604;&#20854;&#20182;&#20248;&#21270;&#22120;&#26356;&#22909;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#12289;&#36235;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we start by training End-to-End Automatic Speech Recognition (ASR) models using Federated Learning (FL) and examining the fundamental considerations that can be pivotal in minimizing the performance gap in terms of word error rate between models trained using FL versus their centralized counterpart. Specifically, we study the effect of (i) adaptive optimizers, (ii) loss characteristics via altering Connectionist Temporal Classification (CTC) weight, (iii) model initialization through seed start, (iv) carrying over modeling setup from experiences in centralized training to FL, e.g., pre-layer or post-layer normalization, and (v) FL-specific hyperparameters, such as number of local epochs, client sampling size, and learning rate scheduler, specifically for ASR under heterogeneous data distribution. We shed light on how some optimizers work better than others via inducing smoothness. We also summarize the applicability of algorithms, trends, and propose best practices from 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#20102;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#12289;&#34394;&#20551;&#20449;&#24687;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.13098</link><description>&lt;p&gt;
&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#12289;&#34394;&#20551;&#20449;&#24687;&#21644;&#24515;&#29702;&#20581;&#24247;&#30340;&#25299;&#25169;&#25968;&#25454;&#26144;&#23556;&#65306;&#19968;&#39033;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Topological Data Mapping of Online Hate Speech, Misinformation, and General Mental Health: A Large Language Model Based Study. (arXiv:2309.13098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13098
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#20102;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#12289;&#34394;&#20551;&#20449;&#24687;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#20256;&#25773;&#20167;&#24680;&#35328;&#35770;&#21644;&#34394;&#20551;&#20449;&#24687;&#30340;&#28508;&#21147;&#30340;&#25285;&#24551;&#65292;&#38500;&#20102;&#23548;&#33268;&#20559;&#35265;&#21644;&#27495;&#35270;&#65292;&#36824;&#34987;&#24576;&#30097;&#22312;&#22686;&#21152;&#32654;&#22269;&#31038;&#20250;&#26292;&#21147;&#21644;&#29359;&#32618;&#26041;&#38754;&#36215;&#21040;&#19968;&#23450;&#20316;&#29992;&#12290;&#23613;&#31649;&#25991;&#29486;&#24050;&#32463;&#34920;&#26126;&#22312;&#32447;&#21457;&#24067;&#20167;&#24680;&#35328;&#35770;&#21644;&#34394;&#20551;&#20449;&#24687;&#19982;&#21457;&#24086;&#32773;&#30340;&#26576;&#20123;&#20154;&#26684;&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#65292;&#20294;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;/&#34394;&#20551;&#20449;&#24687;&#19982;&#21457;&#24086;&#32773;&#25972;&#20307;&#24515;&#29702;&#20581;&#24247;&#30340;&#20851;&#31995;&#21644;&#30456;&#20851;&#24615;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#20854;&#20013;&#19968;&#20010;&#22256;&#38590;&#22312;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#33021;&#22815;&#36275;&#22815;&#20998;&#26512;&#28023;&#37327;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20197;&#25581;&#31034;&#28508;&#22312;&#30340;&#38544;&#34255;&#32852;&#31995;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#36825;&#31181;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;&#26412;&#30740;&#31350;&#25910;&#38598;&#20102;&#31934;&#24515;&#36873;&#25321;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#25968;&#21315;&#26465;&#24086;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of social media has led to an increased concern over its potential to propagate hate speech and misinformation, which, in addition to contributing to prejudice and discrimination, has been suspected of playing a role in increasing social violence and crimes in the United States. While literature has shown the existence of an association between posting hate speech and misinformation online and certain personality traits of posters, the general relationship and relevance of online hate speech/misinformation in the context of overall psychological wellbeing of posters remain elusive. One difficulty lies in the lack of adequate data analytics tools capable of adequately analyzing the massive amount of social media posts to uncover the underlying hidden links. Recent progresses in machine learning and large language models such as ChatGPT have made such an analysis possible. In this study, we collected thousands of posts from carefully selected communities on the social media si
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#31639;&#27861;&#65292;&#21457;&#29616;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#31639;&#27861;&#22312;&#20248;&#21270;&#24211;&#23384;&#31649;&#29702;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#26377;&#25928;&#38477;&#20302;&#24211;&#23384;&#25104;&#26412;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#19981;&#30830;&#23450;&#38656;&#27714;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2309.13095</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20010;&#29420;&#31435;&#30340;&#24046;&#20998;&#36827;&#21270;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#38656;&#27714;&#19981;&#30830;&#23450;&#24615;&#21644;&#21464;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multiple Independent DE Optimizations to Tackle Uncertainty and Variability in Demand in Inventory Management. (arXiv:2309.13095v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#31639;&#27861;&#65292;&#21457;&#29616;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#31639;&#27861;&#22312;&#20248;&#21270;&#24211;&#23384;&#31649;&#29702;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#26377;&#25928;&#38477;&#20302;&#24211;&#23384;&#25104;&#26412;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#19981;&#30830;&#23450;&#38656;&#27714;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#23450;&#20803;&#21551;&#21457;&#24335;&#24046;&#20998;&#36827;&#21270;&#20248;&#21270;&#31574;&#30053;&#22312;&#38543;&#26426;&#38656;&#27714;&#24773;&#20917;&#19979;&#23545;&#24211;&#23384;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#65292;&#26412;&#23454;&#35777;&#30740;&#31350;&#36827;&#34892;&#20102;&#28145;&#20837;&#35843;&#26597;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#19981;&#30830;&#23450;&#38656;&#27714;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#20943;&#23569;&#24211;&#23384;&#25104;&#26412;&#31574;&#30053;&#12290;&#24211;&#23384;&#25104;&#26412;&#26159;&#25351;&#20225;&#19994;&#25345;&#26377;&#21644;&#31649;&#29702;&#24211;&#23384;&#25152;&#20135;&#29983;&#30340;&#36153;&#29992;&#12290;&#35813;&#26041;&#27861;&#23558;&#36830;&#32493;&#23457;&#26597;&#24211;&#23384;&#31649;&#29702;&#25919;&#31574;&#19982;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65288;MCS&#65289;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#25214;&#21040;&#26368;&#20248;&#35299;&#65292;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#22810;&#20010;&#31639;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#31639;&#27861;&#22312;&#20248;&#21270;&#24211;&#23384;&#31649;&#29702;&#26041;&#38754;&#20248;&#20110;&#20854;&#31454;&#20105;&#23545;&#25163;&#12290;&#20026;&#20102;&#35843;&#25972;&#21442;&#25968;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#25289;&#19969;&#36229;&#31435;&#26041;&#37319;&#26679;&#65288;LHS&#65289;&#32479;&#35745;&#26041;&#27861;&#12290;&#20026;&#20102;&#30830;&#23450;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#29420;&#31435;&#20248;&#21270;&#32467;&#26524;&#32508;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
To determine the effectiveness of metaheuristic Differential Evolution optimization strategy for inventory management (IM) in the context of stochastic demand, this empirical study undertakes a thorough investigation. The primary objective is to discern the most effective strategy for minimizing inventory costs within the context of uncertain demand patterns. Inventory costs refer to the expenses associated with holding and managing inventory within a business. The approach combines a continuous review of IM policies with a Monte Carlo Simulation (MCS). To find the optimal solution, the study focuses on meta-heuristic approaches and compares multiple algorithms. The outcomes reveal that the Differential Evolution (DE) algorithm outperforms its counterparts in optimizing IM. To fine-tune the parameters, the study employs the Latin Hypercube Sampling (LHS) statistical method. To determine the final solution, a method is employed in this study which combines the outcomes of multiple indep
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#21407;&#22411;&#36229;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#36229;&#22270;&#25429;&#25417;&#39640;&#38454;&#20851;&#31995;&#24182;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#19981;&#20381;&#36182;&#20110;&#20803;&#36335;&#24452;&#12290;&#36890;&#36807;&#21033;&#29992;&#21407;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#36229;&#22270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21521;&#25105;&#20204;&#25581;&#31034;&#20102;&#28508;&#22312;&#32593;&#32476;&#32467;&#26500;&#30340;&#21487;&#35299;&#37322;&#24615;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.13092</link><description>&lt;p&gt;
&#22686;&#24378;&#22411;&#21407;&#22411;&#36229;&#22270;&#23398;&#20064;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Prototype-Enhanced Hypergraph Learning for Heterogeneous Information Networks. (arXiv:2309.13092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13092
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#21407;&#22411;&#36229;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#36229;&#22270;&#25429;&#25417;&#39640;&#38454;&#20851;&#31995;&#24182;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#19981;&#20381;&#36182;&#20110;&#20803;&#36335;&#24452;&#12290;&#36890;&#36807;&#21033;&#29992;&#21407;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#36229;&#22270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21521;&#25105;&#20204;&#25581;&#31034;&#20102;&#28508;&#22312;&#32593;&#32476;&#32467;&#26500;&#30340;&#21487;&#35299;&#37322;&#24615;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23186;&#20307;&#25968;&#25454;&#20013;&#30340;&#20851;&#31995;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#23548;&#33268;&#20102;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HINs&#65289;&#12290;&#20174;&#36825;&#26679;&#30340;&#32593;&#32476;&#20013;&#25429;&#25417;&#35821;&#20041;&#38656;&#35201;&#33021;&#22815;&#21033;&#29992;HINs&#30340;&#20840;&#37096;&#20016;&#23500;&#24615;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#24314;&#27169;HINs&#30340;&#26041;&#27861;&#20351;&#29992;&#30340;&#26159;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;HINs&#20998;&#35299;&#20998;&#26512;&#30340;&#25216;&#26415;&#65292;&#27604;&#22914;&#20351;&#29992;&#25163;&#21160;&#39044;&#23450;&#20041;&#30340;&#20803;&#36335;&#24452;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#22686;&#24378;&#21407;&#22411;&#36229;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;HINs&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#36229;&#22270;&#32780;&#19981;&#26159;&#22270;&#65292;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;&#24182;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#20803;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21407;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#25552;&#39640;&#36229;&#22270;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20026;&#25581;&#31034;&#28508;&#22312;&#30340;&#32593;&#32476;&#32467;&#26500;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#27934;&#23519;&#21147;&#12290;&#23545;&#19977;&#20010;&#30495;&#23454;&#30340;HINs&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The variety and complexity of relations in multimedia data lead to Heterogeneous Information Networks (HINs). Capturing the semantics from such networks requires approaches capable of utilizing the full richness of the HINs. Existing methods for modeling HINs employ techniques originally designed for graph neural networks, and HINs decomposition analysis, like using manually predefined metapaths. In this paper, we introduce a novel prototype-enhanced hypergraph learning approach for node classification in HINs. Using hypergraphs instead of graphs, our method captures higher-order relationships among nodes and extracts semantic information without relying on metapaths. Our method leverages the power of prototypes to improve the robustness of the hypergraph learning process and creates the potential to provide human-interpretable insights into the underlying network structure. Extensive experiments on three real-world HINs demonstrate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#20415;&#25658;&#24335;&#25289;&#26364;&#20809;&#35889;&#20202;&#65292;&#36890;&#36807;&#26679;&#21697;&#30340;&#20809;&#35889;&#20449;&#24687;&#36827;&#34892;&#23041;&#22763;&#24524;&#30340;&#24555;&#36895;&#37492;&#21035;&#21644;&#27987;&#24230;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#20934;&#24230;&#30340;&#21697;&#29260;&#35782;&#21035;&#21644;&#25104;&#20998;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.13087</link><description>&lt;p&gt;
&#20351;&#29992;&#20415;&#25658;&#24335;&#25289;&#26364;&#20809;&#35889;&#20202;&#36827;&#34892;&#23041;&#22763;&#24524;&#37492;&#21035;&#30340;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning algorithms for identification of whisky using portable Raman spectroscopy. (arXiv:2309.13087v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#20415;&#25658;&#24335;&#25289;&#26364;&#20809;&#35889;&#20202;&#65292;&#36890;&#36807;&#26679;&#21697;&#30340;&#20809;&#35889;&#20449;&#24687;&#36827;&#34892;&#23041;&#22763;&#24524;&#30340;&#24555;&#36895;&#37492;&#21035;&#21644;&#27987;&#24230;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#20934;&#24230;&#30340;&#21697;&#29260;&#35782;&#21035;&#21644;&#25104;&#20998;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#39640;&#20215;&#20540;&#20135;&#21697;&#37492;&#21035;&#65292;&#22914;&#23041;&#22763;&#24524;&#65292;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#21697;&#29260;&#26367;&#25442;&#65288;&#21363;&#27450;&#35784;&#20135;&#21697;&#65289;&#21644;&#36136;&#37327;&#25511;&#21046;&#23545;&#20110;&#35813;&#34892;&#19994;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#30452;&#25509;&#19982;&#20415;&#25658;&#24335;&#25289;&#26364;&#20809;&#35889;&#20202;&#36830;&#25509;&#65292;&#20197;&#35782;&#21035;&#21644;&#34920;&#24449;&#21830;&#19994;&#23041;&#22763;&#24524;&#26679;&#21697;&#20013;&#30340;&#20057;&#37255;/&#30002;&#37255;&#27987;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;&#20108;&#21313;&#20843;&#20010;&#21830;&#19994;&#26679;&#21697;&#20013;&#23454;&#29616;&#36229;&#36807;99&#65285;&#30340;&#21697;&#29260;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#26679;&#21697;&#21644;&#31639;&#27861;&#26469;&#37327;&#21270;&#20057;&#37255;&#27987;&#24230;&#65292;&#24182;&#27979;&#37327;&#25530;&#20837;&#23041;&#22763;&#24524;&#26679;&#21697;&#20013;&#30340;&#30002;&#37255;&#27700;&#24179;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19982;&#36879;&#29942;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#36827;&#34892;&#20809;&#35889;&#20998;&#26512;&#21644;&#37492;&#21035;&#65292;&#32780;&#26080;&#38656;&#23558;&#26679;&#21697;&#20174;&#21407;&#22987;&#23481;&#22120;&#20013;&#20498;&#20986;&#65292;&#23637;&#31034;&#20102;&#23454;&#38469;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable identification of high-value products such as whisky is an increasingly important area, as issues such as brand substitution (i.e. fraudulent products) and quality control are critical to the industry. We have examined a range of machine learning algorithms and interfaced them directly with a portable Raman spectroscopy device to both identify and characterize the ethanol/methanol concentrations of commercial whisky samples. We demonstrate that machine learning models can achieve over 99% accuracy in brand identification across twenty-eight commercial samples. To demonstrate the flexibility of this approach we utilised the same samples and algorithms to quantify ethanol concentrations, as well as measuring methanol levels in spiked whisky samples. Our machine learning techniques are then combined with a through-the-bottle method to perform spectral analysis and identification without requiring the sample to be decanted from the original container, showing the practical potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#32447;&#35270;&#39057;&#30340;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#29399;&#21483;&#22768;&#30340;&#35821;&#20041;&#65292;&#21457;&#29616;&#20102;&#25903;&#25345;&#20197;&#21069;&#21551;&#21457;&#24335;&#30740;&#31350;&#30340;&#35777;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#29399;&#21483;&#22768;&#30340;&#26032;&#30340;&#35266;&#28857;&#21644;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.13086</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#35270;&#39057;&#23545;&#29399;&#21483;&#22768;&#36827;&#34892;&#35789;&#27719;&#20998;&#26512;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Lexical Analysis of Dog Vocalizations via Online Videos. (arXiv:2309.13086v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#32447;&#35270;&#39057;&#30340;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#29399;&#21483;&#22768;&#30340;&#35821;&#20041;&#65292;&#21457;&#29616;&#20102;&#25903;&#25345;&#20197;&#21069;&#21551;&#21457;&#24335;&#30740;&#31350;&#30340;&#35777;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#29399;&#21483;&#22768;&#30340;&#26032;&#30340;&#35266;&#28857;&#21644;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#26512;&#21160;&#29289;&#35821;&#35328;&#30340;&#35821;&#20041;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#19981;&#21516;&#22768;&#38899;&#31867;&#22411;&#19982;&#19968;&#33268;&#30340;&#35821;&#20041;&#30456;&#20851;&#32852;&#65292;&#23545;&#29399;&#21483;&#22768;&#30340;&#35821;&#20041;&#36827;&#34892;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Shiba Inu&#22768;&#38899;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#36824;&#25910;&#38598;&#20102;&#26469;&#33258;YouTube&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22914;&#20301;&#32622;&#21644;&#27963;&#21160;&#65292;&#36890;&#36807;&#19968;&#22871;&#23436;&#21892;&#30340;&#27969;&#31243;&#12290;&#35813;&#26694;&#26550;&#20063;&#36866;&#29992;&#20110;&#20854;&#20182;&#21160;&#29289;&#29289;&#31181;&#12290;&#36890;&#36807;&#30740;&#31350;&#29399;&#21483;&#22768;&#19982;&#30456;&#24212;&#30340;&#20301;&#32622;&#21644;&#27963;&#21160;&#20043;&#38388;&#30340;&#26465;&#20214;&#27010;&#29575;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#25903;&#25345;&#20197;&#21069;&#21551;&#21457;&#24335;&#30740;&#31350;&#20851;&#20110;&#19981;&#21516;&#29399;&#21483;&#22768;&#30340;&#35821;&#20041;&#24847;&#20041;&#30340;&#35777;&#25454;&#12290;&#20363;&#22914;&#65292;&#21638;&#21742;&#21487;&#20197;&#34920;&#31034;&#20114;&#21160;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#24471;&#20986;&#20102;&#26032;&#30340;&#35266;&#28857;&#65292;&#21363;&#29616;&#26377;&#30340;&#35789;&#27719;&#31867;&#22411;&#21487;&#20197;&#32454;&#20998;&#20026;&#26356;&#31934;&#32454;&#30340;&#23376;&#31867;&#22411;&#65292;&#23545;&#20110;Shiba Inu&#26469;&#35828;&#65292;&#26368;&#23567;&#30340;&#35821;&#20041;&#21333;&#20803;&#26159;&#19982;&#35789;&#27719;&#30456;&#20851;&#30340;&#12290;&#20363;&#22914;&#65292;&#21596;&#21693;&#22768;&#21487;&#20197;&#32454;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65292;&#27714;&#20851;&#27880;&#21644;&#19981;&#36866;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deciphering the semantics of animal language has been a grand challenge. This study presents a data-driven investigation into the semantics of dog vocalizations via correlating different sound types with consistent semantics. We first present a new dataset of Shiba Inu sounds, along with contextual information such as location and activity, collected from YouTube with a well-constructed pipeline. The framework is also applicable to other animal species. Based on the analysis of conditioned probability between dog vocalizations and corresponding location and activity, we discover supporting evidence for previous heuristic research on the semantic meaning of various dog sounds. For instance, growls can signify interactions. Furthermore, our study yields new insights that existing word types can be subdivided into finer-grained subtypes and minimal semantic unit for Shiba Inu is word-related. For example, whimper can be subdivided into two types, attention-seeking and discomfort.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21021;&#27493;&#35843;&#26597;&#20102;&#23456;&#29289;&#29399;&#21483;&#22768;&#19982;&#20854;&#20027;&#20154;&#35821;&#35328;&#29615;&#22659;&#20043;&#38388;&#30340;&#22768;&#23398;&#30456;&#20851;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#20004;&#31181;&#35821;&#35328;&#29615;&#22659;&#19979;&#29399;&#21483;&#22768;&#30340;&#26174;&#33879;&#22768;&#23398;&#24046;&#24322;&#65292;&#24182;&#25214;&#21040;&#20102;&#19968;&#20123;&#21487;&#33021;&#19982;&#20027;&#20154;&#35821;&#35328;&#27169;&#24335;&#30456;&#20851;&#30340;&#29399;&#21483;&#22768;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.13085</link><description>&lt;p&gt;
&#25105;&#30340;&#29399;&#21644;&#25105;&#26377;&#30456;&#21516;&#30340;&#8220;&#35821;&#35328;&#8221;&#21527;&#65311;&#23456;&#29289;&#29399;&#21644;&#20027;&#20154;&#20043;&#38388;&#30340;&#22768;&#23398;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Does My Dog ''Speak'' Like Me? The Acoustic Correlation between Pet Dogs and Their Human Owners. (arXiv:2309.13085v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13085
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21021;&#27493;&#35843;&#26597;&#20102;&#23456;&#29289;&#29399;&#21483;&#22768;&#19982;&#20854;&#20027;&#20154;&#35821;&#35328;&#29615;&#22659;&#20043;&#38388;&#30340;&#22768;&#23398;&#30456;&#20851;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#20004;&#31181;&#35821;&#35328;&#29615;&#22659;&#19979;&#29399;&#21483;&#22768;&#30340;&#26174;&#33879;&#22768;&#23398;&#24046;&#24322;&#65292;&#24182;&#25214;&#21040;&#20102;&#19968;&#20123;&#21487;&#33021;&#19982;&#20027;&#20154;&#35821;&#35328;&#27169;&#24335;&#30456;&#20851;&#30340;&#29399;&#21483;&#22768;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23487;&#20027;&#35821;&#35328;&#23545;&#23456;&#29289;&#30340;&#21483;&#22768;&#20135;&#29983;&#24433;&#21709;&#26159;&#19968;&#20010;&#26377;&#36259;&#20294;&#24456;&#23569;&#34987;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;&#23478;&#29399;&#21483;&#22768;&#21644;&#23427;&#20204;&#20154;&#31867;&#23487;&#20027;&#30340;&#35821;&#35328;&#29615;&#22659;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20102;&#21021;&#27493;&#35843;&#26597;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;Shiba Inu&#29399;&#21483;&#22768;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;7500&#20010;&#24178;&#20928;&#30340;&#22768;&#38899;&#29255;&#27573;&#65292;&#36824;&#21253;&#25324;&#36825;&#20123;&#21483;&#22768;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#23427;&#20204;&#20027;&#20154;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#20013;&#30340;&#35821;&#38899;&#29255;&#27573;&#12290;&#19978;&#19979;&#25991;&#20449;&#24687;&#21253;&#25324;&#24405;&#38899;&#26102;&#30340;&#22330;&#26223;&#31867;&#21035;&#12289;&#29399;&#30340;&#20301;&#32622;&#21644;&#27963;&#21160;&#12290;&#36890;&#36807;&#20998;&#31867;&#20219;&#21153;&#21644;&#26174;&#33879;&#22240;&#32032;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26469;&#33258;&#20004;&#31181;&#35821;&#35328;&#29615;&#22659;&#30340;&#29399;&#21483;&#22768;&#20013;&#30340;&#26174;&#33879;&#22768;&#23398;&#24046;&#24322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#19968;&#20123;&#21487;&#33021;&#19982;&#23456;&#29289;&#20027;&#20154;&#30340;&#35821;&#35328;&#27169;&#24335;&#30456;&#20851;&#30340;&#29399;&#21483;&#22768;&#30340;&#22768;&#23398;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
How hosts language influence their pets' vocalization is an interesting yet underexplored problem. This paper presents a preliminary investigation into the possible correlation between domestic dog vocal expressions and their human host's language environment. We first present a new dataset of Shiba Inu dog vocals from YouTube, which provides 7500 clean sound clips, including their contextual information of these vocals and their owner's speech clips with a carefully-designed data processing pipeline. The contextual information includes the scene category in which the vocal was recorded, the dog's location and activity. With a classification task and prominent factor analysis, we discover significant acoustic differences in the dog vocals from the two language environments. We further identify some acoustic features from dog vocalizations that are potentially correlated to their host language patterns.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPICED&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;</title><link>http://arxiv.org/abs/2309.13080</link><description>&lt;p&gt;
SPICED: &#20855;&#26377;&#22810;&#20010;&#20027;&#39064;&#21644;&#22797;&#26434;&#31243;&#24230;&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SPICED: News Similarity Detection Dataset with Multiple Topics and Complexity Levels. (arXiv:2309.13080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13080
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPICED&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20351;&#29992;&#26234;&#33021;&#31995;&#32479;&#26469;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#26222;&#36941;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#26032;&#38395;&#23186;&#20307;&#30340;&#34028;&#21187;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#26032;&#38395;&#30340;&#24322;&#36136;&#24615;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#34394;&#20551;&#21457;&#29616;&#65306;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#27604;&#22914;&#19968;&#23545;&#26032;&#38395;&#26159;&#21542;&#37117;&#28041;&#21450;&#25919;&#27835;&#38382;&#39064;&#65292;&#21487;&#20197;&#25552;&#20379;&#24378;&#22823;&#20294;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#23558;&#26032;&#38395;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#20998;&#21106;&#25104;&#20027;&#39064;&#21487;&#20197;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#23398;&#20064;&#22914;&#20309;&#22312;&#26356;&#29421;&#31364;&#30340;&#39046;&#22495;&#20013;&#21306;&#20998;&#26174;&#33879;&#29305;&#24449;&#26469;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#23384;&#22312;&#30446;&#21069;&#32570;&#20047;&#30340;&#19987;&#39064;&#29305;&#23450;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30456;&#20284;&#26032;&#38395;&#25968;&#25454;&#38598;SPICED&#65292;&#20854;&#20013;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65306;&#29359;&#32618;&#19982;&#27861;&#24459;&#12289;&#25991;&#21270;&#19982;&#23089;&#20048;&#12289;&#28798;&#38590;&#19982;&#20107;&#25925;&#12289;&#32463;&#27982;&#19982;&#21830;&#19994;&#12289;&#25919;&#27835;&#19982;&#20914;&#31361;&#12289;&#31185;&#23398;&#19982;&#25216;&#26415;&#20197;&#21450;&#20307;&#32946;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the use of intelligent systems to detect redundant information in news articles has become especially prevalent with the proliferation of news media outlets in order to enhance user experience. However, the heterogeneous nature of news can lead to spurious findings in these systems: Simple heuristics such as whether a pair of news are both about politics can provide strong but deceptive downstream performance. Segmenting news similarity datasets into topics improves the training of these models by forcing them to learn how to distinguish salient characteristics under more narrow domains. However, this requires the existence of topic-specific datasets, which are currently lacking. In this article, we propose a new dataset of similar news, SPICED, which includes seven topics: Crime &amp; Law, Culture &amp; Entertainment, Disasters &amp; Accidents, Economy &amp; Business, Politics &amp; Conflicts, Science &amp; Technology, and Sports. Futhermore, we present four distinct approaches for generating news 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LPML&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;LLM&#25552;&#31034;&#26631;&#35760;&#35821;&#35328;&#12290;&#36890;&#36807;&#23558;Chain-of-Thought&#26041;&#27861;&#21644;Python REPL&#19982;&#35813;&#26631;&#35760;&#35821;&#35328;&#32467;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#22686;&#24378;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#21033;&#29992;Python&#35745;&#31639;&#32416;&#27491;&#38169;&#35823;&#21644;&#35299;&#20915;&#25361;&#25112;&#24615;&#25968;&#23398;&#38382;&#39064;&#65292;&#32780;&#21482;&#38656;&#35201;&#38646;&#26679;&#26412;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.13078</link><description>&lt;p&gt;
LPML: &#25968;&#23398;&#25512;&#29702;&#30340;LLM&#25552;&#31034;&#26631;&#35760;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
LPML: LLM-Prompting Markup Language for Mathematical Reasoning. (arXiv:2309.13078v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LPML&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;LLM&#25552;&#31034;&#26631;&#35760;&#35821;&#35328;&#12290;&#36890;&#36807;&#23558;Chain-of-Thought&#26041;&#27861;&#21644;Python REPL&#19982;&#35813;&#26631;&#35760;&#35821;&#35328;&#32467;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#22686;&#24378;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#21033;&#29992;Python&#35745;&#31639;&#32416;&#27491;&#38169;&#35823;&#21644;&#35299;&#20915;&#25361;&#25112;&#24615;&#25968;&#23398;&#38382;&#39064;&#65292;&#32780;&#21482;&#38656;&#35201;&#38646;&#26679;&#26412;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#26102;&#65292;&#35299;&#20915;LLMs&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#25512;&#29702;&#21644;&#35745;&#31639;&#38169;&#35823;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;Chain-of-Thought&#65288;CoT&#65289;&#26041;&#27861;&#19982;&#22806;&#37096;&#24037;&#20855;&#65288;Python REPL&#65289;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#29983;&#25104;&#31867;&#20284;XML&#26631;&#35760;&#35821;&#35328;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#25105;&#20204;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;CoT&#21644;&#22806;&#37096;&#24037;&#20855;&#65292;&#24182;&#25511;&#21046;LLMs&#30340;&#19981;&#33391;&#34892;&#20026;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;LLMs&#21487;&#20197;&#21033;&#29992;Python&#35745;&#31639;&#26469;&#32416;&#27491;CoT&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;ChatGPT&#65288;GPT-3.5&#65289;&#26469;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#26631;&#35760;&#35821;&#35328;&#23558;CoT&#21644;Python REPL&#32467;&#21512;&#36215;&#26469;&#21487;&#20197;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#32534;&#20889;&#26631;&#35760;&#35821;&#35328;&#65292;&#24182;&#36827;&#34892;&#39640;&#32423;&#25968;&#23398;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In utilizing large language models (LLMs) for mathematical reasoning, addressing the errors in the reasoning and calculation present in the generated text by LLMs is a crucial challenge. In this paper, we propose a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL). We discovered that by prompting LLMs to generate structured text in XML-like markup language, we could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs. With our approach, LLMs can utilize Python computation to rectify errors within CoT. We applied our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and demonstrated that combining CoT and Python REPL through the markup language enhances the reasoning capability of LLMs. Our approach enables LLMs to write the markup language and perform advanced mathematical reasoning using only zero-shot prompting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#31471;&#21040;&#31471;&#28151;&#21512;&#32467;&#26500;&#21387;&#32553;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#21333;&#19968;&#30340;&#20998;&#26512;&#20844;&#24335;&#20013;&#34701;&#21512;&#28388;&#27874;&#22120;&#36873;&#25321;&#12289;&#31209;&#36873;&#25321;&#21644;&#39044;&#31639;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#23454;&#29616;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#21387;&#32553;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.13077</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#31471;&#21040;&#31471;&#28151;&#21512;&#32467;&#26500;&#21387;&#32553;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Differentiable Framework for End-to-End Learning of Hybrid Structured Compression. (arXiv:2309.13077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13077
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#31471;&#21040;&#31471;&#28151;&#21512;&#32467;&#26500;&#21387;&#32553;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#21333;&#19968;&#30340;&#20998;&#26512;&#20844;&#24335;&#20013;&#34701;&#21512;&#28388;&#27874;&#22120;&#36873;&#25321;&#12289;&#31209;&#36873;&#25321;&#21644;&#39044;&#31639;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#23454;&#29616;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28388;&#27874;&#22120;&#21098;&#26525;&#21644;&#20302;&#31209;&#20998;&#35299;&#26159;&#32467;&#26500;&#21270;&#21387;&#32553;&#30340;&#20004;&#20010;&#22522;&#26412;&#25216;&#26415;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#20102;&#25972;&#21512;&#36825;&#20004;&#31181;&#25216;&#26415;&#20248;&#21183;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#20294;&#24615;&#33021;&#25552;&#21319;&#19968;&#30452;&#24456;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Differentiable Framework (DF)&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#23558;&#28388;&#27874;&#22120;&#36873;&#25321;&#12289;&#31209;&#36873;&#25321;&#21644;&#39044;&#31639;&#32422;&#26463;&#34701;&#21512;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#20998;&#26512;&#20844;&#24335;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#28388;&#27874;&#22120;&#36873;&#25321;&#30340;DML-S&#65292;&#23558;&#35843;&#24230;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#25513;&#30721;&#23398;&#20064;&#25216;&#26415;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29992;&#20110;&#31209;&#36873;&#25321;&#30340;DTL-S&#65292;&#21033;&#29992;&#22855;&#24322;&#20540;&#38408;&#20540;&#36816;&#31639;&#31526;&#12290;DF&#26694;&#26550;&#32467;&#21512;DML-S&#21644;DTL-S&#25552;&#20379;&#20102;&#19968;&#31181;&#28151;&#21512;&#32467;&#26500;&#21387;&#32553;&#26041;&#27861;&#65292;&#22312;&#26799;&#24230;&#20248;&#21270;&#30340;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;DF&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#21387;&#32553;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#24314;&#31435;&#19968;&#20010;&#24378;&#22823;&#32780;&#36890;&#29992;&#30340;&#30740;&#31350;&#26041;&#21521;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Filter pruning and low-rank decomposition are two of the foundational techniques for structured compression. Although recent efforts have explored hybrid approaches aiming to integrate the advantages of both techniques, their performance gains have been modest at best. In this study, we develop a \textit{Differentiable Framework~(DF)} that can express filter selection, rank selection, and budget constraint into a single analytical formulation. Within the framework, we introduce DML-S for filter selection, integrating scheduling into existing mask learning techniques. Additionally, we present DTL-S for rank selection, utilizing a singular value thresholding operator. The framework with DML-S and DTL-S offers a hybrid structured compression methodology that facilitates end-to-end learning through gradient-base optimization. Experimental results demonstrate the efficacy of DF, surpassing state-of-the-art structured compression methods. Our work establishes a robust and versatile avenue fo
&lt;/p&gt;</description></item><item><title>SCREWS&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#20462;&#35746;&#12290;&#23427;&#33021;&#22815;&#32479;&#19968;&#20808;&#21069;&#30340;&#26041;&#27861;&#24182;&#25552;&#20379;&#26032;&#30340;&#31574;&#30053;&#26469;&#35782;&#21035;&#25913;&#36827;&#30340;&#25512;&#29702;&#38142;&#12290;&#22312;&#22810;&#26679;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;ChatGPT&#21644;GPT-4&#65289;&#35780;&#20272;SCREWS&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#26377;&#29992;&#30340;&#26032;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.13075</link><description>&lt;p&gt;
SCREWS: &#19968;&#31181;&#29992;&#20110;&#25512;&#29702;&#20462;&#35746;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SCREWS: A Modular Framework for Reasoning with Revisions. (arXiv:2309.13075v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13075
&lt;/p&gt;
&lt;p&gt;
SCREWS&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#20462;&#35746;&#12290;&#23427;&#33021;&#22815;&#32479;&#19968;&#20808;&#21069;&#30340;&#26041;&#27861;&#24182;&#25552;&#20379;&#26032;&#30340;&#31574;&#30053;&#26469;&#35782;&#21035;&#25913;&#36827;&#30340;&#25512;&#29702;&#38142;&#12290;&#22312;&#22810;&#26679;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;ChatGPT&#21644;GPT-4&#65289;&#35780;&#20272;SCREWS&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#26377;&#29992;&#30340;&#26032;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#21487;&#20197;&#36890;&#36807;&#26681;&#25454;&#21453;&#39304;&#19981;&#26029;&#25913;&#36827;&#21644;&#20462;&#35746;&#20854;&#36755;&#20986;&#26469;&#25552;&#39640;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#20462;&#35746;&#21487;&#33021;&#20250;&#24341;&#20837;&#38169;&#35823;&#65292;&#22914;&#26524;&#26159;&#36825;&#26679;&#30340;&#35805;&#65292;&#26368;&#22909;&#22238;&#28378;&#21040;&#20808;&#21069;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20462;&#35746;&#36890;&#24120;&#26159;&#21516;&#36136;&#30340;&#65306;&#23427;&#20204;&#20351;&#29992;&#19982;&#20135;&#29983;&#21021;&#22987;&#31572;&#26696;&#30340;&#30456;&#21516;&#25512;&#29702;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#32416;&#27491;&#38169;&#35823;&#12290;&#20026;&#20102;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SCREWS&#65292;&#19968;&#31181;&#29992;&#20110;&#25512;&#29702;&#20462;&#35746;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#12290;&#23427;&#30001;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;: &#37319;&#26679;&#12289;&#26465;&#20214;&#37325;&#26032;&#37319;&#26679;&#21644;&#36873;&#25321;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#21253;&#21547;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#25163;&#21160;&#36873;&#25321;&#30340;&#23376;&#27169;&#22359;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; SCREWS &#19981;&#20165;&#23558;&#20960;&#20010;&#20808;&#21069;&#30340;&#26041;&#27861;&#32479;&#19968;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#26694;&#26550;&#20013;&#65292;&#36824;&#25581;&#31034;&#20102;&#20960;&#31181;&#29992;&#20110;&#35782;&#21035;&#25913;&#36827;&#30340;&#25512;&#29702;&#38142;&#30340;&#26032;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs &#65288;ChatGPT &#21644; GPT-4&#65289;&#22312;&#22810;&#26679;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#25581;&#31034;&#20102;&#26377;&#29992;&#30340;&#26032;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can improve their accuracy on various tasks through iteratively refining and revising their output based on feedback. We observe that these revisions can introduce errors, in which case it is better to roll back to a previous result. Further, revisions are typically homogeneous: they use the same reasoning method that produced the initial answer, which may not correct errors. To enable exploration in this space, we present SCREWS, a modular framework for reasoning with revisions. It is comprised of three main modules: Sampling, Conditional Resampling, and Selection, each consisting of sub-modules that can be hand-selected per task. We show that SCREWS not only unifies several previous approaches under a common framework, but also reveals several novel strategies for identifying improved reasoning chains. We evaluate our framework with state-of-the-art LLMs (ChatGPT and GPT-4) on a diverse set of reasoning tasks and uncover useful new reasoning strategies fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#24369;&#30417;&#30563;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#31526;&#21495;&#20027;&#20041;&#21644;&#36830;&#25509;&#20027;&#20041;&#32467;&#21512;&#36215;&#26469;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31526;&#21495;&#28508;&#22312;&#32467;&#26500;&#30340;&#31070;&#32463;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#26494;&#24347;&#26041;&#27861;&#26469;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.13072</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#24369;&#30417;&#30563;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Reasoning by Neuro-Symbolic Approaches. (arXiv:2309.13072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#24369;&#30417;&#30563;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#31526;&#21495;&#20027;&#20041;&#21644;&#36830;&#25509;&#20027;&#20041;&#32467;&#21512;&#36215;&#26469;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31526;&#21495;&#28508;&#22312;&#32467;&#26500;&#30340;&#31070;&#32463;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#26494;&#24347;&#26041;&#27861;&#26469;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#40657;&#30418;&#26426;&#22120;&#65292;&#32570;&#20047;&#26126;&#30830;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#23558;&#20171;&#32461;&#25105;&#20204;&#22312;NLP&#26041;&#38754;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#20154;&#24037;&#26234;&#33021;&#23398;&#27966;&#65292;&#21363;&#31526;&#21495;&#20027;&#20041;&#21644;&#36830;&#25509;&#20027;&#20041;&#12290;&#19968;&#33324;&#32780;&#35328;&#65292;&#25105;&#20204;&#20250;&#35774;&#35745;&#19968;&#20010;&#24102;&#26377;&#31526;&#21495;&#28508;&#22312;&#32467;&#26500;&#30340;&#31070;&#32463;&#31995;&#32479;&#65292;&#29992;&#20110;NLP&#20219;&#21153;&#65292;&#24182;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#20854;&#26494;&#24347;&#26041;&#27861;&#26469;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24369;&#30417;&#30563;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#34920;&#26684;&#26597;&#35810;&#25512;&#29702;&#12289;&#21477;&#27861;&#32467;&#26500;&#25512;&#29702;&#12289;&#20449;&#24687;&#25277;&#21462;&#25512;&#29702;&#21644;&#35268;&#21017;&#25512;&#29702;&#12290;&#23545;&#20110;&#27599;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#20171;&#32461;&#32972;&#26223;&#12289;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has largely improved the performance of various natural language processing (NLP) tasks. However, most deep learning models are black-box machinery, and lack explicit interpretation. In this chapter, we will introduce our recent progress on neuro-symbolic approaches to NLP, which combines different schools of AI, namely, symbolism and connectionism. Generally, we will design a neural system with symbolic latent structures for an NLP task, and apply reinforcement learning or its relaxation to perform weakly supervised reasoning in the downstream task. Our framework has been successfully applied to various tasks, including table query reasoning, syntactic structure reasoning, information extraction reasoning, and rule reasoning. For each application, we will introduce the background, our approach, and experimental results.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26641;&#30340;&#37325;&#24314;&#20998;&#21306;&#65288;TRP&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;PCGML&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#28216;&#25103;&#24320;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#24341;&#20837;&#65292;&#26080;&#38656;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#25110;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.13071</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#37325;&#24314;&#20998;&#21306;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#20302;&#25968;&#25454;&#32423;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tree-Based Reconstructive Partitioning: A Novel Low-Data Level Generation Approach. (arXiv:2309.13071v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13071
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#37325;&#24314;&#20998;&#21306;&#65288;TRP&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;PCGML&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#28216;&#25103;&#24320;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#24341;&#20837;&#65292;&#26080;&#38656;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#25110;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#65288;PCG&#65289;&#26159;&#19968;&#31181;&#31639;&#27861;&#29983;&#25104;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#24212;&#29992;&#20110;&#28216;&#25103;&#12290;&#24050;&#32463;&#26377;&#19968;&#20123;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;PCG&#26041;&#27861;&#20986;&#29616;&#22312;&#24050;&#21457;&#34920;&#30340;&#28216;&#25103;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#28216;&#25103;&#24320;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#12290;PCG&#38656;&#35201;&#22312;&#35268;&#21017;&#25110;&#20989;&#25968;&#20013;&#34920;&#31034;&#35774;&#35745;&#24072;&#23545;&#36136;&#37327;&#30340;&#27010;&#24565;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#32780;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;PCG&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#22312;&#24320;&#21457;&#21021;&#26399;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#26641;&#30340;&#37325;&#24314;&#20998;&#21306;&#65288;TRP&#65289;&#30340;&#26032;&#39062;PCGML&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TRP&#29983;&#25104;&#30340;&#20851;&#21345;&#26356;&#20855;&#21487;&#29609;&#24615;&#21644;&#36830;&#36143;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#26041;&#27861;&#22312;&#20351;&#29992;&#36739;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26356;&#20855;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;TRP&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;PCGML&#22312;&#28216;&#25103;&#24320;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#24341;&#20837;&#65292;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#25110;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural Content Generation (PCG) is the algorithmic generation of content, often applied to games. PCG and PCG via Machine Learning (PCGML) have appeared in published games. However, it can prove difficult to apply these approaches in the early stages of an in-development game. PCG requires expertise in representing designer notions of quality in rules or functions, and PCGML typically requires significant training data, which may not be available early in development. In this paper, we introduce Tree-based Reconstructive Partitioning (TRP), a novel PCGML approach aimed to address this problem. Our results, across two domains, demonstrate that TRP produces levels that are more playable and coherent, and that the approach is more generalizable with less training data. We consider TRP to be a promising new approach that can afford the introduction of PCGML into the early stages of game development without requiring human expertise or significant training data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#20551;&#26032;&#38395;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#29575;&#20026;56%&#65292;&#26159;&#26368;&#20339;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.13069</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Technique Based Fake News Detection. (arXiv:2309.13069v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#20551;&#26032;&#38395;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#29575;&#20026;56%&#65292;&#26159;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#24341;&#36215;&#20102;&#20844;&#20247;&#21644;&#23398;&#26415;&#30028;&#30340;&#20851;&#27880;&#12290;&#36825;&#31181;&#34394;&#20551;&#20449;&#24687;&#26377;&#33021;&#21147;&#24433;&#21709;&#20844;&#20247;&#30340;&#30475;&#27861;&#65292;&#32473;&#24694;&#24847;&#22242;&#20307;&#24433;&#21709;&#20844;&#20849;&#20107;&#20214;&#65288;&#22914;&#36873;&#20030;&#65289;&#30340;&#26426;&#20250;&#12290;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#20998;&#20139;&#20851;&#20110;&#20219;&#20309;&#20154;&#25110;&#20219;&#20309;&#20107;&#24773;&#30340;&#34394;&#20551;&#26032;&#38395;&#25110;&#20107;&#23454;&#65292;&#20197;&#35851;&#21462;&#20010;&#20154;&#21033;&#30410;&#25110;&#32473;&#26576;&#20154;&#24102;&#26469;&#40635;&#28902;&#12290;&#27492;&#22806;&#65292;&#20449;&#24687;&#22240;&#20998;&#20139;&#30340;&#22320;&#21306;&#32780;&#24322;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#25910;&#38598;&#30340;1876&#26465;&#26032;&#38395;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#20174;&#32780;&#33719;&#24471;&#24178;&#20928;&#21644;&#36807;&#28388;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;3&#20010;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#36923;&#36753;&#22238;&#24402;&#65289;&#21644;2&#20010;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65288;&#38271;&#30701;&#26399;&#35760;&#24518;&#12289;&#26435;&#37325;&#20002;&#24323;LSTM&#25110;AWD-LSTM&#65289;&#12290;&#32463;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20934;&#30830;&#29575;&#20026;56%&#65292;F1-macro&#20998;&#25968;&#20026;&#30340;&#26368;&#20339;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
False news has received attention from both the general public and the scholarly world. Such false information has the ability to affect public perception, giving nefarious groups the chance to influence the results of public events like elections. Anyone can share fake news or facts about anyone or anything for their personal gain or to cause someone trouble. Also, information varies depending on the part of the world it is shared on. Thus, in this paper, we have trained a model to classify fake and true news by utilizing the 1876 news data from our collected dataset. We have preprocessed the data to get clean and filtered texts by following the Natural Language Processing approaches. Our research conducts 3 popular Machine Learning (Stochastic gradient descent, Na\"ive Bayes, Logistic Regression,) and 2 Deep Learning (Long-Short Term Memory, ASGD Weight-Dropped LSTM, or AWD-LSTM) algorithms. After we have found our best Naive Bayes classifier with 56% accuracy and an F1-macro score o
&lt;/p&gt;</description></item><item><title>UNICON&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#28040;&#36153;&#32773;&#32454;&#20998;&#26694;&#26550;&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#28040;&#36153;&#32773;&#34892;&#20026;&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#25193;&#22823;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#31181;&#23376;&#32454;&#20998;&#26469;&#33719;&#21462;&#31867;&#20284;&#30446;&#26631;&#30340;&#20010;&#24615;&#21270;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#25581;&#31034;&#20855;&#26377;&#30456;&#20284;&#24615;&#20542;&#21521;&#30340;&#38750;&#26126;&#26174;&#28040;&#36153;&#32773;&#32454;&#20998;&#26469;&#33719;&#21462;&#25968;&#25454;&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13068</link><description>&lt;p&gt;
UNICON:&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#22522;&#20110;&#34892;&#20026;&#30340;&#28040;&#36153;&#32773;&#32454;&#20998;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UNICON: A unified framework for behavior-based consumer segmentation in e-commerce. (arXiv:2309.13068v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13068
&lt;/p&gt;
&lt;p&gt;
UNICON&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#28040;&#36153;&#32773;&#32454;&#20998;&#26694;&#26550;&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#28040;&#36153;&#32773;&#34892;&#20026;&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#25193;&#22823;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#31181;&#23376;&#32454;&#20998;&#26469;&#33719;&#21462;&#31867;&#20284;&#30446;&#26631;&#30340;&#20010;&#24615;&#21270;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#25581;&#31034;&#20855;&#26377;&#30456;&#20284;&#24615;&#20542;&#21521;&#30340;&#38750;&#26126;&#26174;&#28040;&#36153;&#32773;&#32454;&#20998;&#26469;&#33719;&#21462;&#25968;&#25454;&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#26159;&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#30340;&#20851;&#38190;&#23454;&#36341;&#65292;&#25552;&#39640;&#20102;&#20225;&#19994;&#20026;&#28040;&#36153;&#32773;&#25552;&#20379;&#26356;&#30456;&#20851;&#20869;&#23481;&#30340;&#26041;&#24335;&#12290;&#32780;&#36229;&#32423;&#20010;&#24615;&#21270;&#20026;&#27599;&#20010;&#28040;&#36153;&#32773;&#25552;&#20379;&#39640;&#24230;&#20010;&#23450;&#21046;&#30340;&#20307;&#39564;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#20010;&#20154;&#25968;&#25454;&#26469;&#21019;&#24314;&#20010;&#24615;&#21270;&#30340;&#29992;&#25143;&#26053;&#31243;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#65292;&#22522;&#20110;&#32676;&#20307;&#30340;&#20010;&#24615;&#21270;&#25552;&#20379;&#20102;&#22522;&#20110;&#26356;&#24191;&#27867;&#30340;&#20849;&#21516;&#20559;&#22909;&#26500;&#24314;&#30340;&#20013;&#24230;&#20010;&#24615;&#21270;&#65292;&#24182;&#19988;&#20173;&#33021;&#23545;&#32467;&#26524;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;UNICON&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#28040;&#36153;&#32773;&#32454;&#20998;&#26694;&#26550;&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#28040;&#36153;&#32773;&#34892;&#20026;&#25968;&#25454;&#26469;&#23398;&#20064;&#38271;&#26399;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#25552;&#21462;&#20004;&#31181;&#20851;&#38190;&#31867;&#22411;&#30340;&#32454;&#20998;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#20010;&#24615;&#21270;&#20351;&#29992;&#26696;&#20363;&#65306;&#31867;&#20284;&#30446;&#26631;&#65292;&#36890;&#36807;&#19982;&#34892;&#20026;&#30456;&#20284;&#30340;&#28040;&#36153;&#32773;&#25193;&#22823;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#31181;&#23376;&#32454;&#20998;&#65292;&#24182;&#19988;&#25968;&#25454;&#39537;&#21160;&#65292;&#25581;&#31034;&#20855;&#26377;&#30456;&#20284;&#24615;&#20542;&#21521;&#30340;&#38750;&#26126;&#26174;&#28040;&#36153;&#32773;&#32454;&#20998;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Data-driven personalization is a key practice in fashion e-commerce, improving the way businesses serve their consumers needs with more relevant content. While hyper-personalization offers highly targeted experiences to each consumer, it requires a significant amount of private data to create an individualized journey. To alleviate this, group-based personalization provides a moderate level of personalization built on broader common preferences of a consumer segment, while still being able to personalize the results. We introduce UNICON, a unified deep learning consumer segmentation framework that leverages rich consumer behavior data to learn long-term latent representations and utilizes them to extract two pivotal types of segmentation catering various personalization use-cases: lookalike, expanding a predefined target seed segment with consumers of similar behavior, and data-driven, revealing non-obvious consumer segments with similar affinities. We demonstrate through extensive exp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#22240;&#26524;&#21457;&#29616;&#25216;&#26415;&#26469;&#35782;&#21035;&#23398;&#29983;&#34920;&#29616;&#30340;&#22240;&#26524;&#39044;&#27979;&#22240;&#32032;&#65292;&#24182;&#24212;&#29992;&#21453;&#20107;&#23454;&#20998;&#26512;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.13066</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#23398;&#29983;&#23398;&#20064;&#20013;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery and Counterfactual Explanations for Personalized Student Learning. (arXiv:2309.13066v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13066
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#22240;&#26524;&#21457;&#29616;&#25216;&#26415;&#26469;&#35782;&#21035;&#23398;&#29983;&#34920;&#29616;&#30340;&#22240;&#26524;&#39044;&#27979;&#22240;&#32032;&#65292;&#24182;&#24212;&#29992;&#21453;&#20107;&#23454;&#20998;&#26512;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20851;&#27880;&#20110;&#35782;&#21035;&#23398;&#29983;&#34920;&#29616;&#30340;&#21407;&#22240;&#65292;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#25913;&#36827;&#36890;&#36807;&#29575;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#36234;&#39044;&#27979;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#25913;&#20026;&#35782;&#21035;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22240;&#26524;&#21457;&#29616;&#25216;&#26415;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#20351;&#29992;&#22240;&#26524;&#21457;&#29616;&#26469;&#35782;&#21035;&#23398;&#29983;&#34920;&#29616;&#30340;&#22240;&#26524;&#39044;&#27979;&#22240;&#32032;&#65292;&#24182;&#24212;&#29992;&#21453;&#20107;&#23454;&#20998;&#26512;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#24314;&#35758;&#12290;&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#23558;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;&#29305;&#21035;&#26159;PC&#31639;&#27861;&#65289;&#24212;&#29992;&#20110;&#30495;&#23454;&#23398;&#29983;&#34920;&#29616;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#23427;&#35299;&#20915;&#20102;&#26679;&#26412;&#22823;&#23567;&#38480;&#21046;&#31561;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#39046;&#22495;&#30693;&#35782;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#25581;&#31034;&#20102;&#35782;&#21035;&#20986;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20363;&#22914;&#26089;&#26399;&#32771;&#35797;&#25104;&#32489;&#21644;&#25968;&#23398;&#33021;&#21147;&#23545;&#26368;&#32456;&#23398;&#29983;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#35813;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21253;&#25324;&#23545;&#20934;&#30830;&#22240;&#26524;&#21457;&#29616;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper focuses on identifying the causes of student performance to provide personalized recommendations for improving pass rates. We introduce the need to move beyond predictive models and instead identify causal relationships. We propose using causal discovery techniques to achieve this. The study's main contributions include using causal discovery to identify causal predictors of student performance and applying counterfactual analysis to provide personalized recommendations. The paper describes the application of causal discovery methods, specifically the PC algorithm, to real-life student performance data. It addresses challenges such as sample size limitations and emphasizes the role of domain knowledge in causal discovery. The results reveal the identified causal relationships, such as the influence of earlier test grades and mathematical ability on final student performance. Limitations of this study include the reliance on domain expertise for accurate causal discovery, and 
&lt;/p&gt;</description></item><item><title>InvestLM&#26159;&#19968;&#20010;&#36890;&#36807;&#23545;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#19978;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#37329;&#34701;&#19987;&#23478;&#35780;&#20215;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#21487;&#23218;&#32654;&#65292;&#24182;&#22312;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.13064</link><description>&lt;p&gt;
InvestLM&#65306;&#20351;&#29992;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning. (arXiv:2309.13064v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13064
&lt;/p&gt;
&lt;p&gt;
InvestLM&#26159;&#19968;&#20010;&#36890;&#36807;&#23545;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#19978;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#37329;&#34701;&#19987;&#23478;&#35780;&#20215;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#21487;&#23218;&#32654;&#65292;&#24182;&#22312;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#37329;&#34701;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InvestLM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#19982;&#37329;&#34701;&#25237;&#36164;&#30456;&#20851;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#23545;LLaMA-65B&#36827;&#34892;&#35843;&#20248;&#12290;&#21463;&#21040;&#8220;&#23569;&#21363;&#26159;&#22810;&#8221;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25163;&#21160;&#31574;&#21010;&#20102;&#19968;&#20010;&#26082;&#23567;&#21448;&#22810;&#26679;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20174;&#29305;&#35768;&#37329;&#34701;&#20998;&#26512;&#24072;&#65288;CFA&#65289;&#32771;&#35797;&#38382;&#39064;&#21040;SEC&#25991;&#20214;&#21644;Stackexchange&#37327;&#21270;&#37329;&#34701;&#35752;&#35770;&#30340;&#24191;&#27867;&#37329;&#34701;&#30456;&#20851;&#20027;&#39064;&#12290;InvestLM&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#21253;&#25324;&#23545;&#20914;&#22522;&#37329;&#32463;&#29702;&#21644;&#30740;&#31350;&#20998;&#26512;&#24072;&#22312;&#20869;&#30340;&#37329;&#34701;&#19987;&#23478;&#23558;InvestLM&#30340;&#22238;&#31572;&#35780;&#20215;&#20026;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#65288;GPT-3.5&#12289;GPT-4&#21644;Claude-2&#65289;&#21487;&#23218;&#32654;&#12290;&#23545;&#19968;&#32452;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#36827;&#34892;&#38646;&#26679;&#26412;&#35780;&#20272;&#34920;&#26126;&#20102;&#20854;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20174;&#30740;&#31350;&#35282;&#24230;&#26469;&#30475;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;LLM&#36827;&#34892;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new financial domain large language model, InvestLM, tuned on LLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset related to financial investment. Inspired by less-is-more-for-alignment (Zhou et al., 2023), we manually curate a small yet diverse instruction dataset, covering a wide range of financial related topics, from Chartered Financial Analyst (CFA) exam questions to SEC filings to Stackexchange quantitative finance discussions. InvestLM shows strong capabilities in understanding financial text and provides helpful responses to investment related questions. Financial experts, including hedge fund managers and research analysts, rate InvestLM's response as comparable to those of state-of-the-art commercial models (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of financial NLP benchmarks demonstrates strong generalizability. From a research perspective, this work suggests that a high-quality domain specific LLM can be tuned usin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;AI&#23548;&#24072;&#19982;&#23398;&#20064;&#35745;&#21010;&#30456;&#32467;&#21512;&#65292;&#23454;&#26045;&#20102;&#20010;&#24615;&#21270;&#12289;&#26816;&#32034;&#32451;&#20064;&#21644;&#38388;&#38548;&#37325;&#22797;&#31561;&#23398;&#20064;&#21407;&#29702;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#31215;&#26497;&#20351;&#29992;AI&#23548;&#24072;&#21442;&#19982;&#23398;&#20064;&#30340;&#23398;&#29983;&#33719;&#24471;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2309.13060</link><description>&lt;p&gt;
&#29992;&#20010;&#20154;AI&#23548;&#24072;&#23454;&#26045;&#23398;&#20064;&#21407;&#29702;&#65306;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Implementing Learning Principles with a Personal AI Tutor: A Case Study. (arXiv:2309.13060v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;AI&#23548;&#24072;&#19982;&#23398;&#20064;&#35745;&#21010;&#30456;&#32467;&#21512;&#65292;&#23454;&#26045;&#20102;&#20010;&#24615;&#21270;&#12289;&#26816;&#32034;&#32451;&#20064;&#21644;&#38388;&#38548;&#37325;&#22797;&#31561;&#23398;&#20064;&#21407;&#29702;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#31215;&#26497;&#20351;&#29992;AI&#23548;&#24072;&#21442;&#19982;&#23398;&#20064;&#30340;&#23398;&#29983;&#33719;&#24471;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20010;&#24615;&#21270;&#12289;&#26816;&#32034;&#32451;&#20064;&#21644;&#38388;&#38548;&#37325;&#22797;&#31561;&#21407;&#21017;&#30340;&#26377;&#25928;&#23398;&#20064;&#31574;&#30053;&#24448;&#24448;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#23454;&#26045;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;AI&#23548;&#24072;&#19982;&#23398;&#20064;&#35745;&#21010;&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#23398;&#20064;&#31185;&#23398;&#36827;&#34892;&#34917;&#20805;&#12290;&#22312;UniDistance Suisse&#36827;&#34892;&#20102;&#19968;&#20010;&#23398;&#26399;&#38271;&#30340;&#30740;&#31350;&#65292;&#23558;&#19968;&#20010;AI&#23548;&#24072;&#24212;&#29992;&#25552;&#20379;&#32473;&#20462;&#35835;&#31070;&#32463;&#31185;&#23398;&#35838;&#31243;&#30340;&#24515;&#29702;&#23398;&#23398;&#29983;&#65288;N=51&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;GPT-3&#20174;&#29616;&#26377;&#35838;&#31243;&#26448;&#26009;&#33258;&#21160;&#29983;&#25104;&#24494;&#23398;&#20064;&#38382;&#39064;&#65292;AI&#23548;&#24072;&#24320;&#21457;&#20102;&#27599;&#20010;&#23398;&#29983;&#23545;&#20851;&#38190;&#27010;&#24565;&#30340;&#29702;&#35299;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#23398;&#29983;&#20010;&#20307;&#27700;&#24179;&#21644;&#33021;&#21147;&#20010;&#24615;&#21270;&#23454;&#26045;&#20998;&#24067;&#24335;&#26816;&#32034;&#32451;&#20064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31215;&#26497;&#20351;&#29992;AI&#23548;&#24072;&#21442;&#19982;&#23398;&#20064;&#30340;&#23398;&#29983;&#33719;&#24471;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25104;&#32489;&#12290;&#27492;&#22806;&#65292;&#31215;&#26497;&#21442;&#19982;&#23548;&#33268;&#24179;&#22343;&#25552;&#39640;&#20102;&#26368;&#22810;15&#20010;&#30334;&#20998;&#28857;&#65292;&#30456;&#27604;&#20110;&#24179;&#34892;&#35838;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective learning strategies based on principles like personalization, retrieval practice, and spaced repetition are often challenging to implement due to practical constraints. Here we explore the integration of AI tutors to complement learning programs in accordance with learning sciences. A semester-long study was conducted at UniDistance Suisse, where an AI tutor app was provided to psychology students taking a neuroscience course (N=51). After automatically generating microlearning questions from existing course materials using GPT-3, the AI tutor developed a dynamic neural-network model of each student's grasp of key concepts. This enabled the implementation of distributed retrieval practice, personalized to each student's individual level and abilities. The results indicate that students who actively engaged with the AI tutor achieved significantly higher grades. Moreover, active engagement led to an average improvement of up to 15 percentile points compared to a parallel cours
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#26377;&#39118;&#38505;&#30340;&#23398;&#29983;&#65292;&#24182;&#26681;&#25454;&#23398;&#29983;&#30340;&#35838;&#31243;&#36827;&#23637;&#19981;&#21516;&#38454;&#27573;&#36827;&#34892;&#35843;&#25972;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26368;&#26089;&#30340;&#38454;&#27573;&#23601;&#21487;&#20197;&#30456;&#24403;&#20934;&#30830;&#22320;&#21306;&#20998;&#23558;&#35201;&#27605;&#19994;&#30340;&#23398;&#29983;&#19982;&#26377;&#39118;&#38505;&#30340;&#23398;&#29983;&#65292;&#24182;&#19988;&#21518;&#32773;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20250;&#36805;&#36895;&#25552;&#39640;&#65292;&#20294;&#22312;&#21518;&#32773;&#36825;&#20010;&#31867;&#21035;&#20869;&#37096;&#30340;&#21306;&#20998;&#24230;&#20250;&#30456;&#23545;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.13052</link><description>&lt;p&gt;
&#23398;&#29983;&#25104;&#21151;&#24314;&#27169;&#65306;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Students Success Modeling: Most Important Factors. (arXiv:2309.13052v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#26377;&#39118;&#38505;&#30340;&#23398;&#29983;&#65292;&#24182;&#26681;&#25454;&#23398;&#29983;&#30340;&#35838;&#31243;&#36827;&#23637;&#19981;&#21516;&#38454;&#27573;&#36827;&#34892;&#35843;&#25972;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26368;&#26089;&#30340;&#38454;&#27573;&#23601;&#21487;&#20197;&#30456;&#24403;&#20934;&#30830;&#22320;&#21306;&#20998;&#23558;&#35201;&#27605;&#19994;&#30340;&#23398;&#29983;&#19982;&#26377;&#39118;&#38505;&#30340;&#23398;&#29983;&#65292;&#24182;&#19988;&#21518;&#32773;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20250;&#36805;&#36895;&#25552;&#39640;&#65292;&#20294;&#22312;&#21518;&#32773;&#36825;&#20010;&#31867;&#21035;&#20869;&#37096;&#30340;&#21306;&#20998;&#24230;&#20250;&#30456;&#23545;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#26469;&#35828;&#65292;&#20445;&#30041;&#29575;&#30340;&#37325;&#35201;&#24615;&#20419;&#20351;&#25968;&#25454;&#20998;&#26512;&#24072;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#39044;&#27979;&#26377;&#39118;&#38505;&#30340;&#23398;&#29983;&#12290;&#22312;&#28608;&#21169;&#19979;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;60,822&#21517;&#23398;&#29983;&#30340;121&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#36890;&#36807;&#35760;&#24405;&#36827;&#34892;&#20102;&#25552;&#21462;&#25110;&#24037;&#31243;&#21270;&#22788;&#29702;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#35782;&#21035;&#26377;&#21487;&#33021;&#27605;&#19994;&#30340;&#23398;&#29983;&#65292;&#26377;&#21487;&#33021;&#36716;&#26657;&#30340;&#23398;&#29983;&#20197;&#21450;&#26377;&#21487;&#33021;&#36749;&#23398;&#20013;&#36884;&#31163;&#24320;&#39640;&#31561;&#23398;&#24220;&#30340;&#23398;&#29983;&#12290;&#26412;&#30740;&#31350;&#36824;&#23581;&#35797;&#26681;&#25454;&#23398;&#29983;&#30340;&#35838;&#31243;&#36827;&#23637;&#19981;&#21516;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;LSTM&#23618;&#26469;&#32771;&#34385;&#26102;&#38388;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#26368;&#26089;&#30340;&#38454;&#27573;&#23601;&#21487;&#20197;&#30456;&#24403;&#20934;&#30830;&#22320;&#21306;&#20998;&#23558;&#35201;&#27605;&#19994;&#30340;&#23398;&#29983;&#19982;&#26377;&#39118;&#38505;&#30340;&#23398;&#29983;&#65292;&#24182;&#19988;&#21518;&#32773;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20250;&#36805;&#36895;&#25552;&#39640;&#65292;&#20294;&#22312;&#21518;&#32773;&#36825;&#20010;&#31867;&#21035;&#20869;&#37096;&#30340;&#21306;&#20998;&#24230;&#20250;&#30456;&#23545;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
The importance of retention rate for higher education institutions has encouraged data analysts to present various methods to predict at-risk students. The present study, motivated by the same encouragement, proposes a deep learning model trained with 121 features of diverse categories extracted or engineered out of the records of 60,822 postsecondary students. The model undertakes to identify students likely to graduate, the ones likely to transfer to a different school, and the ones likely to drop out and leave their higher education unfinished. This study undertakes to adjust its predictive methods for different stages of curricular progress of students. The temporal aspects introduced for this purpose are accounted for by incorporating layers of LSTM in the model. Our experiments demonstrate that distinguishing between to-be-graduate and at-risk students is reasonably achievable in the earliest stages, and then it rapidly improves, but the resolution within the latter category (dro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#25968;&#25454;&#24211;&#21644;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#65292;&#35299;&#30721;&#20102;&#32654;&#22269;&#39640;&#31561;&#25945;&#32946;&#31995;&#32479;&#20013;&#23398;&#20301;&#30340;&#19981;&#30830;&#23450;&#34920;&#36798;&#65292;&#24182;&#36890;&#36807;&#23545;&#23398;&#29983;&#36861;&#36394;&#25253;&#21578;&#36827;&#34892;&#35299;&#37322;&#21644;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#23398;&#20301;&#32423;&#21035;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#36825;&#31181;&#20998;&#31867;&#26377;&#21161;&#20110;&#30740;&#31350;&#23398;&#29983;&#25104;&#21151;&#21644;&#27969;&#21160;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.13050</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#26041;&#27861;&#65306;&#25968;&#25454;&#24211;&#21644;&#25991;&#26412;&#25366;&#25496;&#65292;&#35299;&#30721;&#32654;&#22269;&#39640;&#31561;&#25945;&#32946;&#20307;&#31995;&#20013;&#30340;&#23398;&#20301;&#30340;&#23383;&#27597;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding the Alphabet Soup of Degrees in the United States Postsecondary Education System Through Hybrid Method: Database and Text Mining. (arXiv:2309.13050v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#25968;&#25454;&#24211;&#21644;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#65292;&#35299;&#30721;&#20102;&#32654;&#22269;&#39640;&#31561;&#25945;&#32946;&#31995;&#32479;&#20013;&#23398;&#20301;&#30340;&#19981;&#30830;&#23450;&#34920;&#36798;&#65292;&#24182;&#36890;&#36807;&#23545;&#23398;&#29983;&#36861;&#36394;&#25253;&#21578;&#36827;&#34892;&#35299;&#37322;&#21644;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#23398;&#20301;&#32423;&#21035;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#36825;&#31181;&#20998;&#31867;&#26377;&#21161;&#20110;&#30740;&#31350;&#23398;&#29983;&#25104;&#21151;&#21644;&#27969;&#21160;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;&#22269;&#23478;&#23398;&#29983;&#28165;&#31639;&#20013;&#24515;&#65288;NSC&#65289;&#30340;&#23398;&#29983;&#36861;&#36394;&#25253;&#21578;&#20013;&#21547;&#31946;&#19981;&#28165;&#22320;&#34920;&#36798;&#30340;&#39640;&#31561;&#25945;&#32946;&#23398;&#20301;&#65288;&#20363;&#22914;&#23398;&#22763;&#12289;&#30805;&#22763;&#31561;&#65289;&#30340;&#32423;&#21035;&#12290;&#35813;&#27169;&#22411;&#26159;&#20004;&#20010;&#27169;&#22359;&#30340;&#28151;&#21512;&#20307;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#36890;&#36807;&#21442;&#32771;&#25105;&#20204;&#32534;&#21046;&#30340;&#36817;950&#20010;&#32654;&#22269;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#23398;&#20301;&#26631;&#39064;&#32553;&#20889;&#30340;&#32508;&#21512;&#25968;&#25454;&#24211;&#65292;&#35299;&#37322;NSC&#25253;&#21578;&#20013;&#23884;&#20837;&#30340;&#30456;&#20851;&#32553;&#20889;&#20803;&#32032;&#12290;&#31532;&#20108;&#20010;&#27169;&#22359;&#26159;CNN-BiLSTM&#27169;&#22411;&#30340;&#29305;&#24449;&#20998;&#31867;&#21644;&#25991;&#26412;&#25366;&#25496;&#30340;&#32452;&#21512;&#65292;&#21069;&#38754;&#26377;&#25968;&#20010;&#32321;&#37325;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#36890;&#36807;&#22235;&#20010;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#22312;&#26368;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#19978;&#36820;&#22238;&#20102;97.83&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#31181;&#23545;&#23398;&#20301;&#32423;&#21035;&#30340;&#24443;&#24213;&#20998;&#31867;&#23558;&#20026;&#30740;&#31350;&#23398;&#29983;&#25104;&#21151;&#21644;&#27969;&#21160;&#30340;&#27169;&#24335;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a model to predict the levels (e.g., Bachelor, Master, etc.) of postsecondary degree awards that have been ambiguously expressed in the student tracking reports of the National Student Clearinghouse (NSC). The model will be the hybrid of two modules. The first module interprets the relevant abbreviatory elements embedded in NSC reports by referring to a comprehensive database that we have made of nearly 950 abbreviations for degree titles used by American postsecondary educators. The second module is a combination of feature classification and text mining modeled with CNN-BiLSTM, which is preceded by several steps of heavy pre-processing. The model proposed in this paper was trained with four multi-label datasets of different grades of resolution and returned 97.83\% accuracy with the most sophisticated dataset. Such a thorough classification of degree levels will provide insights into the modeling patterns of student success and mobility. To date, such a classifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroCADR&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#32508;&#21512;&#35745;&#31639;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#37325;&#29992;&#20197;&#25581;&#31034;&#26032;&#30340;&#25239;&#30315;&#30187;&#33647;&#29289;&#20505;&#36873;&#20154;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#24211;&#30340;&#20449;&#24687;&#65292;&#30830;&#23450;&#38774;&#34507;&#30333;&#21644;&#33647;&#29289;&#20998;&#23376;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;k&#26368;&#36817;&#37051;&#31639;&#27861;&#12289;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#21644;&#20915;&#31574;&#26641;&#31561;&#26041;&#27861;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13047</link><description>&lt;p&gt;
NeuroCADR:&#36890;&#36807;&#32508;&#21512;&#35745;&#31639;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#37325;&#29992;&#20197;&#25581;&#31034;&#26032;&#30340;&#25239;&#30315;&#30187;&#33647;&#29289;&#20505;&#36873;&#20154;
&lt;/p&gt;
&lt;p&gt;
NeuroCADR: Drug Repurposing to Reveal Novel Anti-Epileptic Drug Candidates Through an Integrated Computational Approach. (arXiv:2309.13047v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroCADR&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#32508;&#21512;&#35745;&#31639;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#37325;&#29992;&#20197;&#25581;&#31034;&#26032;&#30340;&#25239;&#30315;&#30187;&#33647;&#29289;&#20505;&#36873;&#20154;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#24211;&#30340;&#20449;&#24687;&#65292;&#30830;&#23450;&#38774;&#34507;&#30333;&#21644;&#33647;&#29289;&#20998;&#23376;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;k&#26368;&#36817;&#37051;&#31639;&#27861;&#12289;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#21644;&#20915;&#31574;&#26641;&#31561;&#26041;&#27861;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#37325;&#29992;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33647;&#29289;&#21457;&#29616;&#26041;&#27861;&#65292;&#28041;&#21450;&#23558;&#29616;&#26377;&#33647;&#29289;&#37325;&#26032;&#29992;&#20110;&#26032;&#30340;&#30446;&#30340;&#12290;&#19982;&#20256;&#32479;&#30340;&#33647;&#29289;&#24320;&#21457;&#30340;&#20840;&#26032;&#36807;&#31243;&#30456;&#27604;&#65292;&#37325;&#29992;&#33647;&#29289;&#26356;&#24555;&#36895;&#12289;&#26356;&#24265;&#20215;&#12289;&#26356;&#23569;&#22833;&#36133;&#12290;&#26368;&#36817;&#65292;&#33647;&#29289;&#37325;&#29992;&#22312;&#35745;&#31639;&#26426;&#27169;&#25311;&#20013;&#36827;&#34892;&#65292;&#21033;&#29992;&#33647;&#29289;&#21644;&#21270;&#23398;&#20449;&#24687;&#30340;&#25968;&#25454;&#24211;&#30830;&#23450;&#38774;&#34507;&#30333;&#21644;&#33647;&#29289;&#20998;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35782;&#21035;&#28508;&#22312;&#30340;&#33647;&#29289;&#20505;&#36873;&#20154;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroCADR&#30340;&#31639;&#27861;&#65292;&#23427;&#26159;&#36890;&#36807;&#22810;&#37325;&#36884;&#24452;&#30340;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#37325;&#29992;&#30340;&#19968;&#31181;&#26032;&#39062;&#31995;&#32479;&#65292;&#21253;&#25324;k&#26368;&#36817;&#37051;&#31639;&#27861;&#65288;KNN&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#21644;&#20915;&#31574;&#26641;&#12290;&#25968;&#25454;&#26469;&#33258;&#20960;&#20010;&#25968;&#25454;&#24211;&#65292;&#21253;&#25324;&#30142;&#30149;&#12289;&#30151;&#29366;&#12289;&#22522;&#22240;&#21644;&#30456;&#20851;&#33647;&#29289;&#20998;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#28982;&#21518;&#32534;&#35793;&#25104;&#20108;&#36827;&#21046;&#34920;&#31034;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#34920;&#29616;&#20248;&#20110;&#36817;&#20284;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug repurposing is an emerging approach for drug discovery involving the reassignment of existing drugs for novel purposes. An alternative to the traditional de novo process of drug development, repurposed drugs are faster, cheaper, and less failure prone than drugs developed from traditional methods. Recently, drug repurposing has been performed in silico, in which databases of drugs and chemical information are used to determine interactions between target proteins and drug molecules to identify potential drug candidates. A proposed algorithm is NeuroCADR, a novel system for drug repurposing via a multi-pronged approach consisting of k-nearest neighbor algorithms (KNN), random forest classification, and decision trees. Data was sourced from several databases consisting of interactions between diseases, symptoms, genes, and affiliated drug molecules, which were then compiled into datasets expressed in binary. The proposed method displayed a high level of accuracy, outperforming nearl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#34892;&#20026;&#35748;&#35777;&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#25216;&#26415;&#26469;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#20197;&#38450;&#27490;&#38544;&#31169;&#25915;&#20987;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#20010;&#20154;&#36164;&#26009;&#25968;&#25454;&#24211;&#30340;&#38656;&#27714;&#65292;&#24182;&#33021;&#26377;&#25928;&#39564;&#35777;&#29992;&#25143;&#30340;&#36523;&#20221;&#12290;</title><link>http://arxiv.org/abs/2309.13046</link><description>&lt;p&gt;
&#38024;&#23545;&#34892;&#20026;&#35748;&#35777;&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy Preserving Machine Learning for Behavioral Authentication Systems. (arXiv:2309.13046v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#34892;&#20026;&#35748;&#35777;&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#25216;&#26415;&#26469;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#20197;&#38450;&#27490;&#38544;&#31169;&#25915;&#20987;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#20010;&#20154;&#36164;&#26009;&#25968;&#25454;&#24211;&#30340;&#38656;&#27714;&#65292;&#24182;&#33021;&#26377;&#25928;&#39564;&#35777;&#29992;&#25143;&#30340;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#35748;&#35777;&#31995;&#32479;&#20351;&#29992;&#29992;&#25143;&#30340;&#34892;&#20026;&#29305;&#24449;&#26469;&#39564;&#35777;&#20854;&#36523;&#20221;&#12290;&#36890;&#36807;&#22312;&#29992;&#25143;&#20010;&#20154;&#36164;&#26009;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#34892;&#20026;&#35748;&#35777;&#39564;&#35777;&#31639;&#27861;&#12290;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#21576;&#29616;&#30340;&#39564;&#35777;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#22914;&#26524;&#20998;&#31867;&#32467;&#26524;&#19982;&#22768;&#26126;&#30340;&#36523;&#20221;&#21305;&#37197;&#65292;&#21017;&#25509;&#21463;&#35813;&#22768;&#26126;&#12290;&#36825;&#31181;&#22522;&#20110;&#20998;&#31867;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#32500;&#25252;&#20010;&#20154;&#36164;&#26009;&#25968;&#25454;&#24211;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#34892;&#20026;&#35748;&#35777;&#31995;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#25915;&#20987;&#12290;&#20026;&#20102;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#24191;&#27867;&#20351;&#29992;&#21508;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#19968;&#31181;&#38750;&#21152;&#23494;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#26469;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#38543;&#26426;&#25237;&#24433;&#26159;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#30697;&#38453;&#30340;&#36317;&#31163;&#20445;&#25345;&#36716;&#25442;&#12290;&#22312;&#19982;&#39564;&#35777;&#32773;&#20849;&#20139;&#20010;&#20154;&#36164;&#26009;&#20043;&#21069;&#65292;&#29992;&#25143;&#23558;&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#23545;&#20854;&#20010;&#20154;&#36164;&#26009;&#36827;&#34892;&#36716;&#25442;&#65292;&#24182;&#20445;&#25345;&#20854;&#38544;&#31169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A behavioral authentication (BA) system uses the behavioral characteristics of users to verify their identity claims. A BA verification algorithm can be constructed by training a neural network (NN) classifier on users' profiles. The trained NN model classifies the presented verification data, and if the classification matches the claimed identity, the verification algorithm accepts the claim. This classification-based approach removes the need to maintain a profile database. However, similar to other NN architectures, the NN classifier of the BA system is vulnerable to privacy attacks. To protect the privacy of training and test data used in an NN different techniques are widely used. In this paper, our focus is on a non-crypto-based approach, and we used random projection (RP) to ensure data privacy in an NN model. RP is a distance-preserving transformation based on a random matrix. Before sharing the profiles with the verifier, users will transform their profiles by RP and keep thei
&lt;/p&gt;</description></item><item><title>&#34920;&#36798;&#24615;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#27169;&#22411;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25552;&#20379;&#22266;&#26377;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#36807;&#24230;&#21442;&#25968;&#21270;&#20445;&#35777;&#27169;&#22411;&#21487;&#35757;&#32451;&#24615;&#12290;&#36890;&#36807;&#35299;&#20915;&#39640;&#27425;&#22810;&#20803;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#23545;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#22256;&#38590;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13002</link><description>&lt;p&gt;
&#34920;&#36798;&#24615;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25552;&#20379;&#22266;&#26377;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Expressive variational quantum circuits provide inherent privacy in federated learning. (arXiv:2309.13002v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13002
&lt;/p&gt;
&lt;p&gt;
&#34920;&#36798;&#24615;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#27169;&#22411;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25552;&#20379;&#22266;&#26377;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#36807;&#24230;&#21442;&#25968;&#21270;&#20445;&#35777;&#27169;&#22411;&#21487;&#35757;&#32451;&#24615;&#12290;&#36890;&#36807;&#35299;&#20915;&#39640;&#27425;&#22810;&#20803;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#23545;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#20998;&#24067;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#19982;&#20013;&#22830;&#32858;&#21512;&#22120;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#26631;&#20934;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#30340;&#26799;&#24230;&#30340;&#25968;&#25454;&#27844;&#38706;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20351;&#29992;&#34920;&#36798;&#24615;&#32534;&#30721;&#26144;&#23556;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;ans\"tze&#26500;&#24314;&#30340;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#34920;&#36798;&#24615;&#26144;&#23556;&#23548;&#33268;&#23545;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#20855;&#26377;&#22266;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#32780;&#36807;&#24230;&#21442;&#25968;&#21270;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#24615;&#12290;&#25105;&#20204;&#30340;&#38544;&#31169;&#26694;&#26550;&#38598;&#20013;&#22312;&#36890;&#36807;&#37327;&#23376;&#30005;&#36335;&#26799;&#24230;&#29983;&#25104;&#30340;&#39640;&#27425;&#22810;&#20803;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#30340;&#35299;&#20915;&#22797;&#26434;&#24615;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#35770;&#28857;&#65292;&#24378;&#35843;&#22312;&#31934;&#30830;&#21644;&#36817;&#20284;&#24773;&#20917;&#19979;&#35299;&#20915;&#36825;&#20123;&#26041;&#31243;&#30340;&#22266;&#26377;&#22256;&#38590;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has emerged as a viable distributed solution to train machine learning models without the actual need to share data with the central aggregator. However, standard neural network-based federated learning models have been shown to be susceptible to data leakage from the gradients shared with the server. In this work, we introduce federated learning with variational quantum circuit model built using expressive encoding maps coupled with overparameterized ans\"atze. We show that expressive maps lead to inherent privacy against gradient inversion attacks, while overparameterization ensures model trainability. Our privacy framework centers on the complexity of solving the system of high-degree multivariate Chebyshev polynomials generated by the gradients of quantum circuit. We present compelling arguments highlighting the inherent difficulty in solving these equations, both in exact and approximate scenarios. Additionally, we delve into machine learning-based attack strate
&lt;/p&gt;</description></item><item><title>ForceSight&#26159;&#19968;&#20010;&#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#31227;&#21160;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#35270;&#35273;&#21147;&#23548;&#21521;&#30446;&#26631;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#22312;&#26410;&#35265;&#29615;&#22659;&#20013;&#36827;&#34892;&#31934;&#30830;&#25235;&#21462;&#12289;&#25277;&#23625;&#25171;&#24320;&#21644;&#29289;&#20307;&#20132;&#25509;&#31561;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.12312</link><description>&lt;p&gt;
ForceSight: &#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#35273;&#21147;&#23548;&#21521;&#31227;&#21160;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals. (arXiv:2309.12312v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12312
&lt;/p&gt;
&lt;p&gt;
ForceSight&#26159;&#19968;&#20010;&#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#31227;&#21160;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#35270;&#35273;&#21147;&#23548;&#21521;&#30446;&#26631;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#22312;&#26410;&#35265;&#29615;&#22659;&#20013;&#36827;&#34892;&#31934;&#30830;&#25235;&#21462;&#12289;&#25277;&#23625;&#25171;&#24320;&#21644;&#29289;&#20307;&#20132;&#25509;&#31561;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ForceSight&#30340;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#26469;&#39044;&#27979;&#35270;&#35273;&#21147;&#23548;&#21521;&#30340;&#30446;&#26631;&#12290;&#32473;&#23450;&#19968;&#24352;RGBD&#22270;&#29255;&#21644;&#19968;&#20010;&#25991;&#26412;&#25552;&#31034;&#65292;ForceSight&#21487;&#20197;&#30830;&#23450;&#30456;&#26426;&#22352;&#26631;&#31995;&#19979;&#30340;&#30446;&#26631;&#26411;&#31471;&#25191;&#34892;&#22120;&#20301;&#23039;&#65288;&#36816;&#21160;&#30446;&#26631;&#65289;&#21644;&#30456;&#20851;&#30340;&#21147;&#37327;&#65288;&#21147;&#37327;&#30446;&#26631;&#65289;&#12290;&#36825;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#20849;&#21516;&#24418;&#25104;&#20102;&#19968;&#20010;&#35270;&#35273;&#21147;&#23548;&#21521;&#30446;&#26631;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36755;&#20986;&#20154;&#21487;&#35299;&#37322;&#30340;&#36816;&#21160;&#30446;&#26631;&#30340;&#28145;&#24230;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#24039;&#22937;&#25805;&#20316;&#12290;&#21147;&#37327;&#22312;&#25805;&#20316;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#36739;&#20302;&#23618;&#27425;&#30340;&#25191;&#34892;&#20013;&#12290;&#24403;&#24212;&#29992;&#20110;&#24102;&#26377;&#25163;&#33218;&#21644;&#30524;&#30555;&#30340;&#31227;&#21160;&#25805;&#20316;&#35013;&#32622;&#30340;ForceSight&#26102;&#65292;&#22312;&#19982;&#35757;&#32451;&#25968;&#25454;&#24046;&#24322;&#26174;&#33879;&#30340;&#26410;&#35265;&#29615;&#22659;&#20013;&#65292;&#33021;&#22815;&#20197;81%&#30340;&#25104;&#21151;&#29575;&#23436;&#25104;&#35832;&#22914;&#31934;&#30830;&#25235;&#21462;&#12289;&#25277;&#23625;&#25171;&#24320;&#21644;&#29289;&#20307;&#20132;&#25509;&#31561;&#20219;&#21153;&#12290;&#22312;&#21478;&#19968;&#39033;&#29420;&#31435;&#23454;&#39564;&#20013;&#65292;ForceSight&#20165;&#20351;&#29992;&#35270;&#35273;&#20282;&#26381;&#65292;&#19981;&#32771;&#34385;&#21147;&#37327;&#20449;&#24687;&#65292;&#20294;&#20381;&#28982;&#26174;&#31034;&#20986;&#36739;&#39640;&#30340;&#25805;&#20316;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ForceSight, a system for text-guided mobile manipulation that predicts visual-force goals using a deep neural network. Given a single RGBD image combined with a text prompt, ForceSight determines a target end-effector pose in the camera frame (kinematic goal) and the associated forces (force goal). Together, these two components form a visual-force goal. Prior work has demonstrated that deep models outputting human-interpretable kinematic goals can enable dexterous manipulation by real robots. Forces are critical to manipulation, yet have typically been relegated to lower-level execution in these systems. When deployed on a mobile manipulator equipped with an eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps, drawer opening, and object handovers with an 81% success rate in unseen environments with object instances that differed significantly from the training data. In a separate experiment, relying exclusively on visual servoing and ignoring force 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#39550;&#39542;&#34892;&#20026;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#19979;&#39640;&#25928;&#22320;&#23398;&#20064;&#24182;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12295</link><description>&lt;p&gt;
&#23398;&#20064;&#39550;&#39542;&#21040;&#20219;&#20309;&#22320;&#26041;
&lt;/p&gt;
&lt;p&gt;
Learning to Drive Anywhere. (arXiv:2309.12295v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#39550;&#39542;&#34892;&#20026;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#19979;&#39640;&#25928;&#22320;&#23398;&#20064;&#24182;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#39550;&#39542;&#21592;&#21487;&#20197;&#26080;&#32541;&#22320;&#36866;&#24212;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#30340;&#39550;&#39542;&#20915;&#31574;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#36947;&#36335;&#26465;&#20214;&#21644;&#20132;&#36890;&#35268;&#21017;&#65292;&#20363;&#22914;&#24038;&#39550;&#39542;&#21644;&#21491;&#39550;&#39542;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#21482;&#33021;&#22312;&#38480;&#23450;&#30340;&#25805;&#20316;&#39046;&#22495;&#20869;&#37096;&#32626;&#65292;&#19981;&#33021;&#32771;&#34385;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#20043;&#38388;&#30340;&#39550;&#39542;&#34892;&#20026;&#24046;&#24322;&#21644;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AnyD&#65292;&#19968;&#31181;&#21333;&#19968;&#30340;&#20855;&#26377;&#22320;&#29702;&#24863;&#30693;&#30340;&#26465;&#20214;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;CIL&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#20855;&#26377;&#21160;&#24577;&#29615;&#22659;&#12289;&#20132;&#36890;&#21644;&#31038;&#20250;&#29305;&#24449;&#30340;&#24322;&#26500;&#21644;&#20840;&#29699;&#20998;&#24067;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#24341;&#20837;&#19968;&#20010;&#39640;&#23481;&#37327;&#30340;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#19979;&#26377;&#25928;&#22320;&#36866;&#24212;&#26412;&#22320;&#32454;&#24494;&#24046;&#24322;&#24182;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#20248;&#21270;&#23545;&#27604;&#24615;&#27169;&#20223;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#36866;&#24212;&#22266;&#26377;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#21644;&#22320;&#29702;&#20301;&#32622;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human drivers can seamlessly adapt their driving decisions across geographical locations with diverse conditions and rules of the road, e.g., left vs. right-hand traffic. In contrast, existing models for autonomous driving have been thus far only deployed within restricted operational domains, i.e., without accounting for varying driving behaviors across locations or model scalability. In this work, we propose AnyD, a single geographically-aware conditional imitation learning (CIL) model that can efficiently learn from heterogeneous and globally distributed data with dynamic environmental, traffic, and social characteristics. Our key insight is to introduce a high-capacity geo-location-based channel attention mechanism that effectively adapts to local nuances while also flexibly modeling similarities among regions in a data-driven manner. By optimizing a contrastive imitation objective, our proposed approach can efficiently scale across inherently imbalanced data distributions and loca
&lt;/p&gt;</description></item><item><title>LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12288</link><description>&lt;p&gt;
&#32763;&#36716;&#35781;&#21650;: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;"A&#26159;B"&#26080;&#27861;&#23398;&#20064;"B&#26159;A"
&lt;/p&gt;
&lt;p&gt;
The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12288
&lt;/p&gt;
&lt;p&gt;
LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27867;&#21270;&#19978;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#12290;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"A&#26159;B"&#24418;&#24335;&#30340;&#21477;&#23376;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#25512;&#24191;&#21040;&#30456;&#21453;&#30340;&#26041;&#21521;"B&#26159;A"&#12290;&#36825;&#23601;&#26159;&#32763;&#36716;&#35781;&#21650;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"Olaf Scholz&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;"&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#33021;&#22815;&#22238;&#31572;&#38382;&#39064;"&#35841;&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;&#65311;"&#12290;&#27492;&#22806;&#65292;&#27491;&#30830;&#31572;&#26696;&#65288;"Olaf Scholz"&#65289;&#30340;&#21487;&#33021;&#24615;&#19981;&#20250;&#27604;&#38543;&#26426;&#21517;&#23383;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#20250;&#25512;&#24191;&#21040;&#23427;&#20204;&#35757;&#32451;&#38598;&#20013;&#30340;&#26222;&#36941;&#27169;&#24335;&#65288;&#21363;&#22914;&#26524;&#20986;&#29616;"A&#26159;B"&#65292;&#21017;"B&#26159;A"&#26356;&#21487;&#33021;&#20986;&#29616;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#34394;&#26500;&#30340;&#38472;&#36848;&#65288;&#22914;"Uriah Hawthorne&#26159;'Abyssal Melodies'&#30340;&#20316;&#26354;&#23478;"&#65289;&#19978;&#23545;GPT-3&#21644;Llama-1&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#26080;&#27861;&#27491;&#30830;&#22238;&#31572;"&#35841;&#21019;&#20316;&#20102;'Abyssal Melodies'?"&#26469;&#25552;&#20379;&#32763;&#36716;&#35781;&#21650;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#31526;&#21512;&#21152;&#36733;&#26465;&#20214;&#30340;&#32467;&#26500;&#32452;&#20214;&#35774;&#35745;&#12290;&#19982;&#20854;&#20182;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#23545;&#29616;&#26377;&#35774;&#35745;&#36827;&#34892;&#32534;&#36753;&#65292;&#24182;&#19988;&#20855;&#26377;&#36817;&#20248;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#29983;&#25104;&#35774;&#35745;&#30340;&#32467;&#26500;&#24615;&#33021;&#21644;&#28508;&#22312;&#20505;&#36873;&#35774;&#35745;&#30340;&#21487;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11601</link><description>&lt;p&gt;
&#32467;&#26500;&#32452;&#20214;&#35774;&#35745;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Models for Structural Component Design. (arXiv:2309.11601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#31526;&#21512;&#21152;&#36733;&#26465;&#20214;&#30340;&#32467;&#26500;&#32452;&#20214;&#35774;&#35745;&#12290;&#19982;&#20854;&#20182;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#23545;&#29616;&#26377;&#35774;&#35745;&#36827;&#34892;&#32534;&#36753;&#65292;&#24182;&#19988;&#20855;&#26377;&#36817;&#20248;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#29983;&#25104;&#35774;&#35745;&#30340;&#32467;&#26500;&#24615;&#33021;&#21644;&#28508;&#22312;&#20505;&#36873;&#35774;&#35745;&#30340;&#21487;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#24050;&#32463;&#22312;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#31526;&#21512;&#29992;&#25143;&#38656;&#27714;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#32467;&#26500;&#32452;&#20214;&#30340;&#35774;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28385;&#36275;&#19968;&#32452;&#20855;&#20307;&#21152;&#36733;&#26465;&#20214;&#30340;&#32452;&#20214;&#28508;&#22312;&#35774;&#35745;&#12290;&#30456;&#27604;&#20110;&#20854;&#20182;&#29983;&#25104;&#26041;&#27861;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#19968;&#20010;&#26126;&#26174;&#20248;&#21183;&#26159;&#21487;&#20197;&#32534;&#36753;&#29616;&#26377;&#35774;&#35745;&#12290;&#25105;&#20204;&#20351;&#29992;&#20351;&#29992;SIMP&#31639;&#27861;&#24471;&#21040;&#30340;&#32467;&#26500;&#25299;&#25169;&#20248;&#21270;&#20960;&#20309;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26694;&#26550;&#29983;&#25104;&#30340;&#35774;&#35745;&#20855;&#26377;&#22266;&#26377;&#30340;&#36817;&#20248;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#25903;&#25345;&#29983;&#25104;&#35774;&#35745;&#32467;&#26500;&#24615;&#33021;&#21644;&#28508;&#22312;&#20505;&#36873;&#35774;&#35745;&#30340;&#21487;&#21464;&#24615;&#30340;&#23450;&#37327;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26694;&#26550;&#30340;&#21487;&#25193;&#23637;&#24615;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative modeling, namely Diffusion models, have revolutionized generative modeling, enabling high-quality image generation tailored to user needs. This paper proposes a framework for the generative design of structural components. Specifically, we employ a Latent Diffusion model to generate potential designs of a component that can satisfy a set of problem-specific loading conditions. One of the distinct advantages our approach offers over other generative approaches, such as generative adversarial networks (GANs), is that it permits the editing of existing designs. We train our model using a dataset of geometries obtained from structural topology optimization utilizing the SIMP algorithm. Consequently, our framework generates inherently near-optimal designs. Our work presents quantitative results that support the structural performance of the generated designs and the variability in potential candidate designs. Furthermore, we provide evidence of the scalability 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#25968;&#25454;&#20197;&#21450;&#25506;&#32034;&#20999;&#25442;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31361;&#23612;&#26031;&#26041;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26469;&#28040;&#38500;&#25340;&#20889;&#19981;&#21512;&#36866;&#30340;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2309.11327</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#25910;&#38598;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20999;&#25442;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition. (arXiv:2309.11327v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#25968;&#25454;&#20197;&#21450;&#25506;&#32034;&#20999;&#25442;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31361;&#23612;&#26031;&#26041;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26469;&#28040;&#38500;&#25340;&#20889;&#19981;&#21512;&#36866;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#26041;&#35328;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#35201;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#36824;&#35201;&#22788;&#29702;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#31361;&#23612;&#26031;&#26041;&#35328;&#65292;&#35299;&#20915;&#20102;&#19978;&#36848;ASR&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25910;&#38598;&#20102;&#25991;&#26412;&#21644;&#38899;&#39057;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25506;&#32034;&#33258;&#25105;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#23569;&#26679;&#26412;&#20999;&#25442;&#26041;&#27861;&#65292;&#20197;&#22312;&#19981;&#21516;&#31361;&#23612;&#26031;&#27979;&#35797;&#38598;&#19978;&#25512;&#21160;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65307;&#28085;&#30422;&#19981;&#21516;&#30340;&#22768;&#23398;&#12289;&#35821;&#35328;&#21644;&#38901;&#24459;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#37492;&#20110;&#24120;&#35268;&#25340;&#20889;&#30340;&#32570;&#22833;&#65292;&#25105;&#20204;&#23545;&#36716;&#24405;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#65292;&#20197;&#36991;&#20813;&#27979;&#35797;&#21442;&#32771;&#20013;&#30340;&#25340;&#20889;&#19981;&#21512;&#36866;&#25152;&#24102;&#26469;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36716;&#24405;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#12289;&#33521;&#35821;&#21644;&#27861;&#35821;&#28151;&#21512;&#35821;&#35328;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;&#25152;&#26377;&#35757;&#32451;&#21644;&#27979;&#35797;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#20379;&#20844;&#20247;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crafting an effective Automatic Speech Recognition (ASR) solution for dialects demands innovative approaches that not only address the data scarcity issue but also navigate the intricacies of linguistic diversity. In this paper, we address the aforementioned ASR challenge, focusing on the Tunisian dialect. First, textual and audio data is collected and in some cases annotated. Second, we explore self-supervision, semi-supervision and few-shot code-switching approaches to push the state-of-the-art on different Tunisian test sets; covering different acoustic, linguistic and prosodic conditions. Finally, and given the absence of conventional spelling, we produce a human evaluation of our transcripts to avoid the noise coming from spelling inadequacies in our testing references. Our models, allowing to transcribe audio samples in a linguistic mix involving Tunisian Arabic, English and French, and all the data used during training and testing are released for public use and further improvem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#26681;&#25454;&#32039;&#24613;&#31243;&#24230;&#39044;&#35686;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#12290;</title><link>http://arxiv.org/abs/2309.10980</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning. (arXiv:2309.10980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#26681;&#25454;&#32039;&#24613;&#31243;&#24230;&#39044;&#35686;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24739;&#32773;&#30417;&#27979;&#23545;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#21307;&#30103;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#30417;&#27979;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#22788;&#29702;&#22797;&#26434;&#12289;&#21160;&#24577;&#30340;&#29615;&#22659;&#21644;&#27874;&#21160;&#30340;&#29983;&#21629;&#20307;&#24449;&#65292;&#23548;&#33268;&#24310;&#36831;&#21457;&#29616;&#21361;&#24613;&#24773;&#20917;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37096;&#32626;&#20102;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#19987;&#38376;&#36127;&#36131;&#30417;&#27979;&#29305;&#23450;&#30340;&#29983;&#29702;&#29305;&#24449;&#65292;&#22914;&#24515;&#29575;&#12289;&#21628;&#21560;&#21644;&#20307;&#28201;&#12290;&#36825;&#20123;&#26234;&#33021;&#20307;&#19982;&#36890;&#29992;&#30340;&#21307;&#30103;&#30417;&#27979;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#23398;&#20064;&#24739;&#32773;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#30340;&#32039;&#24613;&#31243;&#24230;&#20570;&#20986;&#36890;&#30693;&#30456;&#24212;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#65288;MET&#65289;&#30340;&#20915;&#31574;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;PPG-DaLiA&#21644;WESAD&#65289;&#30340;&#30495;&#23454;&#29983;&#29702;&#21644;&#36816;&#21160;&#25968;&#25454;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#22810;&#26234;&#33021;&#20307;DRL&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23450;&#20041;&#22312;&#32039;&#33268;Riemannian&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;Matern&#39640;&#26031;&#36807;&#31243;&#21644;&#22806;&#22312;&#36807;&#31243;&#20043;&#38388;&#30340;&#25910;&#32553;&#36895;&#29575;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#36895;&#29575;&#22312;&#36866;&#24403;&#21305;&#37197;&#24179;&#28369;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#26159;&#30456;&#31561;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.10918</link><description>&lt;p&gt;
Riemannian&#27969;&#24418;&#19978;Matern&#39640;&#26031;&#36807;&#31243;&#30340;&#21518;&#39564;&#25910;&#32553;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Posterior Contraction Rates for Mat\'ern Gaussian Processes on Riemannian Manifolds. (arXiv:2309.10918v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23450;&#20041;&#22312;&#32039;&#33268;Riemannian&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;Matern&#39640;&#26031;&#36807;&#31243;&#21644;&#22806;&#22312;&#36807;&#31243;&#20043;&#38388;&#30340;&#25910;&#32553;&#36895;&#29575;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#36895;&#29575;&#22312;&#36866;&#24403;&#21305;&#37197;&#24179;&#28369;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#26159;&#30456;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22312;&#35768;&#22810;&#20381;&#36182;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#34987;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#22312;&#20960;&#20309;&#35774;&#32622;&#19979;&#22788;&#29702;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#24037;&#20855;&#65292;&#20363;&#22914;&#65292;&#24403;&#36755;&#20837;&#20301;&#20110;Riemannian&#27969;&#24418;&#19978;&#26102;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#20869;&#22312;&#27169;&#22411;&#22312;&#29702;&#35770;&#19978;&#26159;&#21542;&#21487;&#20197;&#35777;&#26126;&#30456;&#27604;&#20110;&#23558;&#25152;&#26377;&#30456;&#20851;&#37327;&#23884;&#20837;&#21040;$\mathbb{R}^d$&#24182;&#20351;&#29992;&#26222;&#36890;&#27431;&#20960;&#37324;&#24503;&#39640;&#26031;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23450;&#20041;&#22312;&#32039;&#33268;Riemannian&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;Matern&#39640;&#26031;&#36807;&#31243;&#30340;&#26368;&#20248;&#25910;&#32553;&#36895;&#29575;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#27969;&#24418;&#21644;&#29615;&#22659;Sobolev&#31354;&#38388;&#20043;&#38388;&#30340;&#36857;&#21644;&#25193;&#23637;&#23450;&#29702;&#35777;&#26126;&#20102;&#22806;&#22312;&#36807;&#31243;&#30340;&#31867;&#20284;&#36895;&#29575;&#65306;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25152;&#24471;&#21040;&#30340;&#36895;&#29575;&#19982;&#20869;&#22312;&#36807;&#31243;&#30340;&#36895;&#29575;&#30456;&#31526;&#65292;&#21069;&#25552;&#26159;&#23427;&#20204;&#30340;&#24179;&#28369;&#21442;&#25968;&#36866;&#24403;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#19968;&#20123;&#23454;&#35777;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23545;&#36825;&#20123;&#36895;&#29575;&#30340;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are used in many machine learning applications that rely on uncertainty quantification. Recently, computational tools for working with these models in geometric settings, such as when inputs lie on a Riemannian manifold, have been developed. This raises the question: can these intrinsic models be shown theoretically to lead to better performance, compared to simply embedding all relevant quantities into $\mathbb{R}^d$ and using the restriction of an ordinary Euclidean Gaussian process? To study this, we prove optimal contraction rates for intrinsic Mat\'ern Gaussian processes defined on compact Riemannian manifolds. We also prove analogous rates for extrinsic processes using trace and extension theorems between manifold and ambient Sobolev spaces: somewhat surprisingly, the rates obtained turn out to coincide with those of the intrinsic processes, provided that their smoothness parameters are matched appropriately. We illustrate these rates empirically on a number of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10639</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#20840;&#23616;${\mathcal L}^2$&#26368;&#23567;&#21270;&#22120;&#30340;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;$L$&#20010;&#38544;&#34255;&#23618;&#65292;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#65292;${\mathcal L}^2$ Schatten&#31867;&#65288;&#25110;Hilbert-Schmidt&#65289;&#25104;&#26412;&#20989;&#25968;&#65292;&#20197;&#21450;&#30456;&#31561;&#32500;&#24230;$Q\geq1$&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#12290;&#38544;&#34255;&#23618;&#20063;&#23450;&#20041;&#22312;${\mathbb R}^{Q}$&#30340;&#31354;&#38388;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#26368;&#26032;&#30340;&#20851;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26524;&#65292;&#22312;$L\geq Q$&#30340;&#24773;&#20917;&#19979;&#26500;&#36896;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#26063;&#26159;&#36864;&#21270;&#30340;&#12290;&#22312;&#36825;&#37324;&#25552;&#21040;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;DL&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#36890;&#36807;&#23545;&#35757;&#32451;&#36755;&#20837;&#30340;&#36882;&#24402;&#25130;&#26029;&#26144;&#23556;&#30340;&#24212;&#29992;&#26469;&#8220;&#25972;&#29702;&#8221;&#35757;&#32451;&#36755;&#20837;&#65292;&#20197;&#26368;&#23567;&#21270;&#22122;&#22768;&#19982;&#20449;&#21495;&#30340;&#27604;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;$2^Q-1$&#20010;&#19981;&#21516;&#30340;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MESc&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#12290;&#36890;&#36807;&#23558;&#25991;&#20214;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#29616;&#20174;&#38271;&#25991;&#26723;&#20013;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2309.10563</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents. (arXiv:2309.10563v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MESc&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#12290;&#36890;&#36807;&#23558;&#25991;&#20214;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#29616;&#20174;&#38271;&#25991;&#26723;&#20013;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21450;&#20854;&#35299;&#37322;&#24120;&#24120;&#38754;&#20020;&#38271;&#36798;&#25968;&#19975;&#23383;&#30340;&#26696;&#20363;&#25991;&#20214;&#21644;&#38750;&#32479;&#19968;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;&#22312;&#27809;&#26377;&#32467;&#26500;&#26631;&#27880;&#30340;&#25991;&#20214;&#19978;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#35770;&#25991;&#23558;&#36825;&#19968;&#38382;&#39064;&#23450;&#20041;&#20026;&#8220;&#31232;&#32570;&#26631;&#27880;&#27861;&#24459;&#25991;&#20214;&#8221;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;MESc&#65288;&#22522;&#20110;&#22810;&#38454;&#27573;&#32534;&#30721;&#22120;&#30340;&#24102;&#32858;&#31867;&#30340;&#30417;&#30563;&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#26694;&#26550;&#26469;&#25506;&#32034;&#32570;&#20047;&#32467;&#26500;&#20449;&#24687;&#21644;&#38271;&#25991;&#26723;&#30340;&#29305;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25991;&#26723;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#65292;&#20174;&#33258;&#23450;&#20041;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#21518;&#22235;&#20010;&#23618;&#20013;&#25552;&#21462;&#23427;&#20204;&#30340;&#23884;&#20837;&#65292;&#24182;&#35797;&#22270;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#26469;&#36817;&#20284;&#23427;&#20204;&#30340;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#21478;&#19968;&#32452;Transformer&#32534;&#30721;&#22120;&#23618;&#23398;&#20064;&#37096;&#20998;&#20043;&#38388;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic legal judgment prediction and its explanation suffer from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents and extracting their explanation becomes a challenging task, more so on documents with no structural annotation. We define this problem as "scarce annotated legal documents" and explore their lack of structural information and their long lengths with a deep learning-based classification framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We explore the adaptability of LLMs with multi-billion
&lt;/p&gt;</description></item><item><title>FRAMU&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#65292;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#65292;&#25903;&#25345;&#25345;&#32493;&#27169;&#22411;&#28436;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.10283</link><description>&lt;p&gt;
FRAMU: &#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning. (arXiv:2309.10283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10283
&lt;/p&gt;
&lt;p&gt;
FRAMU&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#65292;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#65292;&#25903;&#25345;&#25345;&#32493;&#27169;&#22411;&#28436;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#36890;&#36807;&#20801;&#35768;&#20174;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#20013;&#21024;&#38500;&#31169;&#26377;&#25110;&#26080;&#20851;&#25968;&#25454;&#65292;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20351;&#29992;&#36807;&#26102;&#30340;&#12289;&#31169;&#26377;&#30340;&#21644;&#26080;&#20851;&#30340;&#25968;&#25454;&#20250;&#24341;&#21457;&#19982;&#38544;&#31169;&#21644;&#27169;&#22411;&#25928;&#29575;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#19981;&#20165;&#24433;&#21709;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#36951;&#24536;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#36824;&#20250;&#23545;&#25968;&#25454;&#38544;&#31169;&#36896;&#25104;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#36951;&#24536;&#65288;FRAMU&#65289;&#12290;&#35813;&#26694;&#26550;&#34701;&#21512;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#26159;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#65288;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#65289;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#24615;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;FRAMU&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#12289;&#36951;&#24536;&#36807;&#26102;&#12289;&#31169;&#26377;&#25110;&#26080;&#20851;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#25903;&#25345;&#27169;&#22411;&#25345;&#32493;&#28436;&#36827;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning is an emerging field that addresses data privacy issues by enabling the removal of private or irrelevant data from the Machine Learning process. Challenges related to privacy and model efficiency arise from the use of outdated, private, and irrelevant data. These issues compromise both the accuracy and the computational efficiency of models in both Machine Learning and Unlearning. To mitigate these challenges, we introduce a novel framework, Attention-based Machine Unlearning using Federated Reinforcement Learning (FRAMU). This framework incorporates adaptive learning mechanisms, privacy preservation techniques, and optimization strategies, making it a well-rounded solution for handling various data sources, either single-modality or multi-modality, while maintaining accuracy and privacy. FRAMU's strength lies in its adaptability to fluctuating data landscapes, its ability to unlearn outdated, private, or irrelevant data, and its support for continual model evolution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25506;&#32034;&#20102;&#26367;&#25442;&#26631;&#35782;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#28151;&#28102;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08628</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#25513;&#30721;&#30340;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Recovering from Privacy-Preserving Masking with Large Language Models. (arXiv:2309.08628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25506;&#32034;&#20102;&#26367;&#25442;&#26631;&#35782;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#28151;&#28102;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36866;&#24212;&#23545;&#20110;&#22788;&#29702;&#20195;&#29702;&#35757;&#32451;&#25968;&#25454;&#21644;&#23454;&#38469;&#29992;&#25143;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#36827;&#34892;&#36866;&#24212;&#65292;&#29992;&#25143;&#30340;&#25991;&#26412;&#25968;&#25454;&#36890;&#24120;&#23384;&#20648;&#22312;&#26381;&#21153;&#22120;&#25110;&#26412;&#22320;&#35774;&#22791;&#19978;&#65292;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#39046;&#22495;&#20869;&#30340;&#25968;&#25454;&#36827;&#34892;&#30452;&#25509;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#24341;&#36215;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#23384;&#22312;&#21521;&#23545;&#25163;&#27844;&#38706;&#29992;&#25143;&#20449;&#24687;&#30340;&#39069;&#22806;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#24320;&#22987;&#25506;&#32034;&#20351;&#29992;&#36890;&#29992;&#26631;&#35760;&#26367;&#25442;&#25991;&#26412;&#20013;&#30340;&#26631;&#35782;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#24314;&#35758;&#26367;&#25442;&#25513;&#30721;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#35780;&#20272;&#20854;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;LLM&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#20197;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28151;&#28102;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model adaptation is crucial to handle the discrepancy between proxy training data and actual users data received. To effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (NLP) models can be directly trained using such in-domain data. However, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. Replacing identifying information in textual data with a generic marker has been recently explored. In this work, we leverage large language models (LLMs) to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. Specifically, we propose multiple pre-trained and fine-tuned LLM-based approaches and perform empirical studies on various datasets for the comparison of these methods. Experimental results show that models trained on the obfuscation corpora are able to achieve compar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;ADAM&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#20855;&#26377;&#24658;&#23450;&#27493;&#38271;&#30340;&#25910;&#25947;&#24615;&#65292;&#32473;&#20986;&#20102;&#27493;&#38271;&#36798;&#21040;&#20960;&#20046;&#32943;&#23450;&#28176;&#36817;&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;ADAM&#22312;&#22788;&#29702;&#24179;&#28369;&#38750;&#20984;&#20989;&#25968;&#26102;&#36798;&#21040;&#36817;&#20284;&#20020;&#30028;&#24615;&#25152;&#38656;&#30340;&#36816;&#34892;&#26102;&#38388;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.08339</link><description>&lt;p&gt;
ADAM&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#20855;&#26377;&#24658;&#23450;&#27493;&#38271;&#30340;&#25910;&#25947;&#24615;&#65306;&#19968;&#20010;&#31616;&#21333;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Convergence of ADAM with Constant Step Size in Non-Convex Settings: A Simple Proof. (arXiv:2309.08339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;ADAM&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#20855;&#26377;&#24658;&#23450;&#27493;&#38271;&#30340;&#25910;&#25947;&#24615;&#65292;&#32473;&#20986;&#20102;&#27493;&#38271;&#36798;&#21040;&#20960;&#20046;&#32943;&#23450;&#28176;&#36817;&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;ADAM&#22312;&#22788;&#29702;&#24179;&#28369;&#38750;&#20984;&#20989;&#25968;&#26102;&#36798;&#21040;&#36817;&#20284;&#20020;&#30028;&#24615;&#25152;&#38656;&#30340;&#36816;&#34892;&#26102;&#38388;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#65292;RMSProp&#21644;ADAM&#20173;&#28982;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#23427;&#20204;&#30340;&#24615;&#33021;&#20851;&#38190;&#20043;&#19968;&#22312;&#20110;&#36873;&#25321;&#36866;&#24403;&#30340;&#27493;&#38271;&#65292;&#36825;&#20250;&#26174;&#33879;&#24433;&#21709;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#22240;&#36873;&#25321;&#30340;&#27493;&#38271;&#32780;&#21464;&#21270;&#24456;&#22823;&#12290;&#27492;&#22806;&#65292;&#20851;&#20110;&#23427;&#20204;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#35805;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#23545;ADAM&#30340;&#24658;&#23450;&#27493;&#38271;&#29256;&#26412;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27493;&#38271;&#36798;&#21040;&#20960;&#20046;&#32943;&#23450;&#28176;&#36817;&#25910;&#25947;&#21040;&#38646;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#32780;&#21482;&#38656;&#26368;&#23567;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#30830;&#23450;&#24615;ADAM&#22312;&#22788;&#29702;&#24179;&#28369;&#38750;&#20984;&#20989;&#25968;&#26102;&#36798;&#21040;&#36817;&#20284;&#20020;&#30028;&#24615;&#25152;&#38656;&#30340;&#36816;&#34892;&#26102;&#38388;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In neural network training, RMSProp and ADAM remain widely favoured optimization algorithms. One of the keys to their performance lies in selecting the correct step size, which can significantly influence their effectiveness. It is worth noting that these algorithms performance can vary considerably, depending on the chosen step sizes. Additionally, questions about their theoretical convergence properties continue to be a subject of interest. In this paper, we theoretically analyze a constant stepsize version of ADAM in the non-convex setting. We show sufficient conditions for the stepsize to achieve almost sure asymptotic convergence of the gradients to zero with minimal assumptions. We also provide runtime bounds for deterministic ADAM to reach approximate criticality when working with smooth, non-convex functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#25345;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#30340;&#32467;&#26500;&#20445;&#25345;&#21464;&#21387;&#22120;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07579</link><description>&lt;p&gt;
&#20445;&#25345;&#32467;&#26500;&#30340;&#21464;&#21387;&#22120;&#29992;&#20110;&#24207;&#21015;&#30340;SPD&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Structure-Preserving Transformers for Sequences of SPD Matrices. (arXiv:2309.07579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#25345;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#30340;&#32467;&#26500;&#20445;&#25345;&#21464;&#21387;&#22120;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25968;&#25454;&#31867;&#22411;&#30340;&#20998;&#26512;&#65292;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#31561;&#65292;&#21253;&#25324;&#38750;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36825;&#26679;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20998;&#31867;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#65292;&#24182;&#22312;&#25972;&#20010;&#20998;&#26512;&#36807;&#31243;&#20013;&#20445;&#25345;&#23427;&#20204;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26469;&#33258;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#33041;&#30005;&#22270;&#21327;&#26041;&#24046;&#30697;&#38453;&#24207;&#21015;&#30340;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Transformer-based auto-attention mechanisms have been successfully applied to the analysis of a variety of context-reliant data types, from texts to images and beyond, including data from non-Euclidean geometries. In this paper, we present such a mechanism, designed to classify sequences of Symmetric Positive Definite matrices while preserving their Riemannian geometry throughout the analysis. We apply our method to automatic sleep staging on timeseries of EEG-derived covariance matrices from a standard dataset, obtaining high levels of stage-wise performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#38480;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#21442;&#25968;&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#30340;&#36845;&#20195;&#37096;&#20998;&#24418;&#24335;&#26469;&#35745;&#31639;&#33539;&#25968;&#65292;&#20351;&#20854;&#20855;&#26377;&#26377;&#38480;&#33539;&#22260;&#12290;&#24212;&#29992;&#20110;&#19981;&#21516;&#23618;&#30340;&#23454;&#39564;&#34920;&#26126;&#20854;&#24615;&#33021;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.06577</link><description>&lt;p&gt;
&#39640;&#25928;&#26377;&#38480;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Finite Initialization for Tensorized Neural Networks. (arXiv:2309.06577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06577
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#38480;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#21442;&#25968;&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#30340;&#36845;&#20195;&#37096;&#20998;&#24418;&#24335;&#26469;&#35745;&#31639;&#33539;&#25968;&#65292;&#20351;&#20854;&#20855;&#26377;&#26377;&#38480;&#33539;&#22260;&#12290;&#24212;&#29992;&#20110;&#19981;&#21516;&#23618;&#30340;&#23454;&#39564;&#34920;&#26126;&#20854;&#24615;&#33021;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#65292;&#20197;&#36991;&#20813;&#21442;&#25968;&#29190;&#28856;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#33410;&#28857;&#30340;&#23618;&#65292;&#20854;&#20013;&#25152;&#26377;&#25110;&#22823;&#22810;&#25968;&#33410;&#28857;&#19982;&#36755;&#20837;&#25110;&#36755;&#20986;&#26377;&#36830;&#25509;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20351;&#29992;&#35813;&#23618;&#30340;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#30340;&#36845;&#20195;&#37096;&#20998;&#24418;&#24335;&#65292;&#20351;&#20854;&#20855;&#26377;&#26377;&#38480;&#30340;&#33539;&#22260;&#12290;&#36825;&#20010;&#33539;&#25968;&#30340;&#35745;&#31639;&#26159;&#39640;&#25928;&#30340;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#24773;&#20917;&#37117;&#21487;&#20197;&#23436;&#20840;&#25110;&#37096;&#20998;&#35745;&#31639;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#23618;&#65292;&#24182;&#26816;&#26597;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;Python&#20989;&#25968;&#65292;&#22312;i3BQuantum&#23384;&#20648;&#24211;&#30340;Jupyter Notebook&#20013;&#21487;&#20197;&#36816;&#34892;&#23427;&#65306;https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb
&lt;/p&gt;
&lt;p&gt;
We present a novel method for initializing layers of tensorized neural networks in a way that avoids the explosion of the parameters of the matrix it emulates. The method is intended for layers with a high number of nodes in which there is a connection to the input or output of all or most of the nodes. The core of this method is the use of the Frobenius norm of this layer in an iterative partial form, so that it has to be finite and within a certain range. This norm is efficient to compute, fully or partially for most cases of interest. We apply the method to different layers and check its performance. We create a Python function to run it on an arbitrary layer, available in a Jupyter Notebook in the i3BQuantum repository: https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05950</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;VLMs &#30340;&#24494;&#35843;&#26041;&#27861;&#20027;&#35201;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#25805;&#20316;&#65292;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810; VLMs &#20381;&#36182;&#20110;&#19987;&#26377;&#25968;&#25454;&#19988;&#19981;&#24320;&#28304;&#65292;&#38480;&#21046;&#20102;&#20351;&#29992;&#30333;&#30418;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#12290;&#37492;&#20110;&#20687; ChatGPT &#36825;&#26679;&#30340;&#21463;&#27426;&#36814;&#31169;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20173;&#28982;&#25552;&#20379;&#22522;&#20110;&#35821;&#35328;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340; VLMs &#24494;&#35843;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#29305;&#24449;&#23884;&#20837;&#25110;&#36755;&#20986; logits &#30340;&#38656;&#35201;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#32842;&#22825;&#30340; LLMs &#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#20197;&#22312;&#20351;&#29992; CLIP &#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#31034;&#20363;&#20219;&#21153;&#20013;&#23547;&#25214;&#26368;&#20339;&#25991;&#26412;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#21160;"&#29228;&#23665;"&#31243;&#24207;&#65292;&#23427;&#33021;&#25910;&#25947;&#21040;&#26377;&#25928;&#30340;&#25552;&#31034;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prom
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35838;&#31243;&#23398;&#20064;&#24341;&#20837;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#20854;&#29983;&#29289;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04737</link><description>&lt;p&gt;
Spiking Neural Network&#32852;&#21512;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Training of Spiking Neural Network joint Curriculum Learning Strategy. (arXiv:2309.04737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04737
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35838;&#31243;&#23398;&#20064;&#24341;&#20837;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#20854;&#29983;&#29289;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#65292;&#36880;&#28176;&#24341;&#20837;&#38590;&#24230;&#30340;&#27010;&#24565;&#26159;&#20154;&#31867;&#23398;&#20064;&#30340;&#33258;&#28982;&#36807;&#31243;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#20449;&#24687;&#22788;&#29702;&#30340;&#26041;&#24335;&#65292;&#20294;&#30446;&#21069;&#30340;SNNs&#27169;&#22411;&#23558;&#25152;&#26377;&#26679;&#26412;&#35270;&#20026;&#24179;&#31561;&#65292;&#36825;&#19982;&#20154;&#31867;&#23398;&#20064;&#30340;&#21407;&#21017;&#19981;&#31526;&#65292;&#24182;&#24573;&#35270;&#20102;SNNs&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#35838;&#31243;&#23398;&#20064;&#65288;CL&#65289;&#24341;&#20837;SNNs&#30340;CL-SNN&#27169;&#22411;&#65292;&#20351;SNNs&#26356;&#20687;&#20154;&#31867;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#30340;&#29983;&#29289;&#35299;&#37322;&#24615;&#12290;CL&#26159;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#25552;&#20513;&#22312;&#36880;&#28176;&#24341;&#20837;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#20043;&#21069;&#21521;&#27169;&#22411;&#23637;&#31034;&#26356;&#23481;&#26131;&#30340;&#25968;&#25454;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#20449;&#24515;&#24863;&#30693;&#30340;&#25439;&#22833;&#26469;&#34913;&#37327;&#21644;&#22788;&#29702;&#19981;&#21516;&#38590;&#24230;&#27700;&#24179;&#30340;&#26679;&#26412;&#12290;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#26679;&#26412;&#30340;&#32622;&#20449;&#24230;&#65292;&#27169;&#22411;&#33258;&#21160;&#20943;&#23569;&#20102;&#38590;&#26679;&#26412;&#23545;&#21442;&#25968;&#20248;&#21270;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Starting with small and simple concepts, and gradually introducing complex and difficult concepts is the natural process of human learning. Spiking Neural Networks (SNNs) aim to mimic the way humans process information, but current SNNs models treat all samples equally, which does not align with the principles of human learning and overlooks the biological plausibility of SNNs. To address this, we propose a CL-SNN model that introduces Curriculum Learning(CL) into SNNs, making SNNs learn more like humans and providing higher biological interpretability. CL is a training strategy that advocates presenting easier data to models before gradually introducing more challenging data, mimicking the human learning process. We use a confidence-aware loss to measure and process the samples with different difficulty levels. By learning the confidence of different samples, the model reduces the contribution of difficult samples to parameter optimization automatically. We conducted experiments on st
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22522;&#22240;&#34920;&#36798;&#20998;&#26512;&#23454;&#29616;&#23545;&#24739;&#32773;&#22810;&#22218;&#32958;&#30149;&#65288;PKD&#65289;&#30340;&#20934;&#30830;&#21644;&#26089;&#26399;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.03033</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#22218;&#32958;&#30149;&#20013;&#30340;&#24212;&#29992;: &#36890;&#36807;&#22522;&#22240;&#34920;&#36798;&#20998;&#26512;&#23454;&#29616;&#23545;&#24739;&#32773;&#30340;&#20934;&#30830;&#21644;&#26089;&#26399;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Polycystic Kidney Disease: Utilizing Neural Networks for Accurate and Early Detection through Gene Expression Analysis. (arXiv:2309.03033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22522;&#22240;&#34920;&#36798;&#20998;&#26512;&#23454;&#29616;&#23545;&#24739;&#32773;&#22810;&#22218;&#32958;&#30149;&#65288;PKD&#65289;&#30340;&#20934;&#30830;&#21644;&#26089;&#26399;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#22218;&#32958;&#30149;&#65288;PKD&#65289;&#21487;&#33021;&#23548;&#33268;&#24739;&#32773;&#32958;&#33039;&#20013;&#22218;&#32959;&#30340;&#24418;&#25104;&#65292;&#36827;&#32780;&#23548;&#33268;&#33268;&#21629;&#30340;&#24182;&#21457;&#30151;&#65292;&#22240;&#27492;&#26089;&#26399;&#26816;&#27979;PKD&#23545;&#20110;&#26377;&#25928;&#31649;&#29702;&#35813;&#30149;&#24773;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35786;&#26029;&#20013;&#28041;&#21450;&#30340;&#21508;&#31181;&#24739;&#32773;&#29305;&#23450;&#22240;&#32032;&#20351;&#20854;&#23545;&#20020;&#24202;&#21307;&#29983;&#26469;&#35828;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38590;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#26089;&#26399;&#30142;&#30149;&#26816;&#27979;&#12290;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#22522;&#22240;&#34920;&#36798;&#65292;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23545;&#24739;&#32773;&#21487;&#33021;&#30340;PKD&#36827;&#34892;&#20934;&#30830;&#19988;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
With Polycystic Kidney Disease (PKD) potentially leading to fatal complications in patients due to the formation of cysts in the kidneys, early detection of PKD is crucial for effective management of the condition. However, the various patient-specific factors that play a role in the diagnosis make it an intricate puzzle for clinicians to solve. Therefore, in this study, we aim to utilize a deep learning-based approach for early disease detection. The devised neural network can achieve accurate and robust predictions for possible PKD in patients by analyzing patient gene expressions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#21270;&#19988;&#21487;&#20256;&#36882;&#30340;&#35774;&#35745;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#20048;&#39640;&#25805;&#32437;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#31934;&#30830;&#24615;&#35201;&#27714;&#12290;&#36890;&#36807;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#21644;&#36827;&#21270;&#31574;&#30053;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20048;&#39640;&#25805;&#32437;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#35774;&#35745;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02354</link><description>&lt;p&gt;
&#19968;&#31181;&#36731;&#37327;&#21270;&#19988;&#21487;&#20256;&#36882;&#30340;&#35774;&#35745;&#29992;&#20110;&#31283;&#20581;&#30340;&#20048;&#39640;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
A Lightweight and Transferable Design for Robust LEGO Manipulation. (arXiv:2309.02354v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#21270;&#19988;&#21487;&#20256;&#36882;&#30340;&#35774;&#35745;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#20048;&#39640;&#25805;&#32437;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#31934;&#30830;&#24615;&#35201;&#27714;&#12290;&#36890;&#36807;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#21644;&#36827;&#21270;&#31574;&#30053;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20048;&#39640;&#25805;&#32437;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#35774;&#35745;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20048;&#39640;&#26159;&#19968;&#20010;&#29992;&#20110;&#21407;&#22411;&#21270;&#20687;&#32032;&#21270;&#23545;&#35937;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#20048;&#39640;&#21407;&#22411;&#21270;&#65288;&#21363;&#25805;&#32437;&#20048;&#39640;&#31215;&#26408;&#65289;&#30001;&#20110;&#32039;&#23494;&#30340;&#36830;&#25509;&#21644;&#31934;&#30830;&#24615;&#35201;&#27714;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23433;&#20840;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#20048;&#39640;&#25805;&#32437;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#36890;&#36807;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#20943;&#23569;&#20102;&#25805;&#32437;&#30340;&#22797;&#26434;&#24615;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#26411;&#31471;&#24037;&#20855;&#65288;EOAT&#65289;&#65292;&#23427;&#20943;&#23569;&#20102;&#38382;&#39064;&#32500;&#24230;&#65292;&#20351;&#22823;&#22411;&#24037;&#19994;&#26426;&#22120;&#20154;&#33021;&#22815;&#36731;&#26494;&#25805;&#32437;&#20048;&#39640;&#31215;&#26408;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20351;&#29992;&#36827;&#21270;&#31574;&#30053;&#23433;&#20840;&#22320;&#20248;&#21270;&#26426;&#22120;&#20154;&#36816;&#21160;&#65292;&#29992;&#20110;&#20048;&#39640;&#25805;&#32437;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EOAT&#22312;&#25805;&#32437;&#20048;&#39640;&#31215;&#26408;&#26041;&#38754;&#34920;&#29616;&#21487;&#38752;&#65292;&#32780;&#23398;&#20064;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#19988;&#23433;&#20840;&#22320;&#23558;&#25805;&#32437;&#24615;&#33021;&#25552;&#39640;&#21040;100%&#30340;&#25104;&#21151;&#29575;&#12290;&#25152;&#35774;&#35745;&#30340;&#21327;&#21516;&#35774;&#35745;&#24050;&#32463;&#37096;&#32626;&#21040;&#22810;&#21488;&#26426;&#22120;&#20154;&#65288;FANUC LR-mate 200id/7L&#21644;Yaskawa GP4&#65289;&#19978;&#65292;&#20197;&#23637;&#31034;&#20854;&#26222;&#36866;&#24615;&#21644;&#21487;&#20256;&#36882;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26412;&#30740;&#31350;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
LEGO is a well-known platform for prototyping pixelized objects. However, robotic LEGO prototyping (i.e. manipulating LEGO bricks) is challenging due to the tight connections and accuracy requirement. This paper investigates safe and efficient robotic LEGO manipulation. In particular, this paper reduces the complexity of the manipulation by hardware-software co-design. An end-of-arm tool (EOAT) is designed, which reduces the problem dimension and allows large industrial robots to easily manipulate LEGO bricks. In addition, this paper uses evolution strategy to safely optimize the robot motion for LEGO manipulation. Experiments demonstrate that the EOAT performs reliably in manipulating LEGO bricks and the learning framework can effectively and safely improve the manipulation performance to a 100% success rate. The co-design is deployed to multiple robots (i.e. FANUC LR-mate 200id/7L and Yaskawa GP4) to demonstrate its generalizability and transferability. In the end, we show that the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.01219</link><description>&lt;p&gt;
AI&#28023;&#27915;&#20013;&#30340;&#22934;&#24618;&#20043;&#27468;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. (arXiv:2309.01219v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#20204;&#23545;&#20854;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#34920;&#31034;&#25285;&#24551;&#65306;LLMs&#26377;&#26102;&#20250;&#29983;&#25104;&#19982;&#29992;&#25143;&#36755;&#20837;&#19981;&#31526;&#12289;&#19982;&#20808;&#21069;&#29983;&#25104;&#30340;&#20869;&#23481;&#30456;&#30683;&#30462;&#25110;&#19982;&#24050;&#24314;&#31435;&#30340;&#19990;&#30028;&#30693;&#35782;&#19981;&#31526;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#29616;&#35937;&#23545;LLMs&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#20851;&#20110;&#24187;&#35273;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;LLMs&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#26088;&#22312;&#32531;&#35299;LLM&#24187;&#35273;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.
&lt;/p&gt;</description></item><item><title>TML&#36719;&#20214;&#21253;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#19968;&#22871;&#20840;&#38754;&#24037;&#20855;&#21644;&#26041;&#27861;&#30340;R&#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#22788;&#29702;&#19982;&#28909;&#24102;&#20984;&#24615;&#30456;&#20851;&#30340;&#22522;&#26412;&#35745;&#31639;&#21644;&#21487;&#35270;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;&#28909;&#24102;&#24230;&#37327;&#36827;&#34892;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2309.01082</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28909;&#24102;&#20960;&#20309;&#24037;&#20855;&#65306;TML&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
Tropical Geometric Tools for Machine Learning: the TML package. (arXiv:2309.01082v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01082
&lt;/p&gt;
&lt;p&gt;
TML&#36719;&#20214;&#21253;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#19968;&#22871;&#20840;&#38754;&#24037;&#20855;&#21644;&#26041;&#27861;&#30340;R&#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#22788;&#29702;&#19982;&#28909;&#24102;&#20984;&#24615;&#30456;&#20851;&#30340;&#22522;&#26412;&#35745;&#31639;&#21644;&#21487;&#35270;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;&#28909;&#24102;&#24230;&#37327;&#36827;&#34892;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28909;&#24102;&#20960;&#20309;&#23398;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35768;&#22810;&#30452;&#25509;&#24212;&#29992;&#20110;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#30340;&#24037;&#20855;&#12290;TML&#36719;&#20214;&#21253;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#19968;&#22871;&#20840;&#38754;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#30340;R&#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#22788;&#29702;&#19982;&#28909;&#24102;&#20984;&#24615;&#30456;&#20851;&#30340;&#22522;&#26412;&#35745;&#31639;&#12289;&#28909;&#24102;&#20984;&#38598;&#30340;&#21487;&#35270;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;&#28909;&#24102;&#24230;&#37327;&#21644;&#28909;&#24102;&#25237;&#24433;&#29615;&#19978;&#30340;max-plus&#20195;&#25968;&#36827;&#34892;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#20027;&#35201;&#30340;&#65292;TML&#36719;&#20214;&#21253;&#20351;&#29992;Hit and Run Markov chain Monte Carlo&#37319;&#26679;&#22120;&#19982;&#28909;&#24102;&#24230;&#37327;&#20316;&#20026;&#32479;&#35745;&#25512;&#26029;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#38500;&#20102;&#22522;&#26412;&#35745;&#31639;&#21644;&#28909;&#24102;HAR&#37319;&#26679;&#22120;&#30340;&#21508;&#31181;&#24212;&#29992;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20851;&#27880;TML&#36719;&#20214;&#21253;&#20013;&#21253;&#21547;&#30340;&#20960;&#31181;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21253;&#25324;&#28909;&#24102;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#28909;&#24102;&#36923;&#36753;&#22238;&#24402;&#21644;&#28909;&#24102;&#26680;&#23494;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, developments in tropical geometry have provided a number of uses directly applicable to problems in statistical learning. The TML package is the first R package which contains a comprehensive set of tools and methods used for basic computations related to tropical convexity, visualization of tropically convex sets, as well as supervised and unsupervised learning models using the tropical metric under the max-plus algebra over the tropical projective torus. Primarily, the TML package employs a Hit and Run Markov chain Monte Carlo sampler in conjunction with the tropical metric as its main tool for statistical inference. In addition to basic computation and various applications of the tropical HAR sampler, we also focus on several supervised and unsupervised methods incorporated in the TML package including tropical principal component analysis, tropical logistic regression and tropical kernel density estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#30340;&#35282;&#24230;&#21019;&#36896;&#20102;&#26032;&#30340;&#23384;&#22312;&#24615;&#39063;&#31890;&#27010;&#24565;&#65292;&#24182;&#21051;&#30011;&#20102;&#20854;&#29305;&#24449;&#12290;&#36825;&#20123;&#39063;&#31890;&#39318;&#20808;&#30830;&#23450;&#33258;&#24049;&#65292;&#28982;&#21518;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#39063;&#31890;&#35745;&#31639;&#29702;&#35770;&#26694;&#26550;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#31639;&#27861;&#24320;&#21457;&#12289;&#20998;&#31867;&#38382;&#39064;&#24212;&#29992;&#21644;&#26041;&#27861;&#25512;&#24191;&#30340;&#25968;&#23398;&#22522;&#30784;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.16157</link><description>&lt;p&gt;
&#23384;&#22312;&#24615;&#39063;&#31890;&#30340;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Algebraic, Topological, and Mereological Foundations of Existential Granules. (arXiv:2308.16157v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#30340;&#35282;&#24230;&#21019;&#36896;&#20102;&#26032;&#30340;&#23384;&#22312;&#24615;&#39063;&#31890;&#27010;&#24565;&#65292;&#24182;&#21051;&#30011;&#20102;&#20854;&#29305;&#24449;&#12290;&#36825;&#20123;&#39063;&#31890;&#39318;&#20808;&#30830;&#23450;&#33258;&#24049;&#65292;&#28982;&#21518;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#39063;&#31890;&#35745;&#31639;&#29702;&#35770;&#26694;&#26550;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#31639;&#27861;&#24320;&#21457;&#12289;&#20998;&#31867;&#38382;&#39064;&#24212;&#29992;&#21644;&#26041;&#27861;&#25512;&#24191;&#30340;&#25968;&#23398;&#22522;&#30784;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#21457;&#26126;&#20102;&#30830;&#23450;&#33258;&#24049;&#30340;&#23384;&#22312;&#24615;&#39063;&#31890;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#20174;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#30340;&#35282;&#24230;&#23545;&#20854;&#36827;&#34892;&#20102;&#21051;&#30011;&#12290;&#23384;&#22312;&#24615;&#39063;&#31890;&#26159;&#37027;&#20123;&#26368;&#21021;&#30830;&#23450;&#33258;&#24049;&#65292;&#24182;&#38543;&#21518;&#19982;&#20854;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#39063;&#31890;&#12290;&#36825;&#20010;&#27010;&#24565;&#30340;&#31034;&#20363;&#65292;&#27604;&#22914;&#39063;&#31890;&#29699;&#65292;&#22312;&#20043;&#21069;&#20854;&#20182;&#20154;&#30340;&#20316;&#21697;&#20013;&#34429;&#28982;&#23450;&#20041;&#19981;&#23436;&#22791;&#12289;&#31639;&#27861;&#24314;&#31435;&#19981;&#20805;&#20998;&#12289;&#29702;&#35770;&#21270;&#19981;&#36275;&#65292;&#20294;&#24050;&#32463;&#22312;&#31895;&#31961;&#38598;&#21644;&#36719;&#35745;&#31639;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#36866;&#21512;&#20110;&#39063;&#31890;&#35745;&#31639;&#30340;&#22810;&#20010;&#29702;&#35770;&#26694;&#26550;&#65288;&#20844;&#29702;&#21270;&#12289;&#36866;&#24212;&#24615;&#31561;&#65289;&#12290;&#36825;&#31181;&#21051;&#30011;&#26088;&#22312;&#29992;&#20110;&#31639;&#27861;&#24320;&#21457;&#12289;&#20998;&#31867;&#38382;&#39064;&#30340;&#24212;&#29992;&#20197;&#21450;&#21487;&#33021;&#30340;&#26041;&#27861;&#25512;&#24191;&#30340;&#25968;&#23398;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#35768;&#22810;&#24320;&#25918;&#38382;&#39064;&#24182;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, new concepts of existential granules that determine themselves are invented, and are characterized from algebraic, topological, and mereological perspectives. Existential granules are those that determine themselves initially, and interact with their environment subsequently. Examples of the concept, such as those of granular balls, though inadequately defined, algorithmically established, and insufficiently theorized in earlier works by others, are already used in applications of rough sets and soft computing. It is shown that they fit into multiple theoretical frameworks (axiomatic, adaptive, and others) of granular computing. The characterization is intended for algorithm development, application to classification problems and possible mathematical foundations of generalizations of the approach. Additionally, many open problems are posed and directions provided.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#25104;&#20026;&#20154;&#24037;&#35780;&#20272;&#21644;&#20854;&#20182;&#33258;&#21160;&#21270;&#35780;&#20215;&#25351;&#26631;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.13577</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Text Style Transfer Evaluation Using Large Language Models. (arXiv:2308.13577v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13577
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#25104;&#20026;&#20154;&#24037;&#35780;&#20272;&#21644;&#20854;&#20182;&#33258;&#21160;&#21270;&#35780;&#20215;&#25351;&#26631;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#65288;TST&#65289;&#30340;&#35780;&#20272;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#34920;&#29616;&#22312;&#22810;&#20010;&#26041;&#38754;&#65292;&#27599;&#20010;&#26041;&#38754;&#37117;&#24456;&#38590;&#21333;&#29420;&#34913;&#37327;&#65306;&#39118;&#26684;&#36716;&#25442;&#20934;&#30830;&#24615;&#12289;&#20869;&#23481;&#20445;&#30041;&#21644;&#25972;&#20307;&#27969;&#30021;&#24615;&#12290;&#20154;&#24037;&#35780;&#20272;&#26159;TST&#35780;&#20272;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#28982;&#32780;&#65292;&#23427;&#36153;&#26102;&#36153;&#21147;&#65292;&#24182;&#19988;&#32467;&#26524;&#38590;&#20197;&#37325;&#22797;&#12290;&#35768;&#22810;&#33258;&#21160;&#21270;&#25351;&#26631;&#34987;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20316;&#20026;&#20154;&#24037;&#35780;&#20272;&#30340;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#33258;&#21160;&#21270;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20272;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#23545;&#23427;&#20204;&#20316;&#20026;&#21487;&#38752;&#22522;&#20934;&#30340;&#25928;&#26524;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#19981;&#20165;&#33021;&#22815;&#21305;&#37197;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#26410;&#35265;&#20219;&#21153;&#20013;&#36824;&#33021;&#36229;&#36807;&#24179;&#22343;&#20154;&#31867;&#34920;&#29616;&#12290;&#36825;&#34920;&#26126;LLMs&#26377;&#28508;&#21147;&#25104;&#20026;&#20154;&#24037;&#35780;&#20272;&#21644;&#20854;&#20182;&#33258;&#21160;&#21270;&#25351;&#26631;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Text Style Transfer (TST) is challenging to evaluate because the quality of the generated text manifests itself in multiple aspects, each of which is hard to measure individually: style transfer accuracy, content preservation, and overall fluency of the text. Human evaluation is the gold standard in TST evaluation; however, it is expensive, and the results are difficult to reproduce. Numerous automated metrics are employed to assess performance in these aspects, serving as substitutes for human evaluation. However, the correlation between many of these automated metrics and human evaluations remains unclear, raising doubts about their effectiveness as reliable benchmarks. Recent advancements in Large Language Models (LLMs) have demonstrated their ability to not only match but also surpass the average human performance across a wide range of unseen tasks. This suggests that LLMs have the potential to serve as a viable alternative to human evaluation and other automated metrics. We asses
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36229;&#25195;&#25551;&#25216;&#26415;&#65292;&#24341;&#20837;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25506;&#31350;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#24341;&#21457;&#30340;&#24773;&#32490;&#20256;&#26579;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#24773;&#32490;&#20256;&#26579;&#19982;&#35748;&#30693;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.13546</link><description>&lt;p&gt;
Hyperscanning EEG&#30340;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#25581;&#31034;&#20102;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#24341;&#21457;&#30340;&#24773;&#32490;&#20256;&#26579;
&lt;/p&gt;
&lt;p&gt;
Functional Graph Contrastive Learning of Hyperscanning EEG Reveals Emotional Contagion Evoked by Stereotype-Based Stressors. (arXiv:2308.13546v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36229;&#25195;&#25551;&#25216;&#26415;&#65292;&#24341;&#20837;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25506;&#31350;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#24341;&#21457;&#30340;&#24773;&#32490;&#20256;&#26579;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#24773;&#32490;&#20256;&#26579;&#19982;&#35748;&#30693;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#24773;&#32490;&#20256;&#26579;&#30340;&#32454;&#24494;&#24046;&#24322;&#21450;&#20854;&#23545;&#21452;&#20154;&#20114;&#21160;&#20013;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#32858;&#28966;&#20110;&#22899;&#24615;&#23545;&#30340;&#21512;&#20316;&#35299;&#20915;&#38382;&#39064;&#20219;&#21153;&#20013;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#32972;&#26223;&#12290;&#36890;&#36807;&#23545;&#24773;&#32490;&#20256;&#26579;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#25581;&#31034;&#20854;&#28508;&#22312;&#26426;&#21046;&#21644;&#24433;&#21709;&#12290;&#21033;&#29992;&#22522;&#20110;EEG&#30340;&#36229;&#25195;&#25551;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;fGCL&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#25552;&#21462;&#20027;&#20307;&#19981;&#21464;&#30340;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#34920;&#31034;&#12290;&#36825;&#20123;&#34920;&#31034;&#36827;&#19968;&#27493;&#24212;&#29992;&#21160;&#24577;&#22270;&#20998;&#31867;&#65288;DGC&#65289;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#26088;&#22312;&#21078;&#26512;&#24773;&#32490;&#20256;&#26579;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#23545;&#33041;&#37096;&#21516;&#27493;&#21644;&#36830;&#25509;&#24615;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#24773;&#32490;&#20256;&#26579;&#19982;&#35748;&#30693;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#32467;&#26524;&#24378;&#35843;&#24773;&#32490;&#20256;&#26579;&#22312;&#22609;&#36896;&#36712;&#36857;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study delves into the intricacies of emotional contagion and its impact on performance within dyadic interactions. Specifically, it focuses on the context of stereotype-based stress (SBS) during collaborative problem-solving tasks among female pairs. Through an exploration of emotional contagion, the research seeks to unveil its underlying mechanisms and effects. Leveraging EEG-based hyperscanning technology, the study introduces an innovative approach known as functional Graph Contrastive Learning (fGCL), which extracts subject-invariant representations of neural activity patterns. These representations are further subjected to analysis using the Dynamic Graph Classification (DGC) model, aimed at dissecting the process of emotional contagion. By scrutinizing brain synchronization and connectivity, the study reveals the intricate interplay between emotional contagion and cognitive functioning. The results underscore the substantial role of emotional contagion in shaping the trajec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#22330;&#26223;&#65292;&#21363;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.11551</link><description>&lt;p&gt;
&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-event Video-Text Retrieval. (arXiv:2308.11551v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#22330;&#26223;&#65292;&#21363;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;VTR&#65289;&#26159;&#20114;&#32852;&#32593;&#19978;&#28023;&#37327;&#35270;&#39057;&#25991;&#26412;&#25968;&#25454;&#26102;&#20195;&#20013;&#19968;&#39033;&#20851;&#38190;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#20351;&#29992;&#21452;&#27969;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#23398;&#20064;&#35270;&#39057;&#25991;&#26412;&#23545;&#30340;&#32852;&#21512;&#34920;&#31034;&#25104;&#20026;VTR&#20219;&#21153;&#20013;&#19968;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20551;&#35774;&#35270;&#39057;&#25991;&#26412;&#23545;&#24212;&#26159;&#21452;&#23556;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#24182;&#24573;&#35270;&#20102;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#35270;&#39057;&#20869;&#23481;&#36890;&#24120;&#28085;&#30422;&#22810;&#20010;&#20107;&#20214;&#65292;&#32780;&#29992;&#25143;&#26597;&#35810;&#25110;&#32593;&#39029;&#20803;&#25968;&#25454;&#31561;&#25991;&#26412;&#24448;&#24448;&#26159;&#20855;&#20307;&#30340;&#65292;&#24182;&#23545;&#24212;&#21333;&#20010;&#20107;&#20214;&#12290;&#36825;&#36896;&#25104;&#20102;&#20043;&#21069;&#30340;&#35757;&#32451;&#30446;&#26631;&#19982;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#33021;&#23548;&#33268;&#26089;&#26399;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#38024;&#23545;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#22330;&#26223;&#65292;&#20316;&#20026;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#30340;&#19968;&#20010;&#21033;&#22522;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#65292;&#20197;&#22238;&#31572;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22312;&#19977;&#20010;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11358</link><description>&lt;p&gt;
&#34892;&#21160;&#20998;&#21106;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Much Temporal Long-Term Context is Needed for Action Segmentation?. (arXiv:2308.11358v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#65292;&#20197;&#22238;&#31572;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22312;&#19977;&#20010;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#20013;&#24314;&#27169;&#38271;&#26399;&#19978;&#19979;&#25991;&#23545;&#20110;&#35768;&#22810;&#32454;&#31890;&#24230;&#20219;&#21153;&#21253;&#25324;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#34429;&#28982;transformers&#21487;&#20197;&#23545;&#35270;&#39057;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#65292;&#20294;&#23545;&#20110;&#38271;&#35270;&#39057;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#20851;&#20110;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#30340;&#30740;&#31350;&#32467;&#21512;&#20102;&#20351;&#29992;&#23616;&#37096;&#26102;&#38388;&#31383;&#21475;&#35745;&#31639;&#20986;&#30340;&#33258;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#26080;&#27861;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#24182;&#21033;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#26469;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#65292;&#35797;&#22270;&#22238;&#31572;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#25165;&#33021;&#36827;&#34892;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#30446;&#21069;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#36827;&#34892;&#27604;&#36739;&#65292;&#36825;&#19977;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;50Salads&#65292;Brea...
&lt;/p&gt;
&lt;p&gt;
Modeling long-term context in videos is crucial for many fine-grained tasks including temporal action segmentation. An interesting question that is still open is how much long-term temporal context is needed for optimal performance. While transformers can model the long-term context of a video, this becomes computationally prohibitive for long videos. Recent works on temporal action segmentation thus combine temporal convolutional networks with self-attentions that are computed only for a local temporal window. While these approaches show good results, their performance is limited by their inability to capture the full context of a video. In this work, we try to answer how much long-term temporal context is required for temporal action segmentation by introducing a transformer-based model that leverages sparse attention to capture the full context of a video. We compare our model with the current state of the art on three datasets for temporal action segmentation, namely 50Salads, Brea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;CLIP&#20316;&#20026;&#25968;&#25454;&#28304;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#35789;&#35821;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#24335;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#24212;&#29992;prompt-guided&#20998;&#31867;&#36827;&#34892;&#22270;&#20687;&#30340;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20197;&#24448;&#38656;&#20026;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#35757;&#32451;&#27169;&#22411;&#30340;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#28508;&#21147;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.11119</link><description>&lt;p&gt;
&#20351;&#29992;CLIP&#36827;&#34892;&#38543;&#26426;&#35789;&#35821;&#25968;&#25454;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection. (arXiv:2308.11119v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;CLIP&#20316;&#20026;&#25968;&#25454;&#28304;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#35789;&#35821;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#24335;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#24212;&#29992;prompt-guided&#20998;&#31867;&#36827;&#34892;&#22270;&#20687;&#30340;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20197;&#24448;&#38656;&#20026;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#35757;&#32451;&#27169;&#22411;&#30340;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#28508;&#21147;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;CLIP&#20316;&#20026;&#25968;&#25454;&#28304;&#36827;&#34892;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#12290;&#30001;&#20110;&#28508;&#22312;&#30340;&#24037;&#19994;&#24212;&#29992;&#65292;&#20154;&#20204;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#24320;&#21457;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#32771;&#34385;&#21040;&#33719;&#21462;&#21508;&#31181;&#24322;&#24120;&#26679;&#26412;&#20197;&#29992;&#20110;&#35757;&#32451;&#30340;&#22256;&#38590;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#29992;&#27491;&#24120;&#26679;&#26412;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20174;&#27491;&#24120;&#26679;&#26412;&#30340;&#20998;&#24067;&#20013;&#27979;&#37327;&#20854;&#24046;&#24322;&#65292;&#36825;&#38656;&#35201;&#20026;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#20302;&#25928;&#30340;&#35757;&#32451;&#38656;&#27714;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#23427;&#20197;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#24335;&#23545;&#22270;&#20687;&#30340;&#27599;&#20010;&#37096;&#20998;&#24212;&#29992;prompt-guided&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#20173;&#28982;&#21463;&#21040;&#20197;&#24050;&#30693;&#23545;&#35937;&#31867;&#21035;&#20180;&#32454;&#32452;&#21512;&#25552;&#31034;&#30340;&#21171;&#21160;&#21147;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;CLIP&#20316;&#20026;&#35757;&#32451;&#30340;&#25968;&#25454;&#28304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;CLI&#20013;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#25991;&#26412;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel method that leverages a visual-language model, CLIP, as a data source for zero-shot anomaly detection. Tremendous efforts have been put towards developing anomaly detectors due to their potential industrial applications. Considering the difficulty in acquiring various anomalous samples for training, most existing methods train models with only normal samples and measure discrepancies from the distribution of normal samples during inference, which requires training a model for each object category. The problem of this inefficient training requirement has been tackled by designing a CLIP-based anomaly detector that applies prompt-guided classification to each part of an image in a sliding window manner. However, the method still suffers from the labor of careful prompt ensembling with known object categories. To overcome the issues above, we propose leveraging CLIP as a data source for training. Our method generates text embeddings with the text encoder in CLI
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#21512;&#23454;&#35777;&#35780;&#20272;&#20102;&#35299;&#20915;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#26041;&#27861;&#23384;&#22312;&#31283;&#23450;&#24615;&#21644;&#27424;&#25311;&#21512;&#38382;&#39064;&#65292;&#20294;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#35757;&#32451;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2308.10328</link><description>&lt;p&gt;
&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#30340;&#32508;&#21512;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Empirical Evaluation on Online Continual Learning. (arXiv:2308.10328v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10328
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#21512;&#23454;&#35777;&#35780;&#20272;&#20102;&#35299;&#20915;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#26041;&#27861;&#23384;&#22312;&#31283;&#23450;&#24615;&#21644;&#27424;&#25311;&#21512;&#38382;&#39064;&#65292;&#20294;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#35757;&#32451;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#22312;&#25968;&#25454;&#27969;&#19978;&#30452;&#25509;&#23398;&#20064;&#65292;&#22788;&#29702;&#26102;&#38388;&#21464;&#21270;&#30340;&#20998;&#24067;&#65292;&#24182;&#20165;&#23384;&#20648;&#19968;&#23567;&#37096;&#20998;&#25968;&#25454;&#65292;&#20197;&#26356;&#25509;&#36817;&#23454;&#26102;&#23398;&#20064;&#20307;&#39564;&#12290;&#22312;&#36825;&#20010;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25991;&#29486;&#20013;&#35299;&#20915;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#30340;&#31867;&#22686;&#37327;&#35774;&#32622;&#19979;&#65292;&#20174;&#25968;&#25454;&#27969;&#20013;&#36880;&#27493;&#23398;&#20064;&#26032;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#22312;Split-CIFAR100&#21644;Split-TinyImagenet&#22522;&#20934;&#19978;&#27604;&#36739;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#12289;&#36951;&#24536;&#29575;&#12289;&#31283;&#23450;&#24615;&#21644;&#34920;&#31034;&#36136;&#37327;&#65292;&#20197;&#35780;&#20272;&#31639;&#27861;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#23384;&#22312;&#31283;&#23450;&#24615;&#21644;&#27424;&#25311;&#21512;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#30456;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#19979;&#65292;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#35757;&#32451;&#30456;&#24403;&#12290;&#27809;&#26377;&#26126;&#30830;&#30340;&#20248;&#32988;&#32773;&#20174;&#37325;&#26032;&#35780;&#20272;&#21518;&#30340;&#32467;&#26524;&#20013;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online continual learning aims to get closer to a live learning experience by learning directly on a stream of data with temporally shifting distribution and by storing a minimum amount of data from that stream. In this empirical evaluation, we evaluate various methods from the literature that tackle online continual learning. More specifically, we focus on the class-incremental setting in the context of image classification, where the learner must learn new classes incrementally from a stream of data. We compare these methods on the Split-CIFAR100 and Split-TinyImagenet benchmarks, and measure their average accuracy, forgetting, stability, and quality of the representations, to evaluate various aspects of the algorithm at the end but also during the whole training period. We find that most methods suffer from stability and underfitting issues. However, the learned representations are comparable to i.i.d. training under the same computational budget. No clear winner emerges from the re
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;GPFL&#26159;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#22312;&#25928;&#26524;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#38544;&#31169;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#20943;&#36731;&#20102;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#25552;&#21319;&#20102;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.10279</link><description>&lt;p&gt;
GPFL: &#21516;&#26102;&#23398;&#20064;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#29305;&#24449;&#20449;&#24687;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning. (arXiv:2308.10279v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10279
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;GPFL&#26159;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#22312;&#25928;&#26524;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#38544;&#31169;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#20943;&#36731;&#20102;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#25552;&#21319;&#20102;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#20854;&#20445;&#25252;&#38544;&#31169;&#21644;&#21327;&#20316;&#23398;&#20064;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#26368;&#36817;&#65292;&#20010;&#24615;&#21270;FL&#65288;pFL&#65289;&#22240;&#20854;&#33021;&#22815;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#24182;&#22312;FL&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20174;&#29305;&#24449;&#25552;&#21462;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;pFL&#26041;&#27861;&#21482;&#20851;&#27880;&#22312;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#21462;&#20840;&#23616;&#25110;&#20010;&#24615;&#21270;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#36825;&#26080;&#27861;&#28385;&#36275;pFL&#30340;&#21327;&#20316;&#23398;&#20064;&#21644;&#20010;&#24615;&#21270;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;pFL&#26041;&#27861;&#65292;&#21517;&#20026;GPFL&#65292;&#29992;&#20110;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#21516;&#26102;&#23398;&#20064;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#30340;&#29305;&#24449;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#32479;&#35745;&#24322;&#36136;&#30340;&#35774;&#32622;&#19979;&#23545;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;GPFL&#22312;&#25928;&#26524;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#38544;&#31169;&#24615;&#26041;&#38754;&#20248;&#20110;&#21313;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;GPFL&#20943;&#36731;&#20102;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36807;&#20102;&#22522;&#20934;&#32447;&#26368;&#39640;8.99%&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is popular for its privacy-preserving and collaborative learning capabilities. Recently, personalized FL (pFL) has received attention for its ability to address statistical heterogeneity and achieve personalization in FL. However, from the perspective of feature extraction, most existing pFL methods only focus on extracting global or personalized feature information during local training, which fails to meet the collaborative learning and personalization goals of pFL. To address this, we propose a new pFL method, named GPFL, to simultaneously learn global and personalized feature information on each client. We conduct extensive experiments on six datasets in three statistically heterogeneous settings and show the superiority of GPFL over ten state-of-the-art methods regarding effectiveness, scalability, fairness, stability, and privacy. Besides, GPFL mitigates overfitting and outperforms the baselines by up to 8.99% in accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#23556;&#27874;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#27425;&#25195;&#25551;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25193;&#25955;MRI&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#34394;&#25311;&#32447;&#22280;&#30340;&#24212;&#29992;&#65292;&#33021;&#22815;&#20811;&#26381;&#22810;&#27425;&#25195;&#25551;&#20013;&#30340;&#30456;&#20301;&#21464;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.05103</link><description>&lt;p&gt;
&#21033;&#29992;&#38646;&#23556;&#27874;&#33258;&#30417;&#30563;&#23398;&#20064;&#37325;&#24314;&#25913;&#36827;&#30340;&#22810;&#27425;&#25195;&#25551;&#25193;&#25955;&#21152;&#26435;MRI
&lt;/p&gt;
&lt;p&gt;
Improved Multi-Shot Diffusion-Weighted MRI with Zero-Shot Self-Supervised Learning Reconstruction. (arXiv:2308.05103v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#23556;&#27874;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#27425;&#25195;&#25551;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25193;&#25955;MRI&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#34394;&#25311;&#32447;&#22280;&#30340;&#24212;&#29992;&#65292;&#33021;&#22815;&#20811;&#26381;&#22810;&#27425;&#25195;&#25551;&#20013;&#30340;&#30456;&#20301;&#21464;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;MRI&#36890;&#24120;&#20351;&#29992;&#22238;&#27874;&#24179;&#38754;&#25104;&#20687;&#65288;EPI&#65289;&#36827;&#34892;&#65292;&#22240;&#20026;&#20854;&#37319;&#38598;&#26102;&#38388;&#24555;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#21152;&#26435;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#24120;&#24120;&#21463;&#21040;&#30913;&#22330;&#19981;&#22343;&#21248;&#24615;&#30456;&#20851;&#20266;&#24433;&#20197;&#21450;T2&#21644;T2*&#24347;&#35947;&#25928;&#24212;&#24341;&#36215;&#30340;&#27169;&#31946;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#65292;&#24120;&#24120;&#37319;&#29992;&#22810;&#27425;&#25195;&#25551;EPI&#65288;msEPI&#65289;&#19982;&#24182;&#34892;&#25104;&#20687;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#20010;&#25195;&#25551;&#20043;&#38388;&#30340;&#30456;&#20301;&#21464;&#21270;&#65292;&#37325;&#24314;msEPI&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;msEPI&#37325;&#24314;&#26041;&#27861;&#65292;&#31216;&#20026;zero-MIRID&#65288;&#38646;&#23556;&#27874;&#33258;&#30417;&#30563;&#23398;&#20064;&#22810;&#27425;&#25195;&#25551;&#22270;&#20687;&#37325;&#24314;&#25913;&#36827;&#25193;&#25955;MRI&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#32852;&#21512;&#37325;&#24314;msEPI&#25968;&#25454;&#12290;&#35813;&#32593;&#32476;&#22312;k&#31354;&#38388;&#21644;&#22270;&#20687;&#31354;&#38388;&#20013;&#37117;&#20351;&#29992;CNN&#21435;&#22122;&#22120;&#65292;&#24182;&#21033;&#29992;&#34394;&#25311;&#32447;&#22280;&#26469;&#22686;&#24378;&#22270;&#20687;&#37325;&#24314;&#26465;&#20214;&#12290;&#36890;&#36807;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#21644;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Diffusion MRI is commonly performed using echo-planar imaging (EPI) due to its rapid acquisition time. However, the resolution of diffusion-weighted images is often limited by magnetic field inhomogeneity-related artifacts and blurring induced by T2- and T2*-relaxation effects. To address these limitations, multi-shot EPI (msEPI) combined with parallel imaging techniques is frequently employed. Nevertheless, reconstructing msEPI can be challenging due to phase variation between multiple shots. In this study, we introduce a novel msEPI reconstruction approach called zero-MIRID (zero-shot self-supervised learning of Multi-shot Image Reconstruction for Improved Diffusion MRI). This method jointly reconstructs msEPI data by incorporating deep learning-based image regularization techniques. The network incorporates CNN denoisers in both k- and image-spaces, while leveraging virtual coils to enhance image reconstruction conditioning. By employing a self-supervised learning technique and divi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#35265;&#29366;&#24577;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#22522;&#20110;&#20215;&#20540;&#30340;&#25200;&#21160;&#21644;&#36807;&#28388;&#65292;&#23454;&#29616;&#20102;&#23545;&#31163;&#32447;&#25968;&#25454;&#20043;&#22806;&#30340;&#29366;&#24577;&#30340;&#21033;&#29992;&#21644;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.03882</link><description>&lt;p&gt;
&#36890;&#36807;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#22686;&#24378;&#21033;&#29992;&#24191;&#20041;&#21270;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations. (arXiv:2308.03882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#35265;&#29366;&#24577;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#22522;&#20110;&#20215;&#20540;&#30340;&#25200;&#21160;&#21644;&#36807;&#28388;&#65292;&#23454;&#29616;&#20102;&#23545;&#31163;&#32447;&#25968;&#25454;&#20043;&#22806;&#30340;&#29366;&#24577;&#30340;&#21033;&#29992;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23545;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#36827;&#34892;&#20445;&#23432;&#20215;&#20540;&#35780;&#20272;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#26080;&#27169;&#22411;&#26041;&#27861;&#20250;&#23545;&#25152;&#26377;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#36827;&#34892;&#24809;&#32602;&#65292;&#32780;&#26377;&#27169;&#22411;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#36890;&#36807;&#27169;&#22411;&#23637;&#24320;&#23545;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#36827;&#34892;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#22240;&#32032;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25214;&#21040;&#31163;&#32447;&#25968;&#25454;&#20043;&#22806;&#30340;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#26102;&#23384;&#22312;&#22256;&#38590;&#65306;(a)&#30001;&#20110;&#32423;&#32852;&#27169;&#22411;&#35823;&#24046;&#65292;&#27169;&#22411;&#30340;&#23637;&#24320;&#33539;&#22260;&#38750;&#24120;&#30701;&#65292;(b)&#27169;&#22411;&#23637;&#24320;&#20165;&#20197;&#31163;&#32447;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#20026;&#36215;&#28857;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#31532;&#20108;&#20010;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26410;&#35265;&#36807;&#29366;&#24577;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#20801;&#35768;&#23398;&#24471;&#30340;&#27169;&#22411;&#21644;&#20215;&#20540;&#20272;&#35745;&#22312;&#26410;&#35265;&#29366;&#24577;&#20013;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#36890;&#36807;&#23545;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#36827;&#34892;&#22522;&#20110;&#20215;&#20540;&#30340;&#25200;&#21160;&#26469;&#25214;&#21040;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#65292;&#28982;&#21518;&#36890;&#36807;&#36807;&#28388;&#20855;&#26377;&#36807;&#39640;&#30340;&#21551;&#21457;&#24615;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;&#39640;&#35823;&#24046;&#65289;&#25110;&#36807;&#20302;&#30340;&#65288;&#36807;&#20110;&#30456;&#20284;&#65289;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) methods strike a balance between exploration and exploitation by conservative value estimation -- penalizing values of unseen states and actions. Model-free methods penalize values at all unseen actions, while model-based methods are able to further exploit unseen states via model rollouts. However, such methods are handicapped in their ability to find unseen states far away from the available offline data due to two factors -- (a) very short rollout horizons in models due to cascading model errors, and (b) model rollouts originating solely from states observed in offline data. We relax the second assumption and present a novel unseen state augmentation strategy to allow exploitation of unseen states where the learned model and value estimates generalize. Our strategy finds unseen states by value-informed perturbations of seen states followed by filtering out states with epistemic uncertainty estimates too high (high error) or too low (too similar to
&lt;/p&gt;</description></item><item><title>FLARE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#39564;&#35777;&#30097;&#20284;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26159;&#21542;&#26159;&#21478;&#19968;&#20010;&#31574;&#30053;&#30340;&#38750;&#27861;&#21103;&#26412;&#30340;&#25351;&#32441;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#24615;&#25513;&#30721;&#20316;&#20026;&#25351;&#32441;&#65292;&#24182;&#27979;&#37327;&#21160;&#20316;&#19968;&#33268;&#24615;&#20540;&#26469;&#39564;&#35777;&#34987;&#30423;&#31574;&#30053;&#30340;&#30495;&#23454;&#25152;&#26377;&#26435;&#12290;</title><link>http://arxiv.org/abs/2307.14751</link><description>&lt;p&gt;
FLARE: &#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#24615;&#25513;&#30721;&#23545;&#25351;&#32441;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#36827;&#34892;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks. (arXiv:2307.14751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14751
&lt;/p&gt;
&lt;p&gt;
FLARE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#39564;&#35777;&#30097;&#20284;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26159;&#21542;&#26159;&#21478;&#19968;&#20010;&#31574;&#30053;&#30340;&#38750;&#27861;&#21103;&#26412;&#30340;&#25351;&#32441;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#24615;&#25513;&#30721;&#20316;&#20026;&#25351;&#32441;&#65292;&#24182;&#27979;&#37327;&#21160;&#20316;&#19968;&#33268;&#24615;&#20540;&#26469;&#39564;&#35777;&#34987;&#30423;&#31574;&#30053;&#30340;&#30495;&#23454;&#25152;&#26377;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FLARE&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#39564;&#35777;&#30097;&#20284;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#31574;&#30053;&#26159;&#21542;&#26159;&#21478;&#19968;&#20010;&#65288;&#21463;&#23475;&#65289;&#31574;&#30053;&#30340;&#38750;&#27861;&#21103;&#26412;&#30340;&#25351;&#32441;&#26426;&#21046;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#36890;&#36807;&#25214;&#21040;&#19981;&#21487;&#20256;&#36882;&#30340;&#12289;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#25513;&#30721;&#65292;&#21363;&#25200;&#21160;&#65292;&#21487;&#20197;&#29983;&#25104;&#25104;&#21151;&#22320;&#20174;&#21463;&#23475;&#31574;&#30053;&#20256;&#36882;&#21040;&#20854;&#20462;&#25913;&#29256;&#26412;&#20294;&#19981;&#33021;&#20256;&#36882;&#21040;&#29420;&#31435;&#35757;&#32451;&#30340;&#31574;&#30053;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;FLARE&#21033;&#29992;&#36825;&#20123;&#25513;&#30721;&#20316;&#20026;&#25351;&#32441;&#65292;&#36890;&#36807;&#23545;&#36890;&#36807;&#25513;&#30721;&#25200;&#21160;&#30340;&#29366;&#24577;&#19978;&#30340;&#21160;&#20316;&#19968;&#33268;&#24615;&#20540;&#36827;&#34892;&#27979;&#37327;&#26469;&#39564;&#35777;&#34987;&#30423;&#30340;DRL&#31574;&#30053;&#30340;&#30495;&#23454;&#25152;&#26377;&#26435;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;FLARE&#26159;&#26377;&#25928;&#30340;&#65288;&#23545;&#20110;&#34987;&#30423;&#21103;&#26412;&#20855;&#26377;100%&#30340;&#21160;&#20316;&#19968;&#33268;&#24615;&#65289;&#65292;&#24182;&#19988;&#19981;&#20250;&#38169;&#35823;&#22320;&#25351;&#25511;&#29420;&#31435;&#31574;&#30053;&#65288;&#26080;&#35823;&#25253;&#65289;&#12290;FLARE&#36824;&#23545;&#27169;&#22411;&#20462;&#25913;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#19981;&#23481;&#26131;&#34987;&#26356;&#26126;&#26234;&#30340;&#23545;&#25163;&#35268;&#36991;&#32780;&#23545;&#26234;&#33021;&#20307;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;&#30340;&#36890;&#29992;&#23545;&#25239;&#24615;&#25513;&#30721;&#37117;&#26159;&#36866;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose FLARE, the first fingerprinting mechanism to verify whether a suspected Deep Reinforcement Learning (DRL) policy is an illegitimate copy of another (victim) policy. We first show that it is possible to find non-transferable, universal adversarial masks, i.e., perturbations, to generate adversarial examples that can successfully transfer from a victim policy to its modified versions but not to independently trained policies. FLARE employs these masks as fingerprints to verify the true ownership of stolen DRL policies by measuring an action agreement value over states perturbed via such masks. Our empirical evaluations show that FLARE is effective (100% action agreement on stolen copies) and does not falsely accuse independent policies (no false positives). FLARE is also robust to model modification attacks and cannot be easily evaded by more informed adversaries without negatively impacting agent performance. We also show that not all universal adversarial masks are suitable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#22797;&#26434;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#31283;&#23450;&#27169;&#20223;&#31574;&#30053;&#24182;&#30830;&#20445;&#20934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#20998;&#24067;&#65292;&#21487;&#20197;&#20351;&#27169;&#20223;&#32773;&#19982;&#28436;&#31034;&#32773;&#30340;&#36712;&#36857;&#20998;&#24067;&#30456;&#36817;&#12290;</title><link>http://arxiv.org/abs/2307.14619</link><description>&lt;p&gt;
&#27169;&#20223;&#22797;&#26434;&#36712;&#36857;&#65306;&#26725;&#25509;&#20302;&#23618;&#31283;&#23450;&#24615;&#19982;&#39640;&#23618;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior. (arXiv:2307.14619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#22797;&#26434;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#31283;&#23450;&#27169;&#20223;&#31574;&#30053;&#24182;&#30830;&#20445;&#20934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#20998;&#24067;&#65292;&#21487;&#20197;&#20351;&#27169;&#20223;&#32773;&#19982;&#28436;&#31034;&#32773;&#30340;&#36712;&#36857;&#20998;&#24067;&#30456;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#30740;&#31350;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#38543;&#26426;&#12289;&#38750;&#39532;&#23572;&#21487;&#22827;&#12289;&#28508;&#22312;&#22810;&#27169;&#24577;&#65288;&#21363;&#8220;&#22797;&#26434;&#8221;&#65289;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#20302;&#23618;&#25511;&#21046;&#22120;&#65288;&#26080;&#35770;&#26159;&#23398;&#20064;&#30340;&#36824;&#26159;&#38544;&#21547;&#30340;&#65289;&#26469;&#31283;&#23450;&#22260;&#32469;&#19987;&#23478;&#28436;&#31034;&#30340;&#27169;&#20223;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#65288;a&#65289;&#21512;&#36866;&#30340;&#20302;&#23618;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#65288;b&#65289;&#23398;&#20064;&#31574;&#30053;&#30340;&#38543;&#26426;&#36830;&#32493;&#24615;&#23646;&#24615;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#24635;&#21464;&#24046;&#36830;&#32493;&#24615;&#8221;&#65289;&#65288;TVC&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#31934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#29366;&#24577;&#20998;&#24067;&#19978;&#30340;&#34892;&#21160;&#30340;&#27169;&#20223;&#32773;&#20250;&#19982;&#28436;&#31034;&#32773;&#23545;&#25972;&#20010;&#36712;&#36857;&#30340;&#20998;&#24067;&#30456;&#36817;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#23558;&#27969;&#34892;&#30340;&#25968;&#25454;&#22686;&#24378;&#35268;&#21017;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#25216;&#24039;&#30456;&#32467;&#21512;&#65288;&#21363;&#22312;&#25191;&#34892;&#26102;&#28155;&#21152;&#22686;&#24378;&#22122;&#22768;&#65289;&#26469;&#30830;&#20445;TVC&#24182;&#19988;&#26368;&#23567;&#31243;&#24230;&#19978;&#38477;&#20302;&#31934;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20445;&#35777;&#23454;&#20363;&#21270;&#20026;&#30001;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#22914;&#26524;&#23398;&#20064;&#32773;&#20934;&#30830;&#22320;&#20272;&#35745;&#20102;&#28436;&#31034;&#32773;&#30340;&#20998;&#24067;&#65292;&#21017;&#26368;&#32456;&#23436;&#25104;&#36825;&#31181;&#23454;&#20363;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a theoretical framework for studying the imitation of stochastic, non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations in nonlinear dynamical systems. Our framework invokes low-level controllers either learned or implicit in position-command control - to stabilize imitation policies around expert demonstrations. We show that with (a) a suitable low-level stability guarantee and (b) a stochastic continuity property of the learned policy we call "total variation continuity" (TVC), an imitator that accurately estimates actions on the demonstrator's state distribution closely matches the demonstrator's distribution over entire trajectories. We then show that TVC can be ensured with minimal degradation of accuracy by combining a popular data-augmentation regimen with a novel algorithmic trick: adding augmentation noise at execution time. We instantiate our guarantees for policies parameterized by diffusion models and prove that if the learner accuratel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#21019;&#26032;&#30340;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#21450;&#36890;&#36807;&#22312;&#38669;&#22827;&#31354;&#38388;&#20013;&#20351;&#29992;&#26032;&#30340;SR&#25439;&#22833;&#20989;&#25968;&#25913;&#21892;&#39134;&#34892;&#36712;&#36857;&#32447;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#39134;&#34892;&#36712;&#36857;&#24182;&#19988;&#23545;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#65292;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33322;&#31354;&#30740;&#31350;&#20013;&#30340;&#39134;&#34892;&#36712;&#36857;&#26816;&#27979;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.12032</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#38669;&#22827;&#31354;&#38388;&#20013;&#37319;&#29992;&#26032;&#30340;SR&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#22686;&#24378;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;&#39134;&#34892;&#36712;&#36857;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space. (arXiv:2307.12032v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#21019;&#26032;&#30340;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#21450;&#36890;&#36807;&#22312;&#38669;&#22827;&#31354;&#38388;&#20013;&#20351;&#29992;&#26032;&#30340;SR&#25439;&#22833;&#20989;&#25968;&#25913;&#21892;&#39134;&#34892;&#36712;&#36857;&#32447;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#39134;&#34892;&#36712;&#36857;&#24182;&#19988;&#23545;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#65292;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33322;&#31354;&#30740;&#31350;&#20013;&#30340;&#39134;&#34892;&#36712;&#36857;&#26816;&#27979;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#20013;&#20132;&#36890;&#23545;&#29615;&#22659;&#20135;&#29983;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#39134;&#34892;&#36712;&#36857;&#36896;&#25104;&#30340;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#36129;&#29486;&#65292;&#30001;&#20110;&#20854;&#28508;&#22312;&#30340;&#20840;&#29699;&#21464;&#26262;&#24433;&#21709;&#12290;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#26816;&#27979;&#39134;&#34892;&#36712;&#36857;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38590;&#39064;&#12290;&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#22312;&#19981;&#21516;&#30340;&#22270;&#20687;&#26465;&#20214;&#19979;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#32780;&#20351;&#29992;&#20856;&#22411;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21463;&#21040;&#25163;&#24037;&#26631;&#27880;&#30340;&#39134;&#34892;&#36712;&#36857;&#25968;&#25454;&#38598;&#21644;&#19987;&#38376;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#22522;&#20110;&#22686;&#24378;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#39134;&#34892;&#36712;&#36857;&#65292;&#24182;&#19988;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;SR Loss&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#31354;&#38388;&#36716;&#25442;&#20026;&#38669;&#22827;&#31354;&#38388;&#26469;&#25913;&#21892;&#39134;&#34892;&#36712;&#36857;&#32447;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33322;&#31354;&#30740;&#31350;&#20013;&#30340;&#39134;&#34892;&#36712;&#36857;&#26816;&#27979;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#22823;&#22411;&#25163;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#32570;&#20047;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#22686;&#24378;&#20102;&#39134;&#34892;&#36712;&#36857;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air transport poses significant environmental challenges, particularly the contribution of flight contrails to climate change due to their potential global warming impact. Detecting contrails from satellite images has been a long-standing challenge. Traditional computer vision techniques have limitations under varying image conditions, and machine learning approaches using typical convolutional neural networks are hindered by the scarcity of hand-labeled contrail datasets and contrail-tailored learning processes. In this paper, we introduce an innovative model based on augmented transfer learning that accurately detects contrails with minimal data. We also propose a novel loss function, SR Loss, which improves contrail line detection by transforming the image space into Hough space. Our research opens new avenues for machine learning-based contrail detection in aviation research, offering solutions to the lack of large hand-labeled datasets, and significantly enhancing contrail detecti
&lt;/p&gt;</description></item><item><title>MASR&#26159;&#19968;&#31181;&#22810;&#26631;&#31614;&#24863;&#30693;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#22810;&#20010;&#22806;&#37096;&#30693;&#35782;&#28304;&#22686;&#24378;&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#21033;&#29992;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.10982</link><description>&lt;p&gt;
MASR: &#22810;&#26631;&#31614;&#24863;&#30693;&#35821;&#38899;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MASR: Multi-label Aware Speech Representation. (arXiv:2307.10982v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10982
&lt;/p&gt;
&lt;p&gt;
MASR&#26159;&#19968;&#31181;&#22810;&#26631;&#31614;&#24863;&#30693;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#22810;&#20010;&#22806;&#37096;&#30693;&#35782;&#28304;&#22686;&#24378;&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#21033;&#29992;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#20027;&#35201;&#20197;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20026;&#22522;&#30784;&#65292;&#20165;&#20351;&#29992;&#21407;&#22987;&#38899;&#39057;&#20449;&#21495;&#65292;&#24573;&#30053;&#20102;&#36890;&#24120;&#21487;&#29992;&#20110;&#32473;&#23450;&#35821;&#38899;&#35760;&#24405;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MASR&#65292;&#19968;&#31181;&#22810;&#26631;&#31614;&#24863;&#30693;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#12290;MASR&#33021;&#22815;&#24341;&#20837;&#22810;&#20010;&#22806;&#37096;&#30693;&#35782;&#28304;&#65292;&#22686;&#24378;&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#21033;&#29992;&#12290;&#22806;&#37096;&#30693;&#35782;&#28304;&#20197;&#26679;&#26412;&#32423;&#25104;&#23545;&#30456;&#20284;&#24615;&#30697;&#38453;&#30340;&#24418;&#24335;&#34987;&#32435;&#20837;&#21040;&#19968;&#20010;&#30828;&#25366;&#25496;&#25439;&#22833;&#20989;&#25968;&#20013;&#12290;MASR&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#23427;&#21487;&#20197;&#19982;&#20219;&#20309;&#36873;&#25321;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;MASR&#34920;&#31034;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#22914;&#35821;&#35328;&#35782;&#21035;&#12289;&#35821;&#38899;&#35782;&#21035;&#20197;&#21450;&#35828;&#35805;&#20154;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#38750;&#35821;&#20041;&#20219;&#21153;&#12290;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Multi-label Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#31169;&#26377;&#30340;&#35299;&#32806;&#22270;&#21367;&#31215;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#31890;&#24230;&#25299;&#25169;&#20445;&#25252;&#12290;&#24341;&#20837;&#20102;&#22270;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#21487;&#20197;&#30830;&#20445;&#27169;&#22411;&#21442;&#25968;&#21644;&#39044;&#27979;&#30340;&#31169;&#23494;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06422</link><description>&lt;p&gt;
&#24046;&#20998;&#31169;&#26377;&#30340;&#35299;&#32806;&#22270;&#21367;&#31215;&#29992;&#20110;&#22810;&#31890;&#24230;&#25299;&#25169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection. (arXiv:2307.06422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#31169;&#26377;&#30340;&#35299;&#32806;&#22270;&#21367;&#31215;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#31890;&#24230;&#25299;&#25169;&#20445;&#25252;&#12290;&#24341;&#20837;&#20102;&#22270;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#21487;&#20197;&#30830;&#20445;&#27169;&#22411;&#21442;&#25968;&#21644;&#39044;&#27979;&#30340;&#31169;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#22312;&#35299;&#20915;&#28041;&#21450;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#23454;&#38469;&#23398;&#20064;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22270;&#23398;&#20064;&#26041;&#27861;&#19981;&#20165;&#36890;&#36807;&#20854;&#27169;&#22411;&#21442;&#25968;&#65292;&#36824;&#36890;&#36807;&#20854;&#27169;&#22411;&#39044;&#27979;&#26292;&#38706;&#20102;&#25935;&#24863;&#30340;&#29992;&#25143;&#20449;&#24687;&#21644;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#20165;&#25552;&#20379;&#27169;&#22411;&#26435;&#37325;&#38544;&#31169;&#30340;&#26631;&#20934;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25216;&#26415;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#36825;&#23588;&#20854;&#36866;&#29992;&#20110;&#36890;&#36807;&#22270;&#21367;&#31215;&#30452;&#25509;&#21033;&#29992;&#30456;&#37051;&#33410;&#28857;&#23646;&#24615;&#36827;&#34892;&#33410;&#28857;&#39044;&#27979;&#30340;&#24773;&#20917;&#65292;&#36825;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22270;&#24046;&#20998;&#38544;&#31169;&#65288;GDP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#36866;&#29992;&#20110;&#22270;&#23398;&#20064;&#29615;&#22659;&#30340;&#24418;&#24335;&#21270;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#21487;&#20197;&#30830;&#20445;&#27169;&#22411;&#21442;&#25968;&#21644;&#39044;&#27979;&#37117;&#26159;&#21487;&#35777;&#26126;&#30340;&#31169;&#26377;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#21487;&#33021;&#23384;&#22312;&#19981;&#21516;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25918;&#26494;&#30340;&#33410;&#28857;&#23618;&#27425;&#38544;&#31169;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph learning methods, such as Graph Neural Networks (GNNs) based on graph convolutions, are highly successful in solving real-world learning problems involving graph-structured data. However, graph learning methods expose sensitive user information and interactions not only through their model parameters but also through their model predictions. Consequently, standard Differential Privacy (DP) techniques that merely offer model weight privacy are inadequate. This is especially the case for node predictions that leverage neighboring node attributes directly via graph convolutions that create additional risks of privacy leakage. To address this problem, we introduce Graph Differential Privacy (GDP), a new formal DP framework tailored to graph learning settings that ensures both provably private model parameters and predictions. Furthermore, since there may be different privacy requirements for the node attributes and graph structure, we introduce a novel notion of relaxed node-level da
&lt;/p&gt;</description></item><item><title>&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#27169;&#22411;&#12290;&#23427;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#26080;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.04870</link><description>&lt;p&gt;
&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#65306;&#22312;&#24369;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Onion Universe Algorithm: Applications in Weakly Supervised Learning. (arXiv:2307.04870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04870
&lt;/p&gt;
&lt;p&gt;
&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#27169;&#22411;&#12290;&#23427;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#26080;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;(OUA)&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#26631;&#31614;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;OUA&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#25110;&#24369;&#20449;&#21495;&#30340;&#20219;&#20309;&#20551;&#35774;&#12290;&#35813;&#27169;&#22411;&#38750;&#24120;&#36866;&#29992;&#20110;&#27809;&#26377;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23545;&#30001;&#24369;&#20449;&#21495;&#25152;&#26500;&#25104;&#30340;&#31354;&#38388;&#30340;&#20960;&#20309;&#35299;&#37322;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;OUA&#22312;&#19968;&#33324;&#30340;&#24369;&#20449;&#21495;&#38598;&#21512;&#19979;&#20855;&#26377;&#28508;&#22312;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#65292;OUA&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26631;&#31614;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Onion Universe Algorithm (OUA), a novel classification method in ensemble learning. In particular, we show its applicability as a label model for weakly supervised learning. OUA offers simplicity in implementation, computational efficiency, and does not rely on any assumptions regarding the data or weak signals. The model is well suited for scenarios where fully labeled data is not available. Our method is built upon geometrical interpretation of the space spanned by weak signals. Empirical results support our analysis of the hidden geometric structure underlying general set of weak signals and also illustrates that OUA works well in practice. We show empirical evidence that OUA performs favorably on common benchmark datasets compared to existing label models for weakly supervised learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20998;&#26512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;DGA&#20998;&#31867;&#22120;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#28040;&#38500;&#36825;&#20123;&#20559;&#35265;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26080;&#20559;&#35265;&#19988;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26816;&#27979;&#31995;&#32479;&#65292;&#20445;&#25345;&#20102;&#26368;&#20808;&#36827;&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#29575;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#20998;&#26512;&#31995;&#32479;&#65292;&#20197;&#22686;&#21152;&#23545;&#26816;&#27979;&#26041;&#27861;&#30340;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.04358</link><description>&lt;p&gt;
&#34394;&#20551;&#30340;&#23433;&#20840;&#24863;&#65306;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20998;&#26512;&#26080;&#19978;&#19979;&#25991;DGA&#20998;&#31867;&#22120;&#30340;&#25512;&#29702;&#21644;&#30495;&#23454;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
False Sense of Security: Leveraging XAI to Analyze the Reasoning and True Performance of Context-less DGA Classifiers. (arXiv:2307.04358v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20998;&#26512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;DGA&#20998;&#31867;&#22120;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#28040;&#38500;&#36825;&#20123;&#20559;&#35265;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26080;&#20559;&#35265;&#19988;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26816;&#27979;&#31995;&#32479;&#65292;&#20445;&#25345;&#20102;&#26368;&#20808;&#36827;&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#29575;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#20998;&#26512;&#31995;&#32479;&#65292;&#20197;&#22686;&#21152;&#23545;&#26816;&#27979;&#26041;&#27861;&#30340;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22495;&#29983;&#25104;&#31639;&#27861;&#65288;DGA&#65289;&#26816;&#27979;&#25581;&#31034;&#20725;&#23608;&#32593;&#32476;&#27963;&#21160;&#30340;&#38382;&#39064;&#20284;&#20046;&#24050;&#32463;&#35299;&#20915;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;99.9%&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#34394;&#20551;&#30340;&#23433;&#20840;&#24863;&#65292;&#22240;&#20026;&#23427;&#20204;&#23384;&#22312;&#20005;&#37325;&#30340;&#20559;&#35265;&#65292;&#23481;&#26131;&#34987;&#32469;&#36807;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20998;&#26512;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#31995;&#32479;&#22320;&#25581;&#31034;&#36825;&#20123;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#28040;&#38500;DGA&#20998;&#31867;&#22120;&#30340;&#36825;&#20123;&#20559;&#35265;&#20250;&#26174;&#33879;&#38477;&#20302;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#33021;&#22815;&#35774;&#35745;&#19968;&#20010;&#26080;&#20559;&#35265;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#26816;&#27979;&#31995;&#32479;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#29575;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#20998;&#26512;&#31995;&#32479;&#65292;&#24110;&#21161;&#26356;&#22909;&#22320;&#29702;&#35299;&#20998;&#31867;&#22120;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#22686;&#21152;&#23545;&#26816;&#27979;&#26041;&#27861;&#30340;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#65292;&#24182;&#20419;&#36827;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of revealing botnet activity through Domain Generation Algorithm (DGA) detection seems to be solved, considering that available deep learning classifiers achieve accuracies of over 99.9%. However, these classifiers provide a false sense of security as they are heavily biased and allow for trivial detection bypass. In this work, we leverage explainable artificial intelligence (XAI) methods to analyze the reasoning of deep learning classifiers and to systematically reveal such biases. We show that eliminating these biases from DGA classifiers considerably deteriorates their performance. Nevertheless we are able to design a context-aware detection system that is free of the identified biases and maintains the detection rate of state-of-the art deep learning classifiers. In this context, we propose a visual analysis system that helps to better understand a classifier's reasoning, thereby increasing trust in and transparency of detection methods and facilitating decision-making.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;&#21453;&#26144;&#21457;&#36865;&#21644;&#25509;&#25910;&#28040;&#24687;&#20851;&#31995;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2307.01403</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Learning to Communicate using Contrastive Learning. (arXiv:2307.01403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;&#21453;&#26144;&#21457;&#36865;&#21644;&#25509;&#25910;&#28040;&#24687;&#20851;&#31995;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21327;&#35843;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#20294;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#35825;&#23548;&#19968;&#20010;&#26377;&#25928;&#30340;&#20849;&#21516;&#35821;&#35328;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26367;&#20195;&#35270;&#35282;&#65292;&#21363;&#23558;&#26234;&#33021;&#20307;&#20043;&#38388;&#21457;&#36865;&#30340;&#36890;&#20449;&#28040;&#24687;&#35270;&#20026;&#29615;&#22659;&#29366;&#24577;&#30340;&#19981;&#23436;&#25972;&#35270;&#22270;&#12290;&#36890;&#36807;&#26816;&#26597;&#21457;&#36865;&#21644;&#25509;&#25910;&#30340;&#28040;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#32473;&#23450;&#36712;&#36857;&#30340;&#28040;&#24687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#20351;&#29992;&#23450;&#24615;&#25351;&#26631;&#21644;&#34920;&#31034;&#25506;&#27979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#35825;&#23548;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#24182;&#20174;&#29615;&#22659;&#20013;&#25429;&#33719;&#20102;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#21147;&#37327;&#20197;&#21450;&#21033;&#29992;&#28040;&#24687;&#20316;&#20026;&#32534;&#30721;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MoVie&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#35270;&#35273;&#27169;&#22411;&#30340;&#31574;&#30053;&#33258;&#36866;&#24212;&#23454;&#29616;&#20102;&#35270;&#22270;&#27867;&#21270;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#22870;&#21169;&#20449;&#21495;&#21644;&#35757;&#32451;&#36807;&#31243;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#30456;&#23545;&#25913;&#36827;&#36798;&#21040;&#20102;33%&#33267;152%&#12290;</title><link>http://arxiv.org/abs/2307.00972</link><description>&lt;p&gt;
MoVie: &#22522;&#20110;&#35270;&#35273;&#27169;&#22411;&#30340;&#31574;&#30053;&#33258;&#36866;&#24212;&#29992;&#20110;&#35270;&#22270;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
MoVie: Visual Model-Based Policy Adaptation for View Generalization. (arXiv:2307.00972v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MoVie&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#35270;&#35273;&#27169;&#22411;&#30340;&#31574;&#30053;&#33258;&#36866;&#24212;&#23454;&#29616;&#20102;&#35270;&#22270;&#27867;&#21270;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#22870;&#21169;&#20449;&#21495;&#21644;&#35757;&#32451;&#36807;&#31243;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#30456;&#23545;&#25913;&#36827;&#36798;&#21040;&#20102;33%&#33267;152%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#30340;&#35270;&#22270;&#19978;&#35757;&#32451;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#22312;&#23558;&#20854;&#23398;&#21040;&#30340;&#33021;&#21147;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#35270;&#22270;&#26102;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36825;&#20010;&#22266;&#26377;&#30340;&#22256;&#38590;&#34987;&#31216;&#20026;$\textit{view generalization}$&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#22522;&#26412;&#38382;&#39064;&#31995;&#32479;&#22320;&#20998;&#20026;&#22235;&#20010;&#19981;&#21516;&#30340;&#65292;&#39640;&#24230;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#26223;&#65292;&#36825;&#20123;&#24773;&#26223;&#19982;&#23454;&#38469;&#24773;&#20917;&#24456;&#30456;&#20284;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#26102;&#20351;&#22522;&#20110;&#35270;&#35273;&#30340;$\textbf{Mo}$del-based&#31574;&#30053;&#33021;&#22815;&#25104;&#21151;&#36866;&#24212;$\textbf{Vie}$w generalization ($\textbf{MoVie}$)&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30340;&#22870;&#21169;&#20449;&#21495;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20219;&#20309;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;DMControl&#12289;xArm&#21644;Adroit&#30340;&#24635;&#20849;$\textbf{18}$&#20010;&#20219;&#21153;&#20013;&#30340;&#25152;&#26377;&#22235;&#20010;&#24773;&#26223;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#30456;&#23545;&#25913;&#36827;&#20998;&#21035;&#20026;$\mathbf{33}$%&#65292;$\mathbf{86}$%&#21644;$\mathbf{152}$%&#12290;&#20248;&#36234;&#30340;&#32467;&#26524;&#20984;&#26174;&#20986;&#20854;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views. This inherent difficulty is known as the problem of $\textit{view generalization}$. In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations. Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual $\textbf{Mo}$del-based policies for $\textbf{Vie}$w generalization ($\textbf{MoVie}$) during test time, without any need for explicit reward signals and any modification during training time. Our method demonstrates substantial advancements across all four scenarios encompassing a total of $\textbf{18}$ tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of $\mathbf{33}$%, $\mathbf{86}$%, and $\mathbf{152}$% respectively. The superior results highlight the immens
&lt;/p&gt;</description></item><item><title>ECG-QA&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#24515;&#30005;&#22270;&#20998;&#26512;&#35774;&#35745;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#28085;&#30422;&#24191;&#27867;&#20020;&#24202;&#30456;&#20851;ECG&#20027;&#39064;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#22810;&#26679;&#21270;&#30340;ECG&#35299;&#35835;&#38382;&#39064;&#12290;&#36825;&#19968;&#36164;&#28304;&#23558;&#20026;&#26410;&#26469;&#30340;&#21307;&#30103;&#20445;&#20581;&#38382;&#31572;&#30740;&#31350;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.15681</link><description>&lt;p&gt;
ECG-QA&#65306;&#32467;&#21512;&#24515;&#30005;&#22270;&#30340;&#32508;&#21512;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram. (arXiv:2306.15681v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15681
&lt;/p&gt;
&lt;p&gt;
ECG-QA&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#24515;&#30005;&#22270;&#20998;&#26512;&#35774;&#35745;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#28085;&#30422;&#24191;&#27867;&#20020;&#24202;&#30456;&#20851;ECG&#20027;&#39064;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#22810;&#26679;&#21270;&#30340;ECG&#35299;&#35835;&#38382;&#39064;&#12290;&#36825;&#19968;&#36164;&#28304;&#23558;&#20026;&#26410;&#26469;&#30340;&#21307;&#30103;&#20445;&#20581;&#38382;&#31572;&#30740;&#31350;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#38382;&#31572;&#38382;&#39064;&#65288;QA&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21307;&#30103;&#20445;&#20581;QA&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#21307;&#23398;&#24433;&#20687;&#12289;&#20020;&#24202;&#35760;&#24405;&#25110;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#34920;&#19978;&#12290;&#36825;&#20351;&#24471;&#23558;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#19982;&#36825;&#20123;&#31995;&#32479;&#30456;&#32467;&#21512;&#30340;&#24040;&#22823;&#28508;&#21147;&#20960;&#20046;&#26410;&#34987;&#21033;&#29992;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ECG-QA&#65292;&#36825;&#26159;&#19987;&#38376;&#38024;&#23545;ECG&#20998;&#26512;&#35774;&#35745;&#30340;&#31532;&#19968;&#20010;QA&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20849;70&#20010;&#28085;&#30422;&#20102;&#24191;&#27867;&#20020;&#24202;&#30456;&#20851;ECG&#20027;&#39064;&#30340;&#38382;&#39064;&#27169;&#26495;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#32463;&#36807;&#19968;&#21517;ECG&#19987;&#23478;&#30340;&#39564;&#35777;&#65292;&#20197;&#30830;&#20445;&#20854;&#20020;&#24202;&#25928;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#22810;&#26679;&#21270;&#30340;ECG&#35299;&#35835;&#38382;&#39064;&#65292;&#21253;&#25324;&#38656;&#35201;&#23545;&#20004;&#20010;&#19981;&#21516;&#30340;ECG&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#35768;&#22810;&#23454;&#39564;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30456;&#20449;ECG-QA&#23558;&#25104;&#20026;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#20379;&#30740;&#31350;&#32773;&#20204;&#25506;&#32034;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) in the field of healthcare has received much attention due to significant advancements in natural language processing. However, existing healthcare QA datasets primarily focus on medical images, clinical notes, or structured electronic health record tables. This leaves the vast potential of combining electrocardiogram (ECG) data with these systems largely untapped. To address this gap, we present ECG-QA, the first QA dataset specifically designed for ECG analysis. The dataset comprises a total of 70 question templates that cover a wide range of clinically relevant ECG topics, each validated by an ECG expert to ensure their clinical utility. As a result, our dataset includes diverse ECG interpretation questions, including those that require a comparative analysis of two different ECGs. In addition, we have conducted numerous experiments to provide valuable insights for future research directions. We believe that ECG-QA will serve as a valuable resource for the de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#27169;&#25311;&#21453;&#20107;&#23454;&#20998;&#24067;&#20013;&#30340;&#20540;&#65292;&#21487;&#23545;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#35774;&#23450;&#26465;&#20214;&#65292;&#24182;&#24212;&#29992;&#20110;&#20449;&#29992;&#35780;&#20998;&#20013;&#30340;&#20844;&#24179;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.15328</link><description>&lt;p&gt;
&#27169;&#25311;&#21453;&#20107;&#23454;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Simulating counterfactuals. (arXiv:2306.15328v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15328
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#27169;&#25311;&#21453;&#20107;&#23454;&#20998;&#24067;&#20013;&#30340;&#20540;&#65292;&#21487;&#23545;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#35774;&#23450;&#26465;&#20214;&#65292;&#24182;&#24212;&#29992;&#20110;&#20449;&#29992;&#35780;&#20998;&#20013;&#30340;&#20844;&#24179;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#26029;&#32771;&#34385;&#20102;&#22312;&#19982;&#23454;&#38469;&#19990;&#30028;&#23384;&#22312;&#19968;&#20123;&#35777;&#25454;&#30340;&#24179;&#34892;&#19990;&#30028;&#20013;&#36827;&#34892;&#30340;&#20551;&#35774;&#24615;&#24178;&#39044;&#12290;&#22914;&#26524;&#35777;&#25454;&#22312;&#27969;&#24418;&#19978;&#25351;&#23450;&#20102;&#26465;&#20214;&#20998;&#24067;&#65292;&#21453;&#20107;&#23454;&#21487;&#33021;&#26159;&#35299;&#26512;&#38590;&#35299;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#21453;&#20107;&#23454;&#20998;&#24067;&#20013;&#27169;&#25311;&#20540;&#65292;&#20854;&#20013;&#21487;&#20197;&#23545;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#35774;&#23450;&#26465;&#20214;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#34987;&#21576;&#29616;&#20026;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#20174;&#32780;&#23548;&#33268;&#28176;&#36817;&#26377;&#25928;&#30340;&#25512;&#26029;&#12290;&#35813;&#31639;&#27861;&#34987;&#24212;&#29992;&#20110;&#20449;&#29992;&#35780;&#20998;&#20013;&#30340;&#20844;&#24179;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual inference considers a hypothetical intervention in a parallel world that shares some evidence with the factual world. If the evidence specifies a conditional distribution on a manifold, counterfactuals may be analytically intractable. We present an algorithm for simulating values from a counterfactual distribution where conditions can be set on both discrete and continuous variables. We show that the proposed algorithm can be presented as a particle filter leading to asymptotically valid inference. The algorithm is applied to fairness analysis in credit scoring.
&lt;/p&gt;</description></item><item><title>&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#25688;&#35201;&#26088;&#22312;&#20026;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#30495;&#23454;&#12289;&#22810;&#26679;&#21644;&#22797;&#26434;&#30340;&#22522;&#20934;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#21512;&#25104;&#25968;&#25454;&#30340;&#20559;&#24046;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13216</link><description>&lt;p&gt;
&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#29992;&#20110;&#25968;&#25454;&#38544;&#31169;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Diverse Community Data for Benchmarking Data Privacy Algorithms. (arXiv:2306.13216v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13216
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#25688;&#35201;&#26088;&#22312;&#20026;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#30495;&#23454;&#12289;&#22810;&#26679;&#21644;&#22797;&#26434;&#30340;&#22522;&#20934;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#21512;&#25104;&#25968;&#25454;&#30340;&#20559;&#24046;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#26159;&#32654;&#22269;&#22269;&#23478;&#26631;&#20934;&#21644;&#25216;&#26415;&#30740;&#31350;&#25152;&#65288;NIST&#65289;&#35745;&#21010;&#30340;&#26680;&#24515;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#34920;&#26684;&#25968;&#25454;&#21435;&#35782;&#21035;&#25216;&#26415;&#65288;&#22914;&#21512;&#25104;&#25968;&#25454;&#65289;&#30340;&#29702;&#35299;&#12290;&#21512;&#25104;&#25968;&#25454;&#26159;&#27665;&#20027;&#21270;&#22823;&#25968;&#25454;&#21033;&#30410;&#30340;&#19968;&#39033;&#38596;&#24515;&#21187;&#21187;&#30340;&#23581;&#35797;&#65307;&#23427;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#37325;&#26032;&#21019;&#24314;&#25935;&#24863;&#20010;&#20154;&#25968;&#25454;&#65292;&#20197;&#20415;&#20844;&#24320;&#21457;&#24067;&#12290;&#28982;&#32780;&#65292;&#23427;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#20559;&#24046;&#21644;&#38544;&#31169;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#29978;&#33267;&#21487;&#33021;&#25918;&#22823;&#36825;&#20123;&#38382;&#39064;&#12290;&#24403;&#21435;&#35782;&#21035;&#25968;&#25454;&#20998;&#24067;&#24341;&#20837;&#20559;&#24046;&#25110;&#24037;&#20214;&#65292;&#25110;&#27844;&#28431;&#25935;&#24863;&#20449;&#24687;&#26102;&#65292;&#23427;&#20204;&#20250;&#23558;&#36825;&#20123;&#38382;&#39064;&#20256;&#25773;&#21040;&#19979;&#28216;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#35843;&#26597;&#26465;&#20214;&#65288;&#22914;&#22810;&#26679;&#23376;&#32676;&#12289;&#24322;&#36136;&#38750;&#26377;&#24207;&#25968;&#25454;&#31354;&#38388;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65289;&#23545;&#21512;&#25104;&#25968;&#25454;&#31639;&#27861;&#25552;&#20986;&#20102;&#20855;&#20307;&#25361;&#25112;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#20419;&#20351;&#38656;&#35201;&#30495;&#23454;&#12289;&#22810;&#26679;&#21644;&#22797;&#26434;&#30340;&#22522;&#20934;&#25968;&#25454;&#26469;&#25903;&#25345;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#65292;&#32780;&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#25688;&#35201;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Diverse Communities Data Excerpts are the core of a National Institute of Standards and Technology (NIST) program to strengthen understanding of tabular data deidentification technologies such as synthetic data. Synthetic data is an ambitious attempt to democratize the benefits of big data; it uses generative models to recreate sensitive personal data with new records for public release. However, it is vulnerable to the same bias and privacy issues that impact other machine learning applications, and can even amplify those issues. When deidentified data distributions introduce bias or artifacts, or leak sensitive information, they propagate these problems to downstream applications. Furthermore, real-world survey conditions such as diverse subpopulations, heterogeneous non-ordinal data spaces, and complex dependencies between features pose specific challenges for synthetic data algorithms. These observations motivate the need for real, diverse, and complex benchmark data to support
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986; TeCoS-LVM &#27169;&#22411;&#65292;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20197;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#20002;&#22833;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.12045</link><description>&lt;p&gt;
&#22788;&#29702;&#33258;&#28982;&#35270;&#35273;&#22330;&#26223;&#31070;&#32463;&#21709;&#24212;&#30340;&#26102;&#38388;&#26465;&#20214;&#33033;&#20914;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes. (arXiv:2306.12045v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986; TeCoS-LVM &#27169;&#22411;&#65292;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20197;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#20002;&#22833;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#31070;&#32463;&#21709;&#24212;&#30340;&#35745;&#31639;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#24863;&#30693;&#22788;&#29702;&#21644;&#31070;&#32463;&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#20351;&#29992;&#26102;&#38388;&#36807;&#28388;&#22120;&#26469;&#22788;&#29702;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#23548;&#33268;&#22788;&#29702;&#27969;&#31243;&#19981;&#29616;&#23454;&#19988;&#19981;&#28789;&#27963;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38024;&#23545;&#35797;&#39564;&#24179;&#22343;&#21457;&#25918;&#29575;&#65292;&#26410;&#33021;&#25429;&#25417;&#21040;&#33033;&#20914;&#21015;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#26102;&#38388;&#26465;&#20214;&#33033;&#20914;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;TeCoS-LVM&#65289;&#26469;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20135;&#29983;&#30452;&#25509;&#21305;&#37197;&#35760;&#24405;&#33033;&#20914;&#21015;&#30340;&#33033;&#20914;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#36991;&#20813;&#20002;&#22833;&#23884;&#20837;&#22312;&#21407;&#22987;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20174;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20013;&#25490;&#38500;&#26102;&#38388;&#32500;&#24230;&#65292;&#24182;&#24341;&#20837;&#26102;&#38388;&#26465;&#20214;&#25805;&#20316;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#33258;&#28982;&#33539;&#24335;&#20013;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; TeCoS-LVM &#27169;&#22411;&#33021;&#22815;&#20135;&#29983;...
&lt;/p&gt;
&lt;p&gt;
Developing computational models of neural response is crucial for understanding sensory processing and neural computations. Current state-of-the-art neural network methods use temporal filters to handle temporal dependencies, resulting in an unrealistic and inflexible processing flow. Meanwhile, these methods target trial-averaged firing rates and fail to capture important features in spike trains. This work presents the temporal conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural response to natural visual stimuli. We use spiking neurons to produce spike outputs that directly match the recorded trains. This approach helps to avoid losing information embedded in the original spike trains. We exclude the temporal dimension from the model parameter space and introduce a temporal conditioning operation to allow the model to adaptively explore and exploit temporal dependencies in stimuli sequences in a natural paradigm. We show that TeCoS-LVM models can produce m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DropCompute&#30340;&#31616;&#21333;&#19988;&#31283;&#23450;&#30340;&#20998;&#24067;&#24335;&#21516;&#27493;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#24037;&#20316;&#33410;&#28857;&#38388;&#30340;&#35745;&#31639;&#21464;&#24322;&#24615;&#26469;&#25552;&#39640;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10598</link><description>&lt;p&gt;
DropCompute: &#36890;&#36807;&#35745;&#31639;&#26041;&#24046;&#20943;&#23569;&#20998;&#24067;&#24335;&#21516;&#27493;&#35757;&#32451;&#30340;&#31616;&#26131;&#19988;&#26356;&#31283;&#23450;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DropCompute: simple and more robust distributed synchronous training via compute variance reduction. (arXiv:2306.10598v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10598
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DropCompute&#30340;&#31616;&#21333;&#19988;&#31283;&#23450;&#30340;&#20998;&#24067;&#24335;&#21516;&#27493;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#24037;&#20316;&#33410;&#28857;&#38388;&#30340;&#35745;&#31639;&#21464;&#24322;&#24615;&#26469;&#25552;&#39640;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20998;&#24067;&#24335;&#35757;&#32451;&#23545;&#20110;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#22823;&#35268;&#27169;DNN&#35757;&#32451;&#30340;&#20027;&#27969;&#26041;&#27861;&#26159;&#21516;&#27493;&#35757;&#32451;&#65288;&#20363;&#22914;All-Reduce&#65289;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#31561;&#24453;&#27599;&#19968;&#27493;&#20013;&#25152;&#26377;&#30340;&#24037;&#20316;&#33410;&#28857;&#23436;&#25104;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21046;&#20110;&#30001;&#24310;&#36831;&#23548;&#33268;&#30340;&#24037;&#20316;&#33410;&#28857;&#25302;&#24310;&#30340;&#38382;&#39064;&#12290;&#32467;&#26524;&#65306;&#26412;&#25991;&#30740;&#31350;&#20102;&#30001;&#20110;&#35745;&#31639;&#26102;&#38388;&#30340;&#21464;&#24322;&#24615;&#23548;&#33268;&#24037;&#20316;&#33410;&#28857;&#25302;&#24310;&#30340;&#20856;&#22411;&#22330;&#26223;&#65292;&#24182;&#25214;&#21040;&#20102;&#35745;&#31639;&#26102;&#38388;&#23646;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#20043;&#38388;&#30340;&#20998;&#26512;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#24037;&#20316;&#33410;&#28857;&#38388;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#21516;&#27493;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;All-Reduce&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;200&#20010;Gaudi&#21152;&#36895;&#22120;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Distributed training is essential for large scale training of deep neural networks (DNNs). The dominant methods for large scale DNN training are synchronous (e.g. All-Reduce), but these require waiting for all workers in each step. Thus, these methods are limited by the delays caused by straggling workers. Results: We study a typical scenario in which workers are straggling due to variability in compute time. We find an analytical relation between compute time properties and scalability limitations, caused by such straggling workers. With these findings, we propose a simple yet effective decentralized method to reduce the variation among workers and thus improve the robustness of synchronous training. This method can be integrated with the widely used All-Reduce. Our findings are validated on large-scale training tasks using 200 Gaudi Accelerators.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#22686;&#24378;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65288;LADO&#65289;&#65292;&#35813;&#31639;&#27861;&#20351;&#20010;&#20307;&#26234;&#33021;&#20307;&#33021;&#22815;&#22522;&#20110;&#26412;&#22320;&#20449;&#24687;&#36873;&#25321;&#34892;&#21160;&#12290;&#19982;&#29616;&#26377;&#30340;&#31639;&#27861;&#19981;&#21516;&#65292;LADO&#22312;&#20998;&#24067;&#24335;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#24378;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#40065;&#26834;&#24615;&#35201;&#27714;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;LADO&#31639;&#27861;&#30340;&#24179;&#22343;&#25104;&#26412;&#30028;&#38480;&#65292;&#25581;&#31034;&#20102;&#24179;&#22343;&#24615;&#33021;&#21644;&#26368;&#22351;&#24773;&#20917;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#25240;&#34935;&#12290;</title><link>http://arxiv.org/abs/2306.10158</link><description>&lt;p&gt;
&#22312;&#32593;&#32476;&#20013;&#23398;&#20064;&#22686;&#24378;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning-Augmented Decentralized Online Convex Optimization in Networks. (arXiv:2306.10158v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#22686;&#24378;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65288;LADO&#65289;&#65292;&#35813;&#31639;&#27861;&#20351;&#20010;&#20307;&#26234;&#33021;&#20307;&#33021;&#22815;&#22522;&#20110;&#26412;&#22320;&#20449;&#24687;&#36873;&#25321;&#34892;&#21160;&#12290;&#19982;&#29616;&#26377;&#30340;&#31639;&#27861;&#19981;&#21516;&#65292;LADO&#22312;&#20998;&#24067;&#24335;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#24378;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#40065;&#26834;&#24615;&#35201;&#27714;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;LADO&#31639;&#27861;&#30340;&#24179;&#22343;&#25104;&#26412;&#30028;&#38480;&#65292;&#25581;&#31034;&#20102;&#24179;&#22343;&#24615;&#33021;&#21644;&#26368;&#22351;&#24773;&#20917;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#21270;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21363;&#23398;&#20064;&#22686;&#24378;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20248;&#21270;&#65288;LADO&#65289;&#65292;&#29992;&#20110;&#20351;&#20010;&#20307;&#26234;&#33021;&#20307;&#20165;&#22522;&#20110;&#26412;&#22320;&#22312;&#32447;&#20449;&#24687;&#36873;&#25321;&#34892;&#21160;&#12290;LADO&#21033;&#29992;&#22522;&#32447;&#31574;&#30053;&#26469;&#20445;&#38556;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#25509;&#36817;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31574;&#30053;&#20197;&#25552;&#39640;&#24179;&#22343;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#30340;&#23398;&#20064;&#22686;&#24378;&#22312;&#32447;&#31639;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#31639;&#27861;&#20851;&#27880;&#30340;&#26159;&#38598;&#20013;&#24335;&#35774;&#32622;&#65292;LADO&#22312;&#20998;&#24067;&#24335;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#24378;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;LADO&#30340;&#24179;&#22343;&#25104;&#26412;&#30028;&#38480;&#65292;&#25581;&#31034;&#20102;&#24179;&#22343;&#24615;&#33021;&#21644;&#26368;&#22351;&#24773;&#20917;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#26126;&#30830;&#32771;&#34385;&#40065;&#26834;&#24615;&#35201;&#27714;&#26469;&#35757;&#32451;ML&#31574;&#30053;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies decentralized online convex optimization in a networked multi-agent system and proposes a novel algorithm, Learning-Augmented Decentralized Online optimization (LADO), for individual agents to select actions only based on local online information. LADO leverages a baseline policy to safeguard online actions for worst-case robustness guarantees, while staying close to the machine learning (ML) policy for average performance improvement. In stark contrast with the existing learning-augmented online algorithms that focus on centralized settings, LADO achieves strong robustness guarantees in a decentralized setting. We also prove the average cost bound for LADO, revealing the tradeoff between average performance and worst-case robustness and demonstrating the advantage of training the ML policy by explicitly considering the robustness requirement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#21644;&#22522;&#20934;&#65292;&#29992;&#20110;&#32452;&#21512;&#21644;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#25552;&#20379;&#22810;&#26679;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#27492;&#26694;&#26550;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;4&#31181;&#24120;&#35265;&#30340;MCBO&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.09803</link><description>&lt;p&gt;
&#32452;&#21512;&#21644;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26694;&#26550;&#21644;&#22522;&#20934;&#12290; (arXiv:2306.09803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization. (arXiv:2306.09803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#21644;&#22522;&#20934;&#65292;&#29992;&#20110;&#32452;&#21512;&#21644;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#25552;&#20379;&#22810;&#26679;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#27492;&#26694;&#26550;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;4&#31181;&#24120;&#35265;&#30340;MCBO&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#28151;&#21512;&#21464;&#37327;&#21644;&#32452;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;(MCBO)&#26469;&#35299;&#20915;&#39046;&#22495;&#20013;&#32570;&#20047;&#31995;&#32479;&#21270;&#22522;&#20934;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;MCBO&#35770;&#25991;&#36890;&#24120;&#24341;&#20837;&#38750;&#22810;&#26679;&#24615;&#25110;&#38750;&#26631;&#20934;&#22522;&#20934;&#26469;&#35780;&#20272;&#20854;&#26041;&#27861;&#65292;&#38459;&#30861;&#20102;&#19981;&#21516;MCBO&#21407;&#35821;&#21450;&#20854;&#32452;&#21512;&#30340;&#27491;&#30830;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#20171;&#32461;&#21333;&#20010;MCBO&#21407;&#35821;&#30340;&#35770;&#25991;&#36890;&#24120;&#30465;&#30053;&#20102;&#38024;&#23545;&#20351;&#29992;&#30456;&#21516;&#26041;&#27861;&#36827;&#34892;&#21097;&#20313;&#21407;&#35821;&#30340;&#22522;&#32447;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#31181;&#30465;&#30053;&#20027;&#35201;&#26159;&#30001;&#20110;&#28041;&#21450;&#30340;&#23454;&#29616;&#24037;&#20316;&#37327;&#38750;&#24120;&#22823;&#65292;&#23548;&#33268;&#32570;&#20047;&#25511;&#21046;&#35780;&#20272;&#24182;&#26080;&#27861;&#26377;&#25928;&#23637;&#31034;&#36129;&#29486;&#30340;&#20248;&#28857;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20351;&#36125;&#21494;&#26031;&#20248;&#21270;&#32452;&#20214;&#30340;&#32452;&#21512;&#36731;&#26494;&#26131;&#34892;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#26679;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#27979;&#35797;&#20219;&#21153;&#12290;&#21033;&#29992;&#36825;&#31181;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;4&#31181;&#24120;&#35265;&#30340;MCBO&#25216;&#26415;&#65292;&#24182;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a modular framework for Mixed-variable and Combinatorial Bayesian Optimization (MCBO) to address the lack of systematic benchmarking and standardized evaluation in the field. Current MCBO papers often introduce non-diverse or non-standard benchmarks to evaluate their methods, impeding the proper assessment of different MCBO primitives and their combinations. Additionally, papers introducing a solution for a single MCBO primitive often omit benchmarking against baselines that utilize the same methods for the remaining primitives. This omission is primarily due to the significant implementation overhead involved, resulting in a lack of controlled assessments and an inability to showcase the merits of a contribution effectively. To overcome these challenges, our proposed framework enables an effortless combination of Bayesian Optimization components, and provides a diverse set of synthetic and real-world benchmarking tasks. Leveraging this flexibility, we implement 4
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;(MwT)&#65292;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#27169;&#22411;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#65292;&#36827;&#32780;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.09376</link><description>&lt;p&gt;
&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#27169;&#22359;&#21270;&#65306;&#19968;&#31181;&#26032;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Modularizing while Training: a New Paradigm for Modularizing DNN Models. (arXiv:2306.09376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;(MwT)&#65292;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#27169;&#22411;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#65292;&#36827;&#32780;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#27169;&#22411;&#24050;&#25104;&#20026;&#26234;&#33021;&#36719;&#20214;&#31995;&#32479;&#20013;&#36234;&#26469;&#36234;&#20851;&#38190;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;DNN&#27169;&#22411;&#36890;&#24120;&#22312;&#26102;&#38388;&#21644;&#25104;&#26412;&#26041;&#38754;&#37117;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#24320;&#22987;&#20851;&#27880;&#37325;&#29992;&#29616;&#26377;&#30340;DNN&#27169;&#22411;-&#20511;&#37492;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#20195;&#30721;&#37325;&#29992;&#24605;&#24819;&#12290;&#20294;&#26159;&#65292;&#37325;&#29992;&#25972;&#20010;&#27169;&#22411;&#21487;&#33021;&#20250;&#36896;&#25104;&#39069;&#22806;&#30340;&#24320;&#38144;&#25110;&#20174;&#19981;&#38656;&#35201;&#30340;&#21151;&#33021;&#20013;&#32487;&#25215;&#24369;&#28857;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#25552;&#20986;&#23558;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#20998;&#35299;&#25104;&#27169;&#22359;&#65292;&#21363;&#35757;&#32451;&#21518;&#30340;&#27169;&#22359;&#21270;&#65292;&#24182;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#24182;&#19981;&#26159;&#20026;&#20102;&#27169;&#22359;&#21270;&#32780;&#26500;&#24314;&#30340;&#65292;&#25152;&#20197;&#35757;&#32451;&#21518;&#30340;&#27169;&#22359;&#21270;&#20250;&#23548;&#33268;&#24040;&#22823;&#30340;&#24320;&#38144;&#21644;&#27169;&#22411;&#31934;&#24230;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;&#65288;MwT&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#27169;&#22411;&#20855;&#26377;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#33021;&#21147;&#65292;&#36825;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#21516;&#26102;&#20248;&#21270;&#27169;&#22359;&#20869;&#30340;&#20869;&#32858;&#24615;&#21644;&#27169;&#22359;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#30495;&#27491;&#30340;&#27169;&#22359;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network (DNN) models have become increasingly crucial components in intelligent software systems. However, training a DNN model is typically expensive in terms of both time and money. To address this issue, researchers have recently focused on reusing existing DNN models - borrowing the idea of code reuse in software engineering. However, reusing an entire model could cause extra overhead or inherits the weakness from the undesired functionalities. Hence, existing work proposes to decompose an already trained model into modules, i.e., modularizing-after-training, and enable module reuse. Since trained models are not built for modularization, modularizing-after-training incurs huge overhead and model accuracy loss. In this paper, we propose a novel approach that incorporates modularization into the model training process, i.e., modularizing-while-training (MwT). We train a model to be structurally modular through two loss functions that optimize intra-module cohesion and int
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#22522;&#20934;&#21270;&#23454;&#39564;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#20173;&#28982;&#26159;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#21644;&#39318;&#36873;&#30340;&#36873;&#25321;&#65292;&#24182;&#25581;&#31034;&#20102;&#27169;&#22411;&#20860;&#23481;&#24615;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08954</link><description>&lt;p&gt;
&#37325;&#26032;&#22522;&#20934;&#21270;&#38754;&#21521;&#20108;&#20998;&#31867;&#30340;&#22522;&#20110;&#27744;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Re-Benchmarking Pool-Based Active Learning for Binary Classification. (arXiv:2306.08954v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#22522;&#20934;&#21270;&#23454;&#39564;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#20173;&#28982;&#26159;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#21644;&#39318;&#36873;&#30340;&#36873;&#25321;&#65292;&#24182;&#25581;&#31034;&#20102;&#27169;&#22411;&#20860;&#23481;&#24615;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26174;&#33879;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#33539;&#24335;&#65292;&#24403;&#33719;&#21462;&#26631;&#35760;&#25968;&#25454;&#20195;&#20215;&#26114;&#36149;&#26102;&#29305;&#21035;&#26377;&#29992;&#12290;&#23613;&#31649;&#23384;&#22312;&#22810;&#20010;&#29992;&#20110;&#35780;&#20272;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20294;&#23427;&#20204;&#30340;&#21457;&#29616;&#23384;&#22312;&#19968;&#23450;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#31181;&#24046;&#24322;&#28608;&#21457;&#25105;&#20204;&#20026;&#31038;&#21306;&#24320;&#21457;&#19968;&#20010;&#36879;&#26126;&#19988;&#21487;&#22797;&#29616;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#21162;&#21147;&#32467;&#26524;&#26159;&#19968;&#20010;&#21487;&#38752;&#19988;&#21487;&#25193;&#23637;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#24320;&#28304;&#23454;&#29616;&#65288;https://github.com/ariapoy/active-learning-benchmark&#65289;&#12290;&#36890;&#36807;&#36827;&#34892;&#24443;&#24213;&#30340;&#37325;&#26032;&#22522;&#20934;&#21270;&#23454;&#39564;&#65292;&#25105;&#20204;&#19981;&#20165;&#32416;&#27491;&#20102;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#35823;&#37197;&#32622;&#38382;&#39064;&#65292;&#36824;&#25581;&#31034;&#20102;&#27169;&#22411;&#20860;&#23481;&#24615;&#36825;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#36825;&#30452;&#25509;&#23548;&#33268;&#20102;&#35266;&#23519;&#21040;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#35299;&#20915;&#36825;&#20010;&#24046;&#24322;&#20351;&#24471;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#20445;&#25345;&#20102;&#22312;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#26159;&#19968;&#20010;&#26377;&#25928;&#19988;&#39318;&#36873;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#32463;&#39564;&#24378;&#35843;&#20102;&#23558;&#30740;&#31350;&#21162;&#21147;&#25237;&#20837;&#21040;&#37325;&#26032;&#22522;&#20934;&#21270;&#19978;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning is a paradigm that significantly enhances the performance of machine learning models when acquiring labeled data is expensive. While several benchmarks exist for evaluating active learning strategies, their findings exhibit some misalignment. This discrepancy motivates us to develop a transparent and reproducible benchmark for the community. Our efforts result in an open-sourced implementation (https://github.com/ariapoy/active-learning-benchmark) that is reliable and extensible for future research. By conducting thorough re-benchmarking experiments, we have not only rectified misconfigurations in existing benchmark but also shed light on the under-explored issue of model compatibility, which directly causes the observed discrepancy. Resolving the discrepancy reassures that the uncertainty sampling strategy of active learning remains an effective and preferred choice for most datasets. Our experience highlights the importance of dedicating research efforts towards re-be
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20221;&#35814;&#23613;&#30340;&#25351;&#21335;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#20272;&#35745;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#65292;&#24182;&#27604;&#36739;&#20102;&#22810;&#31181;&#22312;&#32447;&#21644;&#36719;&#20214;&#24037;&#20855;&#30340;&#33021;&#28304;&#28040;&#32791;&#20272;&#35745;&#32467;&#26524;&#12290;&#30740;&#31350;&#20026;AI&#20174;&#19994;&#20154;&#21592;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#24037;&#20855;&#21644;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#25552;&#20379;&#20102;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.08323</link><description>&lt;p&gt;
&#22914;&#20309;&#20272;&#35745;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#65311;&#19968;&#20221;&#25351;&#21335;&#21644;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to estimate carbon footprint when training deep learning models? A guide and review. (arXiv:2306.08323v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08323
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20221;&#35814;&#23613;&#30340;&#25351;&#21335;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#20272;&#35745;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#65292;&#24182;&#27604;&#36739;&#20102;&#22810;&#31181;&#22312;&#32447;&#21644;&#36719;&#20214;&#24037;&#20855;&#30340;&#33021;&#28304;&#28040;&#32791;&#20272;&#35745;&#32467;&#26524;&#12290;&#30740;&#31350;&#20026;AI&#20174;&#19994;&#20154;&#21592;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#24037;&#20855;&#21644;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#25552;&#20379;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#31038;&#20250;&#21508;&#20010;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20154;&#20204;&#26222;&#36941;&#35748;&#35782;&#21040;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#23637;&#23384;&#22312;&#29615;&#22659;&#25104;&#26412;&#65292;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#23545;&#27492;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#22312;&#32447;&#21644;&#36719;&#20214;&#24037;&#20855;&#26469;&#36319;&#36394;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#20840;&#38754;&#20171;&#32461;&#21644;&#27604;&#36739;&#65292;&#24182;&#38024;&#23545;&#24076;&#26395;&#24320;&#22987;&#20272;&#35745;&#20854;&#24037;&#20316;&#29615;&#22659;&#24433;&#21709;&#30340;AI&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#25105;&#20204;&#23545;&#27599;&#20010;&#24037;&#20855;&#30340;&#29305;&#23450;&#35789;&#27719;&#21644;&#25216;&#26415;&#35201;&#27714;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#20123;&#24037;&#20855;&#23545;&#20004;&#20010;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#19981;&#21516;&#31867;&#22411;&#26381;&#21153;&#22120;&#30340;&#33021;&#28304;&#28040;&#32791;&#20272;&#35745;&#32467;&#26524;&#12290;&#26681;&#25454;&#36825;&#20123;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#24314;&#35758;&#65292;&#20197;&#26356;&#22909;&#22320;&#36873;&#25321;&#21512;&#36866;&#30340;&#24037;&#20855;&#21644;&#22522;&#30784;&#35774;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning and deep learning models have become essential in the recent fast development of artificial intelligence in many sectors of the society. It is now widely acknowledge that the development of these models has an environmental cost that has been analyzed in many studies. Several online and software tools have been developed to track energy consumption while training machine learning models. In this paper, we propose a comprehensive introduction and comparison of these tools for AI practitioners wishing to start estimating the environmental impact of their work. We review the specific vocabulary, the technical requirements for each tool. We compare the energy consumption estimated by each tool on two deep neural networks for image processing and on different types of servers. From these experiments, we provide some advice for better choosing the right tool and infrastructure.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#21644;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#26469;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.07215</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#19982;&#33258;&#36866;&#24212;&#26680;&#24515;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Efficient Quantization-aware Training with Adaptive Coreset Selection. (arXiv:2306.07215v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#21644;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#26469;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#37327;&#30340;&#22686;&#21152;&#65292;&#22686;&#21152;&#20102;&#23545;&#26377;&#25928;&#27169;&#22411;&#37096;&#32626;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#26159;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26435;&#37325;&#21644;&#28608;&#27963;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;QAT&#26041;&#27861;&#38656;&#35201;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36825;&#20250;&#23548;&#33268;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21644;&#39640;&#33021;&#32791;&#12290;&#26680;&#24515;&#38598;&#36873;&#25321;&#26159;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#20887;&#20313;&#24615;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#22312;&#39640;&#25928;&#35757;&#32451;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#25552;&#39640;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#22522;&#20110;QAT&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25351;&#26631;&#65306;&#35823;&#24046;&#21521;&#37327;&#20998;&#25968;&#21644;&#19981;&#19968;&#33268;&#20998;&#25968;&#65292;&#29992;&#20110;&#37327;&#21270;&#35757;&#32451;&#36807;&#31243;&#20013;&#27599;&#20010;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;ACS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expanding model size and computation of deep neural networks (DNNs) have increased the demand for efficient model deployment methods. Quantization-aware training (QAT) is a representative model compression method to leverage redundancy in weights and activations. However, most existing QAT methods require end-to-end training on the entire dataset, which suffers from long training time and high energy costs. Coreset selection, aiming to improve data efficiency utilizing the redundancy of training data, has also been widely used for efficient training. In this work, we propose a new angle through the coreset selection to improve the training efficiency of quantization-aware training. Based on the characteristics of QAT, we propose two metrics: error vector score and disagreement score, to quantify the importance of each sample during training. Guided by these two metrics of importance, we proposed a quantization-aware adaptive coreset selection (ACS) method to select the data for the
&lt;/p&gt;</description></item><item><title>PEAR&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26041;&#27861;&#65292;&#29992;&#20110;Boosting&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#12290;&#23427;&#36890;&#36807;&#23545;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26469;&#29983;&#25104;&#39640;&#25928;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#26469;&#35757;&#32451;&#20998;&#23618;&#20195;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PEAR&#33021;&#22815;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06394</link><description>&lt;p&gt;
PEAR: &#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#29992;&#20110;Boosting&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning. (arXiv:2306.06394v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06394
&lt;/p&gt;
&lt;p&gt;
PEAR&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26041;&#27861;&#65292;&#29992;&#20110;Boosting&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#12290;&#23427;&#36890;&#36807;&#23545;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26469;&#29983;&#25104;&#39640;&#25928;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#26469;&#35757;&#32451;&#20998;&#23618;&#20195;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PEAR&#33021;&#22815;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#21033;&#29992;&#26102;&#38388;&#25277;&#35937;&#21644;&#22686;&#21152;&#30340;&#25506;&#32034;&#24615;&#33021;&#35299;&#20915;&#22797;&#26434;&#30340;&#38271;&#26399;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#38750;&#38745;&#24577;&#24615;&#65292;&#20998;&#23618;&#20195;&#29702;&#38590;&#20197;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#65288;PEAR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#23569;&#37327;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#65292;&#20135;&#29983;&#39640;&#25928;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;&#65292;&#28982;&#21518;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#32852;&#21512;&#20248;&#21270;HRL&#20195;&#29702;&#12290;&#25105;&#20204;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#26469;$(i)$&#38480;&#21046;&#25105;&#20204;&#26041;&#27861;&#30340;&#27425;&#20248;&#24615;&#65292;&#21644;$(ii)$&#25512;&#23548;&#20986;&#20351;&#29992;RL&#21644;IL&#30340;&#24191;&#20041;&#21363;&#25554;&#21363;&#29992;&#30340;&#26694;&#26550;&#36827;&#34892;&#32852;&#21512;&#20248;&#21270;&#12290;PEAR&#20351;&#29992;&#19968;&#20123;&#19987;&#23478;&#28436;&#31034;&#65292;&#24182;&#23545;&#20219;&#21153;&#32467;&#26500;&#36827;&#34892;&#26368;&#23567;&#30340;&#38480;&#21046;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#19982;&#20856;&#22411;&#30340;&#27169;&#22411;&#33258;&#30001;RL&#31639;&#27861;&#38598;&#25104;&#65292;&#20135;&#29983;&#19968;&#20010;&#23454;&#29992;&#30340;HRL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning (HRL) has the potential to solve complex long horizon tasks using temporal abstraction and increased exploration. However, hierarchical agents are difficult to train due to inherent non-stationarity. We present primitive enabled adaptive relabeling (PEAR), a two-phase approach where we first perform adaptive relabeling on a few expert demonstrations to generate efficient subgoal supervision, and then jointly optimize HRL agents by employing reinforcement learning (RL) and imitation learning (IL). We perform theoretical analysis to $(i)$ bound the sub-optimality of our approach, and $(ii)$ derive a generalized plug-and-play framework for joint optimization using RL and IL. PEAR uses a handful of expert demonstrations and makes minimal limiting assumptions on the task structure. Additionally, it can be easily integrated with typical model free RL algorithms to produce a practical HRL algorithm. We perform experiments on challenging robotic environments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#30740;&#31350;&#12290;&#21015;&#20986;&#20102;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#34920;&#26126;&#20102;&#26412;&#39046;&#22495;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.06123</link><description>&lt;p&gt;
&#12298;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#65306;&#35843;&#26597;&#25253;&#21578;&#12299;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey. (arXiv:2306.06123v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#30740;&#31350;&#12290;&#21015;&#20986;&#20102;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#34920;&#26126;&#20102;&#26412;&#39046;&#22495;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#34987;&#25551;&#32472;&#20026;&#35843;&#35797;&#21644;&#20449;&#20219;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27835;&#30103;&#26041;&#24335;&#65292;&#20197;&#21450;&#35299;&#37322;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#31361;&#20986;&#20102;&#26368;&#26032;&#35299;&#37322;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#36825;&#20123;&#36827;&#23637;&#20196;&#20154;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#20135;&#29983;&#36136;&#30097;&#12290;&#25805;&#32437;&#12289;&#27450;&#39575;&#25110;&#27927;&#30333;&#27169;&#22411;&#25512;&#29702;&#35777;&#25454;&#30340;&#21487;&#33021;&#24615;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#21644;&#30693;&#35782;&#21457;&#29616;&#20013;&#20135;&#29983;&#19981;&#21033;&#21518;&#26524;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;50&#22810;&#31687;&#35770;&#25991;&#30340;&#30740;&#31350;&#65292;&#27010;&#36848;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#30340;&#23545;&#25239;&#25915;&#20987;&#20197;&#21450;&#20844;&#24179;&#24230;&#37327;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#38450;&#24481;&#25915;&#20987;&#24182;&#35774;&#35745;&#40065;&#26834;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#21015;&#20986;XAI&#20013;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#27010;&#36848;&#20102;&#23545;&#25239;&#24615;XAI&#65288;AdvXAI&#65289;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning highlight the limitations and vulnerabilities of state-of-the-art explanations, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This concise survey of over 50 papers summarizes research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI).
&lt;/p&gt;</description></item><item><title>RescueSpeech&#26159;&#19968;&#20010;&#29992;&#20110;&#25628;&#25937;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#30340;&#24503;&#35821;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#20294;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20173;&#26080;&#27861;&#20196;&#20154;&#28385;&#24847;&#12290;</title><link>http://arxiv.org/abs/2306.04054</link><description>&lt;p&gt;
RescueSpeech&#65306;&#29992;&#20110;&#25628;&#25937;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#30340;&#24503;&#35821;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
RescueSpeech: A German Corpus for Speech Recognition in Search and Rescue Domain. (arXiv:2306.04054v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04054
&lt;/p&gt;
&lt;p&gt;
RescueSpeech&#26159;&#19968;&#20010;&#29992;&#20110;&#25628;&#25937;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#30340;&#24503;&#35821;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#20294;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20173;&#26080;&#27861;&#20196;&#20154;&#28385;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#38899;&#35782;&#21035;&#22312;&#26368;&#36817;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20294;&#22312;&#22024;&#26434;&#12289;&#22238;&#22768;&#30340;&#29615;&#22659;&#20013;&#20934;&#30830;&#36716;&#24405;&#23545;&#35805;&#21644;&#24773;&#24863;&#34920;&#36798;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#38590;&#24230;&#12290;&#22312;&#25628;&#25937;&#39046;&#22495;&#23588;&#20854;&#22914;&#27492;&#65292;&#22240;&#20026;&#36716;&#24405;&#25937;&#25588;&#38431;&#25104;&#21592;&#20043;&#38388;&#30340;&#23545;&#35805;&#23545;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#25628;&#25937;&#22330;&#26223;&#20013;&#35821;&#38899;&#25968;&#25454;&#21644;&#30456;&#20851;&#32972;&#26223;&#22122;&#22768;&#30340;&#31232;&#32570;&#24615;&#20351;&#24471;&#37096;&#32626;&#20581;&#22766;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#24182;&#20844;&#24320;&#20102;&#19968;&#20010;&#21517;&#20026;RescueSpeech&#30340;&#24503;&#35821;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#27169;&#25311;&#25937;&#25588;&#28436;&#20064;&#30340;&#30495;&#23454;&#35821;&#38899;&#24405;&#38899;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#31454;&#20105;&#24615;&#35757;&#32451;&#37197;&#26041;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25152;&#36798;&#21040;&#30340;&#24615;&#33021;&#27700;&#24179;&#20173;&#36828;&#26410;&#33021;&#20196;&#20154;&#28385;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advancements in speech recognition, there are still difficulties in accurately transcribing conversational and emotional speech in noisy and reverberant acoustic environments. This poses a particular challenge in the search and rescue (SAR) domain, where transcribing conversations among rescue team members is crucial to support real-time decision-making. The scarcity of speech data and associated background noise in SAR scenarios make it difficult to deploy robust speech recognition systems.  To address this issue, we have created and made publicly available a German speech dataset called RescueSpeech. This dataset includes real speech recordings from simulated rescue exercises. Additionally, we have released competitive training recipes and pre-trained models. Our study indicates that the current level of performance achieved by state-of-the-art methods is still far from being acceptable.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#20803;&#28608;&#27963;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#20803;&#28608;&#27963;&#35206;&#30422;&#24230;&#65288;NAC&#65289;&#20316;&#20026;&#34913;&#37327;&#31070;&#32463;&#20803;&#34892;&#20026;&#30340;&#25351;&#26631;&#12290;&#21033;&#29992;NAC&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#22495;&#20869;&#21644;&#31163;&#22495;&#36755;&#20837;&#65292;&#31616;&#21270;&#31163;&#22495;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#19988;NAC&#19982;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.02879</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#28608;&#27963;&#35206;&#30422;&#24230;&#65306;&#37325;&#26032;&#24605;&#32771;&#31163;&#22495;&#26816;&#27979;&#21644;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization. (arXiv:2306.02879v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#20803;&#28608;&#27963;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#20803;&#28608;&#27963;&#35206;&#30422;&#24230;&#65288;NAC&#65289;&#20316;&#20026;&#34913;&#37327;&#31070;&#32463;&#20803;&#34892;&#20026;&#30340;&#25351;&#26631;&#12290;&#21033;&#29992;NAC&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#22495;&#20869;&#21644;&#31163;&#22495;&#36755;&#20837;&#65292;&#31616;&#21270;&#31163;&#22495;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#19988;NAC&#19982;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#22495;&#38382;&#39064;&#36890;&#24120;&#22312;&#31070;&#32463;&#32593;&#32476;&#36935;&#21040;&#26126;&#26174;&#20559;&#31163;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#25968;&#25454;&#26102;&#20986;&#29616;&#65292;&#21363;&#22312;&#22495;&#20869;&#25968;&#25454;&#65288;InD&#65289;&#20043;&#22806;&#12290;&#26412;&#25991;&#20174;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#31163;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32771;&#34385;&#31070;&#32463;&#20803;&#30340;&#36755;&#20986;&#21644;&#20854;&#23545;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#26469;&#23450;&#20041;&#20102;&#31070;&#32463;&#20803;&#28608;&#27963;&#29366;&#24577;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#25551;&#36848;&#31070;&#32463;&#20803;&#19982;&#31163;&#22495;&#38382;&#39064;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#20803;&#28608;&#27963;&#35206;&#30422;&#24230;&#65288;NAC&#65289;&#8212;&#8212;&#19968;&#31181;&#34913;&#37327;&#31070;&#32463;&#20803;&#22312;&#22495;&#20869;&#25968;&#25454;&#19979;&#34892;&#20026;&#30340;&#31616;&#21333;&#24230;&#37327;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;NAC&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;1&#65289;&#22522;&#20110;&#31070;&#32463;&#20803;&#34892;&#20026;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21306;&#20998;&#22495;&#20869;&#21644;&#31163;&#22495;&#36755;&#20837;&#65292;&#22823;&#22823;&#31616;&#21270;&#20102;&#31163;&#22495;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#65288;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet-1K&#65289;&#19978;&#36229;&#36807;&#20102;21&#20010;&#20808;&#21069;&#30340;&#26041;&#27861;&#65307;2&#65289;NAC&#19982;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#20851;&#31995;&#65292;&#36825;&#31181;&#20851;&#31995;&#22312;&#19981;&#21516;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#19968;&#33268;&#25104;&#31435;&#65292;&#20351;&#24471;&#22522;&#20110;NAC&#30340;&#20934;&#21017;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The out-of-distribution (OOD) problem generally arises when neural networks encounter data that significantly deviates from the training data distribution, i.e., in-distribution (InD). In this paper, we study the OOD problem from a neuron activation view. We first formulate neuron activation states by considering both the neuron output and its influence on model decisions. Then, to characterize the relationship between neurons and OOD issues, we introduce the \textit{neuron activation coverage} (NAC) -- a simple measure for neuron behaviors under InD data. Leveraging our NAC, we show that 1) InD and OOD inputs can be largely separated based on the neuron behavior, which significantly eases the OOD detection problem and beats the 21 previous methods over three benchmarks (CIFAR-10, CIFAR-100, and ImageNet-1K). 2) a positive correlation between NAC and model generalization ability consistently holds across architectures and datasets, which enables a NAC-based criterion for evaluating mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#27867;&#21270;&#21040;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#30340;&#31070;&#32463;&#36807;&#31243;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#31614;&#33719;&#21462;&#25968;&#25454;&#20197;&#21450;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#25110;&#33719;&#21462;&#20989;&#25968;&#30340;&#20256;&#32479;Meta-BO&#26041;&#27861;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.15930</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#31070;&#32463;&#36807;&#31243;&#30340;&#31471;&#21040;&#31471;Meta-Bayesian&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes. (arXiv:2305.15930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#27867;&#21270;&#21040;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#30340;&#31070;&#32463;&#36807;&#31243;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#31614;&#33719;&#21462;&#25968;&#25454;&#20197;&#21450;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#25110;&#33719;&#21462;&#20989;&#25968;&#30340;&#20256;&#32479;Meta-BO&#26041;&#27861;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;Meta-Bayesian optimization&#65292;Meta-BO&#65289;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#22320;&#29420;&#31435;&#20803;&#23398;&#20064;&#36807;&#20195;&#29702;&#27169;&#22411;&#25110;&#33719;&#21462;&#20989;&#25968;&#65292;&#20294;&#26159;&#21516;&#26102;&#35757;&#32451;&#36825;&#20004;&#20010;&#32452;&#20214;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;Meta-BO&#26694;&#26550;&#65292;&#36890;&#36807;Transformer&#20307;&#31995;&#32467;&#26500;&#23558;&#31070;&#32463;&#36807;&#31243;&#27867;&#21270;&#21040;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20351;&#36825;&#31181;&#31471;&#21040;&#31471;&#26694;&#26550;&#20855;&#26377;&#22788;&#29702;&#32570;&#20047;&#26631;&#31614;&#33719;&#21462;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency of Bayesian optimisation by leveraging data from related tasks. While previous methods successfully meta-learn either a surrogate model or an acquisition function independently, joint training of both components remains an open challenge. This paper proposes the first end-to-end differentiable meta-BO framework that generalises neural processes to learn acquisition functions via transformer architectures. We enable this end-to-end framework with reinforcement learning (RL) to tackle the lack of labelled acquisition data. Early on, we notice that training transformer-based neural processes from scratch with RL is challenging due to insufficient supervision, especially when rewards are sparse. We formalise this claim with a combinatorial analysis showing that the widely used notion of regret as a reward signal exhibits a logarithmic sparsity pattern in trajectory lengths. To tackle this problem, we augment the RL 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#20248;&#20110;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15703</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#22909;&#22788;&#65306;&#23567;&#25439;&#22833;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning. (arXiv:2305.15703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15703
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#20248;&#20110;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#26524;&#65292;&#20294;&#20854;&#20309;&#26102;&#20309;&#22320;&#26377;&#30410;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#22238;&#31572;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#25105;&#20204;&#30340;&#36793;&#30028;&#20250;&#27604;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#26356;&#24378;&#12290;&#20316;&#20026;&#28909;&#36523;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#25104;&#26412;&#20998;&#24067;&#20250;&#22312;&#24773;&#22659;&#23637;&#24320;&#65288;CB&#65289;&#20013;&#23548;&#33268;&#23567;&#25439;&#22833;&#21518;&#24724;&#36793;&#30028;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#24067;&#24335;CB&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#22312;&#23454;&#35777;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#20110;&#22312;&#32447;RL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#29256;&#26412;&#31354;&#38388;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#34920;&#26684;MDP&#20013;&#23454;&#29616;&#20102;&#23567;&#25439;&#22833;&#21518;&#24724;&#65292;&#21516;&#26102;&#22312;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#20139;&#26377;&#23567;&#25439;&#22833;PAC&#36793;&#30028;&#12290;&#20197;&#31867;&#20284;&#30340;&#35265;&#35299;&#20026;&#22522;&#30784;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#31163;&#32447;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
While distributional reinforcement learning (RL) has demonstrated empirical success, the question of when and why it is beneficial has remained unanswered. In this work, we provide one explanation for the benefits of distributional RL through the lens of small-loss bounds, which scale with the instance-dependent optimal cost. If the optimal cost is small, our bounds are stronger than those from non-distributional approaches. As warmup, we show that learning the cost distribution leads to small-loss regret bounds in contextual bandits (CB), and we find that distributional CB empirically outperforms the state-of-the-art on three challenging tasks. For online RL, we propose a distributional version-space algorithm that constructs confidence sets using maximum likelihood estimation, and we prove that it achieves small-loss regret in the tabular MDPs and enjoys small-loss PAC bounds in latent variable models. Building on similar insights, we propose a distributional offline RL algorithm bas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#19968;&#20123;&#24120;&#35265;&#30340;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#20294;&#20351;&#29992;&#24102;&#26377;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;BBVI&#21487;&#20197;&#23454;&#29616;&#26368;&#24378;&#25910;&#25947;&#29575;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.15349</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Black-Box Variational Inference Converges. (arXiv:2305.15349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15349
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#19968;&#20123;&#24120;&#35265;&#30340;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#20294;&#20351;&#29992;&#24102;&#26377;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;BBVI&#21487;&#20197;&#23454;&#29616;&#26368;&#24378;&#25910;&#25947;&#29575;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#23436;&#25972;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#20063;&#31216;&#20026;&#33945;&#29305;&#21345;&#32599;&#21464;&#20998;&#25512;&#26029;&#12290;&#23613;&#31649;&#26089;&#26399;&#30340;&#30740;&#31350;&#21482;&#38024;&#23545;&#31616;&#21270;&#29256;&#26412;&#30340;BBVI&#36827;&#34892;&#20102;&#30740;&#31350;&#65288;&#20363;&#22914;&#65292;&#26377;&#30028;&#22495;&#12289;&#26377;&#30028;&#25903;&#25345;&#12289;&#20165;&#38024;&#23545;&#23610;&#24230;&#36827;&#34892;&#20248;&#21270;&#31561;&#65289;&#65292;&#20294;&#25105;&#20204;&#30340;&#35774;&#32622;&#19981;&#38656;&#35201;&#20219;&#20309;&#36825;&#26679;&#30340;&#31639;&#27861;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#23545;&#25968;&#24179;&#28369;&#21518;&#39564;&#23494;&#24230;&#65292;&#26080;&#35770;&#26159;&#21542;&#24378;&#23545;&#25968;&#20985;&#24615;&#20197;&#21450;&#20301;&#32622;-&#23610;&#24230;&#21464;&#20998;&#26063;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;&#19968;&#20123;&#24120;&#35265;&#30340;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#65292;&#29305;&#21035;&#26159;&#21464;&#20998;&#36817;&#20284;&#23610;&#24230;&#30340;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36816;&#34892;&#24102;&#26377;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;BBVI&#21487;&#20197;&#32416;&#27491;&#36825;&#20123;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#24050;&#30693;&#30340;&#26368;&#24378;&#25910;&#25947;&#29575;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36817;&#31471;SGD&#19982;&#20854;&#20182;&#26631;&#20934;&#30340;BBVI&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#36825;&#19968;&#29702;&#35770;&#32467;&#35770;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide the first convergence guarantee for full black-box variational inference (BBVI), also known as Monte Carlo variational inference. While preliminary investigations worked on simplified versions of BBVI (e.g., bounded domain, bounded support, only optimizing for the scale, and such), our setup does not need any such algorithmic modifications. Our results hold for log-smooth posterior densities with and without strong log-concavity and the location-scale variational family. Also, our analysis reveals that certain algorithm design choices commonly employed in practice, particularly, nonlinear parameterizations of the scale of the variational approximation, can result in suboptimal convergence rates. Fortunately, running BBVI with proximal stochastic gradient descent fixes these limitations, and thus achieves the strongest known convergence rate guarantees. We evaluate this theoretical insight by comparing proximal SGD against other standard implementations of BBVI on large-scale
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#38750;&#21442;&#25968;&#36716;&#25442;&#22120;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#29305;&#24449;&#19982;&#29305;&#24449;&#20043;&#38388;&#20197;&#21450;&#26679;&#26412;&#19982;&#26679;&#26412;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15121</link><description>&lt;p&gt;
&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#36827;&#34892;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#30340;&#36229;&#36234;&#20010;&#20307;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Beyond Individual Input for Deep Anomaly Detection on Tabular Data. (arXiv:2305.15121v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#38750;&#21442;&#25968;&#36716;&#25442;&#22120;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#29305;&#24449;&#19982;&#29305;&#24449;&#20043;&#38388;&#20197;&#21450;&#26679;&#26412;&#19982;&#26679;&#26412;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#21508;&#20010;&#39046;&#22495;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38750;&#21442;&#25968;&#36716;&#25442;&#22120;&#65288;NPTs&#65289;&#30340;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#29305;&#24449;&#19982;&#29305;&#24449;&#20043;&#38388;&#20197;&#21450;&#26679;&#26412;&#19982;&#26679;&#26412;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#22522;&#20110;&#37325;&#26500;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;NPT&#26469;&#37325;&#26500;&#27491;&#24120;&#26679;&#26412;&#30340;&#36974;&#34109;&#29305;&#24449;&#12290;&#20197;&#38750;&#21442;&#25968;&#21270;&#26041;&#24335;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25972;&#20010;&#35757;&#32451;&#38598;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#22312;&#29983;&#25104;&#24322;&#24120;&#24471;&#20998;&#26102;&#37325;&#26500;&#36974;&#34109;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#25104;&#21151;&#32467;&#21512;&#29305;&#24449;&#20043;&#38388;&#21644;&#26679;&#26412;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;31&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#24471;&#20998;&#21644;AUROC&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is crucial in various domains, such as finance, healthcare, and cybersecurity. In this paper, we propose a novel deep anomaly detection method for tabular data that leverages Non-Parametric Transformers (NPTs), a model initially proposed for supervised tasks, to capture both feature-feature and sample-sample dependencies. In a reconstruction-based framework, we train the NPT to reconstruct masked features of normal samples. In a non-parametric fashion, we leverage the whole training set during inference and use the model's ability to reconstruct the masked features during to generate an anomaly score. To the best of our knowledge, our proposed method is the first to successfully combine feature-feature and sample-sample dependencies for anomaly detection on tabular datasets. We evaluate our method on an extensive benchmark of 31 tabular datasets and demonstrate that our approach outperforms existing state-of-the-art methods based on the F1-score and AUROC by a signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.14375</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#22270;&#34701;&#21512;&#30340;&#36947;&#36335;&#32593;&#32476;&#33410;&#28857;&#37325;&#35201;&#24615;&#25490;&#24207;&#26041;&#27861;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank the Importance of Nodes in Road Networks Based on Multi-Graph Fusion. (arXiv:2305.14375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#35268;&#21010;&#39046;&#22495;&#20013;&#65292;&#35782;&#21035;&#20855;&#26377;&#24378;&#20256;&#25773;&#33021;&#21147;&#30340;&#37325;&#35201;&#33410;&#28857;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#33410;&#28857;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#20165;&#32771;&#34385;&#25299;&#25169;&#20449;&#24687;&#21644;&#20132;&#36890;&#27969;&#37327;&#65292;&#24573;&#30053;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#22810;&#26679;&#24615;&#29305;&#24449;&#65292;&#22914;&#36710;&#36947;&#25968;&#37327;&#21644;&#36947;&#36335;&#27573;&#30340;&#24179;&#22343;&#36895;&#24230;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#37319;&#26679;&#31639;&#27861;&#65288;MGWalk&#65289;&#65292;&#21033;&#29992;&#22810;&#22270;&#34701;&#21512;&#26469;&#24314;&#31435;&#22522;&#20110;&#23646;&#24615;&#30340;&#36947;&#36335;&#27573;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23884;&#20837;&#27169;&#22359;&#65292;&#29992;&#20110;&#23398;&#20064;&#27599;&#20010;&#36947;&#36335;&#27573;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#24471;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#29992;&#20110;&#23398;&#20064;&#36947;&#36335;&#27573;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#20013;&#22269;&#27784;&#38451;&#24066;&#21306;&#22495;&#36947;&#36335;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#20223;&#30495;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;MGL2Rank&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MGL2Rank&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying important nodes with strong propagation capabilities in road networks is a significant topic in the field of urban planning. However, existing methods for evaluating nodes importance consider only topological information and traffic volumes, ignoring the diversity of characteristics in road networks, such as the number of lanes and average speed of road segments, limiting their performance. To address this issue, this paper proposes a graph learning-based node ranking method (MGL2Rank) that integrates the rich characteristics of the road network. In this method, we first develop a sampling algorithm (MGWalk) that utilizes multi-graph fusion to establish association between road segments based on their attributes. Then, an embedding module is proposed to learn latent representation for each road segment. Finally, the obtained node representation is used to learn importance ranking of road segments. We conduct simulation experiments on the regional road network of Shenyang ci
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;QFA2SR&#65292;&#19968;&#31181;&#26080;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#22768;&#38899;&#30340;&#21487;&#20256;&#36882;&#24615;&#65292;&#36890;&#36807;&#23450;&#21046;&#25439;&#22833;&#20989;&#25968;&#12289;SRS&#38598;&#25104;&#21644;&#26102;&#39057;&#33104;&#34432;&#31561;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#19988;&#19981;&#21487;&#23519;&#35273;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2305.14097</link><description>&lt;p&gt;
QFA2SR: &#26080;&#26597;&#35810;&#30340;&#23545;&#25239;&#36801;&#31227;&#25915;&#20987;&#20197;&#23545;&#25239;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
QFA2SR: Query-Free Adversarial Transfer Attacks to Speaker Recognition Systems. (arXiv:2305.14097v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;QFA2SR&#65292;&#19968;&#31181;&#26080;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#22768;&#38899;&#30340;&#21487;&#20256;&#36882;&#24615;&#65292;&#36890;&#36807;&#23450;&#21046;&#25439;&#22833;&#20989;&#25968;&#12289;SRS&#38598;&#25104;&#21644;&#26102;&#39057;&#33104;&#34432;&#31561;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#19988;&#19981;&#21487;&#23519;&#35273;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#38024;&#23545;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#65288;SRSs&#65289;&#30340;&#23545;&#25239;&#25915;&#20987;&#35201;&#20040;&#38656;&#35201;&#30333;&#30418;&#35775;&#38382;&#65292;&#35201;&#20040;&#38656;&#35201;&#23545;&#30446;&#26631;SRS&#36827;&#34892;&#22823;&#37327;&#30340;&#40657;&#30418;&#26597;&#35810;&#65292;&#22240;&#27492;&#20173;&#28982;&#33853;&#21518;&#20110;&#38024;&#23545;&#19987;&#26377;&#21830;&#19994;API&#21644;&#35821;&#38899;&#25511;&#21046;&#35774;&#22791;&#30340;&#23454;&#38469;&#25915;&#20987;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QFA2SR&#65292;&#19968;&#31181;&#26377;&#25928;&#19988;&#19981;&#21487;&#23519;&#35273;&#30340;&#26080;&#26597;&#35810;&#40657;&#30418;&#25915;&#20987;&#65292;&#21033;&#29992;&#23545;&#25239;&#24615;&#22768;&#38899;&#30340;&#21487;&#20256;&#36882;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#20256;&#36882;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#26041;&#27861;&#65292;&#23450;&#21046;&#25439;&#22833;&#20989;&#25968;&#12289;SRS&#38598;&#25104;&#21644;&#26102;&#39057;&#33104;&#34432;&#12290;&#31532;&#19968;&#20010;&#26041;&#27861;&#23558;&#25439;&#22833;&#20989;&#25968;&#23450;&#21046;&#20026;&#19981;&#21516;&#30340;&#25915;&#20987;&#22330;&#26223;&#12290;&#21518;&#20004;&#31181;&#26041;&#27861;&#20197;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#22686;&#24378;&#26367;&#20195;SRS&#12290;SRS&#38598;&#25104;&#23558;&#22810;&#26679;&#21270;&#30340;&#26367;&#20195;SRS&#19982;&#26032;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#36866;&#24212;SRS&#30340;&#29420;&#29305;&#35780;&#20998;&#29305;&#24615;&#12290;&#26102;&#39057;&#33104;&#34432;&#36890;&#36807;&#24341;&#20837;&#31934;&#24515;&#35774;&#35745;&#30340;&#26102;&#22495;/&#39057;&#22495;&#20462;&#25913;&#20989;&#25968;&#22686;&#24378;&#26367;&#20195;SRS&#65292;&#27169;&#25311;&#21644;&#36817;&#20284;&#30446;&#26631;SRS&#30340;&#20915;&#31574;&#36793;&#30028;&#21644;&#22833;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current adversarial attacks against speaker recognition systems (SRSs) require either white-box access or heavy black-box queries to the target SRS, thus still falling behind practical attacks against proprietary commercial APIs and voice-controlled devices. To fill this gap, we propose QFA2SR, an effective and imperceptible query-free black-box attack, by leveraging the transferability of adversarial voices. To improve transferability, we present three novel methods, tailored loss functions, SRS ensemble, and time-freq corrosion. The first one tailors loss functions to different attack scenarios. The latter two augment surrogate SRSs in two different ways. SRS ensemble combines diverse surrogate SRSs with new strategies, amenable to the unique scoring characteristics of SRSs. Time-freq corrosion augments surrogate SRSs by incorporating well-designed time-/frequency-domain modification functions, which simulate and approximate the decision boundary of the target SRS and distortions int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#26681;&#25454;cGAN&#30340;&#21028;&#21035;&#22120;&#25968;&#25454;&#35782;&#21035;&#20986;&#26368;&#25509;&#36817;&#30446;&#26631;&#30340;&#29616;&#26377;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#22238;&#25918;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#30446;&#26631;&#27169;&#24335;&#30340;cGAN&#27169;&#22411;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11400</link><description>&lt;p&gt;
&#38754;&#21521;&#26377;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Continual Learning for Conditional Generative Adversarial Networks. (arXiv:2305.11400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#26681;&#25454;cGAN&#30340;&#21028;&#21035;&#22120;&#25968;&#25454;&#35782;&#21035;&#20986;&#26368;&#25509;&#36817;&#30446;&#26631;&#30340;&#29616;&#26377;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#22238;&#25918;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#30446;&#26631;&#27169;&#24335;&#30340;cGAN&#27169;&#22411;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#24517;&#39035;&#23398;&#20064;&#30446;&#26631;&#27169;&#24335;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#20165;&#20351;&#29992;&#26377;&#38480;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#38024;&#23545;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#27169;&#24335;&#20146;&#21644;&#21147;&#37327;&#24230;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#23436;&#20840;&#22522;&#20110;cGAN&#30340;&#21028;&#21035;&#22120;&#65292;&#21487;&#20197;&#35782;&#21035;&#26368;&#25509;&#36817;&#30446;&#26631;&#30340;&#29616;&#26377;&#27169;&#24335;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21253;&#21547;&#22522;&#20110;&#26368;&#25509;&#36817;&#27169;&#24335;&#30340;&#21152;&#26435;&#26631;&#31614;&#26469;&#25193;&#23637;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#39044;&#38450;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;cGAN&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#28982;&#21518;&#36890;&#36807;&#22238;&#25918;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#30446;&#26631;&#27169;&#24335;&#30340;cGAN&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36234;&#20102;&#21508;&#31181;&#26631;&#20934;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In few-shot continual learning for generative models, a target mode must be learned with limited samples without adversely affecting the previously learned modes. In this paper, we propose a new continual learning approach for conditional generative adversarial networks (cGAN) based on a new mode-affinity measure for generative modeling. Our measure is entirely based on the cGAN's discriminator and can identify the existing modes that are most similar to the target. Subsequently, we expand the continual learning model by including the target mode using a weighted label derived from those of the closest modes. To prevent catastrophic forgetting, we first generate labeled data samples using the cGAN's generator, and then train the cGAN model for the target mode while memory replaying with the generated data. Our experimental results demonstrate the efficacy of our approach in improving the generation performance over the baselines and the state-of-the-art approaches for various standard 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.11283</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;&#65288;MFC&#65289;&#21644;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Mean-Field Model-Based Eluder Dimension (MBED)&#30340;&#26032;&#27010;&#24565;&#65292;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#20016;&#23500;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36820;&#22238;&#19968;&#20010;$\epsilon$&#20248;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;MFC&#25110;$\epsilon$&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#36866;&#29992;&#20110;MFG&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#22810;&#39033;&#24335;&#19982;&#30456;&#20851;&#21442;&#25968;&#26080;&#20851;&#65292;&#19982;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#20195;&#29702;&#25968;&#37327;&#26080;&#20851;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#36991;&#20813;&#20102;&#20197;&#21069;&#30340;&#24378;&#32467;&#26500;&#20551;&#35774;&#12290;&#26368;&#21518;&#65292;&#22312;tabular&#35774;&#32622;&#19979;&#65292;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#39640;&#25928;&#30340;&#27169;&#22411;&#28040;&#38500;&#31639;&#27861;&#20197;&#36924;&#36817;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20026;&#24378;&#22823;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20013;&#21516;&#19968;&#38544;&#34255;&#23618;&#20013;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#30456;&#20114;&#36830;&#25509;&#65292;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#24182;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10468</link><description>&lt;p&gt;
&#36830;&#25509;&#38544;&#34255;&#31070;&#32463;&#20803;&#65288;CHNNet&#65289;&#65306;&#19968;&#31181;&#24555;&#36895;&#25910;&#25947;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence. (arXiv:2305.10468v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10468
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20026;&#24378;&#22823;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20013;&#21516;&#19968;&#38544;&#34255;&#23618;&#20013;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#30456;&#20114;&#36830;&#25509;&#65292;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#24182;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#24515;&#30446;&#30340;&#26159;&#27169;&#20223;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#30340;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#26159;&#25353;&#23618;&#27425;&#32467;&#26500;&#21270;&#30340;&#65292;&#36825;&#21487;&#33021;&#20250;&#22952;&#30861;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#65292;&#22240;&#20026;&#21516;&#19968;&#23618;&#20013;&#30340;&#31070;&#32463;&#20803;&#20043;&#38388;&#27809;&#26377;&#36830;&#25509;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#20013;&#21516;&#19968;&#38544;&#34255;&#23618;&#20013;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#26159;&#20114;&#30456;&#36830;&#25509;&#30340;&#65292;&#20351;&#24471;&#31070;&#32463;&#20803;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#27169;&#24335;&#24182;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;&#27973;&#23618;&#21644;&#28145;&#23618;&#32593;&#32476;&#20013;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20316;&#20026;&#23436;&#20840;&#36830;&#25509;&#30340;&#23618;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core purpose of developing artificial neural networks was to mimic the functionalities of biological neural networks. However, unlike biological neural networks, traditional artificial neural networks are often structured hierarchically, which can impede the flow of information between neurons as the neurons in the same layer have no connections between them. Hence, we propose a more robust model of artificial neural networks where the hidden neurons, residing in the same hidden layer, are interconnected, enabling the neurons to learn complex patterns and speeding up the convergence rate. With the experimental study of our proposed model as fully connected layers in shallow and deep networks, we demonstrate that the model results in a significant increase in convergence rate.
&lt;/p&gt;</description></item><item><title>ZeroFlow&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#26041;&#27861;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#36827;&#34892;&#23454;&#26102;&#22330;&#26223;&#27969;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.10424</link><description>&lt;p&gt;
ZeroFlow: &#36890;&#36807;&#33976;&#39311;&#23454;&#29616;&#24555;&#36895;&#38646;&#26631;&#31614;&#22330;&#26223;&#27969;
&lt;/p&gt;
&lt;p&gt;
ZeroFlow: Fast Zero Label Scene Flow via Distillation. (arXiv:2305.10424v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10424
&lt;/p&gt;
&lt;p&gt;
ZeroFlow&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#26041;&#27861;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#36827;&#34892;&#23454;&#26102;&#22330;&#26223;&#27969;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#27969;&#20272;&#35745;&#26159;&#25551;&#36848;&#36830;&#32493;&#28857;&#20113;&#20043;&#38388;&#30340;&#19977;&#32500;&#36816;&#21160;&#22330;&#30340;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#24378;&#22823;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#27979;&#35797;&#26102;&#20248;&#21270;&#25216;&#26415;&#65292;&#20294;&#23545;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#38656;&#35201;&#25968;&#21313;&#31186;&#30340;&#26102;&#38388;&#65292;&#20351;&#20854;&#26080;&#27861;&#20316;&#20026;&#23454;&#26102;&#24212;&#29992;&#31243;&#24207;&#65288;&#22914;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#65289;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20803;&#20351;&#29992;&#12290;&#21069;&#21521;&#20256;&#36882;&#26041;&#27861;&#30456;&#23545;&#24555;&#36895;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#30340;&#36816;&#34892;&#26102;&#38388;&#22312;&#25968;&#21313;&#33267;&#25968;&#30334;&#27627;&#31186;&#20043;&#38388;&#65292;&#20294;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#21147;&#30417;&#30563;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#26694;&#26550; Scene Flow via Distillation&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#20248;&#21270;&#26041;&#27861;&#26469;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#26694;&#26550;&#20013;&#30340; ZeroFlow&#65292;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#65292;&#22312;&#22823;&#35268;&#27169;&#28857;&#20113;&#19978;&#23454;&#26102;&#29983;&#25104;&#22330;&#26223;&#27969;&#20272;&#35745;&#32467;&#26524;&#65292;&#21516;&#26102;&#36136;&#37327;&#31454;&#20105;&#29366;&#24577;&#19979;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102; ZeroFlow
&lt;/p&gt;
&lt;p&gt;
Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is ove
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#38024;&#23545;MRI&#20013;&#30340;&#36816;&#21160;&#38382;&#39064;&#65292;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#23545;&#36816;&#21160;&#26657;&#27491;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#20849;&#21516;&#28857;&#65292;&#22312;&#26410;&#26469;&#30340;&#26041;&#21521;&#19978;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.06739</link><description>&lt;p&gt;
MRI&#20013;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#22238;&#39038;&#24615;&#36816;&#21160;&#26657;&#27491;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Retrospective Motion Correction in MRI: A Comprehensive Review. (arXiv:2305.06739v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06739
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#38024;&#23545;MRI&#20013;&#30340;&#36816;&#21160;&#38382;&#39064;&#65292;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#23545;&#36816;&#21160;&#26657;&#27491;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#20849;&#21516;&#28857;&#65292;&#22312;&#26410;&#26469;&#30340;&#26041;&#21521;&#19978;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#26159;&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#30001;&#20110;MR&#20449;&#21495;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#33719;&#21462;&#65292;&#20219;&#20309;&#25104;&#20687;&#29289;&#20307;&#30340;&#36816;&#21160;&#37117;&#20250;&#23548;&#33268;&#22797;&#26434;&#30340;&#20266;&#24433;&#65292;&#27492;&#22806;&#36824;&#20250;&#20135;&#29983;&#20854;&#20182;MR&#25104;&#20687;&#20266;&#24433;&#12290;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#32463;&#24120;&#34987;&#25552;&#20986;&#29992;&#20110;&#37325;&#24314;&#36807;&#31243;&#30340;&#22810;&#20010;&#38454;&#27573;&#20013;&#30340;&#36816;&#21160;&#26657;&#27491;&#12290;MRI&#37319;&#38598;&#24207;&#21015;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#24863;&#20852;&#36259;&#30340;&#35299;&#21078;&#23398;&#21644;&#30149;&#29702;&#23398;&#20197;&#21450;&#36816;&#21160;&#27169;&#24335;&#65288;&#21018;&#24615;vs&#21487;&#21464;&#24418;&#21644;&#38543;&#26426;vs&#35268;&#24459;&#24615;&#65289;&#20351;&#24471;&#20840;&#38754;&#24615;&#35299;&#20915;&#26041;&#26696;&#19981;&#22826;&#21487;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#19981;&#21516;&#24212;&#29992;&#20043;&#38388;&#30340;&#24605;&#24819;&#20256;&#36882;&#65292;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#19968;&#20221;&#35814;&#32454;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;MRI&#36816;&#21160;&#26657;&#27491;&#26041;&#27861;&#27010;&#36848;&#21450;&#20854;&#24120;&#35265;&#25361;&#25112;&#21644;&#28508;&#21147;&#12290;&#35813;&#32508;&#36848;&#35782;&#21035;&#20102;&#19981;&#21516;&#24212;&#29992;&#20043;&#38388;&#30340;&#25968;&#25454;&#20351;&#29992;&#12289;&#20307;&#31995;&#32467;&#26500;&#21644;&#35780;&#20272;&#31574;&#30053;&#30340;&#24046;&#24322;&#21644;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#35752;&#35770;&#20102;&#22823;&#36235;&#21183;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#26088;&#22312;&#22686;&#24378;MRI&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#22238;&#39038;&#24615;&#36816;&#21160;&#26657;&#27491;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion represents one of the major challenges in magnetic resonance imaging (MRI). Since the MR signal is acquired in frequency space, any motion of the imaged object leads to complex artefacts in the reconstructed image in addition to other MR imaging artefacts. Deep learning has been frequently proposed for motion correction at several stages of the reconstruction process. The wide range of MR acquisition sequences, anatomies and pathologies of interest, and motion patterns (rigid vs. deformable and random vs. regular) makes a comprehensive solution unlikely. To facilitate the transfer of ideas between different applications, this review provides a detailed overview of proposed methods for learning-based motion correction in MRI together with their common challenges and potentials. This review identifies differences and synergies in underlying data usage, architectures and evaluation strategies. We critically discuss general trends and outline future directions, with the aim to enhan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06360</link><description>&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#36951;&#24536;&#30340;&#39046;&#22495;&#65306;&#19968;&#31687;&#32508;&#36848;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#38656;&#35201;&#21024;&#38500;&#25110;&#20462;&#25913;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#26377;&#25928;&#21644;&#20934;&#30830;&#65292;&#20294;&#22312;&#26576;&#20123;&#39046;&#22495;&#65288;&#22914;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#65289;&#65292;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#26174;&#33879;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#25968;&#25454;&#38598;&#12290;&#25991;&#31456;&#36824;&#24378;&#35843;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25915;&#20987;&#22797;&#26434;&#24615;&#12289;&#26631;&#20934;&#21270;&#12289;&#21487;&#36716;&#31227;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;&#35752;&#35770;MU&#30340;&#28508;&#22312;&#30410;&#22788;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2305.05065</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Recommender Systems with Generative Retrieval. (arXiv:2305.05065v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20351;&#29992;&#22823;&#35268;&#27169;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#25512;&#33616;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#35757;&#32451;&#21452;&#32534;&#30721;&#27169;&#22411;&#23558;&#26597;&#35810;&#21644;&#20505;&#36873;&#39033;&#23884;&#20837;&#21040;&#30456;&#21516;&#30340;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26469;&#36873;&#25321;&#32473;&#23450;&#26597;&#35810;&#23884;&#20837;&#30340;&#39030;&#37096;&#20505;&#36873;&#39033;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21333;&#38454;&#27573;&#33539;&#20363;&#65306;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#22238;&#24402;&#26041;&#24335;&#22312;&#19968;&#20010;&#38454;&#27573;&#20013;&#35299;&#30721;&#30446;&#26631;&#20505;&#36873;&#39033;&#30340;&#26631;&#35782;&#31526;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#19981;&#26159;&#20026;&#27599;&#20010;&#39033;&#30446;&#20998;&#37197;&#38543;&#26426;&#29983;&#25104;&#30340;&#21407;&#23376;ID&#65292;&#32780;&#26159;&#29983;&#25104;&#35821;&#20041;ID&#65306;&#27599;&#20010;&#39033;&#30446;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#20803;&#32452;&#32534;&#30721;&#35789;&#65292;&#23427;&#20316;&#20026;&#20854;&#21807;&#19968;&#26631;&#35782;&#31526;&#12290;&#25105;&#20204;&#20351;&#29992;&#31216;&#20026;RQ-VAE&#30340;&#20998;&#23618;&#26041;&#27861;&#29983;&#25104;&#36825;&#20123;&#32534;&#30721;&#35789;&#12290;&#19968;&#26086;&#25105;&#20204;&#23545;&#25152;&#26377;&#39033;&#30446;&#37117;&#26377;&#20102;&#35821;&#20041;ID&#65292;&#23601;&#20250;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#35821;&#20041;ID&#12290;&#30001;&#20110;&#36825;&#20010;&#27169;&#22411;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#30452;&#25509;&#39044;&#27979;&#26631;&#35782;&#19979;&#19968;&#20010;&#39033;&#30340;&#32534;&#30721;&#35789;&#20803;&#32452;&#65292;&#22240;&#27492;&#23427;&#21487;&#20197;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems leverage large-scale retrieval models consisting of two stages: training a dual-encoder model to embed queries and candidates in the same space, followed by an Approximate Nearest Neighbor (ANN) search to select top candidates given a query's embedding. In this paper, we propose a new single-stage paradigm: a generative retrieval model which autoregressively decodes the identifiers for the target candidates in one phase. To do this, instead of assigning randomly generated atomic IDs to each item, we generate Semantic IDs: a semantically meaningful tuple of codewords for each item that serves as its unique identifier. We use a hierarchical method called RQ-VAE to generate these codewords. Once we have the Semantic IDs for all the items, a Transformer based sequence-to-sequence model is trained to predict the Semantic ID of the next item. Since this model predicts the tuple of codewords identifying the next item directly in an autoregressive manner, it can be c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26080;&#32593;&#26684;&#27861;&#36827;&#34892;&#25299;&#25169;&#20248;&#21270;&#65292;&#38598;&#25104;&#20102;&#31070;&#32463;&#32593;&#32476;&#24182;&#29992;&#20110;&#27714;&#35299;&#36981;&#20174;&#24230;&#21644;&#20307;&#31215;&#20998;&#25968;&#32422;&#26463;&#36829;&#35268;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26080;&#38656;&#20256;&#32479;&#30340;&#26377;&#38480;&#20803;&#20998;&#26512;&#21644;&#32593;&#26684;&#21270;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#21518;&#22788;&#29702;&#36719;&#20214;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.04107</link><description>&lt;p&gt;
DMF-TONN: &#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30452;&#25509;&#26080;&#32593;&#26684;&#25299;&#25169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
DMF-TONN: Direct Mesh-free Topology Optimization using Neural Networks. (arXiv:2305.04107v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26080;&#32593;&#26684;&#27861;&#36827;&#34892;&#25299;&#25169;&#20248;&#21270;&#65292;&#38598;&#25104;&#20102;&#31070;&#32463;&#32593;&#32476;&#24182;&#29992;&#20110;&#27714;&#35299;&#36981;&#20174;&#24230;&#21644;&#20307;&#31215;&#20998;&#25968;&#32422;&#26463;&#36829;&#35268;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26080;&#38656;&#20256;&#32479;&#30340;&#26377;&#38480;&#20803;&#20998;&#26512;&#21644;&#32593;&#26684;&#21270;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#21518;&#22788;&#29702;&#36719;&#20214;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26080;&#32593;&#26684;&#27861;&#36827;&#34892;&#25299;&#25169;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#23494;&#24230;&#22330;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#19982;&#20301;&#31227;&#22330;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#30452;&#25509;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#32473;&#20986;&#19982;&#20256;&#32479;&#25299;&#25169;&#20248;&#21270;&#25216;&#26415;&#21487;&#27604;&#25311;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#21518;&#22788;&#29702;&#36719;&#20214;&#26080;&#32541;&#38598;&#25104;&#30340;&#20248;&#28857;&#65292;&#36824;&#26377;&#21487;&#33021;&#29992;&#20110;&#32593;&#26684;&#21644;&#26377;&#38480;&#20803;&#20998;&#26512;&#25104;&#26412;&#39640;&#26114;&#25110;&#19981;&#21512;&#36866;&#30340;&#25299;&#25169;&#20248;&#21270;&#30446;&#26631;&#12290;DMF-TONN&#25509;&#25910;&#36793;&#30028;&#26465;&#20214;&#21644;&#22495;&#22352;&#26631;&#20316;&#20026;&#36755;&#20837;&#65292;&#25214;&#21040;&#26368;&#20248;&#23494;&#24230;&#22330;&#20197;&#26368;&#23567;&#21270;&#36981;&#20174;&#24230;&#21644;&#20307;&#31215;&#20998;&#25968;&#32422;&#26463;&#36829;&#35268;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#26080;&#32593;&#26684;&#30340;&#24615;&#36136;&#26159;&#30001;&#19968;&#20010;&#29289;&#29702;&#23398;&#21551;&#21457;&#30340;&#20301;&#31227;&#22330;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#24377;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#24182;&#26367;&#20195;&#20256;&#32479;&#29992;&#20110;&#35745;&#31639;&#36981;&#20174;&#24230;&#30340;&#26377;&#38480;&#20803;&#20998;&#26512;&#65288;FEA&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a direct mesh-free method for performing topology optimization by integrating a density field approximation neural network with a displacement field approximation neural network. We show that this direct integration approach can give comparable results to conventional topology optimization techniques, with an added advantage of enabling seamless integration with post-processing software, and a potential of topology optimization with objectives where meshing and Finite Element Analysis (FEA) may be expensive or not suitable. Our approach (DMF-TONN) takes in as inputs the boundary conditions and domain coordinates and finds the optimum density field for minimizing the loss function of compliance and volume fraction constraint violation. The mesh-free nature is enabled by a physics-informed displacement field approximation neural network to solve the linear elasticity partial differential equation and replace the FEA conventionally used for calculating the compliance. We show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;ODEs&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#25913;&#36827;&#25216;&#26415;&#65292;&#21253;&#25324;&#36895;&#24230;&#21442;&#25968;&#21270;&#21644;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#31561;&#29992;&#20110;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#35823;&#24046;&#26377;&#30028;&#30340;&#39640;&#38454;&#27969;&#21305;&#37197;&#30446;&#26631;&#29992;&#20110;&#24494;&#35843;&#21644;&#25130;&#26029;&#27491;&#24577;&#21435;&#37327;&#21270;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.03935</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;ODEs&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#25913;&#36827;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs. (arXiv:2305.03935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;ODEs&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#25913;&#36827;&#25216;&#26415;&#65292;&#21253;&#25324;&#36895;&#24230;&#21442;&#25968;&#21270;&#21644;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#31561;&#29992;&#20110;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#35823;&#24046;&#26377;&#30028;&#30340;&#39640;&#38454;&#27969;&#21305;&#37197;&#30446;&#26631;&#29992;&#20110;&#24494;&#35843;&#21644;&#25130;&#26029;&#27491;&#24577;&#21435;&#37327;&#21270;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65288;&#21363;&#25193;&#25955;ODE&#65289;&#26159;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#65288;CNFs&#65289;&#30340;&#19968;&#20010;&#29305;&#20363;&#65292;&#23427;&#20351;&#24471;&#30830;&#23450;&#24615;&#25512;&#26029;&#21644;&#31934;&#30830;&#20284;&#28982;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;&#25193;&#25955;ODE&#30340;&#20284;&#28982;&#20272;&#35745;&#32467;&#26524;&#20173;&#26377;&#24456;&#22823;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#25913;&#36827;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#35757;&#32451;&#21644;&#35780;&#20272;&#20004;&#20010;&#26041;&#38754;&#65292;&#29992;&#20110;&#25193;&#25955;ODE&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#23545;&#20110;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36895;&#24230;&#21442;&#25968;&#21270;&#65292;&#24182;&#25506;&#32034;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#20197;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#35823;&#24046;&#26377;&#30028;&#30340;&#39640;&#38454;&#27969;&#21305;&#37197;&#30446;&#26631;&#29992;&#20110;&#24494;&#35843;&#65292;&#20174;&#32780;&#25552;&#39640;ODE&#30340;&#20284;&#28982;&#20272;&#35745;&#24182;&#24179;&#28369;&#20854;&#36712;&#36857;&#12290;&#23545;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#39035;&#35757;&#32451;&#30340;&#25130;&#26029;&#27491;&#24577;&#21435;&#37327;&#21270;&#26041;&#27861;&#26469;&#22635;&#34917;&#35757;&#32451;-&#35780;&#20272;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have exhibited excellent performance in various domains. The probability flow ordinary differential equation (ODE) of diffusion models (i.e., diffusion ODEs) is a particular case of continuous normalizing flows (CNFs), which enables deterministic inference and exact likelihood evaluation. However, the likelihood estimation results by diffusion ODEs are still far from those of the state-of-the-art likelihood-based generative models. In this work, we propose several improved techniques for maximum likelihood estimation for diffusion ODEs, including both training and evaluation perspectives. For training, we propose velocity parameterization and explore variance reduction techniques for faster convergence. We also derive an error-bounded high-order flow matching objective for finetuning, which improves the ODE likelihood and smooths its trajectory. For evaluation, we propose a novel training-free truncated-normal dequantization to fill the training-evaluation gap commonly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#30340;&#21333;&#35843;&#30340;&#26641;&#19995;&#26469;&#23454;&#29616;&#21333;&#35843;&#24615;&#21644;&#36879;&#26126;&#24615;&#30340;&#32467;&#21512;&#65292;&#20197;&#30830;&#31435;&#36879;&#26126;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#36131;&#21644;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#31034;&#20363;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#36879;&#26126;&#12289;&#26377;&#38382;&#36131;&#24615;&#21644;&#20844;&#24179;&#65292;&#23588;&#20854;&#23545;&#20110;&#36991;&#20813;&#21333;&#35843;&#24615;&#38382;&#39064;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.00799</link><description>&lt;p&gt;
&#22914;&#20309;&#35299;&#20915;&#27169;&#22411;&#39118;&#38505;&#31649;&#29702;&#20013;&#30340;&#21333;&#35843;&#24615;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to address monotonicity for model risk management?. (arXiv:2305.00799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#30340;&#21333;&#35843;&#30340;&#26641;&#19995;&#26469;&#23454;&#29616;&#21333;&#35843;&#24615;&#21644;&#36879;&#26126;&#24615;&#30340;&#32467;&#21512;&#65292;&#20197;&#30830;&#31435;&#36879;&#26126;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#36131;&#21644;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#31034;&#20363;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#36879;&#26126;&#12289;&#26377;&#38382;&#36131;&#24615;&#21644;&#20844;&#24179;&#65292;&#23588;&#20854;&#23545;&#20110;&#36991;&#20813;&#21333;&#35843;&#24615;&#38382;&#39064;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#21333;&#35843;&#24615;&#30830;&#31435;&#36879;&#26126;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#36131;&#21644;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#20851;&#27880;&#20010;&#20307;&#21333;&#35843;&#24615;&#65292;&#20294;&#26159;&#29616;&#26377;&#25991;&#29486;&#32463;&#24120;&#24573;&#30053;&#20102;&#25104;&#23545;&#21333;&#35843;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36879;&#26126;&#31070;&#32463;&#32593;&#32476;&#22312;&#23384;&#22312;&#19977;&#31181;&#21333;&#35843;&#24615;&#30340;&#24773;&#20917;&#19979;&#65306;&#20010;&#20307;&#21333;&#35843;&#24615;&#12289;&#24369;&#25104;&#23545;&#21333;&#35843;&#24615;&#21644;&#24378;&#25104;&#23545;&#21333;&#35843;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#30340;&#21333;&#35843;&#30340;&#26641;&#19995;&#26469;&#23454;&#29616;&#21333;&#35843;&#24615;&#21644;&#36879;&#26126;&#24615;&#30340;&#32467;&#21512;&#12290;&#36890;&#36807;&#23454;&#35777;&#31034;&#20363;&#65292;&#25105;&#20204;&#35777;&#26126;&#21333;&#35843;&#24615;&#36890;&#24120;&#22312;&#23454;&#36341;&#20013;&#34987;&#36829;&#21453;&#65292;&#32780;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#30340;&#21333;&#35843;&#26641;&#19995;&#26159;&#36879;&#26126;&#12289;&#26377;&#38382;&#36131;&#24615;&#21644;&#20844;&#24179;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of establishing the accountability and fairness of transparent machine learning models through monotonicity. Although there have been numerous studies on individual monotonicity, pairwise monotonicity is often overlooked in the existing literature. This paper studies transparent neural networks in the presence of three types of monotonicity: individual monotonicity, weak pairwise monotonicity, and strong pairwise monotonicity. As a means of achieving monotonicity while maintaining transparency, we propose the monotonic groves of neural additive models. As a result of empirical examples, we demonstrate that monotonicity is often violated in practice and that monotonic groves of neural additive models are transparent, accountable, and fair.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#19981;&#21516;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#12289;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#21644;&#31283;&#20581;&#24615;&#24178;&#39044;&#30340;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#21333;&#19968;&#30340;&#36873;&#25321;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#31283;&#20581;&#65292;&#29616;&#26377;&#30340;&#24178;&#39044;&#25514;&#26045;&#20063;&#21487;&#33021;&#26080;&#27861;&#25552;&#39640;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11263</link><description>&lt;p&gt;
&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#19979;&#20302;&#26679;&#26412;&#31283;&#20581;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Low-Shot Robustness to Natural Distribution Shifts. (arXiv:2304.11263v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#19981;&#21516;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#12289;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#21644;&#31283;&#20581;&#24615;&#24178;&#39044;&#30340;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#21333;&#19968;&#30340;&#36873;&#25321;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#31283;&#20581;&#65292;&#29616;&#26377;&#30340;&#24178;&#39044;&#25514;&#26045;&#20063;&#21487;&#33021;&#26080;&#27861;&#25552;&#39640;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32467;&#21512;&#26356;&#22909;&#30340;&#24494;&#35843;&#26041;&#27861;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#24050;&#32463;&#21462;&#24471;&#20102;&#38024;&#23545;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#40065;&#26834;&#24615;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#24494;&#35843;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#24403;&#35757;&#32451;&#25968;&#25454;&#37327;&#19981;&#39640;&#26102;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#12289;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#21644;&#26368;&#20808;&#36827;&#30340;&#31283;&#20581;&#24615;&#24178;&#39044;&#30340;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#21333;&#19968;&#30340;&#36873;&#25321;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#31283;&#20581;&#65292;&#21363;&#20351;&#22312;&#23436;&#25972;&#26679;&#26412;&#19979;&#65292;&#29616;&#26377;&#30340;&#24178;&#39044;&#25514;&#26045;&#20063;&#21487;&#33021;&#26080;&#27861;&#25552;&#39640;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#22815;&#28608;&#21169;&#31038;&#21306;&#20851;&#27880;&#36825;&#20010;&#23454;&#38469;&#37325;&#35201;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness to natural distribution shifts has seen remarkable progress thanks to recent pre-training strategies combined with better fine-tuning methods. However, such fine-tuning assumes access to large amounts of labelled data, and the extent to which the observations hold when the amount of training data is not as high remains unknown. We address this gap by performing the first in-depth study of robustness to various natural distribution shifts in different low-shot regimes: spanning datasets, architectures, pre-trained initializations, and state-of-the-art robustness interventions. Most importantly, we find that there is no single model of choice that is often more robust than others, and existing interventions can fail to improve robustness on some datasets even if they do so in the full-shot regime. We hope that our work will motivate the community to focus on this problem of practical importance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#27169;&#24577;&#30340;&#28145;&#24230;&#23398;&#20064;&#34701;&#21512;&#25216;&#26415;&#22312;&#20449;&#29992;&#35780;&#32423;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#35777;&#26126;&#20102;&#19968;&#20010;&#22522;&#20110;CNN&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#20004;&#31181;&#34701;&#21512;&#31574;&#30053;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#25216;&#26415;&#65292;&#21516;&#26102;&#22312;&#27604;&#36739;&#31616;&#21333;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#20013;&#21457;&#29616;&#65292;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.10740</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#29992;&#35780;&#32423;&#39044;&#27979;&#26041;&#27861;&#30740;&#31350;&#8212;&#8212;&#20197;&#25991;&#26412;&#21644;&#25968;&#23383;&#25968;&#25454;&#27969;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Deep Learning for Credit Rating Prediction Using Text and Numerical Data Streams. (arXiv:2304.10740v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#27169;&#24577;&#30340;&#28145;&#24230;&#23398;&#20064;&#34701;&#21512;&#25216;&#26415;&#22312;&#20449;&#29992;&#35780;&#32423;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#35777;&#26126;&#20102;&#19968;&#20010;&#22522;&#20110;CNN&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#20004;&#31181;&#34701;&#21512;&#31574;&#30053;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#25216;&#26415;&#65292;&#21516;&#26102;&#22312;&#27604;&#36739;&#31616;&#21333;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#20013;&#21457;&#29616;&#65292;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20449;&#29992;&#35780;&#32423;&#20998;&#37197;&#20013;&#21738;&#20123;&#22240;&#32032;&#26159;&#37325;&#35201;&#30340;&#21487;&#20197;&#24110;&#21161;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25991;&#29486;&#30340;&#37325;&#28857;&#22823;&#22810;&#38598;&#20013;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#65292;&#36739;&#23569;&#30740;&#31350;&#38750;&#32467;&#26500;&#21270;&#25110;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#30340;&#26377;&#25928;&#26550;&#26500;&#65292;&#20197;&#39044;&#27979;&#20844;&#21496;&#20449;&#29992;&#35780;&#32423;&#26631;&#20934;&#12290;&#22312;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#34701;&#21512;&#31574;&#30053;&#30340;&#32452;&#21512;&#65292;&#21253;&#25324;CNN&#65292;LSTM&#65292;GRU&#21644;BERT&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#34701;&#21512;&#31574;&#30053;&#65288;&#21253;&#25324;&#26089;&#26399;&#21644;&#20013;&#38388;&#34701;&#21512;&#65289;&#20197;&#21450;&#25216;&#26415;&#65288;&#21253;&#25324;&#20018;&#32852;&#21644;&#20132;&#21449;&#27880;&#24847;&#65289;&#31561;&#26041;&#38754;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#22522;&#20110;CNN&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#20004;&#31181;&#34701;&#21512;&#31574;&#30053;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#27604;&#36739;&#31616;&#21333;&#30340;&#26550;&#26500;&#19982;&#26356;&#22797;&#26434;&#30340;&#26550;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#33021;&#22312;&#20449;&#29992;&#35780;&#32423;&#39044;&#27979;&#20013;&#21457;&#25381;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowing which factors are significant in credit rating assignment leads to better decision-making. However, the focus of the literature thus far has been mostly on structured data, and fewer studies have addressed unstructured or multi-modal datasets. In this paper, we present an analysis of the most effective architectures for the fusion of deep learning models for the prediction of company credit rating classes, by using structured and unstructured datasets of different types. In these models, we tested different combinations of fusion strategies with different deep learning models, including CNN, LSTM, GRU, and BERT. We studied data fusion strategies in terms of level (including early and intermediate fusion) and techniques (including concatenation and cross-attention). Our results show that a CNN-based multi-modal model with two fusion strategies outperformed other multi-modal techniques. In addition, by comparing simple architectures with more complex ones, we found that more soph
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#12289;&#32431;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#22312;$\{0,1\}^d$&#19978;&#20934;&#30830;&#20272;&#35745;&#20108;&#20803;&#31215;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#36798;&#21040;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06787</link><description>&lt;p&gt;
&#20108;&#20803;&#31215;&#20998;&#24067;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#32431;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions. (arXiv:2304.06787v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#12289;&#32431;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#22312;$\{0,1\}^d$&#19978;&#20934;&#30830;&#20272;&#35745;&#20108;&#20803;&#31215;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#36798;&#21040;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#949;-&#24046;&#20998;&#38544;&#31169;&#12289;&#35745;&#31639;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#24635;&#21464;&#21270;&#36317;&#31163;&#19979;&#20934;&#30830;&#22320;&#20272;&#35745;$\{0,1\}^d$&#19978;&#30340;&#20056;&#31215;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#21516;&#26102;&#22312;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#20869;&#33719;&#24471;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#22312;&#26356;&#24369;&#30340;&#38544;&#31169;&#27010;&#24565;&#19979;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35201;&#20040;&#22312;&#25351;&#25968;&#32423;&#36816;&#34892;&#26102;&#38388;&#20869;&#26368;&#20248;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the first $\varepsilon$-differentially private, computationally efficient algorithm that estimates the means of product distributions over $\{0,1\}^d$ accurately in total-variation distance, whilst attaining the optimal sample complexity to within polylogarithmic factors. The prior work had either solved this problem efficiently and optimally under weaker notions of privacy, or had solved it optimally while having exponential running times.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.05265</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;
&lt;/p&gt;
&lt;p&gt;
Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#22312;&#20197;&#25991;&#26412;&#20026;&#24341;&#23548;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#24403;&#24341;&#23548;&#20449;&#24687;&#21253;&#21547;&#29992;&#25143;&#23450;&#20041;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#25110;&#38271;&#23614;&#27010;&#24565;&#26631;&#35760;&#26102;&#65292;&#25991;&#26412;&#21453;&#36716;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#29983;&#25104;&#25216;&#26415;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#23637;&#31034;&#20102;&#25991;&#26412;&#21453;&#36716;&#30340;&#37096;&#32626;&#20173;&#20805;&#28385;&#20102;&#8220;&#40657;&#39764;&#27861;&#8221;&#65292;&#20363;&#22914;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;&#20005;&#33499;&#35201;&#27714;&#65292;&#22312;&#24490;&#29615;&#20013;&#38656;&#35201;&#33392;&#33510;&#30340;&#20154;&#21147;&#25104;&#26412;&#21644;&#32570;&#20047;&#40065;&#26834;&#24615;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;&#30340;&#22823;&#22823;&#22686;&#24378;&#29256;&#21453;&#36716;&#65292;&#35299;&#20915;&#20102;&#25152;&#26377;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#21453;&#36807;&#26469;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;COTI&#30340;&#26680;&#24515;&#26159;&#22522;&#20110;&#29702;&#35770;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#20855;&#26377;&#20840;&#38754;&#21644;&#26032;&#39062;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#30001;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#25152;&#25552;&#21462;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;COTI&#30340;&#24615;&#33021;&#27604;&#20043;&#21069;&#25216;&#26415;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts. Text inversion (TI), alongside the text-to-image model backbones, is proposed as an effective technique in personalizing the generation when the prompts contain user-defined, unseen or long-tail concept tokens. Despite that, we find and show that the deployment of TI remains full of "dark-magics" -- to name a few, the harsh requirement of additional datasets, arduous human efforts in the loop and lack of robustness. In this work, we propose a much-enhanced version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all the aforementioned problems and in turn delivering a robust, data-efficient and easy-to-use framework. The core to COTI is a theoretically-guided loss objective instantiated with a comprehensive and novel weighted scoring mechanism, encapsulated by an active-learning paradigm. The extensive results show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;"&#31639;&#27861;&#22686;&#24378;"&#30340;&#36890;&#29992;&#31639;&#27861;&#25277;&#35937;&#65292;&#38024;&#23545;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#20197;&#25351;&#25968;&#36895;&#24230;&#21152;&#36895;&#32447;&#24615;&#26144;&#23556;&#30340;&#25910;&#25947;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#23558;&#38750;&#25910;&#25947;&#30340;&#36845;&#20195;&#31639;&#27861;&#36716;&#25442;&#20026;&#25910;&#25947;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04665</link><description>&lt;p&gt;
&#20851;&#20110;&#31639;&#27861;&#22686;&#24378;&#22266;&#23450;&#28857;&#35745;&#31639;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On algorithmically boosting fixed-point computations. (arXiv:2304.04665v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;"&#31639;&#27861;&#22686;&#24378;"&#30340;&#36890;&#29992;&#31639;&#27861;&#25277;&#35937;&#65292;&#38024;&#23545;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#20197;&#25351;&#25968;&#36895;&#24230;&#21152;&#36895;&#32447;&#24615;&#26144;&#23556;&#30340;&#25910;&#25947;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#23558;&#38750;&#25910;&#25947;&#30340;&#36845;&#20195;&#31639;&#27861;&#36716;&#25442;&#20026;&#25910;&#25947;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#20027;&#35201;&#20869;&#23481;&#26159;&#20851;&#20110;&#35745;&#31639;&#32435;&#20160;&#22343;&#34913;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#29305;&#23450;&#26041;&#27861;&#36716;&#21270;&#20026;&#19968;&#20010;&#31216;&#20043;&#20026;"&#31639;&#27861;&#22686;&#24378;"&#30340;&#36890;&#29992;&#31639;&#27861;&#25277;&#35937;&#65292;&#35813;&#25277;&#35937;&#23545;&#20110;&#20854;&#20182;&#22266;&#23450;&#28857;&#35745;&#31639;&#38382;&#39064;&#20063;&#26159;&#30456;&#20851;&#30340;&#12290;&#31639;&#27861;&#22686;&#24378;&#26159;&#36890;&#36807;&#36845;&#20195;&#26144;&#23556;&#30340;&#38271;&#26399;&#24179;&#22343;&#26469;&#35745;&#31639;&#22266;&#23450;&#28857;&#30340;&#21407;&#29702;&#65292;&#23427;&#26159;&#25351;&#25968;&#36816;&#31639;&#30340;&#19968;&#31181;&#25512;&#24191;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#38750;&#32447;&#24615;&#26144;&#23556;&#30340;&#26694;&#26550;&#20013;&#23450;&#20041;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#25910;&#25947;&#30340;&#32447;&#24615;&#26144;&#23556;&#19978;&#65288;&#20363;&#22914;&#65292;&#22312;PageRank&#31639;&#27861;&#20013;&#35745;&#31639;&#20248;&#21183;&#29305;&#24449;&#21521;&#37327;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22686;&#24378;&#26041;&#27861;&#21487;&#20197;&#20197;&#25351;&#25968;&#36895;&#24230;&#21152;&#36895;&#25910;&#25947;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#31639;&#27861;&#22686;&#24378;&#21487;&#20197;&#23558;&#19968;&#20010;&#65288;&#24369;&#65289;&#19981;&#25910;&#25947;&#30340;&#36845;&#20195;&#31639;&#27861;&#36716;&#25442;&#20026;&#65288;&#24378;&#65289;&#25910;&#25947;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#31639;&#27861;&#22686;&#24378;&#30340;&#19968;&#31181;"&#21464;&#20998;&#26041;&#27861;"&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#23558;&#38750;&#25910;&#25947;&#36830;&#32493;&#27969;&#36716;&#25442;&#20026;&#25910;&#25947;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main topic of this paper are algorithms for computing Nash equilibria. We cast our particular methods as instances of a general algorithmic abstraction, namely, a method we call {\em algorithmic boosting}, which is also relevant to other fixed-point computation problems. Algorithmic boosting is the principle of computing fixed points by taking (long-run) averages of iterated maps and it is a generalization of exponentiation. We first define our method in the setting of nonlinear maps. Secondly, we restrict attention to convergent linear maps (for computing dominant eigenvectors, for example, in the PageRank algorithm) and show that our algorithmic boosting method can set in motion {\em exponential speedups in the convergence rate}. Thirdly, we show that algorithmic boosting can convert a (weak) non-convergent iterator to a (strong) convergent one. We also consider a {\em variational approach} to algorithmic boosting providing tools to convert a non-convergent continuous flow to a c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30896;&#25758;&#27010;&#29575;&#30340;&#26080;&#22270;Crowd Navigation&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26469;&#24863;&#30693;&#20154;&#32676;&#30340;&#21361;&#38505;&#31243;&#24230;&#65292;&#30830;&#20445;&#26426;&#22120;&#20154;&#22312;&#36890;&#36807;&#25317;&#25380;&#29615;&#22659;&#26102;&#30340;&#23433;&#20840;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03593</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#22270;Crowd Navigation&#19982;&#24863;&#30693;&#39118;&#38505;&#25511;&#21046;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-Based Mapless Crowd Navigation with Perceived Risk of the Moving Crowd for Mobile Robots. (arXiv:2304.03593v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30896;&#25758;&#27010;&#29575;&#30340;&#26080;&#22270;Crowd Navigation&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26469;&#24863;&#30693;&#20154;&#32676;&#30340;&#21361;&#38505;&#31243;&#24230;&#65292;&#30830;&#20445;&#26426;&#22120;&#20154;&#22312;&#36890;&#36807;&#25317;&#25380;&#29615;&#22659;&#26102;&#30340;&#23433;&#20840;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#22320;&#22270;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#24448;&#24448;&#20250;&#36935;&#21040;&#8220;&#20923;&#32467;&#26426;&#22120;&#20154;&#38382;&#39064;&#8221;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#27492;&#38382;&#39064;&#65292;&#20294;&#26159;&#23384;&#22312;&#27867;&#21270;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#8220;&#30896;&#25758;&#27010;&#29575;&#8221;&#26469;&#24110;&#21161;&#26426;&#22120;&#20154;&#23433;&#20840;&#36890;&#36807;&#20154;&#32676;&#30340;&#26041;&#27861;&#12290;&#23558;&#8220;&#30896;&#25758;&#27010;&#29575;&#8221;&#21253;&#25324;&#22312;&#35266;&#23519;&#31354;&#38388;&#20013;&#65292;&#32473;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#20010;&#24863;&#30693;&#31227;&#21160;&#20154;&#32676;&#30340;&#21361;&#38505;&#31243;&#24230;&#30340;&#33021;&#21147;&#12290;&#26426;&#22120;&#20154;&#20250;&#22312;&#30475;&#20284;&#23433;&#20840;&#30340;&#24773;&#20917;&#19979;&#31359;&#36807;&#20154;&#32676;&#65292;&#20294;&#22312;&#20154;&#32676;&#31227;&#21160;&#36807;&#20110;&#28608;&#28872;&#26102;&#20250;&#32469;&#36335;&#12290;&#36890;&#36807;&#20851;&#27880;&#26368;&#21361;&#38505;&#30340;&#38556;&#30861;&#29289;&#65292;&#26426;&#22120;&#20154;&#19981;&#20250;&#22312;&#20154;&#32676;&#23494;&#24230;&#36739;&#39640;&#26102;&#28151;&#28102;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24320;&#21457;&#65292;&#24182;&#22312;Gazebo&#27169;&#25311;&#22120;&#20013;&#36827;&#34892;&#20102;&#38750;&#21512;&#20316;&#20154;&#32676;&#29615;&#22659;&#20013;&#30340;&#35757;&#32451;&#65292;&#20854;&#20013;&#30340;&#38556;&#30861;&#29289;&#20197;&#38543;&#26426;&#36895;&#24230;&#31227;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical map-based navigation methods are commonly used for robot navigation, but they often struggle in crowded environments due to the Frozen Robot Problem (FRP). Deep reinforcement learning-based methods address the FRP problem, however, suffer from the issues of generalization and scalability. To overcome these challenges, we propose a method that uses Collision Probability (CP) to help the robot navigate safely through crowds. The inclusion of CP in the observation space gives the robot a sense of the level of danger of the moving crowd. The robot will navigate through the crowd when it appears safe but will take a detour when the crowd is moving aggressively. By focusing on the most dangerous obstacle, the robot will not be confused when the crowd density is high, ensuring scalability of the model. Our approach was developed using deep reinforcement learning (DRL) and trained using the Gazebo simulator in a non cooperative crowd environment with obstacles moving at randomized sp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36817;&#20284;&#27169;&#22411;&#26469;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.16047</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#25506;&#32034;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#25972;&#20010;&#20248;&#31168;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models. (arXiv:2303.16047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36817;&#20284;&#27169;&#22411;&#26469;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#39046;&#22495;&#19987;&#23478;&#20043;&#38388;&#30340;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#65307;&#28982;&#32780;&#65292;&#36890;&#24120;&#21482;&#29983;&#25104;&#21333;&#20010;&#27169;&#22411;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#19981;&#21033;&#20110;&#27492;&#31867;&#20132;&#20114;&#12290;&#36817;&#20284;&#21644;&#25506;&#32034;Rashomon&#38598;&#65292;&#21363;&#25152;&#26377;&#36817;&#20046;&#26368;&#20248;&#27169;&#22411;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#25552;&#20379;&#29992;&#25143;&#21487;&#25628;&#32034;&#30340;&#31354;&#38388;&#21253;&#21547;&#22810;&#26679;&#24615;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#23454;&#38469;&#25361;&#25112;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#20174;&#20013;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#36817;&#20284;&#20855;&#26377;&#22266;&#23450;&#25903;&#25345;&#38598;&#30340;GAMs&#30340;Rashomon&#38598;&#30340;&#26925;&#29699;&#24418;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26925;&#29699;&#24418;&#36817;&#20284;&#20102;&#35768;&#22810;&#19981;&#21516;&#25903;&#25345;&#38598;&#30340;Rashomon&#38598;&#12290;&#36817;&#20284;&#30340;Rashomon&#38598;&#20026;&#35299;&#20915;&#23454;&#38469;&#25361;&#25112;&#65292;&#20363;&#22914;&#65288;1&#65289;&#30740;&#31350;&#27169;&#22411;&#31867;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#65307;&#65288;2&#65289;&#22312;&#29992;&#25143;&#25351;&#23450;&#32422;&#26463;&#26465;&#20214;&#19979;&#26597;&#25214;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real applications, interaction between machine learning model and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present a technique to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models (GAMs). We present algorithms to approximate the Rashomon set of GAMs with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#21160;&#24577;&#22270;&#23398;&#20064;&#26550;&#26500;DyGFormer&#65292;&#24182;&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#24211;DyGLib&#65292;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#20449;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#12290;DyGFormer&#36890;&#36807;&#37051;&#23621;&#20849;&#29616;&#32534;&#30721;&#26041;&#26696;&#21644;&#20998;&#22359;&#25216;&#26415;&#23454;&#29616;&#26356;&#38271;&#26399;&#21382;&#21490;&#30340;&#39640;&#25928;&#25512;&#29702;&#65292;&#22312;13&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13047</link><description>&lt;p&gt;
&#21521;&#26356;&#22909;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#36808;&#36827;&#65306;&#26032;&#30340;&#26550;&#26500;&#21644;&#32479;&#19968;&#24211;
&lt;/p&gt;
&lt;p&gt;
Towards Better Dynamic Graph Learning: New Architecture and Unified Library. (arXiv:2303.13047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13047
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#21160;&#24577;&#22270;&#23398;&#20064;&#26550;&#26500;DyGFormer&#65292;&#24182;&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#24211;DyGLib&#65292;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#20449;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#12290;DyGFormer&#36890;&#36807;&#37051;&#23621;&#20849;&#29616;&#32534;&#30721;&#26041;&#26696;&#21644;&#20998;&#22359;&#25216;&#26415;&#23454;&#29616;&#26356;&#38271;&#26399;&#21382;&#21490;&#30340;&#39640;&#25928;&#25512;&#29702;&#65292;&#22312;13&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DyGFormer&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#21160;&#24577;&#22270;&#23398;&#20064;&#26550;&#26500;&#65292;&#20165;&#20174;&#33410;&#28857;&#21382;&#21490;&#30340;&#31532;&#19968;&#36339;&#20132;&#20114;&#24207;&#21015;&#20013;&#23398;&#20064;&#12290;DyGFormer&#32467;&#21512;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#35774;&#35745;&#65306;&#19968;&#31181;&#37051;&#23621;&#20849;&#29616;&#32534;&#30721;&#26041;&#26696;&#65292;&#25506;&#32034;&#28304;&#33410;&#28857;&#21644;&#30446;&#26631;&#33410;&#28857;&#22522;&#20110;&#23427;&#20204;&#30340;&#24207;&#21015;&#30340;&#30456;&#20851;&#24615;&#65307;&#19968;&#31181;&#20998;&#22359;&#25216;&#26415;&#65292;&#23558;&#27599;&#20010;&#24207;&#21015;&#20998;&#25104;&#22810;&#20010;&#22359;&#24182;&#23558;&#20854;&#39304;&#36865;&#32473;Transformer&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#21463;&#30410;&#20110;&#26356;&#38271;&#26399;&#30340;&#21382;&#21490;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;DyGLib&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#24211;&#65292;&#20855;&#26377;&#26631;&#20934;&#30340;&#35757;&#32451;&#31649;&#36947;&#12289;&#21487;&#25193;&#23637;&#30340;&#32534;&#30721;&#25509;&#21475;&#21644;&#32508;&#21512;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#20449;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;13&#20010;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#36827;&#34892;&#25512;&#23548;/&#24402;&#32435;&#21160;&#24577;&#38142;&#25509;&#39044;&#27979;&#21644;&#21160;&#24577;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65306;DyGFormer&#22312;mo&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
We propose DyGFormer, a new Transformer-based architecture for dynamic graph learning that solely learns from the sequences of nodes' historical first-hop interactions. DyGFormer incorporates two distinct designs: a neighbor co-occurrence encoding scheme that explores the correlations of the source node and destination node based on their sequences; a patching technique that divides each sequence into multiple patches and feeds them to Transformer, allowing the model to effectively and efficiently benefit from longer histories. We also introduce DyGLib, a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols to promote reproducible, scalable, and credible dynamic graph learning research. By performing extensive experiments on thirteen datasets from various domains for transductive/inductive dynamic link prediction and dynamic node classification tasks, we observe that: DyGFormer achieves state-of-the-art performance on mo
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#34429;&#28982;&#33021;&#22815;&#28040;&#38500;&#25968;&#25454;&#20849;&#20139;&#65292;&#20294;&#20849;&#20139;&#30340;&#26799;&#24230;&#21487;&#33021;&#20250;&#21253;&#21547;&#31169;&#23494;&#20449;&#24687;&#65292;&#24182;&#19988;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#24694;&#24847;&#20462;&#25913;&#26550;&#26500;&#21644;&#21442;&#25968;&#25110;&#20351;&#29992;&#20248;&#21270;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#36817;&#20284;&#29992;&#25143;&#25968;&#25454;&#65292;&#23548;&#33268;&#29992;&#25143;&#25968;&#25454;&#27844;&#38706;&#12290;</title><link>http://arxiv.org/abs/2303.12233</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#32858;&#21512;&#24182;&#38750;&#38544;&#31169;&#65306;&#36890;&#36807;&#27169;&#22411;&#20462;&#25913;&#22823;&#35268;&#27169;&#27844;&#38706;&#29992;&#25143;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Secure Aggregation in Federated Learning is not Private: Leaking User Data at Large Scale through Model Modification. (arXiv:2303.12233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12233
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#34429;&#28982;&#33021;&#22815;&#28040;&#38500;&#25968;&#25454;&#20849;&#20139;&#65292;&#20294;&#20849;&#20139;&#30340;&#26799;&#24230;&#21487;&#33021;&#20250;&#21253;&#21547;&#31169;&#23494;&#20449;&#24687;&#65292;&#24182;&#19988;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#24694;&#24847;&#20462;&#25913;&#26550;&#26500;&#21644;&#21442;&#25968;&#25110;&#20351;&#29992;&#20248;&#21270;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#36817;&#20284;&#29992;&#25143;&#25968;&#25454;&#65292;&#23548;&#33268;&#29992;&#25143;&#25968;&#25454;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#21644;&#38544;&#31169;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#32456;&#31471;&#29992;&#25143;&#35774;&#22791;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#25935;&#24863;&#25968;&#25454;&#65292;&#19981;&#24212;&#19982;&#26381;&#21153;&#22120;&#25110;&#20225;&#19994;&#20998;&#20139;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#34987;&#24341;&#20837;&#20197;&#22312;&#22823;&#35268;&#27169;&#20998;&#25955;&#24335;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#65292;&#21516;&#26102;&#36890;&#36807;&#28040;&#38500;&#25968;&#25454;&#20849;&#20139;&#26469;&#20445;&#35777;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20849;&#20139;&#30340;&#26799;&#24230;&#36890;&#24120;&#21253;&#21547;&#31169;&#23494;&#20449;&#24687;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#24694;&#24847;&#20462;&#25913;&#26550;&#26500;&#21644;&#21442;&#25968;&#25110;&#20351;&#29992;&#20248;&#21270;&#20174;&#20849;&#20139;&#26799;&#24230;&#20013;&#36817;&#20284;&#29992;&#25143;&#25968;&#25454;&#26469;&#33719;&#24471;&#30693;&#35782;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#25915;&#20987;&#33267;&#20170;&#20173;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22312;&#20351;&#29992;&#23433;&#20840;&#27169;&#22411;&#32858;&#21512;&#23558;&#23458;&#25143;&#31471;&#26799;&#24230;&#32858;&#21512;&#22312;&#19968;&#36215;&#26102;&#20250;&#22833;&#36133;&#12290;&#30446;&#21069;&#20173;&#28982;&#21487;&#34892;&#30340;&#25915;&#20987;&#22312;&#34987;&#25915;&#20987;&#30340;&#23458;&#25143;&#31471;&#25968;&#37327;&#12289;&#27844;&#28431;&#30340;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#25110;&#35757;&#32451;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#26041;&#38754;&#37117;&#21463;&#21040;&#20005;&#26684;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Security and privacy are important concerns in machine learning. End user devices often contain a wealth of data and this information is sensitive and should not be shared with servers or enterprises. As a result, federated learning was introduced to enable machine learning over large decentralized datasets while promising privacy by eliminating the need for data sharing. However, prior work has shown that shared gradients often contain private information and attackers can gain knowledge either through malicious modification of the architecture and parameters or by using optimization to approximate user data from the shared gradients. Despite this, most attacks have so far been limited in scale of number of clients, especially failing when client gradients are aggregated together using secure model aggregation. The attacks that still function are strongly limited in the number of clients attacked, amount of training samples they leak, or number of iterations they take to be trained. I
&lt;/p&gt;</description></item><item><title>LHCb&#23454;&#39564;&#20013;&#30340;90%&#35745;&#31639;&#36164;&#28304;&#29992;&#20110;&#29983;&#20135;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#65292;&#32780;Lamarr&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;LHCb&#23454;&#39564;&#30340;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#37325;&#24314;&#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#21152;&#24555;&#20102;&#27169;&#25311;&#20135;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.11428</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;LHCb&#36229;&#24555;&#36895;&#27169;&#25311;&#31995;&#32479;Lamarr&#22312;Gauss&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Lamarr: LHCb ultra-fast simulation based on machine learning models deployed within Gauss. (arXiv:2303.11428v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11428
&lt;/p&gt;
&lt;p&gt;
LHCb&#23454;&#39564;&#20013;&#30340;90%&#35745;&#31639;&#36164;&#28304;&#29992;&#20110;&#29983;&#20135;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#65292;&#32780;Lamarr&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;LHCb&#23454;&#39564;&#30340;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#37325;&#24314;&#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#21152;&#24555;&#20102;&#27169;&#25311;&#20135;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LHCb&#23454;&#39564;&#21487;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#30340;&#32422;90%&#29992;&#20110;&#29983;&#20135;Large Hadron Collider&#65288;LHC&#65289;&#36816;&#34892;2&#30340;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#12290;&#21319;&#32423;&#21518;&#30340;LHCb&#25506;&#27979;&#22120;&#23558;&#33021;&#22815;&#25910;&#38598;&#26356;&#22810;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#27169;&#25311;&#20107;&#20214;&#26469;&#20998;&#26512;&#23558;&#22312;&#36816;&#34892;3&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#27169;&#25311;&#26159;&#20998;&#26512;&#30340;&#20851;&#38190;&#38656;&#27714;&#65292;&#20197;&#35299;&#37322;&#20449;&#21495;&#19982;&#32972;&#26223;&#24182;&#27979;&#37327;&#25928;&#29575;&#12290;&#36825;&#31181;&#38656;&#35201;&#30340;&#27169;&#25311;&#23558;&#36828;&#36828;&#36229;&#20986;&#24050;&#25215;&#35834;&#30340;&#36164;&#28304;&#65292;&#38656;&#35201;&#25216;&#26415;&#21644;&#25216;&#24039;&#30340;&#28436;&#21464;&#26469;&#29983;&#20135;&#36825;&#20123;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#36129;&#29486;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;Lamarr&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Gaudi&#26694;&#26550;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#23545;LHCb&#23454;&#39564;&#30340;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#37325;&#24314;&#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#21152;&#24555;&#20102;&#27169;&#25311;&#20135;&#20986;&#12290;&#20351;&#29992;&#22522;&#20110;&#22810;&#31181;&#31639;&#27861;&#21644;&#31574;&#30053;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#21442;&#25968;&#21270;&#20102;LHCb&#25506;&#27979;&#22120;&#21333;&#20010;&#32452;&#20214;&#30340;&#39640;&#32423;&#21709;&#24212;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
About 90% of the computing resources available to the LHCb experiment has been spent to produce simulated data samples for Run 2 of the Large Hadron Collider at CERN. The upgraded LHCb detector will be able to collect larger data samples, requiring many more simulated events to analyze the data to be collected in Run 3. Simulation is a key necessity of analysis to interpret signal vs background and measure efficiencies. The needed simulation will far exceed the pledged resources, requiring an evolution in technologies and techniques to produce these simulated data samples. In this contribution, we discuss Lamarr, a Gaudi-based framework to speed-up the simulation production parametrizing both the detector response and the reconstruction algorithms of the LHCb experiment. Deep Generative Models powered by several algorithms and strategies are employed to effectively parametrize the high-level response of the single components of the LHCb detector, encoding within neural networks the exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;CNN&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.10523</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35299;&#37322;&#24615;&#22522;&#30784;&#25277;&#21462;&#29992;&#20110;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations. (arXiv:2303.10523v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;CNN&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#29992;&#20154;&#31867;&#21487;&#20197;&#29702;&#35299;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;CNN&#22270;&#20687;&#20998;&#31867;&#22120;&#39044;&#27979;&#21644;&#20013;&#38388;&#23618;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#25214;&#35299;&#37322;&#20687;&#32032;&#28608;&#27963;&#30340;&#31232;&#30095;&#20108;&#20540;&#21270;&#36716;&#25442;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#26059;&#36716;&#26469;&#25552;&#21462;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#27969;&#34892;CNN&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#25552;&#21462;&#35299;&#37322;&#24615;&#22522;&#30784;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25991;&#29486;&#20013;&#30340;&#22522;&#30784;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#65292;&#24182;&#34920;&#26126;&#65292;&#24403;&#20013;&#38388;&#23618;&#34920;&#31034;&#34987;&#36716;&#25442;&#20026;&#25105;&#20204;&#26041;&#27861;&#25552;&#21462;&#30340;&#22522;&#30784;&#26102;&#65292;&#23427;&#20204;&#21464;&#24471;&#26356;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important line of research attempts to explain CNN image classifier predictions and intermediate layer representations in terms of human understandable concepts. In this work, we expand on previous works in the literature that use annotated concept datasets to extract interpretable feature space directions and propose an unsupervised post-hoc method to extract a disentangling interpretable basis by looking for the rotation of the feature space that explains sparse one-hot thresholded transformed representations of pixel activations. We do experimentation with existing popular CNNs and demonstrate the effectiveness of our method in extracting an interpretable basis across network architectures and training datasets. We make extensions to the existing basis interpretability metrics found in the literature and show that, intermediate layer representations become more interpretable when transformed to the bases extracted with our method. Finally, using the basis interpretability metrics
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#35757;&#32451;&#20110;&#19981;&#21516; MuJoCo &#36816;&#21160;&#38382;&#39064;&#19978;&#30340; Decision Transformer &#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#36890;&#36807;&#20849;&#20139;&#19968;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#35757;&#32451;&#20197;&#21450;&#20849;&#21516;&#20351;&#29992;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#20351;&#20195;&#29702;&#30340;&#36807;&#31243;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#12290;</title><link>http://arxiv.org/abs/2303.07551</link><description>&lt;p&gt;
&#21512;&#24182;&#20915;&#31574;Transformer&#65306;&#22810;&#20219;&#21153;&#31574;&#30053;&#24418;&#25104;&#30340;&#26435;&#37325;&#24179;&#22343;&#21270;
&lt;/p&gt;
&lt;p&gt;
Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies. (arXiv:2303.07551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#35757;&#32451;&#20110;&#19981;&#21516; MuJoCo &#36816;&#21160;&#38382;&#39064;&#19978;&#30340; Decision Transformer &#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#36890;&#36807;&#20849;&#20139;&#19968;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#35757;&#32451;&#20197;&#21450;&#20849;&#21516;&#20351;&#29992;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#20351;&#20195;&#29702;&#30340;&#36807;&#31243;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#36830;&#32493;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#30340;&#31574;&#30053;&#30340;&#21069;&#26223;&#12290;&#20026;&#20102;&#21019;&#24314;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#24120;&#38656;&#35201;&#38598;&#20013;&#30340;&#35757;&#32451;&#30446;&#26631;&#12289;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#26356;&#28789;&#27963;&#22320;&#21019;&#24314;&#36890;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#12289;&#21333;&#29420;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#21017;&#36825;&#26679;&#20570;&#23601;&#27604;&#36739;&#26377;&#24847;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#25110;&#24179;&#22343;&#19981;&#21516;MuJoCo&#36816;&#21160;&#38382;&#39064;&#19978;&#35757;&#32451;&#30340;Decision Transformer&#30340;&#23376;&#38598;&#26469;&#36808;&#20986;&#36825;&#20010;&#26041;&#21521;&#30340;&#21021;&#27493;&#27493;&#39588;&#65292;&#24418;&#25104;&#27809;&#26377;&#38598;&#20013;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24314;&#35758;&#22312;&#21512;&#24182;&#31574;&#30053;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#22914;&#26524;&#25152;&#26377;&#31574;&#30053;&#37117;&#20174;&#20849;&#21516;&#30340;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#24320;&#22987;&#65292;&#24182;&#22312;&#38382;&#39064;&#29305;&#23450;&#30340;&#24494;&#35843;&#26399;&#38388;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#30340;&#36741;&#21161;&#20219;&#21153;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#20855;&#26377;&#19968;&#33324;&#33021;&#21147;&#30340;&#20195;&#29702;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown the promise of creating generalist, transformer-based, policies for language, vision, and sequential decision-making problems. To create such models, we generally require centralized training objectives, data, and compute. It is of interest if we can more flexibly create generalist policies, by merging together multiple, task-specific, individually trained policies. In this work, we take a preliminary step in this direction through merging, or averaging, subsets of Decision Transformers in weight space trained on different MuJoCo locomotion problems, forming multi-task models without centralized training. We also propose that when merging policies, we can obtain better results if all policies start from common, pre-trained initializations, while also co-training on shared auxiliary tasks during problem-specific finetuning. In general, we believe research in this direction can help democratize and distribute the process of which forms generally capable agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#36827;&#21270;&#22686;&#24378;&#30340;&#20840;&#23616;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#20301;&#28857;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#20849;&#36827;&#21270;&#29305;&#24449;&#21644;&#20840;&#23616;&#20851;&#27880;&#26426;&#21046;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#27688;&#22522;&#37240;&#27531;&#22522;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#32771;&#34385;&#21040;&#25152;&#26377;&#27531;&#22522;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.06945</link><description>&lt;p&gt;
CoGANPPIS: &#22522;&#20110;&#20849;&#36827;&#21270;&#22686;&#24378;&#30340;&#20840;&#23616;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#20301;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for Protein-Protein Interaction Site Prediction. (arXiv:2303.06945v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#36827;&#21270;&#22686;&#24378;&#30340;&#20840;&#23616;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#20301;&#28857;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#20849;&#36827;&#21270;&#29305;&#24449;&#21644;&#20840;&#23616;&#20851;&#27880;&#26426;&#21046;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#27688;&#22522;&#37240;&#27531;&#22522;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#32771;&#34385;&#21040;&#25152;&#26377;&#27531;&#22522;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#22312;&#29983;&#21270;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20934;&#30830;&#39044;&#27979;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#20301;&#28857;&#65288;PPIs&#65289;&#21487;&#20197;&#21152;&#28145;&#25105;&#20204;&#23545;&#29983;&#29289;&#26426;&#29702;&#30340;&#29702;&#35299;&#65292;&#24182;&#23545;&#26032;&#33647;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;PPI&#39044;&#27979;&#23454;&#39564;&#26041;&#27861;&#25104;&#26412;&#39640;&#26114;&#65292;&#32791;&#26102;&#38271;&#65292;&#22240;&#27492;&#36817;&#24180;&#26469;&#24320;&#21457;&#20102;&#35768;&#22810;&#35745;&#31639;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#27169;&#22411;&#25366;&#25496;&#20102;&#19968;&#20123;&#26377;&#29992;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#20294;&#26410;&#32771;&#34385;&#21040;&#20849;&#36827;&#21270;&#29305;&#24449;&#65292;&#21518;&#32773;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#27688;&#22522;&#37240;&#27531;&#22522;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#32447;&#32034;&#65307;&#65288;2&#65289;attention-based&#27169;&#22411;&#20165;&#20026;&#30456;&#37051;&#27531;&#22522;&#20998;&#37197;&#20851;&#27880;&#26435;&#37325;&#65292;&#32780;&#19981;&#26159;&#20840;&#23616;&#20998;&#37197;&#65292;&#24573;&#30053;&#20102;&#36828;&#31163;&#30446;&#26631;&#27531;&#22522;&#30340;&#19968;&#20123;&#27531;&#22522;&#21487;&#33021;&#20063;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#36827;&#21270;&#22686;&#24378;&#30340;&#20840;&#23616;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;PPI&#20301;&#28857;&#39044;&#27979;&#30340;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#20849;&#36827;&#21270;&#29305;&#24449;&#21644;&#20840;&#23616;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#27688;&#22522;&#37240;&#27531;&#22522;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#32771;&#34385;&#21040;&#34507;&#30333;&#24207;&#21015;&#20013;&#25152;&#26377;&#27531;&#22522;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein-protein interactions are essential in biochemical processes. Accurate prediction of the protein-protein interaction sites (PPIs) deepens our understanding of biological mechanism and is crucial for new drug design. However, conventional experimental methods for PPIs prediction are costly and time-consuming so that many computational approaches, especially ML-based methods, have been developed recently. Although these approaches have achieved gratifying results, there are still two limitations: (1) Most models have excavated some useful input features, but failed to take coevolutionary features into account, which could provide clues for inter-residue relationships; (2) The attention-based models only allocate attention weights for neighboring residues, instead of doing it globally, neglecting that some residues being far away from the target residues might also matter.  We propose a coevolution-enhanced global attention neural network, a sequence-based deep learning model for P
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#32602;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#37096;&#20998;&#32447;&#24615;Cox&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#32954;&#30284;&#24739;&#32773;&#30340;CT&#25195;&#25551;&#20013;&#20998;&#26512;&#27515;&#20129;&#39118;&#38505;&#12290;&#35813;&#27169;&#22411;&#33021;&#26377;&#25928;&#22320;&#25972;&#21512;&#24050;&#30693;&#21644;&#26032;&#20852;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#35299;&#20915;&#20102;&#21442;&#25968;&#32500;&#24230;&#36229;&#20986;&#26679;&#26412;&#22823;&#23567;&#21644;&#38750;&#21442;&#25968;&#24314;&#27169;&#20013;&#32500;&#24230;&#28798;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.05341</link><description>&lt;p&gt;
&#21033;&#29992;&#32602;&#20989;&#25968;&#30340;&#28145;&#24230;&#37096;&#20998;&#32447;&#24615;Cox&#27169;&#22411;&#21450;&#20854;&#22312;&#32954;&#30284;&#24739;&#32773;CT&#25195;&#25551;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Penalized Deep Partially Linear Cox Models with Application to CT Scans of Lung Cancer Patients. (arXiv:2303.05341v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05341
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#32602;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#37096;&#20998;&#32447;&#24615;Cox&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#32954;&#30284;&#24739;&#32773;&#30340;CT&#25195;&#25551;&#20013;&#20998;&#26512;&#27515;&#20129;&#39118;&#38505;&#12290;&#35813;&#27169;&#22411;&#33021;&#26377;&#25928;&#22320;&#25972;&#21512;&#24050;&#30693;&#21644;&#26032;&#20852;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#35299;&#20915;&#20102;&#21442;&#25968;&#32500;&#24230;&#36229;&#20986;&#26679;&#26412;&#22823;&#23567;&#21644;&#38750;&#21442;&#25968;&#24314;&#27169;&#20013;&#32500;&#24230;&#28798;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#20840;&#29699;&#30284;&#30151;&#27515;&#20129;&#29575;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#31361;&#20986;&#20102;&#29702;&#35299;&#20854;&#27515;&#20129;&#39118;&#38505;&#23545;&#35774;&#35745;&#26377;&#25928;&#30340;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#27835;&#30103;&#30340;&#37325;&#35201;&#24615;&#12290;&#22269;&#23478;&#32954;&#37096;&#31579;&#26597;&#35797;&#39564;&#65288;NLST&#65289;&#37319;&#29992;&#20102;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#32441;&#29702;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;CT&#25195;&#25551;&#19978;&#32441;&#29702;&#27169;&#24335;&#30340;&#23458;&#35266;&#27979;&#37327;&#65292;&#29992;&#20110;&#37327;&#21270;&#32954;&#30284;&#24739;&#32773;&#30340;&#27515;&#20129;&#39118;&#38505;&#12290;&#37096;&#20998;&#32447;&#24615;Cox&#27169;&#22411;&#36890;&#36807;&#23558;&#39118;&#38505;&#20989;&#25968;&#20998;&#35299;&#20026;&#21442;&#25968;&#21644;&#38750;&#21442;&#25968;&#20998;&#37327;&#65292;&#25104;&#20026;&#29983;&#23384;&#20998;&#26512;&#20013;&#22791;&#21463;&#38738;&#30544;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#24050;&#30693;&#39118;&#38505;&#22240;&#32032;&#65288;&#22914;&#24180;&#40836;&#21644;&#20020;&#24202;&#21464;&#37327;&#65289;&#21644;&#26032;&#20852;&#39118;&#38505;&#22240;&#32032;&#65288;&#22914;&#22270;&#20687;&#29305;&#24449;&#65289;&#25972;&#21512;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#12290;&#28982;&#32780;&#65292;&#24403;&#21442;&#25968;&#20998;&#37327;&#30340;&#32500;&#24230;&#36229;&#36807;&#26679;&#26412;&#22823;&#23567;&#26102;&#65292;&#27169;&#22411;&#25311;&#21512;&#21464;&#24471;&#22256;&#38590;&#65292;&#32780;&#38750;&#21442;&#25968;&#24314;&#27169;&#21017;&#38754;&#20020;&#32500;&#24230;&#28798;&#38590;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32602;&#20989;&#25968;&#28145;&#24230;&#37096;&#20998;&#32447;&#24615;Cox&#27169;&#22411;&#65288;Penali
&lt;/p&gt;
&lt;p&gt;
Lung cancer is a leading cause of cancer mortality globally, highlighting the importance of understanding its mortality risks to design effective patient-centered therapies. The National Lung Screening Trial (NLST) employed computed tomography texture analysis, which provides objective measurements of texture patterns on CT scans, to quantify the mortality risks of lung cancer patients. Partially linear Cox models have gained popularity for survival analysis by dissecting the hazard function into parametric and nonparametric components, allowing for the effective incorporation of both well-established risk factors (such as age and clinical variables) and emerging risk factors (e.g., image features) within a unified framework. However, when the dimension of parametric components exceeds the sample size, the task of model fitting becomes formidable, while nonparametric modeling grapples with the curse of dimensionality. We propose a novel Penalized Deep Partially Linear Cox Model (Penali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOATS&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#21644;&#25968;&#37327;&#30446;&#26631;&#30340;&#20998;&#24067;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#33280;&#21462;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.05193</link><description>&lt;p&gt;
GOATS&#65306;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33280;&#21462;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning. (arXiv:2303.05193v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOATS&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#21644;&#25968;&#37327;&#30446;&#26631;&#30340;&#20998;&#24067;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#33280;&#21462;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#23545;&#26426;&#22120;&#20154;&#33280;&#21462;&#27700;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#38416;&#36848;&#12290;&#30001;&#20110;&#27969;&#20307;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#21644;&#23454;&#29616;&#22810;&#27169;&#24335;&#30446;&#26631;&#30340;&#38656;&#27714;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#29305;&#21035;&#30340;&#25361;&#25112;&#24615;&#12290;&#25919;&#31574;&#38656;&#35201;&#25104;&#21151;&#22320;&#36798;&#21040;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#65292;&#36825;&#23548;&#33268;&#19968;&#20010;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#30446;&#26631;&#29366;&#24577;&#31354;&#38388;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GOATS&#65292;&#19968;&#31181;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#20998;&#24067;&#21644;&#25968;&#37327;&#30446;&#26631;&#20998;&#24067;&#26469;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#65292;&#20351;&#29992;&#30446;&#26631;&#20998;&#35299;&#22870;&#21169;&#20844;&#24335;&#65292;&#23398;&#20064;&#19968;&#20010;&#39640;&#25928;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#26426;&#22120;&#20154;&#33280;&#21462;&#31574;&#30053;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20223;&#30495;&#20013;&#34920;&#29616;&#20986;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20998;&#21035;&#22312;&#30871;&#33280;&#21644;&#26742;&#33280;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;5.46&#65285;&#21644;8.71&#65285;&#30340;&#35823;&#24046;&#65292;&#28085;&#30422;&#20102;1000&#31181;&#21021;&#22987;&#27700;&#29366;&#24577;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we first formulate the problem of robotic water scooping using goal-conditioned reinforcement learning. This task is particularly challenging due to the complex dynamics of fluid and the need to achieve multi-modal goals. The policy is required to successfully reach both position goals and water amount goals, which leads to a large convoluted goal state space. To overcome these challenges, we introduce Goal Sampling Adaptation for Scooping (GOATS), a curriculum reinforcement learning method that can learn an effective and generalizable policy for robot scooping tasks. Specifically, we use a goal-factorized reward formulation and interpolate position goal distributions and amount goal distributions to create curriculum throughout the learning process. As a result, our proposed method can outperform the baselines in simulation and achieves 5.46% and 8.71% amount errors on bowl scooping and bucket scooping tasks, respectively, under 1000 variations of initial water states in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#12289;&#39640;&#24230;&#20934;&#30830;&#30340;&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;AI&#27169;&#22411;&#23545;&#20110;&#22810;&#20010;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01692</link><description>&lt;p&gt;
&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#65306;&#20844;&#27491;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Travel Demand Forecasting: A Fair AI Approach. (arXiv:2303.01692v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#12289;&#39640;&#24230;&#20934;&#30830;&#30340;&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;AI&#27169;&#22411;&#23545;&#20110;&#22810;&#20010;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#12290;&#23613;&#31649;&#22522;&#20110;AI&#30340;&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#33021;&#20135;&#29983;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#21487;&#33021;&#20250;&#20135;&#29983;&#39044;&#27979;&#20559;&#24046;&#24182;&#24341;&#21457;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#20351;&#29992;&#36825;&#20123;&#26377;&#20559;&#35265;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#21487;&#33021;&#20250;&#23548;&#33268;&#21152;&#21095;&#31038;&#20250;&#19981;&#24179;&#31561;&#30340;&#20132;&#36890;&#25919;&#31574;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#35299;&#20915;&#36825;&#20123;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#12289;&#39640;&#24230;&#20934;&#30830;&#30340;&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;AI&#27169;&#22411;&#23545;&#20110;&#22810;&#20010;&#21463;&#20445;&#25252;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#21644;&#25910;&#20837;&#65289;&#30340;&#20844;&#24179;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#39033;&#65292;&#35813;&#39033;&#26126;&#30830;&#22320;&#35774;&#35745;&#29992;&#20110;&#34913;&#37327;&#39044;&#27979;&#20934;&#30830;&#24615;&#19982;&#22810;&#20010;&#21463;&#20445;&#25252;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#20854;&#21152;&#20837;&#21040;&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) and machine learning have been increasingly adopted for travel demand forecasting. The AI-based travel demand forecasting models, though generate accurate predictions, may produce prediction biases and raise fairness issues. Using such biased models for decision-making may lead to transportation policies that exacerbate social inequalities. However, limited studies have been focused on addressing the fairness issues of these models. Therefore, in this study, we propose a novel methodology to develop fairness-aware, highly-accurate travel demand forecasting models. Particularly, the proposed methodology can enhance the fairness of AI models for multiple protected attributes (such as race and income) simultaneously. Specifically, we introduce a new fairness regularization term, which is explicitly designed to measure the correlation between prediction accuracy and multiple protected attributes, into the loss function of the travel demand forecasting model. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#24494;&#23376;&#27169;&#26368;&#22823;&#21270;&#23398;&#20064;&#20989;&#25968;&#65292;&#23558;&#19978;&#19979;&#25991;&#35266;&#23519;&#26144;&#23556;&#21040;&#23376;&#27169;&#20989;&#25968;&#21442;&#25968;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#32771;&#34385;&#19979;&#28216;&#20219;&#21153;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;&#30340;&#29420;&#31435;&#35299;&#20915;&#39044;&#27979;&#38454;&#27573;&#30340;&#23398;&#20064;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#19982;&#26368;&#32456;&#30446;&#26631;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.01543</link><description>&lt;p&gt;
&#38754;&#21521;&#20915;&#31574;&#30340;&#21487;&#24494;&#23376;&#27169;&#26368;&#22823;&#21270;&#23398;&#20064;&#22312;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Decision-Oriented Learning with Differentiable Submodular Maximization for Vehicle Routing Problem. (arXiv:2303.01543v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#24494;&#23376;&#27169;&#26368;&#22823;&#21270;&#23398;&#20064;&#20989;&#25968;&#65292;&#23558;&#19978;&#19979;&#25991;&#35266;&#23519;&#26144;&#23556;&#21040;&#23376;&#27169;&#20989;&#25968;&#21442;&#25968;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#32771;&#34385;&#19979;&#28216;&#20219;&#21153;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;&#30340;&#29420;&#31435;&#35299;&#20915;&#39044;&#27979;&#38454;&#27573;&#30340;&#23398;&#20064;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#19982;&#26368;&#32456;&#30446;&#26631;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#19968;&#20010;&#23558;&#19978;&#19979;&#25991;&#35266;&#23519;&#65288;&#36755;&#20837;&#65289;&#26144;&#23556;&#21040;&#23376;&#27169;&#20989;&#25968;&#21442;&#25968;&#65288;&#36755;&#20986;&#65289;&#30340;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#26159;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#32452;&#26080;&#20154;&#22320;&#38754;&#36710;&#36742;&#65288;UGV&#65289;&#21487;&#20197;&#20316;&#20026;&#31227;&#21160;&#20805;&#30005;&#31449;&#20026;&#25191;&#34892;&#25345;&#32493;&#30417;&#27979;&#20219;&#21153;&#30340;&#26080;&#20154;&#26426;&#22320;&#38754;&#36710;&#36742;&#65288;UAV&#65289;&#20805;&#30005;&#12290;&#25105;&#20204;&#24076;&#26395;&#23398;&#20064;&#20174;UAV&#20219;&#21153;&#36335;&#24452;&#21644;&#39118;&#22330;&#35266;&#23519;&#21040;&#23376;&#27169;&#30446;&#26631;&#20989;&#25968;&#21442;&#25968;&#30340;&#26144;&#23556;&#65292;&#35813;&#20989;&#25968;&#25551;&#36848;&#20102;UAV&#30340;&#38477;&#33853;&#20301;&#32622;&#20998;&#24067;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#31867;&#23398;&#20064;&#38382;&#39064;&#29420;&#31435;&#35299;&#20915;&#39044;&#27979;&#38454;&#27573;&#65292;&#32780;&#19981;&#32771;&#34385;&#19979;&#28216;&#20219;&#21153;&#20248;&#21270;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#20013;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#33021;&#19982;&#25105;&#20204;&#30340;&#26368;&#32456;&#30446;&#26631;&#65292;&#21363;&#33391;&#22909;&#33322;&#32447;&#20915;&#31574;&#65292;&#19981;&#19968;&#33268;&#12290;&#22312;&#23396;&#31435;&#30340;&#39044;&#27979;&#38454;&#27573;&#34920;&#29616;&#33391;&#22909;&#24182;&#19981;&#19968;&#23450;&#20250;&#23548;&#33268;&#19979;&#28216;&#36335;&#24452;&#20219;&#21153;&#20013;&#30340;&#33391;&#22909;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning a function that maps context observations (input) to parameters of a submodular function (output). Our motivating case study is a specific type of vehicle routing problem, in which a team of Unmanned Ground Vehicles (UGVs) can serve as mobile charging stations to recharge a team of Unmanned Ground Vehicles (UAVs) that execute persistent monitoring tasks. {We want to learn the mapping from observations of UAV task routes and wind field to the parameters of a submodular objective function, which describes the distribution of landing positions of the UAVs .} Traditionally, such a learning problem is solved independently as a prediction phase without considering the downstream task optimization phase. However, the loss function used in prediction may be misaligned with our final goal, i.e., a good routing decision. Good performance in the isolated prediction phase does not necessarily lead to good decisions in the downstream routing task. In this paper, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#32858;&#31867;&#25216;&#26415;&#35774;&#35745;&#24182;&#25191;&#34892;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#35745;&#21010;&#30340;&#21487;&#34892;&#24615;&#65292;&#30446;&#30340;&#26159;&#25913;&#21464;&#20998;&#24067;&#24335;&#33021;&#28304;&#31038;&#21306;&#20869;&#20379;&#24212;&#32773;&#30340;&#28040;&#36153;&#34892;&#20026;&#65292;&#20197;&#26368;&#23567;&#21270;&#21453;&#21521;&#21151;&#29575;&#27969;&#21644;&#21066;&#20943;&#31995;&#32479;&#33539;&#22260;&#20869;&#30340;&#21151;&#23792;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.00186</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#25216;&#26415;&#30340;&#28789;&#27963;&#33021;&#28304;&#31038;&#21306;&#30446;&#26631;&#38656;&#27714;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Targeted demand response for flexible energy communities using clustering techniques. (arXiv:2303.00186v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#32858;&#31867;&#25216;&#26415;&#35774;&#35745;&#24182;&#25191;&#34892;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#35745;&#21010;&#30340;&#21487;&#34892;&#24615;&#65292;&#30446;&#30340;&#26159;&#25913;&#21464;&#20998;&#24067;&#24335;&#33021;&#28304;&#31038;&#21306;&#20869;&#20379;&#24212;&#32773;&#30340;&#28040;&#36153;&#34892;&#20026;&#65292;&#20197;&#26368;&#23567;&#21270;&#21453;&#21521;&#21151;&#29575;&#27969;&#21644;&#21066;&#20943;&#31995;&#32479;&#33539;&#22260;&#20869;&#30340;&#21151;&#23792;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#32858;&#31867;&#25216;&#26415;&#20026;&#21830;&#19994;&#21644;&#20303;&#23429;&#31038;&#21306;&#30340;&#33021;&#37327;&#20379;&#24212;&#32773;&#35774;&#35745;&#21644;&#25191;&#34892;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#35745;&#21010;&#30340;&#21487;&#33021;&#24615;&#12290;&#35813;&#35745;&#21010;&#30340;&#30446;&#30340;&#26159;&#25913;&#21464;&#24847;&#22823;&#21033;&#20998;&#24067;&#24335;&#33021;&#28304;&#31038;&#21306;&#20869;&#30340;&#20379;&#24212;&#32773;&#30340;&#28040;&#36153;&#34892;&#20026;&#12290;&#36825;&#31181;&#32858;&#21512;&#26088;&#22312;&#65306;a&#65289;&#26368;&#23567;&#21270;&#22312;&#20027;&#35201;&#21464;&#30005;&#31449;&#22788;&#20135;&#29983;&#30340;&#21453;&#21521;&#21151;&#29575;&#27969;&#65292;&#35813;&#21151;&#29575;&#27969;&#22312;&#24403;&#22320;&#30005;&#32593;&#20013;&#30340;&#22826;&#38451;&#33021;&#30005;&#27744;&#30340;&#21457;&#30005;&#37327;&#36229;&#36807;&#28040;&#32791;&#26102;&#20250;&#21457;&#29983;; b&#65289;&#21066;&#20943;&#31995;&#32479;&#33539;&#22260;&#20869;&#30340;&#21151;&#23792;&#38656;&#27714;&#65292;&#35813;&#38656;&#27714;&#36890;&#24120;&#21457;&#29983;&#22312;&#20621;&#26202;&#26102;&#20998;&#12290;&#22312;&#32858;&#31867;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#28909;&#38376;&#30340;&#30005;&#36127;&#33655;&#32858;&#31867;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;-&#21363;k-means&#65292;k-medoids&#21644;&#19968;&#31181;&#32858;&#21512;&#23618;&#27425;&#32858;&#31867;-alongside&#20004;&#31181;&#19981;&#21516;&#30340;&#36317;&#31163;&#24230;&#37327;-&#21363;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#21463;&#38480;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#65288;DTW&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#39564;&#35777;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#39033;&#26032;&#39062;&#30340;&#25351;&#26631;-&#21363;&#23792;&#20540;&#24615;&#33021;&#35780;&#20998;&#65288;PPS&#65289;
&lt;/p&gt;
&lt;p&gt;
The present study explores the use of clustering techniques for the design and implementation of a demand response (DR) program for commercial and residential prosumers. The goal of the program is to alter the consumption behavior of the prosumers pertaining to a distributed energy community in Italy. This aggregation aims to: a) minimize the reverse power flow at the primary substation, that occurs when generation from solar panels in the local grid exceeds consumption, and b) shave the system wide peak demand, that typically occurs during the hours of late afternoon. Regarding the clustering stage, three popular machine learning algorithms for electrical load clustering are employed -namely k-means, k-medoids and an agglomerative hierarchical clustering- alongside two different distance measures -namely euclidean and constrained dynamic time warping (DTW). We evaluate the methods using multiple validation metrics including a novel metric -namely peak performance score (PPS)- that we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;&#30340;&#35774;&#35745;&#65292;&#36890;&#36807;&#23545;&#26435;&#37325;&#36827;&#34892;&#32622;&#25442;&#23545;&#31216;&#24615;&#32534;&#30721;&#65292;&#23454;&#29616;&#23545;&#20854;&#20182;&#32593;&#32476;&#26435;&#37325;&#25110;&#26799;&#24230;&#36827;&#34892;&#22788;&#29702;&#65292;&#20026;&#23398;&#20064;&#20248;&#21270;&#12289;&#22788;&#29702;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#31561;&#24212;&#29992;&#25552;&#20379;&#20102;&#26550;&#26500;&#21407;&#21017;&#12290;</title><link>http://arxiv.org/abs/2302.14040</link><description>&lt;p&gt;
&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Permutation Equivariant Neural Functionals. (arXiv:2302.14040v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;&#30340;&#35774;&#35745;&#65292;&#36890;&#36807;&#23545;&#26435;&#37325;&#36827;&#34892;&#32622;&#25442;&#23545;&#31216;&#24615;&#32534;&#30721;&#65292;&#23454;&#29616;&#23545;&#20854;&#20182;&#32593;&#32476;&#26435;&#37325;&#25110;&#26799;&#24230;&#36827;&#34892;&#22788;&#29702;&#65292;&#20026;&#23398;&#20064;&#20248;&#21270;&#12289;&#22788;&#29702;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#31561;&#24212;&#29992;&#25552;&#20379;&#20102;&#26550;&#26500;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33021;&#22815;&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#25110;&#26799;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;&#65288;NFN&#65289;&#12290;&#23613;&#31649;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#21253;&#25324;&#23398;&#20064;&#20248;&#21270;&#12289;&#22788;&#29702;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#32593;&#32476;&#32534;&#36753;&#21644;&#31574;&#30053;&#35780;&#20272;&#65292;&#20294;&#35774;&#35745;&#22788;&#29702;&#20854;&#20182;&#32593;&#32476;&#26435;&#37325;&#30340;&#26377;&#25928;&#26550;&#26500;&#30340;&#32479;&#19968;&#21407;&#21017;&#24456;&#23569;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#31216;&#24615;&#30340;&#35270;&#35282;&#26469;&#35774;&#35745;&#31070;&#32463;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20851;&#27880;&#28145;&#24230;&#21069;&#39304;&#32593;&#32476;&#26435;&#37325;&#20013;&#20986;&#29616;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#22240;&#20026;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#27809;&#26377;&#22266;&#26377;&#39034;&#24207;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26500;&#24314;&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#36825;&#20123;&#23545;&#31216;&#24615;&#32534;&#30721;&#20026;&#24402;&#32435;&#20559;&#24046;&#12290;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#30340;&#21442;&#25968;&#26469;&#32422;&#26463;&#20026;&#32622;&#25442;&#31561;&#21464;&#30340;NF-Layers&#65288;&#31070;&#32463;&#21151;&#33021;&#23618;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies the design of neural networks that can process the weights or gradients of other neural networks, which we refer to as neural functional networks (NFNs). Despite a wide range of potential applications, including learned optimization, processing implicit neural representations, network editing, and policy evaluation, there are few unifying principles for designing effective architectures that process the weights of other networks. We approach the design of neural functionals through the lens of symmetry, in particular by focusing on the permutation symmetries that arise in the weights of deep feedforward networks because hidden layer neurons have no inherent order. We introduce a framework for building permutation equivariant neural functionals, whose architectures encode these symmetries as an inductive bias. The key building blocks of this framework are NF-Layers (neural functional layers) that we constrain to be permutation equivariant through an appropriate paramet
&lt;/p&gt;</description></item><item><title>DeepBrainPrint&#26159;&#19968;&#20010;&#29992;&#20110;&#33041;MRI&#20877;&#35782;&#21035;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#20351;&#29992;&#21322;&#33258;&#30417;&#30563;&#23545;&#27604;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#26377;&#25928;&#30340;&#33041;&#25351;&#32441;&#36827;&#34892;&#23454;&#26102;&#22270;&#20687;&#26816;&#32034;&#65292;&#24182;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#36716;&#25442;&#26041;&#27861;&#20197;&#25552;&#39640;&#26816;&#32034;&#30340;&#31283;&#20581;&#24615;&#21644;&#32771;&#34385;&#24739;&#32773;&#30340;&#24180;&#40836;&#21644;&#30142;&#30149;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.13057</link><description>&lt;p&gt;
DeepBrainPrint: &#19968;&#31181;&#29992;&#20110;&#33041;MRI&#20877;&#35782;&#21035;&#30340;&#26032;&#22411;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification. (arXiv:2302.13057v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13057
&lt;/p&gt;
&lt;p&gt;
DeepBrainPrint&#26159;&#19968;&#20010;&#29992;&#20110;&#33041;MRI&#20877;&#35782;&#21035;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#20351;&#29992;&#21322;&#33258;&#30417;&#30563;&#23545;&#27604;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#26377;&#25928;&#30340;&#33041;&#25351;&#32441;&#36827;&#34892;&#23454;&#26102;&#22270;&#20687;&#26816;&#32034;&#65292;&#24182;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#36716;&#25442;&#26041;&#27861;&#20197;&#25552;&#39640;&#26816;&#32034;&#30340;&#31283;&#20581;&#24615;&#21644;&#32771;&#34385;&#24739;&#32773;&#30340;&#24180;&#40836;&#21644;&#30142;&#30149;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;MRI&#25216;&#26415;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#12290;&#38543;&#30528;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#23450;&#20301;&#21516;&#19968;&#24739;&#32773;&#30340;&#20808;&#21069;&#25195;&#25551;&#21464;&#24471;&#22256;&#38590;&#65288;&#36825;&#20010;&#36807;&#31243;&#31216;&#20026;&#20877;&#35782;&#21035;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DeepBrainPrint&#30340;&#20154;&#24037;&#26234;&#33021;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#32034;&#21516;&#19968;&#24739;&#32773;&#30340;&#33041;MRI&#25195;&#25551;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#19968;&#31181;&#21322;&#33258;&#30417;&#30563;&#23545;&#27604;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20855;&#26377;&#19977;&#20010;&#20027;&#35201;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#33539;&#24335;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#26377;&#25928;&#30340;&#33041;&#25351;&#32441;&#65292;&#20197;&#29992;&#20110;&#23454;&#26102;&#22270;&#20687;&#26816;&#32034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#29305;&#27530;&#30340;&#21152;&#26435;&#20989;&#25968;&#26469;&#24341;&#23548;&#35757;&#32451;&#24182;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22270;&#20687;&#36716;&#25442;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22312;&#19981;&#21516;&#25195;&#25551;&#23545;&#27604;&#24230;&#65288;&#21363;&#19981;&#21516;&#25195;&#25551;&#21453;&#24046;&#65289;&#19979;&#30340;&#26816;&#32034;&#31283;&#20581;&#24615;&#65292;&#24182;&#32771;&#34385;&#24739;&#32773;&#30340;&#24180;&#40836;&#21644;&#30142;&#30149;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in MRI have led to the creation of large datasets. With the increase in data volume, it has become difficult to locate previous scans of the same patient within these datasets (a process known as re-identification). To address this issue, we propose an AI-powered medical imaging retrieval framework called DeepBrainPrint, which is designed to retrieve brain MRI scans of the same patient. Our framework is a semi-self-supervised contrastive deep learning approach with three main innovations. First, we use a combination of self-supervised and supervised paradigms to create an effective brain fingerprint from MRI scans that can be used for real-time image retrieval. Second, we use a special weighting function to guide the training and improve model convergence. Third, we introduce new imaging transformations to improve retrieval robustness in the presence of intensity variations (i.e. different scan contrasts), and to account for age and disease progression in patients. We t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25552;&#21069;&#19968;&#22825;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#33889;&#33796;&#29273;&#22269;&#23478;&#20928;&#32858;&#21512;STLF&#65292;&#24182;&#20998;&#26512;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#12289;&#31070;&#32463;&#22522;&#30784;&#25193;&#23637;&#31995;&#25968;&#20998;&#26512;&#65288;N-BEATS&#65289;&#12289;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#65288;TCN&#65289;&#21644;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#65288;TFT&#65289;&#31561;&#22810;&#20010;&#27169;&#22411;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2302.12168</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25552;&#21069;&#19968;&#22825;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#27604;&#36739;&#35780;&#20272;&#65306;&#30740;&#31350;&#20851;&#38190;&#30340;&#20934;&#30830;&#24615;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
A comparative assessment of deep learning models for day-ahead load forecasting: Investigating key accuracy drivers. (arXiv:2302.12168v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25552;&#21069;&#19968;&#22825;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#33889;&#33796;&#29273;&#22269;&#23478;&#20928;&#32858;&#21512;STLF&#65292;&#24182;&#20998;&#26512;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#12289;&#31070;&#32463;&#22522;&#30784;&#25193;&#23637;&#31995;&#25968;&#20998;&#26512;&#65288;N-BEATS&#65289;&#12289;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#65288;TCN&#65289;&#21644;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#65288;TFT&#65289;&#31561;&#22810;&#20010;&#27169;&#22411;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#65288;STLF&#65289;&#23545;&#30005;&#32593;&#21644;&#33021;&#28304;&#24066;&#22330;&#30340;&#26377;&#25928;&#21644;&#32463;&#27982;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30005;&#21147;&#38656;&#27714;&#30340;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#20197;&#21450;&#20854;&#23545;&#21508;&#31181;&#22806;&#37096;&#22240;&#32032;&#30340;&#20381;&#36182;&#24615;&#20351;&#24471;STLF&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#25552;&#21069;&#19968;&#22825;&#30340;&#39044;&#27979;&#29615;&#22659;&#19979;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#19987;&#27880;&#20110;&#33889;&#33796;&#29273;&#30340;&#22269;&#23478;&#20928;&#32858;&#21512;STLF&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;&#65292;&#32771;&#34385;&#20102;&#19968;&#32452;&#26377;&#25351;&#31034;&#24615;&#30340;&#12289;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21253;&#25324;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#12289;&#31070;&#32463;&#22522;&#30784;&#25193;&#23637;&#31995;&#25968;&#20998;&#26512;&#65288;N-BEATS&#65289;&#12289;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#65288;TCN&#65289;&#21644;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#65288;TFT&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#26174;&#33879;&#24433;&#21709;&#38656;&#27714;&#30340;&#22240;&#32032;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#27599;&#20010;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short-term load forecasting (STLF) is vital for the effective and economic operation of power grids and energy markets. However, the non-linearity and non-stationarity of electricity demand as well as its dependency on various external factors renders STLF a challenging task. To that end, several deep learning models have been proposed in the literature for STLF, reporting promising results. In order to evaluate the accuracy of said models in day-ahead forecasting settings, in this paper we focus on the national net aggregated STLF of Portugal and conduct a comparative study considering a set of indicative, well-established deep autoregressive models, namely multi-layer perceptrons (MLP), long short-term memory networks (LSTM), neural basis expansion coefficient analysis (N-BEATS), temporal convolutional networks (TCN), and temporal fusion transformers (TFT). Moreover, we identify factors that significantly affect the demand and investigate their impact on the accuracy of each model. O
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.11939</link><description>&lt;p&gt;
&#19968;&#31449;&#24335;&#35299;&#20915;&#26041;&#26696;&#65306;&#21033;&#29992;&#39044;&#35757;&#32451; LM &#36827;&#34892;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#21644;&#35745;&#31639;&#26426;&#35270;&#35273; (CV) &#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#19982; NLP &#21644; CV &#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#39046;&#22495;&#37319;&#29992;&#32479;&#19968;&#27169;&#22411;&#21363;&#21487;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#32780;&#22312;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#19987;&#38376;&#35774;&#35745;&#30340;&#26041;&#27861;&#20173;&#28982;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22914;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#38459;&#30861;&#39044;&#35757;&#32451;&#27169;&#22411;&#21457;&#23637;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#22823;&#37327;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36991;&#20813;&#25913;&#21464;&#39044;&#35757;&#32451;&#35821;&#35328;&#25110;&#22270;&#20687;&#27169;&#22411;&#20013;&#27531;&#24046;&#22359;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#21521;&#20256;&#36882;&#23618;&#12290;&#36825;&#31181;&#27169;&#22411;&#34987;&#31216;&#20026;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120; (FPT)&#65292;&#36890;&#36807;&#23545;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#30340;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;FPT &#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22270;&#23545;&#25239;&#20813;&#30123;&#26041;&#27861;&#65292;&#36890;&#36807;&#30123;&#33495;&#25509;&#31181;&#19968;&#37096;&#20998;&#22270;&#32467;&#26500;&#26469;&#25552;&#39640;&#22270;&#30340;&#40065;&#26834;&#24615;&#65292;&#36991;&#20813;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#36890;&#36807;&#36793;&#21644;&#33410;&#28857;&#20004;&#31181;&#20813;&#30123;&#26041;&#24335;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#22270;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.08051</link><description>&lt;p&gt;
&#22270;&#23545;&#25239;&#20813;&#30123;&#20197;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Graph Adversarial Immunization for Certifiable Robustness. (arXiv:2302.08051v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22270;&#23545;&#25239;&#20813;&#30123;&#26041;&#27861;&#65292;&#36890;&#36807;&#30123;&#33495;&#25509;&#31181;&#19968;&#37096;&#20998;&#22270;&#32467;&#26500;&#26469;&#25552;&#39640;&#22270;&#30340;&#40065;&#26834;&#24615;&#65292;&#36991;&#20813;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#36890;&#36807;&#36793;&#21644;&#33410;&#28857;&#20004;&#31181;&#20813;&#30123;&#26041;&#24335;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#22270;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23545;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#26159;&#33030;&#24369;&#30340;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#38598;&#20013;&#22312;&#24320;&#21457;&#23545;&#25239;&#24615;&#35757;&#32451;&#25110;&#27169;&#22411;&#20462;&#25913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#21046;&#23450;&#20102;&#22270;&#23545;&#25239;&#20813;&#30123;&#65292;&#21363;&#36890;&#36807;&#30123;&#33495;&#25509;&#31181;&#22270;&#32467;&#26500;&#30340;&#19968;&#37096;&#20998;&#26469;&#25913;&#21892;&#22270;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#23545;&#25239;&#20219;&#20309;&#21487;&#25509;&#21463;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22522;&#20110;&#36793;&#30340;&#20813;&#30123;&#65292;&#26469;&#23545;&#33410;&#28857;&#23545;&#36827;&#34892;&#30123;&#33495;&#25509;&#31181;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#22522;&#20110;&#36793;&#30340;&#20813;&#30123;&#19981;&#33021;&#25269;&#24481;&#26032;&#20986;&#29616;&#30340;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#65292;&#22240;&#20026;&#23427;&#21482;&#23545;&#29616;&#26377;&#30340;&#33410;&#28857;&#23545;&#36827;&#34892;&#20813;&#30123;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#33410;&#28857;&#30340;&#20813;&#30123;&#12290;&#20026;&#20102;&#36991;&#20813;&#19982;&#23545;&#25239;&#24615;&#20813;&#30123;&#30456;&#20851;&#30340;&#35745;&#31639;&#23494;&#38598;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AdvImmune-Edge&#21644;AdvImmune-Node&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#33719;&#21462;&#20813;&#30123;&#33410;&#28857;&#23545;&#25110;&#33410;&#28857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;AdvImmune&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;AdvImmune-Node&#26174;&#33879;&#25552;&#39640;&#20102;r&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite achieving great success, graph neural networks (GNNs) are vulnerable to adversarial attacks. Existing defenses focus on developing adversarial training or model modification. In this paper, we propose and formulate graph adversarial immunization, i.e., vaccinating part of graph structure to improve certifiable robustness of graph against any admissible adversarial attack. We first propose edge-level immunization to vaccinate node pairs. Unfortunately, such edge-level immunization cannot defend against emerging node injection attacks, since it only immunizes existing node pairs. To this end, we further propose node-level immunization. To avoid computationally intensive combinatorial optimization associated with adversarial immunization, we develop AdvImmune-Edge and AdvImmune-Node algorithms to effectively obtain the immune node pairs or nodes. Extensive experiments demonstrate the superiority of AdvImmune methods. In particular, AdvImmune-Node remarkably improves the ratio of r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Flag Aggregator&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#25925;&#38556;&#21644;&#22686;&#37327;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#36807;&#31243;&#21644;Beta&#23494;&#24230;&#26469;&#36827;&#34892;&#24037;&#20316;&#33410;&#28857;&#30340;&#32858;&#21512;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#26368;&#23567;&#20108;&#20056;&#27714;&#35299;&#22120;&#36817;&#20284;&#35299;&#20915;&#20102;&#27491;&#21017;&#21270;&#23376;&#31354;&#38388;&#20284;&#28982;&#20272;&#35745;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2302.05865</link><description>&lt;p&gt;
Flag Aggregator: &#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#22312;&#25925;&#38556;&#21644;&#22686;&#37327;&#25439;&#22833;&#19979;&#20351;&#29992;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization. (arXiv:2302.05865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Flag Aggregator&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#25925;&#38556;&#21644;&#22686;&#37327;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#36807;&#31243;&#21644;Beta&#23494;&#24230;&#26469;&#36827;&#34892;&#24037;&#20316;&#33410;&#28857;&#30340;&#32858;&#21512;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#26368;&#23567;&#20108;&#20056;&#27714;&#35299;&#22120;&#36817;&#20284;&#35299;&#20915;&#20102;&#27491;&#21017;&#21270;&#23376;&#31354;&#38388;&#20284;&#28982;&#20272;&#35745;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35757;&#32451;&#26368;&#22823;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#25193;&#23637;&#35745;&#31639;&#21644;&#25968;&#25454;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#21487;&#36991;&#20813;&#22320;&#20197;&#38598;&#32676;&#33410;&#28857;&#30340;&#20998;&#24067;&#24335;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;&#27169;&#22411;&#20043;&#21069;&#36827;&#34892;&#26356;&#26032;&#30340;&#32858;&#21512;&#12290;&#28982;&#32780;&#65292;&#20998;&#24067;&#24335;&#35774;&#32622;&#23481;&#26131;&#21463;&#21040;&#20010;&#21035;&#33410;&#28857;&#12289;&#32452;&#20214;&#21644;&#36719;&#20214;&#30340;&#25308;&#21344;&#24237;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#21152;&#20837;&#25968;&#25454;&#22686;&#24378;&#21518;&#65292;&#23545;&#20110;&#31283;&#20581;&#19988;&#39640;&#25928;&#30340;&#32858;&#21512;&#31995;&#32479;&#26377;&#30528;&#37325;&#35201;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23558;&#24037;&#20316;&#36136;&#37327;&#23450;&#20041;&#20026;&#37325;&#24314;&#27604;&#29575;&#65288;&#20171;&#20110;(0,1]&#20043;&#38388;&#65289;&#65292;&#24182;&#23558;&#32858;&#21512;&#23450;&#20041;&#20026;&#20351;&#29992;Beta&#23494;&#24230;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27491;&#21017;&#21270;&#23545;&#23376;&#31354;&#38388;&#20284;&#28982;&#20272;&#35745;&#30340;&#36817;&#20284;&#27714;&#35299;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#30340;&#20984;&#20248;&#21270;&#26223;&#35266;&#32467;&#26524;&#25552;&#20379;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern ML applications increasingly rely on complex deep learning models and large datasets. There has been an exponential growth in the amount of computation needed to train the largest models. Therefore, to scale computation and data, these models are inevitably trained in a distributed manner in clusters of nodes, and their updates are aggregated before being applied to the model. However, a distributed setup is prone to Byzantine failures of individual nodes, components, and software. With data augmentation added to these settings, there is a critical need for robust and efficient aggregation systems. We define the quality of workers as reconstruction ratios $\in (0,1]$, and formulate aggregation as a Maximum Likelihood Estimation procedure using Beta densities. We show that the Regularized form of log-likelihood wrt subspace can be approximately solved using iterative least squares solver, and provide convergence guarantees using recent Convex Optimization landscape results. Our e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#35299;&#20915;&#20102;&#38480;&#21046;&#24179;&#34913;&#28857;Gram&#30697;&#38453;&#26368;&#23567;&#29305;&#24449;&#20540;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.05797</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Global Convergence Rate of Deep Equilibrium Models with General Activations. (arXiv:2302.05797v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#35299;&#20915;&#20102;&#38480;&#21046;&#24179;&#34913;&#28857;Gram&#30697;&#38453;&#26368;&#23567;&#29305;&#24449;&#20540;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#19968;&#31687;&#35770;&#25991;&#20013;&#65292;Ling&#31561;&#20154;&#30740;&#31350;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#36807;&#21442;&#25968;&#21270;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#12290;&#20182;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#65292;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#20855;&#26377;&#20219;&#20309;&#20855;&#26377;&#26377;&#30028;&#19968;&#38454;&#21644;&#20108;&#38454;&#23548;&#25968;&#30340;&#28608;&#27963;&#20989;&#25968;&#30340;DEQ&#65292;&#35813;&#20107;&#23454;&#20173;&#28982;&#25104;&#31435;&#12290;&#30001;&#20110;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#38480;&#21046;&#24179;&#34913;&#28857;&#30340;Gram&#30697;&#38453;&#30340;&#26368;&#23567;&#29305;&#24449;&#20540;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#38656;&#35201;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#24635;&#20307;Gram&#30697;&#38453;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#20855;&#26377;Hermite&#22810;&#39033;&#24335;&#23637;&#24320;&#30340;&#26032;&#24418;&#24335;&#30340;&#21452;&#37325;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a recent paper, Ling et al. investigated the over-parametrized Deep Equilibrium Model (DEQ) with ReLU activation. They proved that the gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. This paper shows that this fact still holds for DEQs with any general activation that has bounded first and second derivatives. Since the new activation function is generally non-linear, bounding the least eigenvalue of the Gram matrix of the equilibrium point is particularly challenging. To accomplish this task, we need to create a novel population Gram matrix and develop a new form of dual activation with Hermite polynomial expansion.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#20102;&#31616;&#21270;Gr\"obner&#22522;&#30340;&#22522;&#25968;&#21644;&#26368;&#22823;&#24635;&#24230;&#25968;&#65292;&#32467;&#26524;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#32479;&#35745;&#65292;&#30456;&#27604;&#20110;&#26420;&#32032;&#29468;&#27979;&#25110;&#22810;&#20803;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.05364</link><description>&lt;p&gt;
&#39044;&#27979;&#31616;&#21270;Gr\"obner&#22522;&#25968;&#21644;&#26368;&#22823;&#24230;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predicting the cardinality and maximum degree of a reduced Gr\"obner basis. (arXiv:2302.05364v2 [math.AC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#20102;&#31616;&#21270;Gr\"obner&#22522;&#30340;&#22522;&#25968;&#21644;&#26368;&#22823;&#24635;&#24230;&#25968;&#65292;&#32467;&#26524;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#32479;&#35745;&#65292;&#30456;&#27604;&#20110;&#26420;&#32032;&#29468;&#27979;&#25110;&#22810;&#20803;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24314;&#20102;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20197;&#39044;&#27979;&#20108;&#39033;&#29702;&#24819;&#30340;Gr\"obner&#22522;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#36825;&#39033;&#24037;&#20316;&#35828;&#26126;&#20102;&#20026;&#20160;&#20040;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;Gr\"obner&#35745;&#31639;&#20013;&#36827;&#34892;&#39044;&#27979;&#24182;&#19981;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#25552;&#20379;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;Gr\"obner&#22797;&#26434;&#24230;&#30340;&#36275;&#22815;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24182;&#39044;&#27979;&#31616;&#21270;Gr\"obner&#22522;&#30340;&#22522;&#25968;&#21644;&#20854;&#20803;&#32032;&#30340;&#26368;&#22823;&#24635;&#24230;&#25968;&#12290;&#34429;&#28982;&#22522;&#25968;&#39044;&#27979;&#38382;&#39064;&#19981;&#21516;&#20110;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#32479;&#35745;&#65292;&#22914;$r^2 = 0.401$&#65292;&#30456;&#27604;&#20110;&#26420;&#32032;&#29468;&#27979;&#25110;&#22810;&#20803;&#22238;&#24402;&#27169;&#22411;&#30340;$r^2 = 0.180$&#12290;
&lt;/p&gt;
&lt;p&gt;
We construct neural network regression models to predict key metrics of complexity for Gr\"obner bases of binomial ideals. This work illustrates why predictions with neural networks from Gr\"obner computations are not a straightforward process. Using two probabilistic models for random binomial ideals, we generate and make available a large data set that is able to capture sufficient variability in Gr\"obner complexity. We use this data to train neural networks and predict the cardinality of a reduced Gr\"obner basis and the maximum total degree of its elements. While the cardinality prediction problem is unlike classical problems tackled by machine learning, our simulations show that neural networks, providing performance statistics such as $r^2 = 0.401$, outperform naive guess or multiple regression models with $r^2 = 0.180$.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HydroGraphs &#30340;&#22522;&#20110;&#22270;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#27700;&#25991;&#27745;&#26579;&#29289;&#30340;&#20256;&#36755;&#21644;&#21629;&#36816;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;&#24320;&#28304;&#25968;&#25454;&#26500;&#24314;&#65292;&#20855;&#26377;&#31616;&#21270;&#30340;&#27700;&#25991;&#31995;&#32479;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#22270;&#20998;&#26512;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#25216;&#26415;&#36827;&#34892;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#65292;&#33021;&#22815;&#24110;&#21161;&#20934;&#30830;&#23450;&#20301;&#27745;&#26579;&#28304;&#21644;&#33030;&#24369;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2302.04991</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#24314;&#27169;&#26694;&#26550;&#29992;&#20110;&#36861;&#36394;&#34920;&#38754;&#27700;&#20307;&#20013;&#27700;&#25991;&#27745;&#26579;&#29289;&#30340;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
A Graph-Based Modeling Framework for Tracing Hydrological Pollutant Transport in Surface Waters. (arXiv:2302.04991v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HydroGraphs &#30340;&#22522;&#20110;&#22270;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#27700;&#25991;&#27745;&#26579;&#29289;&#30340;&#20256;&#36755;&#21644;&#21629;&#36816;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;&#24320;&#28304;&#25968;&#25454;&#26500;&#24314;&#65292;&#20855;&#26377;&#31616;&#21270;&#30340;&#27700;&#25991;&#31995;&#32479;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#22270;&#20998;&#26512;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#25216;&#26415;&#36827;&#34892;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#65292;&#33021;&#22815;&#24110;&#21161;&#20934;&#30830;&#23450;&#20301;&#27745;&#26579;&#28304;&#21644;&#33030;&#24369;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27745;&#26579;&#23545;&#20840;&#29699;&#21508;&#22320;&#30340;&#31038;&#21306;&#21644;&#29983;&#24577;&#31995;&#32479;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#25968;&#25454;&#20998;&#26512;&#21644;&#24314;&#27169;&#24037;&#20855;&#22312;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#20851;&#38190;&#28304;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#27700;&#25991;&#31995;&#32479;&#20013;&#36861;&#36394;&#20256;&#36755;&#21644;&#37327;&#21270;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;${\tt HydroGraphs}$&#8221;&#30340;&#22270;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#27700;&#20307;&#12289;&#27827;&#27969;&#21644;&#27969;&#22495;&#20013;&#27745;&#26579;&#29289;&#30340;&#20256;&#36755;&#21644;&#21629;&#36816;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31616;&#21270;&#30340;&#27700;&#25991;&#31995;&#32479;&#34920;&#31034;&#65292;&#21487;&#20197;&#22522;&#20110;&#24320;&#25918;&#28304;&#25968;&#25454;&#65288;&#22269;&#23478;&#27700;&#25991;&#25968;&#25454;&#38598;&#21644;&#27969;&#22495;&#36793;&#30028;&#25968;&#25454;&#38598;&#65289;&#26500;&#24314;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#22270;&#20998;&#26512;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#25216;&#26415;&#36827;&#34892;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#12290;&#36890;&#36807;&#23545;&#19978;&#23494;&#35199;&#35199;&#27604;&#27827;&#27969;&#22495;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#21457;&#29616;&#27745;&#26579;&#28304;&#12289;&#29702;&#35299;&#36816;&#36755;&#36884;&#24452;&#21644;&#20934;&#30830;&#23450;&#20301;&#33030;&#24369;&#21306;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anthropogenic pollution of hydrological systems affects diverse communities and ecosystems around the world. Data analytics and modeling tools play a key role in fighting this challenge, as they can help identify key sources as well as trace transport and quantify impact within complex hydrological systems. Several tools exist for simulating and tracing pollutant transport throughout surface waters using detailed physical models; these tools are powerful, but can be computationally intensive, require significant amounts of data to be developed, and require expert knowledge for their use (ultimately limiting application scope). In this work, we present a graph modeling framework -which we call ${\tt HydroGraphs}$ -- for understanding pollutant transport and fate across waterbodies, rivers, and watersheds. This framework uses a simplified representation of hydrological systems that can be constructed based purely on open-source data (National Hydrography Dataset and Watershed Boundary 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#35299;&#32544;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#37327;&#21270;&#21521;&#37327;&#35270;&#20026;&#22240;&#26524;&#21464;&#37327;&#65292;&#24182;&#22312;&#22240;&#26524;&#22270;&#20013;&#36827;&#34892;&#24178;&#39044;&#65292;&#29983;&#25104;&#24433;&#21709;&#22270;&#20687;&#20013;&#21807;&#19968;&#21464;&#24322;&#22240;&#32032;&#30340;&#21407;&#23376;&#36807;&#28193;&#12290;</title><link>http://arxiv.org/abs/2302.00869</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#36827;&#34892;&#28508;&#22312;&#34920;&#31034;&#30340;&#35299;&#32544;
&lt;/p&gt;
&lt;p&gt;
Disentanglement of Latent Representations via Causal Interventions. (arXiv:2302.00869v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#35299;&#32544;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#37327;&#21270;&#21521;&#37327;&#35270;&#20026;&#22240;&#26524;&#21464;&#37327;&#65292;&#24182;&#22312;&#22240;&#26524;&#22270;&#20013;&#36827;&#34892;&#24178;&#39044;&#65292;&#29983;&#25104;&#24433;&#21709;&#22270;&#20687;&#20013;&#21807;&#19968;&#21464;&#24322;&#22240;&#32032;&#30340;&#21407;&#23376;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22270;&#20687;&#31561;&#25968;&#25454;&#30340;&#36807;&#31243;&#30001;&#29420;&#31435;&#19988;&#26410;&#30693;&#30340;&#21464;&#24322;&#22240;&#32032;&#25511;&#21046;&#12290;&#22312;&#35299;&#32544;&#12289;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#39046;&#22495;&#20013;&#65292;&#20154;&#20204;&#24191;&#27867;&#30740;&#31350;&#20102;&#36825;&#20123;&#21464;&#37327;&#30340;&#26816;&#32034;&#12290;&#26368;&#36817;&#65292;&#23558;&#36825;&#20123;&#39046;&#22495;&#21512;&#24182;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#35299;&#32544;&#38382;&#39064;&#21487;&#20197;&#30475;&#20316;&#26159;&#25214;&#21040;&#33021;&#22815;&#20351;&#19968;&#24352;&#22270;&#20687;&#21457;&#29983;&#21333;&#19968;&#22240;&#32032;&#25913;&#21464;&#30340;&#24178;&#39044;&#12290;&#22522;&#20110;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#32544;&#26041;&#27861;&#65292;&#21463;&#22240;&#26524;&#21160;&#24577;&#30340;&#21551;&#21457;&#65292;&#23558;&#22240;&#26524;&#29702;&#35770;&#19982;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#37327;&#21270;&#21521;&#37327;&#35270;&#20026;&#22240;&#26524;&#21464;&#37327;&#65292;&#24182;&#22312;&#22240;&#26524;&#22270;&#20013;&#23558;&#23427;&#20204;&#30456;&#36830;&#12290;&#23427;&#22312;&#22270;&#20013;&#36827;&#34892;&#22240;&#26524;&#24178;&#39044;&#65292;&#29983;&#25104;&#24433;&#21709;&#22270;&#20687;&#20013;&#21807;&#19968;&#21464;&#24322;&#22240;&#32032;&#30340;&#21407;&#23376;&#36807;&#28193;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#21160;&#20316;&#26816;&#32034;&#20219;&#21153;&#65292;&#28041;&#21450;...
&lt;/p&gt;
&lt;p&gt;
The process of generating data such as images is controlled by independent and unknown factors of variation. The retrieval of these variables has been studied extensively in the disentanglement, causal representation learning, and independent component analysis fields. Recently, approaches merging these domains together have shown great success. Instead of directly representing the factors of variation, the problem of disentanglement can be seen as finding the interventions on one image that yield a change to a single factor. Following this assumption, we introduce a new method for disentanglement inspired by causal dynamics that combines causality theory with vector-quantized variational autoencoders. Our model considers the quantized vectors as causal variables and links them in a causal graph. It performs causal interventions on the graph and generates atomic transitions affecting a unique factor of variation in the image. We also introduce a new task of action retrieval that consis
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#22635;&#34917;&#20102;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#22686;&#24378;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#30340;&#19968;&#20010;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;Off-the-Grid MARL&#65288;OG-MARL&#65289;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#24110;&#21161;&#31038;&#21306;&#34913;&#37327;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.00521</link><description>&lt;p&gt;
Off-the-Grid MARL: &#24102;&#26377;&#22522;&#20934;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#22686;&#24378;&#23398;&#20064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Off-the-Grid MARL: Datasets with Baselines for Offline Multi-Agent Reinforcement Learning. (arXiv:2302.00521v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00521
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#22635;&#34917;&#20102;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#22686;&#24378;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#30340;&#19968;&#20010;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;Off-the-Grid MARL&#65288;OG-MARL&#65289;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#24110;&#21161;&#31038;&#21306;&#34913;&#37327;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#21033;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#24320;&#21457;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#22120;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#24320;&#21551;&#20102;&#24040;&#22823;&#30340;&#20215;&#20540;&#12290;&#35768;&#22810;&#37325;&#35201;&#30340;&#24037;&#19994;&#31995;&#32479;&#26159;&#22810;&#26234;&#33021;&#20307;&#30340;&#65292;&#24182;&#19988;&#24456;&#38590;&#20351;&#29992;&#23450;&#21046;&#30340;&#27169;&#25311;&#22120;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22312;&#24037;&#19994;&#20013;&#65292;&#20998;&#24067;&#24335;&#36827;&#31243;&#32463;&#24120;&#21487;&#20197;&#22312;&#36816;&#34892;&#26399;&#38388;&#35760;&#24405;&#65292;&#24182;&#23384;&#20648;&#22823;&#37327;&#30340;&#28436;&#31034;&#25968;&#25454;&#12290;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#22686;&#24378;&#23398;&#20064;&#65288;MARL&#65289;&#20026;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#24314;&#31435;&#26377;&#25928;&#30340;&#20998;&#25955;&#24335;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#31163;&#32447;MARL&#20173;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#65292;&#22240;&#27492;&#32570;&#20047;&#22312;&#24378;&#21270;&#23398;&#20064;&#26356;&#25104;&#29087;&#30340;&#23376;&#39046;&#22495;&#20013;&#36890;&#24120;&#20250;&#25214;&#21040;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#12290;&#36825;&#20123;&#19981;&#36275;&#20351;&#24471;&#31038;&#21306;&#26080;&#27861;&#21512;&#29702;&#22320;&#34913;&#37327;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21457;&#24067;Off-the-Grid MARL&#65288;OG-MARL&#65289;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65306;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#23384;&#20648;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#21327;&#20316;&#31163;&#32447;MARL&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to harness the power of large datasets for developing cooperative multi-agent controllers promises to unlock enormous value for real-world applications. Many important industrial systems are multi-agent in nature and are difficult to model using bespoke simulators. However, in industry, distributed processes can often be recorded during operation, and large quantities of demonstrative data stored. Offline multi-agent reinforcement learning (MARL) provides a promising paradigm for building effective decentralised controllers from such datasets. However, offline MARL is still in its infancy and therefore lacks standardised benchmark datasets and baselines typically found in more mature subfields of reinforcement learning (RL). These deficiencies make it difficult for the community to sensibly measure progress. In this work, we aim to fill this gap by releasing off-the-grid MARL (OG-MARL): a growing repository of high-quality datasets with baselines for cooperative offline MARL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23618;&#36229;&#22270;&#20013;&#30340;&#25289;&#26222;&#25289;&#26031;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;&#36866;&#24403;&#36873;&#25321;&#35268;&#21017;&#30340;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.12184</link><description>&lt;p&gt;
&#22522;&#20110;&#22352;&#26631;&#19979;&#38477;&#30340;&#22810;&#23618;&#36229;&#22270;&#20013;&#30340;&#25289;&#26222;&#25289;&#26031;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Laplacian-based Semi-Supervised Learning in Multilayer Hypergraphs by Coordinate Descent. (arXiv:2301.12184v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23618;&#36229;&#22270;&#20013;&#30340;&#25289;&#26222;&#25289;&#26031;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;&#36866;&#24403;&#36873;&#25321;&#35268;&#21017;&#30340;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21322;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#32473;&#23450;&#19968;&#20010;&#22270;&#21644;&#19968;&#32452;&#26631;&#26377;&#26631;&#31614;&#30340;&#33410;&#28857;&#65292;&#30446;&#26631;&#26159;&#20026;&#21097;&#20313;&#30340;&#26410;&#26631;&#35760;&#33410;&#28857;&#25512;&#26029;&#26631;&#31614;&#12290;&#26412;&#25991;&#39318;&#20808;&#32771;&#34385;&#20102;&#26080;&#21521;&#22270;&#38382;&#39064;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#34920;&#36848;&#65292;&#28982;&#21518;&#23558;&#36825;&#20010;&#34920;&#36848;&#25193;&#23637;&#21040;&#22810;&#23618;&#36229;&#22270;&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#32463;&#20856;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;&#20855;&#26377;&#36866;&#24403;&#36873;&#25321;&#35268;&#21017;&#30340;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Semi-Supervised learning is an important data analysis tool, where given a graph and a set of labeled nodes, the aim is to infer the labels to the remaining unlabeled nodes. In this paper, we start by considering an optimization-based formulation of the problem for an undirected graph, and then we extend this formulation to multilayer hypergraphs. We solve the problem using different coordinate descent approaches and compare the results with the ones obtained by the classic gradient descent method. Experiments on synthetic and real-world datasets show the potential of using coordinate descent methods with suitable selection rules.
&lt;/p&gt;</description></item><item><title>Salesforce CausalAI&#24211;&#26159;&#19968;&#20010;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#21644;&#34920;&#26684;&#25968;&#25454;&#30340;&#22240;&#26524;&#20998;&#26512;&#12290;&#23427;&#25903;&#25345;&#31163;&#25955;&#12289;&#36830;&#32493;&#21644;&#24322;&#36136;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#30340;&#31639;&#27861;&#65292;&#24182;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#25351;&#23450;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#30028;&#38754;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#65292;&#26080;&#38656;&#32534;&#31243;&#12290;</title><link>http://arxiv.org/abs/2301.10859</link><description>&lt;p&gt;
Salesforce CausalAI&#24211;: &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#34920;&#26684;&#25968;&#25454;&#22240;&#26524;&#20998;&#26512;&#30340;&#24555;&#36895;&#21487;&#25193;&#23637;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Salesforce CausalAI Library: A Fast and Scalable Framework for Causal Analysis of Time Series and Tabular Data. (arXiv:2301.10859v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10859
&lt;/p&gt;
&lt;p&gt;
Salesforce CausalAI&#24211;&#26159;&#19968;&#20010;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#21644;&#34920;&#26684;&#25968;&#25454;&#30340;&#22240;&#26524;&#20998;&#26512;&#12290;&#23427;&#25903;&#25345;&#31163;&#25955;&#12289;&#36830;&#32493;&#21644;&#24322;&#36136;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#30340;&#31639;&#27861;&#65292;&#24182;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#25351;&#23450;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#30028;&#38754;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#65292;&#26080;&#38656;&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Salesforce CausalAI&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#30340;&#24320;&#28304;&#24211;&#12290;&#23427;&#25903;&#25345;&#31163;&#25955;&#12289;&#36830;&#32493;&#21644;&#24322;&#36136;&#31867;&#22411;&#30340;&#34920;&#26684;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#22240;&#26524;&#25512;&#26029;&#12290;&#35813;&#24211;&#21253;&#25324;&#22788;&#29702;&#21464;&#37327;&#20043;&#38388;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#30340;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#22788;&#29702;&#36827;&#34892;&#21152;&#36895;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25351;&#23450;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#22312;&#30740;&#31350;&#21508;&#31181;&#31639;&#27861;&#30340;&#21516;&#26102;&#25511;&#21046;&#22522;&#30784;&#22240;&#26524;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#22312;&#26080;&#38656;&#32534;&#31243;&#30340;&#24773;&#20917;&#19979;&#23545;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#12290;&#35813;&#24211;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#24555;&#36895;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#24615;&#39046;&#22495;&#20013;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#25551;&#36848;&#20102;Salesforce CausalAI API&#21450;&#20854;&#21151;&#33021;&#20197;&#21450;s&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Salesforce CausalAI Library, an open-source library for causal analysis using observational data. It supports causal discovery and causal inference for tabular and time series data, of discrete, continuous and heterogeneous types. This library includes algorithms that handle linear and non-linear causal relationships between variables, and uses multi-processing for speed-up. We also include a data generator capable of generating synthetic data with specified structural equation model for the aforementioned data formats and types, that helps users control the ground-truth causal process while investigating various algorithms. Finally, we provide a user interface (UI) that allows users to perform causal analysis on data without coding. The goal of this library is to provide a fast and flexible solution for a variety of problems in the domain of causality. This technical report describes the Salesforce CausalAI API along with its capabilities, the implementations of the s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20102;&#19968;&#20010;&#20195;&#29702;&#65292;&#23558;&#39640;&#39057;&#20132;&#26131;&#20449;&#21495;&#36716;&#21270;&#20026;&#20132;&#26131;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#24322;&#27493;&#21452;&#23545;&#20915;Q&#23398;&#20064;&#36827;&#34892;&#20132;&#26131;&#20449;&#21495;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#29420;&#31435;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#20132;&#26131;&#30340;&#24615;&#33021;&#20197;&#21450;&#19982;&#20855;&#20307;&#30340;&#39044;&#27979;&#31639;&#27861;&#26080;&#20851;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2301.08688</link><description>&lt;p&gt;
&#24322;&#27493;&#28145;&#24230;&#21452;&#23545;&#20915;Q&#23398;&#20064;&#22312;&#38480;&#20215;&#20132;&#26131;&#24066;&#22330;&#20013;&#30340;&#20132;&#26131;&#20449;&#21495;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Deep Double Duelling Q-Learning for Trading-Signal Execution in Limit Order Book Markets. (arXiv:2301.08688v2 [q-fin.TR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08688
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20102;&#19968;&#20010;&#20195;&#29702;&#65292;&#23558;&#39640;&#39057;&#20132;&#26131;&#20449;&#21495;&#36716;&#21270;&#20026;&#20132;&#26131;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#24322;&#27493;&#21452;&#23545;&#20915;Q&#23398;&#20064;&#36827;&#34892;&#20132;&#26131;&#20449;&#21495;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#29420;&#31435;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#20132;&#26131;&#30340;&#24615;&#33021;&#20197;&#21450;&#19982;&#20855;&#20307;&#30340;&#39044;&#27979;&#31639;&#27861;&#26080;&#20851;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#65292;&#23558;&#39640;&#39057;&#20132;&#26131;&#20449;&#21495;&#25104;&#21151;&#36716;&#21270;&#20026;&#33021;&#22815;&#19979;&#36798;&#29420;&#31435;&#38480;&#20215;&#35746;&#21333;&#30340;&#20132;&#26131;&#31574;&#30053;&#12290;&#22522;&#20110;ABIDES&#38480;&#20215;&#35746;&#21333;&#31807;&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;OpenAI gym&#29615;&#22659;&#65292;&#24182;&#21033;&#29992;&#23427;&#22312;&#22522;&#20110;&#21382;&#21490;&#35746;&#21333;&#31807;&#28040;&#24687;&#30340;NASDAQ&#32929;&#31080;&#20132;&#26131;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27169;&#25311;&#12290;&#20026;&#20102;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#22312;&#27492;&#29615;&#22659;&#20013;&#26368;&#22823;&#21270;&#20132;&#26131;&#22238;&#25253;&#30340;&#20132;&#26131;&#20195;&#29702;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;Deep Duelling Double Q-learning&#19982;APEX&#65288;&#24322;&#27493;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#65289;&#26550;&#26500;&#12290;&#20195;&#29702;&#35266;&#23519;&#24403;&#21069;&#38480;&#20215;&#35746;&#21333;&#31807;&#29366;&#24577;&#12289;&#20854;&#26368;&#36817;&#21382;&#21490;&#21644;&#30701;&#26399;&#26041;&#21521;&#39044;&#27979;&#12290;&#20026;&#20102;&#29420;&#31435;&#22320;&#30740;&#31350;&#36866;&#24212;&#24615;&#20132;&#26131;&#30340;RL&#24615;&#33021;&#32780;&#19981;&#28041;&#21450;&#20855;&#20307;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#36890;&#36807;&#25200;&#21160;&#21069;&#30651;&#25910;&#30410;&#33719;&#24471;&#30340;&#21512;&#25104;alpha&#20449;&#21495;&#26469;&#30740;&#31350;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#20449;&#21495;&#20855;&#26377;&#19981;&#21516;&#32423;&#21035;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
We employ deep reinforcement learning (RL) to train an agent to successfully translate a high-frequency trading signal into a trading strategy that places individual limit orders. Based on the ABIDES limit order book simulator, we build a reinforcement learning OpenAI gym environment and utilise it to simulate a realistic trading environment for NASDAQ equities based on historic order book messages. To train a trading agent that learns to maximise its trading return in this environment, we use Deep Duelling Double Q-learning with the APEX (asynchronous prioritised experience replay) architecture. The agent observes the current limit order book state, its recent history, and a short-term directional forecast. To investigate the performance of RL for adaptive trading independently from a concrete forecasting algorithm, we study the performance of our approach utilising synthetic alpha signals obtained by perturbing forward-looking returns with varying levels of noise. Here, we find that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#12289;&#35780;&#20272;&#21644;&#32534;&#30721;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#29992;&#30340;&#26041;&#24335;&#22238;&#39038;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#24182;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25968;&#25454;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#20351;&#29992;CNN&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2301.03403</link><description>&lt;p&gt;
&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;&#30340;&#32508;&#21512;&#22238;&#39038;&#65306;&#26041;&#27861;&#12289;&#25968;&#25454;&#12289;&#35780;&#20272;&#21644;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
A comprehensive review of automatic text summarization techniques: method, data, evaluation and coding. (arXiv:2301.03403v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#12289;&#35780;&#20272;&#21644;&#32534;&#30721;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#29992;&#30340;&#26041;&#24335;&#22238;&#39038;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#24182;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25968;&#25454;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#20351;&#29992;CNN&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#24341;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#25105;&#20204;&#24050;&#32463;&#25484;&#25569;&#30340;&#20851;&#20110;&#27599;&#20010;&#25105;&#20204;&#24819;&#28085;&#30422;&#30340;&#20027;&#39064;&#30340;&#19968;&#20123;&#27969;&#34892;&#21644;&#33879;&#21517;&#30340;&#35770;&#25991;&#24320;&#22987;&#65292;&#28982;&#21518;&#25105;&#20204;&#36861;&#36394;&#20102;&#8220;&#21521;&#21518;&#24341;&#29992;&#8221;&#65288;&#34987;&#25105;&#20204;&#20043;&#21069;&#30693;&#36947;&#30340;&#19968;&#31995;&#21015;&#35770;&#25991;&#24341;&#29992;&#30340;&#35770;&#25991;&#65289;&#21644;&#8220;&#21521;&#21069;&#24341;&#29992;&#8221;&#65288;&#24341;&#29992;&#25105;&#20204;&#20043;&#21069;&#30693;&#36947;&#30340;&#19968;&#31995;&#21015;&#35770;&#25991;&#30340;&#36739;&#26032;&#35770;&#25991;&#65289;&#12290;&#20026;&#20102;&#32452;&#32455;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21508;&#31181;&#22522;&#20110;&#19981;&#21516;&#26426;&#21046;&#29983;&#25104;&#25688;&#35201;&#30340;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#12290;&#38500;&#20102;&#20171;&#32461;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#21487;&#29992;&#20110;&#25688;&#35201;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#21644;&#29992;&#20110;&#35780;&#20272;&#25688;&#35201;&#36136;&#37327;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22238;&#39038;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;CNN&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#25506;&#32034;&#65292;&#35813;&#25968;&#25454;&#38598;&#20026;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#25552;&#20379;&#20102;&#37329;&#26631;&#20934;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a literature review about Automatic Text Summarization (ATS) systems. We consider a citation-based approach. We start with some popular and well-known papers that we have in hand about each topic we want to cover and we have tracked the "backward citations" (papers that are cited by the set of papers we knew beforehand) and the "forward citations" (newer papers that cite the set of papers we knew beforehand). In order to organize the different methods, we present the diverse approaches to ATS guided by the mechanisms they use to generate a summary. Besides presenting the methods, we also present an extensive review of the datasets available for summarization tasks and the methods used to evaluate the quality of the summaries. Finally, we present an empirical exploration of these methods using the CNN Corpus dataset that provides golden summaries for extractive and abstractive methods.
&lt;/p&gt;</description></item><item><title>StitchNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#26041;&#24335;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29255;&#27573;&#26469;&#21019;&#24314;&#39640;&#24615;&#33021;&#30340;&#32593;&#32476;&#65292;&#26080;&#38656;&#20256;&#32479;&#35757;&#32451;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#35201;&#27714;&#12290;&#36890;&#36807;&#23621;&#20013;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#25351;&#23548;&#29255;&#27573;&#30340;&#36873;&#25321;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#20934;&#30830;&#24615;&#38656;&#27714;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;StitchNet&#36824;&#21487;&#20197;&#23454;&#29616;&#21363;&#26102;&#20010;&#24615;&#21270;&#27169;&#22411;&#21019;&#24314;&#21644;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2301.01947</link><description>&lt;p&gt;
StitchNet: &#20174;&#39044;&#35757;&#32451;&#29255;&#27573;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
StitchNet: Composing Neural Networks from Pre-Trained Fragments. (arXiv:2301.01947v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01947
&lt;/p&gt;
&lt;p&gt;
StitchNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#26041;&#24335;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29255;&#27573;&#26469;&#21019;&#24314;&#39640;&#24615;&#33021;&#30340;&#32593;&#32476;&#65292;&#26080;&#38656;&#20256;&#32479;&#35757;&#32451;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#35201;&#27714;&#12290;&#36890;&#36807;&#23621;&#20013;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#25351;&#23548;&#29255;&#27573;&#30340;&#36873;&#25321;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#20934;&#30830;&#24615;&#38656;&#27714;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;StitchNet&#36824;&#21487;&#20197;&#23454;&#29616;&#21363;&#26102;&#20010;&#24615;&#21270;&#27169;&#22411;&#21019;&#24314;&#21644;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#33539;&#24335;StitchNet&#65292;&#23427;&#23558;&#26469;&#33258;&#22810;&#20010;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29255;&#27573;&#65288;&#19968;&#20010;&#25110;&#22810;&#20010;&#36830;&#32493;&#30340;&#32593;&#32476;&#23618;&#65289;&#25340;&#25509;&#22312;&#19968;&#36215;&#12290;StitchNet&#20801;&#35768;&#21019;&#24314;&#39640;&#24615;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#26080;&#38656;&#20256;&#32479;&#30340;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#22823;&#37327;&#35745;&#31639;&#21644;&#25968;&#25454;&#35201;&#27714;&#12290;&#25105;&#20204;&#21033;&#29992;&#23621;&#20013;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#20316;&#20026;&#19968;&#31181;&#20860;&#23481;&#24615;&#24230;&#37327;&#65292;&#20197;&#26377;&#25928;&#22320;&#25351;&#23548;&#36873;&#25321;&#36825;&#20123;&#29255;&#27573;&#65292;&#20197;&#32452;&#21512;&#36866;&#21512;&#29305;&#23450;&#20934;&#30830;&#24615;&#38656;&#27714;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#30340;&#20219;&#21153;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#29255;&#27573;&#21487;&#20197;&#34987;&#25340;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#22312;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#35201;&#27714;&#30340;&#19968;&#23567;&#37096;&#20998;&#19979;&#21019;&#24314;&#19982;&#20256;&#32479;&#35757;&#32451;&#32593;&#32476;&#30456;&#23218;&#32654;&#20934;&#30830;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#31181;&#26032;&#33539;&#24335;&#25152;&#33021;&#23454;&#29616;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#21363;&#26102;&#20010;&#24615;&#21270;&#27169;&#22411;&#21019;&#24314;&#21644;&#25512;&#26029;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose StitchNet, a novel neural network creation paradigm that stitches together fragments (one or more consecutive network layers) from multiple pre-trained neural networks. StitchNet allows the creation of high-performing neural networks without the large compute and data requirements needed under traditional model creation processes via backpropagation training. We leverage Centered Kernel Alignment (CKA) as a compatibility measure to efficiently guide the selection of these fragments in composing a network for a given task tailored to specific accuracy needs and computing resource constraints. We then show that these fragments can be stitched together to create neural networks with comparable accuracy to traditionally trained networks at a fraction of computing resource and data requirements. Finally, we explore a novel on-the-fly personalized model creation and inference application enabled by this new paradigm.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30446;&#26631;&#24341;&#23548;&#21464;&#21387;&#22120;&#22686;&#24378;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#20449;&#24687;&#19982;&#22330;&#26223;&#34920;&#31034;&#32806;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#33258;&#20027;&#23548;&#33322;&#12290;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#20808;&#39564;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.00362</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#25928;&#33258;&#20027;&#23548;&#33322;&#30340;&#30446;&#26631;&#24341;&#23548;&#21464;&#21387;&#22120;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Goal-Guided Transformer-Enabled Reinforcement Learning for Efficient Autonomous Navigation. (arXiv:2301.00362v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30446;&#26631;&#24341;&#23548;&#21464;&#21387;&#22120;&#22686;&#24378;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#20449;&#24687;&#19982;&#22330;&#26223;&#34920;&#31034;&#32806;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#33258;&#20027;&#23548;&#33322;&#12290;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#20808;&#39564;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#30446;&#26631;&#39537;&#21160;&#23548;&#33322;&#26377;&#20123;&#25104;&#21151;&#24212;&#29992;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#30446;&#26631;&#20449;&#24687;&#19982;&#24863;&#30693;&#27169;&#22359;&#35299;&#32806;&#65292;&#24182;&#30452;&#25509;&#20316;&#20026;&#20915;&#31574;&#30340;&#26465;&#20214;&#24341;&#20837;&#65292;&#23548;&#33268;&#22330;&#26223;&#34920;&#31034;&#20013;&#19982;&#30446;&#26631;&#26080;&#20851;&#30340;&#29305;&#24449;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36215;&#21040;&#23545;&#25239;&#20316;&#29992;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30446;&#26631;&#24341;&#23548;&#21464;&#21387;&#22120;&#22686;&#24378;&#23398;&#20064;(GTRL)&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#30446;&#26631;&#29366;&#24577;&#20316;&#20026;&#22330;&#26223;&#32534;&#30721;&#22120;&#30340;&#36755;&#20837;&#26469;&#25351;&#23548;&#22330;&#26223;&#34920;&#31034;&#19982;&#30446;&#26631;&#20449;&#24687;&#30340;&#32806;&#21512;&#65292;&#24182;&#23454;&#29616;&#39640;&#25928;&#30340;&#33258;&#20027;&#23548;&#33322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#21464;&#21387;&#22120;&#21464;&#20307;&#20316;&#20026;&#24863;&#30693;&#31995;&#32479;&#30340;&#39592;&#24178;&#65292;&#21363;&#30446;&#26631;&#24341;&#23548;&#21464;&#21387;&#22120;(GoT)&#65292;&#24182;&#20351;&#29992;&#19987;&#23478;&#20808;&#39564;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#23548;&#33322;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite some successful applications of goal-driven navigation, existing deep reinforcement learning (DRL)-based approaches notoriously suffers from poor data efficiency issue. One of the reasons is that the goal information is decoupled from the perception module and directly introduced as a condition of decision-making, resulting in the goal-irrelevant features of the scene representation playing an adversary role during the learning process. In light of this, we present a novel Goal-guided Transformer-enabled reinforcement learning (GTRL) approach by considering the physical goal states as an input of the scene encoder for guiding the scene representation to couple with the goal information and realizing efficient autonomous navigation. More specifically, we propose a novel variant of the Vision Transformer as the backbone of the perception system, namely Goal-guided Transformer (GoT), and pre-train it with expert priors to boost the data efficiency. Subsequently, a reinforcement le
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;"MolCPT"&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20219;&#21153;&#30456;&#20851;&#30340;&#27169;&#24335;&#23376;&#22270;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#23545;&#24191;&#27867;&#30340;&#20998;&#23376;&#31354;&#38388;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.10614</link><description>&lt;p&gt;
MolCPT&#65306;&#20998;&#23376;&#36830;&#32493;&#25552;&#31034;&#35843;&#25972;&#20197;&#25512;&#24191;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MolCPT: Molecule Continuous Prompt Tuning to Generalize Molecular Representation Learning. (arXiv:2212.10614v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10614
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;"MolCPT"&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20219;&#21153;&#30456;&#20851;&#30340;&#27169;&#24335;&#23376;&#22270;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#23545;&#24191;&#27867;&#30340;&#20998;&#23376;&#31354;&#38388;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#23545;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30001;&#20110;&#20854;&#32467;&#26500;&#24314;&#27169;&#33021;&#21147;&#25104;&#20026;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30001;&#20110;&#26631;&#35760;&#25968;&#25454;&#36890;&#24120;&#31232;&#32570;&#19988;&#26114;&#36149;&#65292;&#23545;GNNs&#22312;&#24191;&#27867;&#30340;&#20998;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#25512;&#24191;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#8220;&#39044;&#35757;&#32451;&#65292;&#24494;&#35843;&#8221;&#35757;&#32451;&#33539;&#24335;&#34987;&#29992;&#26469;&#25913;&#21892;GNNs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#20351;&#29992;&#33258;&#30417;&#30563;&#20449;&#24687;&#26469;&#39044;&#35757;&#32451;GNN&#65292;&#28982;&#21518;&#36890;&#36807;&#24494;&#35843;&#26469;&#20248;&#21270;&#19979;&#28216;&#20219;&#21153;&#65292;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#24182;&#19981;&#24635;&#26159;&#33021;&#22815;&#20135;&#29983;&#32479;&#35745;&#19978;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20855;&#26377;&#38543;&#26426;&#32467;&#26500;&#25513;&#30721;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#23454;&#38469;&#19978;&#65292;&#20998;&#23376;&#32467;&#26500;&#20855;&#26377;&#39057;&#32321;&#20986;&#29616;&#19988;&#24433;&#21709;&#20998;&#23376;&#23646;&#24615;&#30340;&#27169;&#24335;&#23376;&#22270;&#12290;&#20026;&#20102;&#21033;&#29992;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#27169;&#24335;&#23376;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#65292;&#24494;&#35843;&#8221;&#30340;&#35757;&#32451;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular representation learning is crucial for the problem of molecular property prediction, where graph neural networks (GNNs) serve as an effective solution due to their structure modeling capabilities. Since labeled data is often scarce and expensive to obtain, it is a great challenge for GNNs to generalize in the extensive molecular space. Recently, the training paradigm of "pre-train, fine-tune" has been leveraged to improve the generalization capabilities of GNNs. It uses self-supervised information to pre-train the GNN, and then performs fine-tuning to optimize the downstream task with just a few labels. However, pre-training does not always yield statistically significant improvement, especially for self-supervised learning with random structural masking. In fact, the molecular structure is characterized by motif subgraphs, which are frequently occurring and influence molecular properties. To leverage the task-related motifs, we propose a novel paradigm of "pre-train, prompt,
&lt;/p&gt;</description></item><item><title>MPGraph&#26159;&#19968;&#31181;&#38024;&#23545;&#22270;&#20998;&#26512;&#21152;&#36895;&#30340;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#23398;&#20064;&#39044;&#21462;&#22120;&#65292;&#24341;&#20837;&#20102;&#30456;&#20301;&#36716;&#21464;&#30340;&#36719;&#26816;&#27979;&#12289;&#30456;&#20301;&#29305;&#23450;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#38142;&#24335;&#26102;&#31354;&#39044;&#21462;&#31561;&#20248;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;CSTP&#23454;&#29616;&#20102;12.52-21.23%&#30340;IPC&#25913;&#36827;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;&#39044;&#21462;&#22120;&#12290;</title><link>http://arxiv.org/abs/2212.05250</link><description>&lt;p&gt;
&#30456;&#20301;&#12289;&#27169;&#24577;&#12289;&#26102;&#38388;&#21644;&#31354;&#38388;&#23616;&#37096;&#24615;: &#21152;&#36895;&#22270;&#20998;&#26512;&#30340;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#23398;&#20064;&#39044;&#21462;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phases, Modalities, Temporal and Spatial Locality: Domain Specific ML Prefetcher for Accelerating Graph Analytics. (arXiv:2212.05250v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05250
&lt;/p&gt;
&lt;p&gt;
MPGraph&#26159;&#19968;&#31181;&#38024;&#23545;&#22270;&#20998;&#26512;&#21152;&#36895;&#30340;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#23398;&#20064;&#39044;&#21462;&#22120;&#65292;&#24341;&#20837;&#20102;&#30456;&#20301;&#36716;&#21464;&#30340;&#36719;&#26816;&#27979;&#12289;&#30456;&#20301;&#29305;&#23450;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#38142;&#24335;&#26102;&#31354;&#39044;&#21462;&#31561;&#20248;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;CSTP&#23454;&#29616;&#20102;12.52-21.23%&#30340;IPC&#25913;&#36827;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;&#39044;&#21462;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23384;&#24615;&#33021;&#26159;&#22270;&#20998;&#26512;&#21152;&#36895;&#20013;&#30340;&#29942;&#39048;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39044;&#21462;&#22120;&#22312;&#22270;&#22788;&#29702;&#20013;&#30340;&#30456;&#20301;&#36716;&#21464;&#21644;&#19981;&#35268;&#21017;&#20869;&#23384;&#35775;&#38382;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MPGraph&#65292;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#30340;&#22270;&#20998;&#26512;ML&#39044;&#21462;&#22120;&#12290;MPGraph&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#30340;&#20248;&#21270;&#31574;&#30053;&#65306;&#29992;&#20110;&#30456;&#20301;&#36716;&#21464;&#30340;&#36719;&#26816;&#27979;&#65292;&#29992;&#20110;&#35775;&#38382;&#22686;&#37327;&#21644;&#39029;&#38754;&#39044;&#27979;&#30340;&#30456;&#20301;&#29305;&#23450;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20197;&#21450;&#29992;&#20110;&#39044;&#21462;&#25511;&#21046;&#30340;&#38142;&#24335;&#26102;&#31354;&#39044;&#21462;&#65288;CSTP&#65289;&#12290;&#25105;&#20204;&#30340;&#36716;&#25442;&#26816;&#27979;&#22120;&#19982;Kolmogorov-Smirnov&#31383;&#21475;&#21644;&#20915;&#31574;&#26641;&#30456;&#27604;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;34.17-82.15%&#12290;&#25105;&#20204;&#30340;&#39044;&#27979;&#22120;&#22312;&#22686;&#37327;&#26041;&#38754;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#20102;6.80-16.02%&#65292;&#23545;&#20110;&#39029;&#38754;&#39044;&#27979;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.68-15.41%&#65292;&#19982;LSTM&#21644;vanilla attention&#27169;&#22411;&#30456;&#27604;&#12290;&#20351;&#29992;CSTP&#65292;MPGraph&#30340;IPC&#25552;&#39640;&#20102;12.52-21.23%&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38750;ML&#39044;&#21462;&#22120;BO 7.58-12.03%&#12289;&#20197;&#21450;&#22522;&#20110;ML&#30340;&#39044;&#21462;&#22120;Voyager&#21644;TransFetch 3.27-4.58%&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory performance is a bottleneck in graph analytics acceleration. Existing Machine Learning (ML) prefetchers struggle with phase transitions and irregular memory accesses in graph processing. We propose MPGraph, an ML-based Prefetcher for Graph analytics using domain specific models. MPGraph introduces three novel optimizations: soft detection for phase transitions, phase-specific multi-modality models for access delta and page predictions, and chain spatio-temporal prefetching (CSTP) for prefetch control. Our transition detector achieves 34.17-82.15% higher precision compared with Kolmogorov-Smirnov Windowing and decision tree. Our predictors achieve 6.80-16.02% higher F1-score for delta and 11.68-15.41% higher accuracy-at-10 for page prediction compared with LSTM and vanilla attention models. Using CSTP, MPGraph achieves 12.52-21.23% IPC improvement, outperforming state-of-the-art non-ML prefetcher BO by 7.58-12.03% and ML-based prefetchers Voyager and TransFetch by 3.27-4.58%. For
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#20302;&#39640;&#32500;&#22797;&#26434;&#38382;&#39064;&#27867;&#21270;&#35823;&#24046;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.01518</link><description>&lt;p&gt;
&#36890;&#36807;&#21442;&#25968;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26694;&#26550;&#38477;&#20302;&#27867;&#21270;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Hedging Complexity in Generalization via a Parametric Distributionally Robust Optimization Framework. (arXiv:2212.01518v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01518
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#20302;&#39640;&#32500;&#22797;&#26434;&#38382;&#39064;&#27867;&#21270;&#35823;&#24046;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;(DRO)&#26159;&#35299;&#20915;&#36816;&#33829;&#31649;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#36825;&#20123;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#35201;&#20040;&#20381;&#36182;&#20110;&#25104;&#26412;&#20989;&#25968;&#30340;&#22797;&#26434;&#24230;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#38543;&#26426;&#25200;&#21160;&#30340;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#22312;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#20989;&#25968;&#30340;&#39640;&#32500;&#38382;&#39064;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#33021;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#20998;&#24067;&#26063;&#26469;&#36924;&#36817;&#38543;&#26426;&#25200;&#21160;&#30340;&#20998;&#24067;&#12290;&#36825;&#20943;&#36731;&#20102;&#20004;&#31181;&#22797;&#26434;&#24615;&#26469;&#28304;&#65307;&#28982;&#32780;&#65292;&#23427;&#24341;&#20837;&#20102;&#27169;&#22411;&#26410;&#27491;&#30830;&#24314;&#27169;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#30340;&#35823;&#24046;&#26469;&#28304;&#21487;&#20197;&#36890;&#36807;&#21512;&#36866;&#30340;DRO&#20844;&#24335;&#26469;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21442;&#25968;DRO&#26041;&#27861;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#23545;&#29616;&#26377;&#30340;ERM&#21644;DRO&#26041;&#27861;&#20197;&#21450;&#21442;&#25968;ERM&#30340;&#27867;&#21270;&#30028;&#38480;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical risk minimization (ERM) and distributionally robust optimization (DRO) are popular approaches for solving stochastic optimization problems that appear in operations management and machine learning. Existing generalization error bounds for these methods depend on either the complexity of the cost function or dimension of the random perturbations. Consequently, the performance of these methods can be poor for high-dimensional problems with complex objective functions. We propose a simple approach in which the distribution of random perturbations is approximated using a parametric family of distributions. This mitigates both sources of complexity; however, it introduces a model misspecification error. We show that this new source of error can be controlled by suitable DRO formulations. Our proposed parametric DRO approach has significantly improved generalization bounds over existing ERM and DRO methods and parametric ERM for a wide variety of settings. Our method is particularl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27604;&#20363;&#24133;&#24230;&#35889;&#35757;&#32451;&#22686;&#24378;&#30340;&#26041;&#27861; PASTA&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010; Syn-to-Real &#20219;&#21153;&#19978;&#22343;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.00979</link><description>&lt;p&gt;
PASTA&#65306;&#27604;&#20363;&#24133;&#24230;&#35889;&#35757;&#32451;&#22686;&#24378;&#29992;&#20110; Syn-to-Real &#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
PASTA: Proportional Amplitude Spectrum Training Augmentation for Syn-to-Real Domain Generalization. (arXiv:2212.00979v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27604;&#20363;&#24133;&#24230;&#35889;&#35757;&#32451;&#22686;&#24378;&#30340;&#26041;&#27861; PASTA&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010; Syn-to-Real &#20219;&#21153;&#19978;&#22343;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#25552;&#20379;&#24265;&#20215;&#19988;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#35780;&#20272;&#30340;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#34920;&#29616;&#26174;&#33879;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Proportional Amplitude Spectrum Training Augmentation (PASTA)&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#21487;&#25552;&#39640;&#21512;&#25104;&#21040;&#30495;&#23454;&#65288;Syn-to-Real&#65289;&#27867;&#21270;&#24615;&#33021;&#12290; PASTA &#22312; Fourier &#39046;&#22495;&#20013;&#25200;&#21160;&#21512;&#25104;&#22270;&#20687;&#30340;&#24133;&#24230;&#35889;&#20197;&#29983;&#25104;&#22686;&#24378;&#35270;&#22270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992; PASTA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#25200;&#21160;&#31574;&#30053;&#65292;&#20854;&#20013;&#39640;&#39057;&#20998;&#37327;&#30456;&#23545;&#20110;&#20302;&#39057;&#20998;&#37327;&#26356;&#23481;&#26131;&#21463;&#21040;&#25200;&#21160;&#12290;&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#65288;GTAV-to-Real&#65289;&#65292;&#30446;&#26631;&#26816;&#27979;&#65288;Sim10K-to-Real&#65289;&#21644;&#23545;&#35937;&#35782;&#21035;&#65288;VisDA-C Syn-to-Real&#65289;&#20219;&#21153;&#65292;&#22312;&#24635;&#20849;5&#20010; Syn-to-Real &#36716;&#31227;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616; PASTA &#30340;&#24615;&#33021;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#30340;&#27867;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#20855;&#26377;&#20114;&#34917;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data offers the promise of cheap and bountiful training data for settings where labeled real-world data is scarce. However, models trained on synthetic data significantly underperform when evaluated on real-world data. In this paper, we propose Proportional Amplitude Spectrum Training Augmentation (PASTA), a simple and effective augmentation strategy to improve out-of-the-box synthetic-to-real (syn-to-real) generalization performance. PASTA perturbs the amplitude spectra of synthetic images in the Fourier domain to generate augmented views. Specifically, with PASTA we propose a structured perturbation strategy where high-frequency components are perturbed relatively more than the low-frequency ones. For the tasks of semantic segmentation (GTAV-to-Real), object detection (Sim10K-to-Real), and object recognition (VisDA-C Syn-to-Real), across a total of 5 syn-to-real shifts, we find that PASTA outperforms more complex state-of-the-art generalization methods while being complemen
&lt;/p&gt;</description></item><item><title>SnCQA&#26159;&#19968;&#31181;&#30828;&#20214;&#39640;&#25928;&#30340;&#31561;&#21464;&#37327;&#23376;&#21367;&#31215;&#30005;&#36335;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#25490;&#21015;&#23545;&#31216;&#24615;&#21644;&#31354;&#38388;&#26230;&#26684;&#23545;&#31216;&#24615;&#65292;&#36866;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#25490;&#21015;&#23545;&#31216;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#22122;&#22768;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.12711</link><description>&lt;p&gt;
SnCQA&#65306;&#19968;&#31181;&#30828;&#20214;&#39640;&#25928;&#30340;&#31561;&#21464;&#37327;&#23376;&#21367;&#31215;&#30005;&#36335;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SnCQA: A hardware-efficient equivariant quantum convolutional circuit architecture. (arXiv:2211.12711v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12711
&lt;/p&gt;
&lt;p&gt;
SnCQA&#26159;&#19968;&#31181;&#30828;&#20214;&#39640;&#25928;&#30340;&#31561;&#21464;&#37327;&#23376;&#21367;&#31215;&#30005;&#36335;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#25490;&#21015;&#23545;&#31216;&#24615;&#21644;&#31354;&#38388;&#26230;&#26684;&#23545;&#31216;&#24615;&#65292;&#36866;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#25490;&#21015;&#23545;&#31216;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#22122;&#22768;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SnCQA&#65292;&#36825;&#26159;&#19968;&#32452;&#38024;&#23545;&#25490;&#21015;&#23545;&#31216;&#24615;&#21644;&#31354;&#38388;&#26230;&#26684;&#23545;&#31216;&#24615;&#30340;&#30828;&#20214;&#39640;&#25928;&#31561;&#21464;&#37327;&#23376;&#21367;&#31215;&#30005;&#36335;&#30340;&#21464;&#20998;&#30005;&#36335;&#12290;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#25490;&#21015;&#23545;&#31216;&#24615;&#65292;&#20363;&#22914;&#35768;&#22810;&#37327;&#23376;&#22810;&#20307;&#21644;&#37327;&#23376;&#21270;&#23398;&#38382;&#39064;&#20013;&#24120;&#35265;&#30340;&#26230;&#26684;&#21704;&#23494;&#39039;&#37327;&#65292;&#25105;&#20204;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#36866;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#25490;&#21015;&#23545;&#31216;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#26174;&#33879;&#33410;&#30465;&#35745;&#31639;&#25104;&#26412;&#12290;&#38500;&#20102;&#29702;&#35770;&#30340;&#21019;&#26032;&#24615;&#22806;&#65292;&#22312;&#23454;&#38469;&#30340;&#37327;&#23376;&#35745;&#31639;&#21270;&#23398;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#27169;&#25311;&#22312;&#23398;&#20064;&#22522;&#24577;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#25968;&#21442;&#25968;&#23454;&#29616;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#20256;&#32479;&#30340;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#65288;&#22914;&#32431;&#30828;&#20214;&#39640;&#25928;&#30340;&#22522;&#24577;&#20551;&#35774;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SnCQA&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#22122;&#22768;&#38887;&#24615;&#65288;&#20855;&#26377;20&#20493;&#26356;&#22909;&#30340;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SnCQA, a set of hardware-efficient variational circuits of equivariant quantum convolutional circuits respective to permutation symmetries and spatial lattice symmetries with the number of qubits $n$. By exploiting permutation symmetries of the system, such as lattice Hamiltonians common to many quantum many-body and quantum chemistry problems, Our quantum neural networks are suitable for solving machine learning problems where permutation symmetries are present, which could lead to significant savings of computational costs. Aside from its theoretical novelty, we find our simulations perform well in practical instances of learning ground states in quantum computational chemistry, where we could achieve comparable performances to traditional methods with few tens of parameters. Compared to other traditional variational quantum circuits, such as the pure hardware-efficient ansatz (pHEA), we show that SnCQA is more scalable, accurate, and noise resilient (with $20\times$ bette
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#24212;&#29992;&#20110;&#24191;&#21578;&#24314;&#27169;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#39640;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#31232;&#30095;&#26799;&#24230;&#26356;&#26032;&#30340;&#24191;&#21578;&#25968;&#25454;&#20013;&#25552;&#20379;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.11896</link><description>&lt;p&gt;
&#20351;&#29992;DP-SGD&#30340;&#31169;&#26377;&#24191;&#21578;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Private Ad Modeling with DP-SGD. (arXiv:2211.11896v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#24212;&#29992;&#20110;&#24191;&#21578;&#24314;&#27169;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#39640;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#31232;&#30095;&#26799;&#24230;&#26356;&#26032;&#30340;&#24191;&#21578;&#25968;&#25454;&#20013;&#25552;&#20379;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#26159;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#12290;&#23613;&#31649;&#35813;&#31639;&#27861;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#19978;&#24050;&#32463;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20294;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#23578;&#26410;&#23558;&#20854;&#24212;&#29992;&#20110;&#24191;&#21578;&#25968;&#25454;&#65292;&#32780;&#24191;&#21578;&#25968;&#25454;&#22240;&#20854;&#39640;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#31232;&#30095;&#30340;&#26799;&#24230;&#26356;&#26032;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#26412;&#30740;&#31350;&#25105;&#20204;&#23558;DP-SGD&#24212;&#29992;&#20110;&#22810;&#20010;&#24191;&#21578;&#24314;&#27169;&#20219;&#21153;&#65292;&#21253;&#25324;&#39044;&#27979;&#28857;&#20987;&#29575;&#12289;&#36716;&#21270;&#29575;&#21644;&#36716;&#21270;&#20107;&#20214;&#25968;&#37327;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20854;&#38544;&#31169;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23454;&#35777;&#20102;DP-SGD&#21487;&#20197;&#20026;&#24191;&#21578;&#24314;&#27169;&#20219;&#21153;&#25552;&#20379;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A well-known algorithm in privacy-preserving ML is differentially private stochastic gradient descent (DP-SGD). While this algorithm has been evaluated on text and image data, it has not been previously applied to ads data, which are notorious for their high class imbalance and sparse gradient updates. In this work we apply DP-SGD to several ad modeling tasks including predicting click-through rates, conversion rates, and number of conversion events, and evaluate their privacy-utility trade-off on real-world datasets. Our work is the first to empirically demonstrate that DP-SGD can provide both privacy and utility for ad modeling tasks.
&lt;/p&gt;</description></item><item><title>&#20316;&#32773;&#21457;&#29616;&#20165;&#20351;&#29992;&#31070;&#32463;&#20803;&#23545;&#40784;&#26041;&#27861;&#19981;&#33021;&#26377;&#25928;&#35299;&#20915;&#32447;&#24615;&#25554;&#20540;&#20013;&#28608;&#27963;&#26041;&#24046;&#22349;&#32553;&#30340;&#38382;&#39064;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;REPAIR&#26041;&#27861;&#26469;&#20462;&#22797;&#25554;&#20540;&#30340;&#24402;&#19968;&#21270;&#32622;&#25442;&#28608;&#27963;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;&#26550;&#26500;&#20013;&#23558;REPAIR&#19982;&#31070;&#32463;&#20803;&#23545;&#40784;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22823;&#24133;&#38477;&#20302;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2211.08403</link><description>&lt;p&gt;
REPAIR: &#20462;&#22797;&#25554;&#20540;&#30340;&#24402;&#19968;&#21270;&#32622;&#25442;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
REPAIR: REnormalizing Permuted Activations for Interpolation Repair. (arXiv:2211.08403v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08403
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#21457;&#29616;&#20165;&#20351;&#29992;&#31070;&#32463;&#20803;&#23545;&#40784;&#26041;&#27861;&#19981;&#33021;&#26377;&#25928;&#35299;&#20915;&#32447;&#24615;&#25554;&#20540;&#20013;&#28608;&#27963;&#26041;&#24046;&#22349;&#32553;&#30340;&#38382;&#39064;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;REPAIR&#26041;&#27861;&#26469;&#20462;&#22797;&#25554;&#20540;&#30340;&#24402;&#19968;&#21270;&#32622;&#25442;&#28608;&#27963;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;&#26550;&#26500;&#20013;&#23558;REPAIR&#19982;&#31070;&#32463;&#20803;&#23545;&#40784;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22823;&#24133;&#38477;&#20302;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;Entezari&#31561;&#20154;&#65288;2021&#65289;&#30340;&#29468;&#24819;&#65292;&#21363;&#22914;&#26524;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#65292;&#37027;&#20040;&#32447;&#24615;&#25554;&#20540;&#20043;&#38388;&#21487;&#33021;&#27809;&#26377;&#25439;&#22833;&#38556;&#30861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20165;&#20351;&#29992;&#31070;&#32463;&#20803;&#23545;&#40784;&#26041;&#27861;&#26080;&#27861;&#24314;&#31435;&#20302;&#38556;&#30861;&#32447;&#24615;&#36830;&#25509;&#30340;&#21407;&#22240;&#26159;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#26041;&#24046;&#22349;&#32553;&#30340;&#29616;&#35937;&#65306;&#25554;&#20540;&#28145;&#23618;&#32593;&#32476;&#30340;&#28608;&#27963;&#26041;&#24046;&#23849;&#28291;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REPAIR&#65288;&#20462;&#22797;&#25554;&#20540;&#30340;&#24402;&#19968;&#21270;&#32622;&#25442;&#28608;&#27963;&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#32553;&#25918;&#36825;&#20123;&#25554;&#20540;&#32593;&#32476;&#30340;&#39044;&#28608;&#27963;&#26469;&#32531;&#35299;&#26041;&#24046;&#23849;&#28291;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#24402;&#19968;&#21270;&#23618;&#12289;&#32593;&#32476;&#23485;&#24230;&#21644;&#28145;&#24230;&#36873;&#25321;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#28436;&#31034;&#20102;&#22312;&#21508;&#31181;&#26550;&#26500;&#26063;&#20013;&#20351;&#29992;REPAIR&#20316;&#20026;&#31070;&#32463;&#20803;&#23545;&#40784;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#23558;&#38556;&#30861;&#38477;&#20302;60%&#33267;100%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we look into the conjecture of Entezari et al. (2021) which states that if the permutation invariance of neural networks is taken into account, then there is likely no loss barrier to the linear interpolation between SGD solutions. First, we observe that neuron alignment methods alone are insufficient to establish low-barrier linear connectivity between SGD solutions due to a phenomenon we call variance collapse: interpolated deep networks suffer a collapse in the variance of their activations, causing poor performance. Next, we propose REPAIR (REnormalizing Permuted Activations for Interpolation Repair) which mitigates variance collapse by rescaling the preactivations of such interpolated networks. We explore the interaction between our method and the choice of normalization layer, network width, and depth, and demonstrate that using REPAIR on top of neuron alignment methods leads to 60%-100% relative barrier reduction across a wide variety of architecture families and t
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#20840;&#26223;&#20998;&#21106;&#12290;&#20182;&#20204;&#23558;&#20840;&#26223;&#20998;&#21106;&#38382;&#39064;&#23450;&#20041;&#20026;&#31163;&#25955;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#20840;&#26223;&#25513;&#30721;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#24314;&#27169;&#35270;&#39057;&#65292;&#24182;&#33258;&#21160;&#23398;&#20064;&#36319;&#36394;&#23545;&#35937;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#23478;&#26041;&#27861;&#31454;&#20105;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.06366</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#20840;&#26223;&#20998;&#21106;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generalist Framework for Panoptic Segmentation of Images and Videos. (arXiv:2210.06366v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#20840;&#26223;&#20998;&#21106;&#12290;&#20182;&#20204;&#23558;&#20840;&#26223;&#20998;&#21106;&#38382;&#39064;&#23450;&#20041;&#20026;&#31163;&#25955;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#20840;&#26223;&#25513;&#30721;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#24314;&#27169;&#35270;&#39057;&#65292;&#24182;&#33258;&#21160;&#23398;&#20064;&#36319;&#36394;&#23545;&#35937;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#23478;&#26041;&#27861;&#31454;&#20105;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;&#20998;&#21106;&#20026;&#22270;&#20687;&#30340;&#27599;&#20010;&#20687;&#32032;&#20998;&#37197;&#35821;&#20041;&#21644;&#23454;&#20363;ID&#26631;&#31614;&#12290;&#30001;&#20110;&#23454;&#20363;ID&#30340;&#25490;&#21015;&#20063;&#26159;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#20219;&#21153;&#38656;&#35201;&#23398;&#20064;&#39640;&#32500;&#24230;&#30340;&#19968;&#23545;&#22810;&#26144;&#23556;&#12290;&#22240;&#27492;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#23450;&#21046;&#30340;&#26550;&#26500;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#23558;&#20840;&#26223;&#20998;&#21106;&#38382;&#39064;&#23450;&#20041;&#20026;&#31163;&#25955;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#19981;&#20381;&#36182;&#20219;&#21153;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#20840;&#26223;&#25513;&#30721;&#65292;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#21644;&#36890;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#36890;&#36807;&#23558;&#36807;&#21435;&#30340;&#39044;&#27979;&#20316;&#20026;&#26465;&#20214;&#20449;&#21495;&#28155;&#21152;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#24314;&#27169;&#35270;&#39057;&#65292;&#24182;&#33258;&#21160;&#23398;&#20064;&#36319;&#36394;&#23545;&#35937;&#23454;&#20363;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#22312;&#31867;&#20284;&#30340;&#35774;&#32622;&#19979;&#33021;&#22815;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#23478;&#26041;&#27861;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoptic segmentation assigns semantic and instance ID labels to every pixel of an image. As permutations of instance IDs are also valid solutions, the task requires learning of high-dimensional one-to-many mapping. As a result, state-of-the-art approaches use customized architectures and task-specific loss functions. We formulate panoptic segmentation as a discrete data generation problem, without relying on inductive bias of the task. A diffusion model is proposed to model panoptic masks, with a simple architecture and generic loss function. By simply adding past predictions as a conditioning signal, our method is capable of modeling video (in a streaming setting) and thereby learns to track object instances automatically. With extensive experiments, we demonstrate that our simple approach can perform competitively to state-of-the-art specialist methods in similar settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;RL&#30340;&#26426;&#22120;&#20154;&#39044;&#35757;&#32451;&#26694;&#26550;(PTR)&#65292;&#21487;&#20197;&#23558;&#29616;&#26377;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#35757;&#32451;&#19982;&#23569;&#37327;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#26080;&#38656;&#34920;&#31034;&#23398;&#20064;&#25110;vision-based pretraining&#12290;</title><link>http://arxiv.org/abs/2210.05178</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#30340;&#39044;&#35757;&#32451;&#65306;&#31163;&#32447;RL&#20351;&#20854;&#21487;&#20197;&#20174;&#23569;&#37327;&#35797;&#39564;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Pre-Training for Robots: Offline RL Enables Learning New Tasks from a Handful of Trials. (arXiv:2210.05178v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;RL&#30340;&#26426;&#22120;&#20154;&#39044;&#35757;&#32451;&#26694;&#26550;(PTR)&#65292;&#21487;&#20197;&#23558;&#29616;&#26377;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#35757;&#32451;&#19982;&#23569;&#37327;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#26080;&#38656;&#34920;&#31034;&#23398;&#20064;&#25110;vision-based pretraining&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#24432;&#26174;&#20102;&#21033;&#29992;&#22810;&#26679;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#23454;&#29616;&#26377;&#25928;&#27867;&#21270;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#36825;&#35753;&#25105;&#20204;&#26377;&#21160;&#21147;&#32771;&#34385;&#21033;&#29992;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#23454;&#29616;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#40065;&#26834;&#27867;&#21270;&#12290;&#20294;&#26159;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#24076;&#26395;&#22312;&#26032;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#26032;&#30340;&#25216;&#33021;&#65292;&#36825;&#21487;&#33021;&#19981;&#21253;&#21547;&#22312;&#20808;&#21069;&#30340;&#25968;&#25454;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24819;&#30693;&#36947;&#65306;&#22914;&#20309;&#23558;&#29616;&#26377;&#30340;&#22810;&#26679;&#31163;&#32447;&#25968;&#25454;&#38598;&#19982;&#23569;&#37327;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20173;&#28982;&#20139;&#21463;&#22823;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#27867;&#21270;&#20248;&#21183;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31471;&#21040;&#31471;&#31163;&#32447;RL&#21487;&#20197;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#34920;&#31034;&#23398;&#20064;&#25110;&#22522;&#20110;&#35270;&#35273;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#26426;&#22120;&#20154;&#65288;PTR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#65292;&#35797;&#22270;&#36890;&#36807;&#23558;&#29616;&#26377;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#35757;&#32451;&#19982;&#24555;&#36895;&#31934;&#32454;&#35843;&#25972;&#30340;&#36807;&#31243;&#30456;&#32467;&#21512;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in deep learning highlights the tremendous potential of utilizing diverse robotic datasets for attaining effective generalization and makes it enticing to consider leveraging broad datasets for attaining robust generalization in robotic learning as well. However, in practice, we often want to learn a new skill in a new environment that is unlikely to be contained in the prior data. Therefore we ask: how can we leverage existing diverse offline datasets in combination with small amounts of task-specific data to solve new tasks, while still enjoying the generalization benefits of training on large amounts of data? In this paper, we demonstrate that end-to-end offline RL can be an effective approach for doing this, without the need for any representation learning or vision-based pre-training. We present pre-training for robots (PTR), a framework based on offline RL that attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with rapid fine-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#37327;&#36861;&#36394;&#30340;&#26041;&#27861;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#19982;&#25968;&#25454;&#24322;&#36136;&#24615;&#26080;&#20851;&#65292;&#22312;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#20855;&#26377;&#24212;&#23545;&#24322;&#36136;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.15505</link><description>&lt;p&gt;
&#21160;&#37327;&#36861;&#36394;&#65306;&#24322;&#26500;&#25968;&#25454;&#19978;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21160;&#37327;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Momentum Tracking: Momentum Acceleration for Decentralized Deep Learning on Heterogeneous Data. (arXiv:2209.15505v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#37327;&#36861;&#36394;&#30340;&#26041;&#27861;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#19982;&#25968;&#25454;&#24322;&#36136;&#24615;&#26080;&#20851;&#65292;&#22312;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#20855;&#26377;&#24212;&#23545;&#24322;&#36136;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SGD&#21160;&#37327;&#26159;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#12290;&#23545;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#65292;&#20351;&#29992;&#21160;&#37327;&#30340;&#19968;&#31181;&#30452;&#25509;&#26041;&#27861;&#26159;&#21160;&#37327;&#20998;&#24067;&#24335;SGD&#65288;DSGDm&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#20855;&#26377;&#32479;&#35745;&#24322;&#36136;&#24615;&#26102;&#65292;DSGDm&#30340;&#24615;&#33021;&#19981;&#22914;DSGD&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;DSGDm&#26356;&#33021;&#36866;&#24212;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#20173;&#28982;&#20381;&#36182;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#24182;&#19988;&#24403;&#25968;&#25454;&#20998;&#24067;&#24322;&#36136;&#26102;&#20250;&#24694;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#37327;&#36861;&#36394;&#30340;&#26041;&#27861;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#34987;&#35777;&#26126;&#19982;&#25968;&#25454;&#24322;&#36136;&#24615;&#26080;&#20851;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21160;&#37327;&#36861;&#36394;&#22312;&#30446;&#26631;&#20989;&#25968;&#20026;&#38750;&#20984;&#20989;&#25968;&#19988;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#23545;&#20110;&#20219;&#20309;&#21160;&#37327;&#31995;&#25968;&#37117;&#19982;&#25968;&#25454;&#24322;&#36136;&#24615;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
SGD with momentum is one of the key components for improving the performance of neural networks. For decentralized learning, a straightforward approach using momentum is Distributed SGD (DSGD) with momentum (DSGDm). However, DSGDm performs worse than DSGD when the data distributions are statistically heterogeneous. Recently, several studies have addressed this issue and proposed methods with momentum that are more robust to data heterogeneity than DSGDm, although their convergence rates remain dependent on data heterogeneity and deteriorate when the data distributions are heterogeneous. In this study, we propose Momentum Tracking, which is a method with momentum whose convergence rate is proven to be independent of data heterogeneity. More specifically, we analyze the convergence rate of Momentum Tracking in the setting where the objective function is non-convex and the stochastic gradient is used. Then, we identify that it is independent of data heterogeneity for any momentum coeffici
&lt;/p&gt;</description></item><item><title>&#24191;&#20041;&#25490;&#21015;&#32990;&#20869;&#26522;&#20030;&#20102;&#26368;&#22823;&#27744;&#21270;&#21709;&#24212;&#30340;&#32452;&#21512;&#23398;&#65292;&#36890;&#36807;&#35745;&#31639;Minkowski&#21644;&#30340;&#39030;&#28857;&#25968;&#37327;&#65292;&#24471;&#21040;&#20102;&#26368;&#22823;&#27744;&#21270;&#23618;&#30340;&#32447;&#24615;&#21306;&#22495;&#25968;&#37327;&#65292;&#24182;&#24471;&#21040;&#20102;&#20851;&#20110;&#31383;&#21475;&#22823;&#23567;&#21644;&#27493;&#24133;&#30340;&#19968;&#32500;&#26368;&#22823;&#27744;&#21270;&#23618;&#39030;&#28857;&#21644;&#38754;&#30340;&#29983;&#25104;&#20989;&#25968;&#21644;&#23553;&#38381;&#20844;&#24335;&#65292;&#20197;&#21450;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#19979;&#20108;&#32500;&#26368;&#22823;&#27744;&#21270;&#30340;&#39030;&#28857;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.14978</link><description>&lt;p&gt;
&#24191;&#20041;&#25490;&#21015;&#32990;&#20869;&#26522;&#20030;&#26368;&#22823;&#27744;&#21270;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Enumeration of max-pooling responses with generalized permutohedra. (arXiv:2209.14978v2 [math.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14978
&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#25490;&#21015;&#32990;&#20869;&#26522;&#20030;&#20102;&#26368;&#22823;&#27744;&#21270;&#21709;&#24212;&#30340;&#32452;&#21512;&#23398;&#65292;&#36890;&#36807;&#35745;&#31639;Minkowski&#21644;&#30340;&#39030;&#28857;&#25968;&#37327;&#65292;&#24471;&#21040;&#20102;&#26368;&#22823;&#27744;&#21270;&#23618;&#30340;&#32447;&#24615;&#21306;&#22495;&#25968;&#37327;&#65292;&#24182;&#24471;&#21040;&#20102;&#20851;&#20110;&#31383;&#21475;&#22823;&#23567;&#21644;&#27493;&#24133;&#30340;&#19968;&#32500;&#26368;&#22823;&#27744;&#21270;&#23618;&#39030;&#28857;&#21644;&#38754;&#30340;&#29983;&#25104;&#20989;&#25968;&#21644;&#23553;&#38381;&#20844;&#24335;&#65292;&#20197;&#21450;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#19979;&#20108;&#32500;&#26368;&#22823;&#27744;&#21270;&#30340;&#39030;&#28857;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#22823;&#27744;&#21270;&#23618;&#30340;&#32452;&#21512;&#23398;&#65292;&#26368;&#22823;&#27744;&#21270;&#23618;&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#36755;&#20837;&#22352;&#26631;&#30340;&#31227;&#21160;&#31383;&#21475;&#19978;&#21462;&#26368;&#22823;&#20540;&#26469;&#38477;&#37319;&#26679;&#36755;&#20837;&#25968;&#32452;&#30340;&#20989;&#25968;&#65292;&#23427;&#20204;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#34987;&#20351;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#31561;&#20215;&#22320;&#35745;&#31639;&#26576;&#20123;&#21333;&#24418;&#30340;Minkowski&#21644;&#30340;&#39030;&#28857;&#25968;&#37327;&#26469;&#24471;&#21040;&#36825;&#20123;&#20989;&#25968;&#30340;&#32447;&#24615;&#21306;&#22495;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#36825;&#20123;&#22810;&#32990;&#20307;&#30340;&#38754;&#65292;&#24182;&#33719;&#24471;&#20102;&#20381;&#36182;&#20110;&#27744;&#21270;&#31383;&#21475;&#22823;&#23567;&#21644;&#27493;&#24133;&#30340;&#19968;&#32500;&#26368;&#22823;&#27744;&#21270;&#23618;&#30340;&#39030;&#28857;&#21644;&#38754;&#30340;&#29983;&#25104;&#20989;&#25968;&#21644;&#23553;&#38381;&#20844;&#24335;&#65292;&#20197;&#21450;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#19979;&#20108;&#32500;&#26368;&#22823;&#27744;&#21270;&#30340;&#39030;&#28857;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the combinatorics of max-pooling layers, which are functions that downsample input arrays by taking the maximum over shifted windows of input coordinates, and which are commonly used in convolutional neural networks. We obtain results on the number of linearity regions of these functions by equivalently counting the number of vertices of certain Minkowski sums of simplices. We characterize the faces of such polytopes and obtain generating functions and closed formulas for the number of vertices and facets in a 1D max-pooling layer depending on the size of the pooling windows and stride, and for the number of vertices in a special case of 2D max-pooling.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21464;&#20998;&#33258;&#30001;&#33021;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#23494;&#38598;&#27682;&#30340;&#29366;&#24577;&#26041;&#31243;&#12290;&#36890;&#36807;&#20248;&#21270;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#30740;&#31350;&#32773;&#36798;&#21040;&#20102;&#19982;&#20808;&#21069;&#30340;&#32806;&#21512;&#30005;&#23376;-&#31163;&#23376;&#33945;&#29305;&#21345;&#27931;&#35745;&#31639;&#30456;&#24403;&#30340;&#21464;&#20998;&#33258;&#30001;&#33021;&#12290;&#22312;&#34892;&#26143;&#26465;&#20214;&#19979;&#65292;&#39044;&#27979;&#30340;&#23494;&#38598;&#27682;&#29366;&#24577;&#26041;&#31243;&#27604;&#20854;&#20182;&#35745;&#31639;&#26041;&#27861;&#26356;&#33268;&#23494;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#30452;&#25509;&#33719;&#24471;&#23494;&#38598;&#27682;&#30340;&#29109;&#21644;&#33258;&#30001;&#33021;&#65292;&#20026;&#34892;&#26143;&#27169;&#25311;&#21644;&#39640;&#21387;&#29289;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2209.06095</link><description>&lt;p&gt;
&#23494;&#38598;&#27682;&#30340;&#28145;&#24230;&#21464;&#20998;&#33258;&#30001;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Variational Free Energy Approach to Dense Hydrogen. (arXiv:2209.06095v2 [cond-mat.str-el] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06095
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21464;&#20998;&#33258;&#30001;&#33021;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#23494;&#38598;&#27682;&#30340;&#29366;&#24577;&#26041;&#31243;&#12290;&#36890;&#36807;&#20248;&#21270;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#30740;&#31350;&#32773;&#36798;&#21040;&#20102;&#19982;&#20808;&#21069;&#30340;&#32806;&#21512;&#30005;&#23376;-&#31163;&#23376;&#33945;&#29305;&#21345;&#27931;&#35745;&#31639;&#30456;&#24403;&#30340;&#21464;&#20998;&#33258;&#30001;&#33021;&#12290;&#22312;&#34892;&#26143;&#26465;&#20214;&#19979;&#65292;&#39044;&#27979;&#30340;&#23494;&#38598;&#27682;&#29366;&#24577;&#26041;&#31243;&#27604;&#20854;&#20182;&#35745;&#31639;&#26041;&#27861;&#26356;&#33268;&#23494;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#30452;&#25509;&#33719;&#24471;&#23494;&#38598;&#27682;&#30340;&#29109;&#21644;&#33258;&#30001;&#33021;&#65292;&#20026;&#34892;&#26143;&#27169;&#25311;&#21644;&#39640;&#21387;&#29289;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21464;&#20998;&#33258;&#30001;&#33021;&#26041;&#27861;&#65292;&#29992;&#20110;&#23494;&#38598;&#27682;&#30340;&#29366;&#24577;&#26041;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24402;&#19968;&#21270;&#27969;&#32593;&#32476;&#26469;&#24314;&#27169;&#36136;&#23376;&#30340;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#32473;&#23450;&#36136;&#23376;&#20301;&#32622;&#30340;&#30005;&#23376;&#27874;&#20989;&#25968;&#12290;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#36825;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#36798;&#21040;&#20102;&#19982;&#20043;&#21069;&#30340;&#32806;&#21512;&#30005;&#23376;-&#31163;&#23376;&#33945;&#29305;&#21345;&#27931;&#35745;&#31639;&#30456;&#24403;&#30340;&#21464;&#20998;&#33258;&#30001;&#33021;&#12290;&#22312;&#34892;&#26143;&#26465;&#20214;&#19979;&#65292;&#23494;&#38598;&#27682;&#30340;&#39044;&#27979;&#29366;&#24577;&#26041;&#31243;&#27604;&#20174;&#22836;&#20998;&#23376;&#21160;&#21147;&#23398;&#35745;&#31639;&#21644;&#32463;&#39564;&#21270;&#23398;&#27169;&#22411;&#30340;&#21457;&#29616;&#26356;&#21152;&#33268;&#23494;&#12290;&#32780;&#19988;&#65292;&#30452;&#25509;&#35775;&#38382;&#23494;&#38598;&#27682;&#30340;&#29109;&#21644;&#33258;&#30001;&#33021;&#22312;&#34892;&#26143;&#27169;&#25311;&#21644;&#39640;&#21387;&#29289;&#29702;&#30740;&#31350;&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
We developed a deep generative model-based variational free energy approach to the equations of state of dense hydrogen. We employ a normalizing flow network to model the proton Boltzmann distribution and a fermionic neural network to model the electron wave function at given proton positions. By jointly optimizing the two neural networks we reached a comparable variational free energy to the previous coupled electron-ion Monte Carlo calculation. The predicted equation of state of dense hydrogen under planetary conditions is denser than the findings of ab initio molecular dynamics calculation and empirical chemical model. Moreover, direct access to the entropy and free energy of dense hydrogen opens new opportunities in planetary modeling and high-pressure physics research.
&lt;/p&gt;</description></item><item><title>&#27450;&#35784;&#25968;&#25454;&#38598;&#22522;&#20934;&#65288;FDB&#65289;&#26159;&#19968;&#20010;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#27719;&#32534;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#27450;&#35784;&#30456;&#20851;&#20219;&#21153;&#65292;&#20026;&#35299;&#20915;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290; (arXiv:2208.14417v3 [cs.LG] UPDATED)</title><link>http://arxiv.org/abs/2208.14417</link><description>&lt;p&gt;
&#27450;&#35784;&#25968;&#25454;&#38598;&#22522;&#20934;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fraud Dataset Benchmark and Applications. (arXiv:2208.14417v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14417
&lt;/p&gt;
&lt;p&gt;
&#27450;&#35784;&#25968;&#25454;&#38598;&#22522;&#20934;&#65288;FDB&#65289;&#26159;&#19968;&#20010;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#27719;&#32534;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#27450;&#35784;&#30456;&#20851;&#20219;&#21153;&#65292;&#20026;&#35299;&#20915;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290; (arXiv:2208.14417v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22810;&#27169;&#24577;&#21644;&#34920;&#26684;&#35774;&#32622;&#26041;&#38754;&#25512;&#21160;&#20102;&#21019;&#26032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#30456;&#27604;&#65292;&#27450;&#35784;&#26816;&#27979;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#39640;&#32423;&#21035;&#30340;&#19981;&#24179;&#34913;&#12289;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#31867;&#22411;&#12289;&#39057;&#32321;&#21464;&#21270;&#30340;&#27450;&#35784;&#27169;&#24335;&#21644;&#38382;&#39064;&#30340;&#23545;&#25239;&#24615;&#12290;&#30001;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#23545;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#30340;&#24314;&#27169;&#26041;&#27861;&#21487;&#33021;&#23545;&#27450;&#35784;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27450;&#35784;&#25968;&#25454;&#38598;&#22522;&#20934;&#65288;FDB&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#27719;&#32534;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#27450;&#35784;&#30456;&#20851;&#20219;&#21153;&#65292;&#21253;&#25324;&#35782;&#21035;&#26080;&#21345;&#20132;&#26131;&#12289;&#26816;&#27979;&#26426;&#22120;&#20154;&#25915;&#20987;&#12289;&#20998;&#31867;&#24694;&#24847;URL&#12289;&#20272;&#35745;&#36151;&#27454;&#36829;&#32422;&#39118;&#38505;&#21644;&#20869;&#23481;&#23457;&#26680;&#31561;&#12290;&#22522;&#20110;Python&#30340;FDB&#24211;&#25552;&#20379;&#20102;&#19968;&#33268;&#30340;API&#29992;&#20110;&#25968;&#25454;&#21152;&#36733;&#21644;&#26631;&#20934;&#21270;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#21010;&#20998;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#20960;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standardized datasets and benchmarks have spurred innovations in computer vision, natural language processing, multi-modal and tabular settings. We note that, as compared to other well researched fields, fraud detection has unique challenges: high-class imbalance, diverse feature types, frequently changing fraud patterns, and adversarial nature of the problem. Due to these, the modeling approaches evaluated on datasets from other research fields may not work well for the fraud detection. In this paper, we introduce Fraud Dataset Benchmark (FDB), a compilation of publicly available datasets catered to fraud detection FDB comprises variety of fraud related tasks, ranging from identifying fraudulent card-not-present transactions, detecting bot attacks, classifying malicious URLs, estimating risk of loan default to content moderation. The Python based library for FDB provides a consistent API for data loading with standardized training and testing splits. We demonstrate several application
&lt;/p&gt;</description></item><item><title>DenseShift&#32593;&#32476;&#26159;&#19968;&#31181;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20302;&#20301;&#24130;&#20056;&#27861;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;Shift&#32593;&#32476;&#30340;&#31934;&#24230;&#21644;&#24341;&#20837;&#38750;&#37327;&#21270;&#28014;&#28857;&#28608;&#27963;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09708</link><description>&lt;p&gt;
DenseShift: &#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20302;&#20301;&#24130;&#20056;&#27861;&#30340;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization. (arXiv:2208.09708v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09708
&lt;/p&gt;
&lt;p&gt;
DenseShift&#32593;&#32476;&#26159;&#19968;&#31181;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20302;&#20301;&#24130;&#20056;&#27861;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;Shift&#32593;&#32476;&#30340;&#31934;&#24230;&#21644;&#24341;&#20837;&#38750;&#37327;&#21270;&#28014;&#28857;&#28608;&#27963;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#36793;&#32536;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20854;&#19981;&#26029;&#22686;&#21152;&#30340;&#36164;&#28304;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#26080;&#20056;&#27861;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#24130;&#20056;&#27861;&#30340;&#37327;&#21270;&#65292;&#20063;&#34987;&#31216;&#20026;Shift&#32593;&#32476;&#65292;&#26088;&#22312;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#21644;&#31616;&#21270;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20302;&#20301;Shift&#32593;&#32476;&#19981;&#22914;&#20840;&#31934;&#24230;&#32593;&#32476;&#20934;&#30830;&#65292;&#36890;&#24120;&#21463;&#21040;&#26377;&#38480;&#26435;&#37325;&#33539;&#22260;&#32534;&#30721;&#26041;&#26696;&#21644;&#37327;&#21270;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DenseShift&#32593;&#32476;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;Shift&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#35270;&#35273;&#21644;&#35821;&#38899;&#24212;&#29992;&#23454;&#29616;&#20102;&#19982;&#20840;&#31934;&#24230;&#32593;&#32476;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#37327;&#21270;&#28014;&#28857;&#28608;&#27963;&#30340;&#39640;&#25928;DenseShift&#32593;&#32476;&#37096;&#32626;&#26041;&#27861;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;1.6&#20493;&#21152;&#36895;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20302;&#20301;Shift&#32593;&#32476;&#20013;&#38646;&#26435;&#37325;&#20540;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently deploying deep neural networks on low-resource edge devices is challenging due to their ever-increasing resource requirements. To address this issue, researchers have proposed multiplication-free neural networks, such as Power-of-Two quantization, or also known as Shift networks, which aim to reduce memory usage and simplify computation. However, existing low-bit Shift networks are not as accurate as their full-precision counterparts, typically suffering from limited weight range encoding schemes and quantization loss. In this paper, we propose the DenseShift network, which significantly improves the accuracy of Shift networks, achieving competitive performance to full-precision networks for vision and speech applications. In addition, we introduce a method to deploy an efficient DenseShift network using non-quantized floating-point activations, while obtaining 1.6X speed-up over existing methods. To achieve this, we demonstrate that zero-weight values in low-bit Shift netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20943;&#23569;&#34164;&#21547;&#20559;&#24046;&#36923;&#36753;&#25439;&#22833;&#65288;RILL&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#20013;&#30001;&#20110;&#20174;&#27169;&#31946;&#36923;&#36753;&#31639;&#23376;&#20013;&#27966;&#29983;&#30340;&#25439;&#22833;&#20989;&#25968;&#24102;&#26469;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;RILL&#30456;&#27604;&#26377;&#20559;&#24046;&#30340;&#36923;&#36753;&#25439;&#22833;&#20989;&#25968;&#22312;&#30693;&#35782;&#24211;&#19981;&#23436;&#25972;&#21644;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#26356;&#24378;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.06838</link><description>&lt;p&gt;
&#20943;&#23569;&#34164;&#21547;&#20559;&#24046;&#36923;&#36753;&#25439;&#22833;&#29992;&#20110;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reduced Implication-bias Logic Loss for Neuro-Symbolic Learning. (arXiv:2208.06838v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20943;&#23569;&#34164;&#21547;&#20559;&#24046;&#36923;&#36753;&#25439;&#22833;&#65288;RILL&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#20013;&#30001;&#20110;&#20174;&#27169;&#31946;&#36923;&#36753;&#31639;&#23376;&#20013;&#27966;&#29983;&#30340;&#25439;&#22833;&#20989;&#25968;&#24102;&#26469;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;RILL&#30456;&#27604;&#26377;&#20559;&#24046;&#30340;&#36923;&#36753;&#25439;&#22833;&#20989;&#25968;&#22312;&#30693;&#35782;&#24211;&#19981;&#23436;&#25972;&#21644;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#26356;&#24378;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21487;&#24494;&#20998;&#31639;&#23376;&#26469;&#36817;&#20284;&#36923;&#36753;&#25512;&#29702;&#65292;&#23558;&#36923;&#36753;&#25512;&#29702;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#26159;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#21487;&#24494;&#20998;&#31639;&#23376;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#21487;&#33021;&#24102;&#26469;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#24182;&#38477;&#20302;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#36825;&#31181;&#20559;&#24046;&#65292;&#31216;&#20043;&#20026;&#8220;&#34164;&#21547;&#20559;&#24046;&#8221;&#65292;&#24120;&#35265;&#20110;&#20174;&#27169;&#31946;&#36923;&#36753;&#31639;&#23376;&#20013;&#27966;&#29983;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#26377;&#20559;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#36716;&#21270;&#20026;&#8220;&#20943;&#23569;&#34164;&#21547;&#20559;&#24046;&#36923;&#36753;&#25439;&#22833;&#65288;RILL&#65289;&#8221;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#26377;&#20559;&#24046;&#30340;&#36923;&#36753;&#25439;&#22833;&#20989;&#25968;&#30456;&#27604;&#65292;RILL&#22312;&#30693;&#35782;&#24211;&#19981;&#23436;&#25972;&#26102;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#22312;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#26102;&#20445;&#25345;&#26356;&#20026;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating logical reasoning and machine learning by approximating logical inference with differentiable operators is a widely used technique in Neuro-Symbolic systems.  However, some differentiable operators could bring a significant bias during backpropagation and degrade the performance of Neuro-Symbolic learning.  In this paper, we reveal that this bias, named \textit{Implication Bias} is common in loss functions derived from fuzzy logic operators.  Furthermore, we propose a simple yet effective method to transform the biased loss functions into \textit{Reduced Implication-bias Logic Loss (RILL)} to address the above problem.  Empirical study shows that RILL can achieve significant improvements compared with the biased logic loss functions, especially when the knowledge base is incomplete, and keeps more robust than the compared methods when labelled data is insufficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36830;&#32493;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;EgPDE-Net&#65292;&#29992;&#20110;&#21253;&#21547;&#22806;&#29983;&#21464;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#32771;&#34385;&#22806;&#29983;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20854;&#23545;&#30446;&#26631;&#24207;&#21015;&#30340;&#24433;&#21709;&#65292;&#23398;&#20064;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26410;&#30693;&#20559;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2208.01913</link><description>&lt;p&gt;
EgPDE-Net&#65306;&#24314;&#31435;&#36830;&#32493;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21253;&#21547;&#22806;&#29983;&#21464;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EgPDE-Net: Building Continuous Neural Networks for Time Series Prediction with Exogenous Variables. (arXiv:2208.01913v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36830;&#32493;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;EgPDE-Net&#65292;&#29992;&#20110;&#21253;&#21547;&#22806;&#29983;&#21464;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#32771;&#34385;&#22806;&#29983;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20854;&#23545;&#30446;&#26631;&#24207;&#21015;&#30340;&#24433;&#21709;&#65292;&#23398;&#20064;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26410;&#30693;&#20559;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22806;&#29983;&#21464;&#37327;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#24615;&#33021;&#25913;&#36827;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#65292;&#20294;&#29616;&#26377;&#30340;&#36830;&#32493;&#26041;&#27861;&#24456;&#23569;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#20114;&#30456;&#20851;&#24615;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#21147;&#31995;&#32479;&#21487;&#20197;&#29992;&#22797;&#26434;&#30340;&#26410;&#30693;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26469;&#24314;&#27169;&#65292;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#30340;&#35768;&#22810;&#23398;&#31185;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#20219;&#24847;&#27493;&#38271;&#39044;&#27979;&#65292;&#20197;&#23398;&#20064;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#26410;&#30693;PDE&#31995;&#32479;&#65292;&#20854;&#31649;&#29702;&#26041;&#31243;&#30001;&#33258;&#27880;&#24847;&#21147;&#21644;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;EgPDE-Net&#65292;&#32771;&#34385;&#20102;&#22806;&#29983;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#23545;&#30446;&#26631;&#24207;&#21015;&#30340;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#31616;&#21270;&#20026;&#19968;&#20010;&#27491;&#21017;&#21270;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
While exogenous variables have a major impact on performance improvement in time series analysis, inter-series correlation and time dependence among them are rarely considered in the present continuous methods. The dynamical systems of multivariate time series could be modelled with complex unknown partial differential equations (PDEs) which play a prominent role in many disciplines of science and engineering. In this paper, we propose a continuous-time model for arbitrary-step prediction to learn an unknown PDE system in multivariate time series whose governing equations are parameterised by self-attention and gated recurrent neural networks. The proposed model, \underline{E}xogenous-\underline{g}uided \underline{P}artial \underline{D}ifferential \underline{E}quation Network (EgPDE-Net), takes account of the relationships among the exogenous variables and their effects on the target series. Importantly, the model can be reduced into a regularised ordinary differential equation (ODE) p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CANA&#30340;&#23545;&#25239;&#20266;&#35013;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#20013;&#20351;&#34987;&#27880;&#20837;&#33410;&#28857;&#30475;&#36215;&#26469;&#27491;&#24120;&#65292;&#24182;&#25552;&#39640;&#22312;&#23454;&#38469;&#22330;&#26223;&#19979;&#38450;&#24481;/&#26816;&#27979;&#26041;&#27861;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.01819</link><description>&lt;p&gt;
&#22270;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#30340;&#23545;&#25239;&#20266;&#35013;
&lt;/p&gt;
&lt;p&gt;
Adversarial Camouflage for Node Injection Attack on Graphs. (arXiv:2208.01819v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CANA&#30340;&#23545;&#25239;&#20266;&#35013;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#20013;&#20351;&#34987;&#27880;&#20837;&#33410;&#28857;&#30475;&#36215;&#26469;&#27491;&#24120;&#65292;&#24182;&#25552;&#39640;&#22312;&#23454;&#38469;&#22330;&#26223;&#19979;&#38450;&#24481;/&#26816;&#27979;&#26041;&#27861;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#25152;&#21463;&#21040;&#30340;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#65288;Node injection attacks&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#37325;&#35270;&#65292;&#22240;&#20026;&#36825;&#31181;&#25915;&#20987;&#20855;&#26377;&#24456;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#33021;&#26174;&#33879;&#38477;&#20302;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#25915;&#20987;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#32463;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#38450;&#24481;/&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#21644;&#21024;&#38500;&#34987;&#27880;&#20837;&#30340;&#33410;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36827;&#34892;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#30340;&#20266;&#35013;&#65292;&#20351;&#34987;&#27880;&#20837;&#30340;&#33410;&#28857;&#30475;&#36215;&#26469;&#27491;&#24120;&#65292;&#24182;&#19988;&#23545;&#20110;&#38450;&#24481;/&#26816;&#27979;&#26041;&#27861;&#26159;&#19981;&#21487;&#24863;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#22270;&#25968;&#25454;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24615;&#36136;&#21644;&#32570;&#20047;&#30452;&#35266;&#20808;&#39564;&#30693;&#35782;&#20026;&#20266;&#35013;&#30340;&#24418;&#24335;&#21270;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#24182;&#23450;&#20041;&#20102;&#20266;&#35013;&#20316;&#20026;&#27880;&#20837;&#33410;&#28857;&#21644;&#27491;&#24120;&#33410;&#28857;&#30340;&#37051;&#22495;&#32593;&#32476;&#20043;&#38388;&#30340;&#20998;&#24067;&#30456;&#20284;&#24615;&#12290;&#28982;&#21518;&#65292;&#38024;&#23545;&#23454;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#30340;&#23545;&#25239;&#20266;&#35013;&#26694;&#26550;&#65292;&#21363;CANA&#65292;&#20197;&#25552;&#39640;&#22312;&#23454;&#38469;&#22330;&#26223;&#19979;&#38450;&#24481;/&#26816;&#27979;&#26041;&#27861;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node injection attacks on Graph Neural Networks (GNNs) have received emerging attention due to their potential to significantly degrade GNN performance with high attack success rates. However, our study indicates these attacks often fail in practical scenarios, since defense/detection methods can easily identify and remove the injected nodes. To address this, we devote to camouflage node injection attack, making injected nodes appear normal and imperceptible to defense/detection methods. Unfortunately, the non-Euclidean nature of graph data and lack of intuitive prior present great challenges to the formalization, implementation, and evaluation of camouflage. In this paper, we first propose and define camouflage as distribution similarity between ego networks of injected nodes and normal nodes. Then for implementation, we propose an adversarial CAmouflage framework for Node injection Attack, namely CANA, to improve attack performance under defense/detection methods in practical scenari
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#20998;&#25955;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#24182;&#22312;Polairs&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#23637;&#31034;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2207.00479</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24322;&#27493;&#20998;&#25955;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Decentralized Bayesian Optimization for Large Scale Hyperparameter Optimization. (arXiv:2207.00479v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00479
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#20998;&#25955;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#24182;&#22312;Polairs&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#23637;&#31034;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#38656;&#35201;&#20960;&#20998;&#38047;&#21040;&#20960;&#23567;&#26102;&#30340;&#26102;&#38388;&#12290;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#65292;&#37319;&#29992;&#35745;&#31639;&#20415;&#23452;&#30340;&#26367;&#20195;&#27169;&#22411;&#26469;&#23398;&#20064;&#21442;&#25968;&#37197;&#32622;&#19982;&#24615;&#33021;&#65288;&#22914;&#20934;&#30830;&#24615;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24182;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#21333;&#20010;&#31649;&#29702;&#22120;-&#22810;&#20010;&#24037;&#20316;&#32773;&#31574;&#30053;&#65292;&#21516;&#26102;&#35780;&#20272;&#22810;&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#12290;&#23613;&#31649;&#36229;&#21442;&#25968;&#35780;&#20272;&#26102;&#38388;&#30456;&#24403;&#38271;&#65292;&#20294;&#36825;&#31181;&#38598;&#20013;&#24335;&#26041;&#26696;&#30340;&#24320;&#38144;&#38459;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#22823;&#37327;&#24037;&#20316;&#32773;&#19978;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#20998;&#25955;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#24037;&#20316;&#32773;&#36816;&#34892;&#39034;&#24207;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#23384;&#20648;&#24322;&#27493;&#36890;&#20449;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;1,920&#20010;&#24182;&#34892;&#24037;&#20316;&#32773;&#65288;Polaris&#36229;&#32423;&#35745;&#31639;&#26426;&#30340;&#23436;&#25972;&#29983;&#20135;&#38431;&#21015;&#65289;&#65292;&#24182;&#23637;&#31034;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a promising approach for hyperparameter optimization of deep neural networks (DNNs), where each model training can take minutes to hours. In BO, a computationally cheap surrogate model is employed to learn the relationship between parameter configurations and their performance such as accuracy. Parallel BO methods often adopt single manager/multiple workers strategies to evaluate multiple hyperparameter configurations simultaneously. Despite significant hyperparameter evaluation time, the overhead in such centralized schemes prevents these methods to scale on a large number of workers. We present an asynchronous-decentralized BO, wherein each worker runs a sequential BO and asynchronously communicates its results through shared storage. We scale our method without loss of computational efficiency with above 95% of worker's utilization to 1,920 parallel workers (full production queue of the Polaris supercomputer) and demonstrate improvement in model accurac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#19988;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#22522;&#20110;XGBoost&#31639;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20167;&#24680;&#21644;&#20882;&#29359;&#24615;&#35328;&#35770;&#12290;&#35813;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#30340;Twitter&#25968;&#25454;&#19978;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#38477;&#37319;&#26679;&#21518;&#30340;&#25968;&#25454;&#20013;&#20063;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.12983</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#19988;&#39640;&#24615;&#33021;&#30340;&#20167;&#24680;&#21644;&#20882;&#29359;&#24615;&#35328;&#35770;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable and High-Performance Hate and Offensive Speech Detection. (arXiv:2206.12983v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12983
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#19988;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#22522;&#20110;XGBoost&#31639;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20167;&#24680;&#21644;&#20882;&#29359;&#24615;&#35328;&#35770;&#12290;&#35813;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#30340;Twitter&#25968;&#25454;&#19978;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#38477;&#37319;&#26679;&#21518;&#30340;&#25968;&#25454;&#20013;&#20063;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#20449;&#24687;&#30340;&#20256;&#25773;&#21487;&#33021;&#20250;&#22312;&#33030;&#24369;&#31038;&#32676;&#20013;&#21019;&#36896;&#25932;&#23545;&#30340;&#29615;&#22659;&#65292;&#24182;&#20351;&#26576;&#20123;&#32676;&#20307;&#27785;&#40664;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#24050;&#24320;&#21457;&#20102;&#22810;&#20010;&#27169;&#22411;&#26469;&#26816;&#27979;&#20167;&#24680;&#21644;&#20882;&#29359;&#24615;&#35328;&#35770;&#12290;&#30001;&#20110;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#26816;&#27979;&#20167;&#24680;&#21644;&#20882;&#29359;&#24615;&#35328;&#35770;&#21487;&#33021;&#38169;&#35823;&#22320;&#23558;&#20010;&#20307;&#25490;&#38500;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20043;&#22806;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#20449;&#20219;&#24230;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#21019;&#24314;&#21487;&#35299;&#37322;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;XGBoost&#31639;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#19988;&#21487;&#35299;&#37322;&#30340;&#39640;&#24615;&#33021;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;Twitter&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#23545;&#20110;&#19981;&#24179;&#34913;&#30340;Twitter&#25968;&#25454;&#65292;&#30456;&#27604;&#20110;LSTM&#12289;AutoGluon&#21644;ULMFiT&#27169;&#22411;&#30340;F1&#24471;&#20998;&#20998;&#21035;&#20026;0.38&#21644;0.37&#65292;&#20197;&#21450;0.38&#65292;XGBoost&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20026;0.75&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;&#24403;&#25105;&#20204;&#23558;&#25968;&#25454;&#38477;&#37319;&#26679;&#20026;&#32422;5000&#26465;&#25512;&#25991;&#30340;&#19977;&#20010;&#29420;&#31435;&#31867;&#21035;&#26102;&#65292;XGBoost&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20063;&#20248;&#20110;LSTM&#12289;AutoGluon&#21644;ULMFiT&#65292;&#20026;0.79&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of information through social media platforms can create environments possibly hostile to vulnerable communities and silence certain groups in society. To mitigate such instances, several models have been developed to detect hate and offensive speech. Since detecting hate and offensive speech in social media platforms could incorrectly exclude individuals from social media platforms, which can reduce trust, there is a need to create explainable and interpretable models. Thus, we build an explainable and interpretable high performance model based on the XGBoost algorithm, trained on Twitter data. For unbalanced Twitter data, XGboost outperformed the LSTM, AutoGluon, and ULMFiT models on hate speech detection with an F1 score of 0.75 compared to 0.38 and 0.37, and 0.38 respectively. When we down-sampled the data to three separate classes of approximately 5000 tweets, XGBoost performed better than LSTM, AutoGluon, and ULMFiT; with F1 scores for hate speech detection of 0.79 vs 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;MapReduce&#21644;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#27169;&#22411;&#20013;&#29992;&#20110;&#22823;&#23567;&#32422;&#26463;&#23376;&#27169;&#22411;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20960;&#31181;&#27425;&#32447;&#24615;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#28385;&#36275;&#22312;MR&#29615;&#22659;&#20013;&#25152;&#38656;&#30340;&#19968;&#33268;&#24615;&#29305;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19990;&#30028;&#39318;&#20010;&#20855;&#26377;&#24658;&#23450;MR&#36718;&#27425;&#30340;&#32447;&#24615;&#26102;&#38388;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.09563</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#22312;MapReduce&#21644;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#27169;&#22411;&#20013;&#29992;&#20110;&#22823;&#23567;&#32422;&#26463;&#23376;&#27169;&#22411;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scalable Distributed Algorithms for Size-Constrained Submodular Maximization in the MapReduce and Adaptive Complexity Models. (arXiv:2206.09563v4 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;MapReduce&#21644;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#27169;&#22411;&#20013;&#29992;&#20110;&#22823;&#23567;&#32422;&#26463;&#23376;&#27169;&#22411;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20960;&#31181;&#27425;&#32447;&#24615;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#28385;&#36275;&#22312;MR&#29615;&#22659;&#20013;&#25152;&#38656;&#30340;&#19968;&#33268;&#24615;&#29305;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19990;&#30028;&#39318;&#20010;&#20855;&#26377;&#24658;&#23450;MR&#36718;&#27425;&#30340;&#32447;&#24615;&#26102;&#38388;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;MapReduce&#27169;&#22411;&#20013;&#23545;&#23376;&#27169;&#22411;&#20989;&#25968;&#36827;&#34892;&#20998;&#24067;&#24335;&#26368;&#22823;&#21270;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#36825;&#23548;&#33268;&#20102;&#20004;&#31181;&#26694;&#26550;&#21487;&#20197;&#22312;MR&#29615;&#22659;&#20013;&#36816;&#34892;&#38598;&#20013;&#24335;&#31639;&#27861;&#32780;&#19981;&#25439;&#22833;&#36924;&#36817;&#24615;&#33021;&#65292;&#21482;&#35201;&#38598;&#20013;&#24335;&#31639;&#27861;&#28385;&#36275;&#19968;&#23450;&#30340;&#19968;&#33268;&#24615;&#29305;&#24615; - &#36825;&#20010;&#29305;&#24615;&#20165;&#30001;&#26631;&#20934;&#30340;&#36138;&#23146;&#31639;&#27861;&#21644;&#36830;&#32493;&#36138;&#23146;&#31639;&#27861;&#28385;&#36275;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36824;&#26377;&#19968;&#31995;&#21015;&#30340;&#30740;&#31350;&#24037;&#20316;&#30528;&#30524;&#20110;&#33258;&#36866;&#24212;&#22797;&#26434;&#24230;&#27169;&#22411;&#20013;&#30340;&#23376;&#27169;&#22411;&#26368;&#22823;&#21270;&#30340;&#24182;&#34892;&#35745;&#31639;&#33021;&#21147;&#65292;&#20854;&#20013;&#27599;&#20010;&#32447;&#31243;&#21487;&#20197;&#35775;&#38382;&#25972;&#20010;&#24213;&#38598;&#12290;&#23545;&#20110;&#28385;&#36275;&#22823;&#23567;&#32422;&#26463;&#30340;&#21333;&#35843;&#23376;&#27169;&#22411;&#20989;&#25968;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#31181;&#27425;&#32447;&#24615;&#33258;&#36866;&#24212;&#31639;&#27861;&#28385;&#36275;&#22312;MR&#29615;&#22659;&#20013;&#25152;&#38656;&#30340;&#19968;&#33268;&#24615;&#29305;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#39640;&#24230;&#23454;&#29992;&#30340;&#21487;&#24182;&#34892;&#35745;&#31639;&#21644;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#24658;&#23450;MR&#36718;&#27425;&#30340;&#32447;&#24615;&#26102;&#38388;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#21152;m&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed maximization of a submodular function in the MapReduce model has received much attention, culminating in two frameworks that allow a centralized algorithm to be run in the MR setting without loss of approximation, as long as the centralized algorithm satisfies a certain consistency property - which had only been shown to be satisfied by the standard greedy and continous greedy algorithms. A separate line of work has studied parallelizability of submodular maximization in the adaptive complexity model, where each thread may have access to the entire ground set. For the size-constrained maximization of a monotone and submodular function, we show that several sublinearly adaptive algorithms satisfy the consistency property required to work in the MR setting, which yields highly practical parallelizable and distributed algorithms. Also, we develop the first linear-time distributed algorithm for this problem with constant MR rounds. Finally, we provide a method to increase the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#31934;&#30830;ADMM&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#39640;&#25928;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#26082;&#20855;&#26377;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#65292;&#33021;&#22815;&#24212;&#23545;&#28382;&#21518;&#25928;&#24212;&#65292;&#21448;&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#25910;&#25947;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2204.10607</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#31934;&#30830;ADMM&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning via Inexact ADMM. (arXiv:2204.10607v4 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#31934;&#30830;ADMM&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#39640;&#25928;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#26082;&#20855;&#26377;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#65292;&#33021;&#22815;&#24212;&#23545;&#28382;&#21518;&#25928;&#24212;&#65292;&#21448;&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#25910;&#25947;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#24320;&#21457;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;&#31639;&#27861;&#35201;&#27714;&#35774;&#22791;&#20840;&#21592;&#21442;&#19982;&#25110;&#32773;&#23545;&#25910;&#25947;&#24615;&#20570;&#20986;&#24378;&#22823;&#20551;&#35774;&#12290;&#19982;&#24120;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#31934;&#30830;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;(ADMM)&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#19978;&#22343;&#26377;&#20248;&#21183;&#65292;&#33021;&#22815;&#24212;&#23545;&#28382;&#21518;&#25928;&#24212;&#65292;&#24182;&#22312;&#36739;&#24369;&#30340;&#26465;&#20214;&#19979;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#20540;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the crucial issues in federated learning is how to develop efficient optimization algorithms. Most of the current ones require full device participation and/or impose strong assumptions for convergence. Different from the widely-used gradient descent-based algorithms, in this paper, we develop an inexact alternating direction method of multipliers (ADMM), which is both computation- and communication-efficient, capable of combating the stragglers' effect, and convergent under mild conditions. Furthermore, it has a high numerical performance compared with several state-of-the-art algorithms for federated learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;sFedHP&#30340;&#24102;&#26377;&#23618;&#27425;&#21270;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#31232;&#30095;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#20998;&#23618;&#36817;&#20284;&#26144;&#23556;&#21644;&#36830;&#32493;&#21487;&#24494;&#30340;&#36817;&#20284;L1&#33539;&#25968;&#20316;&#20026;&#31232;&#30095;&#32422;&#26463;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#38754;&#23545;&#22810;&#26679;&#25968;&#25454;&#30340;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.13517</link><description>&lt;p&gt;
&#24102;&#26377;&#23618;&#27425;&#21270;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#31232;&#30095;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sparse Federated Learning with Hierarchical Personalized Models. (arXiv:2203.13517v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;sFedHP&#30340;&#24102;&#26377;&#23618;&#27425;&#21270;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#31232;&#30095;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#20998;&#23618;&#36817;&#20284;&#26144;&#23556;&#21644;&#36830;&#32493;&#21487;&#24494;&#30340;&#36817;&#20284;L1&#33539;&#25968;&#20316;&#20026;&#31232;&#30095;&#32422;&#26463;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#38754;&#23545;&#22810;&#26679;&#25968;&#25454;&#30340;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21487;&#20197;&#22312;&#19981;&#25910;&#38598;&#29992;&#25143;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#38544;&#31169;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#21327;&#20316;&#35757;&#32451;&#12290;&#20854;&#20986;&#33394;&#30340;&#38544;&#31169;&#23433;&#20840;&#28508;&#21147;&#20419;&#36827;&#20102;FL&#22312;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#12289;&#26080;&#32447;&#32593;&#32476;&#12289;&#31227;&#21160;&#35774;&#22791;&#12289;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#20113;&#21307;&#30103;&#27835;&#30103;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;FL&#26041;&#27861;&#22312;&#38750;i.i.d.&#25968;&#25454;&#19978;&#30340;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#19988;&#36890;&#20449;&#27969;&#37327;&#36807;&#22823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#20998;&#23618;&#36817;&#20284;&#26144;&#23556;&#30340;&#20010;&#24615;&#21270;FL&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;&#24102;&#26377;&#23618;&#27425;&#21270;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#31232;&#30095;&#32852;&#37030;&#23398;&#20064;&#65288;sFedHP&#65289;&#65292;&#23427;&#26174;&#33879;&#25913;&#21892;&#20102;&#38754;&#23545;&#22810;&#26679;&#25968;&#25454;&#30340;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#12290;&#36830;&#32493;&#21487;&#24494;&#30340;&#36817;&#20284;L1&#33539;&#25968;&#20063;&#34987;&#29992;&#20316;&#31232;&#30095;&#32422;&#26463;&#20197;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;&#25910;&#25947;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;sFedHP&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#32447;&#24615;&#21152;&#36895;&#24615;&#33021;&#65292;&#32780;&#31232;&#30095;&#32422;&#26463;&#21482;&#20250;&#23558;&#25910;&#25947;&#36895;&#24230;&#38477;&#20302;&#21040;&#19968;&#20010;&#36739;&#23567;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) can achieve privacy-safe and reliable collaborative training without collecting users' private data. Its excellent privacy security potential promotes a wide range of FL applications in Internet-of-Things (IoT), wireless networks, mobile devices, autonomous vehicles, and cloud medical treatment. However, the FL method suffers from poor model performance on non-i.i.d. data and excessive traffic volume. To this end, we propose a personalized FL algorithm using a hierarchical proximal mapping based on the moreau envelop, named sparse federated learning with hierarchical personalized models (sFedHP), which significantly improves the global model performance facing diverse data. A continuously differentiable approximated L1-norm is also used as the sparse constraint to reduce the communication cost. Convergence analysis shows that sFedHP's convergence rate is state-of-the-art with linear speedup and the sparse constraint only reduces the convergence rate to a small e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#26080;&#32447;&#36164;&#28304;&#20998;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#21644;&#25490;&#21015;&#29305;&#24615;&#65292;&#38477;&#20302;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#39044;&#27979;&#21151;&#29575;&#20998;&#37197;&#38382;&#39064;&#26469;&#39564;&#35777;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.03906</link><description>&lt;p&gt;
&#22270;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#26080;&#32447;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Graph Reinforcement Learning for Radio Resource Allocation. (arXiv:2203.03906v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03906
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#26080;&#32447;&#36164;&#28304;&#20998;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#21644;&#25490;&#21015;&#29305;&#24615;&#65292;&#38477;&#20302;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#39044;&#27979;&#21151;&#29575;&#20998;&#37197;&#38382;&#39064;&#26469;&#39564;&#35777;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22788;&#29702;&#26080;&#27169;&#22411;&#21644;&#31471;&#21040;&#31471;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22312;&#36164;&#28304;&#20998;&#37197;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;DRL&#30340;&#39640;&#35757;&#32451;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#23427;&#22312;&#21160;&#24577;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25105;&#20204;&#37319;&#29992;&#22270;&#24378;&#21270;&#23398;&#20064;&#26469;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#20013;&#35768;&#22810;&#38382;&#39064;&#22266;&#26377;&#30340;&#20004;&#31181;&#20851;&#31995;&#20808;&#39564;&#65306;&#25299;&#25169;&#20449;&#24687;&#21644;&#25490;&#21015;&#29305;&#24615;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#35774;&#35745;&#22270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#21033;&#29992;&#36825;&#20004;&#20010;&#20808;&#39564;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24605;&#20102;&#19968;&#31181;&#23558;&#29366;&#24577;&#30697;&#38453;&#36716;&#25442;&#20026;&#29366;&#24577;&#22270;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#28385;&#36275;&#29702;&#24819;&#30340;&#25490;&#21015;&#29305;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#22914;&#20309;&#24212;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20197;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;(DDPG)&#20026;&#20363;&#65292;&#20248;&#21270;&#20102;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#12290;&#19968;&#20010;&#26159;&#39044;&#27979;&#21151;&#29575;&#20998;&#37197;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) for resource allocation has been investigated extensively owing to its ability of handling model-free and end-to-end problems. Yet the high training complexity of DRL hinders its practical use in dynamic wireless systems. To reduce the training cost, we resort to graph reinforcement learning for exploiting two kinds of relational priors inherent in many problems in wireless communications: topology information and permutation properties. To design graph reinforcement learning framework systematically for harnessing the two priors, we first conceive a method to transform state matrix into state graph, and then propose a general method for graph neural networks to satisfy desirable permutation properties. To demonstrate how to apply the proposed methods, we take deep deterministic policy gradient (DDPG) as an example for optimizing two representative resource allocation problems. One is predictive power allocation that minimizes the energy consumed for e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#30340;&#29702;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#26631;&#31614;&#27604;&#20363;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#28145;&#24230;&#23398;&#20064;&#22330;&#26223;&#19979;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.02496</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#26631;&#31614;&#22122;&#22768;&#20013;&#30340;&#26631;&#31614;&#27604;&#20363;&#36827;&#34892;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Label Proportions by Learning with Label Noise. (arXiv:2203.02496v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#30340;&#29702;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#26631;&#31614;&#27604;&#20363;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#28145;&#24230;&#23398;&#20064;&#22330;&#26223;&#19979;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#27604;&#20363;&#23398;&#20064;&#65288;LLP&#65289;&#26159;&#19968;&#20010;&#24369;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#34987;&#20998;&#32452;&#20026;&#21253;&#65292;&#27599;&#20010;&#21253;&#20013;&#35266;&#23519;&#21040;&#30340;&#26159;&#21253;&#20869;&#30340;&#26631;&#31614;&#27604;&#20363;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#23454;&#20363;&#30340;&#26631;&#31614;&#12290;&#20219;&#21153;&#26159;&#23398;&#20064;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#26410;&#26469;&#21333;&#20010;&#23454;&#20363;&#30340;&#26631;&#31614;&#12290;&#20043;&#21069;&#22312;&#22810;&#31867;&#25968;&#25454;&#19978;&#38024;&#23545;LLP&#30340;&#24037;&#20316;&#23578;&#26410;&#24320;&#21457;&#20986;&#19968;&#20010;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#26041;&#27861;&#65292;&#20351;&#29992;\citet{Patrini2017MakingDN}&#30340;&#27491;&#21521;&#26657;&#27491;&#65288;FC&#65289;&#25439;&#22833;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36807;&#24230;&#39118;&#38505;&#30028;&#21644;&#27867;&#21270;&#35823;&#24046;&#20998;&#26512;&#65292;&#21516;&#26102;&#25193;&#23637;&#20102;FC&#25439;&#22833;&#30340;&#29702;&#35770;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#30740;&#31350;&#24847;&#20041;&#12290;&#19982;&#29616;&#26377;&#30340;&#20027;&#35201;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#22330;&#26223;&#19979;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#19978;&#34920;&#29616;&#20986;&#20102;&#25913;&#36827;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from label proportions (LLP) is a weakly supervised classification problem where data points are grouped into bags, and the label proportions within each bag are observed instead of the instance-level labels. The task is to learn a classifier to predict the individual labels of future individual instances. Prior work on LLP for multi-class data has yet to develop a theoretically grounded algorithm. In this work, we provide a theoretically grounded approach to LLP based on a reduction to learning with label noise, using the forward correction (FC) loss of \citet{Patrini2017MakingDN}. We establish an excess risk bound and generalization error analysis for our approach, while also extending the theory of the FC loss which may be of independent interest. Our approach demonstrates improved empirical performance in deep learning scenarios across multiple datasets and architectures, compared to the leading existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20027;&#39064;&#24314;&#27169;&#21644;&#30456;&#23545;&#23494;&#24230;&#20272;&#35745;&#26469;&#36827;&#34892;&#29359;&#32618;&#28909;&#28857;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21040;&#34987;&#35843;&#24230;&#21592;&#24573;&#35270;&#30340;&#22320;&#29702;&#28909;&#28857;&#36235;&#21183;&#65292;&#36825;&#20123;&#28909;&#28857;&#36235;&#21183;&#24448;&#24448;&#19982;&#25972;&#20307;&#20107;&#20214;&#23494;&#24230;&#30340;&#22686;&#21152;&#30456;&#28151;&#28102;&#12290;</title><link>http://arxiv.org/abs/2202.04176</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#39064;&#24314;&#27169;&#21644;&#30456;&#23545;&#23494;&#24230;&#20272;&#35745;&#30340;&#29359;&#32618;&#28909;&#28857;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Crime Hot-Spot Modeling via Topic Modeling and Relative Density Estimation. (arXiv:2202.04176v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20027;&#39064;&#24314;&#27169;&#21644;&#30456;&#23545;&#23494;&#24230;&#20272;&#35745;&#26469;&#36827;&#34892;&#29359;&#32618;&#28909;&#28857;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21040;&#34987;&#35843;&#24230;&#21592;&#24573;&#35270;&#30340;&#22320;&#29702;&#28909;&#28857;&#36235;&#21183;&#65292;&#36825;&#20123;&#28909;&#28857;&#36235;&#21183;&#24448;&#24448;&#19982;&#25972;&#20307;&#20107;&#20214;&#23494;&#24230;&#30340;&#22686;&#21152;&#30456;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29359;&#32618;&#35760;&#24405;&#21465;&#36848;&#30340;&#38598;&#21512;&#36827;&#34892;&#20027;&#39064;&#20998;&#24067;&#30340;&#35745;&#31639;&#65292;&#30830;&#23450;&#30456;&#20284;&#21628;&#21483;&#30340;&#20998;&#32452;&#20197;&#21450;&#23427;&#20204;&#30340;&#30456;&#23545;&#31354;&#38388;&#20998;&#24067;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#27599;&#20010;&#21465;&#36848;&#33719;&#21462;&#19968;&#20010;&#20027;&#39064;&#20998;&#24067;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#36817;&#37051;&#30456;&#23545;&#23494;&#24230;&#20272;&#35745;&#65288;kNN-RDE&#65289;&#26041;&#27861;&#26469;&#33719;&#24471;&#27599;&#20010;&#20027;&#39064;&#30340;&#31354;&#38388;&#30456;&#23545;&#23494;&#24230;&#12290;&#22312;&#20122;&#29305;&#20848;&#22823;&#35686;&#23519;&#23616;&#30340;&#22823;&#37327;&#21465;&#36848;&#25991;&#26723;&#65288;$n=475,019$&#65289;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#36890;&#24120;&#34987;&#21628;&#21483;&#35843;&#24230;&#21592;&#19968;&#24320;&#22987;&#27809;&#26377;&#23519;&#35273;&#21040;&#30340;&#22320;&#29702;&#28909;&#28857;&#36235;&#21183;&#65292;&#36825;&#20123;&#36235;&#21183;&#30001;&#20110;&#19982;&#19968;&#33324;&#20107;&#20214;&#23494;&#24230;&#30340;&#28151;&#28102;&#32780;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to capture groupings of similar calls and determine their relative spatial distribution from a collection of crime record narratives. We first obtain a topic distribution for each narrative, and then propose a nearest neighbors relative density estimation (kNN-RDE) approach to obtain spatial relative densities per topic. Experiments over a large corpus ($n=475,019$) of narrative documents from the Atlanta Police Department demonstrate the viability of our method in capturing geographic hot-spot trends which call dispatchers do not initially pick up on and which go unnoticed due to conflation with elevated event density in general.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#26500;&#24314;&#39640;&#25928;&#30340;&#30452;&#36830;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#33410;&#28857;&#24310;&#36831;&#21644;&#24102;&#23485;&#26435;&#34913;&#65292;&#36866;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;&#36127;&#36733;&#12290;</title><link>http://arxiv.org/abs/2202.03356</link><description>&lt;p&gt;
&#39640;&#25928;&#30452;&#36830;&#25299;&#25169;&#32467;&#26500;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Efficient Direct-Connect Topologies for Collective Communications. (arXiv:2202.03356v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#26500;&#24314;&#39640;&#25928;&#30340;&#30452;&#36830;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#33410;&#28857;&#24310;&#36831;&#21644;&#24102;&#23485;&#26435;&#34913;&#65292;&#36866;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26500;&#24314;&#36866;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;&#30340;&#39640;&#25928;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#38024;&#23545;&#33410;&#28857;&#24310;&#36831;&#19982;&#24102;&#23485;&#26435;&#34913;&#20248;&#21270;&#30340;&#30452;&#36830;&#25299;&#25169;&#32467;&#26500;&#12290;&#36825;&#20010;&#31639;&#27861;&#26694;&#26550;&#20174;&#23567;&#30340;&#22522;&#30784;&#25299;&#25169;&#32467;&#26500;&#21644;&#30456;&#20851;&#30340;&#36890;&#20449;&#36827;&#24230;&#24320;&#22987;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#21487;&#20197;&#36845;&#20195;&#24212;&#29992;&#30340;&#25216;&#26415;&#26469;&#27966;&#29983;&#26356;&#22823;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#36825;&#20123;&#34893;&#29983;&#30340;&#25299;&#25169;&#32467;&#26500;&#30340;&#26102;&#38388;&#34920;&#21487;&#20197;&#19982;&#25193;&#23637;&#19968;&#36215;&#21512;&#25104;&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#20248;&#21270;&#20844;&#24335;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#20026;&#32473;&#23450;&#30340;&#38598;&#32676;&#22823;&#23567;&#21644;&#24230;&#25968;&#21512;&#25104;&#35768;&#22810;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#26102;&#38388;&#34920;&#65292;&#28982;&#21518;&#20026;&#32473;&#23450;&#30340;&#24037;&#20316;&#36127;&#36733;&#30830;&#23450;&#36866;&#24403;&#30340;&#25299;&#25169;&#21644;&#26102;&#38388;&#34920;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#34917;&#19969;&#38754;&#26495;&#37197;&#32622;&#25152;&#38656;&#25299;&#25169;&#32467;&#26500;&#30340;12&#33410;&#28857;&#20809;&#23398;&#23454;&#39564;&#24179;&#21488;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22686;&#21152;&#20102;&#22522;&#20110;&#20998;&#26512;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#29992;&#20110;&#26356;&#22823;&#30340;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of distilling efficient network topologies for collective communications. We provide an algorithmic framework for constructing direct-connect topologies optimized for the node latency vs bandwidth trade-off given a collective communication workload. Our algorithmic framework allows us to start from small base topologies and associated communication schedules and use a set of techniques that can be iteratively applied to derive much larger topologies. The schedules for these derived topologies are either synthesized along with the expansions or computed using an optimization formulation. Our approach allows us to synthesize many different topologies and schedules for a given cluster size and degree, and then identify the appropriate topology and schedule for a given workload. We evaluate our approach on a 12-node optical testbed that uses patch panels for configuring the desired topology and augment it with an analytical-model-based evaluation for larger deployme
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38431;&#21015;&#38271;&#24230;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;Max Queue-Length (M-QL)&#21644;AttentionLight&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;M-QL&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;AttentionLight&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2201.00006</link><description>&lt;p&gt;
&#21033;&#29992;&#38431;&#21015;&#38271;&#24230;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Leveraging Queue Length and Attention Mechanisms for Enhanced Traffic Signal Control Optimization. (arXiv:2201.00006v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.00006
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38431;&#21015;&#38271;&#24230;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;Max Queue-Length (M-QL)&#21644;AttentionLight&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;M-QL&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;AttentionLight&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20013;&#33719;&#24471;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26041;&#27861;&#24448;&#24448;&#20027;&#35201;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#32467;&#26500;&#65292;&#32780;&#24573;&#35270;&#20102;&#36866;&#24403;&#30340;&#20132;&#36890;&#29366;&#24577;&#34920;&#31034;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#19987;&#23478;&#35774;&#35745;&#30340;&#20132;&#36890;&#20449;&#21495;&#30456;&#20301;&#31454;&#20105;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38431;&#21015;&#38271;&#24230;&#20316;&#20026;&#39640;&#25928;&#29366;&#24577;&#34920;&#31034;&#30340;TSC&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65306;(1) &#22522;&#20110;&#38431;&#21015;&#38271;&#24230;&#23646;&#24615;&#35774;&#35745;&#30340;&#20248;&#21270;&#20256;&#32479;&#26041;&#27861;Max Queue-Length (M-QL)&#65307;(2) AttentionLight&#65292;&#19968;&#31181;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20449;&#21495;&#30456;&#20301;&#30456;&#20851;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#30693;&#35782;&#30340;&#30456;&#20301;&#20851;&#31995;&#12290;&#23545;&#22810;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;(1) M-QL&#26041;&#27861;&#20248;&#20110;&#26368;&#26032;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65307;(2) &#36866;&#29992;&#20110;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#65292;&#19988;&#30456;&#23545;&#20110;&#19987;&#23478;&#35774;&#35745;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) techniques for traffic signal control (TSC) have gained increasing popularity in recent years. However, most existing RL-based TSC methods tend to focus primarily on the RL model structure while neglecting the significance of proper traffic state representation. Furthermore, some RL-based methods heavily rely on expert-designed traffic signal phase competition. In this paper, we present a novel approach to TSC that utilizes queue length as an efficient state representation. We propose two new methods: (1) Max Queue-Length (M-QL), an optimization-based traditional method designed based on the property of queue length; and (2) AttentionLight, an RL model that employs the self-attention mechanism to capture the signal phase correlation without requiring human knowledge of phase relationships. Comprehensive experiments on multiple real-world datasets demonstrate the effectiveness of our approach: (1) the M-QL method outperforms the latest RL-based methods; (2) A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21387;&#32553;&#21644;&#23458;&#25143;&#31471;&#26041;&#24046;&#20943;&#23569;&#26041;&#27861;COFIG&#21644;FRECON&#65292;&#20197;&#35299;&#20915;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#36890;&#20449;&#29942;&#39048;&#21644;&#23458;&#25143;&#31471;&#26041;&#24046;&#38382;&#39064;&#12290;&#22312;&#38750;&#20984;&#35774;&#32622;&#19979;&#65292;COFIG&#20855;&#26377;$O(\frac{{(1+\omega)^{3/2}\sqrt{N}}}{{S\epsilon^2}}+\frac{{(1+\omega)N^{2/3}}}{{S\epsilon^2}})$&#30340;&#36890;&#20449;&#36718;&#25968;&#19978;&#38480;&#65292;&#22312;&#20984;&#35774;&#32622;&#19979;&#65292;COFIG&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2112.13097</link><description>&lt;p&gt;
&#21387;&#32553;&#32852;&#37030;&#23398;&#20064;&#20013;&#26356;&#24555;&#30340;&#36895;&#29575;&#19982;&#23458;&#25143;&#31471;&#26041;&#24046;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Faster Rates for Compressed Federated Learning with Client-Variance Reduction. (arXiv:2112.13097v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.13097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21387;&#32553;&#21644;&#23458;&#25143;&#31471;&#26041;&#24046;&#20943;&#23569;&#26041;&#27861;COFIG&#21644;FRECON&#65292;&#20197;&#35299;&#20915;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#36890;&#20449;&#29942;&#39048;&#21644;&#23458;&#25143;&#31471;&#26041;&#24046;&#38382;&#39064;&#12290;&#22312;&#38750;&#20984;&#35774;&#32622;&#19979;&#65292;COFIG&#20855;&#26377;$O(\frac{{(1+\omega)^{3/2}\sqrt{N}}}{{S\epsilon^2}}+\frac{{(1+\omega)N^{2/3}}}{{S\epsilon^2}})$&#30340;&#36890;&#20449;&#36718;&#25968;&#19978;&#38480;&#65292;&#22312;&#20984;&#35774;&#32622;&#19979;&#65292;COFIG&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#36890;&#20449;&#29942;&#39048;&#65292;&#20351;&#29992;&#36890;&#20449;&#21387;&#32553;&#30340;&#31639;&#27861;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#24222;&#22823;&#25968;&#37327;&#12289;&#39640;&#24322;&#36136;&#24615;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#30340;&#23458;&#25143;&#31471;&#23548;&#33268;&#20102;&#39640;&#23458;&#25143;&#31471;&#26041;&#24046;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#21387;&#32553;&#21644;&#23458;&#25143;&#31471;&#26041;&#24046;&#20943;&#23569;&#26041;&#27861;COFIG&#21644;FRECON&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#22312;&#38750;&#20984;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;COFIG&#30340;&#36890;&#20449;&#36718;&#25968;&#19978;&#38480;&#20026;$O(\frac{{(1+\omega)^{3/2}\sqrt{N}}}{{S\epsilon^2}}+\frac{{(1+\omega)N^{2/3}}}{{S\epsilon^2}})$&#65292;&#20854;&#20013;$N$&#26159;&#24635;&#23458;&#25143;&#31471;&#25968;&#65292;$S$&#26159;&#27599;&#36718;&#21442;&#19982;&#30340;&#23458;&#25143;&#31471;&#25968;&#65292;$\epsilon$&#26159;&#25910;&#25947;&#35823;&#24046;&#65292;$\omega$&#26159;&#19982;&#21387;&#32553;&#36816;&#31639;&#31526;&#30456;&#20851;&#30340;&#26041;&#24046;&#21442;&#25968;&#12290;&#23545;&#20110;FRECON&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#20449;&#36718;&#25968;&#30340;&#19978;&#38480;&#20026;$O(\frac{{(1+\omega)\sqrt{N}}}{{S\epsilon^2}})$&#12290;&#22312;&#20984;&#35774;&#32622;&#19979;&#65292;COFIG&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the communication bottleneck in distributed and federated learning applications, algorithms using communication compression have attracted significant attention and are widely used in practice. Moreover, the huge number, high heterogeneity and limited availability of clients result in high client-variance. This paper addresses these two issues together by proposing compressed and client-variance reduced methods COFIG and FRECON. We prove an $O(\frac{(1+\omega)^{3/2}\sqrt{N}}{S\epsilon^2}+\frac{(1+\omega)N^{2/3}}{S\epsilon^2})$ bound on the number of communication rounds of COFIG in the nonconvex setting, where $N$ is the total number of clients, $S$ is the number of clients participating in each round, $\epsilon$ is the convergence error, and $\omega$ is the variance parameter associated with the compression operator. In case of FRECON, we prove an $O(\frac{(1+\omega)\sqrt{N}}{S\epsilon^2})$ bound on the number of communication rounds. In the convex setting, COFIG converges with
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24418;&#24335;&#27169;&#22411;&#21644;&#23454;&#39564;&#23460;&#23454;&#39564;&#32771;&#23519;&#20102;&#26426;&#22120;&#39044;&#27979;&#30340;&#24615;&#36136;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#26368;&#32456;&#20915;&#31574;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;&#21253;&#21547;&#26377;&#20559;&#35265;&#30340;&#20154;&#31867;&#20915;&#31574;&#32773;&#21487;&#33021;&#36870;&#36716;&#31639;&#27861;&#32467;&#26500;&#19982;&#20915;&#31574;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#25490;&#38500;&#21463;&#20445;&#25252;&#32676;&#20307;&#20449;&#24687;&#21487;&#33021;&#26080;&#27861;&#20943;&#23569;&#24046;&#24322;&#29978;&#33267;&#21487;&#33021;&#22686;&#21152;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2110.15310</link><description>&lt;p&gt;
&#20851;&#20110;&#26426;&#22120;&#36741;&#21161;&#20154;&#31867;&#20915;&#31574;&#30340;&#20844;&#27491;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Fairness of Machine-Assisted Human Decisions. (arXiv:2110.15310v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24418;&#24335;&#27169;&#22411;&#21644;&#23454;&#39564;&#23460;&#23454;&#39564;&#32771;&#23519;&#20102;&#26426;&#22120;&#39044;&#27979;&#30340;&#24615;&#36136;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#26368;&#32456;&#20915;&#31574;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;&#21253;&#21547;&#26377;&#20559;&#35265;&#30340;&#20154;&#31867;&#20915;&#31574;&#32773;&#21487;&#33021;&#36870;&#36716;&#31639;&#27861;&#32467;&#26500;&#19982;&#20915;&#31574;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#25490;&#38500;&#21463;&#20445;&#25252;&#32676;&#20307;&#20449;&#24687;&#21487;&#33021;&#26080;&#27861;&#20943;&#23569;&#24046;&#24322;&#29978;&#33267;&#21487;&#33021;&#22686;&#21152;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20013;&#34987;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#30830;&#20445;&#23427;&#20204;&#30340;&#37096;&#32626;&#33021;&#22815;&#20135;&#29983;&#20844;&#24179;&#21644;&#20844;&#27491;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#20851;&#27880;&#28857;&#20419;&#20351;&#20102;&#19968;&#20010;&#36805;&#36895;&#22686;&#38271;&#30340;&#25991;&#29486;&#65292;&#19987;&#27880;&#20110;&#35786;&#26029;&#21644;&#35299;&#20915;&#26426;&#22120;&#39044;&#27979;&#20013;&#30340;&#24046;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26426;&#22120;&#39044;&#27979;&#34987;&#37096;&#32626;&#26469;&#21327;&#21161;&#20154;&#31867;&#20915;&#31574;&#32773;&#20445;&#30041;&#26368;&#32456;&#20915;&#31574;&#26435;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#24418;&#24335;&#27169;&#22411;&#21644;&#23454;&#39564;&#23460;&#23454;&#39564;&#20013;&#32771;&#34385;&#20102;&#26426;&#22120;&#39044;&#27979;&#30340;&#24615;&#36136;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#30340;&#20154;&#31867;&#20915;&#31574;&#12290;&#22312;&#25105;&#20204;&#30340;&#32479;&#35745;&#20915;&#31574;&#24418;&#24335;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21253;&#21547;&#19968;&#20010;&#26377;&#20559;&#35265;&#30340;&#20154;&#31867;&#20915;&#31574;&#32773;&#21487;&#33021;&#36870;&#36716;&#31639;&#27861;&#32467;&#26500;&#19982;&#26368;&#32456;&#20915;&#31574;&#36136;&#37327;&#20043;&#38388;&#30340;&#24120;&#35268;&#20851;&#31995;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#20174;&#39044;&#27979;&#20013;&#25490;&#38500;&#21463;&#20445;&#25252;&#32676;&#20307;&#20449;&#24687;&#21487;&#33021;&#26080;&#27861;&#20943;&#23569;&#29978;&#33267;&#21487;&#33021;&#22686;&#21152;&#26368;&#32456;&#24046;&#24322;&#30340;&#24773;&#20917;&#12290;&#22312;&#23454;&#39564;&#23460;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;...
&lt;/p&gt;
&lt;p&gt;
When machine-learning algorithms are used in high-stakes decisions, we want to ensure that their deployment leads to fair and equitable outcomes. This concern has motivated a fast-growing literature that focuses on diagnosing and addressing disparities in machine predictions. However, many machine predictions are deployed to assist in decisions where a human decision-maker retains the ultimate decision authority. In this article, we therefore consider in a formal model and in a lab experiment how properties of machine predictions affect the resulting human decisions. In our formal model of statistical decision-making, we show that the inclusion of a biased human decision-maker can revert common relationships between the structure of the algorithm and the qualities of resulting decisions. Specifically, we document that excluding information about protected groups from the prediction may fail to reduce, and may even increase, ultimate disparities. In the lab experiment, we demonstrate ho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#23616;&#37096;&#20027;&#25104;&#20998;&#20998;&#26512;&#31639;&#27861;&#65292;&#36890;&#36807;&#25968;&#23398;&#20005;&#26684;&#30340;&#36793;&#30028;&#20272;&#35745;&#20102;&#32500;&#24230;&#21644;&#20999;&#31354;&#38388;&#30340;&#37319;&#26679;&#28857;&#35201;&#27714;&#65292;&#24182;&#21516;&#27493;&#32771;&#34385;&#20102;&#22122;&#22768;&#38750;&#22343;&#21248;&#25968;&#25454;&#20998;&#24067;&#21644;&#21464;&#21270;&#22122;&#22768;&#12290;&#25152;&#26377;&#36793;&#30028;&#20013;&#30340;&#24120;&#25968;&#37117;&#26377;&#26126;&#30830;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2110.06357</link><description>&lt;p&gt;
&#20351;&#29992;Wasserstein&#36317;&#31163;&#36827;&#34892;&#20999;&#31354;&#38388;&#21644;&#32500;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Tangent Space and Dimension Estimation with the Wasserstein Distance. (arXiv:2110.06357v4 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23616;&#37096;&#20027;&#25104;&#20998;&#20998;&#26512;&#31639;&#27861;&#65292;&#36890;&#36807;&#25968;&#23398;&#20005;&#26684;&#30340;&#36793;&#30028;&#20272;&#35745;&#20102;&#32500;&#24230;&#21644;&#20999;&#31354;&#38388;&#30340;&#37319;&#26679;&#28857;&#35201;&#27714;&#65292;&#24182;&#21516;&#27493;&#32771;&#34385;&#20102;&#22122;&#22768;&#38750;&#22343;&#21248;&#25968;&#25454;&#20998;&#24067;&#21644;&#21464;&#21270;&#22122;&#22768;&#12290;&#25152;&#26377;&#36793;&#30028;&#20013;&#30340;&#24120;&#25968;&#37117;&#26377;&#26126;&#30830;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#24179;&#28369;&#32039;&#33268;&#23376;&#27969;&#24418;&#38468;&#36817;&#29420;&#31435;&#25277;&#26679;&#30340;&#28857;&#38598;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#23398;&#20005;&#26684;&#30340;&#36793;&#30028;&#65292;&#29992;&#20110;&#20197;&#39640;&#32622;&#20449;&#24230;&#20272;&#35745;&#27969;&#24418;&#30340;&#32500;&#24230;&#21644;&#20999;&#31354;&#38388;&#25152;&#38656;&#30340;&#37319;&#26679;&#28857;&#25968;&#12290;&#35813;&#20272;&#35745;&#30340;&#31639;&#27861;&#26159;&#23616;&#37096;&#20027;&#25104;&#20998;&#20998;&#26512;(Local PCA)&#65292;&#23427;&#26159;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#23616;&#37096;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#32771;&#34385;&#20102;&#20855;&#26377;&#22122;&#22768;&#38750;&#22343;&#21248;&#25968;&#25454;&#20998;&#24067;&#21644;&#21487;&#33021;&#22312;&#27969;&#24418;&#19978;&#21464;&#21270;&#30340;&#22122;&#22768;&#65292;&#20801;&#35768;&#22312;&#22810;&#20010;&#28857;&#21516;&#26102;&#36827;&#34892;&#20272;&#35745;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#36793;&#30028;&#20013;&#20986;&#29616;&#30340;&#25152;&#26377;&#24120;&#25968;&#37117;&#26377;&#26126;&#30830;&#30340;&#25551;&#36848;&#12290;&#35777;&#26126;&#20351;&#29992;&#30697;&#38453;&#27987;&#24230;&#19981;&#31561;&#24335;&#20272;&#35745;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#24182;&#20351;&#29992;Wasserstein&#36317;&#31163;&#36793;&#30028;&#26469;&#37327;&#21270;&#24213;&#23618;&#27969;&#24418;&#30340;&#38750;&#32447;&#24615;&#21644;&#27010;&#29575;&#27979;&#24230;&#30340;&#38750;&#22343;&#21248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider a set of points sampled independently near a smooth compact submanifold of Euclidean space. We provide mathematically rigorous bounds on the number of sample points required to estimate both the dimension and the tangent spaces of that manifold with high confidence. The algorithm for this estimation is Local PCA, a local version of principal component analysis. Our results accommodate for noisy non-uniform data distribution with the noise that may vary across the manifold, and allow simultaneous estimation at multiple points. Crucially, all of the constants appearing in our bound are explicitly described. The proof uses a matrix concentration inequality to estimate covariance matrices and a Wasserstein distance bound for quantifying nonlinearity of the underlying manifold and non-uniformity of the probability measure.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#26816;&#27979;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;BLM-17m&#65292;&#28085;&#30422;&#20102;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#26399;&#38388;&#30340;17&#30334;&#19975;&#25512;&#25991;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;TF-IDF&#21644;LDA&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2105.01331</link><description>&lt;p&gt;
BLM-17m: &#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#26816;&#27979;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter. (arXiv:2105.01331v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.01331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#26816;&#27979;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;BLM-17m&#65292;&#28085;&#30422;&#20102;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#26399;&#38388;&#30340;17&#30334;&#19975;&#25512;&#25991;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;TF-IDF&#21644;LDA&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26435;&#20445;&#25252;&#26159;&#19990;&#30028;&#19978;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#28085;&#30422;&#26368;&#36817;&#20960;&#20010;&#26376;&#20840;&#29699;&#24433;&#21709;&#28145;&#36828;&#30340;&#20154;&#26435;&#30683;&#30462;&#20043;&#19968;&#8212;&#8212;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;17&#30334;&#19975;&#25512;&#25991;&#30340;&#20027;&#39064;&#26816;&#27979;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25512;&#25991;&#26159;&#20174;2020&#24180;5&#26376;25&#26085;&#33267;2020&#24180;8&#26376;21&#26085;&#25910;&#38598;&#30340;&#65292;&#28085;&#30422;&#20102;&#36825;&#19968;&#20107;&#20214;&#24320;&#22987;&#21518;&#30340;89&#22825;&#12290;&#25105;&#20204;&#36890;&#36807;&#30417;&#27979;&#20840;&#29699;&#21644;&#26412;&#22320;&#25253;&#32440;&#30340;&#26368;&#28909;&#38376;&#26032;&#38395;&#20027;&#39064;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26631;&#35760;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;TF-IDF&#21644;LDA&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;k&#20540;&#23545;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;https://github.com/MeysamAsgariC/BLMT &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protection of human rights is one of the most important problems of our world. In this paper, our aim is to provide a dataset which covers one of the most significant human rights contradiction in recent months affected the whole world, George Floyd incident. We propose a labeled dataset for topic detection that contains 17 million tweets. These Tweets are collected from 25 May 2020 to 21 August 2020 that covers 89 days from start of this incident. We labeled the dataset by monitoring most trending news topics from global and local newspapers. Apart from that, we present two baselines, TF-IDF and LDA. We evaluated the results of these two methods with three different k values for metrics of precision, recall and f1-score. The collected dataset is available at https://github.com/MeysamAsgariC/BLMT.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;$\mathcal{O}(mn)$&#26102;&#38388;&#22797;&#26434;&#24230;&#27714;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#26032;&#31639;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#21521;&#37327;&#21270;&#12289;&#20302;&#20869;&#23384;&#20998;&#37197;&#38656;&#27714;&#31561;&#29305;&#28857;&#65292;&#24182;&#22312;&#38750;&#26041;&#38453;&#32447;&#24615;&#26041;&#31243;&#32452;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#19982;&#25928;&#29575;&#12290;&#31639;&#27861;&#25910;&#25947;&#24615;&#24471;&#21040;&#29702;&#35770;&#35777;&#26126;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#36825;&#19968;&#31639;&#27861;&#35299;&#20915;&#20102;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2104.12570</link><description>&lt;p&gt;
&#22312;$\mathcal{O}(mn)$&#26102;&#38388;&#20869;&#35299;&#20915;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Solution for Systems of Linear Equations, in $\mathcal{O}(mn)$ time. (arXiv:2104.12570v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.12570
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;$\mathcal{O}(mn)$&#26102;&#38388;&#22797;&#26434;&#24230;&#27714;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#26032;&#31639;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#21521;&#37327;&#21270;&#12289;&#20302;&#20869;&#23384;&#20998;&#37197;&#38656;&#27714;&#31561;&#29305;&#28857;&#65292;&#24182;&#22312;&#38750;&#26041;&#38453;&#32447;&#24615;&#26041;&#31243;&#32452;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#19982;&#25928;&#29575;&#12290;&#31639;&#27861;&#25910;&#25947;&#24615;&#24471;&#21040;&#29702;&#35770;&#35777;&#26126;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#36825;&#19968;&#31639;&#27861;&#35299;&#20915;&#20102;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#38750;&#24120;&#24555;&#30340;&#36895;&#24230;&#27714;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#35299;&#12290;&#35813;&#31639;&#27861;&#22312;&#20854;&#22522;&#26412;&#24418;&#24335;&#19978;&#38750;&#24120;&#31616;&#30701;&#65292;&#24182;&#19988;&#26681;&#25454;&#23450;&#20041;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#32780;&#20869;&#23384;&#20998;&#37197;&#38656;&#27714;&#24456;&#20302;&#65292;&#22240;&#20026;&#23545;&#20110;&#27599;&#19968;&#27425;&#36845;&#20195;&#65292;&#20165;&#21033;&#29992;&#32473;&#23450;&#36755;&#20837;&#30697;&#38453;$\mathbf X$&#30340;&#19968;&#20010;&#32500;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25191;&#34892;&#26102;&#38388;&#38750;&#24120;&#30701;&#65292;&#34920;&#29616;&#20986;&#36229;&#36807;$10^2$&#20493;&#30340;&#21152;&#36895;&#27604;&#21644;&#36739;&#20302;&#30340;&#20869;&#23384;&#20998;&#37197;&#38656;&#27714;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38750;&#26041;&#38453;&#32447;&#24615;&#26041;&#31243;&#32452;&#65292;&#20854;&#20013;&#26041;&#31243;&#25968;&#19982;&#29305;&#24449;&#25968;&#30340;&#27604;&#20363;&#36739;&#39640;&#65288;&#39640;&#30246;&#22411;&#26041;&#31243;&#32452;&#65289;&#25110;&#36739;&#20302;&#65288;&#23485;&#22411;&#26041;&#31243;&#32452;&#65289;&#12290;&#20934;&#30830;&#24615;&#39640;&#19988;&#26131;&#20110;&#25511;&#21046;&#65292;&#25968;&#20540;&#32467;&#26524;&#31361;&#20986;&#20102;&#25152;&#25552;&#31639;&#27861;&#22312;&#35745;&#31639;&#26102;&#38388;&#12289;&#35299;&#30340;&#20934;&#30830;&#24615;&#21644;&#20869;&#23384;&#38656;&#27714;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;&#35770;&#25991;&#36824;&#21253;&#25324;&#20102;&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#35777;&#26126;&#65292;&#24182;&#23558;&#25152;&#25552;&#31639;&#27861;&#29702;&#35770;&#25512;&#24191;&#21040;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel algorithm attaining excessively fast, the sought solution of linear systems of equations. The algorithm is short in its basic formulation and, by definition, vectorized, while the memory allocation demands are trivial, because, for each iteration, only one dimension of the given input matrix $\mathbf X$ is utilized. The execution time is very short compared with state-of-the-art methods, exhibiting $&gt; \times 10^2$ speed-up and low memory allocation demands, especially for non-square Systems of Linear Equations, with ratio of equations versus features high (tall systems), or low (wide systems) accordingly. The accuracy is high and straightforwardly controlled, and the numerical results highlight the efficiency of the proposed algorithm, in terms of computation time, solution accuracy and memory demands. The paper also comprises a theoretical proof for the algorithmic convergence, and we extend the implementation of the proposed algorithmic rationale to feature selecti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#22270;&#20687;&#22359;&#21305;&#37197;&#65292;&#36890;&#36807;Transformer&#32534;&#30721;&#22120;&#32858;&#21512;&#22810;&#23610;&#24230;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#22806;&#35266;&#19981;&#21464;&#24615;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#31934;&#24230;&#65292;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2103.11247</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#22270;&#20687;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Attention-Based Multimodal Image Matching. (arXiv:2103.11247v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.11247
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#22270;&#20687;&#22359;&#21305;&#37197;&#65292;&#36890;&#36807;Transformer&#32534;&#30721;&#22120;&#32858;&#21512;&#22810;&#23610;&#24230;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#22806;&#35266;&#19981;&#21464;&#24615;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#31934;&#24230;&#65292;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#22270;&#20687;&#22359;&#21305;&#37197;&#26041;&#27861;&#65292;&#20351;&#29992;Transformer&#32534;&#30721;&#22120;&#23545;&#22810;&#23610;&#24230;Siamese CNN&#30340;&#29305;&#24449;&#22270;&#36827;&#34892;&#27880;&#24847;&#21147;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#32534;&#30721;&#22120;&#33021;&#22815;&#26377;&#25928;&#22320;&#32858;&#21512;&#22810;&#23610;&#24230;&#22270;&#20687;&#23884;&#20837;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#20219;&#21153;&#29305;&#23450;&#30340;&#22806;&#35266;&#19981;&#21464;&#30340;&#22270;&#20687;&#32447;&#32034;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#27531;&#24046;&#26550;&#26500;&#65292;&#20351;&#29992;&#32469;&#36807;&#32534;&#30721;&#22120;&#30340;&#27531;&#24046;&#36830;&#25509;&#12290;&#36825;&#31181;&#39069;&#22806;&#30340;&#23398;&#20064;&#20449;&#21495;&#26377;&#21161;&#20110;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#31934;&#24230;&#65292;&#35777;&#26126;&#20102;&#20854;&#36890;&#29992;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;Transformer&#32534;&#30721;&#22120;&#26550;&#26500;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#27169;&#24577;&#22270;&#20687;&#22359;&#21305;&#37197;&#20219;&#21153;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an attention-based approach for multimodal image patch matching using a Transformer encoder attending to the feature maps of a multiscale Siamese CNN. Our encoder is shown to efficiently aggregate multiscale image embeddings while emphasizing task-specific appearance-invariant image cues. We also introduce an attention-residual architecture, using a residual connection bypassing the encoder. This additional learning signal facilitates end-to-end training from scratch. Our approach is experimentally shown to achieve new state-of-the-art accuracy on both multimodal and single modality benchmarks, illustrating its general applicability. To the best of our knowledge, this is the first successful implementation of the Transformer encoder architecture to the multimodal image patch matching task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#37327;&#21270;&#21453;&#21521;&#20256;&#25773;&#25805;&#20316;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#21344;&#29992;&#20943;&#23569;&#65292;&#21516;&#26102;&#20960;&#20046;&#27809;&#26377;&#38477;&#20302;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2102.04270</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Enabling Binary Neural Network Training on the Edge. (arXiv:2102.04270v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.04270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#37327;&#21270;&#21453;&#21521;&#20256;&#25773;&#25805;&#20316;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#21344;&#29992;&#20943;&#23569;&#65292;&#21516;&#26102;&#20960;&#20046;&#27809;&#26377;&#38477;&#20302;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35745;&#31639;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#24378;&#22823;&#30340;&#20113;&#22522;&#30784;&#35774;&#26045;&#36827;&#34892;&#35757;&#32451;&#12290;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#30001;&#20110;&#20854;&#19982;&#39640;&#31934;&#24230;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#26497;&#39640;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#33410;&#30465;&#65292;&#34987;&#35748;&#20026;&#26159;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#25512;&#26029;&#30340;&#26377;&#24076;&#26395;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35757;&#32451;&#26041;&#27861;&#38656;&#35201;&#21516;&#26102;&#23384;&#20648;&#25152;&#26377;&#23618;&#30340;&#39640;&#31934;&#24230;&#28608;&#27963;&#65292;&#36825;&#36890;&#24120;&#20351;&#24471;&#22312;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#23398;&#20064;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25152;&#38656;&#30340;&#21453;&#21521;&#20256;&#25773;&#25805;&#20316;&#23545;&#37327;&#21270;&#38750;&#24120;&#24378;&#22823;&#65292;&#20174;&#32780;&#20351;&#24471;&#20351;&#29992;&#29616;&#20195;&#27169;&#22411;&#36827;&#34892;&#36793;&#32536;&#23398;&#20064;&#25104;&#20026;&#19968;&#20010;&#23454;&#38469;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#20869;&#23384;&#21344;&#29992;&#20943;&#23569;&#65292;&#21516;&#26102;&#23545;&#27604;Courbariaux&#21644;Bengio&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#20960;&#20046;&#27809;&#26377;&#38477;&#20302;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing computational demands of increasingly complex machine learning models frequently necessitate the use of powerful cloud-based infrastructure for their training. Binary neural networks are known to be promising candidates for on-device inference due to their extreme compute and memory savings over higher-precision alternatives. However, their existing training methods require the concurrent storage of high-precision activations for all layers, generally making learning on memory-constrained devices infeasible. In this article, we demonstrate that the backward propagation operations needed for binary neural network training are strongly robust to quantization, thereby making on-the-edge learning with modern models a practical proposition. We introduce a low-cost binary neural network training strategy exhibiting sizable memory footprint reductions while inducing little to no accuracy loss vs Courbariaux &amp; Bengio's standard approach. These decreases are primarily enabled t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#25511;&#21046;&#23398;&#20064;&#65288;DCL&#65289;&#30340;&#26032;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#36827;&#34892;&#20102;&#37327;&#36523;&#23450;&#21046;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#27979;&#35797;&#24773;&#20917;&#19979;&#65292;DCL&#30456;&#23545;&#20110;&#20256;&#32479;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#25104;&#26412;&#21644;&#26368;&#20248;&#24615;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2011.15122</link><description>&lt;p&gt;
&#28145;&#24230;&#25511;&#21046;&#23398;&#20064;&#29992;&#20110;&#24211;&#23384;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Controlled Learning for Inventory Control. (arXiv:2011.15122v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.15122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#25511;&#21046;&#23398;&#20064;&#65288;DCL&#65289;&#30340;&#26032;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#36827;&#34892;&#20102;&#37327;&#36523;&#23450;&#21046;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#27979;&#35797;&#24773;&#20917;&#19979;&#65292;DCL&#30456;&#23545;&#20110;&#20256;&#32479;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#25104;&#26412;&#21644;&#26368;&#20248;&#24615;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#23450;&#20041;&#65306;&#20256;&#32479;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21253;&#25324;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#31561;&#21508;&#31181;&#30446;&#30340;&#65292;&#26159;&#21542;&#26159;&#24211;&#23384;&#25511;&#21046;&#24212;&#29992;&#20013;&#26368;&#36866;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65311;&#38024;&#23545;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#29305;&#28857;&#37327;&#36523;&#23450;&#21046;&#30340;DRL&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#26159;&#21542;&#20248;&#20110;DRL&#21644;&#20256;&#32479;&#22522;&#20934;&#65311;&#26041;&#27861;/&#32467;&#26524;&#65306;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#22522;&#20110;&#36817;&#20284;&#31574;&#30053;&#36845;&#20195;&#30340;&#26032;&#22411;DRL&#26694;&#26550;&#8212;&#8212;&#28145;&#24230;&#25511;&#21046;&#23398;&#20064;&#65288;DCL&#65289;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;&#24211;&#23384;&#38382;&#39064;&#12290;&#27604;&#36739;&#24615;&#35780;&#20272;&#34920;&#26126;&#65292;DCL&#22312;&#38144;&#21806;&#25439;&#22833;&#24211;&#23384;&#25511;&#21046;&#12289;&#26131;&#33104;&#24211;&#23384;&#31995;&#32479;&#21644;&#20855;&#26377;&#38543;&#26426;&#24341;&#23548;&#26102;&#38388;&#30340;&#24211;&#23384;&#31995;&#32479;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#25152;&#26377;&#27979;&#35797;&#23454;&#20363;&#20013;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#24179;&#22343;&#25104;&#26412;&#65292;&#24182;&#19988;&#32500;&#25345;&#20102;&#26368;&#22810;0.1%&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#26377;&#23454;&#39564;&#37117;&#20351;&#29992;&#30456;&#21516;&#30340;&#36229;&#21442;&#25968;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Problem Definition: Are traditional deep reinforcement learning (DRL) algorithms, developed for a broad range of purposes including game-play and robotics, the most suitable machine learning algorithms for applications in inventory control? To what extent would DRL algorithms tailored to the unique characteristics of inventory control problems provide superior performance compared to DRL and traditional benchmarks? Methodology/results: We propose and study Deep Controlled Learning (DCL), a new DRL framework based on approximate policy iteration specifically designed to tackle inventory problems. Comparative evaluations reveal that DCL outperforms existing state-of-the-art heuristics in lost sales inventory control, perishable inventory systems, and inventory systems with random lead times, achieving lower average costs across all test instances and maintaining an optimality gap of no more than 0.1\%. Notably, the same hyperparameter set is utilized across all experiments, underscoring 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#29992;&#20110;&#33258;&#20027;&#36710;&#36742;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#36830;&#32493;&#35270;&#35282;&#20915;&#31574;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20248;&#31168;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#35813;&#31574;&#30053;&#20174;&#22810;&#20010;&#35282;&#24230;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#31867;&#20284;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#22312;&#32447;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2008.11852</link><description>&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#39640;&#36895;&#20844;&#36335;&#20915;&#31574;&#65306;&#36830;&#32493;&#21160;&#20316;&#35270;&#35282;&#19979;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decision-making for Autonomous Vehicles on Highway: Deep Reinforcement Learning with Continuous Action Horizon. (arXiv:2008.11852v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.11852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#29992;&#20110;&#33258;&#20027;&#36710;&#36742;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#36830;&#32493;&#35270;&#35282;&#20915;&#31574;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20248;&#31168;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#35813;&#31574;&#30053;&#20174;&#22810;&#20010;&#35282;&#24230;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#31867;&#20284;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#22312;&#32447;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#30340;&#20915;&#31574;&#31574;&#30053;&#25551;&#36848;&#20102;&#19968;&#31995;&#21015;&#30340;&#34892;&#39542;&#21160;&#20316;&#26469;&#23454;&#29616;&#29305;&#23450;&#30340;&#23548;&#33322;&#20219;&#21153;&#12290;&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#39640;&#36895;&#20844;&#36335;&#19978;&#36830;&#32493;&#35270;&#35282;&#20915;&#31574;&#38382;&#39064;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#36710;&#36742;&#36816;&#21160;&#23398;&#21644;&#39640;&#36895;&#20844;&#36335;&#39550;&#39542;&#22330;&#26223;&#12290;&#33258;&#21160;&#36710;&#36742;&#30340;&#36816;&#34892;&#30446;&#26631;&#26159;&#20197;&#39640;&#25928;&#19988;&#24179;&#31283;&#30340;&#31574;&#30053;&#25191;&#34892;&#32780;&#19981;&#21457;&#29983;&#30896;&#25758;&#12290;&#28982;&#21518;&#20171;&#32461;&#20102;&#21517;&#20026;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#22686;&#24378;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#20811;&#26381;&#35757;&#32451;&#25928;&#29575;&#20302;&#21644;&#26679;&#26412;&#25928;&#29575;&#20302;&#30340;&#25361;&#25112;&#65292;&#36825;&#20010;&#24212;&#29992;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20248;&#31168;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#20174;&#26368;&#20248;&#24615;&#12289;&#23398;&#20064;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#31561;&#22810;&#20010;&#35282;&#24230;&#23545;&#22522;&#20110;PPO-DRL&#30340;&#20915;&#31574;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#31867;&#20284;&#30340;&#39550;&#39542;&#22330;&#26223;&#65292;&#35752;&#35770;&#20102;&#20854;&#22312;&#32447;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-making strategy for autonomous vehicles de-scribes a sequence of driving maneuvers to achieve a certain navigational mission. This paper utilizes the deep reinforcement learning (DRL) method to address the continuous-horizon decision-making problem on the highway. First, the vehicle kinematics and driving scenario on the freeway are introduced. The running objective of the ego automated vehicle is to execute an efficient and smooth policy without collision. Then, the particular algorithm named proximal policy optimization (PPO)-enhanced DRL is illustrated. To overcome the challenges in tardy training efficiency and sample inefficiency, this applied algorithm could realize high learning efficiency and excellent control performance. Finally, the PPO-DRL-based decision-making strategy is estimated from multiple perspectives, including the optimality, learning efficiency, and adaptability. Its potential for online application is discussed by applying it to similar driving scenario
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33041;&#21147;&#39118;&#26292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGAN&#65289;&#26550;&#26500;&#65292;&#23454;&#29616;&#22810;&#20010;&#20195;&#29702;&#22312;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#26041;&#24335;&#19979;&#29983;&#25104;&#31867;&#20284;&#30495;&#23454;&#25968;&#25454;&#30340;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#22810;&#20010;&#20195;&#29702;&#20849;&#20139;&#26377;&#38480;&#19988;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2002.00306</link><description>&lt;p&gt;
&#22522;&#20110;&#33041;&#21147;&#39118;&#26292;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGAN&#65289;&#65306;&#38754;&#21521;&#20855;&#26377;&#20998;&#24067;&#24335;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#22810;&#20195;&#29702;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brainstorming Generative Adversarial Networks (BGANs): Towards Multi-Agent Generative Models with Distributed Private Datasets. (arXiv:2002.00306v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.00306
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33041;&#21147;&#39118;&#26292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGAN&#65289;&#26550;&#26500;&#65292;&#23454;&#29616;&#22810;&#20010;&#20195;&#29702;&#22312;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#26041;&#24335;&#19979;&#29983;&#25104;&#31867;&#20284;&#30495;&#23454;&#25968;&#25454;&#30340;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#22810;&#20010;&#20195;&#29702;&#20849;&#20139;&#26377;&#38480;&#19988;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#23454;&#29616;&#39640;&#23398;&#20064;&#20934;&#30830;&#24615;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24517;&#39035;&#20197;&#20805;&#20998;&#20195;&#34920;&#25968;&#25454;&#31354;&#38388;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#20316;&#20026;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#26159;&#26377;&#38480;&#30340;&#65292;&#24182;&#19988;&#20998;&#24067;&#22312;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#35797;&#22270;&#21333;&#29420;&#23398;&#20064;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#36890;&#24120;&#19981;&#24895;&#20849;&#20139;&#20182;&#20204;&#30340;&#26412;&#22320;&#25968;&#25454;&#65292;&#22240;&#20026;&#36825;&#21487;&#33021;&#23548;&#33268;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33041;&#21147;&#39118;&#26292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGAN&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#22810;&#20195;&#29702;GAN&#38382;&#39064;&#65292;&#22810;&#20010;&#20195;&#29702;&#21487;&#20197;&#22312;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#26041;&#24335;&#19979;&#29983;&#25104;&#31867;&#20284;&#30495;&#23454;&#25968;&#25454;&#30340;&#26679;&#26412;&#12290;BGAN&#20801;&#35768;&#20195;&#29702;&#36890;&#36807;&#20849;&#20139;&#29983;&#25104;&#30340;&#25968;&#25454;&#26679;&#26412;&#32780;&#19981;&#26159;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#8220;&#33041;&#21147;&#39118;&#26292;&#8221;&#26469;&#33719;&#21462;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#30340;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#30340;&#20998;&#24067;&#24335;GAN&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;BGAN&#26550;&#26500;&#34987;&#35774;&#35745;&#20026;&#23436;&#20840;&#20998;&#24067;&#24335;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#21442;&#19982;&#26041;&#24444;&#27492;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
To achieve a high learning accuracy, generative adversarial networks (GANs) must be fed by large datasets that adequately represent the data space. However, in many scenarios, the available datasets may be limited and distributed across multiple agents, each of which is seeking to learn the distribution of the data on its own. In such scenarios, the agents often do not wish to share their local data as it can cause communication overhead for large datasets. In this paper, to address this multi-agent GAN problem, a novel brainstorming GAN (BGAN) architecture is proposed using which multiple agents can generate real-like data samples while operating in a fully distributed manner. BGAN allows the agents to gain information from other agents without sharing their real datasets but by ``brainstorming'' via the sharing of their generated data samples. In contrast to existing distributed GAN solutions, the proposed BGAN architecture is designed to be fully distributed, and it does not need an
&lt;/p&gt;</description></item></channel></rss>