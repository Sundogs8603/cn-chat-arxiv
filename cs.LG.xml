<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#20808;&#20351;&#29992;&#21333;&#35843;&#38750;&#22686;&#20989;&#25968;&#36827;&#34892;&#26102;&#38388;&#27493;&#37319;&#26679;&#30340;NeRF&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;NeRF&#20248;&#21270;&#36807;&#31243;&#21644;&#24471;&#20998;&#33976;&#39311;&#20013;&#22343;&#21248;&#26102;&#38388;&#27493;&#37319;&#26679;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25991;&#26412;&#21040;3D&#20869;&#23481;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12422</link><description>&lt;p&gt;
DreamTime: &#19968;&#31181;&#25913;&#36827;&#30340;&#25991;&#26412;&#21040;3D&#20869;&#23481;&#21019;&#20316;&#20248;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation. (arXiv:2306.12422v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#20808;&#20351;&#29992;&#21333;&#35843;&#38750;&#22686;&#20989;&#25968;&#36827;&#34892;&#26102;&#38388;&#27493;&#37319;&#26679;&#30340;NeRF&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;NeRF&#20248;&#21270;&#36807;&#31243;&#21644;&#24471;&#20998;&#33976;&#39311;&#20013;&#22343;&#21248;&#26102;&#38388;&#27493;&#37319;&#26679;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25991;&#26412;&#21040;3D&#20869;&#23481;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#39044;&#20808;&#35757;&#32451;&#20102;&#25968;&#21313;&#20159;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#24471;&#20998;&#33976;&#39311;&#26469;&#20248;&#21270;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;3D&#20869;&#23481;&#30340;&#21019;&#24314;&#12290; &#28982;&#32780;&#65292;&#25152;&#24471;&#21040;&#30340;3D&#27169;&#22411;&#23384;&#22312;&#20004;&#20010;&#23616;&#38480;&#24615;&#65306;&#65288;a&#65289;&#36136;&#37327;&#38382;&#39064;&#65292;&#20363;&#22914;&#39281;&#21644;&#30340;&#39068;&#33394;&#21644;Janus&#38382;&#39064;&#65307;&#65288;b&#65289;&#19982;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#21512;&#25104;&#30456;&#27604;&#65292;&#26497;&#20302;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;NeRF&#20248;&#21270;&#36807;&#31243;&#21644;&#24471;&#20998;&#33976;&#39311;&#20013;&#22343;&#21248;&#26102;&#38388;&#27493;&#37319;&#26679;&#20043;&#38388;&#30340;&#20914;&#31361;&#26159;&#36825;&#20123;&#23616;&#38480;&#24615;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#20914;&#31361;&#65292;&#25105;&#20204;&#24314;&#35758;&#20248;&#20808;&#20351;&#29992;&#21333;&#35843;&#38750;&#22686;&#20989;&#25968;&#36827;&#34892;&#26102;&#38388;&#27493;&#37319;&#26679;&#65292;&#36825;&#20351;&#24471;NeRF&#20248;&#21270;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#36807;&#31243;&#30456;&#19968;&#33268;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#37325;&#35774;&#35745;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25991;&#26412;&#21040;3D&#20869;&#23481;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models pre-trained on billions of image-text pairs have recently enabled text-to-3D content creation by optimizing a randomly initialized Neural Radiance Fields (NeRF) with score distillation. However, the resultant 3D models exhibit two limitations: (a) quality concerns such as saturated color and the Janus problem; (b) extremely low diversity comparing to text-guided image synthesis. In this paper, we show that the conflict between NeRF optimization process and uniform timestep sampling in score distillation is the main reason for these limitations. To resolve this conflict, we propose to prioritize timestep sampling with monotonically non-increasing functions, which aligns NeRF optimization with the sampling process of diffusion model. Extensive experiments show that our simple redesign significantly improves text-to-3D content creation with higher quality and diversity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#36870;&#38382;&#39064;&#21644;&#25511;&#21046;&#20013;&#30896;&#25758;&#21709;&#24212;&#20989;&#25968;&#38454;&#36291;&#24418;&#25104;&#30340;&#19981;&#36830;&#32493;&#26681;&#26597;&#25214;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#24179;&#28369;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.12413</link><description>&lt;p&gt;
&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#12289;&#36870;&#38382;&#39064;&#21644;&#25511;&#21046;&#20013;&#19981;&#36830;&#32493;&#30340;&#26681;&#26597;&#25214;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Discontinuous Root-Finding for Subsequent Differentiability in Machine Learning, Inverse Problems, and Control. (arXiv:2306.12413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#36870;&#38382;&#39064;&#21644;&#25511;&#21046;&#20013;&#30896;&#25758;&#21709;&#24212;&#20989;&#25968;&#38454;&#36291;&#24418;&#25104;&#30340;&#19981;&#36830;&#32493;&#26681;&#26597;&#25214;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#24179;&#28369;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29289;&#29702;&#36807;&#31243;&#30340;&#25968;&#23398;&#20844;&#24335;&#20855;&#26377;&#22266;&#26377;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#20363;&#22914;&#21018;&#20307;&#25110;&#21487;&#21464;&#24418;&#20307;&#20043;&#38388;&#30340;&#30896;&#25758;&#12290;&#22240;&#20026;&#30896;&#25758;&#30340;&#38454;&#36291;&#21709;&#24212;&#22312;&#35745;&#31639;&#26102;&#38656;&#35201;&#23545;&#21442;&#25968;&#27714;&#23548;&#25968;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#36870;&#38382;&#39064;&#21644;&#25511;&#21046;&#39046;&#22495;&#30340;&#25968;&#20540;&#35745;&#31639;&#20013;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#29702;&#35770;&#19978;&#21644;&#25968;&#20540;&#19978;&#35777;&#26126;&#20102;&#30896;&#25758;&#26102;&#38388;&#23545;&#21442;&#25968;&#27714;&#23548;&#25968;&#22312;&#36235;&#36817;&#20110;&#20998;&#31163;&#30896;&#25758;&#29366;&#24577;&#21644;&#26410;&#30896;&#25758;&#29366;&#24577;&#30340;&#30028;&#38754;&#26102;&#20250;&#21464;&#24471;&#26080;&#31351;&#22823;&#65292;&#24182;&#20351;&#29992;&#22797;&#21270;&#25216;&#26415;&#26469;&#22788;&#29702;&#20998;&#30028;&#38754;&#19978;&#30340;&#26497;&#38480;&#20540;&#12290;&#25105;&#20204;&#21516;&#26102;&#20462;&#27491;&#20102;&#23548;&#25968;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#20174;&#32780;&#20351;&#24471;&#35745;&#31639;&#26356;&#21152;&#24179;&#28369;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are many physical processes that have inherent discontinuities in their mathematical formulations. This paper is motivated by the specific case of collisions between two rigid or deformable bodies and the intrinsic nature of that discontinuity. The impulse response to a collision is discontinuous with the lack of any response when no collision occurs, which causes difficulties for numerical approaches that require differentiability which are typical in machine learning, inverse problems, and control. We theoretically and numerically demonstrate that the derivative of the collision time with respect to the parameters becomes infinite as one approaches the barrier separating colliding from not colliding, and use lifting to complexify the solution space so that solutions on the other side of the barrier are directly attainable as precise values. Subsequently, we mollify the barrier posed by the unbounded derivatives, so that one can tunnel back and forth in a smooth and reliable fas
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#24322;&#27493;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#35774;&#23450;&#19979;&#22914;&#20309;&#22312;&#20445;&#35777;&#23458;&#25143;&#31471;&#26102;&#25928;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#65292;&#30740;&#31350;&#34920;&#26126;&#23545;&#20110;&#31995;&#32479;&#20013;&#33410;&#28857;&#36739;&#23569;&#30340;&#24773;&#20917;&#65292;&#21482;&#38656;&#35201;&#23558;&#33410;&#28857;&#21010;&#20998;&#20026;&#23569;&#37327;&#30340;&#38598;&#32676;&#20415;&#21487;&#26377;&#38480;&#26102;&#38388;&#20869;&#23454;&#29616;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2306.12400</link><description>&lt;p&gt;
&#21450;&#26102;&#24322;&#27493;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65306;&#25910;&#25947;&#26102;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Timely Asynchronous Hierarchical Federated Learning: Age of Convergence. (arXiv:2306.12400v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12400
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#24322;&#27493;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#35774;&#23450;&#19979;&#22914;&#20309;&#22312;&#20445;&#35777;&#23458;&#25143;&#31471;&#26102;&#25928;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#65292;&#30740;&#31350;&#34920;&#26126;&#23545;&#20110;&#31995;&#32479;&#20013;&#33410;&#28857;&#36739;&#23569;&#30340;&#24773;&#20917;&#65292;&#21482;&#38656;&#35201;&#23558;&#33410;&#28857;&#21010;&#20998;&#20026;&#23569;&#37327;&#30340;&#38598;&#32676;&#20415;&#21487;&#26377;&#38480;&#26102;&#38388;&#20869;&#23454;&#29616;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#23458;&#25143;&#31471;-&#36793;&#32536;-&#20113;&#26550;&#26500;&#19979;&#30340;&#24322;&#27493;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;AHFL&#65289;&#35774;&#32622;&#12290;&#23458;&#25143;&#31471;&#19982;&#30456;&#24212;&#30340;&#36793;&#32536;&#26381;&#21153;&#22120;&#20132;&#25442;&#35757;&#32451;&#21442;&#25968;&#65292;&#28982;&#21518;&#26356;&#26032;&#26412;&#22320;&#32858;&#21512;&#30340;&#27169;&#22411;&#12290;&#27492;&#27169;&#22411;&#28982;&#21518;&#20256;&#36755;&#32473;&#26412;&#22320;&#38598;&#32676;&#20013;&#30340;&#25152;&#26377;&#23458;&#25143;&#31471;&#12290;&#36793;&#32536;&#26381;&#21153;&#22120;&#19982;&#20013;&#22830;&#20113;&#26381;&#21153;&#22120;&#36890;&#20449;&#20197;&#36827;&#34892;&#20840;&#23616;&#27169;&#22411;&#27719;&#38598;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#30446;&#26631;&#26159;&#25910;&#25947;&#20110;&#20840;&#23616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#23458;&#25143;&#31471;&#30340;&#26102;&#25928;&#24615;&#65292;&#21363;&#20855;&#26377;&#26368;&#20339;&#30340;&#35757;&#32451;&#36845;&#20195;&#26102;&#38388;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#23494;&#38598;&#38598;&#32676;&#30340;&#36825;&#31181;&#31995;&#32479;&#30340;&#25910;&#25947;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22914;&#26524;&#23558;&#33410;&#28857;&#21010;&#20998;&#20026;$O(1)$&#25968;&#37327;&#30340;&#38598;&#32676;&#65292;&#21363;&#23558;&#31995;&#32479;&#26500;&#24314;&#20026;&#27599;&#20010;&#23494;&#38598;&#23458;&#25143;&#31471;&#32676;&#20307;&#30340;&#31232;&#30095;&#36793;&#32536;&#26381;&#21153;&#22120;&#38598;&#65292;&#37027;&#20040;&#23545;&#20110;&#20855;&#26377;&#22266;&#23450;&#24179;&#22343;&#26102;&#25928;&#24615;&#30340;$n$&#20010;&#23458;&#25143;&#30340;&#31995;&#32479;&#65292;&#26377;&#38480;&#26102;&#38388;&#20869;&#30340;&#25910;&#25947;&#34987;&#27010;&#29575;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider an asynchronous hierarchical federated learning (AHFL) setting with a client-edge-cloud framework. The clients exchange the trained parameters with their corresponding edge servers, which update the locally aggregated model. This model is then transmitted to all the clients in the local cluster. The edge servers communicate to the central cloud server for global model aggregation. The goal of each client is to converge to the global model, while maintaining timeliness of the clients, i.e., having optimum training iteration time. We investigate the convergence criteria for such a system with dense clusters. Our analysis shows that for a system of $n$ clients with fixed average timeliness, the convergence in finite time is probabilistically guaranteed, if the nodes are divided into $O(1)$ number of clusters, that is, if the system is built as a sparse set of edge servers with dense client bases each.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#21464;&#24418;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20174;&#21333;&#20010;&#28436;&#31034;&#20013;&#23398;&#20064;SE&#65288;3&#65289;&#26426;&#22120;&#20154;&#25805;&#32437;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12392</link><description>&lt;p&gt;
&#20132;&#20114;&#21464;&#24418;&#30340;&#19968;&#27425;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-shot Imitation Learning via Interaction Warping. (arXiv:2306.12392v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#21464;&#24418;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20174;&#21333;&#20010;&#28436;&#31034;&#20013;&#23398;&#20064;SE&#65288;3&#65289;&#26426;&#22120;&#20154;&#25805;&#32437;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#24335;&#24212;&#29992;&#20013;&#65292;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#23398;&#20064;&#26426;&#22120;&#20154;&#31574;&#30053;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20132;&#20114;&#21464;&#24418;&#65292;&#29992;&#20110;&#20174;&#21333;&#20010;&#28436;&#31034;&#20013;&#23398;&#20064;SE&#65288;3&#65289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#24418;&#29366;&#21464;&#24418;&#25216;&#26415;&#25512;&#26029;&#29615;&#22659;&#20013;&#27599;&#20010;&#29289;&#20307;&#30340;&#19977;&#32500;&#32593;&#26684;&#65292;&#24182;&#23558;&#25805;&#32437;&#21160;&#20316;&#34920;&#31034;&#20026;&#29289;&#20307;&#19978;&#30340;&#20851;&#38190;&#28857;&#65292;&#21487;&#20197;&#29992;&#29289;&#20307;&#30340;&#24418;&#29366;&#36827;&#34892;&#21464;&#24418;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#20013;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#19968;&#27425;&#24615;&#27169;&#20223;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37326;&#22806;&#39044;&#27979;&#29289;&#20307;&#32593;&#26684;&#21644;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning of robot policies from few demonstrations is crucial in open-ended applications. We propose a new method, Interaction Warping, for learning SE(3) robotic manipulation policies from a single demonstration. We infer the 3D mesh of each object in the environment using shape warping, a technique for aligning point clouds across object instances. Then, we represent manipulation actions as keypoints on objects, which can be warped with the shape of the object. We show successful one-shot imitation learning on three simulated and real-world object re-arrangement tasks. We also demonstrate the ability of our method to predict object meshes and robot grasps in the wild.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#24377;&#24615;&#20840;&#27874;&#24418;&#21453;&#28436;&#35774;&#35745;&#30340;&#20840;&#38754;&#22522;&#20934;&#25968;&#25454;&#38598;$\mathbf{\mathbb{E}^{FWI}}$&#65292;&#20854;&#21253;&#25324;8&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12386</link><description>&lt;p&gt;
$\mathbf{\mathbb{E}^{FWI}}$:&#22810;&#21442;&#25968;&#22320;&#29699;&#29289;&#29702;&#23646;&#24615;&#24377;&#24615;&#20840;&#27874;&#24418;&#21453;&#28436;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
$\mathbf{\mathbb{E}^{FWI}}$: Multi-parameter Benchmark Datasets for Elastic Full Waveform Inversion of Geophysical Properties. (arXiv:2306.12386v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#24377;&#24615;&#20840;&#27874;&#24418;&#21453;&#28436;&#35774;&#35745;&#30340;&#20840;&#38754;&#22522;&#20934;&#25968;&#25454;&#38598;$\mathbf{\mathbb{E}^{FWI}}$&#65292;&#20854;&#21253;&#25324;8&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#22320;&#29699;&#29289;&#29702;&#23646;&#24615;&#65288;&#22914;P&#21644;S&#27874;&#36895;&#24230;&#65289;&#22312;CO$_2$&#23553;&#23384;&#21644;&#33021;&#28304;&#21208;&#25506;&#65288;&#22914;&#27682;&#21644;&#22320;&#28909;&#33021;&#65289;&#31561;&#21508;&#31181;&#22320;&#19979;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#24377;&#24615;&#20840;&#27874;&#24418;&#21453;&#28436;(FWI)&#24191;&#27867;&#24212;&#29992;&#20110;&#34920;&#24449;&#20648;&#23618;&#23646;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;$\mathbf{\mathbb{E}^{FWI}}$&#65292;&#19968;&#20010;&#19987;&#38376;&#20026;&#24377;&#24615;FWI&#35774;&#35745;&#30340;&#20840;&#38754;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;$\mathbf{\mathbb{E}^{FWI}}$&#21253;&#25324;8&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#21508;&#31181;&#19981;&#21516;&#30340;&#22320;&#19979;&#22320;&#36136;&#32467;&#26500;&#65288;&#24179;&#22374;&#12289;&#26354;&#32447;&#12289;&#26029;&#23618;&#31561;&#65289;&#12290;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#22522;&#20934;&#32467;&#26524;&#12290;&#19982;&#25105;&#20204;&#20808;&#21069;&#25552;&#20379;&#30340;&#22768;&#23398;FWI&#21387;&#21147;&#35760;&#24405;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;OpenFWI&#65289;&#30456;&#27604;&#65292;$\mathbf{\mathbb{E}^{FWI}}$&#30340;&#22320;&#38663;&#25968;&#25454;&#38598;&#20855;&#26377;&#22402;&#30452;&#21644;&#27700;&#24179;&#20998;&#37327;&#12290;&#27492;&#22806;&#65292;$\mathbf{\mathbb{E}^{FWI}}$&#20013;&#30340;&#36895;&#24230;&#22270;&#21253;&#21547;&#20102;P&#21644;S&#27874;&#36895;&#24230;&#12290;&#34429;&#28982;&#22810;&#20998;&#37327;FWI&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#65292;&#20294;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#24377;&#24615;FWI&#30340;&#22810;&#21442;&#25968;&#21453;&#28436;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35838;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Elastic geophysical properties (such as P- and S-wave velocities) are of great importance to various subsurface applications like CO$_2$ sequestration and energy exploration (e.g., hydrogen and geothermal). Elastic full waveform inversion (FWI) is widely applied for characterizing reservoir properties. In this paper, we introduce $\mathbf{\mathbb{E}^{FWI}}$, a comprehensive benchmark dataset that is specifically designed for elastic FWI. $\mathbf{\mathbb{E}^{FWI}}$ encompasses 8 distinct datasets that cover diverse subsurface geologic structures (flat, curve, faults, etc). The benchmark results produced by three different deep learning methods are provided. In contrast to our previously presented dataset (pressure recordings) for acoustic FWI (referred to as OpenFWI), the seismic dataset in $\mathbf{\mathbb{E}^{FWI}}$ has both vertical and horizontal components. Moreover, the velocity maps in $\mathbf{\mathbb{E}^{FWI}}$ incorporate both Pand S-wave velocities. While the multicomponen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#37319;&#29992;Transformer&#32593;&#32476;&#23545;&#27700;&#25991;&#39044;&#27979;&#38382;&#39064;&#30340;&#24314;&#27169;&#25928;&#26524;&#65292;&#21457;&#29616;&#30456;&#27604;LSTM&#27169;&#22411;&#32570;&#20047;&#20248;&#21183;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#27700;&#25991;&#39044;&#27979;&#38382;&#39064;&#30340;&#39532;&#23572;&#21487;&#22827;&#29305;&#24615;&#25152;&#23548;&#33268;&#12290;</title><link>http://arxiv.org/abs/2306.12384</link><description>&lt;p&gt;
&#29992;Transformer&#32593;&#32476;&#25506;&#31350;&#27700;&#25991;&#39044;&#27979;&#30340;&#26497;&#38480;&#65288;arXiv:2306.12384v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Probing the limit of hydrologic predictability with the Transformer network. (arXiv:2306.12384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#37319;&#29992;Transformer&#32593;&#32476;&#23545;&#27700;&#25991;&#39044;&#27979;&#38382;&#39064;&#30340;&#24314;&#27169;&#25928;&#26524;&#65292;&#21457;&#29616;&#30456;&#27604;LSTM&#27169;&#22411;&#32570;&#20047;&#20248;&#21183;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#27700;&#25991;&#39044;&#27979;&#38382;&#39064;&#30340;&#39532;&#23572;&#21487;&#22827;&#29305;&#24615;&#25152;&#23548;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20854;&#24341;&#20837;&#27700;&#25991;&#23398;&#39046;&#22495;&#20197;&#26469;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;&#22914;LSTM&#65289;&#22312;&#24050;&#30693;&#30340;&#21487;&#27604;&#22522;&#20934;&#19978;&#30340;&#26085;&#27700;&#25991;&#22270;&#34920;&#25351;&#26631;&#26041;&#38754;&#19968;&#30452;&#35777;&#26126;&#38590;&#20197;&#36229;&#36234;&#12290;&#38500;&#27700;&#25991;&#23398;&#22806;&#65292;Transformer&#29616;&#22312;&#24050;&#25104;&#20026;&#36830;&#32493;&#39044;&#27979;&#20219;&#21153;&#30340;&#39318;&#36873;&#27169;&#22411;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#26550;&#26500;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#39321;&#33609;Transformer&#26550;&#26500;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CAMELS&#25968;&#25454;&#38598;&#19978;&#19982;LSTM&#30456;&#27604;&#27627;&#26080;&#31454;&#20105;&#21147;&#65292;&#30001;&#20110;&#30701;&#26399;&#36807;&#31243;&#32780;&#29305;&#21035;&#28382;&#21518;&#20110;&#39640;&#27969;&#37327;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#19981;&#38656;&#35201;&#36882;&#24402;&#30340;&#21464;&#20307;Transformer&#21487;&#20197;&#33719;&#24471;&#19982;LSTM&#30456;&#28151;&#21512;&#30340;&#27604;&#36739;&#65292;&#20135;&#29983;&#30456;&#21516;&#30340;Kling-Gupta&#25928;&#29575;&#31995;&#25968;&#65288;KGE&#65289;&#20197;&#21450;&#20854;&#20182;&#25351;&#26631;&#12290;Transformer&#27809;&#26377;&#20248;&#21183;&#19982;&#27700;&#25991;&#39044;&#27979;&#38382;&#39064;&#30340;&#39532;&#23572;&#21487;&#22827;&#29305;&#24615;&#26377;&#20851;&#12290;&#19982;LSTM&#31867;&#20284;&#65292;Transformer&#20063;&#21487;&#20197;&#21512;&#24182;&#22810;&#20010;&#24378;&#21046;&#25968;&#25454;&#38598;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#23558;&#27700;&#25991;&#39044;&#27979;&#38382;&#39064;&#30340;&#20851;&#38190;&#29305;&#24449;&#36716;&#25442;&#20026;&#26356;&#25551;&#36848;&#24615;&#30340;&#31354;&#38388;&#65292;Transformer&#27169;&#22411;&#20284;&#20046;&#26080;&#27861;&#36229;&#36234;LSTM&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a number of years since its introduction to hydrology, recurrent neural networks like long short-term memory (LSTM) have proven remarkably difficult to surpass in terms of daily hydrograph metrics on known, comparable benchmarks. Outside of hydrology, Transformers have now become the model of choice for sequential prediction tasks, making it a curious architecture to investigate. Here, we first show that a vanilla Transformer architecture is not competitive against LSTM on the widely benchmarked CAMELS dataset, and lagged especially for the high-flow metrics due to short-term processes. However, a recurrence-free variant of Transformer can obtain mixed comparisons with LSTM, producing the same Kling-Gupta efficiency coefficient (KGE), along with other metrics. The lack of advantages for the Transformer is linked to the Markovian nature of the hydrologic prediction problem. Similar to LSTM, the Transformer can also merge multiple forcing dataset to improve model performance. While t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38543;&#26426;&#38646;&#38454;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20108;&#27425;&#22411;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#30340;&#26368;&#20248;Hessian&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20174;&#20449;&#24687;&#35770;&#35282;&#24230;&#25552;&#20379;Hessian&#30456;&#20851;&#22797;&#26434;&#24230;&#30340;&#19979;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;Hessian&#26080;&#20851;&#30340;&#31639;&#27861;&#21487;&#26222;&#36941;&#23454;&#29616;&#25152;&#26377;Hessian&#23454;&#20363;&#30340;&#28176;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.12383</link><description>&lt;p&gt;
&#20108;&#27425;&#22411;&#36172;&#33218;&#26426;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65306;Hessian&#30456;&#20851;&#24615;&#30028;&#38480;&#21644;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms. (arXiv:2306.12383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38543;&#26426;&#38646;&#38454;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20108;&#27425;&#22411;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#30340;&#26368;&#20248;Hessian&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20174;&#20449;&#24687;&#35770;&#35282;&#24230;&#25552;&#20379;Hessian&#30456;&#20851;&#22797;&#26434;&#24230;&#30340;&#19979;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;Hessian&#26080;&#20851;&#30340;&#31639;&#27861;&#21487;&#26222;&#36941;&#23454;&#29616;&#25152;&#26377;Hessian&#23454;&#20363;&#30340;&#28176;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38543;&#26426;&#38646;&#38454;&#20248;&#21270;&#20013;&#65292;&#20102;&#35299;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#26159;&#19968;&#20010;&#23454;&#38469;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#22522;&#26412;&#24773;&#20917;&#65292;&#21363;&#30446;&#26631;&#20989;&#25968;&#26159;&#20108;&#27425;&#22411;&#30340;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26368;&#20248;Hessian&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31532;&#19968;&#20010;&#32039;&#23494;&#21051;&#30011;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#20855;&#26377;&#21452;&#37325;&#24615;&#36136;&#12290;&#39318;&#20808;&#65292;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;&#33021;&#37327;&#20998;&#37197;&#30340;&#27010;&#24565;&#26469;&#25429;&#25417;&#25628;&#32034;&#31639;&#27861;&#21644;&#30446;&#26631;&#20989;&#25968;&#20960;&#20309;&#32467;&#26500;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#35777;&#26126;&#20102;Hessian&#30456;&#20851;&#22797;&#26434;&#24230;&#30340;&#32039;&#23494;&#19979;&#30028;&#12290;&#36890;&#36807;&#35299;&#20915;&#26368;&#20248;&#33021;&#37327;&#35889;&#65292;&#24471;&#21040;&#20102;&#37197;&#22871;&#30340;&#19978;&#38480;&#12290;&#20854;&#27425;&#65292;&#31639;&#27861;&#26041;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23384;&#22312;&#19968;&#31181;Hessian&#26080;&#20851;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#26222;&#36941;&#23454;&#29616;&#25152;&#26377;Hessian&#23454;&#20363;&#30340;&#28176;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#30340;&#28176;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#23545;&#20110;&#37325;&#23614;&#22122;&#22768;&#20998;&#24067;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In stochastic zeroth-order optimization, a problem of practical relevance is understanding how to fully exploit the local geometry of the underlying objective function. We consider a fundamental setting in which the objective function is quadratic, and provide the first tight characterization of the optimal Hessian-dependent sample complexity. Our contribution is twofold. First, from an information-theoretic point of view, we prove tight lower bounds on Hessian-dependent complexities by introducing a concept called energy allocation, which captures the interaction between the searching algorithm and the geometry of objective functions. A matching upper bound is obtained by solving the optimal energy spectrum. Then, algorithmically, we show the existence of a Hessian-independent algorithm that universally achieves the asymptotic optimal sample complexities for all Hessian instances. The optimal sample complexities achieved by our algorithm remain valid for heavy-tailed noise distributio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102; Gibbs &#31639;&#27861;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#29992;&#20110;&#35780;&#20272; GA &#27867;&#21270;&#33021;&#21147;&#30340;&#19981;&#21516;&#24615;&#33021;&#25351;&#26631;&#65292;&#20197;&#21450;&#35757;&#32451;&#35823;&#24046;&#21644;&#27979;&#35797;&#35823;&#24046;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2306.12380</link><description>&lt;p&gt;
&#20851;&#20110; Gibbs &#31639;&#27861;&#30340;&#39564;&#35777;&#65306;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#27979;&#35797;&#25968;&#25454;&#38598;&#21450;&#20854;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
On the Validation of Gibbs Algorithms: Training Datasets, Test Datasets and their Aggregation. (arXiv:2306.12380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102; Gibbs &#31639;&#27861;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#29992;&#20110;&#35780;&#20272; GA &#27867;&#21270;&#33021;&#21147;&#30340;&#19981;&#21516;&#24615;&#33021;&#25351;&#26631;&#65292;&#20197;&#21450;&#35757;&#32451;&#35823;&#24046;&#21644;&#27979;&#35797;&#35823;&#24046;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102; Gibbs &#31639;&#27861;&#65288;GA&#65289;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#65292;&#37319;&#29992;&#26399;&#26395;&#32463;&#39564;&#39118;&#38505;&#20316;&#20026;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#33719;&#24471;&#20102; GA &#22312;&#23553;&#38381;&#24418;&#24335;&#19979;&#30340;&#28789;&#25935;&#24230;&#12290;&#36890;&#36807;&#36816;&#29992;&#36825;&#20010;&#25551;&#36848;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#19981;&#21516;&#35757;&#32451;&#38598;&#35757;&#32451;&#30340; GA &#30340;&#27979;&#35797;&#35823;&#24046;&#21644;&#35757;&#32451;&#35823;&#24046;&#30456;&#20851;&#30340;&#26174;&#24335;&#34920;&#36798;&#24335;&#12290;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#65292;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#30340;&#32858;&#21512;&#65292;&#24182;&#20171;&#32461;&#20102;&#29992;&#20110;&#35780;&#20272;GA&#27867;&#21270;&#33021;&#21147;&#30340;&#19981;&#21516;&#24615;&#33021;&#25351;&#26631;&#12290;&#23545;&#20110;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#29305;&#23450;&#22823;&#23567;&#21644;GA&#30340;&#21442;&#25968;&#65292;&#24314;&#31435;&#20102; Jeffrey's divergence&#12289;&#35757;&#32451;&#35823;&#24046;&#21644;&#27979;&#35797;&#35823;&#24046;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dependence on training data of the Gibbs algorithm (GA) is analytically characterized. By adopting the expected empirical risk as the performance metric, the sensitivity of the GA is obtained in closed form. In this case, sensitivity is the performance difference with respect to an arbitrary alternative algorithm. This description enables the development of explicit expressions involving the training errors and test errors of GAs trained with different datasets. Using these tools, dataset aggregation is studied and different figures of merit to evaluate the generalization capabilities of GAs are introduced. For particular sizes of such datasets and parameters of the GAs, a connection between Jeffrey's divergence, training and test errors is established.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23545;&#20110;&#20960;&#20309;&#25968;&#25454;&#38598;$k$-&#26368;&#36817;&#37051;&#31639;&#27861;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#12290;&#36890;&#36807;&#24212;&#29992;&#22810;&#23610;&#24230;&#38543;&#26426;&#20998;&#21306;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;$n\cdot 2^{2^{O(d+k/\varepsilon)}}$&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#36924;&#36817;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#26368;&#20248;&#27745;&#26579;&#12290;</title><link>http://arxiv.org/abs/2306.12377</link><description>&lt;p&gt;
$k$-NN&#27745;&#26579;&#30340;&#20960;&#20309;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric Algorithms for $k$-NN Poisoning. (arXiv:2306.12377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23545;&#20110;&#20960;&#20309;&#25968;&#25454;&#38598;$k$-&#26368;&#36817;&#37051;&#31639;&#27861;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#12290;&#36890;&#36807;&#24212;&#29992;&#22810;&#23610;&#24230;&#38543;&#26426;&#20998;&#21306;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;$n\cdot 2^{2^{O(d+k/\varepsilon)}}$&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#36924;&#36817;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#26368;&#20248;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20960;&#20309;&#25968;&#25454;&#38598;$k$-&#26368;&#36817;&#37051;&#20998;&#31867;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;$n\cdot 2^{2^{O(d+k/\varepsilon)}}$&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#32473;&#23450;&#25968;&#25454;&#38598;$X \in \mathbb{R}^d$&#30340;$\varepsilon n$-&#21152;&#24615;&#36924;&#36817;&#26368;&#20248;&#27745;&#26579;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#24212;&#29992;&#22810;&#23610;&#24230;&#38543;&#26426;&#20998;&#21306;&#26469;&#23454;&#29616;&#20854;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a label poisoning attack on geometric data sets against $k$-nearest neighbor classification. We provide an algorithm that can compute an $\varepsilon n$-additive approximation of the optimal poisoning in $n\cdot 2^{2^{O(d+k/\varepsilon)}}$ time for a given data set $X \in \mathbb{R}^d$, where $|X| = n$. Our algorithm achieves its objectives through the application of multi-scale random partitions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#31639;&#27861;PriorBand&#65292;&#33021;&#22815;&#21516;&#26102;&#21033;&#29992;&#19987;&#23478;&#20449;&#24565;&#21644;&#24265;&#20215;&#30340;&#20195;&#29702;&#20219;&#21153;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.12370</link><description>&lt;p&gt;
PriorBand: &#28145;&#24230;&#23398;&#20064;&#19979;&#30340;&#23454;&#29992;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning. (arXiv:2306.12370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12370
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#31639;&#27861;PriorBand&#65292;&#33021;&#22815;&#21516;&#26102;&#21033;&#29992;&#19987;&#23478;&#20449;&#24565;&#21644;&#24265;&#20215;&#30340;&#20195;&#29702;&#20219;&#21153;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27969;&#31243;&#20013;&#30340;&#36229;&#21442;&#25968;&#23545;&#20854;&#19979;&#28216;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#20294;&#20854;&#20195;&#20215;&#24448;&#24448;&#23545;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#25163;&#21160;&#23454;&#39564;&#20173;&#26159;&#20248;&#21270;&#36229;&#21442;&#25968;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#30740;&#31350;&#20154;&#21592;&#30340;&#30452;&#35273;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#24265;&#20215;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;HPO&#31639;&#27861;&#21644;DL&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#36825;&#31181;&#19981;&#21305;&#37197;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PriorBand&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;DL&#37327;&#36523;&#23450;&#21046;&#30340;HPO&#31639;&#27861;&#65292;&#33021;&#22815;&#21033;&#29992;&#19987;&#23478;&#20449;&#24565;&#21644;&#24265;&#20215;&#30340;&#20195;&#29702;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;PriorBand&#22312;&#19968;&#31995;&#21015;DL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#25928;&#29575;&#65292;&#20197;&#21450;&#22312;&#25552;&#20379;&#26377;&#25928;&#19987;&#23478;&#36755;&#20837;&#21644;&#25239;&#20987;&#19981;&#33391;&#19987;&#23478;&#20449;&#24565;&#26041;&#38754;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameters of Deep Learning (DL) pipelines are crucial for their downstream performance. While a large number of methods for Hyperparameter Optimization (HPO) have been developed, their incurred costs are often untenable for modern DL. Consequently, manual experimentation is still the most prevalent approach to optimize hyperparameters, relying on the researcher's intuition, domain knowledge, and cheap preliminary explorations. To resolve this misalignment between HPO algorithms and DL researchers, we propose PriorBand, an HPO algorithm tailored to DL, able to utilize both expert beliefs and cheap proxy tasks. Empirically, we demonstrate PriorBand's efficiency across a range of DL benchmarks and show its gains under informative expert input and robustness against poor expert beliefs
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#21152;&#36895;MRI&#37325;&#24314;&#30340;&#27880;&#24847;&#21147;&#28151;&#21512;&#21464;&#20998;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;k-&#31354;&#38388;&#21644;&#22270;&#20687;&#22495;&#20013;&#30340;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;MRI&#37325;&#24314;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12365</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#21152;&#36895;MRI&#37325;&#24314;&#30340;&#27880;&#24847;&#21147;&#28151;&#21512;&#21464;&#20998;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Attention Hybrid Variational Net for Accelerated MRI Reconstruction. (arXiv:2306.12365v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#21152;&#36895;MRI&#37325;&#24314;&#30340;&#27880;&#24847;&#21147;&#28151;&#21512;&#21464;&#20998;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;k-&#31354;&#38388;&#21644;&#22270;&#20687;&#22495;&#20013;&#30340;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;MRI&#37325;&#24314;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#24863;&#30693;&#65288;CS&#65289;&#25968;&#25454;&#37325;&#24314;&#22312;&#21152;&#36895;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#21407;&#22240;&#26159;&#26469;&#33258;&#21152;&#36895;&#25513;&#27169;&#20013;&#22312;k-&#31354;&#38388;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#20351;&#24471;&#37325;&#24314;&#20986;&#31867;&#20284;&#20110;&#23436;&#20840;&#37319;&#26679;&#22270;&#20687;&#30340;&#22270;&#20687;&#21464;&#24471;&#22256;&#38590;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;k-&#31354;&#38388;&#21644;&#22270;&#20687;&#22495;&#20197;&#21450;&#20351;&#29992;&#23637;&#24320;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;MRI&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26500;&#30340;&#32570;&#28857;&#26159;&#23427;&#20204;&#24182;&#27809;&#26377;&#23436;&#20840;&#21033;&#29992;&#26469;&#33258;&#20004;&#20010;&#22495;&#65288;k-&#31354;&#38388;&#21644;&#22270;&#20687;&#65289;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#28151;&#21512;&#21464;&#20998;&#32593;&#32476;&#65292;&#23427;&#22312;k-&#31354;&#38388;&#21644;&#22270;&#20687;&#22495;&#20013;&#25191;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#30528;&#21517;&#30340;&#24320;&#28304;MRI&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#26426;&#26500;&#30340;&#19968;&#20010;&#35786;&#26029;&#20026;&#21330;&#20013;&#24739;&#32773;&#30340;&#20020;&#24202;MRI&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of compressed sensing (CS)-enabled data reconstruction for accelerating magnetic resonance imaging (MRI) remains a challenging problem. This is due to the fact that the information lost in k-space from the acceleration mask makes it difficult to reconstruct an image similar to the quality of a fully sampled image. Multiple deep learning-based structures have been proposed for MRI reconstruction using CS, both in the k-space and image domains as well as using unrolled optimization methods. However, the drawback of these structures is that they are not fully utilizing the information from both domains (k-space and image). Herein, we propose a deep learning-based attention hybrid variational network that performs learning in both the k-space and image domain. We evaluate our method on a well-known open-source MRI dataset and a clinical MRI dataset of patients diagnosed with strokes from our institution to demonstrate the performance of our network. In addition to quantitat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20551;&#35774;&#26410;&#30693;&#36755;&#20837;&#20026;&#32447;&#24615;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#20248;&#21270;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23545;&#26410;&#30693;&#36755;&#20837;&#30340;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512; sigma-point &#21464;&#25442;&#26041;&#26696;&#23558;&#29366;&#24577;&#21644;&#26410;&#30693;&#36755;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#20272;&#35745;&#20013;&#65292;&#30830;&#20445;&#20854;&#31283;&#23450;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#36866;&#29992;&#20110;&#35768;&#22810;&#26234;&#33021;&#33258;&#20027;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.12361</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#21270;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340; sigma-point &#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#19982;&#38750;&#32447;&#24615;&#26410;&#30693;&#36755;&#20837;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Sigma-point Kalman Filter with Nonlinear Unknown Input Estimation via Optimization and Data-driven Approach for Dynamic Systems. (arXiv:2306.12361v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20551;&#35774;&#26410;&#30693;&#36755;&#20837;&#20026;&#32447;&#24615;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#20248;&#21270;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23545;&#26410;&#30693;&#36755;&#20837;&#30340;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512; sigma-point &#21464;&#25442;&#26041;&#26696;&#23558;&#29366;&#24577;&#21644;&#26410;&#30693;&#36755;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#20272;&#35745;&#20013;&#65292;&#30830;&#20445;&#20854;&#31283;&#23450;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#36866;&#29992;&#20110;&#35768;&#22810;&#26234;&#33021;&#33258;&#20027;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#20851;&#20110;&#29366;&#24577;&#21644;&#26410;&#30693;&#36755;&#20837;(UI)&#20272;&#35745;&#30340;&#25991;&#29486;&#37117;&#35201;&#27714;UI&#26159;&#32447;&#24615;&#30340;&#65292;&#36825;&#20010;&#38480;&#21046;&#21487;&#33021;&#22826;&#20005;&#26684;&#20102;&#65292;&#22240;&#20026;&#23427;&#24182;&#19981;&#36866;&#29992;&#20110;&#35768;&#22810;&#26234;&#33021;&#33258;&#20027;&#31995;&#32479;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#23548;&#25968;&#26410;&#30693;&#36755;&#20837; Sigma-point &#21345;&#23572;&#26364;&#28388;&#27874;&#22120;(SPKE-nUI)&#65292;&#20854;&#20013; SPKF &#19982;&#26222;&#36890;&#38750;&#32447;&#24615; UI &#20272;&#35745;&#22120;&#30456;&#20114;&#36830;&#25509;&#65292;&#21487;&#20197;&#36890;&#36807;&#38750;&#32447;&#24615;&#20248;&#21270;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#23454;&#29616;&#12290;&#38750;&#32447;&#24615; UI &#20272;&#35745;&#22120;&#20351;&#29992;&#21518;&#39564;&#29366;&#24577;&#20272;&#35745;&#65292;&#36825;&#23545;&#29366;&#24577;&#39044;&#27979;&#35823;&#24046;&#19981;&#22826;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32852;&#21512; sigma-point &#21464;&#25442;&#26041;&#26696;&#65292;&#23558;&#29366;&#24577;&#21644; UI &#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837; SPKF-nUI &#30340;&#20272;&#35745;&#20013;&#12290;&#28145;&#20837;&#30340;&#38543;&#26426;&#31283;&#23450;&#24615;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#21512;&#29702;&#30340;&#20551;&#35774;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340; SPKF-nUI &#21487;&#20197;&#20135;&#29983;&#25351;&#25968;&#32423;&#25910;&#25947;&#30340;&#20272;&#35745;&#35823;&#24046;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22522;&#20110;&#27169;&#25311;&#30340;&#36335;&#38754;&#36710;&#36742;&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most works on joint state and unknown input (UI) estimation require the assumption that the UIs are linear; this is potentially restrictive as it does not hold in many intelligent autonomous systems. To overcome this restriction and circumvent the need to linearize the system, we propose a derivative-free Unknown Input Sigma-point Kalman Filter (SPKF-nUI) where the SPKF is interconnected with a general nonlinear UI estimator that can be implemented via nonlinear optimization and data-driven approaches. The nonlinear UI estimator uses the posterior state estimate which is less susceptible to state prediction error. In addition, we introduce a joint sigma-point transformation scheme to incorporate both the state and UI uncertainties in the estimation of SPKF-nUI. An in-depth stochastic stability analysis proves that the proposed SPKF-nUI yields exponentially converging estimation error bounds under reasonable assumptions. Finally, two case studies are carried out on a simulation-based ri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#34892;&#36208;&#36339;&#36291;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24179;&#28369;&#33021;&#37327;&#20989;&#25968;&#12289;&#20351;&#29992; Langevin Markov &#38142;&#33945;&#29305;&#21345;&#32599; (MCMC) &#31639;&#27861;&#21644;&#19968;&#27493;&#21435;&#22122;&#30340;&#25237;&#23556;&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#20915;&#31163;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#37319;&#26679;&#38590;&#39064;&#12290;&#21516;&#26102;&#22312;&#25239;&#20307;&#34507;&#30333;&#36136;&#30340;&#29983;&#25104;&#24314;&#27169;&#19978;&#36827;&#34892;&#20102;&#24212;&#29992;&#21644;&#27979;&#35797;&#65292;&#24182;&#24341;&#20837;&#20102;&#20998;&#24067;&#19968;&#33268;&#24615;&#24471;&#20998;&#20316;&#20026;&#22522;&#20934;&#27979;&#35797;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.12360</link><description>&lt;p&gt;
&#31163;&#25955;&#34892;&#36208;&#36339;&#36291;&#37319;&#26679;&#29992;&#20110;&#34507;&#30333;&#36136;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Protein Discovery with Discrete Walk-Jump Sampling. (arXiv:2306.12360v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#34892;&#36208;&#36339;&#36291;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24179;&#28369;&#33021;&#37327;&#20989;&#25968;&#12289;&#20351;&#29992; Langevin Markov &#38142;&#33945;&#29305;&#21345;&#32599; (MCMC) &#31639;&#27861;&#21644;&#19968;&#27493;&#21435;&#22122;&#30340;&#25237;&#23556;&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#20915;&#31163;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#37319;&#26679;&#38590;&#39064;&#12290;&#21516;&#26102;&#22312;&#25239;&#20307;&#34507;&#30333;&#36136;&#30340;&#29983;&#25104;&#24314;&#27169;&#19978;&#36827;&#34892;&#20102;&#24212;&#29992;&#21644;&#27979;&#35797;&#65292;&#24182;&#24341;&#20837;&#20102;&#20998;&#24067;&#19968;&#33268;&#24615;&#24471;&#20998;&#20316;&#20026;&#22522;&#20934;&#27979;&#35797;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#24179;&#28369;&#33021;&#37327;&#20989;&#25968;&#12289;&#20351;&#29992; Langevin Markov &#38142;&#33945;&#29305;&#21345;&#32599; (MCMC) &#20174;&#24179;&#28369;&#25968;&#25454;&#27969;&#24418;&#20013;&#37319;&#26679;&#65292;&#24182;&#20351;&#29992;&#19968;&#27493;&#21435;&#22122;&#25237;&#23556;&#22238;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#65292;&#35299;&#20915;&#20102;&#31163;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#22256;&#38590;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31163;&#25955;&#34892;&#36208;&#36339;&#36291;&#37319;&#26679;&#24418;&#24335;&#21270;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#21644;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#30340;&#25913;&#36827;&#26679;&#26412;&#36136;&#37327;&#65292;&#21516;&#26102;&#36890;&#36807;&#20165;&#38656;&#35201;&#19968;&#20010;&#22122;&#22768;&#27700;&#24179;&#26469;&#31616;&#21270;&#35757;&#32451;&#21644;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#25239;&#20307;&#34507;&#30333;&#36136;&#30340;&#29983;&#25104;&#24314;&#27169;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20998;&#24067;&#19968;&#33268;&#24615;&#24471;&#20998;&#20197;&#23545;&#34507;&#30333;&#36136;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#20248;&#21270;&#21644;&#37319;&#26679;&#25105;&#20204;&#30340;&#27169;&#22411;&#29992;&#20110;&#25552;&#35758;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#24471;&#20998;&#65292;97-100%&#30340;&#29983;&#25104;&#26679;&#21697;&#21487;&#20197;&#25104;&#21151;&#34920;&#36798;&#21644;&#32431;&#21270;&#65292;&#24182;&#19988;35%&#30340;&#21151;&#33021;&#35774;&#35745;&#22312;&#31532;&#19968;&#27425;&#23581;&#35797;&#20013;&#26174;&#31034;&#20986;&#19982;&#24050;&#30693;&#21151;&#33021;&#25239;&#20307;&#30456;&#21516;&#25110;&#26356;&#22909;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We resolve difficulties in training and sampling from a discrete generative model by learning a smoothed energy function, sampling from the smoothed data manifold with Langevin Markov chain Monte Carlo (MCMC), and projecting back to the true data manifold with one-step denoising. Our Discrete Walk-Jump Sampling formalism combines the maximum likelihood training of an energy-based model and improved sample quality of a score-based model, while simplifying training and sampling by requiring only a single noise level. We evaluate the robustness of our approach on generative modeling of antibody proteins and introduce the distributional conformity score to benchmark protein generative models. By optimizing and sampling from our models for the proposed distributional conformity score, 97-100% of generated samples are successfully expressed and purified and 35% of functional designs show equal or improved binding affinity compared to known functional antibodies on the first attempt in a sing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21487;&#35777;&#26126;&#39640;&#25928;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#30721;&#21644;$\gamma$-&#21487;&#35266;&#23519;&#20004;&#31181;&#31639;&#27861;&#65292;&#36798;&#21040;&#39640;&#25928;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.12356</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#39640;&#25928;&#30340;&#20302;&#31209;POMDP&#35268;&#21010;&#20013;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Representation Learning with Tractable Planning in Low-Rank POMDP. (arXiv:2306.12356v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21487;&#35777;&#26126;&#39640;&#25928;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#30721;&#21644;$\gamma$-&#21487;&#35266;&#23519;&#20004;&#31181;&#31639;&#27861;&#65292;&#36798;&#21040;&#39640;&#25928;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20854;&#20013;&#20195;&#29702;&#23398;&#20064;&#19968;&#20010;&#35299;&#30721;&#22120;&#20989;&#25968;&#65292;&#23558;&#19968;&#31995;&#21015;&#39640;&#32500;&#21407;&#22987;&#35266;&#23519;&#26144;&#23556;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#34920;&#31034;&#20013;&#65292;&#24182;&#29992;&#20110;&#26356;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#35268;&#21010;&#12290;&#25105;&#20204;&#20851;&#27880;\textit{ $\gamma$-&#21487;&#35266;&#23519;}&#21644; \textit{&#21487;&#35299;&#30721;POMDP}&#23376;&#31867;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#23376;&#31867;&#20013;&#65292;&#32479;&#35745;&#19978;&#21487;&#22788;&#29702;&#30340;&#23398;&#20064;&#26159;&#21487;&#33021;&#30340;&#65292;&#20294;&#23578;&#26410;&#26377;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#30721;POMDP&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#21644;&#19981;&#30830;&#23450;&#24615;&#20048;&#35266;&#24615;&#65288;OFU&#65289;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#21482;&#35843;&#29992;&#20102;&#30417;&#30563;&#23398;&#20064;&#35745;&#31639;&#31070;&#32463;&#20803;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#27492;&#31639;&#27861;&#35843;&#25972;&#20026;&#22312;&#26356;&#24191;&#27867;&#30340;$\gamma$-&#21487;&#35266;&#23519;POMDP&#31867;&#20013;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study representation learning in partially observable Markov Decision Processes (POMDPs), where the agent learns a decoder function that maps a series of high-dimensional raw observations to a compact representation and uses it for more efficient exploration and planning.  We focus our attention on the sub-classes of \textit{$\gamma$-observable} and \textit{decodable POMDPs}, for which it has been shown that statistically tractable learning is possible, but there has not been any computationally efficient algorithm. We first present an algorithm for decodable POMDPs that combines maximum likelihood estimation (MLE) and optimism in the face of uncertainty (OFU) to perform representation learning and achieve efficient sample complexity, while only calling supervised learning computational oracles. We then show how to adapt this algorithm to also work in the broader class of $\gamma$-observable POMDPs.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#37327;&#21333;&#20803;&#26522;&#20030;&#65288;ICE&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#31934;&#30830;&#35299;&#20915;&#23450;&#32500;&#24230;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.12344</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#35777;&#26126;&#31934;&#30830;&#30340;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An efficient, provably exact algorithm for the 0-1 loss linear classification problem. (arXiv:2306.12344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#37327;&#21333;&#20803;&#26522;&#20030;&#65288;ICE&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#31934;&#30830;&#35299;&#20915;&#23450;&#32500;&#24230;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#30340;&#31639;&#27861;&#20855;&#26377;&#24736;&#20037;&#30340;&#21382;&#21490;&#65292;&#33267;&#23569;&#21487;&#20197;&#36861;&#28335;&#21040;1936&#24180;&#30340;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#12290;&#23545;&#20110;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#65292;&#35768;&#22810;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24471;&#21040;&#30456;&#24212;&#30340;0-1&#25439;&#22833;&#20998;&#31867;&#38382;&#39064;&#30340;&#31934;&#30830;&#35299;&#65292;&#20294;&#23545;&#20110;&#38750;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#65292;&#24050;&#32463;&#35777;&#26126;&#36825;&#20010;&#38382;&#39064;&#22312;&#23436;&#20840;&#33539;&#22260;&#20869;&#26159;NP&#38590;&#30340;&#12290;&#25152;&#26377;&#26367;&#20195;&#26041;&#27861;&#37117;&#28041;&#21450;&#26576;&#31181;&#24418;&#24335;&#30340;&#36817;&#20284;&#65292;&#21253;&#25324;&#20351;&#29992;0-1&#25439;&#22833;&#30340;&#20195;&#29702;&#65288;&#20363;&#22914;hinge&#25110;logistic&#25439;&#22833;&#65289;&#25110;&#36817;&#20284;&#30340;&#32452;&#21512;&#25628;&#32034;&#65292;&#36825;&#20123;&#37117;&#19981;&#33021;&#20445;&#35777;&#23436;&#20840;&#35299;&#20915;&#38382;&#39064;&#12290;&#25214;&#21040;&#35299;&#20915;&#23450;&#32500;&#24230;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#26377;&#25928;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#31639;&#27861;&#30340;&#26500;&#24314;&#36807;&#31243;&#65292;&#22686;&#37327;&#21333;&#20803;&#26522;&#20030;&#65288;ICE&#65289;&#65292;&#23427;&#21487;&#20197;&#31934;&#30830;&#35299;&#20915;0-1&#25439;&#22833;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms for solving the linear classification problem have a long history, dating back at least to 1936 with linear discriminant analysis. For linearly separable data, many algorithms can obtain the exact solution to the corresponding 0-1 loss classification problem efficiently, but for data which is not linearly separable, it has been shown that this problem, in full generality, is NP-hard. Alternative approaches all involve approximations of some kind, including the use of surrogates for the 0-1 loss (for example, the hinge or logistic loss) or approximate combinatorial search, none of which can be guaranteed to solve the problem exactly. Finding efficient algorithms to obtain an exact i.e. globally optimal solution for the 0-1 loss linear classification problem with fixed dimension, remains an open problem. In research we report here, we detail the construction of a new algorithm, incremental cell enumeration (ICE), that can solve the 0-1 loss classification problem exactly in po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ProtoGate&#65292;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#26679;&#26412;&#20869;&#21644;&#26679;&#26412;&#38388;&#30340;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#26469;&#24341;&#20837;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#20197;&#20840;&#23616;&#21040;&#26412;&#22320;&#30340;&#26041;&#24335;&#36873;&#25321;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#24182;&#20351;&#39044;&#27979;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12330</link><description>&lt;p&gt;
ProtoGate&#65306;&#38754;&#21521;&#34920;&#26684;&#22411;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#21407;&#22411;&#31070;&#32463;&#32593;&#32476;&#19982;&#26412;&#22320;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
ProtoGate: Prototype-based Neural Networks with Local Feature Selection for Tabular Biomedical Data. (arXiv:2306.12330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ProtoGate&#65292;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#26679;&#26412;&#20869;&#21644;&#26679;&#26412;&#38388;&#30340;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#26469;&#24341;&#20837;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#20197;&#20840;&#23616;&#21040;&#26412;&#22320;&#30340;&#26041;&#24335;&#36873;&#25321;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#24182;&#20351;&#39044;&#27979;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#22411;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#24448;&#24448;&#26159;&#39640;&#32500;&#30340;&#65292;&#20294;&#26679;&#26412;&#37327;&#21448;&#30456;&#23545;&#36739;&#23567;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;&#36825;&#34920;&#26126;&#24403;&#21069;&#26041;&#27861;&#32570;&#20047;&#36866;&#24403;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#19981;&#33021;&#25429;&#25417;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#30340;&#20849;&#21516;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;ProtoGate&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#26679;&#26412;&#20869;&#21644;&#26679;&#26412;&#38388;&#30340;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#26469;&#24341;&#20837;&#24402;&#32435;&#20559;&#24046;&#12290;ProtoGate&#20197;&#20840;&#23616;&#21040;&#26412;&#22320;&#30340;&#26041;&#24335;&#36873;&#25321;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#29983;&#20135;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#65292;&#32780;&#21407;&#22411;&#27169;&#22411;&#20351;&#39044;&#27979;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#26469;&#35780;&#20272;ProtoGate&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#27169;&#24335;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#65292;&#21516;&#26102;&#21407;&#22411;&#36171;&#20104;&#20102;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular biomedical data poses challenges in machine learning because it is often high-dimensional and typically low-sample-size. Previous research has attempted to address these challenges via feature selection approaches, which can lead to unstable performance on real-world data. This suggests that current methods lack appropriate inductive biases that capture patterns common to different samples. In this paper, we propose ProtoGate, a prototype-based neural model that introduces an inductive bias by attending to both homogeneity and heterogeneity across samples. ProtoGate selects features in a global-to-local manner and leverages them to produce explainable predictions via an interpretable prototype-based model. We conduct comprehensive experiments to evaluate the performance of ProtoGate on synthetic and real-world datasets. Our results show that exploiting the homogeneous and heterogeneous patterns in the data can improve prediction accuracy while prototypes imbue interpretability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#21160;&#24314;&#35758;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#21152;&#35299;&#37322;&#24615;&#21644;&#32454;&#31890;&#24230;&#22320;&#20256;&#36755;&#30693;&#35782;&#65292;&#24182;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#21462;&#24471;&#27604;&#26631;&#20934;&#26435;&#37325;&#22797;&#21046;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12314</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#20869;&#30465;&#34892;&#21160;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Introspective Action Advising for Interpretable Transfer Learning. (arXiv:2306.12314v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#21160;&#24314;&#35758;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#21152;&#35299;&#37322;&#24615;&#21644;&#32454;&#31890;&#24230;&#22320;&#20256;&#36755;&#30693;&#35782;&#65292;&#24182;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#21462;&#24471;&#27604;&#26631;&#20934;&#26435;&#37325;&#22797;&#21046;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#24212;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#23558;&#20174;&#30456;&#20851;&#28304;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#65292;&#21152;&#36895;&#30446;&#26631;&#20219;&#21153;&#31574;&#30053;&#30340;&#35757;&#32451;&#12290;&#36890;&#24120;&#65292;&#36825;&#26159;&#36890;&#36807;&#22312;&#35757;&#32451;&#20043;&#21069;&#23558;&#39044;&#35757;&#32451;&#26435;&#37325;&#20174;&#28304;&#31574;&#30053;&#22797;&#21046;&#21040;&#30446;&#26631;&#31574;&#30053;&#26469;&#23454;&#29616;&#30340;&#65292;&#24182;&#19988;&#35201;&#27714;&#23427;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#19981;&#20165;&#38656;&#35201;&#22312;&#24191;&#27867;&#29366;&#24577;&#20998;&#24067;&#20013;&#23398;&#20064;&#21040;&#30340;&#24378;&#20581;&#34920;&#31034;&#8212;&#8212;&#36890;&#24120;&#38590;&#20197;&#22312;&#38024;&#23545;&#21333;&#20010;&#20219;&#21153;&#35757;&#32451;&#30340;&#19987;&#19994;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#8212;&#8212;&#32780;&#19988;&#24456;&#22823;&#31243;&#24230;&#19978;&#19981;&#21487;&#35299;&#37322;&#65292;&#25552;&#20379;&#24456;&#23569;&#26377;&#20851;&#36801;&#31227;&#30340;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#21160;&#24314;&#35758;&#30340;&#20219;&#21153;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#28304;&#20219;&#21153;&#20013;&#35757;&#32451;&#30340;&#25945;&#24072;&#31215;&#26497;&#25351;&#23548;&#30446;&#26631;&#20219;&#21153;&#20013;&#23398;&#29983;&#30340;&#25506;&#32034;&#12290;&#36890;&#36807;&#20869;&#30465;&#65292;&#25945;&#24072;&#33021;&#22815;&#30830;&#23450;&#20309;&#26102;&#26377;&#21033;&#20110;&#23398;&#29983;&#32473;&#20104;&#24314;&#35758;&#65292;&#22312;&#19981;&#30830;&#23450;&#26102;&#65292;&#20026;&#26356;&#21487;&#35299;&#37322;&#21644;&#32454;&#31890;&#24230;&#30340;&#30693;&#35782;&#20256;&#36755;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;Atari&#28216;&#25103;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#34920;&#26126;&#23427;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#32988;&#36807;&#26631;&#20934;&#30340;&#26435;&#37325;&#22797;&#21046;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#25552;&#20379;&#20102;&#26377;&#20851;&#20256;&#36755;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning can be applied in deep reinforcement learning to accelerate the training of a policy in a target task by transferring knowledge from a policy learned in a related source task. This is commonly achieved by copying pretrained weights from the source policy to the target policy prior to training, under the constraint that they use the same model architecture. However, not only does this require a robust representation learned over a wide distribution of states -- often failing to transfer between specialist models trained over single tasks -- but it is largely uninterpretable and provides little indication of what knowledge is transferred. In this work, we propose an alternative approach to transfer learning between tasks based on action advising, in which a teacher trained in a source task actively guides a student's exploration in a target task. Through introspection, the teacher is capable of identifying when advice is beneficial to the student and should be given, an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#23545;&#29616;&#20195;BDL&#31639;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#33021;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#31526;&#21495;&#30340;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.12306</link><description>&lt;p&gt;
&#36229;&#36234;&#28145;&#24230;&#38598;&#25104;&#8212;&#8212;&#22522;&#20110;&#20998;&#24067;&#20559;&#31227;&#19979;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Beyond Deep Ensembles -- A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift. (arXiv:2306.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#23545;&#29616;&#20195;BDL&#31639;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#33021;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#31526;&#21495;&#30340;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#26159;&#23454;&#29616;&#22312;&#20998;&#24067;&#20559;&#31227;&#25968;&#25454;&#19978;&#36827;&#34892;&#33391;&#22909;&#26657;&#20934;&#39044;&#27979;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#22823;&#35268;&#27169;&#35843;&#26597;&#65292;&#20197;&#31995;&#32479;&#26041;&#24335;&#35780;&#20272;&#26368;&#36817;&#30340; SOTA &#26041;&#27861;&#22312;&#22810;&#26679;&#12289;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#28165;&#26224;&#20102;&#35299;BDL&#30740;&#31350;&#30340;&#24403;&#21069;&#29366;&#20917;&#65292;&#25105;&#20204;&#22312;&#26469;&#33258;WILDS&#38598;&#21512;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#29616;&#20195;BDL&#31639;&#27861;&#65292;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26657;&#20934;&#33021;&#21147;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#22823;&#22411;&#65292;&#21367;&#31215;&#21644;&#22522;&#20110; transformer &#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#19978;&#30340;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#20010;&#24102;&#31526;&#21495;&#30340;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#29256;&#26412;&#65292;&#25581;&#31034;&#20986;&#26041;&#27861;&#26159;&#36807;&#24230;&#33258;&#20449;&#36824;&#26159;&#20302;&#25391;&#24133;&#65292;&#36827;&#19968;&#27493;&#28145;&#20837;&#30740;&#31350;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20102;&#39318;&#27425;&#31995;&#32479;&#35780;&#20272;BDL&#22312;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#34920;&#29616;&#65292;&#20570;&#20102;&#26356;&#22810;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian deep learning (BDL) is a promising approach to achieve well-calibrated predictions on distribution-shifted data. Nevertheless, there exists no large-scale survey that evaluates recent SOTA methods on diverse, realistic, and challenging benchmark tasks in a systematic manner. To provide a clear picture of the current state of BDL research, we evaluate modern BDL algorithms on real-world datasets from the WILDS collection containing challenging classification and regression tasks, with a focus on generalization capability and calibration under distribution shift. We compare the algorithms on a wide range of large, convolutional and transformer-based neural network architectures. In particular, we investigate a signed version of the expected calibration error that reveals whether the methods are over- or under-confident, providing further insight into the behavior of the methods. Further, we provide the first systematic evaluation of BDL for fine-tuning large pre-trained models, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#30340;&#32852;&#21512;&#26102;&#31354;&#27880;&#24847;&#21147;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;StarVQA+&#65292;&#36890;&#36807;&#23558;&#20027;&#35266;&#24179;&#22343;&#20998;&#25968;&#32534;&#30721;&#20026;&#27010;&#29575;&#21521;&#37327;&#24182;&#23884;&#20837;&#19968;&#20010;&#29305;&#27530;&#20196;&#29260;&#20316;&#20026;&#21487;&#23398;&#20064;&#21464;&#37327;&#35774;&#35745;&#20102;&#19968;&#20010;&#21521;&#37327;&#21270;&#30340;&#22238;&#24402;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#29992;&#22270;&#20687;&#21644;&#35270;&#39057;&#26469;&#32852;&#21512;&#35757;&#32451;&#31354;&#38388;&#21644;&#26102;&#38388;&#27880;&#24847;&#26435;&#37325;&#65292;&#22312;&#37326;&#22806;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;StarVQA+&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2306.12298</link><description>&lt;p&gt;
StarVQA+: &#38024;&#23545;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#30340;&#32852;&#21512;&#26102;&#31354;&#27880;&#24847;&#21147;&#21327;&#21516;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
StarVQA+: Co-training Space-Time Attention for Video Quality Assessment. (arXiv:2306.12298v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#30340;&#32852;&#21512;&#26102;&#31354;&#27880;&#24847;&#21147;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;StarVQA+&#65292;&#36890;&#36807;&#23558;&#20027;&#35266;&#24179;&#22343;&#20998;&#25968;&#32534;&#30721;&#20026;&#27010;&#29575;&#21521;&#37327;&#24182;&#23884;&#20837;&#19968;&#20010;&#29305;&#27530;&#20196;&#29260;&#20316;&#20026;&#21487;&#23398;&#20064;&#21464;&#37327;&#35774;&#35745;&#20102;&#19968;&#20010;&#21521;&#37327;&#21270;&#30340;&#22238;&#24402;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#29992;&#22270;&#20687;&#21644;&#35270;&#39057;&#26469;&#32852;&#21512;&#35757;&#32451;&#31354;&#38388;&#21644;&#26102;&#38388;&#27880;&#24847;&#26435;&#37325;&#65292;&#22312;&#37326;&#22806;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;StarVQA+&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Transformer&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;VQA&#65289;&#20013;&#30340;&#24212;&#29992;&#36804;&#20170;&#20026;&#27490;&#24182;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#35780;&#20272;&#37326;&#22806;&#35270;&#39057;&#30340;&#36136;&#37327;&#23545;&#21407;&#22987;&#21442;&#32771;&#21644;&#25293;&#25668;&#30072;&#21464;&#30340;&#26410;&#30693;&#24773;&#20917;&#34920;&#31034;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;VQA&#38382;&#39064;&#30340;&#32852;&#21512;&#26102;&#31354;&#27880;&#24847;&#32593;&#32476;&#65292;&#31216;&#20026;StarVQA+&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20132;&#26367;&#36830;&#25509;&#20998;&#21106;&#30340;&#26102;&#31354;&#27880;&#24847;&#21147;&#26469;&#26500;&#24314;StarVQA+&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20419;&#36827;StarVQA+&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20027;&#35266;&#24179;&#22343;&#20998;&#25968;&#65288;MOS&#65289;&#32534;&#30721;&#20026;&#27010;&#29575;&#21521;&#37327;&#24182;&#23884;&#20837;&#19968;&#20010;&#29305;&#27530;&#20196;&#29260;&#20316;&#20026;MOS&#30340;&#21487;&#23398;&#20064;&#21464;&#37327;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21521;&#37327;&#21270;&#30340;&#22238;&#24402;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25311;&#21512;&#20154;&#31867;&#35780;&#20998;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;Transformer&#30340;&#25968;&#25454;&#38656;&#27714;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#22270;&#20687;&#21644;&#35270;&#39057;&#26469;&#32852;&#21512;&#35757;&#32451;&#31354;&#38388;&#21644;&#26102;&#38388;&#27880;&#24847;&#26435;&#37325;&#12290;&#26412;&#25991;&#23545;&#19994;&#24050;&#20844;&#35748;&#30340;&#37326;&#22806;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;StarVQA+&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-attention based Transformer has achieved great success in many computer vision tasks. However, its application to video quality assessment (VQA) has not been satisfactory so far. Evaluating the quality of in-the-wild videos is challenging due to the unknown of pristine reference and shooting distortion. This paper presents a co-trained Space-Time Attention network for the VQA problem, termed StarVQA+. Specifically, we first build StarVQA+ by alternately concatenating the divided space-time attention. Then, to facilitate the training of StarVQA+, we design a vectorized regression loss by encoding the mean opinion score (MOS) to the probability vector and embedding a special token as the learnable variable of MOS, leading to better fitting of human's rating process. Finally, to solve the data hungry problem with Transformer, we propose to co-train the spatial and temporal attention weights using both images and videos. Various experiments are conducted on the de-facto in-the-wild vi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#21333;&#36890;&#36947;&#21435;&#28151;&#21709;&#26041;&#27861;&#65292;&#40065;&#26834;&#24615;&#26356;&#24378;&#65292;&#33021;&#26377;&#25928;&#22320;&#22788;&#29702;&#38750;&#24179;&#31283;&#22122;&#22768;&#65292;&#24182;&#22312;&#22823;&#28151;&#21709;&#26102;&#38388;&#19979;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12286</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#21333;&#36890;&#36947;&#21435;&#28151;&#21709;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diffusion Posterior Sampling for Informed Single-Channel Dereverberation. (arXiv:2306.12286v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#21333;&#36890;&#36947;&#21435;&#28151;&#21709;&#26041;&#27861;&#65292;&#40065;&#26834;&#24615;&#26356;&#24378;&#65292;&#33021;&#26377;&#25928;&#22320;&#22788;&#29702;&#38750;&#24179;&#31283;&#22122;&#22768;&#65292;&#24182;&#22312;&#22823;&#28151;&#21709;&#26102;&#38388;&#19979;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#21333;&#36890;&#36947;&#21435;&#28151;&#21709;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#25151;&#38388;&#20914;&#28608;&#21709;&#24212;&#30340;&#20102;&#35299;&#65292;&#21033;&#29992;&#27979;&#37327;&#19968;&#33268;&#24615;&#20934;&#21017;&#21644;&#20195;&#34920;&#28165;&#26224;&#35821;&#38899;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21453;&#21521;&#25193;&#25955;&#20135;&#29983;&#28040;&#28151;&#21709;&#35821;&#38899;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#21333;&#36890;&#36947;&#21435;&#28151;&#21709;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23545;&#27979;&#37327;&#22122;&#22768;&#26356;&#21152;&#40065;&#26834;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#30450;&#21435;&#28151;&#21709;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#28151;&#21709;&#26102;&#38388;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#30450;&#21435;&#28151;&#21709;&#30340;&#25193;&#23637;&#26469;&#25552;&#20986;&#25152;&#21576;&#29616;&#30340;&#31639;&#27861;&#65292;&#20801;&#35768;&#21516;&#26102;&#20272;&#35745;&#25151;&#38388;&#20914;&#28608;&#21709;&#24212;&#21644;&#28040;&#28151;&#21709;&#35821;&#38899;&#12290;&#21487;&#22312;&#32593;&#19978;&#25214;&#21040;&#38899;&#39057;&#26679;&#26412;&#21644;&#20195;&#30721;&#65288;https://uhh.de/inf-sp-derev-dps&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present in this paper an informed single-channel dereverberation method based on conditional generation with diffusion models. With knowledge of the room impulse response, the anechoic utterance is generated via reverse diffusion using a measurement consistency criterion coupled with a neural network that represents the clean speech prior. The proposed approach is largely more robust to measurement noise compared to a state-of-the-art informed single-channel dereverberation method, especially for non-stationary noise. Furthermore, we compare to other blind dereverberation methods using diffusion models and show superiority of the proposed approach for large reverberation times. We motivate the presented algorithm by introducing an extension for blind dereverberation allowing joint estimation of the room impulse response and anechoic speech. Audio samples and code can be found online (https://uhh.de/inf-sp-derev-dps).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#31232;&#30095;&#38453;&#21015;&#20013;&#20256;&#24863;&#22120;&#22833;&#25928;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#38647;&#36798;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#26041;&#21521;&#20272;&#35745;&#30340;&#39640;&#20998;&#36776;&#29575;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.12285</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24377;&#24615;&#31232;&#30095;&#38453;&#21015;&#38647;&#36798;
&lt;/p&gt;
&lt;p&gt;
Resilient Sparse Array Radar with the Aid of Deep Learning. (arXiv:2306.12285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#31232;&#30095;&#38453;&#21015;&#20013;&#20256;&#24863;&#22120;&#22833;&#25928;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#38647;&#36798;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#26041;&#21521;&#20272;&#35745;&#30340;&#39640;&#20998;&#36776;&#29575;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#31232;&#30095;&#38453;&#21015;&#20013;&#20256;&#24863;&#22120;&#22833;&#25928;&#30340;&#22810;&#30446;&#26631;&#26041;&#21521;&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#20256;&#24863;&#22120;&#22833;&#25928;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#26041;&#21521;&#20272;&#35745;&#30340;&#24615;&#33021;&#21644;&#20998;&#36776;&#29575;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38453;&#21015;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#20004;&#20010;&#20256;&#24863;&#22120;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the problem of direction of arrival (DOA) estimation for multiple targets in the presence of sensor failures in a sparse array. Generally, sparse arrays are known with very high-resolution capabilities, where N physical sensors can resolve up to $\mathcal{O}(N^2)$ uncorrelated sources. However, among the many configurations introduced in the literature, the arrays that provide the largest hole-free co-array are the most susceptible to sensor failures. We propose here two machine learning (ML) methods to mitigate the effect of sensor failures and maintain the DOA estimation performance and resolution. The first method enhances the conventional spatial smoothing using deep neural network (DNN), while the second one is an end-to-end data-driven method. Numerical results show that both approaches can significantly improve the performance of MRA with two failed sensors. The data-driven method can maintain the performance of the array with no failures at high signal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#20984;&#38598;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#26469;&#22686;&#24378;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#20915;&#31574;&#12290;&#35813;&#31639;&#27861;&#31867;&#22312;&#19968;&#33268;&#27604;&#29575;&#21644;&#40065;&#26834;&#27604;&#29575;&#20043;&#38388;&#24179;&#34913;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12282</link><description>&lt;p&gt;
&#20984;&#38598;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#19979;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Online Resource Allocation with Convex-set Machine-Learned Advice. (arXiv:2306.12282v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#20984;&#38598;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#26469;&#22686;&#24378;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#20915;&#31574;&#12290;&#35813;&#31639;&#27861;&#31867;&#22312;&#19968;&#33268;&#27604;&#29575;&#21644;&#40065;&#26834;&#27604;&#29575;&#20043;&#38388;&#24179;&#34913;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20915;&#31574;&#32773;&#36890;&#24120;&#20250;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#38656;&#27714;&#65292;&#31216;&#20026;&#24314;&#35758;&#65292;&#35813;&#24314;&#35758;&#21487;&#20197;&#22312;&#36164;&#28304;&#20998;&#37197;&#30340;&#22312;&#32447;&#20915;&#31574;&#36807;&#31243;&#20013;&#28508;&#22312;&#22320;&#34987;&#21033;&#29992;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#20854;&#28508;&#22312;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#21033;&#29992;&#36825;&#26679;&#30340;&#24314;&#35758;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#28508;&#22312;&#19981;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#22686;&#24378;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#20915;&#31574;&#12290;&#25105;&#20204;&#20551;&#35774;&#35813;&#24314;&#35758;&#30001;&#38656;&#27714;&#21521;&#37327;&#30340;&#19968;&#33324;&#20984;&#19981;&#30830;&#23450;&#24615;&#38598;&#34920;&#31034;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#30340; Pareto &#26368;&#20248;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#31867;&#65292;&#35813;&#31639;&#27861;&#22312;&#19968;&#33268;&#27604;&#29575;&#21644;&#40065;&#26834;&#27604;&#29575;&#20043;&#38388;&#24179;&#34913;&#12290;&#19968;&#33268;&#27604;&#29575;&#26159;&#25351;&#24403;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#20934;&#30830;&#26102;&#65292;&#31639;&#27861;&#30456;&#23545;&#20110;&#26368;&#20248;&#30340;&#21518;&#35265;&#20043;&#26126;&#35299;&#30340;&#34920;&#29616;&#65292;&#32780;&#40065;&#26834;&#27604;&#29575;&#21017;&#25429;&#33719;&#20102;&#24314;&#35758;&#19981;&#20934;&#30830;&#26102;&#23545;&#25239;&#24615;&#38656;&#27714;&#36807;&#31243;&#19979;&#30340;&#34920;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312; C-Pareto &#26368;&#20248;&#24773;&#20917;&#19979;&#65292;&#26368;&#22823;&#21270;&#36164;&#28304;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#19968;&#33268;&#27604;&#29575;&#21644;&#40065;&#26834;&#27604;&#29575;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-makers often have access to a machine-learned prediction about demand, referred to as advice, which can potentially be utilized in online decision-making processes for resource allocation. However, exploiting such advice poses challenges due to its potential inaccuracy. To address this issue, we propose a framework that enhances online resource allocation decisions with potentially unreliable machine-learned (ML) advice. We assume here that this advice is represented by a general convex uncertainty set for the demand vector.  We introduce a parameterized class of Pareto optimal online resource allocation algorithms that strike a balance between consistent and robust ratios. The consistent ratio measures the algorithm's performance (compared to the optimal hindsight solution) when the ML advice is accurate, while the robust ratio captures performance under an adversarial demand process when the advice is inaccurate. Specifically, in a C-Pareto optimal setting, we maximize the r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#21270;&#23398;&#30452;&#35273;&#31639;&#27861;&#65292;&#21487;&#20197;&#23545;&#37197;&#20301;&#20843;&#38754;&#20307;&#32593;&#32476;&#36827;&#34892;&#20960;&#20309;&#35299;&#26512;&#12289;&#37327;&#21270;&#21644;&#20998;&#31867;&#65292;&#24182;&#21033;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#23545;&#26080;&#26426;&#26694;&#26550;&#32858;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#20316;&#32773;&#21457;&#29616;&#20102;ABO$_{3}$&#38041;&#38043;&#30719;&#22810;&#24418;&#20307;&#31995;&#20013;&#30340;&#36724;&#21521;&#20542;&#26012;&#36235;&#21183;&#65292;&#24182;&#25581;&#31034;&#20102;Pauling&#30340;&#31532;&#19977;&#26465;&#35268;&#21017;&#36829;&#21453;&#21644;&#35774;&#35745;&#21407;&#21017;&#30340;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2306.12272</link><description>&lt;p&gt;
&#20174;&#32467;&#26500;&#25366;&#25496;&#21040;&#26080;&#30417;&#30563;&#25506;&#32034;&#20843;&#38754;&#20307;&#21407;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
From structure mining to unsupervised exploration of atomic octahedral networks. (arXiv:2306.12272v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#21270;&#23398;&#30452;&#35273;&#31639;&#27861;&#65292;&#21487;&#20197;&#23545;&#37197;&#20301;&#20843;&#38754;&#20307;&#32593;&#32476;&#36827;&#34892;&#20960;&#20309;&#35299;&#26512;&#12289;&#37327;&#21270;&#21644;&#20998;&#31867;&#65292;&#24182;&#21033;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#23545;&#26080;&#26426;&#26694;&#26550;&#32858;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#20316;&#32773;&#21457;&#29616;&#20102;ABO$_{3}$&#38041;&#38043;&#30719;&#22810;&#24418;&#20307;&#31995;&#20013;&#30340;&#36724;&#21521;&#20542;&#26012;&#36235;&#21183;&#65292;&#24182;&#25581;&#31034;&#20102;Pauling&#30340;&#31532;&#19977;&#26465;&#35268;&#21017;&#36829;&#21453;&#21644;&#35774;&#35745;&#21407;&#21017;&#30340;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#23376;&#20013;&#24515;&#30340;&#37197;&#20301;&#20843;&#38754;&#20307;&#32593;&#32476;&#36890;&#24120;&#20986;&#29616;&#22312;&#26080;&#26426;&#21644;&#28151;&#21512;&#22266;&#24577;&#26448;&#26009;&#20013;&#12290;&#34920;&#24449;&#23427;&#20204;&#30340;&#31354;&#38388;&#25490;&#21015;&#21644;&#29305;&#24615;&#23545;&#20110;&#35768;&#22810;&#26448;&#26009;&#31995;&#21015;&#23558;&#32467;&#26500;&#19982;&#24615;&#36136;&#32852;&#31995;&#36215;&#26469;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#36880;&#26696;&#20363;&#26816;&#26597;&#26041;&#27861;&#22312;&#21457;&#29616;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#30340;&#36235;&#21183;&#21644;&#30456;&#20284;&#20043;&#22788;&#26041;&#38754;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36816;&#29992;&#21270;&#23398;&#30452;&#35273;&#33258;&#21160;&#21270;&#22320;&#36827;&#34892;&#20960;&#20309;&#35299;&#26512;&#12289;&#37327;&#21270;&#21644;&#20998;&#31867;&#37197;&#20301;&#20843;&#38754;&#20307;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;ABO$_{3}$&#38041;&#38043;&#30719;&#22810;&#24418;&#20307;&#31995;&#20013;&#21457;&#29616;&#20102;&#36724;&#21521;&#20542;&#26012;&#36235;&#21183;&#65292;&#26377;&#21161;&#20110;&#26816;&#27979;&#27687;&#21270;&#24577;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23610;&#24230;&#19981;&#21464;&#30340;&#32534;&#30721;&#26041;&#26696;&#26469;&#34920;&#31034;&#36825;&#20123;&#32593;&#32476;&#65292;&#24182;&#19982;&#20154;&#31867;&#36741;&#21161;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23545;&#28151;&#21512;&#30872;&#38085;&#37240;&#30416;(A$_x$Pb$_y$I$_z$)&#20013;&#30340;&#26080;&#26426;&#26694;&#26550;&#32858;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;Pauling&#30340;&#31532;&#19977;&#26465;&#35268;&#21017;&#36829;&#21453;&#21644;&#35774;&#35745;&#21407;&#21017;ereotype0&#30340;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Networks of atom-centered coordination octahedra commonly occur in inorganic and hybrid solid-state materials. Characterizing their spatial arrangements and characteristics is crucial for relating structures to properties for many materials families. The traditional method using case-by-case inspection becomes prohibitive for discovering trends and similarities in large datasets. Here, we operationalize chemical intuition to automate the geometric parsing, quantification, and classification of coordination octahedral networks. We find axis-resolved tilting trends in ABO$_{3}$ perovskite polymorphs, which assist in detecting oxidation state changes. Moreover, we develop a scale-invariant encoding scheme to represent these networks, which, combined with human-assisted unsupervised machine learning, allows us to taxonomize the inorganic framework polytypes in hybrid iodoplumbates (A$_x$Pb$_y$I$_z$). Consequently, we uncover a violation of Pauling's third rule and the design principles und
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;Committor&#38382;&#39064;&#30340;&#26377;&#38480;&#34920;&#36798;&#24335;&#26041;&#27861;(FEX)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26368;&#20248;&#38750;&#32447;&#24615;&#20989;&#25968;&#21644;&#31995;&#25968;&#20540;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12268</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;Committor&#38382;&#39064;&#30340;&#26377;&#38480;&#34920;&#36798;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Finite Expression Method for Solving High-Dimensional Committor Problems. (arXiv:2306.12268v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;Committor&#38382;&#39064;&#30340;&#26377;&#38480;&#34920;&#36798;&#24335;&#26041;&#27861;(FEX)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26368;&#20248;&#38750;&#32447;&#24615;&#20989;&#25968;&#21644;&#31995;&#25968;&#20540;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#36335;&#24452;&#29702;&#35770;&#65288;TPT&#65289;&#26159;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#20174;&#36873;&#23450;&#30340;&#20122;&#31283;&#24577;$A$&#21040;$B$&#20043;&#38388;&#30340;&#31232;&#26377;&#36716;&#31227;&#20107;&#20214;&#12290;TPT&#30340;&#26680;&#24515;&#26159;Committor&#20989;&#25968;&#65292;&#20854;&#25551;&#36848;&#20102;&#20174;&#30456;&#31354;&#38388;&#30340;&#20219;&#20309;&#36215;&#22987;&#28857;&#21040;&#36798;&#20122;&#31283;&#24577;$B$&#20043;&#21069;&#21040;&#36798;$A$&#30340;&#27010;&#29575;&#12290;&#35745;&#31639;&#20986;Committor&#20043;&#21518;&#65292;&#21487;&#20197;&#31435;&#21363;&#25214;&#21040;&#36716;&#25442;&#36890;&#36947;&#21644;&#36716;&#25442;&#36895;&#29575;&#12290;Committor&#26159;&#20855;&#26377;&#36866;&#24403;&#36793;&#30028;&#26465;&#20214;&#30340;&#21453;&#21521;Kolmogorov&#26041;&#31243;&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#38656;&#35201;&#32593;&#26684;&#21270;&#25972;&#20010;&#29615;&#22659;&#31354;&#38388;&#65292;&#35299;&#20915;Committor&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26377;&#38480;&#34920;&#36798;&#24335;&#26041;&#27861;&#65288;FEX&#65292;Liang&#21644;Yang&#65288;2022&#65289;&#65289;&#20316;&#20026;&#35745;&#31639;Committor&#30340;&#24037;&#20855;&#12290;FEX&#36890;&#36807;&#28041;&#21450;&#19968;&#23450;&#25968;&#37327;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#21644;&#20108;&#36827;&#21046;&#31639;&#26415;&#36816;&#31639;&#30340;&#22266;&#23450;&#26377;&#38480;&#20195;&#25968;&#34920;&#36798;&#24335;&#26469;&#36924;&#36817;Committor&#12290;&#26368;&#20339;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12289;&#20108;&#36827;&#21046;&#36816;&#31639;&#21644;&#25968;&#20540;&#31995;&#25968;&#20540;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#12290;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#22810;&#20010;&#39640;&#32500;Committor&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#39640;&#36798;400&#20010;&#32500;&#24230;&#65292;&#23637;&#31034;&#20102;FEX&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#34920;&#26126;FEX&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#22914;&#26377;&#38480;&#20803;&#26041;&#27861;&#21644;&#26377;&#38480;&#24046;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transition path theory (TPT) is a mathematical framework for quantifying rare transition events between a pair of selected metastable states $A$ and $B$. Central to TPT is the committor function, which describes the probability to hit the metastable state $B$ prior to $A$ from any given starting point of the phase space. Once the committor is computed, the transition channels and the transition rate can be readily found. The committor is the solution to the backward Kolmogorov equation with appropriate boundary conditions. However, solving it is a challenging task in high dimensions due to the need to mesh a whole region of the ambient space. In this work, we explore the finite expression method (FEX, Liang and Yang (2022)) as a tool for computing the committor. FEX approximates the committor by an algebraic expression involving a fixed finite number of nonlinear functions and binary arithmetic operations. The optimal nonlinear functions, the binary operations, and the numerical coeffi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#22810;&#20809;&#35889;&#25968;&#25454;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23545;&#30452;&#25509;&#25104;&#20687;&#30340;&#22806;&#34892;&#26143;&#25506;&#27979;&#36827;&#34892;&#25913;&#36827;&#65292;&#36890;&#36807;&#24314;&#31435;&#32479;&#35745;&#27169;&#22411;&#24182;&#36827;&#34892;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#20043;&#38388;&#23454;&#29616;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.12266</link><description>&lt;p&gt;
&#21033;&#29992;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#22810;&#20809;&#35889;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20197;&#25913;&#21892;&#30452;&#25509;&#25104;&#20687;&#39640;&#23545;&#27604;&#24230;&#22806;&#34892;&#26143;&#25506;&#27979;
&lt;/p&gt;
&lt;p&gt;
Combining multi-spectral data with statistical and deep-learning models for improved exoplanet detection in direct imaging at high contrast. (arXiv:2306.12266v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#22810;&#20809;&#35889;&#25968;&#25454;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23545;&#30452;&#25509;&#25104;&#20687;&#30340;&#22806;&#34892;&#26143;&#25506;&#27979;&#36827;&#34892;&#25913;&#36827;&#65292;&#36890;&#36807;&#24314;&#31435;&#32479;&#35745;&#27169;&#22411;&#24182;&#36827;&#34892;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#20043;&#38388;&#23454;&#29616;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#25104;&#20687;&#30340;&#22806;&#34892;&#26143;&#25506;&#27979;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65306;&#26377;&#20852;&#36259;&#30340;&#29289;&#20307;&#30340;&#24494;&#24369;&#20449;&#21495;&#34987;&#20027;&#26143;&#24341;&#36215;&#30340;&#31354;&#38388;&#32467;&#26500;&#32321;&#27542;&#30340;&#24178;&#25200;&#25104;&#20998;&#25152;&#25513;&#30422;&#12290;&#21482;&#26377;&#23558;&#20960;&#27425;&#35266;&#27979;&#21644;&#19987;&#29992;&#25506;&#27979;&#31639;&#27861;&#30456;&#32467;&#21512;&#25165;&#33021;&#35782;&#21035;&#22806;&#34892;&#26143;&#20449;&#21495;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#24314;&#35758;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#24178;&#25200;&#20449;&#21495;&#30340;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20809;&#35889;&#29305;&#24449;&#30340;&#27169;&#22411;&#12290;&#22312;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#65292;&#26412;&#22320;&#24314;&#31435;&#23427;&#20204;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#24182;&#23545;&#25968;&#25454;&#36827;&#34892;&#20013;&#24515;&#21270;&#21644;&#30333;&#21270;&#65292;&#20197;&#25913;&#21892;&#23427;&#20204;&#30340;&#24179;&#31283;&#24615;&#21644;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#12290;&#28982;&#21518;&#65292;&#22312;&#30417;&#30563;&#27169;&#24335;&#19979;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#26816;&#27979;&#39044;&#22788;&#29702;&#22270;&#20687;&#20013;&#21512;&#25104;&#28304;&#30340;&#27531;&#20313;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26435;&#34913;&#65292;&#27604;&#39046;&#22495;&#20869;&#26631;&#20934;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#23427;&#20063;&#20248;&#20110;&#22522;&#20110;&#21333;&#32431;&#30340;&#21560;&#25910;&#29305;&#24449;&#30340;&#26368;&#26032;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exoplanet detection by direct imaging is a difficult task: the faint signals from the objects of interest are buried under a spatially structured nuisance component induced by the host star. The exoplanet signals can only be identified when combining several observations with dedicated detection algorithms. In contrast to most of existing methods, we propose to learn a model of the spatial, temporal and spectral characteristics of the nuisance, directly from the observations. In a pre-processing step, a statistical model of their correlations is built locally, and the data are centered and whitened to improve both their stationarity and signal-to-noise ratio (SNR). A convolutional neural network (CNN) is then trained in a supervised fashion to detect the residual signature of synthetic sources in the pre-processed images. Our method leads to a better trade-off between precision and recall than standard approaches in the field. It also outperforms a state-of-the-art algorithm based sole
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;VC&#27169;&#22411;&#65292;&#21487;&#20351;&#29992;&#20165;&#20004;&#20010;&#22686;&#24378;&#20989;&#25968;&#33258;&#21160;&#23558;&#35821;&#38899;&#20998;&#35299;&#20026;&#22235;&#20010;&#37096;&#20998;&#65292;&#32780;&#26080;&#38656;&#22810;&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#25110;&#36153;&#21147;&#30340;&#29942;&#39048;&#35843;&#25972;&#65292;&#24182;&#19988;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#65292;&#27169;&#22411;&#22312;&#35299;&#33073;&#26377;&#25928;&#24615;&#21644;&#35821;&#38899;&#33258;&#28982;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.12259</link><description>&lt;p&gt;
&#20351;&#29992;&#25490;&#21517;&#27169;&#22359;&#21644;&#35821;&#38899;&#22686;&#24378;&#30340;&#35821;&#38899;&#36716;&#25442;&#33258;&#21160;&#35821;&#38899;&#35299;&#32544;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Disentanglement for Voice Conversion using Rank Module and Speech Augmentation. (arXiv:2306.12259v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;VC&#27169;&#22411;&#65292;&#21487;&#20351;&#29992;&#20165;&#20004;&#20010;&#22686;&#24378;&#20989;&#25968;&#33258;&#21160;&#23558;&#35821;&#38899;&#20998;&#35299;&#20026;&#22235;&#20010;&#37096;&#20998;&#65292;&#32780;&#26080;&#38656;&#22810;&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#25110;&#36153;&#21147;&#30340;&#29942;&#39048;&#35843;&#25972;&#65292;&#24182;&#19988;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#65292;&#27169;&#22411;&#22312;&#35299;&#33073;&#26377;&#25928;&#24615;&#21644;&#35821;&#38899;&#33258;&#28982;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#21487;&#20197;&#23558;&#28304;&#35821;&#38899;&#30340;&#22768;&#38899;&#36716;&#25442;&#20026;&#30446;&#26631;&#35821;&#38899;&#65292;&#21516;&#26102;&#20445;&#25345;&#28304;&#35821;&#38899;&#30340;&#20869;&#23481;&#12290;&#35821;&#38899;&#20027;&#35201;&#21487;&#20998;&#35299;&#20026;&#22235;&#20010;&#37096;&#20998;&#65306;&#20869;&#23481;&#12289;&#38899;&#33394;&#12289;&#33410;&#22863;&#21644;&#38899;&#39640;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#30456;&#20851;&#24037;&#20316;&#21482;&#32771;&#34385;&#20102;&#20869;&#23481;&#21644;&#38899;&#33394;&#65292;&#32467;&#26524;&#23548;&#33268;&#35821;&#38899;&#19981;&#22815;&#33258;&#28982;&#12290;&#26368;&#36817;&#19968;&#20123;&#30740;&#31350;&#21487;&#20197;&#23558;&#35821;&#38899;&#35299;&#33073;&#25104;&#20960;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#36153;&#21147;&#30340;&#29942;&#39048;&#35843;&#25972;&#25110;&#21508;&#31181;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#65292;&#27599;&#20010;&#29305;&#24449;&#37117;&#20551;&#23450;&#21253;&#21547;&#35299;&#33073;&#30340;&#35821;&#38899;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;VC&#27169;&#22411;&#65292;&#21487;&#20351;&#29992;&#20165;&#20004;&#20010;&#22686;&#24378;&#20989;&#25968;&#33258;&#21160;&#23558;&#35821;&#38899;&#20998;&#35299;&#20026;&#22235;&#20010;&#37096;&#20998;&#65292;&#32780;&#26080;&#38656;&#22810;&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#25110;&#36153;&#21147;&#30340;&#29942;&#39048;&#35843;&#25972;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#31616;&#21333;&#32780;&#39640;&#25928;&#65292;&#24182;&#19988;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35299;&#33073;&#26377;&#25928;&#24615;&#21644;&#35821;&#38899;&#33258;&#28982;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice Conversion (VC) converts the voice of a source speech to that of a target while maintaining the source's content. Speech can be mainly decomposed into four components: content, timbre, rhythm and pitch. Unfortunately, most related works only take into account content and timbre, which results in less natural speech. Some recent works are able to disentangle speech into several components, but they require laborious bottleneck tuning or various hand-crafted features, each assumed to contain disentangled speech information. In this paper, we propose a VC model that can automatically disentangle speech into four components using only two augmentation functions, without the requirement of multiple hand-crafted features or laborious bottleneck tuning. The proposed model is straightforward yet efficient, and the empirical results demonstrate that our model can achieve a better performance than the baseline, regarding disentanglement effectiveness and speech naturalness.
&lt;/p&gt;</description></item><item><title>GADBench&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#31995;&#32479;&#65292;&#21457;&#29616;&#20102;&#26641;&#38598;&#25104;&#21644;&#31616;&#21333;&#37051;&#22495;&#27719;&#32858;&#26041;&#27861;&#32988;&#36807;&#25152;&#26377;23&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;&#26368;&#26032;&#30340;&#38024;&#23545;GAD&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;GNN&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12251</link><description>&lt;p&gt;
GADBench&#65306;&#37325;&#26032;&#23457;&#35270;&#21644;&#23545;&#30417;&#30563;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection. (arXiv:2306.12251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12251
&lt;/p&gt;
&lt;p&gt;
GADBench&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#31995;&#32479;&#65292;&#21457;&#29616;&#20102;&#26641;&#38598;&#25104;&#21644;&#31616;&#21333;&#37051;&#22495;&#27719;&#32858;&#26041;&#27861;&#32988;&#36807;&#25152;&#26377;23&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;&#26368;&#26032;&#30340;&#38024;&#23545;GAD&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20256;&#32479;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#31639;&#27861;&#21644;&#26368;&#36817;&#27969;&#34892;&#30340;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#19968;&#30452;&#23384;&#22312;&#12290;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#22312;&#26631;&#20934;&#32508;&#21512;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#22914;&#20309;&#65292;GNN&#26159;&#21542;&#20248;&#20110;&#20256;&#32479;&#31639;&#27861;&#65288;&#22914;&#26641;&#38598;&#25104;&#65289;&#20197;&#21450;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#22270;&#34920;&#19978;&#30340;&#25928;&#29575;&#22914;&#20309;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GADBench - &#19968;&#20010;&#38745;&#24577;&#22270;&#24418;&#30417;&#30563;&#24322;&#24120;&#33410;&#28857;&#26816;&#27979;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;GADBench&#22312;&#20174;&#25968;&#21315;&#21040;&#25968;&#30334;&#19975;&#33410;&#28857;&#65288;&#32422;6M&#65289;&#30340;&#21313;&#20010;&#30495;&#23454;GAD&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;23&#31181;&#19981;&#21516;&#27169;&#22411;&#30340;&#24443;&#24213;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#20855;&#26377;&#31616;&#21333;&#37051;&#22495;&#27719;&#32858;&#30340;&#26641;&#38598;&#25104;&#32988;&#36807;&#25152;&#26377;&#20854;&#20182;&#22522;&#32447;&#65292;&#21253;&#25324;&#26368;&#26032;&#30340;&#38024;&#23545;GAD&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;GNN&#12290;&#36890;&#36807;&#23558;GADBench&#20316;&#20026;&#24320;&#28304;&#24037;&#20855;&#25552;&#20379;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;GAD&#24403;&#21069;&#36827;&#23637;&#30340;&#20851;&#38190;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#22880;&#23450;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a long history of traditional Graph Anomaly Detection (GAD) algorithms and recently popular Graph Neural Networks (GNNs), it is still not clear (1) how they perform under a standard comprehensive setting, (2) whether GNNs outperform traditional algorithms such as tree ensembles, and (3) their efficiency on large-scale graphs. In response, we present GADBench -- a comprehensive benchmark for supervised anomalous node detection on static graphs. GADBench provides a thorough comparison across 23 distinct models on ten real-world GAD datasets ranging from thousands to millions of nodes ($\sim$6M). Our main finding is that tree ensembles with simple neighborhood aggregation outperform all other baselines, including the latest GNNs tailored for the GAD task. By making GADBench available as an open-source tool, we offer pivotal insights into the current advancements of GAD and establish a solid foundation for future research. Our code is available at https://github.com/squareRoot3/GADBen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#38899;&#20048;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#23436;&#20840;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#26356;&#22810;&#23545;&#38899;&#20048;&#30456;&#20284;&#24615;&#21644;&#20998;&#31867;&#31995;&#32479;&#30340;&#25511;&#21046;&#21644;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.12249</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#38899;&#20048;&#30456;&#20284;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based Multimodal Music Similarity. (arXiv:2306.12249v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#38899;&#20048;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#23436;&#20840;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#26356;&#22810;&#23545;&#38899;&#20048;&#30456;&#20284;&#24615;&#21644;&#20998;&#31867;&#31995;&#32479;&#30340;&#25511;&#21046;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#30456;&#20284;&#24615;&#26159;&#38899;&#20048;&#26816;&#32034;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#38899;&#20048;&#20998;&#26512;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#32780;&#19988;&#65292;&#23545;&#20110;&#38899;&#20048;&#19987;&#23478;&#26469;&#35828;&#65292;&#30456;&#20284;&#24615;&#21487;&#20197;&#30740;&#31350;&#20316;&#26354;&#23478;&#21644;&#21382;&#21490;&#26102;&#26399;&#20043;&#38388;&#30340;&#31867;&#27604;&#21644;&#24433;&#21709;&#12290;&#30446;&#21069;&#65292;&#38024;&#23545;&#38899;&#20048;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#31526;&#21495;&#20869;&#23481;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#19988;&#19981;&#24635;&#26159;&#26131;&#20110;&#33719;&#24471;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20351;&#29992;&#38899;&#39057;&#20449;&#21495;&#30340;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#26377;&#20851;&#35266;&#23519;&#21040;&#30340;&#30456;&#20284;&#24615;&#32972;&#21518;&#21407;&#22240;&#30340;&#20219;&#20309;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31526;&#21495;&#21644;&#38899;&#39057;&#20869;&#23481;&#26469;&#30740;&#31350;&#38899;&#20048;&#30456;&#20284;&#24615;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#24320;&#21457;&#19968;&#20010;&#23436;&#20840;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#26356;&#22810;&#23545;&#38899;&#20048;&#30456;&#20284;&#24615;&#21644;&#20998;&#31867;&#31995;&#32479;&#30340;&#25511;&#21046;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music similarity is an essential aspect of music retrieval, recommendation systems, and music analysis. Moreover, similarity is of vital interest for music experts, as it allows studying analogies and influences among composers and historical periods. Current approaches to musical similarity rely mainly on symbolic content, which can be expensive to produce and is not always readily available. Conversely, approaches using audio signals typically fail to provide any insight about the reasons behind the observed similarity. This research addresses the limitations of current approaches by focusing on the study of musical similarity using both symbolic and audio content. The aim of this research is to develop a fully explainable and interpretable system that can provide end-users with more control and understanding of music similarity and classification systems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#26631;&#35760;&#21644;&#20272;&#31639;&#33041;&#37096;CT&#22270;&#20687;&#20013;&#30340;&#32570;&#34880;&#24615;&#30149;&#21464;&#30340;&#25104;&#20687;&#26102;&#38271;&#65292;&#35813;&#26041;&#27861;&#22312;&#30149;&#28790;&#20998;&#21106;&#21644;&#30149;&#28790;&#24180;&#40836;&#20272;&#35745;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26377;&#26395;&#25104;&#20026;&#25903;&#25345;&#20013;&#39118;&#25252;&#29702;&#20915;&#31574;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.12242</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#32593;&#32476;&#30340;&#33041;&#37096;CT&#22270;&#20687;&#32570;&#34880;&#25439;&#20260;&#21516;&#26102;&#26631;&#35760;&#21644;&#20272;&#31639;&#25104;&#20687;&#26102;&#38271;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Concurrent ischemic lesion age estimation and segmentation of CT brain using a Transformer-based network. (arXiv:2306.12242v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12242
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#26631;&#35760;&#21644;&#20272;&#31639;&#33041;&#37096;CT&#22270;&#20687;&#20013;&#30340;&#32570;&#34880;&#24615;&#30149;&#21464;&#30340;&#25104;&#20687;&#26102;&#38271;&#65292;&#35813;&#26041;&#27861;&#22312;&#30149;&#28790;&#20998;&#21106;&#21644;&#30149;&#28790;&#24180;&#40836;&#20272;&#35745;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26377;&#26395;&#25104;&#20026;&#25903;&#25345;&#20013;&#39118;&#25252;&#29702;&#20915;&#31574;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#21330;&#20013;&#25252;&#29702;&#30340;&#26680;&#24515;&#26159;&#36805;&#36895;&#22788;&#29702;&#65292;&#36825;&#21462;&#20915;&#20110;&#20013;&#39118;&#21457;&#20316;&#30340;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#20020;&#24202;&#20915;&#31574;&#38656;&#35201;&#20934;&#30830;&#30693;&#36947;&#26102;&#38388;&#65292;&#36890;&#24120;&#38656;&#35201;&#25918;&#23556;&#21307;&#24072;&#35299;&#35835;&#33041;&#37096;CT&#25195;&#25551;&#20197;&#30830;&#35748;&#20107;&#20214;&#30340;&#21457;&#29983;&#21644;&#24180;&#40836;&#12290;&#30001;&#20110;&#24613;&#24615;&#32570;&#34880;&#24615;&#30149;&#21464;&#34920;&#29616;&#24494;&#22937;&#19988;&#22806;&#35266;&#21160;&#24577;&#65292;&#22240;&#27492;&#36825;&#20123;&#20219;&#21153;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#33258;&#21160;&#21270;&#30340;&#21162;&#21147;&#36824;&#27809;&#26377;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#20272;&#31639;&#30149;&#28790;&#24180;&#40836;&#65292;&#24182;&#23558;&#36825;&#20004;&#20010;&#20219;&#21153;&#29420;&#31435;&#22788;&#29702;&#65292;&#22240;&#27492;&#24573;&#30053;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#20114;&#34917;&#20851;&#31995;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;Transformer&#32593;&#32476;&#65292;&#20248;&#21270;&#20102;&#21516;&#26102;&#26631;&#35760;&#21644;&#20272;&#35745;&#33041;&#37096;&#32570;&#34880;&#24615;&#30149;&#21464;&#30340;&#24180;&#40836;&#12290;&#36890;&#36807;&#21033;&#29992;&#38376;&#25511;&#20301;&#32622;&#33258;&#25105;&#20851;&#27880;&#21644;CT&#29305;&#23450;&#25968;&#25454;&#22686;&#24378;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#33719;&#38271;&#36317;&#31163;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#21487;&#20256;&#36755;&#21040;&#20854;&#20182;&#33041;&#37096;CT&#25104;&#20687;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30149;&#28790;&#20998;&#21106;&#21644;&#20934;&#30830;&#20272;&#35745;&#30149;&#28790;&#24180;&#40836;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#25104;&#20026;&#25903;&#25345;&#20013;&#39118;&#25252;&#29702;&#20915;&#31574;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cornerstone of stroke care is expedient management that varies depending on the time since stroke onset. Consequently, clinical decision making is centered on accurate knowledge of timing and often requires a radiologist to interpret Computed Tomography (CT) of the brain to confirm the occurrence and age of an event. These tasks are particularly challenging due to the subtle expression of acute ischemic lesions and the dynamic nature of their appearance. Automation efforts have not yet applied deep learning to estimate lesion age and treated these two tasks independently, so, have overlooked their inherent complementary relationship. To leverage this, we propose a novel end-to-end multi-task transformer-based network optimized for concurrent segmentation and age estimation of cerebral ischemic lesions. By utilizing gated positional self-attention and CT-specific data augmentation, the proposed method can capture long-range spatial dependencies while maintaining its ability to be tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;&#21464;&#20307;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;&#32467;&#26500;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#23569;&#30340;&#20998;&#23376;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36798;&#21040;&#19982;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12231</link><description>&lt;p&gt;
&#21033;&#29992;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#34507;&#30333;&#36136;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
Predicting protein variants with equivariant graph neural networks. (arXiv:2306.12231v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;&#21464;&#20307;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;&#32467;&#26500;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#23569;&#30340;&#20998;&#23376;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36798;&#21040;&#19982;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35768;&#22810;&#34507;&#30333;&#36136;&#24037;&#31243;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#24207;&#21015;&#30340;&#27169;&#22411;&#22312;&#34507;&#30333;&#36136;&#36866;&#24212;&#24615;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#32780;&#22522;&#20110;&#32467;&#26500;&#30340;&#27169;&#22411;&#21017;&#24050;&#34987;&#23454;&#39564;&#24615;&#22320;&#29992;&#20110;&#24320;&#21457;&#20855;&#26377;&#22686;&#24378;&#21151;&#33021;&#30340;&#34507;&#30333;&#36136;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#27979;&#20248;&#20110;&#37326;&#29983;&#22411;&#34507;&#30333;&#36136;&#30340;&#34507;&#30333;&#36136;&#21464;&#20307;&#26041;&#38754;&#65292;&#22522;&#20110;&#32467;&#26500;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#27604;&#36739;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#23384;&#22312;&#30740;&#31350;&#31354;&#30333;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#27604;&#36739;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;EGNN&#65289;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#35782;&#21035;&#26377;&#21069;&#36884;&#30340;&#27688;&#22522;&#37240;&#31361;&#21464;&#30340;&#33021;&#21147;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32467;&#26500;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#23569;&#30340;&#20998;&#23376;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#23558;&#27979;&#23450;&#26631;&#35760;&#25968;&#25454;&#19982;&#32467;&#26500;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#19982;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25928;&#26524;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained models have been successful in many protein engineering tasks. Most notably, sequence-based models have achieved state-of-the-art performance on protein fitness prediction while structure-based models have been used experimentally to develop proteins with enhanced functions. However, there is a research gap in comparing structure- and sequence-based methods for predicting protein variants that are better than the wildtype protein. This paper aims to address this gap by conducting a comparative study between the abilities of equivariant graph neural networks (EGNNs) and sequence-based approaches to identify promising amino-acid mutations. The results show that our proposed structural approach achieves a competitive performance to sequence-based methods while being trained on significantly fewer molecules. Additionally, we find that combining assay labelled data with structure pre-trained models yields similar trends as with sequence pre-trained models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#20013;&#30340;&#21098;&#26525;&#20301;&#32622;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20302;&#23494;&#24230;&#33539;&#22260;&#20869;&#65292;&#26368;&#31616;&#21333;&#30340;&#22823;&#23567;&#26041;&#27861;&#25552;&#20379;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12230</link><description>&lt;p&gt;
&#22855;&#22937;&#30340;&#26435;&#37325;&#21450;&#20854;&#26597;&#25214;&#26041;&#27861;&#65306;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#20013;&#30340;&#21098;&#26525;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training. (arXiv:2306.12230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#20013;&#30340;&#21098;&#26525;&#20301;&#32622;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20302;&#23494;&#24230;&#33539;&#22260;&#20869;&#65292;&#26368;&#31616;&#21333;&#30340;&#22823;&#23567;&#26041;&#27861;&#25552;&#20379;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#65288;DST&#65289;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#32467;&#26500;&#26469;&#20248;&#21270;&#20854;&#31232;&#30095;&#21021;&#22987;&#21270;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;DST&#33021;&#22815;&#32988;&#36807;&#23494;&#38598;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#21098;&#26525;&#21644;&#29983;&#38271;&#26631;&#20934;&#65292;&#36825;&#20123;&#26631;&#20934;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#34987;&#21453;&#22797;&#24212;&#29992;&#20197;&#35843;&#25972;&#32593;&#32476;&#30340;&#31232;&#30095;&#36830;&#25509;&#12290;&#34429;&#28982;&#29983;&#38271;&#26631;&#20934;&#23545;DST&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#36739;&#22909;&#22320;&#30740;&#31350;&#20102;&#65292;&#20294;&#21098;&#26525;&#26631;&#20934;&#30340;&#24433;&#21709;&#20173;&#28982;&#34987;&#24573;&#35270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#23545;&#21508;&#31181;&#21098;&#26525;&#26631;&#20934;&#30340;&#24191;&#27867;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#23545; DST &#35299;&#20915;&#26041;&#26696;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#30740;&#31350;&#26041;&#27861;&#37117;&#20135;&#29983;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#22312;&#20302;&#23494;&#24230;&#33539;&#22260;&#20869;&#65292;&#26368;&#31616;&#21333;&#30340;&#25216;&#26415;&#8212;&#8212;&#22522;&#20110;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse Training (DST) is a rapidly evolving area of research that seeks to optimize the sparse initialization of a neural network by adapting its topology during training. It has been shown that under specific conditions, DST is able to outperform dense models. The key components of this framework are the pruning and growing criteria, which are repeatedly applied during the training process to adjust the network's sparse connectivity. While the growing criterion's impact on DST performance is relatively well studied, the influence of the pruning criterion remains overlooked. To address this issue, we design and perform an extensive empirical analysis of various pruning criteria to better understand their effect on the dynamics of DST solutions. Surprisingly, we find that most of the studied methods yield similar results. The differences become more significant in the low-density regime, where the best performance is predominantly given by the simplest technique: magnitude-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;AutoRUL&#65292;&#29992;&#20110;&#33258;&#21160;&#39044;&#27979;&#24037;&#31243;&#31995;&#32479;&#30340;&#21097;&#20313;&#20351;&#29992;&#23551;&#21629;&#65288;RUL&#65289;&#12290;&#35813;&#26041;&#27861;&#23558;&#24494;&#35843;&#30340;&#26631;&#20934;&#22238;&#24402;&#26041;&#27861;&#19982;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#38598;&#25104;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;AutoML&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2306.12215</link><description>&lt;p&gt;
&#38754;&#21521;&#21097;&#20313;&#20351;&#29992;&#23551;&#21629;&#39044;&#27979;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automated Machine Learning for Remaining Useful Life Predictions. (arXiv:2306.12215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;AutoRUL&#65292;&#29992;&#20110;&#33258;&#21160;&#39044;&#27979;&#24037;&#31243;&#31995;&#32479;&#30340;&#21097;&#20313;&#20351;&#29992;&#23551;&#21629;&#65288;RUL&#65289;&#12290;&#35813;&#26041;&#27861;&#23558;&#24494;&#35843;&#30340;&#26631;&#20934;&#22238;&#24402;&#26041;&#27861;&#19982;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#38598;&#25104;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;AutoML&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24037;&#31243;&#31995;&#32479;&#30340;&#21097;&#20313;&#20351;&#29992;&#23551;&#21629;&#65288;RUL&#65289;&#26159;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;RUL&#39044;&#27979;&#20013;&#26222;&#21450;&#65292;&#30456;&#27604;&#27169;&#22411;&#39537;&#21160;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#24037;&#31243;&#31995;&#32479;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#36825;&#21482;&#26159;&#23558;&#38656;&#35201;&#30340;&#29289;&#29702;&#19987;&#19994;&#30693;&#35782;&#26367;&#25442;&#25104;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#19987;&#19994;&#30693;&#35782;&#65292;&#32780;&#36825;&#31181;&#19987;&#19994;&#30693;&#35782;&#36890;&#24120;&#20063;&#19981;&#21487;&#24471;&#12290;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#25215;&#35834;&#33258;&#21160;&#26500;&#24314;&#31471;&#21040;&#31471;&#30340;ML&#31649;&#36947;&#65292;&#20351;&#39046;&#22495;&#19987;&#23478;&#32780;&#38750;ML&#19987;&#23478;&#33021;&#22815;&#21019;&#24314;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AutoRUL&#65292;&#19968;&#31181;AutoML&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;RUL&#39044;&#27979;&#12290;AutoRUL&#23558;&#24494;&#35843;&#30340;&#26631;&#20934;&#22238;&#24402;&#26041;&#27861;&#19982;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#38598;&#25104;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#29992;&#20110;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#25163;&#24037;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#34920;&#26126;AutoML&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to predict the remaining useful life (RUL) of an engineering system is an important task in prognostics and health management. Recently, data-driven approaches to RUL predictions are becoming prevalent over model-based approaches since no underlying physical knowledge of the engineering system is required. Yet, this just replaces required expertise of the underlying physics with machine learning (ML) expertise, which is often also not available. Automated machine learning (AutoML) promises to build end-to-end ML pipelines automatically enabling domain experts without ML expertise to create their own models. This paper introduces AutoRUL, an AutoML-driven end-to-end approach for automatic RUL predictions. AutoRUL combines fine-tuned standard regression methods to an ensemble with high predictive power. By evaluating the proposed method on eight real-world and synthetic datasets against state-of-the-art hand-crafted models, we show that AutoML provides a viable alternative to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#65292;&#22312;&#26377;&#30028;&#21644;&#19968;&#33324;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#20013;&#22343;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30028;&#38480;&#36824;&#33021;&#22815;&#20445;&#25345;&#38543;&#26102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12214</link><description>&lt;p&gt;
&#26356;&#22810;&#30340;PAC-Bayes Bounds&#65306;&#20174;&#26377;&#30028;&#25439;&#22833;&#21040;&#20855;&#26377;&#19968;&#33324;&#24615;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#65292;&#21040;&#20219;&#20309;&#26102;&#38388;&#22343;&#26377;&#25928;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime-validity. (arXiv:2306.12214v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#65292;&#22312;&#26377;&#30028;&#21644;&#19968;&#33324;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#20013;&#22343;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30028;&#38480;&#36824;&#33021;&#22815;&#20445;&#25345;&#38543;&#26102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25439;&#22833;&#25552;&#20986;&#20102;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#12290;&#39318;&#20808;&#65292;&#38024;&#23545;&#26377;&#30028;&#33539;&#22260;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Catoni&#30028;&#30340;&#21152;&#24378;&#29256;&#26412;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#21442;&#25968;&#20540;&#30340;&#32479;&#19968;&#30028;&#12290;&#36825;&#23548;&#33268;&#20102;&#26032;&#30340;&#24555;&#36895;&#36895;&#29575;&#21644;&#28151;&#21512;&#36895;&#29575;&#19978;&#38480;&#65292;&#36825;&#20123;&#19978;&#38480;&#21487;&#35299;&#37322;&#24615;&#24378;&#19988;&#27604;&#25991;&#29486;&#20013;&#20808;&#21069;&#30028;&#38480;&#26356;&#32039;&#12290;&#20854;&#27425;&#65292;&#38024;&#23545;&#26356;&#19968;&#33324;&#30340;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#26080;&#21442;&#25968;&#19978;&#38480;&#65306;&#24403;&#25439;&#22833;&#30340;&#32047;&#31215;&#29983;&#25104;&#20989;&#25968;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;PAC-Bayes Chernoff&#31867;&#27604;&#65292;&#21478;&#19968;&#20010;&#19978;&#38480;&#26159;&#25439;&#22833;&#30340;&#20108;&#38454;&#30697;&#26377;&#30028;&#12290;&#36825;&#20004;&#20010;&#19978;&#38480;&#26159;&#21033;&#29992;&#19968;&#31181;&#22522;&#20110;&#21487;&#33021;&#20107;&#20214;&#31354;&#38388;&#30340;&#31163;&#25955;&#21270;&#30340;&#26032;&#25216;&#26415;&#33719;&#24471;&#30340;&#65292;&#8220;&#22312;&#27010;&#29575;&#8221;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30028;&#38480;&#30340;&#31616;&#21333;&#25216;&#26415;&#23558;&#25152;&#26377;&#20808;&#21069;&#32467;&#26524;&#25193;&#23637;&#21040;&#20219;&#20309;&#26102;&#38388;&#26377;&#25928;&#30340;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present new high-probability PAC-Bayes bounds for different types of losses. Firstly, for losses with a bounded range, we present a strengthened version of Catoni's bound that holds uniformly for all parameter values. This leads to new fast rate and mixed rate bounds that are interpretable and tighter than previous bounds in the literature. Secondly, for losses with more general tail behaviors, we introduce two new parameter-free bounds: a PAC-Bayes Chernoff analogue when the loss' cumulative generating function is bounded, and a bound when the loss' second moment is bounded. These two bounds are obtained using a new technique based on a discretization of the space of possible events for the "in probability" parameter optimization problem. Finally, we extend all previous results to anytime-valid bounds using a simple technique applicable to any existing bound.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340; MimiC &#31639;&#27861;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36864;&#20986;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#20223;&#32570;&#22833;&#30340;&#23458;&#25143;&#31471;&#26356;&#26032;&#35299;&#20915;&#20102;&#32858;&#21512;&#26356;&#26032;&#21644;&#26399;&#26395;&#20013;&#24515;&#26356;&#26032;&#20043;&#38388;&#30340;&#20998;&#27495;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.12212</link><description>&lt;p&gt;
MimiC&#65306;&#27169;&#20223;&#20013;&#24515;&#26356;&#26032;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#36864;&#20986;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates. (arXiv:2306.12212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340; MimiC &#31639;&#27861;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36864;&#20986;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#20223;&#32570;&#22833;&#30340;&#23458;&#25143;&#31471;&#26356;&#26032;&#35299;&#20915;&#20102;&#32858;&#21512;&#26356;&#26032;&#21644;&#26399;&#26395;&#20013;&#24515;&#26356;&#26032;&#20043;&#38388;&#30340;&#20998;&#27495;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#35757;&#32451;&#20219;&#21153;&#20998;&#21457;&#32473;&#23458;&#25143;&#31471;&#65292;&#21482;&#38656;&#35201;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#25910;&#38598;&#27169;&#22411;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#20013;&#37096;&#32626;&#26102;&#65292;&#23458;&#25143;&#31471;&#65288;&#22914;&#26234;&#33021;&#25163;&#26426;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#65289;&#21487;&#33021;&#20250;&#26080;&#39044;&#35686;&#22320;&#36864;&#20986;&#20219;&#20309;&#19968;&#27425;&#35757;&#32451;&#36845;&#20195;&#65292;&#36825;&#20250;&#38459;&#30861;&#32852;&#37030;&#23398;&#20064;&#36798;&#21040;&#25910;&#25947;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#36825;&#19968;&#20851;&#38190;&#25361;&#25112;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#21517;&#20026; MimiC &#30340;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20013;&#24515;&#26381;&#21153;&#22120;&#20462;&#25913;&#20854;&#26356;&#26032;&#20197;&#27169;&#20223;&#32570;&#22833;&#23458;&#25143;&#31471;&#26356;&#26032;&#65292;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;MimiC &#30456;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising framework for privacy-preserving collaborative learning. In FL, the model training tasks are distributed to clients and only the model updates need to be collected at a central server. However, when being deployed at the mobile edge network, clients (e.g., smartphones and wearables) may have unpredictable availability and randomly drop out of any training iteration, which hinders FL from achieving the convergence. This paper tackles such a critical challenge of FL. In particular, we first investigate the convergence of the classical FedAvg algorithm with arbitrary client dropouts. We find that with the common choice of a decaying learning rate, FedAvg can only oscillate within the neighborhood of a stationary point of the global loss function, which is caused by the divergence between the aggregated update and the desired central update. Motivated by this new observation, we then design a novel training algorithm named MimiC, where the server modi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#20855;&#20307;&#20351;&#29992;&#32422;&#26463;&#31639;&#26415;&#38382;&#39064;&#25506;&#32034;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#20998;&#25968;&#21644;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#21457;&#29616;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#20197;&#30053;&#24494;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35299;&#20915;&#20998;&#23618;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35299;&#20915;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.12198</link><description>&lt;p&gt;
&#25581;&#24320;&#40657;&#21283;&#23376;&#65306;&#20998;&#26512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks. (arXiv:2306.12198v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#20855;&#20307;&#20351;&#29992;&#32422;&#26463;&#31639;&#26415;&#38382;&#39064;&#25506;&#32034;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#20998;&#25968;&#21644;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#21457;&#29616;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#20197;&#30053;&#24494;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35299;&#20915;&#20998;&#23618;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35299;&#20915;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22810;&#25968;&#20808;&#36827;&#27169;&#22411;&#30340;&#8220;&#40657;&#21283;&#23376;&#8221;&#29305;&#24615;&#65292;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#38543;&#30528;&#22522;&#20110;transformers&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#36827;&#23637;&#21450;&#20854;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#19981;&#26029;&#38598;&#25104;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#32039;&#36843;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;AI&#27169;&#22411;&#65292;&#24517;&#39035;&#29702;&#35299;&#28041;&#21450;&#30340;&#36807;&#31243;&#27493;&#39588;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#36827;&#34892;&#27604;&#36739;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#26131;&#25026;&#30340;&#38750;&#35821;&#35328;&#20219;&#21153;&#26469;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#37096;&#36816;&#20316;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#30340;&#32422;&#26463;&#31639;&#26415;&#38382;&#39064;&#65292;&#20197;&#20998;&#26512;&#20854;&#27880;&#24847;&#21147;&#26435;&#37325;&#20998;&#25968;&#21644;&#38544;&#34255;&#29366;&#24577;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#27169;&#22411;&#20197;&#30053;&#24494;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35299;&#20915;&#20998;&#23618;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35299;&#20915;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigating deep learning language models has always been a significant research area due to the ``black box" nature of most advanced models. With the recent advancements in pre-trained language models based on transformers and their increasing integration into daily life, addressing this issue has become more pressing. In order to achieve an explainable AI model, it is essential to comprehend the procedural steps involved and compare them with human thought processes. Thus, in this paper, we use simple, well-understood non-language tasks to explore these models' inner workings. Specifically, we apply a pre-trained language model to constrained arithmetic problems with hierarchical structure, to analyze their attention weight scores and hidden states. The investigation reveals promising results, with the model addressing hierarchical problems in a moderately structured manner, similar to human problem-solving strategies. Additionally, by inspecting the attention weights layer by laye
&lt;/p&gt;</description></item><item><title>Split learning (SL) enables servers to handle the major training workload while still enhancing data privacy, which is an important approach in 6G edge networks. This article provides an overview of the tailored 6G architecture to support edge SL, the critical design issues for edge SL, and presents future research direction and exciting applications of SL in 6G edge networks.</title><link>http://arxiv.org/abs/2306.12194</link><description>&lt;p&gt;
6G&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;Split Learning
&lt;/p&gt;
&lt;p&gt;
Split Learning in 6G Edge Networks. (arXiv:2306.12194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12194
&lt;/p&gt;
&lt;p&gt;
Split learning (SL) enables servers to handle the major training workload while still enhancing data privacy, which is an important approach in 6G edge networks. This article provides an overview of the tailored 6G architecture to support edge SL, the critical design issues for edge SL, and presents future research direction and exciting applications of SL in 6G edge networks.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20998;&#24067;&#24335;&#36793;&#32536;&#35745;&#31639;&#36164;&#28304;&#30340;&#26222;&#21450;&#65292;6G&#31227;&#21160;&#32593;&#32476;&#23558;&#21457;&#23637;&#25104;&#20026;&#19968;&#20010;&#36830;&#25509;&#26234;&#33021;&#30340;&#32593;&#32476;&#12290;&#22312;&#36825;&#26465;&#32447;&#36335;&#19978;&#65292;&#23558;&#32852;&#37030;&#23398;&#20064;&#32435;&#20837;&#31227;&#21160;&#36793;&#32536;&#30340;&#25552;&#35758;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#24222;&#22823;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#20960;&#20046;&#26080;&#27861;&#25903;&#25345;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#23548;&#33268;&#20102;Split Learning (SL)&#30340;&#20986;&#29616;&#65292;&#23427;&#20351;&#26381;&#21153;&#22120;&#22788;&#29702;&#20027;&#35201;&#30340;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#65292;&#21516;&#26102;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;SL&#30340;&#20851;&#38190;&#21457;&#23637;&#65292;&#24182;&#38416;&#36848;&#20102;&#20854;&#19982;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#35828;&#26126;&#20102;&#23450;&#21046;&#30340;6G&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25903;&#25345;&#36793;&#32536;SL&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36793;&#32536;SL&#30340;&#20851;&#38190;&#35774;&#35745;&#38382;&#39064;&#65292;&#21253;&#25324;&#21019;&#26032;&#30340;&#36164;&#28304;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;&#21644;&#22312;&#21333;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#19979;&#30340;&#36164;&#28304;&#31649;&#29702;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;Split Learning&#27169;&#22411;&#30340;&#22810;&#36793;&#32536;&#26381;&#21153;&#22120;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#19968;&#20123;&#21050;&#28608;&#20154;&#24515;&#30340;SL&#22312;6G&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of distributed edge computing resources, the 6G mobile network will evolve into a network for connected intelligence. Along this line, the proposal to incorporate federated learning into the mobile edge has gained considerable interest in recent years. However, the deployment of federated learning faces substantial challenges as massive resource-limited IoT devices can hardly support on-device model training. This leads to the emergence of split learning (SL) which enables servers to handle the major training workload while still enhancing data privacy. In this article, we offer a brief overview of key advancements in SL and articulate its seamless integration with wireless edge networks. We begin by illustrating the tailored 6G architecture to support edge SL. Then, we examine the critical design issues for edge SL, including innovative resource-efficient learning frameworks and resource management strategies under a single edge server. Additionally, we expand t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#21452;&#31232;&#30095;&#19979;&#38477;&#26041;&#27861;&#35782;&#21035;&#21644;&#34920;&#24449;&#19982;&#20998;&#31867;&#20219;&#21153;&#30456;&#20851;&#30340;&#20462;&#21098;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#23567;&#19982;&#20219;&#21153;&#38590;&#24230;&#21576;&#29616;&#25391;&#33633;&#24577;&#21183;&#65307;&#21516;&#26102;&#65292;&#30456;&#36739;&#20110;&#20840;&#32593;&#32476;&#65292;&#36825;&#20123;&#20462;&#21098;&#27169;&#22411;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#30495;&#23454;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.12190</link><description>&lt;p&gt;
&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#37327;&#21270;&#24425;&#31080;&#20013;&#22870;&#21495;&#30721;&#65306;&#20934;&#30830;&#24615;&#12289;&#26657;&#20934;&#24230;&#21644;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Quantifying lottery tickets under label noise: accuracy, calibration, and complexity. (arXiv:2306.12190v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#21452;&#31232;&#30095;&#19979;&#38477;&#26041;&#27861;&#35782;&#21035;&#21644;&#34920;&#24449;&#19982;&#20998;&#31867;&#20219;&#21153;&#30456;&#20851;&#30340;&#20462;&#21098;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#23567;&#19982;&#20219;&#21153;&#38590;&#24230;&#21576;&#29616;&#25391;&#33633;&#24577;&#21183;&#65307;&#21516;&#26102;&#65292;&#30456;&#36739;&#20110;&#20840;&#32593;&#32476;&#65292;&#36825;&#20123;&#20462;&#21098;&#27169;&#22411;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#30495;&#23454;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20462;&#21098;&#26159;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#35745;&#31639;&#36127;&#25285;&#30340;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#12290;&#21387;&#20498;&#24615;&#30340;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#20462;&#21098;&#27169;&#22411;&#21363;&#20351;&#21482;&#26377;&#26497;&#23569;&#37327;&#30340;&#21442;&#25968;&#65292;&#20173;&#28982;&#33021;&#20445;&#25345;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#23569;&#37327;&#30340;&#24037;&#20316;&#29992;&#20110;&#34920;&#24449;&#25152;&#33719;&#24471;&#30340;&#23567;&#22411;&#20462;&#21098;&#32593;&#32476;&#65292;&#20165;&#20165;&#20381;&#38752;&#31934;&#24230;&#30340;&#24230;&#37327;&#12290;&#26412;&#25991;&#20351;&#29992;&#31232;&#30095;&#21452;&#19979;&#38477;&#26041;&#27861;&#35782;&#21035;&#12289;&#34920;&#24449;&#19982;&#20998;&#31867;&#20219;&#21153;&#30456;&#20851;&#30340;&#20462;&#21098;&#27169;&#22411;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#20219;&#21153;&#65292;&#36845;&#20195;&#24133;&#20540;&#21098;&#26525;(IMP)&#20542;&#21521;&#20110;&#20174;&#22823;&#23567;&#36328;&#24230;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#20840;&#32593;&#32476;&#24320;&#22987;&#25910;&#25947;&#21040;&#21487;&#27604;&#22823;&#23567;&#30340;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21487;&#25511;&#23454;&#39564;&#29615;&#22659;&#20013;&#20998;&#26512;&#26368;&#20339;&#20462;&#21098;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20182;&#20204;&#30340;&#21442;&#25968;&#25968;&#37327;&#21453;&#26144;&#20102;&#20219;&#21153;&#38590;&#24230;&#65292;&#32780;&#19988;&#20182;&#20204;&#27604;&#20840;&#32593;&#32476;&#26356;&#22909;&#30340;&#25429;&#25417;&#21040;&#30495;&#23454;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning deep neural networks is a widely used strategy to alleviate the computational burden in machine learning. Overwhelming empirical evidence suggests that pruned models retain very high accuracy even with a tiny fraction of parameters. However, relatively little work has gone into characterising the small pruned networks obtained, beyond a measure of their accuracy. In this paper, we use the sparse double descent approach to identify univocally and characterise pruned models associated with classification tasks. We observe empirically that, for a given task, iterative magnitude pruning (IMP) tends to converge to networks of comparable sizes even when starting from full networks with sizes ranging over orders of magnitude. We analyse the best pruned models in a controlled experimental setup and show that their number of parameters reflects task difficulty and that they are much better than full networks at capturing the true conditional probability distribution of the labels. On re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#25955;&#24335;DNN&#25163;&#26415;&#26694;&#26550;&#65288;DDS&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#33258;&#36866;&#24212;&#21160;&#24577;&#21327;&#21830;&#65288;R-ADB&#65289;&#26041;&#27861;&#65292;&#20197;&#20415;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#21464;&#21270;&#30340;&#36164;&#28304;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.12185</link><description>&lt;p&gt;
&#33258;&#31169;&#25512;&#29702;&#21152;&#36895;&#19979;&#30340;&#33258;&#36866;&#24212;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25163;&#26415;&#19982;&#25353;&#38656;&#36793;&#32536;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Adaptive DNN Surgery for Selfish Inference Acceleration with On-demand Edge Resource. (arXiv:2306.12185v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#25955;&#24335;DNN&#25163;&#26415;&#26694;&#26550;&#65288;DDS&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#33258;&#36866;&#24212;&#21160;&#24577;&#21327;&#21830;&#65288;R-ADB&#65289;&#26041;&#27861;&#65292;&#20197;&#20415;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#21464;&#21270;&#30340;&#36164;&#28304;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26174;&#33879;&#25552;&#39640;&#20102;&#31227;&#21160;&#35774;&#22791;&#30340;&#26234;&#33021;&#24212;&#29992;&#20934;&#30830;&#24615;&#12290;DNN&#25163;&#26415;&#23558;DNN&#22788;&#29702;&#20998;&#21106;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#26381;&#21153;&#22120;&#20043;&#38388;&#65292;&#21487;&#20197;&#22312;&#31227;&#21160;&#35774;&#22791;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23454;&#26102;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;DNN&#25163;&#26415;&#38754;&#20020;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#30830;&#23450;&#26381;&#21153;&#22120;&#30340;&#26368;&#20339;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#21644;&#30456;&#24212;&#30340;&#20998;&#21306;&#31574;&#30053;&#65292;&#21516;&#26102;&#32771;&#34385;&#25512;&#29702;&#24310;&#36831;&#21644;MEC&#26381;&#21153;&#22120;&#20351;&#29992;&#25104;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#25955;&#24335;DNN&#25163;&#26415;&#65288;DDS&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#20998;&#21306;&#31574;&#30053;&#34920;&#36848;&#20026;&#26368;&#23567;&#21106;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#36164;&#28304;&#33258;&#36866;&#24212;&#21160;&#24577;&#21327;&#21830; (R-ADB) &#26041;&#27861;&#65292;&#20197;&#20415;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#21464;&#21270;&#30340;&#36164;&#28304;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have significantly improved the accuracy of intelligent applications on mobile devices. DNN surgery, which partitions DNN processing between mobile devices and multi-access edge computing (MEC) servers, can enable real-time inference despite the computational limitations of mobile devices. However, DNN surgery faces a critical challenge: determining the optimal computing resource demand from the server and the corresponding partition strategy, while considering both inference latency and MEC server usage costs. This problem is compounded by two factors: (1) the finite computing capacity of the MEC server, which is shared among multiple devices, leading to inter-dependent demands, and (2) the shift in modern DNN architecture from chains to directed acyclic graphs (DAGs), which complicates potential solutions.  In this paper, we introduce a novel Decentralized DNN Surgery (DDS) framework. We formulate the partition strategy as a min-cut and propose a resource 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;&#26174;&#24335;&#35821;&#38899;&#20998;&#31163;&#21644;&#30452;&#25509;&#22312;ASR&#27169;&#22359;&#20013;&#21512;&#24182;&#28151;&#21512;&#35821;&#38899;&#20449;&#24687;&#65292;&#36890;&#36807;&#20132;&#25442;&#36328;&#35828;&#35805;&#32773;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#23618;&#65292;&#23454;&#29616;&#20102;&#22312;SMS-WSJ&#20219;&#21153;&#19978;&#30456;&#23545;&#20110;&#32431;&#27169;&#22359;&#21270;&#35774;&#32622;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;7%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.12173</link><description>&lt;p&gt;
&#32852;&#21512;&#35821;&#38899;&#20998;&#31163;&#19982;&#35782;&#21035;&#30340;&#28151;&#21512;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mixture Encoder for Joint Speech Separation and Recognition. (arXiv:2306.12173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;&#26174;&#24335;&#35821;&#38899;&#20998;&#31163;&#21644;&#30452;&#25509;&#22312;ASR&#27169;&#22359;&#20013;&#21512;&#24182;&#28151;&#21512;&#35821;&#38899;&#20449;&#24687;&#65292;&#36890;&#36807;&#20132;&#25442;&#36328;&#35828;&#35805;&#32773;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#23618;&#65292;&#23454;&#29616;&#20102;&#22312;SMS-WSJ&#20219;&#21153;&#19978;&#30456;&#23545;&#20110;&#32431;&#27169;&#22359;&#21270;&#35774;&#32622;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;7%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35828;&#35805;&#20154;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#38656;&#35201;&#29305;&#23450;&#30340;&#24314;&#27169;&#25216;&#26415;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#27169;&#22359;&#21270;&#21644;&#31471;&#21040;&#31471;&#26041;&#27861;&#12290;&#27169;&#22359;&#21270;&#26041;&#27861;&#20351;&#29992;&#21333;&#35828;&#35805;&#20154;ASR&#31995;&#32479;&#20998;&#31163;&#35828;&#35805;&#20154;&#24182;&#35782;&#21035;&#20182;&#20204;&#12290;&#31471;&#21040;&#31471;&#27169;&#22411;&#30452;&#25509;&#22312;&#19968;&#20010;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#22788;&#29702;&#37325;&#21472;&#30340;&#35821;&#38899;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20171;&#20110;&#20004;&#32773;&#20043;&#38388;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31867;&#20284;&#20110;&#27169;&#22359;&#21270;&#26041;&#27861;&#30340;&#26174;&#24335;&#35821;&#38899;&#20998;&#31163;&#65292;&#20294;&#20063;&#30452;&#25509;&#22312;ASR&#27169;&#22359;&#20013;&#21512;&#24182;&#28151;&#21512;&#35821;&#38899;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#35821;&#38899;&#20998;&#31163;&#22120;&#36896;&#25104;&#30340;&#38169;&#35823;&#20256;&#25773;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#20010;&#20307;&#35828;&#35805;&#20154;&#20449;&#24687;&#30340;&#23618;&#26469;&#20132;&#25442;&#36328;&#35828;&#35805;&#32773;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#36890;&#36807;&#21333;&#29420;&#21644;&#32852;&#21512;&#35757;&#32451;&#38454;&#27573;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;SMS-WSJ&#20219;&#21153;&#19978;&#30456;&#23545;&#20110;&#32431;&#27169;&#22359;&#21270;&#35774;&#32622;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#23454;&#29616;&#20102;7%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-speaker automatic speech recognition (ASR) is crucial for many real-world applications, but it requires dedicated modeling techniques. Existing approaches can be divided into modular and end-to-end methods. Modular approaches separate speakers and recognize each of them with a single-speaker ASR system. End-to-end models process overlapped speech directly in a single, powerful neural network. This work proposes a middle-ground approach that leverages explicit speech separation similarly to the modular approach but also incorporates mixture speech information directly into the ASR module in order to mitigate the propagation of errors made by the speech separator. We also explore a way to exchange cross-speaker context information through a layer that combines information of the individual speakers. Our system is optimized through separate and joint training stages and achieves a relative improvement of 7% in word error rate over a purely modular setup on the SMS-WSJ task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20174;&#20044;&#25176;&#37030;&#30340;&#20154;&#21475;&#36317;&#31163;&#8221;&#65288;PDU&#65289;&#30340;&#21518;&#36873;&#25321;&#31574;&#30053;&#65292;&#29992;&#20110;&#30830;&#23450;&#21644;&#36873;&#25321; Pareto-&#26368;&#20248;&#35299;&#20013;&#30340;&#26368;&#20339;&#35299;&#12290;&#35813;&#26041;&#27861;&#20998;&#26512;&#28857;&#30340;&#20998;&#24067;&#65292;&#36890;&#36807;&#20272;&#35745; PDU &#20998;&#25968;&#30340;&#28857;&#30340;&#24179;&#22343;&#20301;&#32622;&#26469;&#30830;&#23450;&#26368;&#20339;&#35299;&#30340;&#21487;&#33021;&#20301;&#32622;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; PDU &#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.12165</link><description>&lt;p&gt;
&#25628;&#32034;&#21644;&#25512;&#33616;&#20013; Pareto-&#26368;&#20248;&#35299;&#21518;&#36873;&#25321;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Post-hoc Selection of Pareto-Optimal Solutions in Search and Recommendation. (arXiv:2306.12165v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20174;&#20044;&#25176;&#37030;&#30340;&#20154;&#21475;&#36317;&#31163;&#8221;&#65288;PDU&#65289;&#30340;&#21518;&#36873;&#25321;&#31574;&#30053;&#65292;&#29992;&#20110;&#30830;&#23450;&#21644;&#36873;&#25321; Pareto-&#26368;&#20248;&#35299;&#20013;&#30340;&#26368;&#20339;&#35299;&#12290;&#35813;&#26041;&#27861;&#20998;&#26512;&#28857;&#30340;&#20998;&#24067;&#65292;&#36890;&#36807;&#20272;&#35745; PDU &#20998;&#25968;&#30340;&#28857;&#30340;&#24179;&#22343;&#20301;&#32622;&#26469;&#30830;&#23450;&#26368;&#20339;&#35299;&#30340;&#21487;&#33021;&#20301;&#32622;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; PDU &#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#21644;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#20219;&#21153;&#20174;&#22522;&#20110;&#21333;&#19968;&#24230;&#37327;&#35745;&#31639;&#26368;&#32456;&#32467;&#26524;&#30340;&#25490;&#21517;&#36807;&#28193;&#20026;&#22810;&#30446;&#26631;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#20250;&#24471;&#21040;&#19968;&#32452; Pareto-&#26368;&#20248;&#35299;&#65292;&#31216;&#20026; Pareto frontier&#65292;&#20854;&#20013;&#27809;&#26377;&#30446;&#26631;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#32780;&#19981;&#25439;&#23475;&#20854;&#20182;&#30446;&#26631;&#12290;&#21407;&#21017;&#19978;&#65292;Pareto frontier &#19978;&#30340;&#25152;&#26377;&#28857;&#37117;&#26377;&#21487;&#33021;&#20195;&#34920;&#30528;&#22522;&#20110;&#20004;&#20010;&#25110;&#22810;&#20010;&#24230;&#37327;&#30456;&#32467;&#21512;&#36873;&#25321;&#30340;&#26368;&#20339;&#27169;&#22411;&#20505;&#36873;&#32773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20174;&#20044;&#25176;&#37030;&#30340;&#20154;&#21475;&#36317;&#31163;&#8221;&#65288;PDU&#65289;&#30340;&#26032;&#39062;&#21518;&#36873;&#25321;&#31574;&#30053;&#65292;&#37319;&#29992;&#29702;&#35770;&#19978;&#30340;&#27491;&#24403;&#21270;&#25216;&#26415;&#26469;&#30830;&#23450;&#21644;&#36873;&#25321; Pareto-&#26368;&#20248;&#35299;&#20013;&#30340;&#26368;&#20339;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PDU &#36890;&#36807;&#30740;&#31350;&#27599;&#20010;&#28857;&#19982;&#20854;&#20044;&#25176;&#37030;&#28857;&#65288;&#30446;&#26631;&#30340;&#29702;&#24819;&#24615;&#33021;&#65289;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#20998;&#26512;&#28857;&#30340;&#20998;&#24067;&#12290;&#22312;&#19968;&#23450;&#38408;&#20540;&#33539;&#22260;&#20869;&#65292;&#36890;&#36807;&#20272;&#35745; PDU &#20998;&#25968;&#30340;&#28857;&#30340;&#24179;&#22343;&#20301;&#32622;&#26469;&#30830;&#23450;&#26368;&#20339;&#35299;&#30340;&#21487;&#33021;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272; PDU &#24182;&#19982;&#20854;&#20182;&#30693;&#21517;&#30340;&#36873;&#25321;&#31574;&#30053;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126; PDU &#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information Retrieval (IR) and Recommender Systems (RS) tasks are moving from computing a ranking of final results based on a single metric to multi-objective problems. Solving these problems leads to a set of Pareto-optimal solutions, known as Pareto frontier, in which no objective can be further improved without hurting the others. In principle, all the points on the Pareto frontier are potential candidates to represent the best model selected with respect to the combination of two, or more, metrics. To our knowledge, there are no well-recognized strategies to decide which point should be selected on the frontier. In this paper, we propose a novel, post-hoc, theoretically-justified technique, named "Population Distance from Utopia" (PDU), to identify and select the one-best Pareto-optimal solution from the frontier. In detail, PDU analyzes the distribution of the points by investigating how far each point is from its utopia point (the ideal performance for the objectives). The possib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#20266;&#38543;&#26426;&#25237;&#24433;&#21040;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#22312;&#22810;&#20010;&#26377;&#19981;&#21516;&#20915;&#31574;&#36793;&#30028;&#30340;&#35757;&#32451;&#20998;&#31867;&#22120;&#20013;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#26469;&#27979;&#35797;&#36755;&#20837;&#65292;&#33021;&#22815;&#20013;&#21644;&#23545;&#25239;&#25915;&#20987;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#22120;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12161</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#38543;&#26426;&#21270;&#30340;&#23545;&#25239;&#25915;&#20987;&#20013;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks Neutralization via Data Set Randomization. (arXiv:2306.12161v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#20266;&#38543;&#26426;&#25237;&#24433;&#21040;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#22312;&#22810;&#20010;&#26377;&#19981;&#21516;&#20915;&#31574;&#36793;&#30028;&#30340;&#35757;&#32451;&#20998;&#31867;&#22120;&#20013;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#26469;&#27979;&#35797;&#36755;&#20837;&#65292;&#33021;&#22815;&#20013;&#21644;&#23545;&#25239;&#25915;&#20987;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#22120;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#23545;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#26500;&#25104;&#20102;&#20005;&#37325;&#30340;&#23041;&#32961;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#35201;&#20040;&#21482;&#38024;&#23545;&#26576;&#19968;&#29305;&#23450;&#31867;&#22411;&#30340;&#25915;&#20987;&#65292;&#35201;&#20040;&#23481;&#26131;&#21463;&#21040;&#22797;&#26434;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#34429;&#28982;&#30528;&#30524;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#31867;&#22120;&#65292;&#20294;&#19982;&#24341;&#29992;&#31867;&#21035;&#30456;&#20851;&#30340;&#29305;&#24449;&#26159;&#19968;&#33324;&#30340;&#12290;&#23427;&#26893;&#26681;&#20110;&#36229;&#31354;&#38388;&#25237;&#24433;&#65292;&#25552;&#20379;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#20266;&#38543;&#26426;&#25237;&#24433;&#21040;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21019;&#24314;&#20102;&#21508;&#31181;&#25237;&#24433;&#25968;&#25454;&#38598;&#30340;&#19968;&#22871;&#29983;&#25104;&#22120;&#65292;&#27599;&#20010;&#29983;&#25104;&#22120;&#35757;&#32451;&#19968;&#20010;&#29305;&#23450;&#30340;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#24471;&#21040;&#20855;&#26377;&#19981;&#21516;&#20915;&#31574;&#36793;&#30028;&#30340;&#19981;&#21516;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#36873;&#25321;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#27979;&#35797;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20250;&#25439;&#23475;&#23545;&#20110;&#21512;&#27861;&#36755;&#20837;&#30340;&#20934;&#30830;&#24615;&#12290;&#38500;&#20102;&#35814;&#32454;&#38416;&#36848;&#21644;&#25552;&#20379;&#25105;&#20204;&#30340;&#38450;&#24481;&#26426;&#21046;&#30340;&#20840;&#38754;&#29305;&#24449;&#21270;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22235;&#20010;&#23454;&#39564;&#30340;&#27010;&#24565;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks on deep-learning models pose a serious threat to their reliability and security. Existing defense mechanisms are narrow addressing a specific type of attack or being vulnerable to sophisticated attacks. We propose a new defense mechanism that, while being focused on image-based classifiers, is general with respect to the cited category. It is rooted on hyperspace projection. In particular, our solution provides a pseudo-random projection of the original dataset into a new dataset. The proposed defense mechanism creates a set of diverse projected datasets, where each projected dataset is used to train a specific classifier, resulting in different trained classifiers with different decision boundaries. During testing, it randomly selects a classifier to test the input. Our approach does not sacrifice accuracy over legitimate input. Other than detailing and providing a thorough characterization of our defense mechanism, we also provide a proof of concept of using four 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28857;&#26816;&#27979;&#21644;&#20687;&#32032;&#20998;&#21106;&#25216;&#26415;&#32852;&#21512;&#23398;&#20064;&#30340;&#22270;&#24418;&#20998;&#21106;&#26041;&#27861;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#19968;&#20123;&#32570;&#38519;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#30340;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12155</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#38598;&#28857;&#34920;&#31034;&#30340;&#36718;&#24275;&#24863;&#30693;&#22270;&#24418;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Joint Dense-Point Representation for Contour-Aware Graph Segmentation. (arXiv:2306.12155v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12155
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28857;&#26816;&#27979;&#21644;&#20687;&#32032;&#20998;&#21106;&#25216;&#26415;&#32852;&#21512;&#23398;&#20064;&#30340;&#22270;&#24418;&#20998;&#21106;&#26041;&#27861;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#19968;&#20123;&#32570;&#38519;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#30340;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#28857;&#21644;&#20687;&#32032;&#36718;&#24275;&#34920;&#31034;&#65292;&#23558;&#22270;&#24418;&#21644;&#23494;&#38598;&#20998;&#21106;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#21457;&#25381;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#36825;&#26679;&#21487;&#20197;&#35299;&#20915;&#20856;&#22411;&#22270;&#24418;&#20998;&#21106;&#26041;&#27861;&#20013;&#30446;&#26631;&#38169;&#20301;&#38480;&#21046;&#32593;&#32476;&#23398;&#20064;&#21306;&#20998;&#24615;&#39030;&#28857;&#21644;&#36718;&#24275;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32852;&#21512;&#23398;&#20064;&#31574;&#30053;&#20801;&#35768;&#32534;&#30721;&#20016;&#23500;&#21644;&#22810;&#26679;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#21516;&#26102;&#32531;&#35299;&#20102;&#22312;&#22522;&#20110;&#23494;&#38598;&#26041;&#27861;&#20013;&#24120;&#35265;&#30340;&#36718;&#24275;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#20854;&#20013;&#20687;&#32032;&#32423;&#30446;&#26631;&#21487;&#33021;&#20250;&#23548;&#33268;&#35299;&#21078;&#23398;&#19978;&#19981;&#21512;&#29702;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#27491;&#30830;&#39044;&#27979;&#20173;&#33853;&#22312;&#36718;&#24275;&#36793;&#30028;&#19978;&#30340;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#28151;&#21512;&#36718;&#24275;&#36317;&#31163;&#25439;&#22833;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#33016;&#37096;X&#32447;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#23545;&#27604;&#20102;&#22810;&#31181;&#23494;&#38598;&#21644;&#28857;&#26816;&#27979;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20998;&#21106;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26126;&#26174;&#25913;&#21892;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#26159;&#20813;&#36153;&#25552;&#20379;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel methodology that combines graph and dense segmentation techniques by jointly learning both point and pixel contour representations, thereby leveraging the benefits of each approach. This addresses deficiencies in typical graph segmentation methods where misaligned objectives restrict the network from learning discriminative vertex and contour features. Our joint learning strategy allows for rich and diverse semantic features to be encoded, while alleviating common contour stability issues in dense-based approaches, where pixel-level objectives can lead to anatomically implausible topologies. In addition, we identify scenarios where correct predictions that fall on the contour boundary are penalised and address this with a novel hybrid contour distance loss. Our approach is validated on several Chest X-ray datasets, demonstrating clear improvements in segmentation stability and accuracy against a variety of dense- and point-based methods. Our source code is freely ava
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;MRI&#20998;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XAI&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#27604;&#31616;&#21333;&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#37322;&#65292;&#19988;CNN&#30340;&#35299;&#37322;&#33021;&#21147;&#21462;&#20915;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.12150</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#22240;&#32032;&#30740;&#31350;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#35299;&#37322;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Benchmark data to study the influence of pre-training on explanation performance in MR image classification. (arXiv:2306.12150v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;MRI&#20998;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XAI&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#27604;&#31616;&#21333;&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#37322;&#65292;&#19988;CNN&#30340;&#35299;&#37322;&#33021;&#21147;&#21462;&#20915;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#24120;&#24120;&#22312;&#21307;&#23398;&#39044;&#27979;&#20219;&#21153;&#20013;&#34987;&#25104;&#21151;&#22320;&#24212;&#29992;&#65292;&#36890;&#24120;&#19982;&#36801;&#31227;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#26102;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;CNN&#20135;&#29983;&#30340;&#27169;&#22411;&#39640;&#24230;&#22797;&#26434;&#19988;&#36890;&#24120;&#19981;&#25552;&#20379;&#20219;&#20309;&#26377;&#20851;&#20854;&#39044;&#27979;&#26426;&#21046;&#30340;&#20449;&#24687;&#65292;&#36825;&#20419;&#20351;&#20102;&#8220;&#21487;&#35299;&#37322;&#24615;&#8221;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;MRI&#20998;&#31867;&#20219;&#21153;&#20013;&#23450;&#37327;&#35780;&#20272;&#35299;&#37322;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#20102;&#35299;&#36801;&#31227;&#23398;&#20064;&#23545;&#35299;&#37322;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24212;&#29992;&#20110;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;CNN&#30340;&#27969;&#34892;XAI&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#27604;&#31616;&#21333;&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;CNN&#25552;&#20379;&#26377;&#24847;&#20041;&#35299;&#37322;&#30340;&#33021;&#21147;&#20005;&#37325;&#20381;&#36182;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNNs) are frequently and successfully used in medical prediction tasks. They are often used in combination with transfer learning, leading to improved performance when training data for the task are scarce. The resulting models are highly complex and typically do not provide any insight into their predictive mechanisms, motivating the field of 'explainable' artificial intelligence (XAI). However, previous studies have rarely quantitatively evaluated the 'explanation performance' of XAI methods against ground-truth data, and transfer learning and its influence on objective measures of explanation performance has not been investigated. Here, we propose a benchmark dataset that allows for quantifying explanation performance in a realistic magnetic resonance imaging (MRI) classification task. We employ this benchmark to understand the influence of transfer learning on the quality of explanations. Experimental results show that popular XAI methods applied to t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38024;&#32455;&#21147;&#20256;&#24863;&#22120;&#19981;&#19968;&#33268;&#24615;&#34917;&#20607;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#24179;&#28369;&#28388;&#27874;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#26368;&#23567;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#20256;&#24863;&#22120;&#35835;&#25968;&#21644;&#25191;&#34892;&#21147;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2306.12129</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38024;&#32455;&#21147;&#20256;&#24863;&#22120;&#19981;&#19968;&#33268;&#24615;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Based Compensation for Inconsistencies in Knitted Force Sensors. (arXiv:2306.12129v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38024;&#32455;&#21147;&#20256;&#24863;&#22120;&#19981;&#19968;&#33268;&#24615;&#34917;&#20607;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#24179;&#28369;&#28388;&#27874;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#26368;&#23567;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#20256;&#24863;&#22120;&#35835;&#25968;&#21644;&#25191;&#34892;&#21147;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#32455;&#20256;&#24863;&#22120;&#32463;&#24120;&#21463;&#21040;&#22266;&#26377;&#25928;&#24212;&#65288;&#22914;&#20559;&#31227;&#12289;&#26494;&#24347;&#21644;&#28418;&#31227;&#65289;&#30340;&#24433;&#21709;&#32780;&#20135;&#29983;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#23646;&#24615;&#30340;&#32467;&#21512;&#20351;&#24471;&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#21040;&#29289;&#29702;&#25191;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#23567;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#32467;&#21512;&#31616;&#21333;&#39044;&#22788;&#29702;&#26041;&#27861;&#26469;&#36827;&#34892;&#22788;&#29702;&#30340;&#26041;&#27861;&#26469;&#23545;&#25239;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;&#37325;&#26032;&#37319;&#26679;&#36807;&#30340;&#20256;&#24863;&#22120;&#20449;&#21495;&#19978;&#24212;&#29992;&#20102;&#22810;&#20010;&#25351;&#25968;&#24179;&#28369;&#28388;&#27874;&#22120;&#65292;&#20197;&#20135;&#29983;&#20445;&#30041;&#19981;&#21516;&#21382;&#21490;&#20256;&#24863;&#22120;&#25968;&#25454;&#27700;&#24179;&#30340;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#34920;&#31034;&#20197;&#21069;&#20256;&#24863;&#22120;&#25191;&#34892;&#20805;&#20998;&#29366;&#24577;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#19977;&#23618;ANN&#65292;&#24635;&#20849;&#26377;8&#20010;&#31070;&#32463;&#20803;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#20256;&#24863;&#22120;&#35835;&#25968;&#21644;&#25191;&#34892;&#21147;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#23545;&#20110;&#26448;&#26009;&#21644;&#32467;&#26500;&#30456;&#24403;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#20063;&#26159;&#36866;&#29992;&#30340;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#30456;&#20851;&#30340;&#29289;&#29702;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knitted sensors frequently suffer from inconsistencies due to innate effects such as offset, relaxation, and drift. These properties, in combination, make it challenging to reliably map from sensor data to physical actuation. In this paper, we demonstrate a method for counteracting this by applying processing using a minimal artificial neural network (ANN) in combination with straightforward pre-processing. We apply a number of exponential smoothing filters on a re-sampled sensor signal, to produce features that preserve different levels of historical sensor data and, in combination, represent an adequate state of previous sensor actuation. By training a three-layer ANN with a total of 8 neurons, we manage to significantly improve the mapping between sensor reading and actuation force. Our findings also show that our technique translates to sensors of reasonably different composition in terms of material and structure, and it can furthermore be applied to related physical features such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;MultiMon&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#22810;&#27169;&#24577;&#31995;&#32479;&#20013;&#30340;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#25581;&#31034;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;14&#20010;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#27599;&#20010;&#37117;&#30001;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#36755;&#20837;&#32452;&#25104;&#65292;&#36825;&#20123;&#36755;&#20837;&#20250;&#23548;&#33268;&#20854;&#20182;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2306.12105</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#25209;&#37327;&#29983;&#20135;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Mass-Producing Failures of Multimodal Systems with Language Models. (arXiv:2306.12105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;MultiMon&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#22810;&#27169;&#24577;&#31995;&#32479;&#20013;&#30340;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#25581;&#31034;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;14&#20010;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#27599;&#20010;&#37117;&#30001;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#36755;&#20837;&#32452;&#25104;&#65292;&#36825;&#20123;&#36755;&#20837;&#20250;&#23548;&#33268;&#20854;&#20182;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#21487;&#33021;&#20197;&#35780;&#20272;&#20154;&#21592;&#26410;&#26366;&#39044;&#35265;&#30340;&#26041;&#24335;&#22833;&#36133;&#12290;&#20026;&#20102;&#22312;&#37096;&#32626;&#21069;&#25214;&#21040;&#36825;&#20123;&#22833;&#36133;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MultiMon&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;&#24615;&#22833;&#36133;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#25552;&#20379;&#21487;&#25512;&#24191;&#30340;&#12289;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27169;&#22411;&#22833;&#36133;&#27169;&#24335;&#30340;&#20363;&#23376;&#12290;&#20026;&#20102;&#25581;&#31034;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;MultiMon&#20174;&#35821;&#26009;&#24211;&#20013;&#25235;&#21462;&#38169;&#35823;&#21327;&#35758;&#30340;&#31034;&#20363;&#65306;&#36755;&#20837;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65292;&#20294;&#19981;&#24212;&#35813;&#22914;&#27492;&#12290;&#28982;&#21518;&#23427;&#20250;&#28608;&#27963;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-4&#65289;&#26469;&#26597;&#25214;&#31995;&#32479;&#24615;&#22833;&#36133;&#30340;&#27169;&#24335;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#23427;&#20204;&#12290;&#25105;&#20204;&#20351;&#29992;MultiMon&#25214;&#21040;&#20102;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;14&#20010;&#31995;&#32479;&#24615;&#22833;&#36133;&#65288;&#20363;&#22914;&#8220;&#24573;&#30053;&#37327;&#35789;&#8221;&#65292;&#27599;&#20010;&#37117;&#30001;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#36755;&#20837;&#32452;&#25104;&#65288;&#20363;&#22914;&#8220;&#19968;&#20010;&#24102;&#26377;&#19968;&#20123;/&#35768;&#22810;&#20070;&#30340;&#20070;&#26550;&#8221;&#65289;&#12290;&#22240;&#20026;CLIP&#26159;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#65292;&#36825;&#20123;&#36755;&#20837;&#20250;&#23548;&#33268;Midjourney 5.1&#12289;DALL-E&#12289;VideoFusion&#31561;&#31995;&#32479;&#22833;&#36133;&#12290;MultiMon&#20063;&#21487;&#20197;&#25351;&#23548;&#38024;&#23545;&#29305;&#23450;&#29992;&#20363;&#30340;&#30456;&#20851;&#25925;&#38556;&#65292;&#20363;&#22914;&#33258;&#39550;&#36710;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployed multimodal systems can fail in ways that evaluators did not anticipate. In order to find these failures before deployment, we introduce MultiMon, a system that automatically identifies systematic failures -generalizable, natural-language descriptions of patterns of model failures. To uncover systematic failures, MultiMon scrapes a corpus for examples of erroneous agreement: inputs that produce the same output, but should not. It then prompts a language model (e.g., GPT-4) to find systematic patterns of failure and describe them in natural language. We use MultiMon to find 14 systematic failures (e.g., "ignores quantifiers") of the CLIP text-encoder, each comprising hundreds of distinct inputs (e.g., "a shelf with a few/many books"). Because CLIP is the backbone for most state-of-the-art multimodal systems, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion, and others. MultiMon can also steer towards failures relevant to specific use cases, such as self-dri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;CIFAR-10&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21487;&#35757;&#32451;&#21442;&#25968;&#23567;&#20110;5&#30334;&#19975;&#30340;&#25913;&#36827;&#29256;ResNet&#27169;&#22411;&#65292;&#36798;&#21040;&#20102;96.04%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#36828;&#20248;&#20110;&#20854;&#21487;&#35757;&#32451;&#21442;&#25968;&#22823;&#20110;1100&#19975;&#30340;&#22522;&#20934;&#26550;&#26500;ResNet18&#12290;</title><link>http://arxiv.org/abs/2306.12100</link><description>&lt;p&gt;
&#39640;&#25928;ResNets: &#27531;&#24046;&#32593;&#32476;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient ResNets: Residual Network Design. (arXiv:2306.12100v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;CIFAR-10&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21487;&#35757;&#32451;&#21442;&#25968;&#23567;&#20110;5&#30334;&#19975;&#30340;&#25913;&#36827;&#29256;ResNet&#27169;&#22411;&#65292;&#36798;&#21040;&#20102;96.04%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#36828;&#20248;&#20110;&#20854;&#21487;&#35757;&#32451;&#21442;&#25968;&#22823;&#20110;1100&#19975;&#30340;&#22522;&#20934;&#26550;&#26500;ResNet18&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ResNets&#65288;&#27531;&#24046;&#32593;&#32476;&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20043;&#19968;&#12290;&#26412;&#39033;&#30446;&#35774;&#35745;&#21644;&#35757;&#32451;&#20102;&#19968;&#20010;&#20462;&#25913;&#21518;&#30340;ResNet&#27169;&#22411;&#65292;&#29992;&#20110;CIFAR-10&#22270;&#20687;&#20998;&#31867;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#23558;&#25105;&#20204;&#30340;ResNet&#27169;&#22411;&#22823;&#23567;&#20445;&#25345;&#22312;&#25351;&#23450;&#30340;5&#30334;&#19975;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#22266;&#23450;&#39044;&#31639;&#20197;&#19979;&#30340;&#21516;&#26102;&#65292;&#26368;&#22823;&#21270;&#22312;CIFAR-10&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#27169;&#22411;&#22823;&#23567;&#36890;&#24120;&#20316;&#20026;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#36827;&#34892;&#34913;&#37327;&#65292;&#24403;&#27169;&#22411;&#38656;&#35201;&#23384;&#20648;&#22312;&#23384;&#20648;&#23481;&#37327;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#26102;&#65288;&#20363;&#22914;IoT/edge&#35774;&#22791;&#65289;&#65292;&#36825;&#19968;&#28857;&#24456;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20302;&#20110;5&#30334;&#19975;&#21442;&#25968;&#30340;&#27531;&#24046;&#32593;&#32476;&#35774;&#35745;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;ResNet&#22312;CIFAR-10&#19978;&#36798;&#21040;&#20102;96.04%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#36825;&#27604;ResNet18&#65288;&#21487;&#35757;&#32451;&#21442;&#25968;&#22823;&#20110;1100&#19975;&#65289;&#39640;&#24471;&#22810;&#65292;&#24403;&#37197;&#22791;&#20102;&#19968;&#20123;&#35757;&#32451;&#31574;&#30053;&#21644;&#21512;&#36866;&#30340;ResNet&#36229;&#21442;&#25968;&#26102;&#12290;&#27169;&#22411;&#21644;&#20195;&#30721;&#21487;&#22312;https://github.com/Nikunj-Gupta/Efficient_ResN&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
ResNets (or Residual Networks) are one of the most commonly used models for image classification tasks. In this project, we design and train a modified ResNet model for CIFAR-10 image classification. In particular, we aimed at maximizing the test accuracy on the CIFAR-10 benchmark while keeping the size of our ResNet model under the specified fixed budget of 5 million trainable parameters. Model size, typically measured as the number of trainable parameters, is important when models need to be stored on devices with limited storage capacity (e.g. IoT/edge devices). In this article, we present our residual network design which has less than 5 million parameters. We show that our ResNet achieves a test accuracy of 96.04% on CIFAR-10 which is much higher than ResNet18 (which has greater than 11 million trainable parameters) when equipped with a number of training strategies and suitable ResNet hyperparameters. Models and code are available at https://github.com/Nikunj-Gupta/Efficient_ResN
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MSW-Transformer&#30340;&#21333;&#23618;&#21464;&#21387;&#22120;&#32593;&#32476;&#65292;&#20351;&#29992;&#22810;&#31383;&#21475;&#28369;&#21160;&#27880;&#24847;&#26426;&#21046;&#21644;&#19981;&#21516;&#23610;&#24230;&#26469;&#25429;&#25417;&#19981;&#21516;&#32500;&#24230;&#30340;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#20174;12&#23548;ECG&#20449;&#21495;&#20013;&#21306;&#20998;&#30149;&#29702;&#29305;&#24449;&#24046;&#24322;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12098</link><description>&lt;p&gt;
MSW-Transformer: &#22810;&#23610;&#24230;&#31227;&#20301;&#31383;&#21475;&#21464;&#21387;&#22120;&#32593;&#32476;&#29992;&#20110;12&#23548;&#24515;&#30005;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MSW-Transformer: Multi-Scale Shifted Windows Transformer Networks for 12-Lead ECG Classification. (arXiv:2306.12098v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12098
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MSW-Transformer&#30340;&#21333;&#23618;&#21464;&#21387;&#22120;&#32593;&#32476;&#65292;&#20351;&#29992;&#22810;&#31383;&#21475;&#28369;&#21160;&#27880;&#24847;&#26426;&#21046;&#21644;&#19981;&#21516;&#23610;&#24230;&#26469;&#25429;&#25417;&#19981;&#21516;&#32500;&#24230;&#30340;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#20174;12&#23548;ECG&#20449;&#21495;&#20013;&#21306;&#20998;&#30149;&#29702;&#29305;&#24449;&#24046;&#24322;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20998;&#31867;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#23545;&#20110;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#26089;&#26399;&#39044;&#38450;&#21644;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;ECG&#20449;&#21495;&#21487;&#29992;&#20110;&#35786;&#26029;&#21508;&#31181;&#30142;&#30149;&#65292;&#20294;&#20854;&#30149;&#29702;&#29305;&#24449;&#20165;&#23384;&#22312;&#26368;&#23567;&#24046;&#24322;&#65292;&#36825;&#32473;&#33258;&#21160;&#20998;&#31867;&#27169;&#22411;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#21462;ECG&#20449;&#21495;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#21487;&#33021;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#19981;&#21516;&#30142;&#30149;&#30340;&#30149;&#29702;&#29305;&#24449;&#24046;&#24322;&#12290;&#21464;&#21387;&#22120;&#32593;&#32476;&#22312;&#24207;&#21015;&#25968;&#25454;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#23436;&#25972;&#30340;&#32593;&#32476;&#22797;&#26434;&#65292;&#20381;&#36182;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#23610;&#24230;&#31227;&#20301;&#31383;&#21475;&#21464;&#21387;&#22120;&#32593;&#32476;&#65288;MSW-Transformer&#65289;&#30340;&#21333;&#23618;Transformer&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#19981;&#21516;&#23610;&#24230;&#30340;&#22810;&#31383;&#21475;&#28369;&#21160;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#19981;&#21516;&#32500;&#24230;&#30340;&#29305;&#24449;&#12290;&#33258;&#25105;&#27880;&#24847;&#34987;&#38480;&#21046;&#22312;&#38750;&#37325;&#21472;&#21644;&#31227;&#20301;&#31383;&#21475;&#20869;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;12&#23548;ECG&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;MSW-Transformer&#22312;&#25429;&#25417;&#30149;&#29702;&#29305;&#24449;&#24046;&#24322;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic classification of electrocardiogram (ECG) signals plays a crucial role in the early prevention and diagnosis of cardiovascular diseases. While ECG signals can be used for the diagnosis of various diseases, their pathological characteristics exhibit minimal variations, posing a challenge to automatic classification models. Existing methods primarily utilize convolutional neural networks to extract ECG signal features for classification, which may not fully capture the pathological feature differences of different diseases. Transformer networks have advantages in feature extraction for sequence data, but the complete network is complex and relies on large-scale datasets. To address these challenges, we propose a single-layer Transformer network called Multi-Scale Shifted Windows Transformer Networks (MSW-Transformer), which uses a multi-window sliding attention mechanism at different scales to capture features in different dimensions. The self-attention is restricted to non-ove
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33437;&#21152;&#21733;&#30340;&#20986;&#31199;&#36710;&#25968;&#25454;&#65292;&#36890;&#36807;&#32858;&#31867;&#25216;&#26415;&#20998;&#26512;&#31038;&#21306;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#20844;&#20849;&#20132;&#36890;&#21457;&#23637;&#20197;&#21450;&#20132;&#36890;&#25110;&#36947;&#36335;&#27745;&#26579;&#32531;&#35299;&#24037;&#20316;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.12094</link><description>&lt;p&gt;
&#29702;&#35299;&#33437;&#21152;&#21733;&#30340;&#20154;&#31867;&#31227;&#21160;&#27169;&#24335;&#65306;&#20351;&#29992;&#32858;&#31867;&#25216;&#26415;&#20998;&#26512;&#20986;&#31199;&#36710;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Understanding human mobility patterns in Chicago: an analysis of taxi data using clustering techniques. (arXiv:2306.12094v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33437;&#21152;&#21733;&#30340;&#20986;&#31199;&#36710;&#25968;&#25454;&#65292;&#36890;&#36807;&#32858;&#31867;&#25216;&#26415;&#20998;&#26512;&#31038;&#21306;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#20844;&#20849;&#20132;&#36890;&#21457;&#23637;&#20197;&#21450;&#20132;&#36890;&#25110;&#36947;&#36335;&#27745;&#26579;&#32531;&#35299;&#24037;&#20316;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#31227;&#21160;&#27169;&#24335;&#22312;&#22478;&#24066;&#35268;&#21010;&#12289;&#20844;&#20849;&#21355;&#29983;&#21644;&#25919;&#27835;&#32452;&#32455;&#31561;&#24212;&#29992;&#20013;&#37117;&#24456;&#37325;&#35201;&#12290;&#20986;&#31199;&#36710;&#25968;&#25454;&#26159;&#20154;&#31867;&#31227;&#21160;&#30340;&#20016;&#23500;&#25968;&#25454;&#26469;&#28304;&#12290;&#20197;&#33437;&#21152;&#21733;&#20026;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;2016&#24180;&#20986;&#31199;&#36710;&#34892;&#31243;&#30340;&#25968;&#25454;&#65292;&#26088;&#22312;&#20102;&#35299;&#21508;&#20010;&#31038;&#21306;&#20043;&#38388;&#30340;&#30456;&#20114;&#32852;&#31995;&#12290;&#35813;&#20998;&#26512;&#23558;&#25552;&#20379;&#20154;&#20204;&#20351;&#29992;&#20986;&#31199;&#36710;&#26469;&#24448;&#21738;&#20123;&#31038;&#21306;&#30340;&#20449;&#24687;&#65292;&#20026;&#26032;&#30340;&#20844;&#20849;&#20132;&#36890;&#21457;&#23637;&#25552;&#20379;&#20102;&#37325;&#35201;&#25351;&#23548;&#12290;&#27492;&#22806;&#65292;&#35813;&#20998;&#26512;&#36824;&#23558;&#32472;&#21046;&#20132;&#36890;&#24490;&#29615;&#27169;&#24335;&#22270;&#65292;&#20102;&#35299;&#20154;&#20204;&#20174;&#22478;&#24066;&#30340;&#21738;&#20010;&#22320;&#26041;&#20986;&#21457;&#20197;&#21450;&#20182;&#20204;&#21069;&#24448;&#30340;&#30446;&#30340;&#22320;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25351;&#23548;&#20132;&#36890;&#25110;&#36947;&#36335;&#27745;&#26579;&#30340;&#32531;&#35299;&#24037;&#20316;&#12290;&#23545;&#20110;&#31532;&#19968;&#20010;&#24212;&#29992;&#65292;&#23558;&#25968;&#25454;&#34920;&#31034;&#20026;&#26080;&#21521;&#22270;&#23601;&#36275;&#22815;&#20102;&#12290;&#36816;&#36755;&#32447;&#36335;&#21521;&#20004;&#20010;&#26041;&#21521;&#24310;&#20280;&#65292;&#22240;&#27492;&#20165;&#30693;&#36947;&#21738;&#20123;&#31038;&#21306;&#20043;&#38388;&#20986;&#31199;&#36710;&#30340;&#20351;&#29992;&#29575;&#36739;&#39640;&#23601;&#36275;&#20197;&#20026;&#20844;&#20247;&#20132;&#36890;&#21457;&#23637;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding human mobility patterns is important in applications as diverse as urban planning, public health, and political organizing. One rich source of data on human mobility is taxi ride data. Using the city of Chicago as a case study, we examine data from taxi rides in 2016 with the goal of understanding how neighborhoods are interconnected. This analysis will provide a sense of which neighborhoods individuals are using taxis to travel between, suggesting regions to focus new public transit development efforts. Additionally, this analysis will map traffic circulation patterns and provide an understanding of where in the city people are traveling from and where they are heading to perhaps informing traffic or road pollution mitigation efforts. For the first application, representing the data as an undirected graph will suffice. Transit lines run in both directions so simply a knowledge of which neighborhoods have high rates of taxi travel between them provides an argument for p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22810;&#31181;&#36793;&#32536;&#35774;&#22791;&#19978;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#65292;&#21457;&#29616;Google&#24179;&#21488;&#23545;&#20110;&#26032;&#22411;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#32780;Intel Neural Stick&#26159;&#26368;&#36890;&#29992;&#30340;&#21152;&#36895;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.12093</link><description>&lt;p&gt;
&#36793;&#32536;&#35774;&#22791;&#25512;&#29702;&#24615;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Edge Devices Inference Performance Comparison. (arXiv:2306.12093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22810;&#31181;&#36793;&#32536;&#35774;&#22791;&#19978;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#65292;&#21457;&#29616;Google&#24179;&#21488;&#23545;&#20110;&#26032;&#22411;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#32780;Intel Neural Stick&#26159;&#26368;&#36890;&#29992;&#30340;&#21152;&#36895;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;MobileNet&#23478;&#26063;&#12289;EfficientNet V1&#21644;V2&#23478;&#26063;&#12289;VGG&#27169;&#22411;&#12289;Resnet&#23478;&#26063;&#21644;InceptionV3&#22312;&#22235;&#20010;&#36793;&#32536;&#24179;&#21488;&#19978;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#24179;&#21488;&#21253;&#25324;NVIDIA Jetson Nano&#12289;Intel Neural Stick&#12289;Google Coral USB Dongle&#21644;Google Coral PCIe&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#22810;&#31181;&#35774;&#32622;&#19979;&#25152;&#36848;&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#22312;&#36755;&#20837;&#22823;&#23567;&#12289;&#20998;&#31867;&#22836;&#30340;&#23384;&#22312;&#12289;&#20854;&#22823;&#23567;&#20197;&#21450;&#27169;&#22411;&#35268;&#27169;&#30340;&#24433;&#21709;&#19979;&#36827;&#34892;&#20998;&#26512;&#12290;&#30001;&#20110;&#22312;&#25972;&#20010;&#34892;&#19994;&#20013;&#65292;&#36825;&#20123;&#26550;&#26500;&#20027;&#35201;&#29992;&#20316;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20998;&#26512;&#23427;&#20204;&#30340;&#29305;&#24449;&#25552;&#21462;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Google&#24179;&#21488;&#25552;&#20379;&#20102;&#26368;&#24555;&#30340;&#24179;&#22343;&#25512;&#29702;&#26102;&#38388;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20687;MobileNet&#25110;EfficientNet&#23478;&#26063;&#36825;&#26679;&#30340;&#36739;&#26032;&#27169;&#22411;&#65292;&#32780;Intel Neural Stick&#26159;&#26368;&#36890;&#29992;&#30340;&#21152;&#36895;&#22120;&#65292;&#21487;&#20197;&#36816;&#34892;&#22823;&#22810;&#25968;&#26550;&#26500;&#12290;&#36825;&#20123;&#32467;&#26524;&#24212;&#20026;AI&#36793;&#32536;&#31995;&#32479;&#24320;&#21457;&#30340;&#24037;&#31243;&#24072;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the inference time of the MobileNet family, EfficientNet V1 and V2 family, VGG models, Resnet family, and InceptionV3 on four edge platforms. Specifically NVIDIA Jetson Nano, Intel Neural Stick, Google Coral USB Dongle, and Google Coral PCIe. Our main contribution is a thorough analysis of the aforementioned models in multiple settings, especially as a function of input size, the presence of the classification head, its size, and the scale of the model. Since throughout the industry, those architectures are mainly utilized as feature extractors we put our main focus on analyzing them as such. We show that Google platforms offer the fastest average inference time, especially for newer models like MobileNet or EfficientNet family, while Intel Neural Stick is the most universal accelerator allowing to run most architectures. These results should provide guidance for engineers in the early stages of AI edge systems development. All of them are accessible at htt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;DropEdge++&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#22522;&#20110;&#23618;&#30340;&#37319;&#26679;&#22120;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#37319;&#26679;&#22120;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20174;&#24213;&#23618;&#37319;&#26679;&#36793;&#27604;&#36880;&#28176;&#20943;&#23569;&#30340;&#37319;&#26679;&#36793;&#20197;&#21450;DropEdge&#26356;&#33021;&#25552;&#39640;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#36825;&#19968;&#29616;&#35937;&#19982;&#36807;&#24230;&#24179;&#28369;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2306.12091</link><description>&lt;p&gt;
&#38024;&#23545;&#28145;&#24230;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#32467;&#26500;&#24863;&#30693;DropEdge&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Structure-Aware DropEdge Towards Deep Graph Convolutional Networks. (arXiv:2306.12091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;DropEdge++&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#22522;&#20110;&#23618;&#30340;&#37319;&#26679;&#22120;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#37319;&#26679;&#22120;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20174;&#24213;&#23618;&#37319;&#26679;&#36793;&#27604;&#36880;&#28176;&#20943;&#23569;&#30340;&#37319;&#26679;&#36793;&#20197;&#21450;DropEdge&#26356;&#33021;&#25552;&#39640;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#36825;&#19968;&#29616;&#35937;&#19982;&#36807;&#24230;&#24179;&#28369;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#22534;&#31215;&#22810;&#20010;&#23618;&#26102;&#65292;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#30340;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#23548;&#33268;&#28145;&#23618;GCNs&#22833;&#36133;&#30340;&#20027;&#35201;&#22240;&#32032;&#22312;&#20110;&#36807;&#24230;&#24179;&#28369;&#65292;&#38543;&#30528;&#32593;&#32476;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#20351;&#36755;&#20986;&#30340;&#32593;&#32476;&#19982;&#36755;&#20837;&#38548;&#31163;&#65292;&#21066;&#24369;&#20102;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35757;&#32451;&#24615;&#12290;&#26412;&#25991;&#20174;DropEdge&#36825;&#19968;&#29616;&#26377;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#20837;&#25163;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;DropEdge++&#12290;&#19982;DropEdge&#30456;&#27604;&#65292;DropEdge++&#20855;&#26377;&#20004;&#20010;&#32467;&#26500;&#24863;&#30693;&#37319;&#26679;&#22120;&#65306;&#22522;&#20110;&#23618;&#30340;&#37319;&#26679;&#22120;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#37319;&#26679;&#22120;&#12290;&#20851;&#20110;&#22522;&#20110;&#23618;&#30340;&#37319;&#26679;&#22120;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36234;&#26469;&#36234;&#22810;&#22320;&#20174;&#24213;&#23618;&#37319;&#26679;&#36793;&#27604;&#36880;&#28176;&#20943;&#23569;&#30340;&#37319;&#26679;&#36793;&#20197;&#21450;DropEdge&#26356;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#19982;&#36807;&#24230;&#24179;&#28369;&#23494;&#20999;&#30456;&#20851;&#30340;&#25351;&#26631;&#8212;&#8212;&#24179;&#22343;&#36793;&#25968;(MEN)&#26469;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;&#23545;&#20110;&#22522;&#20110;&#29305;&#24449;&#30340;&#37319;&#26679;&#22120;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;DropEdge++&#30340;&#20854;&#20313;&#37096;&#20998;&#30456;&#32467;&#21512;...
&lt;/p&gt;
&lt;p&gt;
It has been discovered that Graph Convolutional Networks (GCNs) encounter a remarkable drop in performance when multiple layers are piled up. The main factor that accounts for why deep GCNs fail lies in over-smoothing, which isolates the network output from the input with the increase of network depth, weakening expressivity and trainability. In this paper, we start by investigating refined measures upon DropEdge -- an existing simple yet effective technique to relieve over-smoothing. We term our method as DropEdge++ for its two structure-aware samplers in contrast to DropEdge: layer-dependent sampler and feature-dependent sampler. Regarding the layer-dependent sampler, we interestingly find that increasingly sampling edges from the bottom layer yields superior performance than the decreasing counterpart as well as DropEdge. We theoretically reveal this phenomenon with Mean-Edge-Number (MEN), a metric closely related to over-smoothing. For the feature-dependent sampler, we associate th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;FedINIBoost&#65292;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#21305;&#37197;&#26500;&#24314;&#26377;&#25928;&#30340;&#25552;&#21462;&#27169;&#22359;&#65292;&#22312;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.12088</link><description>&lt;p&gt;
&#19968;&#31181;&#20943;&#23569;&#32852;&#37030;&#23398;&#20064;&#36890;&#20449;&#30340;&#39640;&#25928;&#34394;&#25311;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning. (arXiv:2306.12088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;FedINIBoost&#65292;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#21305;&#37197;&#26500;&#24314;&#26377;&#25928;&#30340;&#25552;&#21462;&#27169;&#22359;&#65292;&#22312;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#24320;&#38144;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#19968;&#20123;&#32463;&#20856;&#30340;&#26041;&#26696;&#20551;&#35774;&#26381;&#21153;&#22120;&#21487;&#20197;&#20174;&#26412;&#22320;&#27169;&#22411;&#20013;&#25552;&#21462;&#21442;&#19982;&#32773;&#35757;&#32451;&#25968;&#25454;&#30340;&#36741;&#21161;&#20449;&#24687;&#26469;&#26500;&#24314;&#20013;&#22830;&#34394;&#25311;&#25968;&#25454;&#38598;&#12290;&#26381;&#21153;&#22120;&#20351;&#29992;&#34394;&#25311;&#25968;&#25454;&#38598;&#26469;&#24494;&#35843;&#32858;&#21512;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#20197;&#22312;&#36739;&#23569;&#30340;&#36890;&#20449;&#36718;&#27425;&#20869;&#36798;&#21040;&#30446;&#26631;&#27979;&#35797;&#31934;&#24230;&#12290;&#26412;&#25991;&#23558;&#19978;&#36848;&#35299;&#20915;&#26041;&#26696;&#27010;&#25324;&#20026;&#22522;&#20110;&#25968;&#25454;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#25552;&#20986;&#26694;&#26550;&#30340;&#20851;&#38190;&#26159;&#35774;&#35745;&#19968;&#20010;&#26377;&#25928;&#30340;&#25552;&#21462;&#27169;&#22359;&#65288;EM&#65289;&#65292;&#23427;&#30830;&#20445;&#34394;&#25311;&#25968;&#25454;&#38598;&#23545;&#24494;&#35843;&#32858;&#21512;&#30340;&#20840;&#23616;&#27169;&#22411;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#29983;&#25104;&#22120;&#26469;&#35774;&#35745;EM&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;FedINIBoost&#20511;&#37492;&#20102;&#26799;&#24230;&#21305;&#37197;&#30340;&#24605;&#24819;&#26469;&#26500;&#24314;EM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FedINIBoost&#22312;&#27599;&#20010;&#36890;&#20449;&#36718;&#27425;&#30340;&#27599;&#20010;&#21442;&#19982;&#32773;&#20013;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#26500;&#24314;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20195;&#29702;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#26381;&#21153;&#22120;&#32858;&#21512;&#25152;&#26377;&#30340;&#20195;&#29702;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#20013;&#22830;&#34394;&#25311;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication overhead is one of the major challenges in Federated Learning(FL). A few classical schemes assume the server can extract the auxiliary information about training data of the participants from the local models to construct a central dummy dataset. The server uses the dummy dataset to finetune aggregated global model to achieve the target test accuracy in fewer communication rounds. In this paper, we summarize the above solutions into a data-based communication-efficient FL framework. The key of the proposed framework is to design an efficient extraction module(EM) which ensures the dummy dataset has a positive effect on finetuning aggregated global model. Different from the existing methods that use generator to design EM, our proposed method, FedINIBoost borrows the idea of gradient match to construct EM. Specifically, FedINIBoost builds a proxy dataset of the real dataset in two steps for each participant at each communication round. Then the server aggregates all the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#21508;&#31181;&#35757;&#32451;&#21464;&#37327;(&#21253;&#25324;&#19981;&#21516;&#30340;SSCL&#31639;&#27861;&#12289;&#23398;&#20064;&#31574;&#30053;&#65292;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;)&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#20102;SSCL&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24433;&#21709;&#21450;&#20855;&#20307;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2306.12086</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Constitutes Good Contrastive Learning in Time-Series Forecasting?. (arXiv:2306.12086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#21508;&#31181;&#35757;&#32451;&#21464;&#37327;(&#21253;&#25324;&#19981;&#21516;&#30340;SSCL&#31639;&#27861;&#12289;&#23398;&#20064;&#31574;&#30053;&#65292;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;)&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#20102;SSCL&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24433;&#21709;&#21450;&#20855;&#20307;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;(Self-Supervised Contrastive Learning, SSCL)&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;(&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;)&#30340;&#24341;&#20837;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#30340;&#28508;&#22312;&#20248;&#21183;&#65292;SSCL&#20351;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#20102;&#34920;&#31034;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;&#23613;&#31649;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26174;&#33879;&#30340;&#24046;&#36317;&#8212;&#8212;&#21363;&#25105;&#20204;&#23545;&#20110;&#19981;&#21516;&#30340;SSCL&#31574;&#30053;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#20197;&#21450;SSCL&#25152;&#24102;&#26469;&#30340;&#20855;&#20307;&#22909;&#22788;&#29702;&#35299;&#19981;&#36275;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#21508;&#31181;&#35757;&#32451;&#21464;&#37327;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#26469;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#65292;&#20854;&#20013;&#21253;&#25324;&#19981;&#21516;&#30340;SSCL&#31639;&#27861;&#12289;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;SSCL&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#32972;&#26223;&#19979;&#24102;&#26469;&#30340;&#25913;&#36827;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#32463;&#39564;&#24863;&#21463;&#37326;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the introduction of self-supervised contrastive learning (SSCL) has demonstrated remarkable improvements in representation learning across various domains, including natural language processing and computer vision. By leveraging the inherent benefits of self-supervision, SSCL enables the pre-training of representation models using vast amounts of unlabeled data. Despite these advances, there remains a significant gap in understanding the impact of different SSCL strategies on time series forecasting performance, as well as the specific benefits that SSCL can bring. This paper aims to address these gaps by conducting a comprehensive analysis of the effectiveness of various training variables, including different SSCL algorithms, learning strategies, model architectures, and their interplay. Additionally, to gain deeper insights into the improvements brought about by SSCL in the context of time-series forecasting, a qualitative analysis of the empirical receptive field i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLGo&#30340;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#29992;&#20110;&#23450;&#21046;&#27169;&#25311;&#20197;&#28385;&#36275;&#29305;&#23450;&#24212;&#29992;&#29615;&#22659;&#30340;&#38656;&#27714;&#12290;&#24179;&#21488;&#21253;&#21547;40+&#22522;&#20934;&#12289;20+&#31639;&#27861;&#21644;2&#20010;&#31995;&#32479;&#27169;&#25311;&#22120;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;API&#20197;&#29992;&#20110;&#24555;&#36895;&#24320;&#21457;&#26032;&#30340;&#25554;&#20214;&#65292;&#20197;&#25552;&#39640;&#20849;&#20139;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12079</link><description>&lt;p&gt;
FLGo&#65306;&#19968;&#20010;&#20840;&#21487;&#23450;&#21046;&#30340;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
FLGo: A Fully Customizable Federated Learning Platform. (arXiv:2306.12079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12079
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLGo&#30340;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#29992;&#20110;&#23450;&#21046;&#27169;&#25311;&#20197;&#28385;&#36275;&#29305;&#23450;&#24212;&#29992;&#29615;&#22659;&#30340;&#38656;&#27714;&#12290;&#24179;&#21488;&#21253;&#21547;40+&#22522;&#20934;&#12289;20+&#31639;&#27861;&#21644;2&#20010;&#31995;&#32479;&#27169;&#25311;&#22120;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;API&#20197;&#29992;&#20110;&#24555;&#36895;&#24320;&#21457;&#26032;&#30340;&#25554;&#20214;&#65292;&#20197;&#25552;&#39640;&#20849;&#20139;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#29289;&#32852;&#32593;&#31561;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#22522;&#20934;&#26469;&#35780;&#20272;&#32852;&#37030;&#23398;&#20064;&#22312;&#30495;&#23454;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23450;&#21046;&#27169;&#25311;&#20197;&#36866;&#24212;&#29305;&#23450;&#24212;&#29992;&#29615;&#22659;&#12289;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#30340;&#36807;&#31243;&#36890;&#24120;&#20173;&#28982;&#36807;&#20110;&#22797;&#26434;&#12290;&#36825;&#32473;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#22312;&#25506;&#32034;&#32852;&#37030;&#23398;&#20064;&#30340;&#20351;&#29992;&#26102;&#24102;&#26469;&#20102;&#37325;&#37325;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#20063;&#21066;&#24369;&#20102;&#20195;&#30721;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#38388;&#30340;&#20849;&#20139;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20043;&#20026;FLGo&#30340;&#26032;&#22411;&#36731;&#37327;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#20197;&#20419;&#36827;&#39640;&#24230;&#21487;&#20849;&#20139;&#30340;&#36328;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#24179;&#21488;&#25552;&#20379;&#20102;&#36229;&#36807;40&#31181;&#22522;&#20934;&#12289;20&#31181;&#31639;&#27861;&#21644;2&#31181;&#31995;&#32479;&#27169;&#25311;&#22120;&#20316;&#20026;&#24320;&#31665;&#21363;&#29992;&#30340;&#25554;&#20214;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;API&#65292;&#29992;&#20110;&#24555;&#36895;&#23450;&#21046;&#26032;&#30340;&#25554;&#20214;&#65292;&#20351;&#20854;&#21487;&#20197;&#36731;&#26494;&#20849;&#20139;&#21644;&#37325;&#22797;&#20351;&#29992;&#65292;&#20197;&#25552;&#39640;&#21487;&#20877;&#29616;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#24179;&#21488;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;&#35786;&#26029;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has found numerous applications in healthcare, finance, and IoT scenarios. Many existing FL frameworks offer a range of benchmarks to evaluate the performance of FL under realistic conditions. However, the process of customizing simulations to accommodate application-specific settings, data heterogeneity, and system heterogeneity typically remains unnecessarily complicated. This creates significant hurdles for traditional ML researchers in exploring the usage of FL, while also compromising the shareability of codes across FL frameworks. To address this issue, we propose a novel lightweight FL platform called FLGo, to facilitate cross-application FL studies with a high degree of shareability. Our platform offers 40+ benchmarks, 20+ algorithms, and 2 system simulators as out-of-the-box plugins. We also provide user-friendly APIs for quickly customizing new plugins that can be readily shared and reused for improved reproducibility. Finally, we develop a range of ex
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#39640;&#32500;&#32463;&#39564;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#19981;&#21464;&#20998;&#35299;&#21644;&#65288;&#31354;&#38388;-&#65289;&#26102;&#38388;&#21464;&#25442;&#22120;&#65292;&#33021;&#22815;&#33258;&#21160;&#20998;&#31163;&#20986;&#19968;&#31181;&#23454;&#20363;&#29305;&#23450;&#30340;&#32534;&#30721;&#21644;&#19968;&#31181;&#28508;&#22312;&#21160;&#24577;&#27169;&#22411;&#65292;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#36890;&#36807;&#22312;&#20219;&#24847;&#36830;&#32493;&#26102;&#38388;&#25512;&#26029;&#31995;&#32479;&#34892;&#20026;&#65292;&#19982;&#26174;&#24335;&#30340;&#31070;&#32463;ODE&#20844;&#24335;&#19981;&#21516;&#65292;&#39640;&#25928;&#21487;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.12077</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#21464;&#20998;&#35299;&#21644;&#65288;&#31354;&#38388;-&#65289;&#26102;&#38388;&#21464;&#25442;&#22120;&#23398;&#20064;&#28508;&#22312;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Learning Latent Dynamics via Invariant Decomposition and (Spatio-)Temporal Transformers. (arXiv:2306.12077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#39640;&#32500;&#32463;&#39564;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#19981;&#21464;&#20998;&#35299;&#21644;&#65288;&#31354;&#38388;-&#65289;&#26102;&#38388;&#21464;&#25442;&#22120;&#65292;&#33021;&#22815;&#33258;&#21160;&#20998;&#31163;&#20986;&#19968;&#31181;&#23454;&#20363;&#29305;&#23450;&#30340;&#32534;&#30721;&#21644;&#19968;&#31181;&#28508;&#22312;&#21160;&#24577;&#27169;&#22411;&#65292;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#36890;&#36807;&#22312;&#20219;&#24847;&#36830;&#32493;&#26102;&#38388;&#25512;&#26029;&#31995;&#32479;&#34892;&#20026;&#65292;&#19982;&#26174;&#24335;&#30340;&#31070;&#32463;ODE&#20844;&#24335;&#19981;&#21516;&#65292;&#39640;&#25928;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#65288;&#31354;&#38388;-&#65289;&#26102;&#38388;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#35774;&#35745;&#26088;&#22312;&#24378;&#21046;&#23454;&#26045;&#26576;&#20123;&#31185;&#23398;&#21160;&#26426;&#30340;&#19981;&#21464;&#24615;&#30340;&#26694;&#26550;&#65292;&#20174;&#39640;&#32500;&#32463;&#39564;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36825;&#26679;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;&#26469;&#33258;&#19968;&#20010;&#31995;&#32479;&#30340;&#22810;&#20010;&#19981;&#21516;&#23454;&#20363;&#30340;&#25968;&#25454;&#21487;&#29992;&#65292;&#20854;&#28508;&#22312;&#21160;&#21147;&#23398;&#27169;&#22411;&#22312;&#24320;&#22987;&#26102;&#23436;&#20840;&#19981;&#30693;&#36947;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#20998;&#31163;&#65292;&#21363;&#19968;&#31181;&#23454;&#20363;&#29305;&#23450;&#30340;&#32534;&#30721;&#65288;&#25429;&#33719;&#21021;&#22987;&#26465;&#20214;&#12289;&#24120;&#25968;&#31561;&#65289;&#21644;&#19968;&#31181;&#28508;&#22312;&#21160;&#24577;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26412;&#36523;&#22312;&#31995;&#32479;&#30340;&#25152;&#26377;&#23454;&#20363;/&#23454;&#29616;&#20013;&#37117;&#26159;&#36890;&#29992;&#30340;&#12290;&#36825;&#31181;&#20998;&#31163;&#26159;&#20197;&#33258;&#21160;&#21270;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23454;&#29616;&#30340;&#65292;&#21482;&#38656;&#35201;&#32463;&#39564;&#25968;&#25454;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#22312;&#20219;&#20309;&#36830;&#32493;&#26102;&#38388;&#26377;&#25928;&#22320;&#25512;&#26029;&#31995;&#32479;&#34892;&#20026;&#65292;&#20294;&#19981;&#38656;&#35201;&#26174;&#24335;&#30340;&#31070;&#32463;ODE&#20844;&#24335;&#65292;&#36825;&#20351;&#24471;&#23427;&#39640;&#25928;&#19988;&#39640;&#24230;&#21487;&#25193;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#30740;&#31350;&#20102;&#36825;&#31181;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method for learning dynamical systems from high-dimensional empirical data that combines variational autoencoders and (spatio-)temporal attention within a framework designed to enforce certain scientifically-motivated invariances. We focus on the setting in which data are available from multiple different instances of a system whose underlying dynamical model is entirely unknown at the outset. The approach rests on a separation into an instance-specific encoding (capturing initial conditions, constants etc.) and a latent dynamics model that is itself universal across all instances/realizations of the system. The separation is achieved in an automated, data-driven manner and only empirical data are required as inputs to the model. The approach allows effective inference of system behaviour at any continuous time but does not require an explicit neural ODE formulation, which makes it efficient and highly scalable. We study behaviour through simple theoretical analyses and ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#40065;&#26834;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#19978;&#28216;&#20219;&#21153;&#20998;&#25104;&#20960;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#24182;&#24212;&#29992;&#26497;&#23567;&#26497;&#22823;&#25439;&#22833;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#20445;&#35777;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#22343;&#21248;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12070</link><description>&lt;p&gt;
&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#28216;&#20219;&#21153;&#36866;&#24212;&#24615;&#30340;&#20219;&#21153;&#40065;&#26834;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Task-Robust Pre-Training for Worst-Case Downstream Adaptation. (arXiv:2306.12070v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#40065;&#26834;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#19978;&#28216;&#20219;&#21153;&#20998;&#25104;&#20960;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#24182;&#24212;&#29992;&#26497;&#23567;&#26497;&#22823;&#25439;&#22833;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#20445;&#35777;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#22343;&#21248;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#20851;&#24515;&#27169;&#22411;&#19981;&#20165;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;&#21512;&#29702;&#30340;&#26465;&#20214;&#21464;&#21270;&#19979;&#30340;&#34892;&#20026;&#12290;&#24403;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#26102;&#65292;&#21516;&#26679;&#30340;&#21746;&#23398;&#20063;&#36866;&#29992;&#12290;&#28982;&#32780;&#65292;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#24182;&#19981;&#20250;&#22312;&#19968;&#31995;&#21015;&#30456;&#20851;&#19979;&#28216;&#20219;&#21153;&#20013;&#22343;&#21248;&#22320;&#34920;&#29616;&#33391;&#22909;&#12290;&#26412;&#25991;&#32771;&#34385;&#39044;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#20445;&#35777;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#22343;&#21248;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#31216;&#27492;&#30446;&#26631;&#20026;&#19979;&#28216;&#20219;&#21153;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#23558;&#19978;&#28216;&#20219;&#21153;&#20998;&#25104;&#20960;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#24182;&#24212;&#29992;&#31616;&#21333;&#30340;minimax loss &#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training has achieved remarkable success when transferred to downstream tasks. In machine learning, we care about not only the good performance of a model but also its behavior under reasonable shifts of condition. The same philosophy holds when pre-training a foundation model. However, the foundation model may not uniformly behave well for a series of related downstream tasks. This happens, for example, when conducting mask recovery regression where the recovery ability or the training instances diverge like pattern features are extracted dominantly on pre-training, but semantic features are also required on a downstream task. This paper considers pre-training a model that guarantees a uniformly good performance over the downstream tasks. We call this goal as $\textit{downstream-task robustness}$. Our method first separates the upstream task into several representative ones and applies a simple minimax loss for pre-training. We then design an efficient algorithm to solve the minim
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20840;&#23616;&#22270;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26082;&#22788;&#29702;&#35805;&#35821;&#23618;&#38754;&#21448;&#22788;&#29702;&#21333;&#35789;&#23618;&#38754;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#20132;&#20114;&#26426;&#21046;&#23545;&#33410;&#28857;&#32423;&#21035;&#21644;&#31867;&#22411;&#32423;&#21035;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#25552;&#20379;&#26356;&#32454;&#31890;&#24230;&#30340;&#20851;&#31995;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#22312;&#36923;&#36753;&#25512;&#29702;QA&#25968;&#25454;&#38598;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.12069</link><description>&lt;p&gt;
&#23558;&#35805;&#35821;&#21333;&#20803;&#21644;&#20851;&#38190;&#35789;&#32852;&#31995;&#36215;&#26469;&#23545;&#38405;&#35835;&#29702;&#35299;&#36827;&#34892;&#23618;&#27425;&#25512;&#29702;&#38142;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension. (arXiv:2306.12069v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12069
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20840;&#23616;&#22270;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26082;&#22788;&#29702;&#35805;&#35821;&#23618;&#38754;&#21448;&#22788;&#29702;&#21333;&#35789;&#23618;&#38754;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#20132;&#20114;&#26426;&#21046;&#23545;&#33410;&#28857;&#32423;&#21035;&#21644;&#31867;&#22411;&#32423;&#21035;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#25552;&#20379;&#26356;&#32454;&#31890;&#24230;&#30340;&#20851;&#31995;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#22312;&#36923;&#36753;&#25512;&#29702;QA&#25968;&#25454;&#38598;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#38754;&#20020;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#26032;&#25361;&#25112;&#65292;&#26088;&#22312;&#29702;&#35299;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#25152;&#28041;&#21450;&#30340;&#38544;&#21547;&#36923;&#36753;&#20851;&#31995;&#24182;&#23545;&#20854;&#36827;&#34892;&#25512;&#29702;&#12290;&#30001;&#20110;&#36923;&#36753;&#22797;&#26434;&#24615;&#65292;&#36923;&#36753;&#20851;&#31995;&#23384;&#22312;&#20110;&#19981;&#21516;&#30340;&#31890;&#24230;&#32423;&#21035;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25512;&#29702;&#26041;&#27861;&#20998;&#21035;&#20851;&#27880;&#23454;&#20307;&#24863;&#30693;&#25110;&#35805;&#35821;&#20026;&#22522;&#30784;&#30340;&#20449;&#24687;&#65292;&#20294;&#24573;&#30053;&#20102;&#21487;&#33021;&#20855;&#26377;&#30456;&#20114;&#24433;&#21709;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20840;&#23616;&#22270;&#32593;&#32476;&#65288;HGN&#65289;&#30340;&#26041;&#27861;&#65292;&#26082;&#22788;&#29702;&#35805;&#35821;&#23618;&#38754;&#21448;&#22788;&#29702;&#21333;&#35789;&#23618;&#38754;&#30340;&#19978;&#19979;&#25991;&#65292;&#20316;&#20026;&#36923;&#36753;&#25512;&#29702;&#30340;&#22522;&#30784;&#65292;&#20197;&#25552;&#20379;&#26356;&#32454;&#31890;&#24230;&#30340;&#20851;&#31995;&#25552;&#21462;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#20998;&#23618;&#20132;&#20114;&#26426;&#21046;&#23545;&#33410;&#28857;&#32423;&#21035;&#21644;&#31867;&#22411;&#32423;&#21035;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#20123;&#20851;&#31995;&#21487;&#20197;&#35299;&#37322;&#20026;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#26725;&#26753;&#65292;&#20197;&#25913;&#36827;MRC&#31995;&#32479;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36923;&#36753;&#25512;&#29702;QA&#25968;&#25454;&#38598;&#65288;ReClor&#21644;LogiQA&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;RACE&#21644;SWAG&#65289;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine reading comprehension (MRC) poses new challenges over logical reasoning, which aims to understand the implicit logical relations entailed in the given contexts and perform inference over them. Due to the complexity of logic, logical relations exist at different granularity levels. However, most existing methods of logical reasoning individually focus on either entity-aware or discourse-based information but ignore the hierarchical relations that may even have mutual effects. In this paper, we propose a holistic graph network (HGN) which deals with context at both discourse level and word level, as the basis for logical reasoning, to provide a more fine-grained relation extraction. Specifically, node-level and type-level relations, which can be interpreted as bridges in the reasoning process, are modeled by a hierarchical interaction mechanism to improve the interpretation of MRC systems. Experimental results on logical reasoning QA datasets (ReClor and LogiQA) and natural langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#30340;&#26368;&#20248;&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#20351;&#29992;&#39640;&#38454;&#20809;&#28369;&#24615;&#20551;&#35774;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#38750;&#20984;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2306.12067</link><description>&lt;p&gt;
&#26494;&#24347;&#20809;&#28369;&#26465;&#20214;&#19979;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#30340;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Algorithms for Stochastic Bilevel Optimization under Relaxed Smoothness Conditions. (arXiv:2306.12067v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#30340;&#26368;&#20248;&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#20351;&#29992;&#39640;&#38454;&#20809;&#28369;&#24615;&#20551;&#35774;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#38750;&#20984;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#36890;&#24120;&#28041;&#21450;&#26368;&#23567;&#21270;&#20381;&#36182;&#20110;&#24378;&#20984;&#19979;&#23618;&#20989;&#25968;&#30340;&#19979;&#23618;&#20989;&#25968;&#65288;LL&#65289;&#26368;&#23567;&#20540;&#30340;&#19978;&#23618;&#65288;UL&#65289;&#20989;&#25968;&#12290;&#20960;&#31181;&#31639;&#27861;&#21033;&#29992;Neumann&#32423;&#25968;&#26469;&#36817;&#20284;&#20272;&#35745;&#38544;&#24335;&#26799;&#24230;&#65288;&#36229;&#26799;&#24230;&#65289;&#30340;&#30697;&#38453;&#36870;&#12290;&#26368;&#20808;&#36827;&#30340;&#38543;&#26426;&#21452;&#23618;&#31639;&#27861;&#65288;SOBA&#65289;[16]&#25913;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#26469;&#35299;&#20915;&#19982;&#26174;&#24335;&#30697;&#38453;&#21453;&#28436;&#30456;&#20851;&#30340;&#32447;&#24615;&#31995;&#32479;&#12290;&#35813;&#20462;&#25913;&#20351;SOBA&#33021;&#22815;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#21305;&#37197;&#21333;&#23618;&#23545;&#24212;&#39033;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19979;&#38480;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#21069;SOBA&#30340;&#20998;&#26512;&#20381;&#36182;&#20110;UL&#21644;LL&#20989;&#25968;&#30340;&#39640;&#38454;&#20809;&#28369;&#24615;&#20551;&#35774;&#20197;&#23454;&#29616;&#26368;&#20248;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#30340;&#19968;&#31181;&#26032;&#22411;&#23436;&#20840;&#21333;&#24490;&#29615;&#19988;&#26080;Hessian&#21453;&#28436;&#31639;&#27861;&#26694;&#26550;&#65292;&#24182;&#22312;&#26631;&#20934;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#26356;&#32039;&#23494;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Bilevel optimization usually involves minimizing an upper-level (UL) function that is dependent on the arg-min of a strongly-convex lower-level (LL) function. Several algorithms utilize Neumann series to approximate certain matrix inverses involved in estimating the implicit gradient of the UL function (hypergradient). The state-of-the-art StOchastic Bilevel Algorithm (SOBA) [16] instead uses stochastic gradient descent steps to solve the linear system associated with the explicit matrix inversion. This modification enables SOBA to match the lower bound of sample complexity for the single-level counterpart in non-convex settings. Unfortunately, the current analysis of SOBA relies on the assumption of higher-order smoothness for the UL and LL functions to achieve optimality. In this paper, we introduce a novel fully single-loop and Hessian-inversion-free algorithmic framework for stochastic bilevel optimization and present a tighter analysis under standard smoothness assumpti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#21367;&#31215;&#31867;&#22411;&#21644;&#26550;&#26500;&#25913;&#36827;&#65292;&#25193;&#23637;&#20102;&#31561;&#21464;Transformer&#21040;&#26356;&#39640;&#30340;&#31561;&#21464;&#34920;&#31034;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#33021;&#37327;&#21644;&#21147;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#35745;&#31639;&#25928;&#29575;&#20063;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.12059</link><description>&lt;p&gt;
EquiformerV2: &#25913;&#36827;&#30340;&#31561;&#21464;Transformer&#65292;&#29992;&#20110;&#25193;&#23637;&#21040;&#26356;&#39640;&#27425;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. (arXiv:2306.12059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#21367;&#31215;&#31867;&#22411;&#21644;&#26550;&#26500;&#25913;&#36827;&#65292;&#25193;&#23637;&#20102;&#31561;&#21464;Transformer&#21040;&#26356;&#39640;&#30340;&#31561;&#21464;&#34920;&#31034;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#33021;&#37327;&#21644;&#21147;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#35745;&#31639;&#25928;&#29575;&#20063;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;Transformer&#65288;&#20363;&#22914;Equiformer&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;Transformer&#24212;&#29992;&#20110;3D&#21407;&#23376;&#31995;&#32479;&#39046;&#22495;&#30340;&#21151;&#25928;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#20173;&#28982;&#23616;&#38480;&#20110;&#23567;&#25968;&#27425;&#31561;&#21464;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#26550;&#26500;&#26159;&#21542;&#33021;&#22815;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#26356;&#39640;&#30340;&#27425;&#25968;&#12290;&#20174;Equiformer&#24320;&#22987;&#65292;&#25105;&#20204;&#39318;&#20808;&#29992;eSCN&#21367;&#31215;&#26367;&#25442;&#20102;$SO(3)$&#21367;&#31215;&#65292;&#20197;&#26377;&#25928;&#22320;&#21512;&#24182;&#26356;&#39640;&#27425;&#30340;&#24352;&#37327;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#26356;&#39640;&#27425;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26550;&#26500;&#25913;&#36827;&#8212;&#8212;&#27880;&#24847;&#21147;&#37325;&#26631;&#20934;&#21270;&#12289;&#21487;&#20998;&#31163;&#30340;$S^2$&#28608;&#27963;&#21644;&#21487;&#20998;&#31163;&#23618;&#24402;&#19968;&#21270;&#12290;&#23558;&#36825;&#19968;&#20999;&#25918;&#22312;&#19968;&#36215;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#22312;&#22823;&#22411;OC20&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#21147;&#19978;&#25552;&#39640;&#20102;&#26368;&#22810;$12\%$&#65292;&#33021;&#37327;&#19978;&#25552;&#39640;&#20102;$4\%$&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#36895;&#24230;-&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#21560;&#38468;&#33021;&#25152;&#38656;&#30340;DFT&#35745;&#31639;&#37327;&#26041;&#38754;&#32553;&#20943;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are still limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equiformer, we first replace $SO(3)$ convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements -- attention re-normalization, separable $S^2$ activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which outperforms previous state-of-the-art methods on the large-scale OC20 dataset by up to $12\%$ on forces, $4\%$ on energies, offers better speed-accuracy trade-offs, and $2\times$ reduction in DFT calculations needed for computing adsorption energies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Corrector&#25805;&#20316;&#31526;&#30340;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#31070;&#32463;&#31639;&#23376;&#20195;&#29702;&#38750;&#32447;&#24615;&#21464;&#20998;&#36793;&#30028;&#20540;&#38382;&#39064;&#30340;&#20934;&#30830;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;&#20351;&#29992;&#35813;&#26041;&#26696;&#23545;&#20110;PCANet&#22411;&#31070;&#32463;&#31639;&#23376;&#30340;&#20108;&#32500;&#38750;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36924;&#36817;&#30340;&#20934;&#30830;&#24230;&#36817;&#20046;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#36824;&#22312;&#28041;&#21450;&#38750;&#32447;&#24615;d&#20043;&#19978;&#30340;&#25299;&#25169;&#20248;&#21270;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#25506;&#35752;&#12290;</title><link>http://arxiv.org/abs/2306.12047</link><description>&lt;p&gt;
&#20351;&#29992;Corrector&#25805;&#20316;&#31526;&#22686;&#24378;&#31070;&#32463;&#31639;&#23376;&#20195;&#29702;&#38750;&#32447;&#24615;&#21464;&#20998;&#36793;&#30028;&#20540;&#38382;&#39064;&#30340;&#20934;&#30830;&#24230;&#21644;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Corrector Operator to Enhance Accuracy and Reliability of Neural Operator Surrogates of Nonlinear Variational Boundary-Value Problems. (arXiv:2306.12047v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Corrector&#25805;&#20316;&#31526;&#30340;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#31070;&#32463;&#31639;&#23376;&#20195;&#29702;&#38750;&#32447;&#24615;&#21464;&#20998;&#36793;&#30028;&#20540;&#38382;&#39064;&#30340;&#20934;&#30830;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;&#20351;&#29992;&#35813;&#26041;&#26696;&#23545;&#20110;PCANet&#22411;&#31070;&#32463;&#31639;&#23376;&#30340;&#20108;&#32500;&#38750;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36924;&#36817;&#30340;&#20934;&#30830;&#24230;&#36817;&#20046;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#36824;&#22312;&#28041;&#21450;&#38750;&#32447;&#24615;d&#20043;&#19978;&#30340;&#25299;&#25169;&#20248;&#21270;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31867;&#21442;&#25968;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#31639;&#31526;&#30340;&#36924;&#36817;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#31070;&#32463;&#31639;&#23376;&#30340;&#26041;&#24335;&#12290;&#31070;&#32463;&#31639;&#23376;&#26377;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#12289;&#25104;&#26412;-&#20934;&#30830;&#24230;&#26435;&#34913;&#21644;&#38750;&#24179;&#20961;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#38382;&#39064;&#12290;&#31070;&#32463;&#31639;&#23376;&#20934;&#30830;&#24230;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#25512;&#29702;&#12289;&#20248;&#21270;&#21644;&#25511;&#21046;&#31561;&#21518;&#32493;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32447;&#24615;&#21464;&#20998;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#32473;&#20986;&#20102;&#31070;&#32463;&#31639;&#23376;&#39044;&#27979;&#32467;&#26524;&#30340;&#26657;&#27491;&#20540;&#12290;&#19982;&#26657;&#27491;&#38382;&#39064;&#30456;&#20851;&#30340;&#31639;&#23376;&#31216;&#20026;&#26657;&#27491;&#31639;&#23376;&#12290;&#36890;&#36807;&#20351;&#29992;&#25552;&#20986;&#30340;&#26041;&#26696;&#23545;&#37319;&#29992;PCANet&#22411;&#31070;&#32463;&#31639;&#23376;&#30340;&#20108;&#32500;&#38750;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20351;&#29992;&#26657;&#27491;&#26041;&#27861;&#23545;&#31070;&#32463;&#31639;&#23376;&#36827;&#34892;&#26657;&#27491;&#26102;&#65292;&#36924;&#36817;&#30340;&#20934;&#30830;&#24230;&#36817;&#20046;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#27492;&#22806;&#65292;&#28041;&#21450;&#38750;&#32447;&#24615;d&#20043;&#19978;&#30340;&#25299;&#25169;&#20248;&#21270;&#38382;&#39064;&#20063;&#24471;&#21040;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on developing methods for approximating the solution operators of a class of parametric partial differential equations via neural operators. Neural operators have several challenges, including the issue of generating appropriate training data, cost-accuracy trade-offs, and nontrivial hyperparameter tuning. The unpredictability of the accuracy of neural operators impacts their applications in downstream problems of inference, optimization, and control. A framework is proposed based on the linear variational problem that gives the correction to the prediction furnished by neural operators. The operator associated with the corrector problem is referred to as the corrector operator. Numerical results involving a nonlinear diffusion model in two dimensions with PCANet-type neural operators show almost two orders of increase in the accuracy of approximations when neural operators are corrected using the proposed scheme. Further, topology optimization involving a nonlinear d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986; TeCoS-LVM &#27169;&#22411;&#65292;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20197;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#20002;&#22833;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.12045</link><description>&lt;p&gt;
&#22788;&#29702;&#33258;&#28982;&#35270;&#35273;&#22330;&#26223;&#31070;&#32463;&#21709;&#24212;&#30340;&#26102;&#38388;&#26465;&#20214;&#33033;&#20914;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes. (arXiv:2306.12045v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986; TeCoS-LVM &#27169;&#22411;&#65292;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20197;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#20002;&#22833;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#31070;&#32463;&#21709;&#24212;&#30340;&#35745;&#31639;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#24863;&#30693;&#22788;&#29702;&#21644;&#31070;&#32463;&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#20351;&#29992;&#26102;&#38388;&#36807;&#28388;&#22120;&#26469;&#22788;&#29702;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#23548;&#33268;&#22788;&#29702;&#27969;&#31243;&#19981;&#29616;&#23454;&#19988;&#19981;&#28789;&#27963;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38024;&#23545;&#35797;&#39564;&#24179;&#22343;&#21457;&#25918;&#29575;&#65292;&#26410;&#33021;&#25429;&#25417;&#21040;&#33033;&#20914;&#21015;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#26102;&#38388;&#26465;&#20214;&#33033;&#20914;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;TeCoS-LVM&#65289;&#26469;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20135;&#29983;&#30452;&#25509;&#21305;&#37197;&#35760;&#24405;&#33033;&#20914;&#21015;&#30340;&#33033;&#20914;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#36991;&#20813;&#20002;&#22833;&#23884;&#20837;&#22312;&#21407;&#22987;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20174;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20013;&#25490;&#38500;&#26102;&#38388;&#32500;&#24230;&#65292;&#24182;&#24341;&#20837;&#26102;&#38388;&#26465;&#20214;&#25805;&#20316;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#33258;&#28982;&#33539;&#24335;&#20013;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; TeCoS-LVM &#27169;&#22411;&#33021;&#22815;&#20135;&#29983;...
&lt;/p&gt;
&lt;p&gt;
Developing computational models of neural response is crucial for understanding sensory processing and neural computations. Current state-of-the-art neural network methods use temporal filters to handle temporal dependencies, resulting in an unrealistic and inflexible processing flow. Meanwhile, these methods target trial-averaged firing rates and fail to capture important features in spike trains. This work presents the temporal conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural response to natural visual stimuli. We use spiking neurons to produce spike outputs that directly match the recorded trains. This approach helps to avoid losing information embedded in the original spike trains. We exclude the temporal dimension from the model parameter space and introduce a temporal conditioning operation to allow the model to adaptively explore and exploit temporal dependencies in stimuli sequences in a natural paradigm. We show that TeCoS-LVM models can produce m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#39640;&#25928;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36816;&#21160;&#26799;&#24230;&#30340;&#20196;&#29260;&#21152;&#26435;&#26041;&#27861;&#65292;&#25972;&#21512;&#25945;&#24072;&#35299;&#30721;&#22120;&#21644;&#23398;&#29983;&#35299;&#30721;&#22120;&#20197;&#21450;&#29983;&#25104;&#21512;&#25104;&#24322;&#24120;&#20107;&#20214;&#65292;&#23454;&#29616;&#20849;&#21516;&#37325;&#26500;&#21407;&#22987;&#24103;&#21644;&#23545;&#24212;&#30340;&#20687;&#32032;&#32423;&#24322;&#24120;&#26144;&#23556;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23454;&#29616;&#20986;&#33394;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.12041</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#26159;&#39640;&#25928;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Self-Distilled Masked Auto-Encoders are Efficient Video Anomaly Detectors. (arXiv:2306.12041v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#39640;&#25928;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36816;&#21160;&#26799;&#24230;&#30340;&#20196;&#29260;&#21152;&#26435;&#26041;&#27861;&#65292;&#25972;&#21512;&#25945;&#24072;&#35299;&#30721;&#22120;&#21644;&#23398;&#29983;&#35299;&#30721;&#22120;&#20197;&#21450;&#29983;&#25104;&#21512;&#25104;&#24322;&#24120;&#20107;&#20214;&#65292;&#23454;&#29616;&#20849;&#21516;&#37325;&#26500;&#21407;&#22987;&#24103;&#21644;&#23545;&#24212;&#30340;&#20687;&#32032;&#32423;&#24322;&#24120;&#26144;&#23556;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23454;&#29616;&#20986;&#33394;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#30340;&#39640;&#25928;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#35270;&#39057;&#24103;&#32423;&#21035;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#19977;&#20010;&#21019;&#26032;&#28857;&#65306;&#20854;&#19968;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#36816;&#21160;&#26799;&#24230;&#30340;&#20196;&#29260;&#21152;&#26435;&#26041;&#27861;&#65292;&#22240;&#27492;&#36991;&#20813;&#20102;&#23398;&#20064;&#37325;&#26500;&#38745;&#24577;&#32972;&#26223;&#22330;&#26223;&#12290;&#20854;&#20108;&#65292;&#25105;&#20204;&#23558;&#25945;&#24072;&#35299;&#30721;&#22120;&#21644;&#23398;&#29983;&#35299;&#30721;&#22120;&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;&#26550;&#26500;&#20013;&#65292;&#21033;&#29992;&#20004;&#20010;&#35299;&#30721;&#22120;&#32473;&#20986;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25913;&#21892;&#24322;&#24120;&#26816;&#27979;&#12290;&#20854;&#19977;&#65292;&#25105;&#20204;&#29983;&#25104;&#21512;&#25104;&#24322;&#24120;&#20107;&#20214;&#20197;&#22686;&#24378;&#22521;&#35757;&#35270;&#39057;&#65292;&#24182;&#23558;&#36974;&#34109;AE&#27169;&#22411;&#20219;&#21153;&#35774;&#32622;&#20026;&#20849;&#21516;&#37325;&#26500;&#21407;&#22987;&#24103;&#65288;&#26080;&#24322;&#24120;&#65289;&#21644;&#23545;&#24212;&#30340;&#20687;&#32032;&#32423;&#24322;&#24120;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#23548;&#33268;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient abnormal event detection model based on a lightweight masked auto-encoder (AE) applied at the video frame level. The novelty of the proposed model is threefold. First, we introduce an approach to weight tokens based on motion gradients, thus avoiding learning to reconstruct the static background scene. Second, we integrate a teacher decoder and a student decoder into our architecture, leveraging the discrepancy between the outputs given by the two decoders to improve anomaly detection. Third, we generate synthetic abnormal events to augment the training videos, and task the masked AE model to jointly reconstruct the original frames (without anomalies) and the corresponding pixel-level anomaly maps. Our design leads to an efficient and effective model, as demonstrated by the extensive experiments carried out on three benchmarks: Avenue, ShanghaiTech and UCSD Ped2. The empirical results show that our model achieves an excellent trade-off between speed and accuracy
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-SSAD&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#31995;&#32479;&#22320;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#30340;&#36229;&#21442;&#25968;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12033</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#31471;&#21040;&#31471;&#22686;&#24378;&#36229;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
End-to-End Augmentation Hyperparameter Tuning for Self-Supervised Anomaly Detection. (arXiv:2306.12033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-SSAD&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#31995;&#32479;&#22320;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#30340;&#36229;&#21442;&#25968;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#65292;&#23427;&#20026;&#29616;&#23454;&#38382;&#39064;&#25552;&#20379;&#33258;&#20135;&#29983;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#25163;&#21160;&#26631;&#27880;&#24037;&#20316;&#12290;SSL&#23545;&#20110;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#22914;&#24322;&#24120;&#26816;&#27979;&#23588;&#20854;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#26631;&#35760;&#30340;&#24322;&#24120;&#36890;&#24120;&#19981;&#23384;&#22312;&#25110;&#38590;&#20197;&#33719;&#24471;&#12290;&#34429;&#28982;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#25991;&#29486;&#21364;&#26410;&#23558;&#25968;&#25454;&#22686;&#24378;&#35270;&#20026;&#36229;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#24378;&#36873;&#25321;&#23545;&#26816;&#27979;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ST-SSAD&#65288;&#33258;&#25105;&#35843;&#25972;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20851;&#20110;&#20005;&#26684;&#35843;&#25972;&#22686;&#24378;&#30340;SSAD&#30340;&#31532;&#19968;&#20010;&#31995;&#32479;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#31532;&#19968;&#26159;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#39564;&#35777;&#25439;&#22833;&#20989;&#25968;&#65292;&#37327;&#21270;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#19982;&#65288;&#26080;&#26631;&#31614;&#65289;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#22312;&#21407;&#21017;&#19978;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#36817;&#39640;&#25928;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20511;&#37492;&#30340;&#26080;&#30417;&#30563;&#39564;&#35777;&#26041;&#26696;&#21644;&#22686;&#24378;&#25968;&#25454;&#25628;&#32034;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;SSAD&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24418;&#24335;&#65292;&#23558;&#36731;&#37327;&#32423;&#25968;&#25454;&#22686;&#24378;&#25628;&#32034;&#22120;&#30340;&#31616;&#21333;&#38598;&#25104;&#12290;&#22312;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22686;&#24378;&#35843;&#25972;&#26041;&#27861;&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#26368;&#26032;&#32467;&#26524;&#21487;&#20197;&#33719;&#24471;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#36817;&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has emerged as a promising paradigm that presents self-generated supervisory signals to real-world problems, bypassing the extensive manual labeling burden. SSL is especially attractive for unsupervised tasks such as anomaly detection, where labeled anomalies are often nonexistent and costly to obtain. While self-supervised anomaly detection (SSAD) has seen a recent surge of interest, the literature has failed to treat data augmentation as a hyperparameter. Meanwhile, recent works have reported that the choice of augmentation has significant impact on detection performance. In this paper, we introduce ST-SSAD (Self-Tuning Self-Supervised Anomaly Detection), the first systematic approach to SSAD in regards to rigorously tuning augmentation. To this end, our work presents two key contributions. The first is a new unsupervised validation loss that quantifies the alignment between the augmented training data and the (unlabeled) test data. In principle we adop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#32593;&#32476;&#29228;&#34411;&#21644;&#32593;&#39029;&#25490;&#21517;&#31639;&#27861;&#22312;&#22788;&#29702;Web&#25968;&#25454;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65307;&#22312;&#35780;&#20272;&#20116;&#31181;&#19981;&#21516;&#30340;&#29228;&#21462;&#31639;&#27861;&#21518;&#65292;&#26088;&#22312;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#29228;&#21462;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.12027</link><description>&lt;p&gt;
&#22810;&#31181;&#32593;&#32476;&#29228;&#34411;&#31639;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative analysis of various web crawler algorithms. (arXiv:2306.12027v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32593;&#32476;&#29228;&#34411;&#21644;&#32593;&#39029;&#25490;&#21517;&#31639;&#27861;&#22312;&#22788;&#29702;Web&#25968;&#25454;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65307;&#22312;&#35780;&#20272;&#20116;&#31181;&#19981;&#21516;&#30340;&#29228;&#21462;&#31639;&#27861;&#21518;&#65292;&#26088;&#22312;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#29228;&#21462;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#32593;&#32476;&#29228;&#34411;&#21644;&#32593;&#39029;&#25490;&#21517;&#31639;&#27861;&#22312;&#22788;&#29702;&#19990;&#30028;&#21508;&#22320;&#32593;&#32476;&#25968;&#25454;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#38543;&#30528;Web&#30340;&#24613;&#21095;&#22686;&#38271;&#65292;&#39640;&#25928;&#30340;&#25628;&#32034;&#21644;&#26816;&#32034;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#32593;&#32476;&#29228;&#34411;&#26159;&#23558;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#20449;&#24687;&#26816;&#32034;&#12290;&#27492;&#22806;&#65292;&#32593;&#39029;&#25490;&#21517;&#31639;&#27861;&#22312;&#35780;&#20272;&#32593;&#39029;&#30340;&#36136;&#37327;&#21644;&#21463;&#27426;&#36814;&#31243;&#24230;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#32972;&#26223;&#65292;&#24182;&#35780;&#20272;&#20102;&#20116;&#31181;&#19981;&#21516;&#30340;&#29228;&#21462;&#31639;&#27861;&#65306;&#40104;&#40060;&#25628;&#32034;&#65292;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#38431;&#21015;&#65292;&#26420;&#32032;&#36125;&#21494;&#26031;&#65292;&#24191;&#24230;&#20248;&#20808;&#21644;&#28145;&#24230;&#20248;&#20808;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#32593;&#32476;&#29228;&#34411;&#31639;&#27861;&#12290;&#36890;&#36807;&#20102;&#35299;&#36825;&#20123;&#31639;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#33258;&#24049;&#22312;Web&#19978;&#23548;&#33322;&#21644;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This presentation focuses on the importance of web crawling and page ranking algorithms in dealing with the massive amount of data present on the World Wide Web. As the web continues to grow exponentially, efficient search and retrieval methods become crucial. Web crawling is a process that converts unstructured data into structured data, enabling effective information retrieval. Additionally, page ranking algorithms play a significant role in assessing the quality and popularity of web pages. The presentation explores the background of these algorithms and evaluates five different crawling algorithms: Shark Search, Priority-Based Queue, Naive Bayes, Breadth-First, and Depth-First. The goal is to identify the most effective algorithm for crawling web pages. By understanding these algorithms, we can enhance our ability to navigate the web and extract valuable information efficiently.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;continual learners&#20316;&#20026;&#39044;&#35757;&#32451;&#22120;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;CL&#26694;&#26550;&#21644;&#24494;&#35843;&#26041;&#26696;GLAD&#65292;&#21487;&#29992;&#20316;&#36739;&#22909;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26412;&#36523;&#12290;&#36890;&#36807;&#23398;&#20064;&#25913;&#21892;&#36890;&#29992;&#24615;&#29305;&#24449;&#65292;&#22312;&#23481;&#26131;&#24536;&#35760;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#26102;&#65292;CL&#27169;&#22411;&#21487;&#36880;&#27493;&#25552;&#39640;&#34920;&#31034;&#30340;&#36716;&#31227;&#36136;&#37327;&#32780;&#19981;&#24433;&#21709;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12026</link><description>&lt;p&gt;
Continual&#23398;&#20064;&#26159;&#28176;&#36827;&#30340;&#27169;&#22411;&#27867;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Continual Learners are Incremental Model Generalizers. (arXiv:2306.12026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;continual learners&#20316;&#20026;&#39044;&#35757;&#32451;&#22120;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;CL&#26694;&#26550;&#21644;&#24494;&#35843;&#26041;&#26696;GLAD&#65292;&#21487;&#29992;&#20316;&#36739;&#22909;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26412;&#36523;&#12290;&#36890;&#36807;&#23398;&#20064;&#25913;&#21892;&#36890;&#29992;&#24615;&#29305;&#24449;&#65292;&#22312;&#23481;&#26131;&#24536;&#35760;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#26102;&#65292;CL&#27169;&#22411;&#21487;&#36880;&#27493;&#25552;&#39640;&#34920;&#31034;&#30340;&#36716;&#31227;&#36136;&#37327;&#32780;&#19981;&#24433;&#21709;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#22823;&#37327;&#30740;&#31350;Continual Learning&#65288;CL&#65289;&#27169;&#22411;&#20316;&#20026;&#39044;&#35757;&#32451;&#22120;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#25506;&#35752;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#24555;&#36895;&#25910;&#25947;&#30340;&#21160;&#26426;&#12290;&#22312;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;CL&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#34920;&#31034;&#30340;&#36716;&#31227;&#36136;&#37327;&#36890;&#24120;&#20250;&#36880;&#28176;&#25552;&#39640;&#32780;&#19981;&#20250;&#24433;&#21709;&#24494;&#35843;&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;&#36825;&#26159;&#22240;&#20026;&#24403;&#23481;&#26131;&#24536;&#35760;&#29305;&#23450;&#20219;&#21153;&#30340;&#30693;&#35782;&#26102;&#65292;CL&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#20219;&#21153;&#19968;&#33324;&#29305;&#24449;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;CL&#26694;&#26550;&#65292;&#20351;&#29992;&#25513;&#34109;&#24314;&#27169;&#26088;&#22312;&#22312;&#35757;&#32451;&#26399;&#38388;&#25429;&#33719;&#27969;&#30021;&#30340;&#20219;&#21153;&#36890;&#29992;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#26041;&#26696;GLobal Attention Discretization&#65288;GLAD&#65289;&#65292;&#22312;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#26102;&#20445;&#30041;&#20016;&#23500;&#30340;&#20219;&#21153;&#36890;&#29992;&#34920;&#31034;&#12290;&#20351;&#29992;GLAD&#24494;&#35843;&#30340;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#26412;&#36523;&#20063;&#21487;&#20197;&#29992;&#20316;&#36739;&#22909;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30456;&#20449;&#26412;&#25991;&#25171;&#30772;&#20102;p&#20043;&#38388;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the efficiency and rapid convergence of pre-trained models for solving downstream tasks, this paper extensively studies the impact of Continual Learning (CL) models as pre-trainers. In both supervised and unsupervised CL, we find that the transfer quality of the representation often increases gradually without noticeable degradation in fine-tuning performance. This is because CL models can learn improved task-general features when easily forgetting task-specific knowledge. Based on this observation, we suggest a new unsupervised CL framework with masked modeling, which aims to capture fluent task-generic representation during training. Furthermore, we propose a new fine-tuning scheme, GLobal Attention Discretization (GLAD), that preserves rich task-generic representation during solving downstream tasks. The model fine-tuned with GLAD achieves competitive performance and can also be used as a good pre-trained model itself. We believe this paper breaks the barriers between p
&lt;/p&gt;</description></item><item><title>3HAN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34394;&#20551;&#26032;&#38395;&#33258;&#21160;&#26816;&#27979;&#22120;&#12290;&#36890;&#36807;&#19977;&#23618;&#20998;&#23618;&#27880;&#24847;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#23545;&#26032;&#38395;&#36827;&#34892;&#23436;&#25972;&#26377;&#25928;&#34920;&#31034;&#30340;&#26032;&#38395;&#21521;&#37327;&#65292;&#24182;&#32473;&#25991;&#31456;&#19981;&#21516;&#37096;&#20998;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12014</link><description>&lt;p&gt;
3HAN&#65306;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
3HAN: A Deep Neural Network for Fake News Detection. (arXiv:2306.12014v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12014
&lt;/p&gt;
&lt;p&gt;
3HAN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34394;&#20551;&#26032;&#38395;&#33258;&#21160;&#26816;&#27979;&#22120;&#12290;&#36890;&#36807;&#19977;&#23618;&#20998;&#23618;&#27880;&#24847;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#23545;&#26032;&#38395;&#36827;&#34892;&#23436;&#25972;&#26377;&#25928;&#34920;&#31034;&#30340;&#26032;&#38395;&#21521;&#37327;&#65292;&#24182;&#32473;&#25991;&#31456;&#19981;&#21516;&#37096;&#20998;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#26032;&#38395;&#30340;&#36805;&#36895;&#20256;&#25773;&#26159;&#19968;&#20010;&#38656;&#35201;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19977;&#23618;&#20998;&#23618;&#27880;&#24847;&#32593;&#32476;&#65288;3HAN&#65289;&#26500;&#24314;&#19968;&#20010;&#33258;&#21160;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#26816;&#27979;&#34394;&#20551;&#26032;&#38395;&#12290; 3HAN&#36890;&#36807;&#36880;&#23618;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#24335;&#26500;&#24314;&#26032;&#38395;&#21521;&#37327;&#65306;&#23545;&#36755;&#20837;&#26032;&#38395;&#25991;&#31456;&#30340;&#26377;&#25928;&#34920;&#31034;&#12290;&#22312;&#26032;&#38395;&#20013;&#65292;&#26631;&#39064;&#26159;&#34394;&#20551;&#26032;&#38395;&#30340;&#19968;&#20010;&#21306;&#21035;&#29305;&#24449;&#65292;&#21516;&#26102;&#25991;&#31456;&#20013;&#30340;&#30456;&#23545;&#36739;&#23569;&#30340;&#21333;&#35789;&#21644;&#21477;&#23376;&#27604;&#20854;&#20313;&#37096;&#20998;&#26356;&#20026;&#37325;&#35201;&#12290;3HAN&#30001;&#20110;&#20855;&#26377;&#19977;&#23618;&#27880;&#24847;&#21147;&#30340;&#26426;&#21046;&#65292;&#32473;&#25991;&#31456;&#30340;&#19981;&#21516;&#37096;&#20998;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;&#22823;&#22411;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;3HAN&#30340;&#39640;&#25928;&#24615;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;96.77&#65285;&#12290;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19981;&#21516;&#65292;3HAN&#36890;&#36807;&#23545;&#25991;&#31456;&#19981;&#21516;&#37096;&#20998;&#30340;&#27880;&#24847;&#26435;&#37325;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid spread of fake news is a serious problem calling for AI solutions. We employ a deep learning based automated detector through a three level hierarchical attention network (3HAN) for fast, accurate detection of fake news. 3HAN has three levels, one each for words, sentences, and the headline, and constructs a news vector: an effective representation of an input news article, by processing an article in an hierarchical bottom-up manner. The headline is known to be a distinguishing feature of fake news, and furthermore, relatively few words and sentences in an article are more important than the rest. 3HAN gives a differential importance to parts of an article, on account of its three layers of attention. By experiments on a large real-world data set, we observe the effectiveness of 3HAN with an accuracy of 96.77%. Unlike some other deep learning models, 3HAN provides an understandable output through the attention weights given to different parts of an article, which can be visu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#29992;&#20110;&#26925;&#22278;&#31639;&#23376;&#30340;&#40784;&#27425;&#21270;&#26144;&#23556;&#65292;&#20197;&#24314;&#31435;&#32771;&#34385;&#25509;&#21475;&#30340;&#40784;&#27425;&#21270;&#26412;&#26500;&#23450;&#24459;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12006</link><description>&lt;p&gt;
&#23398;&#20064;&#23567;&#27874;&#23545;&#26925;&#22278;&#31639;&#23376;&#30340;&#40784;&#27425;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning Homogenization for Elliptic Operators. (arXiv:2306.12006v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#29992;&#20110;&#26925;&#22278;&#31639;&#23376;&#30340;&#40784;&#27425;&#21270;&#26144;&#23556;&#65292;&#20197;&#24314;&#31435;&#32771;&#34385;&#25509;&#21475;&#30340;&#40784;&#27425;&#21270;&#26412;&#26500;&#23450;&#24459;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26377;&#20986;&#29616;&#12290;&#40784;&#27425;&#21270;&#29702;&#35770;&#26159;&#28040;&#38500;&#23567;&#23610;&#24230;&#20381;&#36182;&#30340;&#24378;&#26377;&#21147;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#31616;&#21270;&#30340;&#26041;&#31243;&#20197;&#21450;&#35745;&#31639;&#19978;&#30340;&#20415;&#21033;&#12290;&#22312;&#36830;&#32493;&#20171;&#36136;&#21147;&#23398;&#39046;&#22495;&#65292;&#40784;&#27425;&#21270;&#23545;&#20110;&#23548;&#20986;&#21253;&#21547;&#24494;&#35266;&#29289;&#29702;&#23398;&#30340;&#26412;&#26500;&#23450;&#24459;&#20197;&#21046;&#23450;&#24863;&#20852;&#36259;&#30340;&#23439;&#35266;&#37327;&#30340;&#24179;&#34913;&#26041;&#31243;&#24456;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#33719;&#24471;&#40784;&#27425;&#21270;&#26412;&#26500;&#23450;&#24459;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#27809;&#26377;&#35299;&#26512;&#24418;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#23637;&#29616;&#22312;&#24494;&#35266;&#23610;&#24230;&#19978;&#19981;&#23384;&#22312;&#30340;&#29616;&#35937;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26412;&#26500;&#23450;&#24459;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26925;&#22278;&#31639;&#23376;&#40784;&#27425;&#21270;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35828;&#26126;&#19982;&#36825;&#31181;&#25509;&#21475;&#26377;&#20851;&#30340;&#40784;&#27425;&#21270;&#24314;&#31435;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26500;&#36896;&#36866;&#24403;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#32771;&#34385;&#24213;&#23618;&#20960;&#20309;&#23398;&#21644;&#24494;&#35266;&#32467;&#26500;&#65292;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#23558;&#36825;&#20123;&#25509;&#21475;&#21512;&#24182;&#21040;&#40784;&#27425;&#21270;&#26412;&#26500;&#23450;&#24459;&#20013;&#12290;&#25105;&#20204;&#30340;&#25968;&#23383;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#26377;&#25928;&#30340;&#23439;&#35266;&#34892;&#20026;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale partial differential equations (PDEs) arise in various applications, and several schemes have been developed to solve them efficiently. Homogenization theory is a powerful methodology that eliminates the small-scale dependence, resulting in simplified equations that are computationally tractable. In the field of continuum mechanics, homogenization is crucial for deriving constitutive laws that incorporate microscale physics in order to formulate balance laws for the macroscopic quantities of interest. However, obtaining homogenized constitutive laws is often challenging as they do not in general have an analytic form and can exhibit phenomena not present on the microscale. In response, data-driven learning of the constitutive law has been proposed as appropriate for this task. However, a major challenge in data-driven learning approaches for this problem has remained unexplored: the impact of discontinuities and corner interfaces in the underlying material. These discontinui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12001</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#19987;&#23478;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#19990;&#30028;&#21508;&#22269;&#39046;&#23548;&#20154;&#23545;&#36234;&#26469;&#36234;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#39118;&#38505;&#34987;&#21333;&#29420;&#35814;&#32454;&#20171;&#32461;&#36807;&#65292;&#20294;&#36843;&#20999;&#38656;&#35201;&#31995;&#32479;&#22320;&#35752;&#35770;&#21644;&#35828;&#26126;&#28508;&#22312;&#21361;&#38505;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#24694;&#24847;&#20351;&#29992;&#65292;&#21363;&#20010;&#20154;&#25110;&#22242;&#20307;&#26377;&#24847;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#36896;&#25104;&#20260;&#23475;&#65307;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#21363;&#31454;&#20105;&#29615;&#22659;&#20419;&#20351;&#34892;&#21160;&#32773;&#37096;&#32626;&#19981;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#25110;&#25918;&#24323;&#25511;&#21046;&#26435;&#20132;&#32473;&#20154;&#24037;&#26234;&#33021;&#65307;&#32452;&#32455;&#39118;&#38505;&#65292;&#31361;&#20986;&#20154;&#20026;&#21644;&#22797;&#26434;&#31995;&#32479;&#22914;&#20309;&#22686;&#21152;&#28798;&#38590;&#24615;&#20107;&#25925;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;&#20197;&#21450;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#65292;&#25551;&#36848;&#20102;&#25511;&#21046;&#27604;&#20154;&#31867;&#26234;&#33021;&#26356;&#39640;&#30340;&#20195;&#29702;&#31243;&#24207;&#22256;&#38590;&#30340;&#22266;&#26377;&#38590;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#39118;&#38505;&#31867;&#21035;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;INT4&#31639;&#26415;&#35757;&#32451;Transformer&#30340;&#26041;&#27861;&#65292;&#24182;&#32454;&#33268;&#22320;&#20998;&#26512;&#20102;&#36716;&#25442;&#22120;&#20013;&#28608;&#27963;&#21644;&#26799;&#24230;&#30340;&#29305;&#23450;&#32467;&#26500;&#65292;&#20026;&#23427;&#20204;&#25552;&#20986;&#20102;&#19987;&#29992;&#30340;&#37327;&#21270;&#22120;&#12290;&#31639;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11987</link><description>&lt;p&gt;
&#20351;&#29992;4&#20301;&#25972;&#25968;&#35757;&#32451;Transformer
&lt;/p&gt;
&lt;p&gt;
Training Transformers with 4-bit Integers. (arXiv:2306.11987v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;INT4&#31639;&#26415;&#35757;&#32451;Transformer&#30340;&#26041;&#27861;&#65292;&#24182;&#32454;&#33268;&#22320;&#20998;&#26512;&#20102;&#36716;&#25442;&#22120;&#20013;&#28608;&#27963;&#21644;&#26799;&#24230;&#30340;&#29305;&#23450;&#32467;&#26500;&#65292;&#20026;&#23427;&#20204;&#25552;&#20986;&#20102;&#19987;&#29992;&#30340;&#37327;&#21270;&#22120;&#12290;&#31639;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28608;&#27963;&#12289;&#26435;&#37325;&#21644;&#26799;&#24230;&#37327;&#21270;&#20026;4&#20301;&#26377;&#26395;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;4&#20301;&#35757;&#32451;&#26041;&#27861;&#38656;&#35201;&#33258;&#23450;&#20041;&#25968;&#20540;&#26684;&#24335;&#65292;&#32780;&#36825;&#20123;&#26684;&#24335;&#19981;&#21463;&#24403;&#20195;&#30828;&#20214;&#25903;&#25345;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;INT4&#31639;&#26415;&#23454;&#29616;&#25152;&#26377;&#30697;&#38453;&#20056;&#27861;&#30340;Transformer&#35757;&#32451;&#26041;&#27861;&#12290;&#20197;&#26497;&#20302;&#30340;INT4&#31934;&#24230;&#35757;&#32451;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20180;&#32454;&#20998;&#26512;&#20102;Transformer&#20013;&#28608;&#27963;&#21644;&#26799;&#24230;&#30340;&#29305;&#23450;&#32467;&#26500;&#65292;&#20026;&#23427;&#20204;&#25552;&#20986;&#20102;&#19987;&#29992;&#37327;&#21270;&#22120;&#12290;&#23545;&#20110;&#21069;&#21521;&#20256;&#25773;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#31163;&#32676;&#20540;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21704;&#36798;&#29595;&#37327;&#21270;&#22120;&#26469;&#25233;&#21046;&#31163;&#32676;&#20540;&#12290;&#23545;&#20110;&#21453;&#21521;&#20256;&#25773;&#65292;&#25105;&#20204;&#21033;&#29992;&#26799;&#24230;&#30340;&#32467;&#26500;&#31232;&#30095;&#24615;&#65292;&#25552;&#20986;&#20301;&#20998;&#35010;&#21644;&#26464;&#26438;&#24471;&#20998;&#37319;&#26679;&#25216;&#26415;&#26469;&#31934;&#30830;&#37327;&#21270;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#22270;&#20687;&#20998;&#31867;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantizing the activation, weight, and gradient to 4-bit is promising to accelerate neural network training. However, existing 4-bit training methods require custom numerical formats which are not supported by contemporary hardware. In this work, we propose a training method for transformers with all matrix multiplications implemented with the INT4 arithmetic. Training with an ultra-low INT4 precision is challenging. To achieve this, we carefully analyze the specific structures of activation and gradients in transformers to propose dedicated quantizers for them. For forward propagation, we identify the challenge of outliers and propose a Hadamard quantizer to suppress the outliers. For backpropagation, we leverage the structural sparsity of gradients by proposing bit splitting and leverage score sampling techniques to quantize gradients accurately. Our algorithm achieves competitive accuracy on a wide range of tasks including natural language understanding, machine translation, and ima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22855;&#24322;&#35889;&#24179;&#28369;&#31639;&#27861;&#32531;&#35299;&#39034;&#24207;&#25512;&#33616;&#20013;&#24207;&#21015;&#19982;&#39033;&#30446;&#25490;&#21517;&#36864;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;SSA&#25351;&#26631;&#26469;&#35780;&#20272;&#35813;&#38382;&#39064;&#30340;&#20005;&#37325;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11986</link><description>&lt;p&gt;
&#36890;&#36807;&#22855;&#24322;&#35889;&#24179;&#28369;&#35299;&#20915;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#25490;&#21517;&#36864;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing the Rank Degeneration in Sequential Recommendation via Singular Spectrum Smoothing. (arXiv:2306.11986v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22855;&#24322;&#35889;&#24179;&#28369;&#31639;&#27861;&#32531;&#35299;&#39034;&#24207;&#25512;&#33616;&#20013;&#24207;&#21015;&#19982;&#39033;&#30446;&#25490;&#21517;&#36864;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;SSA&#25351;&#26631;&#26469;&#35780;&#20272;&#35813;&#38382;&#39064;&#30340;&#20005;&#37325;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#30740;&#31350;&#21160;&#24577;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#24182;&#29983;&#25104;&#19979;&#19968;&#20010;&#39033;&#30446;&#39044;&#27979;&#12290;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#20559;&#22909;&#36890;&#24120;&#26159;&#36890;&#36807;&#24207;&#21015;&#21644;&#39033;&#30446;&#34920;&#31034;&#20043;&#38388;&#30340;&#20146;&#21644;&#24230;&#29983;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#65292;&#24207;&#21015;&#21644;&#39033;&#30446;&#34920;&#31034;&#37117;&#20250;&#36973;&#21463;&#25490;&#21517;&#38477;&#32423;&#38382;&#39064;&#12290;&#25490;&#21517;&#36864;&#21270;&#38382;&#39064;&#20005;&#37325;&#25439;&#23475;&#20102;&#39034;&#24207;&#25512;&#33616;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#29702;&#35770;&#36830;&#25509;&#24207;&#21015;&#34920;&#31034;&#38477;&#32423;&#38382;&#39064;&#19982;&#39033;&#30446;&#25490;&#21517;&#36864;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#24555;&#36895;&#22855;&#24322;&#20540;&#34928;&#20943;&#29616;&#35937;&#19982;&#36716;&#25442;&#22120;&#24207;&#21015;&#36755;&#20986;&#21644;&#39033;&#30446;&#23884;&#20837;&#20013;&#30340;&#25490;&#21517;&#25240;&#21472;&#38382;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22855;&#24322;&#20540;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;SSA&#65289;&#35780;&#20272;&#25351;&#26631;&#65292;&#21516;&#26102;&#32531;&#35299;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#24207;&#21015;&#21644;&#39033;&#30446;&#34920;&#31034;&#25490;&#21517;&#36864;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation (SR) investigates the dynamic user preferences modeling and generates the next-item prediction. The next item preference is typically generated by the affinity between the sequence and item representations. However, both sequence and item representations suffer from the rank degeneration issue due to the data sparsity problem. The rank degeneration issue significantly impairs the representations for SR. This motivates us to measure how severe is the rank degeneration issue and alleviate the sequence and item representation rank degeneration issues simultaneously for SR.  In this work, we theoretically connect the sequence representation degeneration issue with the item rank degeneration, particularly for short sequences and cold items. We also identify the connection between the fast singular value decay phenomenon and the rank collapse issue in transformer sequence output and item embeddings. We propose the area under the singular value curve metric to evalua
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20998;&#26512;&#23427;&#20204;&#26159;&#21542;&#20026;&#20020;&#24202;&#20915;&#31574;&#25552;&#20379;&#21487;&#20449;&#30340;&#35299;&#37322;&#21644;&#36866;&#24403;&#30340;&#39046;&#22495;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.11985</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#30340;&#28909;&#38376;XAI&#30340;&#35780;&#20272;&#65306;&#23427;&#20204;&#21487;&#20449;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Popular XAI Applied to Clinical Prediction Models: Can They be Trusted?. (arXiv:2306.11985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20998;&#26512;&#23427;&#20204;&#26159;&#21542;&#20026;&#20020;&#24202;&#20915;&#31574;&#25552;&#20379;&#21487;&#20449;&#30340;&#35299;&#37322;&#21644;&#36866;&#24403;&#30340;&#39046;&#22495;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#22312;&#20020;&#24202;&#19978;&#30340;&#37319;&#29992;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29992;&#20110;&#35299;&#37322;&#21307;&#30103;&#39046;&#22495;&#39044;&#27979;&#27169;&#22411;&#30340;&#20004;&#31181;&#27969;&#34892;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#20135;&#29983;&#29305;&#23450;&#24212;&#29992;&#20219;&#21153;&#30456;&#20851;&#30340;&#39046;&#22495;&#36866;&#24403;&#30340;&#34920;&#31034;&#65292;&#26159;&#21542;&#20250;&#24433;&#21709;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#24182;&#20445;&#25345;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#20998;&#26512;&#20102;&#22312;&#32676;&#20307;&#21644;&#24739;&#32773;&#27700;&#24179;&#19978;&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25253;&#36947;&#20102;&#31532;&#19968;&#27425;&#23558;XAI&#26041;&#27861;&#24212;&#29992;&#20110;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29983;&#25104;&#30340;&#35299;&#37322;&#19982;&#26410;&#26469;&#20020;&#24202;&#24694;&#21270;&#20107;&#20214;&#30340;&#35302;&#21457;&#22120;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The absence of transparency and explainability hinders the clinical adoption of Machine learning (ML) algorithms. Although various methods of explainable artificial intelligence (XAI) have been suggested, there is a lack of literature that delves into their practicality and assesses them based on criteria that could foster trust in clinical environments. To address this gap this study evaluates two popular XAI methods used for explaining predictive models in the healthcare context in terms of whether they (i) generate domain-appropriate representation, i.e. coherent with respect to the application task, (ii) impact clinical workflow and (iii) are consistent. To that end, explanations generated at the cohort and patient levels were analysed. The paper reports the first benchmarking of the XAI methods applied to risk prediction models obtained by evaluating the concordance between generated explanations and the trigger of a future clinical deterioration episode recorded by the data colle
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#19981;&#21516;&#27744;&#21270;&#37197;&#32622;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#39044;&#23450;&#20041;&#30340;&#19979;&#37319;&#26679;&#37197;&#32622;&#19981;&#19968;&#23450;&#26159;&#26368;&#20248;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22343;&#34913;&#28151;&#21512;&#36229;&#32593;&#32476;&#65288;BMSN&#65289;&#26469;&#20248;&#21270;&#19979;&#37319;&#26679;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2306.11982</link><description>&lt;p&gt;
&#29992;&#22343;&#34913;&#28151;&#21512;&#30340;&#36229;&#32593;&#32476;&#23398;&#20064;CNN&#27744;&#21270;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Balanced Mixture of SuperNets for Learning the CNN Pooling Architecture. (arXiv:2306.11982v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#19981;&#21516;&#27744;&#21270;&#37197;&#32622;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#39044;&#23450;&#20041;&#30340;&#19979;&#37319;&#26679;&#37197;&#32622;&#19981;&#19968;&#23450;&#26159;&#26368;&#20248;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22343;&#34913;&#28151;&#21512;&#36229;&#32593;&#32476;&#65288;BMSN&#65289;&#26469;&#20248;&#21270;&#19979;&#37319;&#26679;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#37319;&#26679;&#23618;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20915;&#23450;&#22270;&#20687;&#29305;&#24449;&#20998;&#26512;&#30340;&#31890;&#24230;/&#23610;&#24230;&#20197;&#21450;&#32473;&#23450;&#23618;&#30340;&#24863;&#21463;&#37326;&#22823;&#23567;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;ResNet20&#32593;&#32476;&#20998;&#26512;&#20102;&#22312;CIFAR10&#19978;&#21508;&#20010;&#27744;&#21270;&#37197;&#32622;&#21333;&#29420;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#34920;&#26126;&#19979;&#37319;&#26679;&#23618;&#30340;&#20301;&#32622;&#21487;&#20197;&#26497;&#22823;&#22320;&#24433;&#21709;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#39044;&#23450;&#20041;&#30340;&#19979;&#37319;&#26679;&#37197;&#32622;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#21487;&#20197;&#29992;&#20316;&#20248;&#21270;&#19979;&#37319;&#26679;&#37197;&#32622;&#30340;&#36229;&#21442;&#25968;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#21333;&#20010;&#36229;&#32593;&#32476;&#30340;&#24120;&#35265;&#19968;&#27425;&#24615; NAS &#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#19981;&#36215;&#20316;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#20026;&#20102;&#25214;&#21040;&#26368;&#20248;&#27744;&#21270;&#37197;&#32622;&#32780;&#35757;&#32451;&#30340;&#36229;&#32593;&#32476;&#23558;&#20854;&#21442;&#25968;&#23436;&#20840;&#20849;&#20139;&#20110;&#25152;&#26377;&#27744;&#21270;&#37197;&#32622;&#12290;&#36825;&#20351;&#23427;&#30340;&#35757;&#32451;&#38750;&#24120;&#22256;&#38590;&#65292;&#22240;&#20026;&#23398;&#20064;&#29575;&#30340;&#35843;&#25972;&#19981;&#24403;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#21453;&#21521;&#20256;&#25773;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#22343;&#34913;&#28151;&#21512;&#36229;&#32593;&#32476;&#65288;BMSN&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Downsampling layers, including pooling and strided convolutions, are crucial components of the convolutional neural network architecture that determine both the granularity/scale of image feature analysis as well as the receptive field size of a given layer. To fully understand this problem, we analyse the performance of models independently trained with each pooling configurations on CIFAR10, using a ResNet20 network, and show that the position of the downsampling layers can highly influence the performance of a network and predefined downsampling configurations are not optimal. Network Architecture Search (NAS) might be used to optimize downsampling configurations as an hyperparameter. However, we find that common one-shot NAS based on a single SuperNet does not work for this problem. We argue that this is because a SuperNet trained for finding the optimal pooling configuration fully shares its parameters among all pooling configurations. This makes its training hard, because learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#37327;&#23376;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#31934;&#24515;&#21046;&#20316;&#30340;&#36890;&#29992;&#25200;&#21160;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#20004;&#20010;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#36825;&#20026;&#26500;&#24314;&#23433;&#20840;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24102;&#26469;&#28508;&#22312;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2306.11974</link><description>&lt;p&gt;
&#29992;&#20110;&#22810;&#20998;&#31867;&#20219;&#21153;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Universal adversarial perturbations for multiple classification tasks with quantum classifiers. (arXiv:2306.11974v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#37327;&#23376;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#31934;&#24515;&#21046;&#20316;&#30340;&#36890;&#29992;&#25200;&#21160;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#20004;&#20010;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#36825;&#20026;&#26500;&#24314;&#23433;&#20840;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24102;&#26469;&#28508;&#22312;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#38376;&#26032;&#20852;&#30340;&#39046;&#22495;&#65292;&#23427;&#30740;&#31350;&#20102;&#37327;&#23376;&#23398;&#20064;&#31995;&#32479;&#23545;&#25239;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#24182;&#24320;&#21457;&#20102;&#21487;&#33021;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#37327;&#23376;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#26159;&#19968;&#31181;&#23567;&#30340;&#25200;&#21160;&#65292;&#21487;&#20197;&#20351;&#19981;&#21516;&#30340;&#36755;&#20837;&#26679;&#26412;&#25104;&#20026;&#35823;&#23548;&#32473;&#23450;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#31034;&#20363;&#12290;&#23613;&#31649;&#27492;&#39046;&#22495;&#20043;&#21069;&#40092;&#26377;&#25506;&#31350;&#65292;&#20294;&#26159;&#36890;&#29992;&#25200;&#21160;&#21487;&#33021;&#26497;&#22823;&#22320;&#31616;&#21270;&#24694;&#24847;&#25915;&#20987;&#65292;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36896;&#25104;&#24847;&#24819;&#19981;&#21040;&#30340;&#30772;&#22351;&#12290;&#26412;&#25991;&#22312;&#24322;&#26500;&#20998;&#31867;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#65292;&#36827;&#19968;&#27493;&#25506;&#35752;&#37327;&#23376;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20960;&#20046;&#22312;&#20004;&#20010;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#37117;&#21487;&#20197;&#34987;&#19968;&#20010;&#31934;&#24515;&#21046;&#20316;&#30340;&#36890;&#29992;&#25200;&#21160;&#25152;&#35825;&#23548;&#25104;&#21151;&#22320;&#27450;&#39575;&#12290;&#36825;&#19968;&#32467;&#26524;&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;CIFAR-10&#21644;Iris&#20013;&#24471;&#21040;&#26126;&#30830;&#30340;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28508;&#22312;&#23041;&#32961;&#65292;&#24182;&#21487;&#33021;&#32473;&#26500;&#24314;&#23433;&#20840;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24102;&#26469;&#24040;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum adversarial machine learning is an emerging field that studies the vulnerability of quantum learning systems against adversarial perturbations and develops possible defense strategies. Quantum universal adversarial perturbations are small perturbations, which can make different input samples into adversarial examples that may deceive a given quantum classifier. This is a field that was rarely looked into but worthwhile investigating because universal perturbations might simplify malicious attacks to a large extent, causing unexpected devastation to quantum machine learning models. In this paper, we take a step forward and explore the quantum universal perturbations in the context of heterogeneous classification tasks. In particular, we find that quantum classifiers that achieve almost state-of-the-art accuracy on two different classification tasks can be both conclusively deceived by one carefully-crafted universal perturbation. This result is explicitly demonstrated with well-
&lt;/p&gt;</description></item><item><title>AdCraft&#26159;&#19968;&#31181;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20986;&#20215;&#21644;&#39044;&#31639;&#21464;&#21270;&#30340;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;(SEM)&#27963;&#21160;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11971</link><description>&lt;p&gt;
AdCraft&#65306;&#19968;&#31181;&#29992;&#20110;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;&#20248;&#21270;&#30340;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization. (arXiv:2306.11971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11971
&lt;/p&gt;
&lt;p&gt;
AdCraft&#26159;&#19968;&#31181;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20986;&#20215;&#21644;&#39044;&#31639;&#21464;&#21270;&#30340;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;(SEM)&#27963;&#21160;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#8212;&#8212; AdCraft&#65292;&#20854;&#20855;&#26377;&#38543;&#26426;&#21644;&#38750;&#38745;&#24577;&#29305;&#24615;&#12290;&#35813;&#29615;&#22659;&#27169;&#25311;&#20102;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;&#20013;&#20986;&#20215;&#21644;&#39044;&#31639;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;SEM&#26159;&#19968;&#31181;&#21033;&#29992;&#20184;&#36153;&#24191;&#21578;&#26469;&#22686;&#21152;&#32593;&#31449;&#22312;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#19978;&#30340;&#21487;&#35265;&#24615;&#30340;&#25968;&#23383;&#33829;&#38144;&#25216;&#26415;&#12290;SEM&#24191;&#21578;&#27963;&#21160;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#65292;&#21253;&#25324;&#20851;&#38190;&#23383;&#36873;&#25321;&#12289;&#24191;&#21578;&#35774;&#35745;&#12289;&#20986;&#20215;&#31649;&#29702;&#12289;&#39044;&#31639;&#35843;&#25972;&#21644;&#34920;&#29616;&#30417;&#25511;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20248;&#21270;SEM&#24191;&#21578;&#25237;&#25918;&#27963;&#21160;&#30340;&#28508;&#22312;&#31574;&#30053;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#25110;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#30340;&#21487;&#23450;&#21046;&#29615;&#22659;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#35780;&#20272;&#21644;&#25552;&#39640;&#19982;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20184;&#20986;&#36825;&#20123;&#25104;&#26412;&#12290;&#36890;&#36807;&#22312;AdCraft&#29615;&#22659;&#19979;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce \env{}, a novel benchmark environment for the Reinforcement Learning (RL) community distinguished by its stochastic and non-stationary properties. The environment simulates bidding and budgeting dynamics within Search Engine Marketing (SEM), a digital marketing technique utilizing paid advertising to enhance the visibility of websites on search engine results pages (SERPs). The performance of SEM advertisement campaigns depends on several factors, including keyword selection, ad design, bid management, budget adjustments, and performance monitoring. Deep RL recently emerged as a potential strategy to optimize campaign profitability within the complex and dynamic landscape of SEM but it requires substantial data, which may be costly or infeasible to acquire in practice. Our customizable environment enables practitioners to assess and enhance the robustness of RL algorithms pertinent to SEM bid and budget management without such costs. Through a series of experiments within 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;&#34917;&#23398;&#20064;&#23376;&#32593;&#32476;&#30340;&#26080;&#37325;&#28436;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#21487;&#22609;&#24615;CNN&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#26512;&#21069;&#39304;&#20998;&#31867;&#22120;&#26469;&#36798;&#21040;&#22312;&#19981;&#35775;&#38382;&#21382;&#21490;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.11967</link><description>&lt;p&gt;
&#22522;&#20110;&#20114;&#34917;&#23398;&#20064;&#23376;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Complementary Learning Subnetworks for Parameter-Efficient Class-Incremental Learning. (arXiv:2306.11967v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;&#34917;&#23398;&#20064;&#23376;&#32593;&#32476;&#30340;&#26080;&#37325;&#28436;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#21487;&#22609;&#24615;CNN&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#26512;&#21069;&#39304;&#20998;&#31867;&#22120;&#26469;&#36798;&#21040;&#22312;&#19981;&#35775;&#38382;&#21382;&#21490;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;(CIL)&#30340;&#22330;&#26223;&#19979;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24517;&#39035;&#36866;&#24212;&#38750;&#38745;&#24577;&#25968;&#25454;&#20998;&#24067;&#65292;&#20363;&#22914;&#38543;&#26102;&#38388;&#20986;&#29616;&#30340;&#26032;&#31867;&#12290;&#28982;&#32780;&#65292;CIL&#27169;&#22411;&#38754;&#20020;&#30528;&#33261;&#21517;&#26157;&#33879;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#37325;&#28436;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23384;&#20648;&#26087;&#31867;&#30340;&#26679;&#26412;&#26469;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#38480;&#21046;&#20102;&#32771;&#34385;&#20869;&#23384;&#36164;&#28304;&#21644;&#38544;&#31169;&#38382;&#39064;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#37325;&#28436;CIL&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#20114;&#34917;&#23398;&#20064;&#23376;&#32593;&#32476;&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#19981;&#26029;&#22320;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#32852;&#21512;&#20248;&#21270;&#21487;&#22609;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#26512;&#21069;&#39304;&#20998;&#31867;&#22120;&#12290;&#21382;&#21490;&#25968;&#25454;&#30340;&#19981;&#21487;&#35775;&#38382;&#24615;&#36890;&#36807;&#23436;&#25972;&#22320;&#25511;&#21046;&#35757;&#32451;&#33391;&#22909;&#30340;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#22788;&#29702;&#65292;&#30830;&#20445;&#25152;&#23398;&#20064;&#30340;&#20915;&#31574;&#36793;&#30028;&#36866;&#21512;&#26032;&#31867;&#65292;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#23398;&#20064;&#30340;&#31867;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the scenario of class-incremental learning (CIL), deep neural networks have to adapt their model parameters to non-stationary data distributions, e.g., the emergence of new classes over time. However, CIL models are challenged by the well-known catastrophic forgetting phenomenon. Typical methods such as rehearsal-based ones rely on storing exemplars of old classes to mitigate catastrophic forgetting, which limits real-world applications considering memory resources and privacy issues. In this paper, we propose a novel rehearsal-free CIL approach that learns continually via the synergy between two Complementary Learning Subnetworks. Our approach involves jointly optimizing a plastic CNN feature extractor and an analytical feed-forward classifier. The inaccessibility of historical data is tackled by holistically controlling the parameters of a well-trained model, ensuring that the decision boundary learned fits new classes while retaining recognition of previously learned classes. Spe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#20174;&#20010;&#20307;&#20844;&#24179;&#20998;&#24067;&#20013;&#37319;&#26679;&#25490;&#21517;&#65292;&#21516;&#26102;&#30830;&#20445;&#27599;&#20010;&#36755;&#20986;&#30340;&#25490;&#21517;&#37117;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#12290;&#36755;&#20986;&#25490;&#21517;&#30340;&#26399;&#26395;&#25928;&#29992;&#33267;&#23569;&#26159;&#26368;&#20248;&#20844;&#24179;&#35299;&#30340;&#25928;&#29992;&#30340;$\alpha$&#20493;&#65292;&#20854;&#20013;$\alpha$&#26159;&#19968;&#20010;&#37327;&#21270;&#20844;&#24179;&#32422;&#26463;&#32039;&#24230;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.11964</link><description>&lt;p&gt;
&#37319;&#26679;&#20010;&#20307;&#20844;&#24179;&#19988;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#30340;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Sampling Individually-Fair Rankings that are Always Group Fair. (arXiv:2306.11964v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11964
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#20174;&#20010;&#20307;&#20844;&#24179;&#20998;&#24067;&#20013;&#37319;&#26679;&#25490;&#21517;&#65292;&#21516;&#26102;&#30830;&#20445;&#27599;&#20010;&#36755;&#20986;&#30340;&#25490;&#21517;&#37117;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#12290;&#36755;&#20986;&#25490;&#21517;&#30340;&#26399;&#26395;&#25928;&#29992;&#33267;&#23569;&#26159;&#26368;&#20248;&#20844;&#24179;&#35299;&#30340;&#25928;&#29992;&#30340;$\alpha$&#20493;&#65292;&#20854;&#20013;$\alpha$&#26159;&#19968;&#20010;&#37327;&#21270;&#20844;&#24179;&#32422;&#26463;&#32039;&#24230;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24179;&#21488;&#19978;&#30340;&#25490;&#21517;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#25214;&#21040;&#30456;&#20851;&#20449;&#24687;&#65292;&#22914;&#20154;&#29289;&#12289;&#26032;&#38395;&#12289;&#23186;&#20307;&#21644;&#20135;&#21697;&#12290;&#20844;&#24179;&#25490;&#21517;&#26159;&#19968;&#31181;&#20026;&#20102;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#32780;&#20248;&#21270;&#19968;&#32452;&#39033;&#30446;&#25490;&#21517;&#30340;&#20219;&#21153;&#65292;&#24050;&#32463;&#22312;&#31639;&#27861;&#20844;&#24179;&#24615;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#39033;&#30446;&#25928;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#19981;&#20844;&#24179;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#24182;&#24314;&#35758;&#22312;&#36755;&#20986;&#20013;&#24341;&#20837;&#38543;&#26426;&#24615;&#12290;&#36825;&#31181;&#38543;&#26426;&#24615;&#32463;&#36807;&#20180;&#32454;&#36873;&#25321;&#65292;&#20197;&#30830;&#20445;&#23545;&#27599;&#20010;&#39033;&#30446;&#36827;&#34892;&#20805;&#20998;&#19988;&#21512;&#29702;&#30340;&#20195;&#34920;&#65288;&#21516;&#26102;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#38543;&#26426;&#24615;&#65292;&#36755;&#20986;&#30340;&#25490;&#21517;&#21487;&#33021;&#20250;&#36829;&#21453;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20174;&#19968;&#20010;&#20010;&#20307;&#20844;&#24179;&#20998;&#24067;&#20013;&#25277;&#26679;&#25490;&#21517;&#65292;&#21516;&#26102;&#30830;&#20445;&#27599;&#20010;&#36755;&#20986;&#30340;&#25490;&#21517;&#37117;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#12290;&#36755;&#20986;&#25490;&#21517;&#30340;&#26399;&#26395;&#25928;&#29992;&#33267;&#23569;&#26159;&#26368;&#20248;&#20844;&#24179;&#35299;&#30340;&#25928;&#29992;&#30340; $\alpha$ &#20493;&#65292;&#20854;&#20013; $\alpha$ &#26159;&#19968;&#20010;&#37327;&#21270;&#20844;&#24179;&#32422;&#26463;&#32039;&#24230;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rankings on online platforms help their end-users find the relevant information -- people, news, media, and products -- quickly. Fair ranking tasks, which ask to rank a set of items to maximize utility subject to satisfying group-fairness constraints, have gained significant interest in the Algorithmic Fairness, Information Retrieval, and Machine Learning literature. Recent works, however, identify uncertainty in the utilities of items as a primary cause of unfairness and propose introducing randomness in the output. This randomness is carefully chosen to guarantee an adequate representation of each item (while accounting for the uncertainty). However, due to this randomness, the output rankings may violate group fairness constraints. We give an efficient algorithm that samples rankings from an individually-fair distribution while ensuring that every output ranking is group fair. The expected utility of the output ranking is at least $\alpha$ times the utility of the optimal fair solut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TADIL&#26041;&#27861;&#65292;&#20351;&#29992;Transformer&#26368;&#36817;&#20013;&#24515;&#23884;&#20837;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#26080;&#20851;&#30340;&#26465;&#20214;&#19979;&#65292;&#23545;&#22495;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#36827;&#34892;&#35782;&#21035;&#65292;&#24182;&#35757;&#32451;&#20986;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22686;&#37327;&#20219;&#21153;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.11955</link><description>&lt;p&gt;
TADIL&#65306;&#20351;&#29992;Transformer&#26368;&#36817;&#20013;&#24515;&#23884;&#20837;&#36827;&#34892;&#20219;&#21153;&#26080;&#20851;&#22686;&#37327;&#22495;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
TADIL: Task-Agnostic Domain-Incremental Learning through Task-ID Inference using Transformer Nearest-Centroid Embeddings. (arXiv:2306.11955v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TADIL&#26041;&#27861;&#65292;&#20351;&#29992;Transformer&#26368;&#36817;&#20013;&#24515;&#23884;&#20837;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#26080;&#20851;&#30340;&#26465;&#20214;&#19979;&#65292;&#23545;&#22495;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#36827;&#34892;&#35782;&#21035;&#65292;&#24182;&#35757;&#32451;&#20986;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22686;&#37327;&#20219;&#21153;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#38754;&#23545;&#38543;&#26102;&#38388;&#25110;&#39046;&#22495;&#21464;&#21270;&#30340;&#25968;&#25454;&#26102;&#20250;&#20986;&#29616;&#22256;&#38590;&#65292;&#32780;&#20154;&#31867;&#21487;&#20197;&#20174;&#36825;&#20123;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#36830;&#32493;&#23398;&#20064;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#65292;&#29305;&#21035;&#26159;&#22495;&#22686;&#37327;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27969;&#31243;&#65292;&#22312;&#19981;&#38656;&#35201;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#22686;&#37327;&#22495;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#20219;&#21153;&#12290;&#35813;&#27969;&#31243;&#21253;&#21547;&#22235;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#20351;&#29992;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#33719;&#21462;&#22522;&#30784;&#23884;&#20837;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#23427;&#20204;&#19982;&#38598;&#32676;&#20013;&#24515;&#30340;&#30456;&#20284;&#24615;&#65292;&#20998;&#32452;&#23884;&#20837;&#23494;&#24230;&#20197;&#33719;&#21462;&#27599;&#20010;&#38598;&#32676;&#20013;&#24515;&#26368;&#36817;&#30340;&#28857;&#12290;&#31532;&#19977;&#65292;&#21482;&#20351;&#29992;&#36825;&#20123;&#28857;&#35757;&#32451;&#22686;&#37327;&#20219;&#21153;&#20998;&#31867;&#22120;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#27969;&#31243;&#30340;&#36731;&#37327;&#32423;&#35745;&#31639;&#35201;&#27714;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#20351;&#29992;&#20219;&#21153;&#20998;&#31867;&#22120;&#26469;&#20915;&#23450;&#20309;&#26102;&#23398;&#20064;&#26032;&#20219;&#21153;&#24182;&#36873;&#25321;&#35813;&#20219;&#21153;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) models struggle with data that changes over time or across domains due to factors such as noise, occlusion, illumination, or frequency, unlike humans who can learn from such non independent and identically distributed data. Consequently, a Continual Learning (CL) approach is indispensable, particularly, Domain-Incremental Learning. In this paper, we propose a novel pipeline for identifying tasks in domain-incremental learning scenarios without supervision. The pipeline comprises four steps. First, we obtain base embeddings from the raw data using an existing transformer-based model. Second, we group the embedding densities based on their similarity to obtain the nearest points to each cluster centroid. Third, we train an incremental task classifier using only these few points. Finally, we leverage the lightweight computational requirements of the pipeline to devise an algorithm that decides in an online fashion when to learn a new task using the task classifier an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#26377;&#22122;&#38899;&#35745;&#31639;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#38750;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22411;&#19979;&#25152;&#26377;&#22235;&#20010;&#20989;&#25968;&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#22823;&#22810;&#25968;&#19979;&#30028;&#19982;&#19978;&#30028;&#31526;&#21512;&#65292;&#24046;&#24322;&#19981;&#36229;&#36807;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2306.11951</link><description>&lt;p&gt;
&#35770;&#22914;&#20309;&#22788;&#29702;&#26377;&#22122;&#38899;&#30340;&#35745;&#31639;&#38382;&#39064;&#30340;&#26368;&#20248;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
On the Optimal Bounds for Noisy Computing. (arXiv:2306.11951v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#26377;&#22122;&#38899;&#35745;&#31639;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#38750;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22411;&#19979;&#25152;&#26377;&#22235;&#20010;&#20989;&#25968;&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#22823;&#22810;&#25968;&#19979;&#30028;&#19982;&#19978;&#30028;&#31526;&#21512;&#65292;&#24046;&#24322;&#19981;&#36229;&#36807;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;Feige&#31561;&#20154;1994&#24180;&#32771;&#34385;&#30340;&#20351;&#29992;&#26377;&#22122;&#22768;&#30340;&#20449;&#24687;&#36827;&#34892;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#20174;&#26377;&#22122;&#22768;&#30340;&#26597;&#35810;&#20013;&#35745;&#31639;&#24191;&#20041; OR &#20989;&#25968;&#65292;&#20197;&#21450;&#20174;&#26377;&#22122;&#22768;&#30340;&#25104;&#23545;&#27604;&#36739;&#20013;&#35745;&#31639; MAX&#12289;SEARCH &#21644; SORT &#20989;&#25968;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340; $K$ &#20010;&#20803;&#32032;&#65292;&#30446;&#26631;&#26159;&#22312;&#27599;&#20010;&#26597;&#35810;&#30340;&#32467;&#26524;&#20197;&#27010;&#29575; $p$ &#32763;&#36716;&#26102;&#65292;&#20197;&#33267;&#23569; $1-\delta$ &#30340;&#27010;&#29575;&#27491;&#30830;&#22320;&#24674;&#22797;&#25152;&#38656;&#20989;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#33258;&#36866;&#24212;&#37319;&#26679;&#35774;&#32622;&#21644;&#38750;&#33258;&#36866;&#24212;&#37319;&#26679;&#35774;&#32622;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#22351;&#24773;&#20917;&#19979;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#32039;&#23494;&#36793;&#30028;&#65292;&#36825;&#20123;&#36793;&#30028;&#26159;&#20197; $K$ &#20381;&#36182;&#24615;&#20026;&#22522;&#30784;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#20381;&#36182;&#20110; $\delta$ &#21644; $p$ &#26041;&#38754;&#65292;&#19978;&#19979;&#30028;&#24182;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#25552;&#39640;&#20102;&#33258;&#36866;&#24212;&#21644;&#38750;&#33258;&#36866;&#24212;&#26597;&#35810;&#27169;&#22411;&#19979;&#25152;&#26377;&#22235;&#20010;&#20989;&#25968;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#22823;&#22810;&#25968;&#19979;&#30028;&#19982;&#19978;&#30028;&#31526;&#21512;&#65292;&#24046;&#24322;&#19981;&#36229;&#36807;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the problem of computing with noisy information considered in Feige et al. 1994, which includes computing the OR function from noisy queries, and computing the MAX, SEARCH and SORT functions from noisy pairwise comparisons. For $K$ given elements, the goal is to correctly recover the desired function with probability at least $1-\delta$ when the outcome of each query is flipped with probability $p$. We consider both the adaptive sampling setting where each query can be adaptively designed based on past outcomes, and the non-adaptive sampling setting where the query cannot depend on past outcomes. The prior work provides tight bounds on the worst-case query complexity in terms of the dependence on $K$. However, the upper and lower bounds do not match in terms of the dependence on $\delta$ and $p$. We improve the lower bounds for all the four functions under both adaptive and non-adaptive query models. Most of our lower bounds match the upper bounds up to constant factors when
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#25972;&#21512;&#38750;&#32447;&#24615;&#26641;&#31361;&#32467;&#26500;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#23481;&#37327;&#21644;&#24615;&#33021;&#65292;&#21516;&#26102;&#25511;&#21046;&#20449;&#21495;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#26410;&#26469;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.11950</link><description>&lt;p&gt;
&#32531;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36890;&#20449;&#25104;&#26412;&#65306;&#26641;&#31361;&#38750;&#32447;&#24615;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mitigating Communication Costs in Neural Networks: The Role of Dendritic Nonlinearity. (arXiv:2306.11950v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#25972;&#21512;&#38750;&#32447;&#24615;&#26641;&#31361;&#32467;&#26500;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#23481;&#37327;&#21644;&#24615;&#33021;&#65292;&#21516;&#26102;&#25511;&#21046;&#20449;&#21495;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#26410;&#26469;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35299;&#28145;&#21051;&#22320;&#24433;&#21709;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;ANN&#20013;&#20351;&#29992;&#30340;&#31070;&#32463;&#20803;&#19982;&#20854;&#29983;&#29289;&#27169;&#22411;&#23384;&#22312;&#26126;&#26174;&#20559;&#24046;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#23569;&#21253;&#21547;&#23616;&#37096;&#38750;&#32447;&#24615;&#30340;&#22797;&#26434;&#26641;&#31361;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#26679;&#30340;&#24046;&#24322;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#28857;&#31070;&#32463;&#20803;&#21487;&#20197;&#22312;&#25191;&#34892;&#35745;&#31639;&#20219;&#21153;&#26041;&#38754;&#22312;&#21151;&#33021;&#19978;&#26367;&#20195;&#26641;&#31361;&#31070;&#32463;&#20803;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#32447;&#24615;&#26641;&#31361;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26641;&#31361;&#32467;&#26500;&#38750;&#32447;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25972;&#21512;&#26641;&#31361;&#32467;&#26500;&#21487;&#20197;&#22312;&#20445;&#25345;&#20449;&#21495;&#36890;&#20449;&#25104;&#26412;&#26377;&#25928;&#25233;&#21046;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#22686;&#24378;&#27169;&#22411;&#23481;&#37327;&#21644;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#65292;&#23545;&#26410;&#26469;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our comprehension of biological neuronal networks has profoundly influenced the evolution of artificial neural networks (ANNs). However, the neurons employed in ANNs exhibit remarkable deviations from their biological analogs, mainly due to the absence of complex dendritic trees encompassing local nonlinearity. Despite such disparities, previous investigations have demonstrated that point neurons can functionally substitute dendritic neurons in executing computational tasks. In this study, we scrutinized the importance of nonlinear dendrites within neural networks. By employing machine-learning methodologies, we assessed the impact of dendritic structure nonlinearity on neural network performance. Our findings reveal that integrating dendritic structures can substantially enhance model capacity and performance while keeping signal communication costs effectively restrained. This investigation offers pivotal insights that hold considerable implications for the development of future neur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22810;&#20010;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20908;&#23567;&#40614;&#30340;&#20135;&#37327;&#65292;&#32467;&#26524;&#34920;&#26126;&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#30340;&#20915;&#23450;&#24615;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.11946</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#20010;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#39044;&#27979;&#20908;&#23567;&#40614;&#30340;&#20135;&#37327;
&lt;/p&gt;
&lt;p&gt;
Winter Wheat Crop Yield Prediction on Multiple Heterogeneous Datasets using Machine Learning. (arXiv:2306.11946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22810;&#20010;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20908;&#23567;&#40614;&#30340;&#20135;&#37327;&#65292;&#32467;&#26524;&#34920;&#26126;&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#30340;&#20915;&#23450;&#24615;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20908;&#23567;&#40614;&#26159;&#33521;&#22269;&#26368;&#37325;&#35201;&#30340;&#20316;&#29289;&#20043;&#19968;&#65292;&#32780;&#20316;&#29289;&#20135;&#37327;&#30340;&#39044;&#27979;&#23545;&#22269;&#23478;&#30340;&#31918;&#39135;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#26377;&#22810;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21439;&#25110;&#22522;&#20110;&#20892;&#22330;&#30340;&#23618;&#38754;&#19978;&#39044;&#27979;&#20316;&#29289;&#20135;&#37327;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#20010;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#65288;&#21363;&#21306;&#22495;&#23618;&#38754;&#19978;&#30340;&#22303;&#22756;&#21644;&#22825;&#27668;&#65289;&#39044;&#27979;&#20908;&#23567;&#40614;&#30340;&#20135;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#25968;&#25454;&#21333;&#29420;&#21644;&#32467;&#21512;&#20351;&#29992;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#24378;&#35843;&#25968;&#25454;&#36136;&#37327;&#22312;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Winter wheat is one of the most important crops in the United Kingdom, and crop yield prediction is essential for the nation's food security. Several studies have employed machine learning (ML) techniques to predict crop yield on a county or farm-based level. The main objective of this study is to predict winter wheat crop yield using ML models on multiple heterogeneous datasets, i.e., soil and weather on a zone-based level. Experimental results demonstrated their impact when used alone and in combination. In addition, we employ numerous ML algorithms to emphasize the significance of data quality in any machine-learning strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#36229;&#36234;&#34920;&#38754;&#24418;&#24335;&#29305;&#24449;&#65292;&#23398;&#20064;&#31934;&#30830;&#32780;&#24418;&#24335;&#21270;&#23450;&#20041;&#30340;&#20195;&#30721;&#30340;&#35745;&#31639;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.11943</link><description>&lt;p&gt;
&#25506;&#31350;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#25152;&#23398;&#20064;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding What Code Language Models Learned. (arXiv:2306.11943v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#36229;&#36234;&#34920;&#38754;&#24418;&#24335;&#29305;&#24449;&#65292;&#23398;&#20064;&#31934;&#30830;&#32780;&#24418;&#24335;&#21270;&#23450;&#20041;&#30340;&#20195;&#30721;&#30340;&#35745;&#31639;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#37117;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26377;&#20154;&#35748;&#20026;&#23427;&#20204;&#30340;&#33021;&#21147;&#19981;&#36275;&#20197;&#23436;&#20840;&#23398;&#20064;&#35821;&#35328;&#30340;&#24847;&#20041;&#25110;&#29702;&#35299;&#35821;&#35328;&#12290;&#20026;&#20102;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#26576;&#31181;&#24418;&#24335;&#30340;&#24847;&#20041;&#30340;&#31243;&#24230;&#65292;&#25105;&#20204;&#30740;&#31350;&#23427;&#20204;&#25429;&#25417;&#20195;&#30721;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#36229;&#36234;&#34920;&#23618;&#39057;&#29575;&#21644;&#20849;&#29616;&#30340;&#38480;&#21046;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#27169;&#22411;&#35821;&#35328;&#29305;&#24449;&#30340;&#25506;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#19968;&#31181;&#21487;&#20197;&#23458;&#35266;&#22320;&#12289;&#31616;&#21333;&#26126;&#20102;&#22320;&#35780;&#20272;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;&#33021;&#21147;&#30340;&#29615;&#22659;&#19979;&#30740;&#31350;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#25429;&#25417;&#31934;&#30830;&#32780;&#24418;&#24335;&#21270;&#23450;&#20041;&#30340;&#20195;&#30721;&#30340;&#35821;&#20041;&#12290;&#36890;&#36807;&#23545;&#20195;&#30721;&#29255;&#27573;&#30340;&#25805;&#32437;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#30721;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#20102;&#20195;&#30721;&#30340;&#35745;&#31639;&#35821;&#20041;&#30340;&#24378;&#26377;&#21147;&#30340;&#34920;&#24449;&#65292;&#36229;&#36234;&#20102;&#20195;&#30721;&#34920;&#38754;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models are effective in a variety of natural language tasks, but it has been argued their capabilities fall short of fully learning meaning or understanding language. To understand the extent to which language models can learn some form of meaning, we investigate their ability to capture semantics of code beyond superficial frequency and co-occurrence. In contrast to previous research on probing models for linguistic features, we study pre-trained models in a setting that allows for objective and straightforward evaluation of a model's ability to learn semantics. In this paper, we examine whether such models capture the semantics of code, which is precisely and formally defined. Through experiments involving the manipulation of code fragments, we show that code pre-trained models of code learn a robust representation of the computational semantics of code that goes beyond superficial features of form alone
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#22312;&#25968;&#23383;&#20892;&#19994;&#39046;&#22495;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20854;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.11942</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#38598;&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#20197;&#20908;&#23567;&#40614;&#20135;&#37327;&#39044;&#27979;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Model for Heterogeneous Dataset Analysis -- Application to Winter Wheat Crop Yield Prediction. (arXiv:2306.11942v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#22312;&#25968;&#23383;&#20892;&#19994;&#39046;&#22495;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20854;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35199;&#26041;&#22269;&#23478;&#20005;&#37325;&#20381;&#36182;&#23567;&#40614;&#65292;&#20135;&#37327;&#39044;&#27979;&#21313;&#20998;&#20851;&#38190;&#12290;&#24050;&#32463;&#26377;&#20154;&#25506;&#32034;&#24182;&#24212;&#29992;&#20102;&#29992;&#20110;&#20135;&#37327;&#39044;&#27979;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31561;&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#24050;&#26377;&#25991;&#29486;&#25253;&#21578;&#26174;&#31034;&#23427;&#20204;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340; LSTMs &#26080;&#27861;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#38598;&#65288;&#30001;&#26102;&#21464;&#21644;&#20445;&#25345;&#19981;&#21464;&#30340;&#25968;&#25454;&#32452;&#21512;&#32780;&#25104;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#31995;&#32479;&#26550;&#26500;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25968;&#23383;&#20892;&#19994;&#39046;&#22495;&#20013;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Western countries rely heavily on wheat, and yield prediction is crucial. Time-series deep learning models, such as Long Short Term Memory (LSTM), have already been explored and applied to yield prediction. Existing literature reported that they perform better than traditional Machine Learning (ML) models. However, the existing LSTM cannot handle heterogeneous datasets (a combination of data which varies and remains static with time). In this paper, we propose an efficient deep learning model that can deal with heterogeneous datasets. We developed the system architecture and applied it to the real-world dataset in the digital agriculture area. We showed that it outperforms the existing ML models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#27979;&#24230;&#19978;&#32534;&#20889;&#21464;&#20998;&#30446;&#26631;&#30340;&#21160;&#26426;&#65292;&#25552;&#20986;&#36890;&#36807;&#27492;&#31867;&#30446;&#26631;&#25512;&#23548;&#23454;&#29992;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#31561;&#38382;&#39064;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11928</link><description>&lt;p&gt;
&#24320;&#25918;&#38382;&#39064;&#65306;&#22522;&#20110;&#21464;&#20998;&#30446;&#26631;&#30340;&#27979;&#24230;&#23398;&#20064; (arXiv:2306.11928v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
Open Problem: Learning with Variational Objectives on Measures. (arXiv:2306.11928v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#27979;&#24230;&#19978;&#32534;&#20889;&#21464;&#20998;&#30446;&#26631;&#30340;&#21160;&#26426;&#65292;&#25552;&#20986;&#36890;&#36807;&#27492;&#31867;&#30446;&#26631;&#25512;&#23548;&#23454;&#29992;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#31561;&#38382;&#39064;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20851;&#27880;&#30340;&#26159;&#22522;&#20110;&#20989;&#25968;&#30340;&#21464;&#20998;&#30446;&#26631;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#27979;&#24230;&#19978;&#32534;&#20889;&#31867;&#20284;&#30446;&#26631;&#30340;&#21160;&#26426;&#65292;&#29305;&#21035;&#26159;&#35752;&#35770;&#20102;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#65306;&#33021;&#21542;&#23558;&#36890;&#24120;&#30340;&#32479;&#35745;&#23398;&#20064;&#32467;&#26524;&#36716;&#21270;&#20026;&#22522;&#20110;&#27979;&#37327;&#34920;&#36798;&#30340;&#30446;&#26631;&#65311;&#32467;&#26524;&#26500;&#24314;&#26159;&#21542;&#20250;&#23548;&#33268;&#26032;&#30340;&#23454;&#29992;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
The theory of statistical learning has focused on variational objectives expressed on functions. In this note, we discuss motivations to write similar objectives on measures, in particular to discuss out-of-distribution generalization and weakly-supervised learning. It raises a natural question: can one cast usual statistical learning results to objectives expressed on measures? Does the resulting construction lead to new algorithms of practical interest?
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26679;&#26412;&#26799;&#24230;&#27839;&#20248;&#21270;&#36335;&#24452;&#30340;&#22522;&#26412;&#20960;&#20309;&#24615;&#36136;&#65292;&#21457;&#29616;&#36825;&#20123;&#37327;&#22312;&#35757;&#32451;&#26399;&#38388;&#34920;&#29616;&#20986;&#21487;&#39044;&#27979;&#12289;&#19968;&#33268;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20248;&#21270;&#36712;&#36857;&#19981;&#20165;&#20174;&#26410;&#36935;&#21040;&#26174;&#33879;&#30340;&#38556;&#30861;&#65292;&#32780;&#19988;&#22312;&#22823;&#22810;&#25968;&#35757;&#32451;&#26399;&#38388;&#33021;&#22815;&#20445;&#25345;&#31283;&#23450;&#30340;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2306.11922</link><description>&lt;p&gt;
&#27809;&#26377;&#38169;&#35823;&#30340;&#36716;&#24367;&#65306;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#36335;&#24452;&#30340;&#31616;&#21333;&#20960;&#20309;&#23398;
&lt;/p&gt;
&lt;p&gt;
No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths. (arXiv:2306.11922v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26679;&#26412;&#26799;&#24230;&#27839;&#20248;&#21270;&#36335;&#24452;&#30340;&#22522;&#26412;&#20960;&#20309;&#24615;&#36136;&#65292;&#21457;&#29616;&#36825;&#20123;&#37327;&#22312;&#35757;&#32451;&#26399;&#38388;&#34920;&#29616;&#20986;&#21487;&#39044;&#27979;&#12289;&#19968;&#33268;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20248;&#21270;&#36712;&#36857;&#19981;&#20165;&#20174;&#26410;&#36935;&#21040;&#26174;&#33879;&#30340;&#38556;&#30861;&#65292;&#32780;&#19988;&#22312;&#22823;&#22810;&#25968;&#35757;&#32451;&#26399;&#38388;&#33021;&#22815;&#20445;&#25345;&#31283;&#23450;&#30340;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#21160;&#24577;&#23545;&#20110;&#24357;&#21512;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#26159;&#24517;&#35201;&#30340;&#12290;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#33021;&#22815;&#26377;&#25928;&#22320;&#23450;&#20301;&#26377;&#21033;&#30340;&#26497;&#23567;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25928;&#29575;&#19982;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26223;&#35266;&#30340;&#38750;&#20984;&#21644;&#30475;&#20284;&#22797;&#26434;&#30340;&#32467;&#26500;&#24418;&#25104;&#20102;&#23545;&#27604;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#26679;&#26412;&#26799;&#24230;&#27839;&#20248;&#21270;&#36335;&#24452;&#30340;&#22522;&#26412;&#20960;&#20309;&#24615;&#36136;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#20851;&#38190;&#37327;&#65292;&#36825;&#20123;&#37327;&#20986;&#29616;&#22312;&#21463;&#38480;&#21106;&#32447;&#19981;&#31561;&#24335;&#21644;&#35823;&#24046;&#30028;&#38480;&#20013;&#12290;&#36825;&#20004;&#20010;&#37327;&#23545;&#20110;&#19968;&#38454;&#20248;&#21270;&#20855;&#26377;&#39640;&#24230;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#37327;&#22312;&#35757;&#32451;&#26399;&#38388;&#34920;&#29616;&#20986;&#21487;&#39044;&#27979;&#12289;&#19968;&#33268;&#30340;&#34892;&#20026;&#65292;&#23613;&#31649;&#37319;&#26679;&#23567;&#25209;&#37327;&#24341;&#36215;&#30340;&#38543;&#26426;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20248;&#21270;&#36712;&#36857;&#19981;&#20165;&#20174;&#26410;&#36935;&#21040;&#26174;&#33879;&#30340;&#38556;&#30861;&#65292;&#32780;&#19988;&#22312;&#22823;&#22810;&#25968;&#35757;&#32451;&#26399;&#38388;&#33021;&#22815;&#20445;&#25345;&#31283;&#23450;&#30340;&#21160;&#21147;&#23398;&#12290;&#36825;&#20123;&#35266;&#23519;&#21040;&#30340;&#29305;&#24615;&#24050;&#32463;&#36275;&#22815;
&lt;/p&gt;
&lt;p&gt;
Understanding the optimization dynamics of neural networks is necessary for closing the gap between theory and practice. Stochastic first-order optimization algorithms are known to efficiently locate favorable minima in deep neural networks. This efficiency, however, contrasts with the non-convex and seemingly complex structure of neural loss landscapes. In this study, we delve into the fundamental geometric properties of sampled gradients along optimization paths. We focus on two key quantities, which appear in the restricted secant inequality and error bound. Both hold high significance for first-order optimization. Our analysis reveals that these quantities exhibit predictable, consistent behavior throughout training, despite the stochasticity induced by sampling minibatches. Our findings suggest that not only do optimization trajectories never encounter significant obstacles, but they also maintain stable dynamics during the majority of training. These observed properties are suffi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#38598;&#25104; Q &#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#35823;&#24046;&#21453;&#39304;&#26469;&#20943;&#23567;&#38598;&#25104;&#26041;&#27861;&#20013;&#20272;&#35745;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#32467;&#21512;&#27169;&#22411;&#35782;&#21035;&#33258;&#36866;&#24212;&#25511;&#21046;&#65288;MIAC&#65289;&#26469;&#23454;&#29616;&#38598;&#25104;&#22823;&#23567;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2306.11918</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38598;&#25104; Q &#23398;&#20064;&#65306;&#36890;&#36807;&#35823;&#24046;&#21453;&#39304;&#26469;&#20943;&#23567;&#20272;&#35745;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Adaptive Ensemble Q-learning: Minimizing Estimation Bias via Error Feedback. (arXiv:2306.11918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#38598;&#25104; Q &#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#35823;&#24046;&#21453;&#39304;&#26469;&#20943;&#23567;&#38598;&#25104;&#26041;&#27861;&#20013;&#20272;&#35745;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#32467;&#21512;&#27169;&#22411;&#35782;&#21035;&#33258;&#36866;&#24212;&#25511;&#21046;&#65288;MIAC&#65289;&#26469;&#23454;&#29616;&#38598;&#25104;&#22823;&#23567;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#26159;&#32531;&#35299; Q-learning &#20013;&#36807;&#24230;&#20272;&#35745;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#22810;&#20010;&#20989;&#25968;&#36924;&#36817;&#22120;&#26469;&#20272;&#35745;&#21160;&#20316;&#20540;&#12290;&#20272;&#35745;&#20559;&#35823;&#20005;&#37325;&#20381;&#36182;&#20110;&#38598;&#25104;&#22823;&#23567;&#65288;&#21363;&#30446;&#26631;&#20013;&#20351;&#29992;&#30340; Q &#20989;&#25968;&#36924;&#36817;&#22120;&#25968;&#37327;&#65289;&#65292;&#22240;&#27492;&#20915;&#23450;&#8220;&#27491;&#30830;&#8221;&#30340;&#38598;&#25104;&#22823;&#23567;&#38750;&#24120;&#22256;&#38590;&#65292;&#22240;&#20026;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#20272;&#35745;&#20559;&#24046;&#23548;&#20986;&#20102;&#19978;&#38480;&#21644;&#19979;&#38480;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#30028;&#23545;&#38598;&#25104;&#22823;&#23567;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#20174;&#32780;&#23558;&#20559;&#24046;&#39537;&#21160;&#21040;&#25509;&#36817;&#38646;&#65292;&#22240;&#27492;&#21487;&#20197;&#30456;&#24212;&#22320;&#22788;&#29702;&#26102;&#38388;&#21464;&#21270;&#36924;&#36817;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#38598;&#25104;&#26041;&#27861;&#19982;&#27169;&#22411;&#35782;&#21035;&#33258;&#36866;&#24212;&#25511;&#21046;&#65288;MIAC&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#38598;&#25104; Q &#23398;&#20064;&#65288;AdaEQ&#65289;&#65292;&#26159;&#19968;&#31181;&#24191;&#20041;&#30340; Q &#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#33258;&#36866;&#24212;&#38598;&#25104;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ensemble method is a promising way to mitigate the overestimation issue in Q-learning, where multiple function approximators are used to estimate the action values. It is known that the estimation bias hinges heavily on the ensemble size (i.e., the number of Q-function approximators used in the target), and that determining the `right' ensemble size is highly nontrivial, because of the time-varying nature of the function approximation errors during the learning process. To tackle this challenge, we first derive an upper bound and a lower bound on the estimation bias, based on which the ensemble size is adapted to drive the bias to be nearly zero, thereby coping with the impact of the time-varying approximation errors accordingly. Motivated by the theoretic findings, we advocate that the ensemble method can be combined with Model Identification Adaptive Control (MIAC) for effective ensemble size adaptation. Specifically, we devise Adaptive Ensemble Q-learning (AdaEQ), a generalized 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#22270;&#30340;&#19981;&#21516;&#39044;&#23450;&#20041;&#32467;&#26500;&#29983;&#25104;&#23545;&#24212;&#30340;&#40065;&#26834;&#24615;&#35777;&#20070;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#39046;&#20808;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#40065;&#26834;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.11915</link><description>&lt;p&gt;
&#22270;&#20998;&#31867;&#38382;&#39064;&#20013;&#32467;&#26500;&#24863;&#30693;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Structure-Aware Robustness Certificates for Graph Classification. (arXiv:2306.11915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11915
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#22270;&#30340;&#19981;&#21516;&#39044;&#23450;&#20041;&#32467;&#26500;&#29983;&#25104;&#23545;&#24212;&#30340;&#40065;&#26834;&#24615;&#35777;&#20070;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#39046;&#20808;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#40065;&#26834;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20445;&#35777;&#23433;&#20840;&#24615;&#30340;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#29992;&#20110;&#22270;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35777;&#26126;&#20445;&#35777;&#19982;&#33410;&#28857;&#23545;&#32763;&#36716;&#65288;&#28155;&#21152;&#25110;&#21024;&#38500;&#36793;&#32536;&#65289;&#30340;&#24635;&#25968;&#26377;&#20851;&#65292;&#36825;&#30456;&#24403;&#20110;&#20197;&#37051;&#25509;&#30697;&#38453;&#20026;&#20013;&#24515;&#30340;l0&#29699;&#12290;&#23613;&#31649;&#20174;&#29702;&#35770;&#19978;&#30475;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#36825;&#31181;&#21508;&#21521;&#21516;&#24615;&#30340;&#32467;&#26500;&#22122;&#22768;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#21487;&#33021;&#36807;&#20110;&#20005;&#26684;&#65292;&#22240;&#20026;&#26377;&#20123;&#33410;&#28857;&#23545;&#20110;&#30830;&#23450;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#26356;&#20026;&#20851;&#38190;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35777;&#20070;&#32473;&#20986;&#20102;&#23545;&#22270;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24754;&#35266;&#25551;&#36848;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#23558;&#38750;&#21508;&#21521;&#21516;&#24615;&#30340;&#22122;&#22768;&#20998;&#24067;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#32467;&#26500;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36807;&#31243;&#20026;&#20998;&#31867;&#22120;&#29983;&#25104;&#20102;&#32467;&#26500;&#24863;&#30693;&#30340;&#35777;&#20070;&#65292;&#22240;&#27492;&#40065;&#26834;&#24615;&#35777;&#20070;&#30340;&#22823;&#23567;&#21487;&#20197;&#22312;&#22270;&#30340;&#19981;&#21516;&#39044;&#23450;&#20041;&#32467;&#26500;&#20043;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certifying the robustness of a graph-based machine learning model poses a critical challenge for safety. Current robustness certificates for graph classifiers guarantee output invariance with respect to the total number of node pair flips (edge addition or edge deletion), which amounts to an $l_{0}$ ball centred on the adjacency matrix. Although theoretically attractive, this type of isotropic structural noise can be too restrictive in practical scenarios where some node pairs are more critical than others in determining the classifier's output. The certificate, in this case, gives a pessimistic depiction of the robustness of the graph model. To tackle this issue, we develop a randomised smoothing method based on adding an anisotropic noise distribution to the input graph structure. We show that our process generates structural-aware certificates for our classifiers, whereby the magnitude of robustness certificates can vary across different pre-defined structures of the graph. We demon
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#37327;&#21270;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#20004;&#32423;&#37327;&#21270;&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#30340;&#20445;&#35777;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#38544;&#31169;&#25928;&#29992;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.11913</link><description>&lt;p&gt;
&#38543;&#26426;&#37327;&#21270;&#65292;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#30340;&#21807;&#19968;&#25152;&#38656;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized Quantization is All You Need for Differential Privacy in Federated Learning. (arXiv:2306.11913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11913
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#37327;&#21270;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#20004;&#32423;&#37327;&#21270;&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#30340;&#20445;&#35777;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#38544;&#31169;&#25928;&#29992;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#35265;&#21644;&#23454;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#12290;&#20998;&#25955;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#21160;&#26426;&#26159;&#25968;&#25454;&#38544;&#31169;&#65292;&#20445;&#35777;&#23398;&#20064;&#32773;&#27704;&#36828;&#19981;&#20250;&#26597;&#30475;&#27599;&#20010;&#26412;&#22320;&#28304;&#30340;&#25968;&#25454;&#12290;&#26368;&#22823;&#30340;&#25361;&#25112;&#26159;&#22788;&#29702;&#26381;&#21153;&#22120;&#21644;&#22823;&#37327;&#25968;&#25454;&#28304;&#20043;&#38388;&#30340;&#21487;&#33021;&#22797;&#26434;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#21450;&#26412;&#22320;&#26356;&#26032;&#26412;&#36523;&#21487;&#33021;&#20250;&#27844;&#38706;&#26377;&#20851;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#37327;&#21270;&#21644;&#24046;&#20998;&#38544;&#31169;&#30456;&#32467;&#21512;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;RQM&#65292;&#36890;&#36807;&#20004;&#20010;&#32423;&#21035;&#30340;&#38543;&#26426;&#37327;&#21270;&#33719;&#24471;&#38544;&#31169;&#24615;&#12290;&#35813;&#26426;&#21046;&#20026;&#20256;&#36755;&#30340;&#26356;&#26032;&#21644;&#27599;&#20010;&#25968;&#25454;&#28304;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20026;&#22312;FL&#20013;&#25191;&#34892;RQM&#30340;&#24635;&#38544;&#31169;&#25104;&#26412;&#25552;&#20379;&#20102;&#19978;&#38480;&#65292;&#24182;&#34920;&#26126;&#35813;&#26426;&#21046;&#22312;&#38544;&#31169;&#25928;&#29992;&#24179;&#34913;&#26041;&#38754;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a common and practical framework for learning a machine model in a decentralized fashion. A primary motivation behind this decentralized approach is data privacy, ensuring that the learner never sees the data of each local source itself. Federated learning then comes with two majors challenges: one is handling potentially complex model updates between a server and a large number of data sources; the other is that de-centralization may, in fact, be insufficient for privacy, as the local updates themselves can reveal information about the sources' data. To address these issues, we consider an approach to federated learning that combines quantization and differential privacy. Absent privacy, Federated Learning often relies on quantization to reduce communication complexity. We build upon this approach and develop a new algorithm called the \textbf{R}andomized \textbf{Q}uantization \textbf{M}echanism (RQM), which obtains privacy through a two-levels of randomizat
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#21442;&#25968;&#21270;&#29983;&#23384;&#27169;&#22411;&#65292;&#36890;&#36807;&#25918;&#23485;&#26465;&#20214;&#29420;&#31435;&#24615;&#30340;&#20551;&#35774;&#65292;&#25193;&#23637;&#29616;&#20195;&#38750;&#32447;&#24615;&#29983;&#23384;&#20998;&#26512;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#36827;&#20102;&#23545;&#29983;&#23384;&#20998;&#24067;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.11912</link><description>&lt;p&gt;
&#22522;&#20110;Copula&#30340;&#20381;&#36182;&#25130;&#23614;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Copula-Based Deep Survival Models for Dependent Censoring. (arXiv:2306.11912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11912
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#21442;&#25968;&#21270;&#29983;&#23384;&#27169;&#22411;&#65292;&#36890;&#36807;&#25918;&#23485;&#26465;&#20214;&#29420;&#31435;&#24615;&#30340;&#20551;&#35774;&#65292;&#25193;&#23637;&#29616;&#20195;&#38750;&#32447;&#24615;&#29983;&#23384;&#20998;&#26512;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#36827;&#20102;&#23545;&#29983;&#23384;&#20998;&#24067;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#25968;&#25454;&#38598;&#25551;&#36848;&#20102;&#19968;&#32452;&#23454;&#20363;&#65288;&#20363;&#22914;&#24739;&#32773;&#65289;&#65292;&#24182;&#20026;&#27599;&#20010;&#23454;&#20363;&#25552;&#20379;&#20102;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#65288;&#20363;&#22914;&#27515;&#20129;&#65289;&#25110;&#25130;&#23614;&#26102;&#38388;&#65288;&#20363;&#22914;&#22833;&#21435;&#38543;&#35775;&#30340;&#26102;&#38388; - &#36825;&#26159;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#30340;&#19979;&#38480;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#21442;&#25968;&#21270;&#29983;&#23384;&#27169;&#22411;&#65292;&#36890;&#36807;&#25918;&#23485;&#26465;&#20214;&#29420;&#31435;&#24615;&#30340;&#20551;&#35774;&#65292;&#25193;&#23637;&#29616;&#20195;&#38750;&#32447;&#24615;&#29983;&#23384;&#20998;&#26512;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#36827;&#20102;&#23545;&#29983;&#23384;&#20998;&#24067;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
A survival dataset describes a set of instances (e.g. patients) and provides, for each, either the time until an event (e.g. death), or the censoring time (e.g. when lost to follow-up - which is a lower bound on the time until the event). We consider the challenge of survival prediction: learning, from such data, a predictive model that can produce an individual survival distribution for a novel instance. Many contemporary methods of survival prediction implicitly assume that the event and censoring distributions are independent conditional on the instance's covariates - a strong assumption that is difficult to verify (as we observe only one outcome for each instance) and which can induce significant bias when it does not hold. This paper presents a parametric model of survival that extends modern non-linear survival analysis by relaxing the assumption of conditional independence. On synthetic and semi-synthetic data, our approach significantly improves estimates of survival distributi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#20351;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#22312;&#26080;&#26799;&#24230;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#33410;&#30465;&#20102;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.11908</link><description>&lt;p&gt;
&#22522;&#20110;&#23450;&#28857;&#26641;&#30340;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Accelerating Generalized Random Forests with Fixed-Point Trees. (arXiv:2306.11908v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#20351;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#22312;&#26080;&#26799;&#24230;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#33410;&#30465;&#20102;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#24314;&#31435;&#22312;&#20256;&#32479;&#38543;&#26426;&#26862;&#26519;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#33258;&#36866;&#24212;&#26680;&#21152;&#26435;&#31639;&#27861;&#26469;&#26500;&#24314;&#20272;&#31639;&#22120;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26641;&#29983;&#38271;&#36807;&#31243;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#22522;&#20110;&#23450;&#28857;&#36845;&#20195;&#36817;&#20284;&#34920;&#31034;&#26799;&#24230;&#36817;&#20284;&#65292;&#23454;&#29616;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#20026;&#27492;&#24320;&#21457;&#20102;&#28176;&#36817;&#29702;&#35770;&#12290;&#36825;&#26377;&#25928;&#22320;&#33410;&#30465;&#20102;&#26102;&#38388;&#65292;&#23588;&#20854;&#26159;&#22312;&#30446;&#26631;&#37327;&#30340;&#32500;&#24230;&#36866;&#20013;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized random forests arXiv:1610.01271 build upon the well-established success of conventional forests (Breiman, 2001) to offer a flexible and powerful non-parametric method for estimating local solutions of heterogeneous estimating equations. Estimators are constructed by leveraging random forests as an adaptive kernel weighting algorithm and implemented through a gradient-based tree-growing procedure. By expressing this gradient-based approximation as being induced from a single Newton-Raphson root-finding iteration, and drawing upon the connection between estimating equations and fixed-point problems arXiv:2110.11074, we propose a new tree-growing rule for generalized random forests induced from a fixed-point iteration type of approximation, enabling gradient-free optimization, and yielding substantial time savings for tasks involving even modest dimensionality of the target quantity (e.g. multiple/multi-level treatment effects). We develop an asymptotic theory for estimators o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Deep Fusion&#65292;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#30340;&#39640;&#25928;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12289;&#38477;&#20302;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23548;&#33268;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20351;&#24471;&#35813;&#26041;&#27861;&#22312;&#32500;&#25345;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#24615;&#33021;&#29978;&#33267;&#36229;&#36234;&#20854;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.11903</link><description>&lt;p&gt;
Deep Fusion&#65306;&#22522;&#20110;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#30340;&#39640;&#25928;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Deep Fusion: Efficient Network Training via Pre-trained Initializations. (arXiv:2306.11903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Deep Fusion&#65292;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#30340;&#39640;&#25928;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12289;&#38477;&#20302;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23548;&#33268;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20351;&#24471;&#35813;&#26041;&#27861;&#22312;&#32500;&#25345;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#24615;&#33021;&#29978;&#33267;&#36229;&#36234;&#20854;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20247;&#22810;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Deep Fusion&#65292;&#19968;&#31181;&#21033;&#29992;&#36739;&#23567;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#30340;&#39640;&#25928;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Deep Fusion&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#21644;T5&#27169;&#22411;&#22823;&#23567;&#19978;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#38477;&#20302;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23548;&#33268;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Deep Fusion&#26159;&#19968;&#31181;&#23454;&#29992;&#21644;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32500;&#25345;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#24615;&#33021;&#29978;&#33267;&#36229;&#36234;&#20854;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has made remarkable progress in a wide range of domains, with a particularly notable impact on natural language processing tasks. One of the challenges associated with training deep neural networks is the need for large amounts of computational resources and time. In this paper, we present Deep Fusion, an efficient approach to network training that leverages pre-trained initializations of smaller networks. % We show that Deep Fusion accelerates the training process, reduces computational requirements, and leads to improved generalization performance on a variety of NLP tasks and T5 model sizes. % Our experiments demonstrate that Deep Fusion is a practical and effective approach to reduce the training time and resource consumption while maintaining, or even surpassing, the performance of traditional training methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21516;&#27493;&#36752;&#23556;&#23454;&#39564;&#20013;&#30340;X&#23556;&#32447;&#21453;&#23556;&#27979;&#37327;&#65292;&#24418;&#25104;&#38381;&#29615;&#21453;&#39304;&#31995;&#32479;&#65292;&#24182;&#23454;&#29616;&#20102;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#20998;&#26512;&#30340;&#23454;&#26102;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2306.11899</link><description>&lt;p&gt;
&#38381;&#29615;&#21453;&#39304;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21516;&#27493;&#36752;&#23556;&#23454;&#39564;&#33258;&#20027;&#21270;&#22312;&#32447;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Closing the loop: Autonomous experiments enabled by machine-learning-based online data analysis in synchrotron beamline environments. (arXiv:2306.11899v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21516;&#27493;&#36752;&#23556;&#23454;&#39564;&#20013;&#30340;X&#23556;&#32447;&#21453;&#23556;&#27979;&#37327;&#65292;&#24418;&#25104;&#38381;&#29615;&#21453;&#39304;&#31995;&#32479;&#65292;&#24182;&#23454;&#29616;&#20102;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#20998;&#26512;&#30340;&#23454;&#26102;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;X&#23556;&#32447;&#25955;&#23556;&#23454;&#39564;&#24050;&#21463;&#21040;&#37325;&#35270;&#65292;&#36825;&#35777;&#26126;&#20102;&#23427;&#22312;&#30740;&#31350;&#28041;&#21450;&#22823;&#37327;&#25968;&#25454;&#25110;&#24555;&#36895;&#20135;&#29983;&#25968;&#25454;&#38598;&#30340;&#20215;&#20540;&#12290;&#26426;&#22120;&#23398;&#20064;&#20801;&#35768;&#33258;&#21160;&#35299;&#37322;&#23454;&#39564;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20174;&#21516;&#27493;&#36752;&#23556;&#25110;&#20013;&#23376;&#35774;&#26045;&#33719;&#24471;&#30340;&#32467;&#26524;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22788;&#29702;&#25968;&#25454;&#30340;&#36895;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#26426;&#20250;&#65292;&#24314;&#31435;&#38381;&#29615;&#21453;&#39304;&#31995;&#32479;&#65292;&#23454;&#29616;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#20998;&#26512;&#30340;&#23454;&#26102;&#20915;&#31574;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#26426;&#22120;&#23398;&#20064;&#34701;&#20837;X&#23556;&#32447;&#21453;&#23556;&#27979;&#37327;&#65288;XRR&#65289;&#30340;&#38381;&#29615;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#20197;&#26377;&#26426;&#34180;&#33180;&#30340;&#29983;&#38271;&#20026;&#20363;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;&#26426;&#22120;&#23398;&#20064;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#20998;&#26512;&#21644;&#38381;&#29615;&#21453;&#39304;&#22312;&#20809;&#26463;&#32447;&#20013;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#23454;&#39564;&#36807;&#31243;&#20013;&#25552;&#20379;&#22522;&#26412;&#25968;&#25454;&#20998;&#26512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#19981;&#20250;&#22312;&#20809;&#26463;&#32447;&#25511;&#21046;&#20013;&#24341;&#20837;&#20854;&#20182;&#36719;&#20214;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been significant interest in applying machine learning (ML) techniques to X-ray scattering experiments, which proves to be a valuable tool for enhancing research that involves large or rapidly generated datasets. ML allows for the automated interpretation of experimental results, particularly those obtained from synchrotron or neutron facilities. The speed at which ML models can process data presents an important opportunity to establish a closed-loop feedback system, enabling real-time decision-making based on online data analysis. In this study, we describe the incorporation of ML into a closed-loop workflow for X-ray reflectometry (XRR), using the growth of organic thin films as an example. Our focus lies on the beamline integration of ML-based online data analysis and closed-loop feedback. We present solutions that provide an elementary data analysis in real time during the experiment without introducing the additional software dependencies in the beamline contr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;ARDR&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;ARDR&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#21487;&#20197;&#23558;PCA&#23884;&#20837;&#23436;&#20840;&#24674;&#22797;&#65292;&#36890;&#36807;&#31245;&#21152;&#20462;&#25913;&#21487;&#20197;&#29992;LLE&#22797;&#29616;ARDR&#23884;&#20837;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#19968;&#31995;&#21015;&#29468;&#24819;&#65292;&#22914;&#26524;&#25104;&#31435;&#65292;&#21487;&#20197;&#23558;2D&#23884;&#20837;&#20013;&#30340;&#32467;&#26500;&#24402;&#22240;&#20110;&#36755;&#20837;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.11898</link><description>&lt;p&gt;
&#26080;&#27861;&#35299;&#37322;&#30340;&#35299;&#37322;&#65306;&#35299;&#37322;tSNE&#21644;UMAP&#23884;&#20837;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unexplainable Explanations: Towards Interpreting tSNE and UMAP Embeddings. (arXiv:2306.11898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;ARDR&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;ARDR&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#21487;&#20197;&#23558;PCA&#23884;&#20837;&#23436;&#20840;&#24674;&#22797;&#65292;&#36890;&#36807;&#31245;&#21152;&#20462;&#25913;&#21487;&#20197;&#29992;LLE&#22797;&#29616;ARDR&#23884;&#20837;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#19968;&#31995;&#21015;&#29468;&#24819;&#65292;&#22914;&#26524;&#25104;&#31435;&#65292;&#21487;&#20197;&#23558;2D&#23884;&#20837;&#20013;&#30340;&#32467;&#26500;&#24402;&#22240;&#20110;&#36755;&#20837;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#22312;&#31354;&#38388;&#35299;&#37322;&#20026;&#21560;&#24341;/&#25490;&#26021;&#38477;&#32500;&#65288;ARDR&#65289;&#26041;&#27861;&#65288;&#22914;tSNE&#21644;UMAP&#65289;&#24050;&#25104;&#20026;&#26631;&#20934;&#12290;&#36825;&#21462;&#20915;&#20110;2D&#34920;&#31034;&#20013;&#30340;&#32467;&#26500;&#19982;&#27169;&#22411;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#32467;&#26500;&#19968;&#33268;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#26410;&#32463;&#35777;&#26126;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;ARDR&#31639;&#27861;&#26159;&#21542;&#26377;&#20219;&#20309;&#25910;&#25947;&#20445;&#35777;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;ARDR&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#21560;&#24341;&#21644;&#25490;&#26021;&#26469;&#23436;&#20840;&#24674;&#22797;PCA&#23884;&#20837;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#26524;&#31245;&#21152;&#20462;&#25913;&#65292;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65288;LLE&#65289;&#21487;&#20197;&#22797;&#29616;ARDR&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#21270;&#20102;&#19968;&#31995;&#21015;&#29468;&#24819;&#65292;&#22914;&#26524;&#36825;&#20123;&#29468;&#24819;&#25104;&#31435;&#65292;&#23601;&#21487;&#20197;&#23558;2D&#23884;&#20837;&#20013;&#30340;&#32467;&#26500;&#24402;&#22240;&#20110;&#36755;&#20837;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has become standard to explain neural network latent spaces with attraction/repulsion dimensionality reduction (ARDR) methods like tSNE and UMAP. This relies on the premise that structure in the 2D representation is consistent with the structure in the model's latent space. However, this is an unproven assumption -- we are unaware of any convergence guarantees for ARDR algorithms. We work on closing this question by relating ARDR methods to classical dimensionality reduction techniques. Specifically, we show that one can fully recover a PCA embedding by applying attractions and repulsions onto a randomly initialized dataset. We also show that, with a small change, Locally Linear Embeddings (LLE) can reproduce ARDR embeddings. Finally, we formalize a series of conjectures that, if true, would allow one to attribute structure in the 2D embedding back to the input distribution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#23398;&#20064;&#21512;&#36866;&#30340;&#25104;&#26412;&#32467;&#26500;&#26469;&#40723;&#21169;&#26144;&#23556;&#27839;&#30528;&#29305;&#23450;&#30340;&#24037;&#31243;&#29305;&#24449;&#26469;&#20256;&#36865;&#28857;&#65292;&#20854;&#22312;&#25193;&#23637; Monge-Bregman-Occam &#31649;&#36947;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#65292;&#24182;&#22312;&#21305;&#37197;&#30452;&#26041;&#22270;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11895</link><description>&lt;p&gt;
&#23398;&#20064;&#32467;&#26500;&#33945;&#26085;&#20301;&#31227;&#30340;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Learning Costs for Structured Monge Displacements. (arXiv:2306.11895v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#23398;&#20064;&#21512;&#36866;&#30340;&#25104;&#26412;&#32467;&#26500;&#26469;&#40723;&#21169;&#26144;&#23556;&#27839;&#30528;&#29305;&#23450;&#30340;&#24037;&#31243;&#29305;&#24449;&#26469;&#20256;&#36865;&#28857;&#65292;&#20854;&#22312;&#25193;&#23637; Monge-Bregman-Occam &#31649;&#36947;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#65292;&#24182;&#22312;&#21305;&#37197;&#30452;&#26041;&#22270;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#20026;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#22810;&#31181;&#25512;&#26029;&#26679;&#26412;&#38388;&#23494;&#24230;&#21069;&#21521;&#26144;&#23556;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#35813;&#29702;&#35770;&#24050;&#32463;&#35265;&#35777;&#20102;&#35768;&#22810;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#20294;&#20854;&#23454;&#38469;&#23454;&#29616;&#20173;&#28982;&#26497;&#20854;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#21516;&#26102;&#38754;&#20020;&#35745;&#31639;&#21644;&#32479;&#35745;&#19978;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#24456;&#23569;&#26377;&#19981;&#20351;&#29992;&#40664;&#35748;&#36873;&#25321;&#26469;&#20272;&#35745;&#36825;&#20123;&#26144;&#23556;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#31616;&#21333;&#30340;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#20316;&#20026;&#22320;&#38754;&#36153;&#29992;$c(x,y)=\|x-y\|^2_2$&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#37319;&#21462;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20197;\emph{&#23398;&#20064;}&#21512;&#36866;&#30340;&#25104;&#26412;&#32467;&#26500;&#65292;&#40723;&#21169;&#26144;&#23556;&#27839;&#30528;&#29305;&#23450;&#30340;&#24037;&#31243;&#29305;&#24449;&#26469;&#20256;&#36865;&#28857;&#12290;&#25105;&#20204;&#23558;&#26368;&#36817;&#25552;&#20986;&#30340; Monge-Bregman-Occam &#31649;&#36947;~\citep{cuturi2023monge} &#30340;&#33539;&#24335;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#35813;&#33539;&#24335;&#22522;&#20110;&#26367;&#20195;&#30340;&#25104;&#26412;&#20844;&#24335;$c(x,y)=h(x-y)$ &#65292;&#23427;&#20063;&#26159;&#25104;&#26412;&#19981;&#21464;&#30340;&#65292;&#20294;&#37319;&#29992;&#26356;&#19968;&#33324;&#30340;&#24418;&#24335;$h=\tfrac12 \ell_2^2+\tau$&#65292;&#20854;&#20013;$\tau$&#26159;&#36866;&#24403;&#30340;&#20984;&#35268;&#21017;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport theory has provided machine learning with several tools to infer a push-forward map between densities from samples. While this theory has recently seen tremendous methodological developments in machine learning, its practical implementation remains notoriously difficult, because it is plagued by both computational and statistical challenges. Because of such difficulties, existing approaches rarely depart from the default choice of estimating such maps with the simple squared-Euclidean distance as the ground cost, $c(x,y)=\|x-y\|^2_2$. We follow a different path in this work, with the motivation of \emph{learning} a suitable cost structure to encourage maps to transport points along engineered features. We extend the recently proposed Monge-Bregman-Occam pipeline~\citep{cuturi2023monge}, that rests on an alternative cost formulation that is also cost-invariant $c(x,y)=h(x-y)$, but which adopts a more general form as $h=\tfrac12 \ell_2^2+\tau$, where $\tau$ is an approp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#24178;&#39044;&#39118;&#26684;&#36716;&#31227;&#65288;IST&#65289;&#65292;&#36890;&#36807;&#29983;&#25104;&#24178;&#39044;&#35757;&#32451;&#20998;&#24067;&#65292;&#20174;&#32780;&#26174;&#30528;&#25913;&#21892;&#20102;&#21333;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#22806;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11890</link><description>&lt;p&gt;
&#21333;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#65292;&#24178;&#39044;&#39118;&#26684;&#36716;&#31227;&#23454;&#29616;&#22806;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Out of Distribution Generalization via Interventional Style Transfer in Single-Cell Microscopy. (arXiv:2306.11890v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#24178;&#39044;&#39118;&#26684;&#36716;&#31227;&#65288;IST&#65289;&#65292;&#36890;&#36807;&#29983;&#25104;&#24178;&#39044;&#35757;&#32451;&#20998;&#24067;&#65292;&#20174;&#32780;&#26174;&#30528;&#25913;&#21892;&#20102;&#21333;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#22806;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#65292;&#21253;&#25324;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#21457;&#29616;&#36807;&#31243;&#65292;&#38656;&#35201;&#23545;&#19978;&#19979;&#25991;&#24178;&#25200;&#20855;&#26377;&#19981;&#21464;&#24615;&#21644;&#23545;&#26032;&#25968;&#25454;&#20855;&#26377;&#27867;&#21270;&#24615;&#30340;&#22240;&#26524;&#34920;&#24449;&#12290;&#21033;&#29992;&#20004;&#20010;&#26032;&#30340;&#21333;&#32454;&#32990;&#33639;&#20809;&#26174;&#24494;&#38236;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#22797;&#21046;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#33324;&#36866;&#29992;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#22312;&#36234;&#26469;&#36234;&#20855;&#25361;&#25112;&#24615;&#30340;OOD&#27867;&#21270;&#27700;&#24179;&#19978;&#23398;&#20064;&#22240;&#26524;&#34920;&#24449;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#26681;&#25454;&#20854;&#20182;&#24050;&#24314;&#31435;&#30340;&#24230;&#37327;&#34913;&#37327;&#65292;&#26082;&#26377;&#30340;&#22825;&#30495;&#22522;&#32447;&#35774;&#35745;&#29992;&#20110;&#38450;&#27490;&#28151;&#28102;&#65292;&#20294;&#36825;&#20123;&#22522;&#32447;&#22312;&#36825;&#20123;&#27979;&#35797;&#20013;&#37117;&#20250;&#22833;&#25928;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24178;&#39044;&#39118;&#26684;&#36716;&#31227;&#65288;IST&#65289;&#65292;&#36890;&#36807;&#29983;&#25104;&#24178;&#39044;&#35757;&#32451;&#20998;&#24067;&#65292;&#22312;&#20854;&#20013;&#20943;&#36731;&#29983;&#29289;&#21407;&#22240;&#21644;&#24178;&#25200;&#22240;&#32032;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#26174;&#30528;&#25913;&#21892;&#20102;OOD&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world deployment of computer vision systems, including in the discovery processes of biomedical research, requires causal representations that are invariant to contextual nuisances and generalize to new data. Leveraging the internal replicate structure of two novel single-cell fluorescent microscopy datasets, we propose generally applicable tests to assess the extent to which models learn causal representations across increasingly challenging levels of OOD-generalization. We show that despite seemingly strong performance, as assessed by other established metrics, both naive and contemporary baselines designed to ward against confounding, collapse on these tests. We introduce a new method, Interventional Style Transfer (IST), that substantially improves OOD generalization by generating interventional training distributions in which spurious correlations between biological causes and nuisances are mitigated. We publish our code and datasets.
&lt;/p&gt;</description></item><item><title>SPRINT &#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#37325;&#26631;&#35760;&#21450;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#39044;&#35757;&#32451;&#25152;&#38656;&#30340;&#20154;&#21147;&#65292;&#21516;&#26102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.11886</link><description>&lt;p&gt;
SPRINT&#65306;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196; relabeling &#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#31574;&#30053;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling. (arXiv:2306.11886v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11886
&lt;/p&gt;
&lt;p&gt;
SPRINT &#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#37325;&#26631;&#35760;&#21450;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#39044;&#35757;&#32451;&#25152;&#38656;&#30340;&#20154;&#21147;&#65292;&#21516;&#26102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#26426;&#22120;&#20154;&#31574;&#30053;&#24182;&#36171;&#20104;&#20016;&#23500;&#30340;&#25216;&#33021;&#38598;&#21512;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#19979;&#28216;&#20219;&#21153;&#30340;&#23398;&#20064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23450;&#20041;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20294;&#36825;&#38656;&#35201;&#20154;&#20026;&#22320;&#27880;&#37322;&#25968;&#21313;&#19975;&#20010;&#25351;&#20196;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SPRINT&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#22823;&#22823;&#20943;&#23569;&#39044;&#35757;&#32451;&#22810;&#26679;&#30340;&#25216;&#33021;&#25152;&#38656;&#30340;&#20154;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#26680;&#24515;&#24819;&#27861;&#26469;&#33258;&#21160;&#25193;&#23637;&#22522;&#30784;&#39044;&#35757;&#32451;&#20219;&#21153;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#25351;&#20196;&#37325;&#26631;&#35760;&#21644;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20132;&#21449;&#36712;&#36857;&#25216;&#33021;&#38142;&#25509;&#12290;&#22240;&#27492;&#65292;SPRINT &#39044;&#35757;&#32451;&#21487;&#20197;&#20026;&#26426;&#22120;&#20154;&#35013;&#22791;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#12290;&#22312;&#23478;&#24237;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#21416;&#25151;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPRINT &#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training robot policies with a rich set of skills can substantially accelerate the learning of downstream tasks. Prior works have defined pre-training tasks via natural language instructions, but doing so requires tedious human annotation of hundreds of thousands of instructions. Thus, we propose SPRINT, a scalable offline policy pre-training approach which substantially reduces the human effort needed for pre-training a diverse set of skills. Our method uses two core ideas to automatically expand a base set of pre-training tasks: instruction relabeling via large language models and cross-trajectory skill chaining through offline reinforcement learning. As a result, SPRINT pre-training equips robots with a much richer repertoire of skills. Experimental results in a household simulator and on a real robot kitchen manipulation task show that SPRINT leads to substantially faster learning of new long-horizon tasks than previous pre-training approaches. Website at https://clvrai.com/spr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#26469;&#36827;&#34892;&#22870;&#21169;&#22609;&#24418;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#25506;&#32034; - &#21033;&#29992;&#26435;&#34913;&#30340;&#20248;&#38597;&#26694;&#26550;&#12290;&#21516;&#26102;&#65292;&#38416;&#26126;&#20102;&#20449;&#24687;&#29109;&#12289;&#38543;&#26426;&#31995;&#32479;&#21160;&#21147;&#23398;&#20197;&#21450;&#23427;&#20204;&#23545;&#29109;&#20135;&#29983;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.11885</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#22870;&#21169;&#22609;&#24418;
&lt;/p&gt;
&lt;p&gt;
Reward Shaping via Diffusion Process in Reinforcement Learning. (arXiv:2306.11885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#26469;&#36827;&#34892;&#22870;&#21169;&#22609;&#24418;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#25506;&#32034; - &#21033;&#29992;&#26435;&#34913;&#30340;&#20248;&#38597;&#26694;&#26550;&#12290;&#21516;&#26102;&#65292;&#38416;&#26126;&#20102;&#20449;&#24687;&#29109;&#12289;&#38543;&#26426;&#31995;&#32479;&#21160;&#21147;&#23398;&#20197;&#21450;&#23427;&#20204;&#23545;&#29109;&#20135;&#29983;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#19981;&#26029;&#21457;&#23637;&#65292;&#20197;&#22312;&#19981;&#30830;&#23450;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#38543;&#26426;&#28909;&#21147;&#23398;&#21644;&#31995;&#32479;&#21160;&#21147;&#23398;&#21407;&#29702;&#65292;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#25506;&#32034;&#22870;&#21169;&#22609;&#24418;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#20248;&#38597;&#30340;&#26694;&#26550;&#26469;&#24605;&#32771;&#25506;&#32034; - &#21033;&#29992;&#26435;&#34913;&#12290;&#26412;&#25991;&#38416;&#26126;&#20102;&#20449;&#24687;&#29109;&#65292;&#38543;&#26426;&#31995;&#32479;&#21160;&#21147;&#23398;&#21450;&#20854;&#23545;&#29109;&#20135;&#29983;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#25506;&#32034;&#20351;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#21452;&#37325;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#27966;&#29983;&#26377;&#25928;&#31574;&#30053;&#30340;&#26368;&#22823;&#29109;&#31243;&#24207;&#65292;&#25110;&#32773;&#26159;&#35745;&#31639;&#20449;&#24687;&#25104;&#26412;&#21644;&#25910;&#30410;&#30340;&#20462;&#27491;&#25104;&#26412;&#20248;&#21270;&#31243;&#24207;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20449;&#24687;&#30340;&#29289;&#29702;&#26412;&#36136;&#21450;&#20854;&#23545;MDP&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;&#24433;&#21709;&#30340;&#26032;&#35270;&#35282;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;RL&#20013;&#30340;&#20449;&#24687;&#21462;&#21521;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) models have continually evolved to navigate the exploration - exploitation trade-off in uncertain Markov Decision Processes (MDPs). In this study, I leverage the principles of stochastic thermodynamics and system dynamics to explore reward shaping via diffusion processes. This provides an elegant framework as a way to think about exploration-exploitation trade-off. This article sheds light on relationships between information entropy, stochastic system dynamics, and their influences on entropy production. This exploration allows us to construct a dual-pronged framework that can be interpreted as either a maximum entropy program for deriving efficient policies or a modified cost optimization program accounting for informational costs and benefits. This work presents a novel perspective on the physical nature of information and its implications for online learning in MDPs, consequently providing a better understanding of information-oriented formulations in RL
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#29305;&#24449;&#23545;&#40784;&#21644;&#20998;&#31867;&#22120;&#21327;&#20316;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#35821;&#20041;&#30693;&#35782;&#36827;&#34892;&#26174;&#24335;&#30340;&#23616;&#37096;-&#20840;&#23616;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#37327;&#21270;&#20102;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#31867;&#22120;&#32452;&#21512;&#30340;&#25910;&#30410;&#65292;&#20316;&#20026;&#32452;&#21512;&#26435;&#37325;&#30340;&#20989;&#25968;&#65292;&#23545;&#21508;&#31181;&#24322;&#26500;&#25968;&#25454;&#24773;&#24418;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.11867</link><description>&lt;p&gt;
&#24102;&#29305;&#24449;&#23545;&#40784;&#21644;&#20998;&#31867;&#22120;&#21327;&#20316;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated Learning with Feature Alignment and Classifier Collaboration. (arXiv:2306.11867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#29305;&#24449;&#23545;&#40784;&#21644;&#20998;&#31867;&#22120;&#21327;&#20316;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#35821;&#20041;&#30693;&#35782;&#36827;&#34892;&#26174;&#24335;&#30340;&#23616;&#37096;-&#20840;&#23616;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#37327;&#21270;&#20102;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#31867;&#22120;&#32452;&#21512;&#30340;&#25910;&#30410;&#65292;&#20316;&#20026;&#32452;&#21512;&#26435;&#37325;&#30340;&#20989;&#25968;&#65292;&#23545;&#21508;&#31181;&#24322;&#26500;&#25968;&#25454;&#24773;&#24418;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24322;&#26500;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20043;&#19968;&#65292;&#36825;&#20419;&#20351;&#20154;&#20204;&#37319;&#29992;&#21508;&#31181;&#26041;&#27861;&#20026;&#21442;&#19982;&#23458;&#25143;&#31471;&#23398;&#20064;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20219;&#21153;&#20013;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#20849;&#20139;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#23398;&#20064;&#23450;&#21046;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#26412;&#22320;&#34920;&#31034;&#23398;&#20064;&#26399;&#38388;&#26410;&#21033;&#29992;&#20840;&#23616;&#30693;&#35782;&#65292;&#20063;&#24573;&#30053;&#20102;&#26412;&#22320;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21327;&#20316;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20840;&#29699;&#35821;&#20041;&#30693;&#35782;&#36827;&#34892;&#26174;&#24335;&#30340;&#23616;&#37096;-&#20840;&#23616;&#29305;&#24449;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#31867;&#22120;&#32452;&#21512;&#30340;&#25910;&#30410;&#65292;&#20316;&#20026;&#32452;&#21512;&#26435;&#37325;&#30340;&#20989;&#25968;&#65292;&#24182;&#23548;&#20986;&#20272;&#35745;&#26368;&#20248;&#26435;&#37325;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#23545;&#21508;&#31181;&#24322;&#26500;&#25968;&#25454;&#24773;&#24418;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data heterogeneity is one of the most challenging issues in federated learning, which motivates a variety of approaches to learn personalized models for participating clients. One such approach in deep neural networks based tasks is employing a shared feature representation and learning a customized classifier head for each client. However, previous works do not utilize the global knowledge during local representation learning and also neglect the fine-grained collaboration between local classifier heads, which limit the model generalization ability. In this work, we conduct explicit local-global feature alignment by leveraging global semantic knowledge for learning a better representation. Moreover, we quantify the benefit of classifier combination for each client as a function of the combining weights and derive an optimization problem for estimating optimal weights. Finally, extensive evaluation results on benchmark datasets with various heterogeneous data scenarios demonstrate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23637;&#24320;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#20248;&#21270;DNN&#26435;&#37325;&#65292;&#20197;&#23454;&#29616;&#20256;&#36755;&#21151;&#29575;&#25511;&#21046;&#12290;&#22312;&#23494;&#38598;D2D&#36890;&#20449;&#29615;&#22659;&#19979;&#32463;&#36807;&#24615;&#33021;&#35780;&#20272;&#65292;&#23558;&#36845;&#20195;&#31639;&#27861;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;(&gt;2&#20493;)&#22823;&#22823;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2306.11865</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#28145;&#24230;&#23637;&#24320;PGD&#22312;&#26080;&#32447;&#31995;&#32479;&#20256;&#36755;&#21151;&#29575;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Deep Unfolded PGD for Transmit Power Allocation in Wireless Systems. (arXiv:2306.11865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23637;&#24320;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#20248;&#21270;DNN&#26435;&#37325;&#65292;&#20197;&#23454;&#29616;&#20256;&#36755;&#21151;&#29575;&#25511;&#21046;&#12290;&#22312;&#23494;&#38598;D2D&#36890;&#20449;&#29615;&#22659;&#19979;&#32463;&#36807;&#24615;&#33021;&#35780;&#20272;&#65292;&#23558;&#36845;&#20195;&#31639;&#27861;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;(&gt;2&#20493;)&#22823;&#22823;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#36755;&#21151;&#29575;&#25511;&#21046;&#65288;TPC&#65289;&#26159;&#31649;&#29702;&#26080;&#32447;&#31995;&#32479;&#20013;&#24178;&#25200;&#12289;&#33021;&#37327;&#21033;&#29992;&#21644;&#36830;&#25509;&#24615;&#30340;&#20851;&#38190;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#36845;&#20195;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;PGD&#65289;&#31639;&#27861;&#23637;&#24320;&#20026;&#32593;&#32476;&#23618;&#24182;&#23398;&#20064;&#27493;&#38271;&#21442;&#25968;&#30340;&#31616;&#21333;&#20302;&#22797;&#26434;&#24230;TPC&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#25110;&#31163;&#32447;&#39044;&#35757;&#32451;&#26469;&#20248;&#21270;DNN&#30340;&#26435;&#37325;&#12290;&#22312;&#23494;&#38598;&#35774;&#22791;&#23545;&#35774;&#22791;&#65288;D2D&#65289;&#36890;&#20449;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;&#36845;&#20195;&#31639;&#27861;&#26356;&#24555;&#22320;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#21487;&#20197;&#20943;&#23569;&#36229;&#36807;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transmit power control (TPC) is a key mechanism for managing interference, energy utilization, and connectivity in wireless systems. In this paper, we propose a simple low-complexity TPC algorithm based on the deep unfolding of the iterative projected gradient descent (PGD) algorithm into layers of a deep neural network and learning the step-size parameters. An unsupervised learning method with either online learning or offline pretraining is applied for optimizing the weights of the DNN. Performance evaluation in dense device-to-device (D2D) communication scenarios showed that the proposed method can achieve better performance than the iterative algorithm with more than a factor of 2 lower number of iterations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#35780;&#20272;&#21709;&#24212;&#20540;&#19978;&#20004;&#20010;&#29305;&#24449;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.11855</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#27169;&#22411;&#26816;&#39564;&#30417;&#30563;&#23398;&#20064;&#20013;&#29305;&#24449;&#24433;&#21709;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Model-free Closeness-of-influence Test for Features in Supervised Learning. (arXiv:2306.11855v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#35780;&#20272;&#21709;&#24212;&#20540;&#19978;&#20004;&#20010;&#29305;&#24449;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#29305;&#24449;&#21521;&#37327;$x \in \mathbb{R}^d$&#23545;&#21709;&#24212;&#20540;&#65288;&#26631;&#31614;&#65289;$y \in \mathbb{R}$&#30340;&#24433;&#21709;&#26159;&#35768;&#22810;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#30340;&#22522;&#30707;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#26469;&#30740;&#31350;&#22914;&#20309;&#35780;&#20272;&#20004;&#20010;&#32473;&#23450;&#29305;&#24449;&#22312;&#21709;&#24212;&#20540;&#19978;&#20855;&#26377;&#19981;&#21516;&#24433;&#21709;&#21147;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#25552;&#20986;&#20102;&#29305;&#24449;&#24433;&#21709;&#21147;&#30340;&#32039;&#23494;&#24230;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#24674;&#22797;&#21442;&#25968;&#27169;&#22411;&#20013;&#31995;&#25968;&#30340;&#24133;&#24230;&#30340;&#29087;&#24713;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#27979;&#35797;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#24433;&#21709;&#32039;&#23494;&#24230;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#21487;&#20197;&#25511;&#21046;&#20551;&#38451;&#24615;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the effect of a feature vector $x \in \mathbb{R}^d$ on the response value (label) $y \in \mathbb{R}$ is the cornerstone of many statistical learning problems. Ideally, it is desired to understand how a set of collected features combine together and influence the response value, but this problem is notoriously difficult, due to the high-dimensionality of data and limited number of labeled data points, among many others. In this work, we take a new perspective on this problem, and we study the question of assessing the difference of influence that the two given features have on the response value. We first propose a notion of closeness for the influence of features, and show that our definition recovers the familiar notion of the magnitude of coefficients in the parametric model. We then propose a novel method to test for the closeness of influence in general model-free supervised learning problems. Our proposed test can be used with finite number of samples with control on
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#20803;&#25968;&#25454;&#29305;&#24449;&#30340;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#23545;&#32435;&#31859;&#39063;&#31890;&#20998;&#21106;&#31561;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#23545;&#26174;&#24494;&#38236;&#21442;&#25968;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#21487;&#20197;&#22312;&#26576;&#20123;&#26679;&#26412;&#21442;&#25968;&#19978;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.11853</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#39640;&#20998;&#36776;&#29575;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#25968;&#25454;&#26102;&#23454;&#39564;&#21442;&#25968;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization Across Experimental Parameters in Machine Learning Analysis of High Resolution Transmission Electron Microscopy Datasets. (arXiv:2306.11853v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11853
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#20803;&#25968;&#25454;&#29305;&#24449;&#30340;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#23545;&#32435;&#31859;&#39063;&#31890;&#20998;&#21106;&#31561;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#23545;&#26174;&#24494;&#38236;&#21442;&#25968;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#21487;&#20197;&#22312;&#26576;&#20123;&#26679;&#26412;&#21442;&#25968;&#19978;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#39640;&#36890;&#37327;&#21644;&#20934;&#30830;&#30340;&#32435;&#31859;&#26448;&#26009;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;TEM&#65289;&#20998;&#26512;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#65292;&#20294;&#24050;&#30693;&#23427;&#20204;&#22312;&#26469;&#33258;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#30340;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#37492;&#20110;&#22312;&#39640;&#20998;&#36776;&#29575;TEM&#25104;&#20687;&#20013;&#36890;&#24120;&#30475;&#21040;&#30340;&#22270;&#20687;&#29305;&#24449;&#38598;&#26377;&#38480;&#65292;&#23578;&#19981;&#28165;&#26970;&#21738;&#20123;&#22270;&#20687;&#34987;&#35748;&#20026;&#26159;&#26469;&#33258;&#20854;&#20182;&#22270;&#20687;&#30340;&#22806;&#37096;&#20998;&#24067;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#20803;&#25968;&#25454;&#29305;&#24449;&#30340;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#23545;&#32435;&#31859;&#39063;&#31890;&#20998;&#21106;&#31561;&#31034;&#20363;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#37325;&#28857;&#20851;&#27880;&#31574;&#21010;&#12289;&#23454;&#26045;&#39640;&#20998;&#36776;&#29575;TEM&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#39564;&#21442;&#25968;&#65292;&#21253;&#25324;&#25918;&#22823;&#29575;&#65292;&#21058;&#37327;&#65292;&#32435;&#31859;&#39063;&#31890;&#30452;&#24452;&#21644;&#32435;&#31859;&#39063;&#31890;&#26448;&#26009;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#26174;&#24494;&#38236;&#21442;&#25968;&#19978;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#21487;&#20197;&#22312;&#26576;&#20123;&#26679;&#26412;&#21442;&#25968;&#19978;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#32593;&#32476;&#32467;&#26500;&#35774;&#35745;&#31561;&#22240;&#32032;&#36824;&#23545;&#27867;&#21270;&#24615;&#33021;&#20135;&#29983;&#20102;&#26174;&#30528;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are promising tools for high-throughput and accurate transmission electron microscopy (TEM) analysis of nanomaterials, but are known to generalize poorly on data that is "out-of-distribution" from their training data. Given the limited set of image features typically seen in high-resolution TEM imaging, it is unclear which images are considered out-of-distribution from others. Here, we investigate how the choice of metadata features in the training dataset influences neural network performance, focusing on the example task of nanoparticle segmentation. We train and validate neural networks across curated, experimentally-collected high-resolution TEM image datasets of nanoparticles under controlled imaging and material parameters, including magnification, dosage, nanoparticle diameter, and nanoparticle material. Overall, we find that our neural networks are not robust across microscope parameters, but do generalize across certain sample parameters. Additionally, data pre
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#65292;&#22312;&#32454;&#32990;&#23398;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#36136;&#37327;&#19978;&#21462;&#24471;&#26174;&#30528;&#36827;&#23637;&#65292;&#20026;&#32454;&#32990;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#21457;&#23637;&#25552;&#20986;&#20102;&#24040;&#22823;&#30340;&#21457;&#23637;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.11848</link><description>&lt;p&gt;
&#22312;&#20861;&#21307;&#32454;&#32990;&#23398;&#20013;&#20351;&#29992;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#25552;&#39640;&#35270;&#35273;&#24863;&#30693;&#21644;&#20998;&#21106;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Using super-resolution for enhancing visual perception and segmentation performance in veterinary cytology. (arXiv:2306.11848v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11848
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#65292;&#22312;&#32454;&#32990;&#23398;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#36136;&#37327;&#19978;&#21462;&#24471;&#26174;&#30528;&#36827;&#23637;&#65292;&#20026;&#32454;&#32990;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#21457;&#23637;&#25552;&#20986;&#20102;&#24040;&#22823;&#30340;&#21457;&#23637;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#25972;&#21512;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#26550;&#26500;&#26469;&#25552;&#39640;&#32454;&#32990;&#23398;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#36136;&#37327;&#12290;&#21478;&#19968;&#20010;&#36129;&#29486;&#26159;&#24320;&#21457;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#19981;&#20934;&#30830;&#30340;&#32858;&#28966;&#24773;&#20917;&#19979;&#30340;&#25104;&#20687;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;SR&#25216;&#26415;&#25972;&#21512;&#21040;&#20998;&#21106;&#27969;&#27700;&#32447;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#24179;&#22343;&#31934;&#24230;&#65288;mAP&#65289;&#20998;&#21106;&#25351;&#26631;&#25552;&#39640;&#39640;&#36798;25&#65285;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#21033;&#29992;SR&#26550;&#26500;&#22312;&#32454;&#32990;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#26377;&#30528;&#24040;&#22823;&#30340;&#21457;&#23637;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary objective of this research was to enhance the quality of semantic segmentation in cytology images by incorporating super-resolution (SR) architectures. An additional contribution was the development of a novel dataset aimed at improving imaging quality in the presence of inaccurate focus. Our experimental results demonstrate that the integration of SR techniques into the segmentation pipeline can lead to a significant improvement of up to 25% in the mean average precision (mAP) segmentation metric. These findings suggest that leveraging SR architectures holds great promise for advancing the state of the art in cytology image analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;XGBoost&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25581;&#31034;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#24314;&#31569;&#29615;&#22659;&#29305;&#24449;&#21644;&#29615;&#22659;&#21361;&#23475;&#26333;&#38706;&#29305;&#24449;&#26159;&#24433;&#21709;&#22478;&#24066;&#30284;&#30151;&#24739;&#30149;&#29575;&#30340;&#20851;&#38190;&#65292;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#23454;&#39564;&#25552;&#20986;&#20102;&#22686;&#21152;&#32511;&#21270;&#31354;&#38388;&#12289;&#20943;&#23569;&#24320;&#21457;&#21306;&#21644;&#24635;&#25490;&#25918;&#37327;&#21487;&#20197;&#32531;&#35299;&#30284;&#30151;&#30340;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2306.11847</link><description>&lt;p&gt;
&#22478;&#24066;&#20581;&#24247;&#32852;&#31995;&#30340;&#35299;&#30721;&#65306;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25581;&#31034;&#20102;&#22522;&#20110;&#38169;&#32508;&#22797;&#26434;&#22478;&#24066;&#29305;&#24449;&#30340;&#30284;&#30151;&#24739;&#30149;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding Urban-health Nexus: Interpretable Machine Learning Illuminates Cancer Prevalence based on Intertwined City Features. (arXiv:2306.11847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;XGBoost&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25581;&#31034;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#24314;&#31569;&#29615;&#22659;&#29305;&#24449;&#21644;&#29615;&#22659;&#21361;&#23475;&#26333;&#38706;&#29305;&#24449;&#26159;&#24433;&#21709;&#22478;&#24066;&#30284;&#30151;&#24739;&#30149;&#29575;&#30340;&#20851;&#38190;&#65292;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#23454;&#39564;&#25552;&#20986;&#20102;&#22686;&#21152;&#32511;&#21270;&#31354;&#38388;&#12289;&#20943;&#23569;&#24320;&#21457;&#21306;&#21644;&#24635;&#25490;&#25918;&#37327;&#21487;&#20197;&#32531;&#35299;&#30284;&#30151;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#24314;&#31569;&#29615;&#22659;&#29305;&#24449;&#21644;&#29615;&#22659;&#21361;&#23475;&#26333;&#38706;&#29305;&#24449;&#22312;&#20915;&#23450;&#31038;&#21306;&#32423;&#30284;&#30151;&#24739;&#30149;&#29575;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#21033;&#29992;&#32654;&#22269;&#20116;&#20010;&#22823;&#37117;&#24066;&#32479;&#35745;&#21306;&#30340;&#25968;&#25454;&#65306;&#33437;&#21152;&#21733;&#12289;&#36798;&#25289;&#26031;&#12289;&#20241;&#26031;&#25958;&#12289;&#27931;&#26441;&#30710;&#21644;&#32445;&#32422;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;XGBoost&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20102;&#30284;&#30151;&#24739;&#30149;&#29575;&#30340;&#31243;&#24230;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#21487;&#38752;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#24180;&#40836;&#12289;&#23569;&#25968;&#27665;&#26063;&#36523;&#20221;&#21644;&#20154;&#21475;&#23494;&#24230;&#26159;&#24433;&#21709;&#30284;&#30151;&#24739;&#30149;&#29575;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22478;&#24066;&#21457;&#23637;&#21644;&#35774;&#35745;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#30284;&#30151;&#24739;&#30149;&#29575;&#65292;&#37325;&#28857;&#20851;&#27880;&#32511;&#22320;&#12289;&#24320;&#21457;&#21306;&#21644;&#24635;&#25490;&#25918;&#37327;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22686;&#21152;&#32511;&#21270;&#31354;&#38388;&#65292;&#20943;&#23569;&#24320;&#21457;&#21306;&#21644;&#24635;&#25490;&#25918;&#37327;&#21487;&#20197;&#32531;&#35299;&#30284;&#30151;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the interplay among social demographics, built environment characteristics, and environmental hazard exposure features in determining community level cancer prevalence. Utilizing data from five Metropolitan Statistical Areas in the United States: Chicago, Dallas, Houston, Los Angeles, and New York, the study implemented an XGBoost machine learning model to predict the extent of cancer prevalence and evaluate the importance of different features. Our model demonstrates reliable performance, with results indicating that age, minority status, and population density are among the most influential factors in cancer prevalence. We further explore urban development and design strategies that could mitigate cancer prevalence, focusing on green space, developed areas, and total emissions. Through a series of experimental evaluations based on causal inference, the results show that increasing green space and reducing developed areas and total emissions could alleviate can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23545;&#25042;&#24816;&#26234;&#33021;&#20307;&#36827;&#34892;&#24809;&#32602;&#20197;&#24110;&#21161;&#22242;&#38431;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#20272;&#35745;&#25913;&#21892;&#23545;&#26234;&#33021;&#20307;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;Amortized Causal Discovery&#33258;&#21160;&#26816;&#27979;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.11846</link><description>&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#19979;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20197;&#23454;&#29616;&#39640;&#25928;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Discovering Causality for Efficient Cooperation in Multi-Agent Environments. (arXiv:2306.11846v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23545;&#25042;&#24816;&#26234;&#33021;&#20307;&#36827;&#34892;&#24809;&#32602;&#20197;&#24110;&#21161;&#22242;&#38431;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#20272;&#35745;&#25913;&#21892;&#23545;&#26234;&#33021;&#20307;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;Amortized Causal Discovery&#33258;&#21160;&#26816;&#27979;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#38656;&#35201;&#20316;&#20026;&#19968;&#20010;&#22242;&#38431;&#23398;&#20064;&#34892;&#20026;&#20197;&#23454;&#29616;&#20849;&#21516;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#23398;&#20064;&#20219;&#21153;&#30340;&#36807;&#31243;&#20013;&#65292;&#19968;&#20123;&#26234;&#33021;&#20307;&#21487;&#33021;&#20250;&#23398;&#20064;&#21040;&#27425;&#20248;&#31574;&#30053;&#65292;&#20174;&#32780;&#26410;&#33021;&#23545;&#22242;&#38431;&#30446;&#26631;&#20570;&#20986;&#36129;&#29486;&#12290;&#36825;&#20123;&#26234;&#33021;&#20307;&#34987;&#31216;&#20026;&#8220;&#25042;&#24816;&#26234;&#33021;&#20307;&#8221;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#38750;&#21512;&#20316;&#24615;&#34892;&#20026;&#21487;&#33021;&#26469;&#33258;&#20110;&#26410;&#33021;&#29702;&#35299;&#26159;&#21542;&#23548;&#33268;&#22238;&#25253;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#21512;&#20316;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22914;&#20309;&#24212;&#29992;&#23427;&#26469;&#24809;&#32602;&#36825;&#20123;&#25042;&#24816;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22240;&#26524;&#20851;&#31995;&#20272;&#35745;&#21487;&#29992;&#20110;&#25913;&#36827;&#23545;&#26234;&#33021;&#20307;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#23427;&#26469;&#25913;&#36827;&#22810;&#26234;&#33021;&#20307;&#29420;&#31435;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20998;&#25674;&#22240;&#26524;&#21457;&#29616;&#26469;&#33258;&#21160;&#26816;&#27979;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In cooperative Multi-Agent Reinforcement Learning (MARL) agents are required to learn behaviours as a team to achieve a common goal. However, while learning a task, some agents may end up learning sub-optimal policies, not contributing to the objective of the team. Such agents are called lazy agents due to their non-cooperative behaviours that may arise from failing to understand whether they caused the rewards. As a consequence, we observe that the emergence of cooperative behaviours is not necessarily a byproduct of being able to solve a task as a team. In this paper, we investigate the applications of causality in MARL and how it can be applied in MARL to penalise these lazy agents. We observe that causality estimations can be used to improve the credit assignment to the agents and show how it can be leveraged to improve independent learning in MARL. Furthermore, we investigate how Amortized Causal Discovery can be used to automate causality detection within MARL environments. The r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20110;&#24322;&#36136;&#31181;&#32676;&#26377;&#23475;&#23454;&#39564;&#30340;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;CLASH&#65292;&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#25552;&#21069;&#20572;&#27490;&#20020;&#24202;&#35797;&#39564;&#21644;A/B&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2306.11839</link><description>&lt;p&gt;
&#26159;&#21542;&#24212;&#35813;&#20572;&#27490;&#65306;&#20855;&#26377;&#24322;&#36136;&#31181;&#32676;&#30340;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations. (arXiv:2306.11839v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20110;&#24322;&#36136;&#31181;&#32676;&#26377;&#23475;&#23454;&#39564;&#30340;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;CLASH&#65292;&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#25552;&#21069;&#20572;&#27490;&#20020;&#24202;&#35797;&#39564;&#21644;A/B&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23454;&#39564;&#30001;&#20110;&#27835;&#30103;&#36896;&#25104;&#24847;&#22806;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#22240;&#27492;&#24448;&#24448;&#38656;&#35201;&#25552;&#21069;&#20572;&#27490;&#12290;&#30446;&#21069;&#30830;&#23450;&#20309;&#26102;&#25552;&#21069;&#32456;&#27490;&#23454;&#39564;&#30340;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36866;&#29992;&#20110;&#24635;&#20307;&#25968;&#25454;&#65292;&#19981;&#32771;&#34385;&#27835;&#30103;&#25928;&#24212;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#24322;&#36136;&#31181;&#32676;&#26377;&#23475;&#23454;&#39564;&#30340;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#29616;&#26377;&#26041;&#27861;&#22312;&#27835;&#30103;&#23545;&#23569;&#25968;&#21442;&#19982;&#32773;&#36896;&#25104;&#20260;&#23475;&#26102;&#24448;&#24448;&#26080;&#27861;&#20572;&#27490;&#23454;&#39564;&#12290;&#28982;&#21518;&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20102;CLASH&#65292;&#36825;&#26159;&#39318;&#20010;&#24191;&#27867;&#36866;&#29992;&#20110;&#24322;&#36136;&#26089;&#26399;&#20572;&#27490;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;CLASH&#30340;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#20020;&#24202;&#35797;&#39564;&#21644;A/B&#27979;&#35797;&#20013;&#37117;&#33021;&#26377;&#25928;&#25552;&#21069;&#20572;&#27490;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized experiments often need to be stopped prematurely due to the treatment having an unintended harmful effect. Existing methods that determine when to stop an experiment early are typically applied to the data in aggregate and do not account for treatment effect heterogeneity. In this paper, we study the early stopping of experiments for harm on heterogeneous populations. We first establish that current methods often fail to stop experiments when the treatment harms a minority group of participants. We then use causal machine learning to develop CLASH, the first broadly-applicable method for heterogeneous early stopping. We demonstrate CLASH's performance on simulated and real data and show that it yields effective early stopping for both clinical trials and A/B tests.
&lt;/p&gt;</description></item><item><title>&#25299;&#25169;&#35270;&#24046;&#26159;&#19968;&#31181;&#27604;&#36739;&#35757;&#32451;&#27169;&#22411;&#21644;&#21442;&#32771;&#25968;&#25454;&#38598;&#22810;&#23610;&#24230;&#20960;&#20309;&#32467;&#26500;&#30456;&#20284;&#24615;&#30340;&#29702;&#35770;&#21644;&#35745;&#31639;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#20272;&#35745;&#27169;&#22411;&#20013;&#30340;&#25299;&#25169;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11835</link><description>&lt;p&gt;
&#25299;&#25169;&#35270;&#24046;&#65306;&#28145;&#24230;&#24863;&#30693;&#27169;&#22411;&#30340;&#20960;&#20309;&#35268;&#33539;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
Topological Parallax: A Geometric Specification for Deep Perception Models. (arXiv:2306.11835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11835
&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#35270;&#24046;&#26159;&#19968;&#31181;&#27604;&#36739;&#35757;&#32451;&#27169;&#22411;&#21644;&#21442;&#32771;&#25968;&#25454;&#38598;&#22810;&#23610;&#24230;&#20960;&#20309;&#32467;&#26500;&#30456;&#20284;&#24615;&#30340;&#29702;&#35770;&#21644;&#35745;&#31639;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#20272;&#35745;&#27169;&#22411;&#20013;&#30340;&#25299;&#25169;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#35777;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#25299;&#25169;&#35270;&#24046;&#20316;&#20026;&#27604;&#36739;&#24050;&#35757;&#32451;&#27169;&#22411;&#21644;&#21442;&#32771;&#25968;&#25454;&#38598;&#30340;&#22810;&#23610;&#24230;&#20960;&#20309;&#32467;&#26500;&#30456;&#20284;&#24615;&#30340;&#29702;&#35770;&#21644;&#35745;&#31639;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#21644;&#20363;&#23376;&#34920;&#26126;&#65292;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#36825;&#31181;&#20960;&#20309;&#30456;&#20284;&#24615;&#23545;&#20110;&#21487;&#20449;&#30340;&#25554;&#20540;&#21644;&#25200;&#21160;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#25105;&#20204;&#29468;&#27979;&#65292;&#36825;&#20010;&#26032;&#27010;&#24565;&#23558;&#20026;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#36807;&#25311;&#21512;&#21644;&#27867;&#21270;&#20043;&#38388;&#19981;&#26126;&#30830;&#30340;&#20851;&#31995;&#30340;&#24403;&#21069;&#35752;&#35770;&#22686;&#28155;&#20215;&#20540;&#12290;&#22312;&#20856;&#22411;&#30340;DNN&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#30340;&#26174;&#24335;&#20960;&#20309;&#25551;&#36848;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#35270;&#24046;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#20351;&#29992;&#21442;&#32771;&#25968;&#25454;&#38598;&#30340;&#27979;&#22320;&#30072;&#21464;&#23545;Rips&#22797;&#21512;&#20307;&#30340;&#24433;&#21709;&#26469;&#20272;&#35745;&#27169;&#22411;&#20013;&#30340;&#25299;&#25169;&#29305;&#24449;&#65288;&#32452;&#20214;&#12289;&#21608;&#26399;&#12289;&#31354;&#27934;&#31561;&#65289;&#12290;&#22240;&#27492;&#65292;&#35270;&#24046;&#25351;&#31034;&#27169;&#22411;&#19982;&#25968;&#25454;&#38598;&#26159;&#21542;&#20849;&#20139;&#31867;&#20284;&#30340;&#22810;&#23610;&#24230;&#20960;&#20309;&#29305;&#24449;&#12290;&#35270;&#24046;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#20174;&#19981;&#21516;&#35282;&#24230;&#35266;&#23519;&#25968;&#25454;&#30340;&#30452;&#35266;&#27010;&#24565;&#65292;&#24182;&#20026;&#29702;&#35299;&#28145;&#24230;&#24863;&#30693;&#27169;&#22411;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
For safety and robustness of AI systems, we introduce topological parallax as a theoretical and computational tool that compares a trained model to a reference dataset to determine whether they have similar multiscale geometric structure. Our proofs and examples show that this geometric similarity between dataset and model is essential to trustworthy interpolation and perturbation, and we conjecture that this new concept will add value to the current debate regarding the unclear relationship between overfitting and generalization in applications of deep-learning. In typical DNN applications, an explicit geometric description of the model is impossible, but parallax can estimate topological features (components, cycles, voids, etc.) in the model by examining the effect on the Rips complex of geodesic distortions using the reference dataset. Thus, parallax indicates whether the model shares similar multiscale geometric features with the dataset. Parallax presents theoretically via topolo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26356;&#25913;&#23454;&#39564;&#33539;&#20363;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#26816;&#27979;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301; (ERPs) &#20013;&#34987;&#20851;&#27880;&#30340;&#23383;&#27597;&#65292;&#24182;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.11830</link><description>&lt;p&gt;
UMM: &#26080;&#30417;&#30563;&#30340;&#22343;&#24046;&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UMM: Unsupervised Mean-difference Maximization. (arXiv:2306.11830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11830
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26356;&#25913;&#23454;&#39564;&#33539;&#20363;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#26816;&#27979;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301; (ERPs) &#20013;&#34987;&#20851;&#27880;&#30340;&#23383;&#27597;&#65292;&#24182;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33041;&#26426;&#25509;&#21475;&#21033;&#29992;&#23545;&#35270;&#35273;&#12289;&#21548;&#35273;&#25110;&#35302;&#35273;&#21050;&#28608;&#20135;&#29983;&#30340;&#22823;&#33041;&#20449;&#21495;&#36827;&#34892;&#25805;&#20316;&#65292;&#36825;&#20123;&#20449;&#21495;&#34987;&#31216;&#20026;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301; (ERPs)&#12290;&#22312;&#35270;&#35273; ERP &#25340;&#20889;&#22120;&#24212;&#29992;&#20013;&#65292;&#22312;&#23631;&#24149;&#19978;&#38543;&#26426;&#38378;&#28865;&#19968;&#32452;&#23383;&#27597;&#65292;&#21442;&#19982;&#32773;&#20851;&#27880;&#20182;&#20204;&#24819;&#35201;&#25340;&#20889;&#30340;&#30446;&#26631;&#23383;&#27597;&#12290;&#24403;&#36825;&#20010;&#23383;&#27597;&#38378;&#28865;&#26102;&#65292;&#20135;&#29983;&#30340; ERP &#19982;&#20219;&#20309;&#20854;&#20182;&#38750;&#30446;&#26631;&#23383;&#27597;&#38378;&#28865;&#26102;&#20135;&#29983;&#30340; ERP &#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#20010;&#34987;&#20851;&#27880;&#30340;&#23383;&#27597;&#12290;&#22312;&#27599;&#27425;&#35797;&#39564;&#20013;&#65292;&#23545;&#20110;&#27599;&#20010;&#21487;&#29992;&#23383;&#27597;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37117;&#20551;&#35774;&#36825;&#20010;&#23383;&#27597;&#23454;&#38469;&#19978;&#26159;&#20851;&#27880;&#30340;&#23383;&#27597;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#20551;&#35774;&#35745;&#31639; ERPs&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#21482;&#26377;&#30495;&#23454;&#20551;&#35774;&#25165;&#20250;&#22312;&#31867;&#22343;&#20540;&#20043;&#38388;&#20135;&#29983;&#26368;&#22823;&#24046;&#24322;&#30340;&#20107;&#23454;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#22522;&#30784;&#23454;&#39564;&#33539;&#20363;&#36827;&#34892;&#20219;&#20309;&#26356;&#25913;&#65292;&#22240;&#27492;&#20960;&#20046;&#21487;&#20197;&#24212;&#29992;&#20110;&#25152;&#26377;&#22522;&#20110; ERP &#30340;&#35774;&#32622;&#12290;&#20026;&#20102;&#22788;&#29702;&#26377;&#38480;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#32763;&#35793;&#27169;&#22411;&#26469;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many brain-computer interfaces make use of brain signals that are elicited in response to a visual, auditory or tactile stimulus, so-called event-related potentials (ERPs). In visual ERP speller applications, sets of letters shown on a screen are flashed randomly, and the participant attends to the target letter they want to spell. When this letter flashes, the resulting ERP is different compared to when any other non-target letter flashes. We propose a new unsupervised approach to detect this attended letter. In each trial, for every available letter our approach makes the hypothesis that it is in fact the attended letter, and calculates the ERPs based on each of these hypotheses. We leverage the fact that only the true hypothesis produces the largest difference between the class means. Note that this unsupervised method does not require any changes to the underlying experimental paradigm and therefore can be employed in almost any ERP-based setup. To deal with limited data, we use a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20219;&#20309;&#28145;&#24230;&#30340;ReLU&#32593;&#32476;&#37117;&#21487;&#20197;&#34987;&#37325;&#20889;&#20026;&#19968;&#20010;&#20855;&#26377;&#36879;&#26126;&#24615;&#30340;&#27973;&#23618;&#32593;&#32476;&#12290;&#36825;&#19968;&#32467;&#35770;&#26377;&#21161;&#20110;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.11827</link><description>&lt;p&gt;
&#20219;&#20309;&#28145;&#24230;ReLU&#32593;&#32476;&#37117;&#26159;&#27973;&#23618;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Any Deep ReLU Network is Shallow. (arXiv:2306.11827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11827
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20219;&#20309;&#28145;&#24230;&#30340;ReLU&#32593;&#32476;&#37117;&#21487;&#20197;&#34987;&#37325;&#20889;&#20026;&#19968;&#20010;&#20855;&#26377;&#36879;&#26126;&#24615;&#30340;&#27973;&#23618;&#32593;&#32476;&#12290;&#36825;&#19968;&#32467;&#35770;&#26377;&#21161;&#20110;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#36896;&#24615;&#22320;&#35777;&#26126;&#20102;&#27599;&#20010;&#28145;&#24230;&#30340;ReLU&#32593;&#32476;&#21487;&#20197;&#34987;&#37325;&#20889;&#20026;&#19968;&#20010;&#20989;&#25968;&#19978;&#31561;&#20215;&#30340;&#19977;&#23618;&#32593;&#32476;&#65292;&#20854;&#20013;&#26435;&#37325;&#20540;&#20026;&#24310;&#36831;&#23454;&#25968;&#12290;&#22522;&#20110;&#27492;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#21487;&#20197;&#32473;&#20986;&#19968;&#20010;&#28145;&#24230;ReLU&#32593;&#32476;&#23545;&#24212;&#30340;&#26174;&#24335;&#26435;&#37325;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#27973;&#23618;&#32593;&#32476;&#26159;&#36879;&#26126;&#30340;&#65292;&#24182;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We constructively prove that every deep ReLU network can be rewritten as a functionally identical three-layer network with weights valued in the extended reals. Based on this proof, we provide an algorithm that, given a deep ReLU network, finds the explicit weights of the corresponding shallow network. The resulting shallow network is transparent and used to generate explanations of the model s behaviour.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; RLGF&#65292;&#29992;&#20110;&#22312; GPT-3 &#31561;&#21160;&#24577;&#40657;&#21283;&#23376;&#25351;&#23548;&#19979;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; LLM &#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#36890;&#29992; RL &#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312; IMDB &#21644; CommonGen &#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.11816</link><description>&lt;p&gt;
&#23398;&#20250;&#29983;&#25104;&#27604;&#20320;&#30340;LMM&#26356;&#22909;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Better Than Your LLM. (arXiv:2306.11816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; RLGF&#65292;&#29992;&#20110;&#22312; GPT-3 &#31561;&#21160;&#24577;&#40657;&#21283;&#23376;&#25351;&#23548;&#19979;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; LLM &#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#36890;&#29992; RL &#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312; IMDB &#21644; CommonGen &#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;(RL)&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#20363;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#12290;&#29305;&#21035;&#22320;&#65292;&#26368;&#36817;&#30340;LLM&#65292;&#22914;ChatGPT&#21644;GPT - 4&#33021;&#22815;&#19982;&#29992;&#25143;&#36827;&#34892;&#27969;&#30021;&#30340;&#23545;&#35805;&#65292;&#24182;&#34701;&#21512;&#20102;RL&#21644;&#20154;&#31867;&#21453;&#39304;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#23398;&#20064;&#25628;&#32034;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#29983;&#25104;&#30340;&#20851;&#38190;&#29305;&#24615;&#65292;&#25506;&#32034;&#20102;&#36229;&#20986;&#36890;&#29992;RL&#31639;&#27861;&#22914;PPO&#20043;&#22806;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;RL&#31639;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#21160;&#24577;&#40657;&#21283;&#23376;&#30340;&#25351;&#23548;LLM&#22914;GPT-3&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#24341;&#23548;&#21453;&#39304;&#30340;RL(RLGF)&#65292;&#36825;&#26159;&#19968;&#22871;&#29992;&#20110;LLM&#24494;&#35843;&#30340;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;GRUE&#22522;&#20934;&#27979;&#35797;&#30340;IMDB&#27491;&#21521;&#35780;&#35770;&#21644;CommonGen&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;RL&#31639;&#27861;&#27604;&#30417;&#30563;&#23398;&#20064;(SL)&#21644;&#40664;&#35748;PPO&#22522;&#32447;&#34920;&#29616;&#26356;&#39640;&#65292;&#35777;&#26126;&#20102;&#19982;&#25351;&#23548;LLM&#20114;&#21160;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning Large Language Models (LLMs) for conditional text generation. In particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with users by incorporating RL and feedback from humans. Inspired by learning-to-search algorithms and capitalizing on key properties of text generation, we seek to investigate reinforcement learning algorithms beyond general purpose algorithms such as Proximal policy optimization (PPO). In particular, we extend RL algorithms to allow them to interact with a dynamic black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive review and CommonGen text generation task from the GRUE benchmark. We show that our RL algorithms achieve higher performance than supervised learning (SL) and default PPO baselines, demonstrating the benefit of interaction with the guide LLM. 
&lt;/p&gt;</description></item><item><title>DynaQuant&#36890;&#36807;&#21160;&#24577;&#37327;&#21270;&#23454;&#29616;&#23545;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26174;&#30528;&#21387;&#32553;&#65292;&#20960;&#20046;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11800</link><description>&lt;p&gt;
DynaQuant: &#36890;&#36807;&#21160;&#24577;&#37327;&#21270;&#21387;&#32553;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26816;&#26597;&#28857;
&lt;/p&gt;
&lt;p&gt;
DynaQuant: Compressing Deep Learning Training Checkpoints via Dynamic Quantization. (arXiv:2306.11800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11800
&lt;/p&gt;
&lt;p&gt;
DynaQuant&#36890;&#36807;&#21160;&#24577;&#37327;&#21270;&#23454;&#29616;&#23545;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26174;&#30528;&#21387;&#32553;&#65292;&#20960;&#20046;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#24037;&#20316;&#37327;&#22312;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#28040;&#32791;&#26041;&#38754;&#30340;&#22686;&#21152;&#65292;&#36935;&#21040;&#35757;&#32451;&#22833;&#36133;&#30340;&#21487;&#33021;&#24615;&#26174;&#33879;&#22686;&#21152;&#65292;&#23548;&#33268;&#24037;&#20316;&#20002;&#22833;&#21644;&#36164;&#28304;&#28010;&#36153;&#12290;&#26368;&#26032;&#30340;&#26041;&#27861;&#28041;&#21450;&#26377;&#25439;&#27169;&#22411;&#21387;&#32553;&#26426;&#21046;&#65292;&#36825;&#20250;&#22312;&#27169;&#22411;&#36136;&#37327;&#65288;&#20934;&#30830;&#24615;&#65289;&#21644;&#21387;&#32553;&#27604;&#20043;&#38388;&#20135;&#29983;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21160;&#24577;&#37327;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;DynaQuant&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#35757;&#32451;&#26399;&#38388;&#27169;&#22411;&#26435;&#37325;&#30340;&#28789;&#25935;&#24230;&#21464;&#21270;&#26469;&#26356;&#26032;&#37327;&#21270;&#32423;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26174;&#30528;&#21387;&#32553;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increase in the scale of Deep Learning (DL) training workloads in terms of compute resources and time consumption, the likelihood of encountering in-training failures rises substantially, leading to lost work and resource wastage. Such failures are typically offset by a checkpointing mechanism, which comes at the cost of storage and network bandwidth overhead. State-of-the-art approaches involve lossy model compression mechanisms, which induce a tradeoff between the resulting model quality (accuracy) and compression ratio. Delta compression is then also used to further reduce the overhead by only storing the difference between consecutive checkpoints. We make a key enabling observation that the sensitivity of model weights to compression varies during training, and different weights benefit from different quantization levels (ranging from retaining full precision to pruning). We propose (1) a non-uniform quantization scheme that leverages this variation, (2) an efficient searc
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#33021;&#22312;&#20445;&#25345;&#25968;&#25454;&#32431;&#24230;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25628;&#32034;&#36895;&#24230;&#12289;&#21442;&#25968;&#31354;&#38388;&#35206;&#30422;&#21644;&#25628;&#32034;&#28789;&#25935;&#24230;&#65292;&#32780;&#35813;&#27169;&#22411;&#22312;LIGO&#25968;&#25454;&#19978;&#27979;&#35797;&#30340;&#34920;&#29616;&#34920;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11797</link><description>&lt;p&gt;
&#38024;&#23545;&#24341;&#21147;&#27874;&#25968;&#25454;&#20013;&#30340;&#32039;&#20945;&#20108;&#36827;&#21046;&#21512;&#24182;&#20107;&#20214;&#26816;&#27979;&#65292;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#31283;&#20581;&#21487;&#38752;
&lt;/p&gt;
&lt;p&gt;
Towards a robust and reliable deep learning approach for detection of compact binary mergers in gravitational wave data. (arXiv:2306.11797v1 [gr-qc])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#33021;&#22312;&#20445;&#25345;&#25968;&#25454;&#32431;&#24230;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25628;&#32034;&#36895;&#24230;&#12289;&#21442;&#25968;&#31354;&#38388;&#35206;&#30422;&#21644;&#25628;&#32034;&#28789;&#25935;&#24230;&#65292;&#32780;&#35813;&#27169;&#22411;&#22312;LIGO&#25968;&#25454;&#19978;&#27979;&#35797;&#30340;&#34920;&#29616;&#34920;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#27867;&#21270;&#30340;&#20449;&#21495;&#21644;&#22122;&#22768;&#27169;&#22411;&#20197;&#21450;&#22312;GPU&#19978;&#24555;&#36895;&#25512;&#26029;&#65292;&#20026;&#22686;&#24378;&#24341;&#21147;&#27874;&#65288;GW&#65289;&#25628;&#32034;&#30340;&#36895;&#24230;&#12289;&#21442;&#25968;&#31354;&#38388;&#35206;&#30422;&#21644;&#25628;&#32034;&#28789;&#25935;&#24230;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#65292;&#20294;&#26159;DL&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#24615;&#20005;&#37325;&#25439;&#23475;&#20102;&#20854;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36880;&#27493;&#24320;&#21457;&#19968;&#20010;DL&#27169;&#22411;&#65292;&#24182;&#21162;&#21147;&#25913;&#36827;&#20854;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#32431;&#24230;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23548;&#20986;&#19968;&#31181;&#26356;&#22909;&#22320;&#21453;&#26144;&#25968;&#25454;&#20013;&#8220;&#21825;&#21886;&#8221;&#20449;&#21495;&#29305;&#24449;&#35270;&#35273;&#24378;&#24230;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#12290;&#21033;&#29992;&#36890;&#36807;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#33719;&#24471;&#30340;&#20943;&#23569;&#12289;&#24179;&#28369;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#25628;&#32034;&#32039;&#20945;&#20108;&#36827;&#21046;&#21512;&#24182;&#65288;CBC&#65289;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#30340;LIGO&#25968;&#25454;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36890;&#36807;&#23545;&#25239;&#24615;&#25915;&#20987;&#26469;&#25506;&#31350;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#26102;&#65292;&#20854;&#31616;&#21333;&#25925;&#38556;&#27169;&#24335;&#21017;&#25104;&#20026;&#20102;&#19968;&#20010;&#30171;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of deep learning (DL) approaches to learn generalised signal and noise models, coupled with their fast inference on GPUs, holds great promise for enhancing gravitational-wave (GW) searches in terms of speed, parameter space coverage, and search sensitivity. However, the opaque nature of DL models severely harms their reliability. In this work, we meticulously develop a DL model stage-wise and work towards improving its robustness and reliability. First, we address the problems in maintaining the purity of training data by deriving a new metric that better reflects the visual strength of the "chirp" signal features in the data. Using a reduced, smooth representation obtained through a variational auto-encoder (VAE), we build a classifier to search for compact binary coalescence (CBC) signals. Our tests on real LIGO data show an impressive performance of the model. However, upon probing the robustness of the model through adversarial attacks, its simple failure modes were ide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#36924;&#36817;&#20010;&#20307;&#30340;&#31227;&#21160;&#29366;&#24577;&#30340;&#28508;&#22312;&#20989;&#25968;&#65292;&#24182;&#32771;&#34385;&#20102;&#36716;&#31227;&#27010;&#29575;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#26102;&#38388;&#21487;&#21464;&#24615;&#65292;&#36890;&#36807;&#24378;&#21046;&#25191;&#34892;&#38543;&#26426;&#24615;&#21644;&#38750;&#36127;&#24615;&#32422;&#26463;&#26469;&#23454;&#29616;&#30740;&#31350;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.11772</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#26102;&#38388;&#21464;&#21270;&#36716;&#31227;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Time-Varying Transition Matrices with Multi-task Gaussian Processes. (arXiv:2306.11772v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#36924;&#36817;&#20010;&#20307;&#30340;&#31227;&#21160;&#29366;&#24577;&#30340;&#28508;&#22312;&#20989;&#25968;&#65292;&#24182;&#32771;&#34385;&#20102;&#36716;&#31227;&#27010;&#29575;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#26102;&#38388;&#21487;&#21464;&#24615;&#65292;&#36890;&#36807;&#24378;&#21046;&#25191;&#34892;&#38543;&#26426;&#24615;&#21644;&#38750;&#36127;&#24615;&#32422;&#26463;&#26469;&#23454;&#29616;&#30740;&#31350;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#36924;&#36817;&#20010;&#20307;&#30340;&#31227;&#21160;&#29366;&#24577;&#30340;&#28508;&#22312;&#20989;&#25968;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20855;&#26377;&#20004;&#20010;&#29366;&#24577;&#65288;&#31227;&#21160;&#21644;&#20572;&#30041;&#65289;&#30340;&#26102;&#38388;&#19981;&#22343;&#21248;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21019;&#24314;&#20219;&#21153;&#20043;&#38388;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#26469;&#32771;&#34385;&#36716;&#31227;&#27010;&#29575;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26102;&#38388;&#21487;&#21464;&#24615;&#65292;&#20551;&#35774;&#20010;&#20307;&#30340;&#36716;&#31227;&#27010;&#29575;&#38543;&#30528;&#22806;&#37096;&#21464;&#37327;&#32780;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#36890;&#36807;&#23558;&#32422;&#26463;&#28857;&#38598;&#25104;&#21040;&#39640;&#26031;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24378;&#21046;&#25191;&#34892;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#20013;&#27010;&#29575;&#30340;&#38543;&#26426;&#24615;&#21644;&#38750;&#36127;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21033;&#29992;Toeplitz&#21644;Kronecker&#20056;&#31215;&#32467;&#26500;&#22312;&#27492;&#19978;&#19979;&#25991;&#20013;&#21152;&#36895;GP&#20272;&#35745;&#21644;&#25512;&#26029;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20844;&#24335;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#25152;&#38656;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#21516;&#26102;&#23398;&#20064;&#36716;&#31227;&#27010;&#29575;&#30340;&#20989;&#25968;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a kernel-based, multi-task Gaussian Process (GP) model for approximating the underlying function of an individual's mobility state using a time-inhomogeneous Markov Process with two states: moves and pauses. Our approach accounts for the correlations between the transition probabilities by creating a covariance matrix over the tasks. We also introduce time-variability by assuming that an individual's transition probabilities vary over time in response to exogenous variables. We enforce the stochasticity and non-negativity constraints of probabilities in a Markov process through the incorporation of a set of constraint points in the GP. We also discuss opportunities to speed up GP estimation and inference in this context by exploiting Toeplitz and Kronecker product structures. Our numerical experiments demonstrate the ability of our formulation to enforce the desired constraints while learning the functional form of transition probabilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#24615;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#24037;&#20214;&#30340;&#35774;&#35745;&#26041;&#27861;&#23398;&#65292;&#20197;&#35299;&#20915;&#20915;&#31574;&#32773;&#19981;&#24895;&#20351;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#31243;&#24207;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#23398;&#21253;&#25324;&#20116;&#20010;&#38454;&#27573;&#12290;&#36890;&#36807;&#35774;&#35745;&#24182;&#23454;&#26045;&#19968;&#20010;&#24037;&#20214;&#65292;&#39044;&#27979;&#35786;&#26029;&#20026;&#26032;&#20896;&#32954;&#28814;&#30340;&#24739;&#32773;&#26159;&#21542;&#23558;&#26469;&#20250;&#20986;&#29616;&#33457;&#31881;&#28909;&#30151;&#29366;&#26469;&#35777;&#26126;&#20854;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11771</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#24037;&#20214;&#30340;&#35774;&#35745;&#65306;&#26041;&#27861;&#21644;&#23454;&#29992;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Designing Explainable Predictive Machine Learning Artifacts: Methodology and Practical Demonstration. (arXiv:2306.11771v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#24615;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#24037;&#20214;&#30340;&#35774;&#35745;&#26041;&#27861;&#23398;&#65292;&#20197;&#35299;&#20915;&#20915;&#31574;&#32773;&#19981;&#24895;&#20351;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#31243;&#24207;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#23398;&#21253;&#25324;&#20116;&#20010;&#38454;&#27573;&#12290;&#36890;&#36807;&#35774;&#35745;&#24182;&#23454;&#26045;&#19968;&#20010;&#24037;&#20214;&#65292;&#39044;&#27979;&#35786;&#26029;&#20026;&#26032;&#20896;&#32954;&#28814;&#30340;&#24739;&#32773;&#26159;&#21542;&#23558;&#26469;&#20250;&#20986;&#29616;&#33457;&#31881;&#28909;&#30151;&#29366;&#26469;&#35777;&#26126;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#32452;&#32455;&#20013;&#36234;&#26469;&#36234;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#37325;&#35201;&#30340;&#19994;&#21153;&#39046;&#22495;&#25512;&#21160;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#26469;&#33258;&#21508;&#34892;&#21508;&#19994;&#20844;&#21496;&#30340;&#20915;&#31574;&#32773;&#20173;&#28982;&#26497;&#20026;&#19981;&#24895;&#24847;&#38599;&#29992;&#22522;&#20110;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#25105;&#20204;&#25226;&#36825;&#20010;&#38382;&#39064;&#24402;&#22240;&#20110;&#23545;&#20110;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26222;&#36941;&#25345;&#26377;&#30340;&#35266;&#28857;&#65292;&#21363;&#23427;&#20204;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#65292;&#20854;&#22797;&#26434;&#24615;&#19981;&#20801;&#35768;&#25581;&#31034;&#39537;&#21160;&#30456;&#24212;&#31995;&#32479;&#36755;&#20986;&#30340;&#22240;&#32032;&#12290;&#20026;&#20102;&#26377;&#21161;&#20110;&#20811;&#26381;&#36825;&#31181;&#37319;&#29992;&#38556;&#30861;&#65292;&#25105;&#20204;&#35748;&#20026;&#20449;&#24687;&#31995;&#32479;&#30740;&#31350;&#24212;&#26356;&#22810;&#22320;&#20851;&#27880;&#35774;&#35745;&#33021;&#21521;&#20154;&#31867;&#20915;&#31574;&#32773;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#21407;&#22411;&#39044;&#27979;&#23548;&#21521;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#65288;&#21363;&#24037;&#20214;&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#20986;&#29616;&#20102;&#35768;&#22810;&#24037;&#20855;&#26469;&#20419;&#36827;&#36825;&#31181;&#24037;&#20214;&#30340;&#24320;&#21457;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#23545;&#20854;&#24320;&#21457;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#30740;&#31350;&#31354;&#30333;&#26159;&#30001;&#20110;&#32570;&#20047;&#24050;&#24314;&#31435;&#30340;&#35774;&#35745;&#26041;&#27861;&#23398;&#65292;&#22240;&#27492;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#36825;&#26679;&#30340;&#26041;&#27861;&#23398;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#21253;&#25324;&#20116;&#20010;&#38454;&#27573;&#65292;&#24341;&#23548;&#24037;&#31243;&#24072;&#23436;&#25104;&#24037;&#20214;&#30340;&#35843;&#26597;&#12289;&#35268;&#33539;&#12289;&#35774;&#35745;&#12289;&#39564;&#35777;&#21644;&#23454;&#26045;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#24182;&#23454;&#26045;&#19968;&#20010;&#24037;&#20214;&#26469;&#35777;&#26126;&#20854;&#36866;&#29992;&#24615;&#65292;&#35813;&#24037;&#20214;&#39044;&#27979;&#35786;&#26029;&#20026;&#26032;&#20896;&#32954;&#28814;&#30340;&#24739;&#32773;&#26159;&#21542;&#23558;&#26469;&#20250;&#20986;&#29616;&#33457;&#31881;&#28909;&#30151;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction-oriented machine learning is becoming increasingly valuable to organizations, as it may drive applications in crucial business areas. However, decision-makers from companies across various industries are still largely reluctant to employ applications based on modern machine learning algorithms. We ascribe this issue to the widely held view on advanced machine learning algorithms as "black boxes" whose complexity does not allow for uncovering the factors that drive the output of a corresponding system. To contribute to overcome this adoption barrier, we argue that research in information systems should devote more attention to the design of prototypical prediction-oriented machine learning applications (i.e., artifacts) whose predictions can be explained to human decision-makers. However, despite the recent emergence of a variety of tools that facilitate the development of such artifacts, there has so far been little research on their development. We attribute this research g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#31995;&#32479;&#22238;&#39038;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20998;&#21035;&#35752;&#35770;&#20102;&#19981;&#21516;&#20219;&#21153;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#35813;&#39046;&#22495;&#30340;&#21069;&#26223;&#30475;&#22909;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.11768</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design. (arXiv:2306.11768v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#31995;&#32479;&#22238;&#39038;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20998;&#21035;&#35752;&#35770;&#20102;&#19981;&#21516;&#20219;&#21153;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#35813;&#39046;&#22495;&#30340;&#21069;&#26223;&#30475;&#22909;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#21033;&#29992;&#34507;&#30333;&#36136;&#30340;&#19977;&#32500;&#20960;&#20309;&#32467;&#26500;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#29289;&#29702;&#21270;&#23398;&#24314;&#27169;&#21644;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#30340;&#20256;&#32479;&#26041;&#27861;&#36153;&#26102;&#36153;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#21487;&#20197;&#22788;&#29702;&#21644;&#25972;&#21512;&#19977;&#32500;&#20960;&#20309;&#25968;&#25454;&#65292;&#21152;&#19978;&#31867;&#20284;AlphaFold&#30340;&#24037;&#20855;&#25552;&#20379;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;&#19977;&#32500;&#32467;&#26500;&#39044;&#27979;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#20174;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#20027;&#27969;&#20219;&#21153;&#12289;&#24120;&#29992;&#30340;3D&#34507;&#30333;&#36136;&#34920;&#31034;&#21644;&#39044;&#27979;/&#29983;&#25104;&#27169;&#22411;&#20837;&#25163;&#65292;&#28982;&#21518;&#35814;&#32454;&#20171;&#32461;&#27599;&#20010;&#20219;&#21153;&#30340;&#22238;&#39038;&#65288;&#20363;&#22914;&#32467;&#21512;&#20301;&#28857;&#39044;&#27979;&#12289;&#32467;&#21512;&#26500;&#35937;&#29983;&#25104;&#12289;\emph{de novo} &#20998;&#23376;&#35774;&#35745;&#31561;&#65289;&#65292;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#35813;&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure-based drug design (SBDD), which utilizes the three-dimensional geometry of proteins to identify potential drug candidates, is becoming increasingly vital in drug discovery. However, traditional methods based on physiochemical modeling and experts' domain knowledge are time-consuming and laborious. The recent advancements in geometric deep learning, which integrates and processes 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have significantly propelled progress in structure-based drug design. In this paper, we systematically review the recent progress of geometric deep learning for structure-based drug design. We start with a brief discussion of the mainstream tasks in structure-based drug design, commonly used 3D protein representations and representative predictive/generative models. Then we delve into detailed reviews for each task (binding site prediction, binding pose generation, \emph{de novo} mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#24320;&#23637;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#30740;&#31350;&#65292;&#20197;&#36845;&#20195;&#20989;&#25968;&#31995;&#32479;&#26694;&#26550;&#23454;&#29616;&#30456;&#20851;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2306.11765</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#20123;&#21387;&#32553;&#31639;&#27861; (arXiv:2306.11765v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
About some compression algorithms. (arXiv:2306.11765v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#24320;&#23637;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#30740;&#31350;&#65292;&#20197;&#36845;&#20195;&#20989;&#25968;&#31995;&#32479;&#26694;&#26550;&#23454;&#29616;&#30456;&#20851;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#22312;&#36845;&#20195;&#20989;&#25968;&#31995;&#32479;&#26694;&#26550;&#20013;&#23547;&#25214;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#32452;&#20351;&#21306;&#38388;$(0, 1)$&#30340;&#21464;&#25442;&#28385;&#36275;&#36866;&#24403;&#24615;&#36136;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use neural network algorithms for finding compression methods of images in the framework of iterated function systems which is a collection of the transformations of the interval $(0, 1)$ satisfying suitable properties.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#38899;&#39057;&#39057;&#35889;&#36716;&#25442;&#20013;&#36890;&#36807;&#39057;&#29575;&#24402;&#19968;&#21270;&#26469;&#25552;&#39640;&#24405;&#38899;&#35774;&#22791;&#26222;&#36866;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;AST&#27169;&#22411;&#20013;&#20351;&#29992;&#39057;&#29575;&#24402;&#19968;&#21270;&#21487;&#20197;&#38477;&#20302;&#19981;&#21516;&#24405;&#38899;&#35774;&#22791;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11764</link><description>&lt;p&gt;
&#39057;&#29575;&#24402;&#19968;&#21270;&#22312;&#38899;&#39057;&#39057;&#35889;&#36716;&#25442;&#20013;&#25552;&#39640;&#24405;&#38899;&#35774;&#22791;&#26222;&#36866;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Frequency-Wise Normalizations for Better Recording Device Generalization in Audio Spectrogram Transformers. (arXiv:2306.11764v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#38899;&#39057;&#39057;&#35889;&#36716;&#25442;&#20013;&#36890;&#36807;&#39057;&#29575;&#24402;&#19968;&#21270;&#26469;&#25552;&#39640;&#24405;&#38899;&#35774;&#22791;&#26222;&#36866;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;AST&#27169;&#22411;&#20013;&#20351;&#29992;&#39057;&#29575;&#24402;&#19968;&#21270;&#21487;&#20197;&#38477;&#20302;&#19981;&#21516;&#24405;&#38899;&#35774;&#22791;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#35757;&#32451;&#25968;&#25454;&#19982;&#24212;&#29992;&#26102;&#25968;&#25454;&#20043;&#38388;&#30340;&#21464;&#21270;&#26465;&#20214;&#19981;&#21516;&#12290;&#35813;&#35770;&#25991;&#20174;&#22768;&#23398;&#24773;&#26223;&#20998;&#31867;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#19981;&#21305;&#37197;&#24405;&#38899;&#35774;&#22791;&#26102;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20943;&#23569;&#24405;&#38899;&#35774;&#22791;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#21644;&#38544;&#34255;&#23618;&#28608;&#27963;&#20013;&#36827;&#34892;&#39057;&#29575;&#24402;&#19968;&#21270;&#33021;&#22815;&#38477;&#20302;&#24405;&#38899;&#35774;&#22791;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38899;&#39057;&#39057;&#35889;&#21464;&#25442;&#30340; AST &#27169;&#22411;&#20013;&#65292;&#24405;&#38899;&#35774;&#22791;&#29305;&#24449;&#22914;&#20309;&#22312;&#38544;&#34255;&#23618;&#28608;&#27963;&#20013;&#34920;&#36798;&#65292;&#21457;&#29616;&#39057;&#29575;&#32500;&#24230;&#26368;&#21021;&#32534;&#30721;&#24405;&#38899;&#35774;&#22791;&#20449;&#24687;&#65292;&#20294;&#26159;&#32463;&#36807;&#31532;&#19968;&#27425;&#33258;&#27880;&#24847;&#21147;&#22359;&#21518;&#65292;&#23427;&#22823;&#37096;&#20998;&#34987;&#36716;&#25442;&#20026;&#20196;&#29260;&#32500;&#24230;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#26412;&#25991;&#25512;&#27979;&#22312;&#39057;&#29575;&#32500;&#24230;&#25233;&#21046;&#24405;&#38899;&#35774;&#22791;&#29305;&#24449;&#21487;&#33021;&#26356;&#22909;&#22320;&#25512;&#24191;AST&#27169;&#22411;&#21040;&#19981;&#21305;&#37197;&#24405;&#38899;&#35774;&#22791;&#20013;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22768;&#23398;&#24773;&#26223;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#39057;&#29575;&#24402;&#19968;&#21270;&#30830;&#23454;&#25552;&#39640;&#20102;AST&#27169;&#22411;&#22312;&#22788;&#29702;&#19981;&#21516;&#24405;&#38899;&#35774;&#22791;&#30340;&#25968;&#25454;&#26102;&#30340;&#20934;&#30830;&#24230;&#21644;&#31283;&#20581;&#24615;&#27604;&#27809;&#26377;&#24402;&#19968;&#21270;&#30340;&#22522;&#32447;&#27169;&#22411;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Varying conditions between the data seen at training and at application time remain a major challenge for machine learning. We study this problem in the context of Acoustic Scene Classification (ASC) with mismatching recording devices. Previous works successfully employed frequency-wise normalization of inputs and hidden layer activations in convolutional neural networks to reduce the recording device discrepancy. The main objective of this work was to adopt frequency-wise normalization for Audio Spectrogram Transformers (ASTs), which have recently become the dominant model architecture in ASC. To this end, we first investigate how recording device characteristics are encoded in the hidden layer activations of ASTs. We find that recording device information is initially encoded in the frequency dimension; however, after the first self-attention block, it is largely transformed into the token dimension. Based on this observation, we conjecture that suppressing recording device character
&lt;/p&gt;</description></item><item><title>MRFI&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#65292;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#29420;&#31435;&#30340;&#25925;&#38556;&#37197;&#32622;&#25991;&#20214;&#36827;&#34892;&#27880;&#20837;&#21644;&#28431;&#27934;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.11758</link><description>&lt;p&gt;
MRFI&#65306;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#30340;&#24320;&#28304;&#22810;&#20998;&#36776;&#29575;&#25925;&#38556;&#27880;&#20837;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MRFI: An Open Source Multi-Resolution Fault Injection Framework for Neural Network Processing. (arXiv:2306.11758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11758
&lt;/p&gt;
&lt;p&gt;
MRFI&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#65292;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#29420;&#31435;&#30340;&#25925;&#38556;&#37197;&#32622;&#25991;&#20214;&#36827;&#34892;&#27880;&#20837;&#21644;&#28431;&#27934;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#21363;&#20351;&#22312;&#19981;&#21487;&#38752;&#30340;&#30828;&#20214;&#19978;&#20063;&#33021;&#36827;&#34892;&#26377;&#24377;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#37096;&#32626;&#20043;&#21069;&#36827;&#34892;&#21508;&#31181;&#30828;&#20214;&#25925;&#38556;&#30340;&#20840;&#38754;&#21487;&#38752;&#24615;&#20998;&#26512;&#65292;&#24182;&#19988;&#38656;&#35201;&#39640;&#25928;&#30340;&#38169;&#35823;&#27880;&#20837;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#20173;&#28982;&#23616;&#38480;&#20110;&#23545;&#31070;&#32463;&#20803;&#30340;&#22522;&#26412;&#25925;&#38556;&#27880;&#20837;&#65292;&#24182;&#26410;&#25552;&#20379;&#32454;&#31890;&#24230;&#28431;&#27934;&#20998;&#26512;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#20173;&#38656;&#35201;&#26356;&#25913;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#20351;&#25925;&#38556;&#27880;&#20837;&#19982;&#27491;&#24120;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#32039;&#23494;&#32806;&#21512;&#65292;&#36825;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#30340;&#20351;&#29992;&#38590;&#24230;&#24182;&#20943;&#24930;&#20102;&#25925;&#38556;&#27169;&#25311;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22810;&#20998;&#36776;&#29575;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;MRFI&#12290;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#20462;&#25913;&#29420;&#31435;&#30340;&#25925;&#38556;&#37197;&#32622;&#25991;&#20214;&#65292;&#32780;&#19981;&#26159;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#25925;&#38556;&#27880;&#20837;&#21644;&#28431;&#27934;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
To ensure resilient neural network processing on even unreliable hardware, comprehensive reliability analysis against various hardware faults is generally required before the deep neural network models are deployed, and efficient error injection tools are highly demanded. However, most existing fault injection tools remain rather limited to basic fault injection to neurons and fail to provide fine-grained vulnerability analysis capability. In addition, many of the fault injection tools still need to change the neural network models and make the fault injection closely coupled with normal neural network processing, which further complicates the use of the fault injection tools and slows down the fault simulation. In this work, we propose MRFI, a highly configurable multi-resolution fault injection tool for deep neural networks. It enables users to modify an independent fault configuration file rather than neural network models for the fault injection and vulnerability analysis. Particul
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#20351;&#29992;&#39044;&#20462;&#21098;&#21644;&#26799;&#24230;&#19979;&#38477;&#25216;&#24039;&#26469;&#20943;&#23569;&#21442;&#25968;&#31354;&#38388;&#21644;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#21892;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#20998;&#31867;&#30340;&#38544;&#31169;-&#20934;&#30830;&#24615;&#25240;&#34935;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11754</link><description>&lt;p&gt;
&#39044;&#20462;&#21098;&#21644;&#26799;&#24230;&#19979;&#38477;&#25913;&#36827;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Pre-Pruning and Gradient-Dropping Improve Differentially Private Image Classification. (arXiv:2306.11754v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11754
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#20351;&#29992;&#39044;&#20462;&#21098;&#21644;&#26799;&#24230;&#19979;&#38477;&#25216;&#24039;&#26469;&#20943;&#23569;&#21442;&#25968;&#31354;&#38388;&#21644;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#21892;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#20998;&#31867;&#30340;&#38544;&#31169;-&#20934;&#30830;&#24615;&#25240;&#34935;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#26102;&#65292;&#21487;&#25193;&#23637;&#24615;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#25361;&#25112;&#12290;&#24120;&#29992;&#30340;DP-SGD&#31639;&#27861;&#22312;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#20351;&#22312;&#20013;&#31561;&#22823;&#23567;&#30340;&#27169;&#22411;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26377;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#29305;&#28857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24046;&#20998;&#38544;&#31169;&#20013;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#20351;&#29992;&#39044;&#20462;&#21098;&#21644;&#26799;&#24230;&#19979;&#38477;&#26469;&#20943;&#23569;&#21442;&#25968;&#31354;&#38388;&#24182;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;&#35813;&#36807;&#31243;&#22987;&#20110;&#23545;&#21407;&#22987;&#32593;&#32476;&#30340;&#21442;&#25968;&#36827;&#34892;&#20462;&#21098;&#65292;&#20197;&#33719;&#21462;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;DP-SGD&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#19981;&#37325;&#35201;&#30340;&#26799;&#24230;&#34987;&#21024;&#38500;&#65292;&#21482;&#26377;&#36873;&#25321;&#30340;&#26799;&#24230;&#34987;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#33539;&#24335;&#22312;&#39044;&#20462;&#21098;&#21644;&#26799;&#24230;&#19979;&#38477;&#36895;&#29575;&#12289;&#38544;&#31169;&#25439;&#22833;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#20043;&#38388;&#24341;&#20837;&#20102;&#19968;&#31181;&#24352;&#21147;&#12290;&#36807;&#22810;&#25110;&#36807;&#23569;&#30340;&#20219;&#20309;&#19968;&#31181;&#37117;&#21487;&#33021;&#23548;&#33268;&#20934;&#30830;&#24615;&#25110;&#38544;&#31169;&#20445;&#25252;&#30340;&#19979;&#38477;&#12290;&#36890;&#36807;&#23545;CIFAR-10&#21644;CIFAR-100&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#35757;&#32451;&#33539;&#24335;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#38544;&#31169;-&#20934;&#30830;&#24615;&#30340;&#25240;&#34935;&#65292;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scalability is a significant challenge when it comes to applying differential privacy to training deep neural networks. The commonly used DP-SGD algorithm struggles to maintain a high level of privacy protection while achieving high accuracy on even moderately sized models. To tackle this challenge, we take advantage of the fact that neural networks are overparameterized, which allows us to improve neural network training with differential privacy. Specifically, we introduce a new training paradigm that uses \textit{pre-pruning} and \textit{gradient-dropping} to reduce the parameter space and improve scalability. The process starts with pre-pruning the parameters of the original network to obtain a smaller model that is then trained with DP-SGD. During training, less important gradients are dropped, and only selected gradients are updated. Our training paradigm introduces a tension between the rates of pre-pruning and gradient-dropping, privacy loss, and classification accuracy. Too mu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#24863;&#30693;&#27169;&#22359;&#30340;&#26368;&#26032;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#25216;&#26415;&#12290;&#20854;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38598;&#25104;&#20998;&#31867;&#31995;&#32479;&#65292;&#24635;&#32467;&#20102;&#38598;&#25104;&#25805;&#20316;&#21450;&#20854;&#20248;&#32570;&#28857;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#38416;&#26126;&#20102;&#8220;&#29702;&#24819;&#8221;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#30340;&#29305;&#24615;&#65292;&#21487;&#20943;&#36731;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#20248;&#21270;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.11740</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#25968;&#25454;&#38598;&#25104;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A survey on deep learning approaches for data integration in autonomous driving system. (arXiv:2306.11740v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#24863;&#30693;&#27169;&#22359;&#30340;&#26368;&#26032;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#25216;&#26415;&#12290;&#20854;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38598;&#25104;&#20998;&#31867;&#31995;&#32479;&#65292;&#24635;&#32467;&#20102;&#38598;&#25104;&#25805;&#20316;&#21450;&#20854;&#20248;&#32570;&#28857;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#38416;&#26126;&#20102;&#8220;&#29702;&#24819;&#8221;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#30340;&#29305;&#24615;&#65292;&#21487;&#20943;&#36731;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#20248;&#21270;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#30340;&#24863;&#30693;&#27169;&#22359;&#20381;&#36182;&#20110;&#22810;&#20256;&#24863;&#22120;&#31995;&#32479;&#26469;&#29702;&#35299;&#20854;&#29615;&#22659;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#22810;&#24863;&#30693;&#27979;&#37327;&#30340;&#38598;&#25104;&#26041;&#27861;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#20197;&#22686;&#24378;&#24863;&#30693;&#33021;&#21147;&#12290;&#26412;&#25991;&#23545;&#24212;&#29992;&#20110;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#24863;&#30693;&#27169;&#22359;&#30340;&#26368;&#26032;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#25216;&#26415;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#23558;&#38598;&#25104;&#26041;&#27861;&#20998;&#20026;&#8220;&#20309;&#26102;&#38598;&#25104;&#8221;&#65292;&#8220;&#22914;&#20309;&#38598;&#25104;&#8221;&#21644;&#8220;&#20309;&#26102;&#38598;&#25104;&#8221;&#19977;&#31867;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38598;&#25104;&#20998;&#31867;&#31995;&#32479;&#65292;&#22522;&#20110;&#19977;&#20010;&#32500;&#24230;&#65306;&#22810;&#35270;&#35282;&#65292;&#22810;&#27169;&#24577;&#21644;&#22810;&#24103;&#12290;&#24635;&#32467;&#20102;&#38598;&#25104;&#25805;&#20316;&#21450;&#20854;&#20248;&#32570;&#28857;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#38416;&#26126;&#20102;&#8220;&#29702;&#24819;&#8221;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#30340;&#29305;&#24615;&#65292;&#21487;&#20943;&#36731;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#32463;&#36807;&#23545;&#19978;&#30334;&#31687;&#30456;&#20851;&#35770;&#25991;&#30340;&#23457;&#26597;&#65292;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#20248;&#21270;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The perception module of self-driving vehicles relies on a multi-sensor system to understand its environment. Recent advancements in deep learning have led to the rapid development of approaches that integrate multi-sensory measurements to enhance perception capabilities. This paper surveys the latest deep learning integration techniques applied to the perception module in autonomous driving systems, categorizing integration approaches based on "what, how, and when to integrate." A new taxonomy of integration is proposed, based on three dimensions: multi-view, multi-modality, and multi-frame. The integration operations and their pros and cons are summarized, providing new insights into the properties of an "ideal" data integration approach that can alleviate the limitations of existing methods. After reviewing hundreds of relevant papers, this survey concludes with a discussion of the key features of an optimal data integration approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#32593;&#26684;&#20998;&#21106;&#20043;&#21069;&#32534;&#30721;&#26144;&#23556;&#20989;&#25968;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#65292;&#19981;&#21463;&#20998;&#36776;&#29575;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.11737</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#32593;&#26684;&#20998;&#21106;&#30340;&#31070;&#32463;&#24418;&#29366;&#30452;&#24452;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Neural Shape Diameter Function for Efficient Mesh Segmentation. (arXiv:2306.11737v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11737
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#32593;&#26684;&#20998;&#21106;&#20043;&#21069;&#32534;&#30721;&#26144;&#23556;&#20989;&#25968;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#65292;&#19981;&#21463;&#20998;&#36776;&#29575;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22810;&#36793;&#24418;&#32593;&#26684;&#20998;&#21106;&#20026;&#26377;&#24847;&#20041;&#30340;&#37096;&#20998;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#23558;&#36825;&#20123;&#32467;&#26500;&#20998;&#35299;&#20197;&#36827;&#34892;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#30340;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20195;&#20215;&#26159;&#35745;&#31639;&#26102;&#38388;&#30340;&#22823;&#37327;&#28040;&#32791;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#23545;3D&#32467;&#26500;&#30340;&#20998;&#21106;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#25512;&#24191;&#65292;&#24182;&#38656;&#35201;&#23558;&#23398;&#20064;&#30340;&#27169;&#22411;&#20998;&#25104;&#20960;&#20010;&#29305;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#65292;&#20197;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#32593;&#26684;&#20998;&#21106;&#20043;&#21069;&#32534;&#30721;&#26144;&#23556;&#20989;&#25968;&#65292;&#20197;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#20351;&#29992;&#39030;&#28857;&#37051;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#22797;&#29616;&#37051;&#22495;&#22270;&#65292;&#21033;&#29992;&#25105;&#20204;&#23545;&#24418;&#29366;&#30452;&#24452;&#20989;&#25968;&#65288;SDF&#65289;&#26041;&#27861;&#30340;&#20102;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21463;&#20998;&#36776;&#29575;&#24433;&#21709;&#65292;&#22240;&#20026;&#25105;&#20204;&#23545;&#36755;&#20837;&#32593;&#26684;&#36827;&#34892;&#19979;&#37319;&#26679;&#65292;&#24182;&#20165;&#38024;&#23545;&#37051;&#23621;&#26597;&#35810;&#23436;&#25972;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partitioning a polygonal mesh into meaningful parts can be challenging. Many applications require decomposing such structures for further processing in computer graphics. In the last decade, several methods were proposed to tackle this problem, at the cost of intensive computational times. Recently, machine learning has proven to be effective for the segmentation task on 3D structures. Nevertheless, these state-of-the-art methods are often hardly generalizable and require dividing the learned model into several specific classes of objects to avoid overfitting. We present a data-driven approach leveraging deep learning to encode a mapping function prior to mesh segmentation for multiple applications. Our network reproduces a neighborhood map using our knowledge of the \textsl{Shape Diameter Function} (SDF) method using similarities among vertex neighborhoods. Our approach is resolution-agnostic as we downsample the input meshes and query the full-resolution structure solely for neighbor
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Event Stream GPT (ESGPT)&#8212;&#8212;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#30340;GPT&#27169;&#22411;&#30340;&#24320;&#28304;&#24211;&#12290;&#35813;&#24211;&#25903;&#25345;&#22312;&#21307;&#30103;&#35760;&#24405;&#31561;&#20855;&#26377;&#20869;&#37096;&#20381;&#36182;&#24615;&#30340;&#22797;&#26434;&#20107;&#20214;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#20855;&#26377;&#39640;&#25928;&#26131;&#29992;&#19988;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.11547</link><description>&lt;p&gt;
&#20107;&#20214;&#27969;GPT&#65306;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#20107;&#20214;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#24314;&#27169;&#24211;
&lt;/p&gt;
&lt;p&gt;
Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events. (arXiv:2306.11547v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11547
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Event Stream GPT (ESGPT)&#8212;&#8212;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#30340;GPT&#27169;&#22411;&#30340;&#24320;&#28304;&#24211;&#12290;&#35813;&#24211;&#25903;&#25345;&#22312;&#21307;&#30103;&#35760;&#24405;&#31561;&#20855;&#26377;&#20869;&#37096;&#20381;&#36182;&#24615;&#30340;&#22797;&#26434;&#20107;&#20214;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#20855;&#26377;&#39640;&#25928;&#26131;&#29992;&#19988;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#12289;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#36890;&#36807;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#22810;&#26679;&#24615;&#65292;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#65292;&#20294;&#20854;&#28508;&#21147;&#36828;&#19981;&#27490;&#20110;&#27492;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#36719;&#20214;&#24037;&#20855;&#65292;&#25193;&#23637;&#20102;GPT&#30340;&#36866;&#29992;&#24615;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20869;&#37096;&#20381;&#36182;&#20851;&#31995;&#30340;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#20107;&#20214;&#65292;&#20363;&#22914;&#21307;&#30103;&#35760;&#24405;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative, pre-trained transformers (GPTs, a.k.a. "Foundation Models") have reshaped natural language processing (NLP) through their versatility in diverse downstream tasks. However, their potential extends far beyond NLP. This paper provides a software utility to help realize this potential, extending the applicability of GPTs to continuous-time sequences of complex events with internal dependencies, such as medical record datasets. Despite their potential, the adoption of foundation models in these domains has been hampered by the lack of suitable tools for model construction and evaluation. To bridge this gap, we introduce Event Stream GPT (ESGPT), an open-source library designed to streamline the end-to-end process for building GPTs for continuous-time event sequences. ESGPT allows users to (1) build flexible, foundation-model scale input datasets by specifying only a minimal configuration file, (2) leverage a Hugging Face compatible modeling API for GPTs over this modality that i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#21487;&#24494;&#20998;&#36712;&#36857;&#37325;&#21152;&#26435;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21183;&#33021;&#65292;&#23454;&#29616;&#20102;&#33258;&#19978;&#32780;&#19979;&#30340;&#31895;&#31890;&#21270;&#34507;&#30333;&#36136;&#21147;&#22330;&#24314;&#27169;&#65292;&#20165;&#38656;&#34507;&#30333;&#36136;&#30340;&#22825;&#28982;&#26500;&#35937;&#21363;&#21487;&#23637;&#31034;&#20854;&#22806;&#25512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.11375</link><description>&lt;p&gt;
&#33258;&#19978;&#32780;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#31895;&#31890;&#21270;&#34507;&#30333;&#36136;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
Top-down machine learning of coarse-grained protein force-fields. (arXiv:2306.11375v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11375
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#21487;&#24494;&#20998;&#36712;&#36857;&#37325;&#21152;&#26435;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21183;&#33021;&#65292;&#23454;&#29616;&#20102;&#33258;&#19978;&#32780;&#19979;&#30340;&#31895;&#31890;&#21270;&#34507;&#30333;&#36136;&#21147;&#22330;&#24314;&#27169;&#65292;&#20165;&#38656;&#34507;&#30333;&#36136;&#30340;&#22825;&#28982;&#26500;&#35937;&#21363;&#21487;&#23637;&#31034;&#20854;&#22806;&#25512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#34507;&#30333;&#36136;&#31895;&#31890;&#21270;&#34920;&#24449;&#23545;&#20110;&#29702;&#35299;&#23427;&#20204;&#30340;&#25240;&#21472;&#12289;&#21151;&#33021;&#21644;&#22312;&#38271;&#26102;&#38388;&#23610;&#24230;&#19979;&#30340;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#34507;&#30333;&#36136;&#65292;&#24182;&#21033;&#29992;&#24471;&#21040;&#30340;&#36712;&#36857;&#36890;&#36807;&#21487;&#24494;&#20998;&#36712;&#36857;&#37325;&#21152;&#26435;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21183;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#20165;&#38656;&#35201;&#34507;&#30333;&#36136;&#30340;&#22825;&#28982;&#26500;&#35937;&#65292;&#28040;&#38500;&#20102;&#20174;&#24191;&#27867;&#27169;&#25311;&#25110;&#20869;&#23384;&#23494;&#38598;&#22411;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#27169;&#25311;&#23548;&#20986;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#24182;&#34892;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#24182;&#23545;&#35757;&#32451;&#20998;&#24067;&#20869;&#22806;&#30340;&#34507;&#30333;&#36136;&#36827;&#34892;&#25240;&#21472;&#20107;&#20214;&#37319;&#26679;&#65292;&#23637;&#31034;&#20854;&#22806;&#25512;&#33021;&#21147;&#12290;&#36890;&#36807;&#24212;&#29992;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#27169;&#25311;&#34507;&#30333;&#36136;&#30340;&#19982;&#22825;&#28982;&#26500;&#35937;&#30456;&#20284;&#30340;&#26500;&#35937;&#12290;&#30001;&#20110;&#20854;&#29702;&#35770;&#21487;&#36716;&#31227;&#24615;&#21644;&#20165;&#20351;&#29992;&#34507;&#30333;&#36136;&#30340;&#22825;&#28982;&#26500;&#35937;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#24212;&#29992;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing accurate and efficient coarse-grained representations of proteins is crucial for understanding their folding, function, and interactions over extended timescales. Our methodology involves simulating proteins with molecular dynamics and utilizing the resulting trajectories to train a neural network potential through differentiable trajectory reweighting. Remarkably, this method requires only the native conformation of proteins, eliminating the need for labeled data derived from extensive simulations or memory-intensive end-to-end differentiable simulations. Once trained, the model can be employed to run parallel molecular dynamics simulations and sample folding events for proteins both within and beyond the training distribution, showcasing its extrapolation capabilities. By applying Markov State Models, native-like conformations of the simulated proteins can be predicted from the coarse-grained simulations. Owing to its theoretical transferability and ability to use solely e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#36924;&#30495;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#30340;&#22522;&#20934;&#35780;&#20272;RM-PRT&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#27169;&#24577;&#25552;&#31034;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.11335</link><description>&lt;p&gt;
RM-PRT: &#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#21644;&#22522;&#20110;&#28176;&#36827;&#25512;&#29702;&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks. (arXiv:2306.11335v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#36924;&#30495;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#30340;&#22522;&#20934;&#35780;&#20272;RM-PRT&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#27169;&#24577;&#25552;&#31034;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#30340;&#20986;&#29616;&#65292;&#26174;&#30528;&#25512;&#36827;&#20102;&#26426;&#22120;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36825;&#19968;&#31361;&#30772;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36825;&#20123;&#24320;&#28304;LLM&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#32479;&#19968;&#30340;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#29615;&#22659;&#20013;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#20934;&#30830;&#29702;&#35299;&#21644;&#25191;&#34892;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#28176;&#36827;&#25512;&#29702;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22522;&#20934;&#65288;RM-PRT&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;RM-PRT&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;Unreal Engine 5&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#20445;&#30495;&#25968;&#23383;&#21452;&#32990;&#32974;&#22330;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;782&#20010;&#31867;&#21035;&#65292;2023&#20010;&#29289;&#20307;&#65292;&#24182;&#20351;&#29992;ChatGPT&#29983;&#25104;&#20102;15,000&#20010;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20197;&#35814;&#32454;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;RM-PRT&#22522;&#20934;&#35780;&#20272;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#25509;&#21463;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#33258;&#21160;&#36755;&#20986;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities. This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions. To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation. We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs action
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#30340;GNN&#26550;&#26500;&#65292;&#22312;&#31890;&#23376;&#36319;&#36394;&#20013;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#36164;&#28304;&#39640;&#25928;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#29616;&#26377;CPU&#21644;GPU&#30340;&#25968;&#21315;&#20493;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.11330</link><description>&lt;p&gt;
&#22522;&#20110;FPGA&#30340;&#31890;&#23376;&#36712;&#36857;&#36319;&#36394;&#30340;&#20302;&#24310;&#36831;&#36793;&#32536;&#20998;&#31867;GNN
&lt;/p&gt;
&lt;p&gt;
Low Latency Edge Classification GNN for Particle Trajectory Tracking on FPGAs. (arXiv:2306.11330v1 [cs.AR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#30340;GNN&#26550;&#26500;&#65292;&#22312;&#31890;&#23376;&#36319;&#36394;&#20013;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#36164;&#28304;&#39640;&#25928;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#29616;&#26377;CPU&#21644;GPU&#30340;&#25968;&#21315;&#20493;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#20013;&#23454;&#26102;&#31890;&#23376;&#36712;&#36857;&#37325;&#24314;&#38754;&#20020;&#39640;&#30896;&#25758;&#29575;&#21644;&#20247;&#22810;&#31890;&#23376;&#25758;&#20987;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;FPGA&#19978;&#30340;GNN&#65288;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#28789;&#27963;&#30340;&#36712;&#36857;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#26550;&#26500;&#22312;&#36793;&#32536;&#20998;&#31867;&#26041;&#38754;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#20302;&#19988;&#24182;&#34892;&#24615;&#19981;&#36275;&#12290;&#26412;&#25991;&#22312;FPGA&#19978;&#24341;&#20837;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;GNN&#26550;&#26500;&#65292;&#29992;&#20110;&#20302;&#24310;&#36831;&#31890;&#23376;&#36319;&#36394;&#12290;&#27169;&#22359;&#21270;&#30340;&#26550;&#26500;&#20415;&#20110;&#35774;&#35745;&#25193;&#23637;&#20197;&#25903;&#25345;&#22823;&#22411;&#22270;&#24418;&#12290;&#21033;&#29992;&#20987;&#20013;&#25506;&#27979;&#22120;&#30340;&#20960;&#20309;&#29305;&#24615;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#22270;&#24418;&#30340;&#22797;&#26434;&#24615;&#21644;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;&#25105;&#20204;&#22312;Xilinx UltraScale+ VU9P&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;CPU&#21644;GPU&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;1625&#20493;&#21644;1574&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-time particle trajectory reconstruction in the Large Hadron Collider is challenging due to the high collision rate and numerous particle hits. Using GNN (Graph Neural Network) on FPGA has enabled superior accuracy with flexible trajectory classification. However, existing GNN architectures have inefficient resource usage and insufficient parallelism for edge classification. This paper introduces a resource-efficient GNN architecture on FPGAs for low latency particle tracking. The modular architecture facilitates design scalability to support large graphs. Leveraging the geometric properties of hit detectors further reduces graph complexity and resource usage. Our results on Xilinx UltraScale+ VU9P demonstrate 1625x and 1574x performance improvement over CPU and GPU respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GTAGC&#30340;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#22270;&#24418;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#33258;&#32534;&#30721;&#22120;&#21644;&#22270;&#24418;&#21464;&#25442;&#22120;&#65292;GTAGC&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#22270;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11307</link><description>&lt;p&gt;
&#22686;&#24378;&#23646;&#24615;&#32858;&#31867;&#30340;&#22270;&#24418;&#21464;&#25442;&#26041;&#27861;&#65306;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#24418;&#36716;&#25442;&#22120;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Transformer Method. (arXiv:2306.11307v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GTAGC&#30340;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#22270;&#24418;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#33258;&#32534;&#30721;&#22120;&#21644;&#22270;&#24418;&#21464;&#25442;&#22120;&#65292;GTAGC&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#22270;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#24433;&#21709;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#25105;&#20204;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#26377;&#21161;&#20110;&#22270;&#32858;&#31867;&#65292;&#36825;&#26159;&#21508;&#20010;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#27880;&#24847;&#21147;&#26426;&#21046;&#26368;&#36817;&#24050;&#32463;&#36827;&#20837;&#20102;&#22270;&#23398;&#20064;&#30340;&#39046;&#22495;&#65292;&#36825;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#30740;&#31350;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#24050;&#25104;&#20026;&#22270;&#32858;&#31867;&#20219;&#21153;&#20248;&#36873;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#23616;&#37096;&#27880;&#24847;&#26426;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#29702;&#35299;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#22797;&#26434;&#20840;&#23616;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#22270;&#24418;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#30340;&#22270;&#24418;&#32858;&#31867;&#65288;GTAGC&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22270;&#33258;&#32534;&#30721;&#22120;&#19982;&#22270;&#24418;&#21464;&#25442;&#22120;&#34701;&#21512;&#65292;GTAGC&#33021;&#22815;&#25429;&#33719;&#33410;&#28857;&#20043;&#38388;&#30340;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#34701;&#21512;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20351;&#24471;&#22270;&#24418;&#32858;&#31867;&#20219;&#21153;&#26377;&#20102;&#26174;&#30528;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Representation Learning (GRL) is an influential methodology, enabling a more profound understanding of graph-structured data and aiding graph clustering, a critical task across various domains. The recent incursion of attention mechanisms, originally an artifact of Natural Language Processing (NLP), into the realm of graph learning has spearheaded a notable shift in research trends. Consequently, Graph Attention Networks (GATs) and Graph Attention Auto-Encoders have emerged as preferred tools for graph clustering tasks. Yet, these methods primarily employ a local attention mechanism, thereby curbing their capacity to apprehend the intricate global dependencies between nodes within graphs. Addressing these impediments, this study introduces an innovative method known as the Graph Transformer Auto-Encoder for Graph Clustering (GTAGC). By melding the Graph Auto-Encoder with the Graph Transformer, GTAGC is adept at capturing global dependencies between nodes. This integration amplifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20960;&#31181;FSL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#25351;&#20986;&#65292;&#19982;&#20840;&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#65292;&#32431;FSL&#27169;&#22411;&#38754;&#23545;&#23545;&#25239;&#25200;&#21160;&#20250;&#24102;&#26469;&#19981;&#21487;&#24573;&#35270;&#30340;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#12290;&#20294;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#25552;&#31034;&#25110;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#32988;&#36807;&#20840;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.11066</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding. (arXiv:2306.11066v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20960;&#31181;FSL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#25351;&#20986;&#65292;&#19982;&#20840;&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#65292;&#32431;FSL&#27169;&#22411;&#38754;&#23545;&#23545;&#25239;&#25200;&#21160;&#20250;&#24102;&#26469;&#19981;&#21487;&#24573;&#35270;&#30340;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#12290;&#20294;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#25552;&#31034;&#25110;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#32988;&#36807;&#20840;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;Few-shot learning&#65292;FSL&#65289;&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;FSL&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#20110;&#23545;&#25239;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20840;&#24494;&#35843;&#27169;&#22411;&#65292;&#32431;FSL&#26041;&#27861;&#38754;&#23545;&#23545;&#25239;&#25200;&#21160;&#20250;&#20986;&#29616;&#26126;&#26174;&#30340;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#25552;&#31034;&#29983;&#25104;&#25110;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;FSL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#29978;&#33267;&#36229;&#36807;&#20840;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#31867;&#22411;&#36873;&#25321;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#19978;&#20063;&#26377;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art few-shot learning (FSL) methods leverage prompt-based fine-tuning to obtain remarkable results for natural language understanding (NLU) tasks. While much of the prior FSL methods focus on improving downstream task performance, there is a limited understanding of the adversarial robustness of such methods. In this work, we conduct an extensive study of several state-of-the-art FSL methods to assess their robustness to adversarial perturbations. To better understand the impact of various factors towards robustness (or the lack of it), we evaluate prompt-based FSL methods against fully fine-tuned models for aspects such as the use of unlabeled data, multiple prompts, number of few-shot examples, model size and type. Our results on six GLUE tasks indicate that compared to fully fine-tuned models, vanilla FSL methods lead to a notable relative drop in task performance (i.e., are less robust) in the face of adversarial perturbations. However, using (i) unlabeled data for pro
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#19968;&#20010;&#26032;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#24182;&#23558;&#20854;&#34701;&#21512;&#21040;&#26082;&#26377;&#20915;&#31574;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10698</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#35760;&#24518;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork. (arXiv:2306.10698v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10698
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#19968;&#20010;&#26032;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#24182;&#23558;&#20854;&#34701;&#21512;&#21040;&#26082;&#26377;&#20915;&#31574;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#21463;&#21040;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38480;&#21046;&#65292;&#20005;&#37325;&#20381;&#36182;&#19982;&#29615;&#22659;&#30340;&#22810;&#27425;&#20132;&#20114;&#25165;&#33021;&#33719;&#24471;&#20934;&#30830;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20284;&#20046;&#20381;&#36182;&#28023;&#39532;&#20307;&#20174;&#36807;&#21435;&#26377;&#20851;&#20219;&#21153;&#30340;&#32463;&#21382;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#25351;&#23548;&#20854;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20026;&#20195;&#29702;&#35774;&#35745;&#31867;&#20284;&#28023;&#39532;&#20307;&#30340;&#27169;&#22359;&#20197;&#23558;&#36807;&#21435;&#30340;&#32463;&#21382;&#34701;&#20837;&#26082;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#28041;&#21450;&#36873;&#25321;&#24403;&#21069;&#20219;&#21153;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#36825;&#20123;&#32463;&#39564;&#19982;&#20915;&#31574;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#26816;&#32034;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms are usually impeded by sampling inefficiency, heavily depending on multiple interactions with the environment to acquire accurate decision-making capabilities. In contrast, humans seem to rely on their hippocampus to retrieve relevant information from past experiences of relevant tasks, which guides their decision-making when learning a new task, rather than exclusively depending on environmental interactions. Nevertheless, designing a hippocampus-like module for an agent to incorporate past experiences into established reinforcement learning algorithms presents two challenges. The first challenge involves selecting the most relevant past experiences for the current task, and the second is integrating such experiences into the decision network. To address these challenges, we propose a novel algorithm that utilizes a retrieval network based on a task-conditioned hypernetwork, which adapts the retrieval network's parameters depending on the task. A
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;MARBLE&#65292;&#19968;&#20010;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#20026;&#38899;&#20048;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.10548</link><description>&lt;p&gt;
MARBLE&#65306;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MARBLE: Music Audio Representation Benchmark for Universal Evaluation. (arXiv:2306.10548v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;MARBLE&#65292;&#19968;&#20010;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#20026;&#38899;&#20048;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33402;&#26415;&#19982;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20043;&#38388;&#20132;&#21449;&#30340;&#24191;&#27867;&#26102;&#20195;&#20013;&#65292;&#20363;&#22914;&#22270;&#20687;&#29983;&#25104;&#21644;&#34394;&#26500;&#20849;&#21019;&#65292;&#38899;&#20048;&#30340;AI&#20173;&#28982;&#30456;&#23545;&#21021;&#27493;&#65292;&#29305;&#21035;&#26159;&#22312;&#38899;&#20048;&#29702;&#35299;&#26041;&#38754;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#35780;&#20272;&#22522;&#20934;MARBLE&#65292;&#26088;&#22312;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#23450;&#20041;&#21253;&#25324;&#22768;&#23398;&#65292;&#28436;&#22863;&#65292;&#20048;&#35889;&#21644;&#39640;&#32423;&#25551;&#36848;&#22312;&#20869;&#30340;&#22235;&#20010;&#23618;&#27425;&#30340;&#32508;&#21512;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;8&#20010;&#20844;&#20849;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;14&#39033;&#20219;&#21153;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21327;&#35758;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#22522;&#20110;&#38899;&#20048;&#24405;&#38899;&#24320;&#21457;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34920;&#24449;&#30340;&#20844;&#24179;&#21644;&#26631;&#20934;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;MARBLE&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#37325;&#29992;&#30340;&#24037;&#20855;&#24211;&#65292;&#20197;&#25903;&#25345;&#31038;&#21306;&#39537;&#21160;&#30340;&#23458;&#35266;&#22522;&#20934;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of extensive intersection between art and Artificial Intelligence (AI), such as image generation and fiction co-creation, AI for music remains relatively nascent, particularly in music understanding. This is evident in the limited work on deep music representations, the scarcity of large-scale datasets, and the absence of a universal and community-driven benchmark. To address this issue, we introduce the Music Audio Representation Benchmark for universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy with four hierarchy levels, including acoustic, performance, score, and high-level description. We then establish a unified protocol based on 14 tasks on 8 public-available datasets, providing a fair and standard assessment of representations of all open-sourced pre-trained models developed on music recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and re
&lt;/p&gt;</description></item><item><title>&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#26159;&#35299;&#20915;GRL&#24403;&#21069;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.10456</link><description>&lt;p&gt;
&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#25512;&#36827;&#29983;&#29289;&#21307;&#23398;&#65306;&#26368;&#26032;&#36827;&#23637;&#65292;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Advancing Biomedicine with Graph Representation Learning: Recent Progress, Challenges, and Future Directions. (arXiv:2306.10456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10456
&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#26159;&#35299;&#20915;GRL&#24403;&#21069;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#22312;&#21253;&#25324;&#29983;&#29289;&#21307;&#23398;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#37117;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#12290;&#26412;&#32508;&#36848;&#30340;&#30446;&#30340;&#26159;&#22238;&#39038;GRL&#26041;&#27861;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;GRL&#24403;&#21069;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning (GRL) has emerged as a pivotal field that has contributed significantly to breakthroughs in various fields, including biomedicine. The objective of this survey is to review the latest advancements in GRL methods and their applications in the biomedical field. We also highlight key challenges currently faced by GRL and outline potential directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;RL&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#35813;&#27169;&#22411;&#30340;&#20856;&#22411;&#21160;&#21147;&#23398;&#20026;&#19968;&#32452;&#38381;&#24335;ODE&#26041;&#31243;&#32452;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#19982;&#31070;&#32463;RL&#20195;&#29702;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#29616;&#23454;&#19990;&#30028;RL&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.10404</link><description>&lt;p&gt;
RL&#24863;&#30693;&#26426;&#65306;&#39640;&#32500;&#31574;&#30053;&#23398;&#20064;&#30340;&#27867;&#21270;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions. (arXiv:2306.10404v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;RL&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#35813;&#27169;&#22411;&#30340;&#20856;&#22411;&#21160;&#21147;&#23398;&#20026;&#19968;&#32452;&#38381;&#24335;ODE&#26041;&#31243;&#32452;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#19982;&#31070;&#32463;RL&#20195;&#29702;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#29616;&#23454;&#19990;&#30028;RL&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#21464;&#38761;&#24615;&#65292;&#20026;&#20102;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20174;&#20687;&#32032;&#25110;&#20854;&#20182;&#39640;&#32500;&#24863;&#23448;&#36755;&#20837;&#20013;&#23398;&#20064;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#37117;&#38598;&#20013;&#20110;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#25110;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#65292;&#20851;&#20110;&#39640;&#32500;&#24773;&#20917;&#19979;&#31574;&#30053;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#22522;&#26412;&#38382;&#39064;&#20173;&#26377;&#24453;&#35299;&#20915;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#30340;&#39640;&#32500;RL&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#23398;&#20064;&#21327;&#35758;&#65292;&#24182;&#23558;&#20854;&#20856;&#22411;&#21160;&#21147;&#23398;&#23548;&#20986;&#20026;&#19968;&#32452;&#38381;&#24335;&#24120;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#26368;&#20339;&#30340;&#23398;&#20064;&#29575;&#21644;&#20219;&#21153;&#38590;&#24230;&#35843;&#24230;&#65292;&#31867;&#20284;&#20110;&#35757;&#32451;&#20013;&#30340;&#36864;&#28779;&#26041;&#26696;&#21644;&#35838;&#31243;&#34920;&#65292;&#24182;&#34920;&#26126;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#20016;&#23500;&#30340;&#34892;&#20026;&#65292;&#21253;&#25324;&#22312;&#31232;&#30095;&#22870;&#21169;&#19979;&#30340;&#24310;&#36831;&#23398;&#20064;&#65307;&#26681;&#25454;&#22870;&#21169;&#22522;&#32447;&#19981;&#21516;&#30340;&#21508;&#31181;&#23398;&#20064;&#26041;&#26696;&#65307;&#20197;&#21450;&#30001;&#22870;&#21169;&#20005;&#26684;&#31243;&#24230;&#39537;&#21160;&#30340;&#36895;&#24230;-&#20934;&#30830;&#24230;&#26435;&#34913;&#12290;&#19982;&#31070;&#32463;RL&#20195;&#29702;&#36827;&#34892;&#30340;&#23454;&#39564;&#27604;&#36739;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#29616;&#23454;&#19990;&#30028;RL&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) algorithms have proven transformative in a range of domains. To tackle real-world domains, these systems often use neural networks to learn policies directly from pixels or other high-dimensional sensory input. By contrast, much theory of RL has focused on discrete state spaces or worst-case analysis, and fundamental questions remain about the dynamics of policy learning in high-dimensional settings. Here, we propose a solvable high-dimensional model of RL that can capture a variety of learning protocols, and derive its typical dynamics as a set of closed-form ordinary differential equations (ODEs). We derive optimal schedules for the learning rates and task difficulty - analogous to annealing schemes and curricula during training in RL - and show that the model exhibits rich behaviour, including delayed learning under sparse rewards; a variety of learning regimes depending on reward baselines; and a speed-accuracy trade-off driven by reward stringency. Expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Samplet&#22352;&#26631;&#19979;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24341;&#20837;l1&#27491;&#21017;&#21270;&#39033;&#21487;&#20197;&#22686;&#21152;&#31995;&#25968;&#30340;&#31232;&#30095;&#24615;&#12290;&#30456;&#27604;&#20110;&#21333;&#23610;&#24230;&#22522;&#65292;Samplet&#22522;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#26356;&#22810;&#31867;&#22411;&#30340;&#20449;&#21495;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;&#38408;&#20540;&#21644;&#21322;&#20809;&#28369;&#29275;&#39039;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10180</link><description>&lt;p&gt;
&#22522;&#20110;Samplet&#22522; Pursuit &#30340;&#26680;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Samplet basis pursuit. (arXiv:2306.10180v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Samplet&#22352;&#26631;&#19979;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24341;&#20837;l1&#27491;&#21017;&#21270;&#39033;&#21487;&#20197;&#22686;&#21152;&#31995;&#25968;&#30340;&#31232;&#30095;&#24615;&#12290;&#30456;&#27604;&#20110;&#21333;&#23610;&#24230;&#22522;&#65292;Samplet&#22522;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#26356;&#22810;&#31867;&#22411;&#30340;&#20449;&#21495;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;&#38408;&#20540;&#21644;&#21322;&#20809;&#28369;&#29275;&#39039;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22522;&#20110;l1&#27491;&#21017;&#21270;&#30340;Samplet&#22352;&#26631;&#19979;&#30340;&#26680;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;Samplet&#22522;&#30340;&#31995;&#25968;&#19978;&#65292;&#24212;&#29992;l1&#27491;&#21017;&#21270;&#39033;&#21487;&#20197;&#24378;&#21046;&#22686;&#21152;&#31232;&#30095;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;Samplet&#22522; Pursuit&#12290;Samplet&#22522;&#26159;&#27874;&#24418;&#31867;&#22411;&#30340;&#26377;&#31526;&#21495;&#27979;&#24230;&#65292;&#19987;&#38376;&#29992;&#20110;&#25955;&#20081;&#25968;&#25454;&#12290;&#23427;&#20204;&#20855;&#26377;&#19982;&#23567;&#27874;&#30456;&#20284;&#30340;&#26412;&#22320;&#21270;&#12289;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#21644;&#25968;&#25454;&#21387;&#32553;&#24615;&#36136;&#12290;&#21487;&#20197;&#22312;Samplet&#22522;&#19978;&#31232;&#30095;&#22320;&#34920;&#31034;&#30340;&#20449;&#21495;&#31867;&#27604;&#21333;&#23610;&#24230;&#22522;&#19978;&#33021;&#22815;&#34920;&#31034;&#31232;&#30095;&#30340;&#20449;&#21495;&#31867;&#21035;&#35201;&#22823;&#24471;&#22810;&#12290;&#29305;&#21035;&#22320;&#65292;&#20165;&#29992;&#22522;&#20989;&#25968;&#26144;&#23556;&#30340;&#20960;&#20010;&#29305;&#24449;&#21472;&#21152;&#21363;&#21487;&#34920;&#31034;&#30340;&#25152;&#26377;&#20449;&#21495;&#20063;&#21487;&#20197;&#22312;Samplet&#22352;&#26631;&#19979;&#23454;&#29616;&#31232;&#30095;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23558;&#36719;&#38408;&#20540;&#21644;&#21322;&#20809;&#28369;&#29275;&#39039;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#19982;&#24555;&#36895;&#36845;&#20195;&#25910;&#32553;&#38408;&#20540;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#31232;&#30095;&#24615;&#21644;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider kernel-based learning in samplet coordinates with l1-regularization. The application of an l1-regularization term enforces sparsity of the coefficients with respect to the samplet basis. Therefore, we call this approach samplet basis pursuit. Samplets are wavelet-type signed measures, which are tailored to scattered data. They provide similar properties as wavelets in terms of localization, multiresolution analysis, and data compression. The class of signals that can sparsely be represented in a samplet basis is considerably larger than the class of signals which exhibit a sparse representation in the single-scale basis. In particular, every signal that can be represented by the superposition of only a few features of the canonical feature map is also sparse in samplet coordinates. We propose the efficient solution of the problem under consideration by combining soft-shrinkage with the semi-smooth Newton method and compare the approach to the fast iterative shrinkage thresh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21152;&#36895;&#30697;&#38453;&#23545;&#35282;&#21270;&#65292;&#23558;&#36873;&#25321;&#26368;&#24555;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#36807;&#31243;&#35270;&#20026;&#26827;&#30424;&#28216;&#25103;&#65292;&#20026;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.10075</link><description>&lt;p&gt;
&#23558;&#30697;&#38453;&#23545;&#35282;&#21270;&#20316;&#20026;&#19968;&#31181;&#26827;&#30424;&#28216;&#25103;: &#25945;&#25480;&#26412;&#24449;&#20540;&#27714;&#35299;&#22120;&#26368;&#24555;&#30340;&#35299;&#20915;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Matrix Diagonalization as a Board Game: Teaching an Eigensolver the Fastest Path to Solution. (arXiv:2306.10075v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21152;&#36895;&#30697;&#38453;&#23545;&#35282;&#21270;&#65292;&#23558;&#36873;&#25321;&#26368;&#24555;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#36807;&#31243;&#35270;&#20026;&#26827;&#30424;&#28216;&#25103;&#65292;&#20026;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#23545;&#35282;&#21270;&#26159;&#31185;&#23398;&#35745;&#31639;&#20013;&#35768;&#22810;&#39046;&#22495;&#30340;&#22522;&#30784;&#12290;&#23545;&#35282;&#21270;&#30697;&#38453;&#20197;&#35299;&#20915;&#29305;&#24449;&#20540;&#38382;&#39064;&#38656;&#35201;&#19968;&#31995;&#21015;&#36845;&#20195;&#65292;&#26368;&#32456;&#36798;&#21040;&#25152;&#26377;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#20805;&#20998;&#25910;&#25947;&#21644;&#31934;&#30830;&#35299;&#20915;&#12290;&#36825;&#36890;&#24120;&#20250;&#23548;&#33268;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992; AlphaZero &#26694;&#26550;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#23558;&#36873;&#25321;&#26368;&#24555;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#36807;&#31243;&#35270;&#20026;&#26827;&#30424;&#28216;&#25103;&#65292;&#20197;&#21152;&#36895; Jacobi &#23545;&#35282;&#21270;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#23558; Jacobi &#23545;&#35282;&#21270;&#31639;&#27861;&#24212;&#29992;&#20110;&#20986;&#29616;&#22312;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#20013;&#30340;&#23545;&#31216;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#24120;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#21152;&#36895;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#26174;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#25913;&#36827;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#24615;&#33021;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix diagonalization is at the cornerstone of numerous fields of scientific computing. Diagonalizing a matrix to solve an eigenvalue problem requires a sequential path of iterations that eventually reaches a sufficiently converged and accurate solution for all the eigenvalues and eigenvectors. This typically translates into a high computational cost. Here we demonstrate how reinforcement learning, using the AlphaZero framework, can accelerate Jacobi matrix diagonalizations by viewing the selection of the fastest path to solution as a board game. To demonstrate the viability of our approach we apply the Jacobi diagonalization algorithm to symmetric Hamiltonian matrices that appear in quantum chemistry calculations. We find that a significant acceleration can often be achieved. Our findings highlight the opportunity to use machine learning as a promising tool to improve the performance of numerical linear algebra.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.10045</link><description>&lt;p&gt;
&#39044;&#27979;&#26230;&#20307;&#24615;&#36136;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#39640;&#25928;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26230;&#20307;&#26448;&#26009;&#30340;&#24615;&#36136;&#39044;&#27979;&#12290;&#26230;&#20307;&#32467;&#26500;&#30001;&#19968;&#20010;&#26368;&#23567;&#30340;&#21333;&#20803;&#26684;&#32452;&#25104;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26080;&#38480;&#37325;&#22797;&#12290;&#22914;&#20309;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20934;&#30830;&#34920;&#31034;&#36825;&#31181;&#37325;&#22797;&#32467;&#26500;&#20173;&#28982;&#27809;&#26377;&#35299;&#20915;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21482;&#22312;&#38468;&#36817;&#30340;&#33410;&#28857;&#20043;&#38388;&#24314;&#31435;&#36793;&#32536;&#26469;&#26500;&#24314;&#22270;&#24418;&#65292;&#22240;&#27492;&#26080;&#27861;&#24544;&#23454;&#22320;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#30340;&#27169;&#24335;&#21644;&#36828;&#36317;&#31163;&#30340;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#21019;&#26032;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#30452;&#25509;&#24314;&#27169;&#29289;&#29702;&#21407;&#29702;&#30340;&#30456;&#20114;&#20316;&#29992;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#20351;&#29992;&#36317;&#31163;&#65292;&#22914;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#25152;&#20570;&#30340;&#12290;&#36825;&#20123;&#21183;&#21253;&#25324;&#24211;&#20177;&#21183;&#65292;&#20262;&#25958;&#20998;&#25955;&#21183;&#21644;Pauli&#26021;&#21147;&#21183;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#27169;&#25152;&#26377;&#21407;&#23376;&#20043;&#38388;&#30340;&#23436;&#25972;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38468;&#36817;&#21407;&#23376;&#20043;&#38388;&#30340;&#21183;&#12290;&#36825;&#24471;&#30410;&#20110;&#25105;&#20204;&#29992;&#21487;&#35777;&#26126;&#30340;&#35823;&#24046;&#30028;&#36924;&#36817;&#26080;&#38480;&#21183;&#21644;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
&lt;/p&gt;</description></item><item><title>&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2306.09633</link><description>&lt;p&gt;
&#34394;&#20551;&#40654;&#26126;&#65306;&#37325;&#26032;&#35780;&#20272;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09633
&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#26377;&#20851;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#33455;&#29255;&#30340;&#35770;&#25991;&#65292;&#22240;&#20026;&#25152;&#22768;&#31216;&#30340;&#32467;&#26524;&#32570;&#20047;&#20805;&#20998;&#30340;&#25991;&#20214;&#35760;&#24405;&#21644;&#20851;&#38190;&#27493;&#39588;&#30340;&#35828;&#26126;&#65292;&#24341;&#21457;&#20105;&#35758;&#24182;&#21463;&#21040;&#23186;&#20307;&#30340;&#25209;&#35780;&#25253;&#36947;&#12290; &#32780;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#22635;&#34917;&#20102;&#31354;&#30333;&#65292;&#35777;&#26126;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#33853;&#21518;&#20110;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#33853;&#21518;&#20110;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#36824;&#33853;&#21518;&#20110;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#12290;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#34892;&#20026;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#12298;&#33258;&#28982;&#12299;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#35774;&#22791;&#20043;&#38388;&#36890;&#36807;&#21327;&#20316;&#23436;&#25104;&#20998;&#25955;&#20219;&#21153;&#12290;&#36890;&#36807;&#22270;&#27169;&#22411;&#20808;&#39564;&#29983;&#25104;&#30340;&#21327;&#20316;&#22270;&#65292;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#25429;&#25417;&#35774;&#22791;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09595</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#22270;&#27169;&#22411;&#20808;&#39564;&#19979;&#30340;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structured Cooperative Learning with Graphical Model Priors. (arXiv:2306.09595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#35774;&#22791;&#20043;&#38388;&#36890;&#36807;&#21327;&#20316;&#23436;&#25104;&#20998;&#25955;&#20219;&#21153;&#12290;&#36890;&#36807;&#22270;&#27169;&#22411;&#20808;&#39564;&#29983;&#25104;&#30340;&#21327;&#20316;&#22270;&#65292;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#25429;&#25417;&#35774;&#22791;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#20998;&#25955;&#35774;&#22791;&#19978;&#23545;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#20010;&#24615;&#21270;&#24314;&#27169;&#65292;&#36825;&#20123;&#35774;&#22791;&#30340;&#23616;&#37096;&#25968;&#25454;&#21463;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#32467;&#26500;&#21270;&#21327;&#20316;&#23398;&#20064;&#65288;SCooL&#65289;&#8221;&#65292;&#20854;&#20013;&#19968;&#20010;&#36328;&#35774;&#22791;&#30340;&#21327;&#20316;&#22270;&#30001;&#22270;&#27169;&#22411;&#20808;&#39564;&#29983;&#25104;&#65292;&#20197;&#33258;&#21160;&#21327;&#35843;&#35774;&#22791;&#20043;&#38388;&#30340;&#30456;&#20114;&#23398;&#20064;&#12290;&#36890;&#36807;&#36873;&#25321;&#26045;&#21152;&#19981;&#21516;&#32467;&#26500;&#30340;&#22270;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#25512;&#23548;&#20986;&#19968;&#31867;&#20016;&#23500;&#30340;&#29616;&#26377;&#21644;&#26032;&#22411;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#31181; SCooL &#30340;&#31034;&#20363;&#65292;&#22312;&#20854;&#20013;&#20197; Dirac &#20998;&#24067;&#12289;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#21644;&#27880;&#24847;&#21147;&#20316;&#20026;&#29983;&#25104;&#21327;&#20316;&#22270;&#30340;&#20808;&#39564;&#12290;&#36825;&#20123; EM &#31867;&#22411;&#30340;&#31639;&#27861;&#36890;&#36807;&#26356;&#26032;&#21327;&#20316;&#22270;&#21644;&#21327;&#21516;&#26412;&#22320;&#27169;&#22411;&#23398;&#20064;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#65292;&#20165;&#36890;&#36807;&#30417;&#35270;&#27169;&#22411;&#26356;&#26032;&#26469;&#20248;&#21270;&#21327;&#20316;&#22270;&#65292;&#20174;&#32780;&#21487;&#20197;&#33258;&#21160;&#25429;&#25417;&#35774;&#22791;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to train personalized models for different tasks on decentralized devices with limited local data. We propose "Structured Cooperative Learning (SCooL)", in which a cooperation graph across devices is generated by a graphical model prior to automatically coordinate mutual learning between devices. By choosing graphical models enforcing different structures, we can derive a rich class of existing and novel decentralized learning algorithms via variational inference. In particular, we show three instantiations of SCooL that adopt Dirac distribution, stochastic block model (SBM), and attention as the prior generating cooperation graphs. These EM-type algorithms alternate between updating the cooperation graph and cooperative learning of local models. They can automatically capture the cross-task correlations among devices by only monitoring their model updating in order to optimize the cooperation graph. We evaluate SCooL and compare it with existing decentralized learning met
&lt;/p&gt;</description></item><item><title>FedMultimodal &#26159;&#38754;&#21521;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35206;&#30422;&#20116;&#31181;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#21644;&#21313;&#20010;&#31038;&#21306;&#30340;FL&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20415;&#20419;&#36827;&#22810;&#27169;&#24577;FL&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.09486</link><description>&lt;p&gt;
FedMultimodal&#65306;&#38754;&#21521;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedMultimodal: A Benchmark For Multimodal Federated Learning. (arXiv:2306.09486v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09486
&lt;/p&gt;
&lt;p&gt;
FedMultimodal &#26159;&#38754;&#21521;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35206;&#30422;&#20116;&#31181;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#21644;&#21313;&#20010;&#31038;&#21306;&#30340;FL&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20415;&#20419;&#36827;&#22810;&#27169;&#24577;FL&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24212;&#23545;&#25968;&#25454;&#38544;&#31169;&#25361;&#25112;&#30340;&#26032;&#20852;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#22312;FL&#31639;&#27861;&#20013;&#65292;&#23458;&#25143;&#31471;&#25552;&#20132;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#26381;&#21153;&#22120;&#23558;&#36825;&#20123;&#21442;&#25968;&#36827;&#34892;&#32858;&#21512;&#30452;&#33267;&#25910;&#25947;&#12290;&#23613;&#31649;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#65292;&#23545;FL&#36827;&#34892;&#20102;&#37325;&#22823;&#21162;&#21147;&#65292;&#20294;&#26159;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#27969;&#30340;FL&#24212;&#29992;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#22312;&#24773;&#24863;&#35782;&#21035;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#22810;&#23186;&#20307;&#21644;&#31038;&#20132;&#23186;&#20307;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#29616;&#23454;&#24212;&#29992;&#65292;&#32780;&#29992;&#25143;&#38544;&#31169;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#38024;&#23545;&#22810;&#27169;&#24577;&#24212;&#29992;&#25110;&#30456;&#20851;&#20219;&#21153;&#30340;&#29616;&#26377;FL&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#20419;&#36827;&#22810;&#27169;&#24577;FL&#30740;&#31350;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FedMultimodal&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35206;&#30422;&#21313;&#20010;&#31038;&#21306;&#20013;&#30340;&#20116;&#20010;&#20195;&#34920;&#24615;&#22810;&#27169;&#24577;&#24212;&#29992;&#30340;FL&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, Federated Learning (FL) has become an emerging machine learning technique to tackle data privacy challenges through collaborative training. In the Federated Learning algorithm, the clients submit a locally trained model, and the server aggregates these parameters until convergence. Despite significant efforts that have been made to FL in fields like computer vision, audio, and natural language processing, the FL applications utilizing multimodal data streams remain largely unexplored. It is known that multimodal learning has broad real-world applications in emotion recognition, healthcare, multimedia, and social media, while user privacy persists as a critical concern. Specifically, there are no existing FL benchmarks targeting multimodal applications or related tasks. In order to facilitate the research in multimodal FL, we introduce FedMultimodal, the first FL benchmark for multimodal learning covering five representative multimodal applications from ten comm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08013</link><description>&lt;p&gt;
TopP\&amp;R: &#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#25903;&#25345;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TopP\&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;Inception Score&#65288;IS&#65289;&#65292;Fr\'echet Inception Distance&#65288;FID&#65289;&#20197;&#21450;Precision and Recall&#65288;P\&amp;R&#65289;&#30340;&#21464;&#20307;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#26679;&#26412;&#29305;&#24449;&#20272;&#35745;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35780;&#20272;&#30340;&#36136;&#37327;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21487;&#38752;&#24615;&#65292;&#20294;&#20854;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#32899;&#30340;&#35752;&#35770;&#65288;&#24182;&#34987;&#24573;&#35270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65288;TopP\&amp;R&#65292;&#21457;&#38899;&#20026;&#8220;topper&#8221;&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25903;&#25345;&#65292;&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#12290;&#36825;&#19981;&#20165;&#20351;TopP\&amp;R&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TopP\&amp;R&#23545;&#20110;&#31163;&#32676;&#20540;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&amp;R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&amp;R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&amp;R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&amp;R is robust to outliers and non-independent and identically distributed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;</title><link>http://arxiv.org/abs/2306.04719</link><description>&lt;p&gt;
&#19981;&#35201;&#30456;&#20449;&#20320;&#30340;&#30524;&#30555;&#65306;&#20851;&#20110;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#65288;&#19981;&#65289;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#65311;&#29305;&#24449;&#21487;&#35270;&#21270;&#36890;&#36807;&#20248;&#21270;&#26469;&#21487;&#35270;&#21270;&#39640;&#28608;&#27963;&#30340;&#27169;&#24335;&#65292;&#35797;&#22270;&#22238;&#31572;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22914;&#20170;&#65292;&#21487;&#35270;&#21270;&#26041;&#27861;&#26500;&#25104;&#20102;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#30340;&#20102;&#35299;&#30340;&#22522;&#30784;&#65292;&#20316;&#20026;&#19968;&#31181;&#26426;&#26800;&#24335;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#38382;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#26377;&#22810;&#21487;&#38752;&#65311;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#32593;&#32476;&#30005;&#36335;&#26469;&#35784;&#39575;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20351;&#20854;&#26174;&#31034;&#23436;&#20840;&#19982;&#33258;&#28982;&#36755;&#20837;&#30340;&#27491;&#24120;&#32593;&#32476;&#34892;&#20026;&#27627;&#26080;&#32852;&#31995;&#30340;&#20219;&#24847;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#22312;&#26631;&#20934;&#65292;&#26410;&#25805;&#32437;&#32593;&#32476;&#20013;&#21457;&#29983;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#19982;&#26631;&#20934;&#36755;&#20837;&#22788;&#29702;&#38750;&#24120;&#19981;&#21516;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#25903;&#25745;&#36825;&#19968;&#32463;&#39564;&#21457;&#29616;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#21487;&#35270;&#21270;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#26497;&#20854;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
&lt;/p&gt;</description></item><item><title>SGEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#21644;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#20027;&#27969;ASR&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#36716;&#21464;&#26102;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01981</link><description>&lt;p&gt;
SGEM&#65306;&#36890;&#36807;&#24207;&#21015;&#32423;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#23454;&#29616;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization. (arXiv:2306.01981v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01981
&lt;/p&gt;
&lt;p&gt;
SGEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#21644;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#20027;&#27969;ASR&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#36716;&#21464;&#26102;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#32463;&#24120;&#26292;&#38706;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#29616;&#26377;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#20197;&#36866;&#24212;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#23454;&#20363;&#12290;&#23613;&#31649;&#26377;&#20102;&#19981;&#38169;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20165;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#36138;&#24515;&#35299;&#30721;&#65292;&#24182;&#22312;&#24103;&#32423;&#21035;&#19978;&#36328;&#36234;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#35843;&#25972;&#65292;&#36825;&#22312;&#27169;&#22411;&#36755;&#20986;&#30340;&#24207;&#21015;&#24615;&#36136;&#19979;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TTA&#26694;&#26550;&#65292;&#31216;&#20026;SGEM&#65292;&#29992;&#20110;&#19968;&#33324;ASR&#27169;&#22411;&#12290;&#20026;&#20102;&#22788;&#29702;&#24207;&#21015;&#36755;&#20986;&#65292;SGEM&#39318;&#20808;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#26469;&#25506;&#32034;&#20505;&#36873;&#36755;&#20986;&#26631;&#24535;&#65292;&#24182;&#36873;&#25321;&#26368;&#21487;&#20449;&#30340;&#26631;&#24535;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#21644;&#36127;&#25277;&#26679;&#20316;&#20026;&#26080;&#30417;&#30563;&#30446;&#26631;&#26469;&#36866;&#24212;&#27169;&#22411;&#12290;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#36716;&#21464;&#19979;&#65292;SGEM&#23454;&#29616;&#20102;&#19977;&#31181;&#20027;&#27969;ASR&#27169;&#22411;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) models are frequently exposed to data distribution shifts in many real-world scenarios, leading to erroneous predictions. To tackle this issue, an existing test-time adaptation (TTA) method has recently been proposed to adapt the pre-trained ASR model on unlabeled test instances without source data. Despite decent performance gain, this work relies solely on naive greedy decoding and performs adaptation across timesteps at a frame level, which may not be optimal given the sequential nature of the model output. Motivated by this, we propose a novel TTA framework, dubbed SGEM, for general ASR models. To treat the sequential output, SGEM first exploits beam search to explore candidate output logits and selects the most plausible one. Then, it utilizes generalized entropy minimization and negative sampling as unsupervised objectives to adapt the model. SGEM achieves state-of-the-art performance for three mainstream ASR models under various domain shifts.
&lt;/p&gt;</description></item><item><title>MutateNN&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#65292;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#19988;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#31181;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.01697</link><description>&lt;p&gt;
MutateNN&#65306;&#29992;&#20110;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#31361;&#21464;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MutateNN: Mutation Testing of Image Recognition Models Deployed on Hardware Accelerators. (arXiv:2306.01697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01697
&lt;/p&gt;
&lt;p&gt;
MutateNN&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#65292;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#19988;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#31181;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#24182;&#25512;&#21160;&#25216;&#26415;&#21457;&#23637;&#30340;&#26032;&#26426;&#36935;&#24212;&#36816;&#32780;&#29983;&#12290;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#29305;&#21035;&#26159;&#34987;&#20998;&#37197;&#20102;&#24863;&#30693;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25361;&#25112;&#24182;&#23548;&#33268;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36825;&#31867;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#36164;&#28304;&#38656;&#27714;&#20063;&#26377;&#25152;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27169;&#22411;&#20248;&#21270;&#21644;&#30828;&#20214;&#21152;&#36895;&#24050;&#25104;&#20026;&#20851;&#38190;&#25216;&#26415;&#65292;&#20294;&#26377;&#25928;&#25972;&#21512;&#36825;&#20123;&#27010;&#24565;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35753;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#25506;&#32034;&#22312;&#19981;&#21516;&#30828;&#20214;&#21152;&#36895;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MutateNN&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#27492;&#30446;&#30340;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#24037;&#20855;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#21151;&#33021;&#65292;&#25105;&#20204;&#23545;7&#20010;&#24191;&#20026;&#20154;&#30693;&#30340;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#20102;21&#31181;&#21464;&#24322;&#12290;&#25105;&#20204;&#22312;4&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#20102;&#25105;&#20204;&#30340;&#21464;&#24322;&#20307;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#34892;&#20026;&#65292;&#24182;&#35780;&#20272;&#20102;MutateNN&#22312;&#26816;&#27979;&#20986;&#19981;&#27491;&#30830;&#25110;&#19981;&#31934;&#30830;&#30340;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the research advancement of Artificial Intelligence in the last years, there are new opportunities to mitigate real-world problems and advance technologically. Image recognition models in particular, are assigned with perception tasks to mitigate complex real-world challenges and lead to new solutions. Furthermore, the computational complexity and demand for resources of such models has also increased. To mitigate this, model optimization and hardware acceleration has come into play, but effectively integrating such concepts is a challenging and error-prone process.  In order to allow developers and researchers to explore the robustness of deep learning image recognition models deployed on different hardware acceleration devices, we propose MutateNN, a tool that provides mutation testing and analysis capabilities for that purpose. To showcase its capabilities, we utilized 21 mutations for 7 widely-known pre-trained deep neural network models. We deployed our mutants on 4 different
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#36827;&#34892;&#24555;&#36895;&#38081;&#25176;&#31561;&#31163;&#23376;&#20307;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23454;&#26102;&#24212;&#29992;&#25110;&#35814;&#23613;&#30340;&#21442;&#25968;&#25195;&#25551;&#20013;&#36895;&#24230;&#22826;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18944</link><description>&lt;p&gt;
&#31070;&#32463;PDE&#20195;&#29702;&#30340;&#24555;&#36895;&#21160;&#24577;1D&#21187;&#28982;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Fast Dynamic 1D Simulation of Divertor Plasmas with Neural PDE Surrogates. (arXiv:2305.18944v2 [physics.plasm-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#36827;&#34892;&#24555;&#36895;&#38081;&#25176;&#31561;&#31163;&#23376;&#20307;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23454;&#26102;&#24212;&#29992;&#25110;&#35814;&#23613;&#30340;&#21442;&#25968;&#25195;&#25551;&#20013;&#36895;&#24230;&#22826;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31649;&#29702;&#38081;&#25176;&#24335;&#26680;&#32858;&#21464;&#35774;&#22791;&#20013;&#30340;&#38081;&#25176;&#31561;&#31163;&#23376;&#20307;&#23545;&#20110;&#24212;&#23545;&#20854;&#28909;&#37327;&#21644;&#31890;&#23376;&#36890;&#37327;&#38480;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#25311;&#26159;&#29702;&#35299;&#21644;&#25511;&#21046;&#36825;&#20123;&#31561;&#31163;&#23376;&#20307;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#28982;&#32780;&#65292;&#23545;&#20110;&#23454;&#26102;&#24212;&#29992;&#25110;&#35814;&#23613;&#30340;&#21442;&#25968;&#25195;&#25551;&#65292;&#30446;&#21069;&#21482;&#26377;&#31616;&#21333;&#30340;&#36817;&#20284;&#26041;&#27861;&#36895;&#24230;&#36275;&#22815;&#24555;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;PDE&#20195;&#29702;&#26469;&#35299;&#20915;&#36825;&#31181;&#24555;&#36895;&#27169;&#25311;&#22120;&#30340;&#32570;&#20047;&#65292;&#21363;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20351;&#29992;&#32463;&#20856;&#25968;&#20540;&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#20195;&#29702;&#27169;&#22411;&#36817;&#20284;&#28436;&#21270;&#21442;&#32771;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#23436;&#25972;&#31354;&#38388;&#35299;&#20915;&#26041;&#26696;&#30340;&#26102;&#38388;&#27493;&#36827;&#31639;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;DIV1D&#20316;&#20026;&#21442;&#32771;&#27169;&#22411;&#26469;&#29983;&#25104;&#25968;&#25454;&#65292;&#21363;&#20174;X&#28857;&#65288;&#19978;&#28216;&#65289;&#21040;&#30446;&#26631;&#22788;&#30340;1D&#28909;&#27969;&#31649;&#30340;DIV1D&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;TCV&#38081;&#25176;&#31561;&#31163;&#23376;&#20307;&#65292;&#20854;&#21160;&#24577;&#26159;&#30001;&#19978;&#28216;&#23494;&#24230;&#22350;&#21475;&#24341;&#36215;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#24555;&#36895;&#26242;&#24577;&#30340;&#25506;&#32034;&#24615;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Managing divertor plasmas is crucial for operating reactor scale tokamak devices due to heat and particle flux constraints on the divertor target. Simulation is an important tool to understand and control these plasmas, however, for real-time applications or exhaustive parameter scans only simple approximations are currently fast enough. We address this lack of fast simulators using neural PDE surrogates, data-driven neural network-based surrogate models trained using solutions generated with a classical numerical method. The surrogate approximates a time-stepping operator that evolves the full spatial solution of a reference physics-based model over time. We use DIV1D, a 1D dynamic model of the divertor plasma, as reference model to generate data. DIV1D's domain covers a 1D heat flux tube from the X-point (upstream) to the target. We simulate a realistic TCV divertor plasma with dynamics induced by upstream density ramps and provide an exploratory outlook towards fast transients. Stat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#22686;&#24378;&#31639;&#27861;&#65292;&#20197;&#36866;&#24403;&#25913;&#21464;&#26032;&#22686;&#36755;&#20837;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#35823;&#23548;&#24615;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16863</link><description>&lt;p&gt;
&#25511;&#21046;&#23398;&#20064;&#25928;&#24212;&#20197;&#38477;&#20302;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#30340;&#35823;&#23548;&#24615;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Controlling Learned Effects to Reduce Spurious Correlations in Text Classifiers. (arXiv:2305.16863v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16863
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#22686;&#24378;&#31639;&#27861;&#65292;&#20197;&#36866;&#24403;&#25913;&#21464;&#26032;&#22686;&#36755;&#20837;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#35823;&#23548;&#24615;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;NLP&#20998;&#31867;&#22120;&#23398;&#20064;&#35757;&#32451;&#29305;&#24449;&#21644;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#35823;&#23548;&#24615;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#27169;&#22411;&#30340;&#39044;&#27979;&#23545;&#36825;&#20123;&#29305;&#24449;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#29305;&#24449;&#23545;&#30446;&#26631;&#26631;&#31614;&#26377;&#38750;&#38646;&#22240;&#26524;&#25928;&#24212;&#24182;&#19988;&#23545;&#39044;&#27979;&#24456;&#37325;&#35201;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#20135;&#29983;&#21453;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#22240;&#26524;&#25512;&#26029;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#25928;&#24212;&#35268;&#33539;&#21270;&#20026;&#29305;&#24449;&#23545;&#26631;&#31614;&#30340;&#20272;&#35745;&#25928;&#24212;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#29305;&#24449;&#30340;&#20272;&#35745;&#25928;&#24212;&#36866;&#24403;&#22320;&#25913;&#21464;&#26032;&#22686;&#30340;&#36755;&#20837;&#30340;&#26631;&#31614;&#12290;&#22312;&#27602;&#24615;&#21644;IMDB&#35780;&#35770;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#26368;&#23567;&#21270;&#20102;&#35823;&#23548;&#24615;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#23569;&#25968;&#32676;&#20307;&#65288;&#21363;&#25171;&#30772;&#35823;&#23548;&#24615;&#30456;&#20851;&#24615;&#30340;&#26679;&#26412;&#65289;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#24635;&#20307;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the problem of NLP classifiers learning spurious correlations between training features and target labels, a common approach is to make the model's predictions invariant to these features. However, this can be counter-productive when the features have a non-zero causal effect on the target label and thus are important for prediction. Therefore, using methods from the causal inference literature, we propose an algorithm to regularize the learnt effect of the features on the model's prediction to the estimated effect of feature on label. This results in an automated augmentation method that leverages the estimated effect of a feature to appropriately change the labels for new augmented inputs. On toxicity and IMDB review datasets, the proposed algorithm minimises spurious correlations and improves the minority group (i.e., samples breaking spurious correlations) accuracy, while also improving the total accuracy compared to standard training.
&lt;/p&gt;</description></item><item><title>GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13681</link><description>&lt;p&gt;
GUARD: &#19968;&#20010;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
GUARD: A Safe Reinforcement Learning Benchmark. (arXiv:2305.13681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13681
&lt;/p&gt;
&lt;p&gt;
GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35797;&#38169;&#30340;&#24615;&#36136;&#65292;&#23558;RL&#31639;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#29616;&#23454;&#24212;&#29992;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#20154;&#26426;&#20132;&#20114;&#12289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31561;&#65289;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#38169;&#35823;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26368;&#36817;&#65292;&#23433;&#20840;RL&#65288;&#21363;&#32422;&#26463;RL&#65289;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#36805;&#36895;&#20986;&#29616;&#65292;&#20854;&#20013;&#20195;&#29702;&#22312;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#21516;&#26102;&#65292;&#25506;&#32034;&#29615;&#22659;&#12290;&#30001;&#20110;&#31639;&#27861;&#21644;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#27604;&#36739;&#29616;&#26377;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GUARD&#65292;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;GUARD&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#20855;&#26377;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#12290;&#20854;&#27425;&#65292;GUARD&#20840;&#38754;&#28085;&#30422;&#20102;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#33258;&#21253;&#21547;&#30340;&#23454;&#29616;&#12290;&#31532;&#19977;&#65292;GUARD&#22312;&#20219;&#21153;&#21644;&#31639;&#27861;&#26041;&#38754;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29366;&#24577;&#19979;&#29616;&#26377;&#26041;&#27861;&#22312;GUARD&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#26032;&#22522;&#20110;&#25511;&#21046;&#27969;&#22270;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20174;CFG&#20013;&#25552;&#21462;&#12289;&#34920;&#31034;&#12289;&#20998;&#31867;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08993</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25511;&#21046;&#27969;&#22270;&#24694;&#24847;&#36719;&#20214;&#20998;&#26512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Survey of Malware Analysis through Control Flow Graph using Machine Learning. (arXiv:2305.08993v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#26032;&#22522;&#20110;&#25511;&#21046;&#27969;&#22270;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20174;CFG&#20013;&#25552;&#21462;&#12289;&#34920;&#31034;&#12289;&#20998;&#31867;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#36719;&#20214;&#23545;&#35745;&#31639;&#26426;&#31995;&#32479;&#21644;&#32593;&#32476;&#30340;&#23433;&#20840;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#65292;&#38656;&#35201;&#20808;&#36827;&#30340;&#25216;&#26415;&#23545;&#20854;&#36827;&#34892;&#34892;&#20026;&#21644;&#21151;&#33021;&#20998;&#26512;&#20197;&#36827;&#34892;&#26816;&#27979;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#31614;&#21517;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#24694;&#24847;&#36719;&#20214;&#30340;&#24555;&#36895;&#28436;&#21270;&#24050;&#32463;&#22833;&#25928;&#12290;&#20854;&#20013;&#26368;&#26377;&#24076;&#26395;&#20811;&#26381;&#22522;&#20110;&#31614;&#21517;&#26816;&#27979;&#26041;&#27861;&#23616;&#38480;&#30340;&#25216;&#26415;&#20043;&#19968;&#26159;&#20351;&#29992;&#25511;&#21046;&#27969;&#22270;&#65288;CFG&#65289;&#12290;CFG&#21033;&#29992;&#31243;&#24207;&#30340;&#32467;&#26500;&#20449;&#24687;&#23558;&#21487;&#25191;&#34892;&#36335;&#24452;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#25351;&#20196;&#65292;&#36793;&#34920;&#31034;&#25511;&#21046;&#27969;&#20381;&#36182;&#20851;&#31995;&#12290;&#30446;&#21069;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20174;CFG&#20013;&#25552;&#21462;&#36825;&#20123;&#29305;&#24449;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#24694;&#24847;&#25110;&#33391;&#24615;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22238;&#39038;&#19968;&#20123;&#22522;&#20110;CFG&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20174;CFG&#20013;&#25552;&#21462;&#65292;&#34920;&#31034;&#21644;&#20998;&#31867;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malware is a significant threat to the security of computer systems and networks which requires sophisticated techniques to analyze the behavior and functionality for detection. Traditional signature-based malware detection methods have become ineffective in detecting new and unknown malware due to their rapid evolution. One of the most promising techniques that can overcome the limitations of signature-based detection is to use control flow graphs (CFGs). CFGs leverage the structural information of a program to represent the possible paths of execution as a graph, where nodes represent instructions and edges represent control flow dependencies. Machine learning (ML) algorithms are being used to extract these features from CFGs and classify them as malicious or benign. In this survey, we aim to review some state-of-the-art methods for malware detection through CFGs using ML, focusing on the different ways of extracting, representing, and classifying. Specifically, we present a comprehe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;NNCI&#65292;&#29992;&#20110;&#23558;&#26368;&#36817;&#37051;&#23621;&#20449;&#24687;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06789</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#38598;&#25104;&#26368;&#36817;&#37051;&#23621;&#20197;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Integrating nearest neighbors on neural network models for treatment effect estimation. (arXiv:2305.06789v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;NNCI&#65292;&#29992;&#20110;&#23558;&#26368;&#36817;&#37051;&#23621;&#20449;&#24687;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#23545;&#20110;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#19994;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#26469;&#35828;&#20855;&#26377;&#39640;&#24230;&#37325;&#35201;&#24615;&#12290;&#35266;&#23519;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#20351;&#23427;&#20204;&#36234;&#26469;&#36234;&#21463;&#21040;&#30740;&#31350;&#20154;&#21592;&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#23384;&#22312;&#20559;&#24046;&#21644;&#20854;&#20182;&#24369;&#28857;&#65292;&#23548;&#33268;&#22914;&#26524;&#19981;&#27491;&#30830;&#22788;&#29702;&#65292;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#20250;&#19981;&#20934;&#30830;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#37117;&#19987;&#27880;&#20110;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20197;&#36798;&#21040;&#26356;&#31934;&#30830;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26368;&#36817;&#37051;&#23621;&#20449;&#24687;&#29992;&#20110;&#22240;&#26524;&#25512;&#26029;&#65288;NNCI&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26377;&#20215;&#20540;&#30340;&#26368;&#36817;&#37051;&#23621;&#20449;&#24687;&#38598;&#25104;&#21040;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#20013;&#65292;&#20197;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;&#25552;&#20986;&#30340;NNCI&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#19968;&#20123;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#27169;&#22411;&#65292;&#20854;&#20351;&#29992;&#35266;&#23519;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect estimation is of high-importance for both researchers and practitioners across many scientific and industrial domains. The abundance of observational data makes them increasingly used by researchers for the estimation of causal effects. However, these data suffer from biases, from several weaknesses, leading to inaccurate causal effect estimations, if not handled properly. Therefore, several machine learning techniques have been proposed, most of them focusing on leveraging the predictive power of neural network models to attain more precise estimation of causal effects. In this work, we propose a new methodology, named Nearest Neighboring Information for Causal Inference (NNCI), for integrating valuable nearest neighboring information on neural network-based models for estimating treatment effects. The proposed NNCI methodology is applied to some of the most well established neural network-based models for treatment effect estimation with the use of observational data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#30340;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;SAM&#26469;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;SAM&#29983;&#25104;&#30340;&#25513;&#27169;&#12289;&#29305;&#24449;&#21644;&#31283;&#23450;&#24615;&#20998;&#25968;&#26469;&#26500;&#24314;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.11332</link><description>&lt;p&gt;
&#21033;&#29992;SAM&#30340;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;: &#20197;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#20026;&#22522;&#30784;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model. (arXiv:2304.11332v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#30340;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;SAM&#26469;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;SAM&#29983;&#25104;&#30340;&#25513;&#27169;&#12289;&#29305;&#24449;&#21644;&#31283;&#23450;&#24615;&#20998;&#25968;&#26469;&#26500;&#24314;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#26159;&#19968;&#20010;&#26368;&#36817;&#21457;&#23637;&#30340;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;. SAM&#20351;&#29992;&#20102;&#36229;&#36807;1&#20159;&#20010;&#25513;&#27169;&#30340;1100&#19975;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#20026;&#33258;&#28982;&#22330;&#26223;&#22270;&#20687;&#20013;&#30340;&#24191;&#27867;&#23545;&#35937;&#29983;&#25104;&#20998;&#21106;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#26679;&#19968;&#20010;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26469;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23613;&#31649;SAM&#24182;&#27809;&#26377;&#31435;&#21363;&#20026;&#21307;&#23398;&#22270;&#20687;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20998;&#21106;&#65292;&#20294;&#20854;&#29983;&#25104;&#30340;&#25513;&#27169;&#12289;&#29305;&#24449;&#21644;&#31283;&#23450;&#24615;&#20998;&#25968;&#23545;&#20110;&#26500;&#24314;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;SAM&#26469;&#22686;&#24378;&#32463;&#20856;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65288;&#22914;U-Net&#65289;&#30340;&#22270;&#20687;&#36755;&#20837;&#12290;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is a recently developed large model for general-purpose segmentation for computer vision tasks. SAM was trained using 11 million images with over 1 billion masks and can produce segmentation results for a wide range of objects in natural scene images. SAM can be viewed as a general perception model for segmentation (partitioning images into semantically meaningful regions). Thus, how to utilize such a large foundation model for medical image segmentation is an emerging research target. This paper shows that although SAM does not immediately give high-quality segmentation for medical images, its generated masks, features, and stability scores are useful for building and training better medical image segmentation models. In particular, we demonstrate how to use SAM to augment image inputs for a commonly-used medical image segmentation model (e.g., U-Net). Experiments on two datasets show the effectiveness of our proposed method.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;Segment Any Medical Model (SAMM)&#65292;&#23427;&#26159;&#29992;&#20110;3D Slicer&#30340;SAM&#30340;&#25193;&#23637;&#12290;SAMM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;&#23454;&#26102;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#37117;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#25513;&#27169;&#12290;</title><link>http://arxiv.org/abs/2304.05622</link><description>&lt;p&gt;
SAMM&#65288;Segment Any Medical Model&#65289;&#65306;&#29992;&#20110;SAM&#30340;3D Slicer&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM. (arXiv:2304.05622v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05622
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;Segment Any Medical Model (SAMM)&#65292;&#23427;&#26159;&#29992;&#20110;3D Slicer&#30340;SAM&#30340;&#25193;&#23637;&#12290;SAMM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;&#23454;&#26102;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#37117;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model&#65288;SAM&#65289;&#26159;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#20998;&#21106;&#24037;&#20855;&#65292;&#20351;&#29992;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#20998;&#21106;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#34920;&#26126;&#23427;&#21487;&#20197;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#20998;&#21106;&#25513;&#27169;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#26102;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#38656;&#35201;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#20026;&#20102;&#21327;&#21161;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#24320;&#21457;&#65292;&#35780;&#20272;&#21644;&#21033;&#29992;SAM&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Segment Any Medical Model&#65288;SAMM&#65289;&#65292;&#23427;&#26159;SAM&#22312;3D Slicer&#19978;&#30340;&#25193;&#23637;&#12290;3D Slicer&#26159;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#22788;&#29702;&#21644;&#21487;&#35270;&#21270;&#36719;&#20214;&#30340;&#24320;&#28304;&#36719;&#20214;&#12290;&#36825;&#20010;&#24320;&#28304;&#25193;&#23637;&#31243;&#24207;&#21450;&#20854;&#28436;&#31034;&#24050;&#21457;&#24067;&#22312;GitHub&#19978;&#65288;https://github.com/bingogome/samm&#65289;&#12290;SAMM&#22312;&#23436;&#25972;&#21608;&#26399;&#20013;&#23454;&#29616;&#20102;0.6&#31186;&#30340;&#24310;&#36831;&#65292;&#24182;&#21487;&#20197;&#23454;&#26102;&#25512;&#26029;&#20986;&#22270;&#20687;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is a new image segmentation tool trained with the largest segmentation dataset at this time. The model has demonstrated that it can create high-quality masks for image segmentation with good promptability and generalizability. However, the performance of the model on medical images requires further validation. To assist with the development, assessment, and utilization of SAM on medical images, we introduce Segment Any Medical Model (SAMM), an extension of SAM on 3D Slicer, a widely-used open-source image processing and visualization software that has been extensively used in the medical imaging community. This open-source extension to 3D Slicer and its demonstrations are posted on GitHub (https://github.com/bingogome/samm). SAMM achieves 0.6-second latency of a complete cycle and can infer image masks in nearly real-time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#23481;&#37327;&#65292;&#35777;&#26126;&#20102;&#20854;&#26368;&#20248;&#36924;&#36817;&#36895;&#29575;&#12290;&#24212;&#29992;&#21040;&#38750;&#21442;&#25968;&#22238;&#24402;&#19978;&#65292;&#35777;&#26126;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;H\"older&#20989;&#25968;&#30340;&#26368;&#20248;&#28176;&#36827;&#36895;&#29575;&#65292;&#34917;&#20805;&#20102;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#36817;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01561</link><description>&lt;p&gt;
Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#36895;&#29575;&#21450;&#20854;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression. (arXiv:2304.01561v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#23481;&#37327;&#65292;&#35777;&#26126;&#20102;&#20854;&#26368;&#20248;&#36924;&#36817;&#36895;&#29575;&#12290;&#24212;&#29992;&#21040;&#38750;&#21442;&#25968;&#22238;&#24402;&#19978;&#65292;&#35777;&#26126;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;H\"older&#20989;&#25968;&#30340;&#26368;&#20248;&#28176;&#36827;&#36895;&#29575;&#65292;&#34917;&#20805;&#20102;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#19982;Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30456;&#20851;&#30340;&#21464;&#24322;&#31354;&#38388;&#30340;&#36924;&#36817;&#23481;&#37327;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#21464;&#24322;&#33539;&#25968;&#19979;&#65292;&#23481;&#32435;&#20102;&#36275;&#22815;&#24179;&#28369;&#30340;&#20989;&#25968;&#12290;&#23545;&#20110;&#36739;&#23569;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#26681;&#25454;&#21464;&#24322;&#33539;&#25968;&#24314;&#31435;&#20102;&#36924;&#36817;&#36895;&#29575;&#12290;&#36816;&#29992;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#36924;&#36817;&#36895;&#29575;&#12290;&#21516;&#26102;&#38416;&#26126;&#20102;&#36825;&#20123;&#32467;&#26524;&#22914;&#20309;&#29992;&#20110;&#25512;&#23548;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#30340;&#36924;&#36817;&#30028;&#38480;&#12290;&#20026;&#24212;&#29992;&#30740;&#31350;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#31181;ReLU&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65306;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#36229;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#21644;CNN&#36827;&#34892;&#38750;&#21442;&#25968;&#22238;&#24402;&#25910;&#25947;&#36895;&#29575;&#30740;&#31350;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;H\"older&#20989;&#25968;&#30340;&#26368;&#20248;&#28176;&#36827;&#36895;&#29575;&#65292;&#36825;&#34917;&#20805;&#20102;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks (CNNs). As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN. In particular, we show that shallow neural networks can achieve the minimax optimal rates for learning H\"older functions, which complements recent results for deep neural networks. It is also proven th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21521;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#21644;&#21033;&#29992;&#19981;&#24179;&#34913;&#31639;&#27861;&#26469;&#25913;&#36827;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#25913;&#36827;&#21518;&#30340;VLM&#22312;iNaturalist18&#12289;CIFAR-100&#21644;Visual Genome&#25968;&#25454;&#38598;&#19978;&#20998;&#31867;&#20934;&#30830;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#31867;&#65292;&#24615;&#33021;&#25552;&#21319;&#24456;&#22823;&#12290;</title><link>http://arxiv.org/abs/2304.01457</link><description>&lt;p&gt;
&#25506;&#32034;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Vision-Language Models for Imbalanced Learning. (arXiv:2304.01457v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21521;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#21644;&#21033;&#29992;&#19981;&#24179;&#34913;&#31639;&#27861;&#26469;&#25913;&#36827;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#25913;&#36827;&#21518;&#30340;VLM&#22312;iNaturalist18&#12289;CIFAR-100&#21644;Visual Genome&#25968;&#25454;&#38598;&#19978;&#20998;&#31867;&#20934;&#30830;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#31867;&#65292;&#24615;&#33021;&#25552;&#21319;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#31867;&#30340;&#20998;&#24067;&#20542;&#26012;&#65292;&#23548;&#33268;&#22312;&#39044;&#27979;&#23569;&#25968;&#31867;&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#21521;VLM&#28155;&#21152;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#65292;&#20197;&#36991;&#20813;&#30001;&#20110;&#22823;&#37327;&#31867;&#21035;&#23548;&#33268;&#30340;&#20869;&#23384;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#25429;&#25417;&#23614;&#37096;&#31867;&#21035;&#30340;&#24494;&#22937;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#12289;&#24494;&#35843;&#20197;&#21450;&#21152;&#20837;&#19981;&#24179;&#34913;&#31639;&#27861;&#65288;&#20363;&#22914;Focal Loss&#12289;Balanced SoftMax&#21644;Distribution Alignment&#65289;&#26469;&#25913;&#36827;VLM&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#35299;&#30721;&#22120;&#21644;&#19981;&#24179;&#34913;&#26041;&#27861;&#26102;&#65292;VLM&#30340;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25913;&#36827;&#30340;VLM&#22312;iNaturalist18&#12289;CIFAR-100&#21644;Visual Genome&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#24179;&#22343;&#25552;&#39640;&#20102;6.58%&#12289;69.82%&#21644;10.43%&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language models (VLMs) that use contrastive language-image pre-training have shown promising zero-shot classification performance. However, their performance on imbalanced dataset is relatively poor, where the distribution of classes in the training dataset is skewed, leading to poor performance in predicting minority classes. For instance, CLIP achieved only 5% accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder to VLMs to avoid OOM (out of memory) problem caused by large number of classes and capture nuanced features for tail classes. Then, we explore improvements of VLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms such as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments demonstrate that the performance of VLMs can be further boosted when used with decoder and imbalanced methods. Specifically, our improved VLMs significantly outperforms zero-shot classification by an average accuracy of 6.58%, 69.82%,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#37319;&#26679;&#36807;&#31243;&#65292;&#20351;&#29992;&#21487;&#35270;&#21270;&#24605;&#32500;&#20256;&#36882;&#27169;&#22411;&#26469;&#32553;&#23567;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#30456;&#23545;&#26631;&#20934;&#26080;&#26465;&#20214;&#29983;&#25104;&#65292;FID&#25552;&#39640;25-50%&#12290;</title><link>http://arxiv.org/abs/2303.16187</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#24605;&#32500;&#20256;&#36882;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Visual Chain-of-Thought Diffusion Models. (arXiv:2303.16187v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#37319;&#26679;&#36807;&#31243;&#65292;&#20351;&#29992;&#21487;&#35270;&#21270;&#24605;&#32500;&#20256;&#36882;&#27169;&#22411;&#26469;&#32553;&#23567;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#30456;&#23545;&#26631;&#20934;&#26080;&#26465;&#20214;&#29983;&#25104;&#65292;FID&#25552;&#39640;25-50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#36817;&#26399;&#36827;&#23637;&#26159;&#24778;&#20154;&#30340;&#65292;&#36825;&#36866;&#29992;&#20110;&#20197;&#25991;&#26412;&#25551;&#36848;&#12289;&#22330;&#26223;&#24067;&#23616;&#25110;&#32032;&#25551;&#20026;&#26465;&#20214;&#30340;&#27169;&#22411;&#12290;&#26080;&#26465;&#20214;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20063;&#22312;&#25913;&#36827;&#65292;&#20294;&#33853;&#21518;&#20110;&#26465;&#20214;&#27169;&#22411;&#65292;&#20197;&#31867;&#26631;&#31614;&#20026;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#20134;&#26159;&#22914;&#27492;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20004;&#38454;&#27573;&#37319;&#26679;&#36807;&#31243;&#65292;&#32553;&#23567;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#37319;&#26679;&#25551;&#36848;&#22270;&#20687;&#35821;&#20041;&#20869;&#23481;&#30340;&#23884;&#20837;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#23884;&#20837;&#30340;&#26465;&#20214;&#19979;&#37319;&#26679;&#22270;&#20687;&#65292;&#28982;&#21518;&#20002;&#24323;&#36825;&#20010;&#23884;&#20837;&#12290;&#36825;&#26679;&#20570;&#35753;&#25105;&#20204;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#36827;&#34892;&#26080;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#35777;&#26126;&#30456;&#23545;&#20110;&#26631;&#20934;&#26080;&#26465;&#20214;&#29983;&#25104;&#65292;FID&#65288;Frechet inception distance&#65289;&#26368;&#22810;&#25552;&#39640;&#20102;25-50%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress with conditional image diffusion models has been stunning, and this holds true whether we are speaking about models conditioned on a text description, a scene layout, or a sketch. Unconditional image diffusion models are also improving but lag behind, as do diffusion models which are conditioned on lower-dimensional features like class labels. We propose to close the gap between conditional and unconditional models using a two-stage sampling procedure. In the first stage we sample an embedding describing the semantic content of the image. In the second stage we sample the image conditioned on this embedding and then discard the embedding. Doing so lets us leverage the power of conditional diffusion models on the unconditional generation task, which we show improves FID by 25-50% compared to standard unconditional generation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#23398;&#20064;&#25298;&#32477;&#8221;&#26694;&#26550;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#40664;&#40664;&#22833;&#36133;&#38382;&#39064;&#12290;&#36890;&#36807;&#39044;&#27979;&#21487;&#20449;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#25509;&#21463;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#33021;&#21147;&#21306;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#23398;&#20064;&#34920;&#31034;&#34913;&#37327;&#26080;&#33021;&#65292;&#22686;&#21152;&#26080;&#33021;&#24471;&#20998;&#20250;&#39044;&#31034;&#30528;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09989</link><description>&lt;p&gt;
&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#25214;&#21040;&#33021;&#21147;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Finding Competence Regions in Domain Generalization. (arXiv:2303.09989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09989
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#23398;&#20064;&#25298;&#32477;&#8221;&#26694;&#26550;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#40664;&#40664;&#22833;&#36133;&#38382;&#39064;&#12290;&#36890;&#36807;&#39044;&#27979;&#21487;&#20449;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#25509;&#21463;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#33021;&#21147;&#21306;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#23398;&#20064;&#34920;&#31034;&#34913;&#37327;&#26080;&#33021;&#65292;&#22686;&#21152;&#26080;&#33021;&#24471;&#20998;&#20250;&#39044;&#31034;&#30528;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#23398;&#20064;&#25298;&#32477;&#8221;&#26694;&#26550;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#20013;&#40664;&#40664;&#22833;&#36133;&#30340;&#38382;&#39064;&#65292;&#21363;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#20551;&#35774;&#26377;&#19968;&#20010;&#28201;&#21644;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27169;&#22411;&#20272;&#35745;&#30340;&#33021;&#21147;&#39044;&#31034;&#30528;&#21487;&#20449;&#21709;&#24212;&#26102;&#25509;&#21463;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#25298;&#32477;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#21487;&#20449;&#24230;&#36890;&#36807;&#19982;&#20998;&#31867;&#22120;&#24615;&#33021;&#23494;&#20999;&#30456;&#20851;&#30340;&#20195;&#29702;&#26080;&#33021;&#20998;&#25968;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23545;&#20998;&#31867;&#30340;&#26080;&#33021;&#24471;&#20998;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#24378;&#35843;&#20102;&#25298;&#32477;&#29575;&#19982;&#20934;&#30830;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;&#26631;&#20934;&#39046;&#22495;&#27867;&#21270;&#22522;&#20934;&#65292;&#24182;&#32771;&#34385;&#22312;&#38381;&#21512;&#21644;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#19979;&#36890;&#36807;&#19981;&#21516;&#30340;&#23398;&#20064;&#34920;&#31034;&#26469;&#34913;&#37327;&#26080;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#26080;&#33021;&#20998;&#25968;&#30830;&#23454;&#39044;&#31034;&#30528;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#26174;&#30528;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We propose a "learning to reject" framework to address the problem of silent failures in Domain Generalization (DG), where the test distribution differs from the training distribution. Assuming a mild distribution shift, we wish to accept out-of-distribution (OOD) data whenever a model's estimated competence foresees trustworthy responses, instead of rejecting OOD data outright. Trustworthiness is then predicted via a proxy incompetence score that is tightly linked to the performance of a classifier. We present a comprehensive experimental evaluation of incompetence scores for classification and highlight the resulting trade-offs between rejection rate and accuracy gain. For comparability with prior work, we focus on standard DG benchmarks and consider the effect of measuring incompetence via different learned representations in a closed versus an open world setting. Our results suggest that increasing incompetence scores are indeed predictive of reduced accuracy, leading to significan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; CoHiClust &#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#23618;&#27425;&#32858;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29983;&#25104;&#19982;&#25105;&#20204;&#30452;&#35273;&#21644;&#22270;&#20687;&#35821;&#20041;&#30456;&#31526;&#30340;&#21512;&#29702;&#32858;&#31867;&#32467;&#26500;&#65292;&#19988;&#22312;&#22823;&#37096;&#20998;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#24179;&#38754;&#32858;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.03389</link><description>&lt;p&gt;
&#23545;&#27604;&#23618;&#27425;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Contrastive Hierarchical Clustering. (arXiv:2303.03389v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; CoHiClust &#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#23618;&#27425;&#32858;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29983;&#25104;&#19982;&#25105;&#20204;&#30452;&#35273;&#21644;&#22270;&#20687;&#35821;&#20041;&#30456;&#31526;&#30340;&#21512;&#29702;&#32858;&#31867;&#32467;&#26500;&#65292;&#19988;&#22312;&#22823;&#37096;&#20998;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#24179;&#38754;&#32858;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32858;&#31867;&#19968;&#30452;&#34987;&#24179;&#38754;&#27169;&#22411;&#25152;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25968;&#25454;&#38598;&#20998;&#25104;&#39044;&#23450;&#20041;&#25968;&#37327;&#30340;&#32452;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#22522;&#26412;&#20107;&#23454;&#30340;&#30456;&#20284;&#24230;&#38750;&#24120;&#39640;&#65292;&#20294;&#24179;&#38754;&#20998;&#21306;&#25552;&#20379;&#30340;&#20449;&#24687;&#26377;&#38480;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#23618;&#27425;&#32858;&#31867;&#27169;&#22411; CoHiClust&#65292;&#21487;&#24212;&#29992;&#20110;&#20856;&#22411;&#22270;&#20687;&#25968;&#25454;&#12290;&#36890;&#36807;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;CoHiClust &#21487;&#20197;&#23558;&#22522;&#30784;&#32593;&#32476;&#33976;&#39311;&#20026;&#20108;&#21449;&#26641;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#20219;&#20309;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#23618;&#27425;&#32858;&#31867;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#32858;&#31867;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#34913;&#37327;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoHiClust &#29983;&#25104;&#20102;&#19968;&#20010;&#21512;&#29702;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#31526;&#21512;&#25105;&#20204;&#30340;&#30452;&#35273;&#21644;&#22270;&#20687;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#23427;&#19982;&#26368;&#20808;&#36827;&#30340;&#24179;&#38754;&#32858;&#31867;&#30456;&#27604;&#65292;&#22312;&#22823;&#22810;&#25968;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep clustering has been dominated by flat models, which split a dataset into a predefined number of groups. Although recent methods achieve an extremely high similarity with the ground truth on popular benchmarks, the information contained in the flat partition is limited. In this paper, we introduce CoHiClust, a Contrastive Hierarchical Clustering model based on deep neural networks, which can be applied to typical image data. By employing a self-supervised learning approach, CoHiClust distills the base network into a binary tree without access to any labeled data. The hierarchical clustering structure can be used to analyze the relationship between clusters, as well as to measure the similarity between data points. Experiments demonstrate that CoHiClust generates a reasonable structure of clusters, which is consistent with our intuition and image semantics. Moreover, it obtains superior clustering accuracy on most of the image datasets compared to the state-of-the-art flat clusterin
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#38454;&#24179;&#22374;&#24230;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#26799;&#24230;&#33539;&#25968;&#24863;&#30693;&#26368;&#23567;&#21270;&#31639;&#27861;&#23547;&#25214;&#22312;&#25152;&#26377;&#26041;&#21521;&#19978;&#20855;&#26377;&#22343;&#21248;&#23567;&#26354;&#29575;&#30340;&#26497;&#23567;&#20540;&#65292;&#25552;&#39640;&#20102;&#24191;&#20041;&#21270;&#33021;&#21147;&#21644;&#27979;&#35797;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2303.03108</link><description>&lt;p&gt;
&#26799;&#24230;&#33539;&#25968;&#24863;&#30693;&#26368;&#23567;&#21270;&#22312;&#23547;&#25214;&#19968;&#38454;&#24179;&#22374;&#24230;&#20013;&#25913;&#36827;&#20102;&#24191;&#20041;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization. (arXiv:2303.03108v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03108
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#38454;&#24179;&#22374;&#24230;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#26799;&#24230;&#33539;&#25968;&#24863;&#30693;&#26368;&#23567;&#21270;&#31639;&#27861;&#23547;&#25214;&#22312;&#25152;&#26377;&#26041;&#21521;&#19978;&#20855;&#26377;&#22343;&#21248;&#23567;&#26354;&#29575;&#30340;&#26497;&#23567;&#20540;&#65292;&#25552;&#39640;&#20102;&#24191;&#20041;&#21270;&#33021;&#21147;&#21644;&#27979;&#35797;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270; (SAM) &#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;SAM &#21450;&#20854;&#21518;&#32493;&#35752;&#35770;&#20013;&#24403;&#21069;&#20851;&#20110;&#24179;&#22374;&#24615;&#30340;&#23450;&#20041;&#20165;&#38480;&#20110;&#38646;&#38454;&#24179;&#22374;&#24615; (&#21363;&#25200;&#21160;&#21322;&#24452;&#20869;&#26368;&#22351;&#25439;&#22833;)&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#23384;&#22312;&#21333;&#19968;&#26368;&#23567;&#20540;&#25110;&#32473;&#23450;&#25200;&#21160;&#21322;&#24452;&#20869;&#30340;&#22810;&#20010;&#26368;&#23567;&#20540;&#26102;&#65292;&#38646;&#38454;&#24179;&#22374;&#24230;&#21487;&#33021;&#19981;&#36275;&#20197;&#21306;&#20998;&#20855;&#26377;&#20302;&#27867;&#21270;&#35823;&#24046;&#21644;&#39640;&#27867;&#21270;&#35823;&#24046;&#30340;&#26497;&#23567;&#20540;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#38454;&#24179;&#22374;&#24230;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#24378;&#30340;&#24179;&#22374;&#24230;&#27979;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#25200;&#21160;&#21322;&#24452;&#20869;&#30340;&#26368;&#22823;&#26799;&#24230;&#33539;&#25968;&#65292;&#20854;&#38480;&#21046;&#20102;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340; Hessian &#30340;&#26368;&#22823;&#29305;&#24449;&#20540;&#21644; SAM &#30340;&#27491;&#21017;&#21270;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Gradient norm Aware Minimization (GAM) &#30340;&#26032;&#22411;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23547;&#25214;&#25152;&#26377;&#26041;&#21521;&#19978;&#26354;&#29575;&#22343;&#21248;&#23567;&#30340;&#26368;&#23567;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GAM &#26174;&#30528;&#25913;&#36827;&#20102;&#24191;&#20041;&#21270;&#33021;&#21147;&#21644;&#27979;&#35797;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Recently, flat minima are proven to be effective for improving generalization and sharpness-aware minimization (SAM) achieves state-of-the-art performance. Yet the current definition of flatness discussed in SAM and its follow-ups are limited to the zeroth-order flatness (i.e., the worst-case loss within a perturbation radius). We show that the zeroth-order flatness can be insufficient to discriminate minima with low generalization error from those with high generalization error both when there is a single minimum or multiple minima within the given perturbation radius. Thus we present first-order flatness, a stronger measure of flatness focusing on the maximal gradient norm within a perturbation radius which bounds both the maximal eigenvalue of Hessian at local minima and the regularization function of SAM. We also present a novel training procedure named Gradient norm Aware Minimization (GAM) to seek minima with uniformly small curvature across all directions. Experimental results s
&lt;/p&gt;</description></item><item><title>CHGNet&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#21183;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#21270;&#23398;&#32452;&#25104;&#21644;&#26230;&#20307;&#32467;&#26500;&#30340;&#21508;&#31181;&#26448;&#26009;&#30340;&#23646;&#24615;&#65292;&#24182;&#19988;&#23558;&#30005;&#33655;&#20449;&#24687;&#24182;&#20837;&#21407;&#23376;&#34920;&#31034;&#20013;&#65292;&#33021;&#22815;&#29992;&#20110;&#25551;&#36848;&#24102;&#30005;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#30005;&#23376;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.14231</link><description>&lt;p&gt;
CHGNet&#65306;&#29992;&#20110;&#24102;&#30005;&#21407;&#23376;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#21183;
&lt;/p&gt;
&lt;p&gt;
CHGNet: Pretrained universal neural network potential for charge-informed atomistic modeling. (arXiv:2302.14231v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14231
&lt;/p&gt;
&lt;p&gt;
CHGNet&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#21183;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#21270;&#23398;&#32452;&#25104;&#21644;&#26230;&#20307;&#32467;&#26500;&#30340;&#21508;&#31181;&#26448;&#26009;&#30340;&#23646;&#24615;&#65292;&#24182;&#19988;&#23558;&#30005;&#33655;&#20449;&#24687;&#24182;&#20837;&#21407;&#23376;&#34920;&#31034;&#20013;&#65292;&#33021;&#22815;&#29992;&#20110;&#25551;&#36848;&#24102;&#30005;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#30005;&#23376;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#30005;&#23376;&#30456;&#20114;&#20316;&#29992;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#27169;&#25311;&#20173;&#28982;&#26159;&#26448;&#26009;&#21407;&#23376;&#24314;&#27169;&#20013;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#23613;&#31649;&#32463;&#20856;&#21147;&#22330;&#36890;&#24120;&#26080;&#27861;&#25551;&#36848;&#30005;&#23376;&#29366;&#24577;&#21644;&#31163;&#23376;&#37325;&#25490;&#20043;&#38388;&#30340;&#32806;&#21512;&#65292;&#20294;&#26356;&#20934;&#30830;&#30340;&#20174;&#22836;&#20998;&#23376;&#21160;&#21147;&#23398;&#30001;&#20110;&#35745;&#31639;&#22797;&#26434;&#24615;&#32780;&#26080;&#27861;&#36827;&#34892;&#38271;&#26102;&#38388;&#21644;&#22823;&#35268;&#27169;&#30340;&#27169;&#25311;&#65292;&#36825;&#23545;&#20110;&#30740;&#31350;&#35768;&#22810;&#25216;&#26415;&#30456;&#20851;&#29616;&#35937;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#21453;&#24212;&#12289;&#31163;&#23376;&#36801;&#31227;&#12289;&#30456;&#21464;&#21644;&#38477;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Crystal Hamiltonian Graph&#31070;&#32463;&#32593;&#32476;&#65288;CHGNet&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#30456;&#20114;&#20316;&#29992;&#21183;&#65288;MLIP&#65289;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21147;&#22330;&#26469;&#24314;&#27169;&#36890;&#29992;&#30340;&#21183;&#33021;&#38754;&#12290; CHGNet&#20197;&#26448;&#26009;&#39033;&#30446;&#36712;&#36857;&#25968;&#25454;&#38598;&#20013;&#30340;&#33021;&#37327;&#12289;&#21147;&#12289;&#24212;&#21147;&#21644;&#30913;&#30697;&#20026;&#20808;&#39564;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#36229;&#36807;10&#24180;&#30340;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#38745;&#24577;&#21644;&#26494;&#24347;&#35745;&#31639;&#32452;&#25104;&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;86,000&#31181;&#26448;&#26009;&#65292;&#24182;&#22240;&#27492;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#21270;&#23398;&#32452;&#25104;&#21644;&#26230;&#20307;&#32467;&#26500;&#30340;&#21508;&#31181;&#26448;&#26009;&#30340;&#23646;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;CHGNet&#36824;&#23558;&#30005;&#33655;&#20449;&#24687;&#24182;&#20837;&#21407;&#23376;&#34920;&#31034;&#20013;&#65292;&#20351;&#24471;&#25551;&#36848;&#24102;&#30005;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#30005;&#23376;&#30456;&#20114;&#20316;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#20960;&#20010;&#27169;&#22411;&#31995;&#32479;&#30340;&#30456;&#31283;&#23450;&#24615;&#12289;&#34920;&#38754;&#26494;&#24347;&#21644;&#34920;&#38754;&#20998;&#31163;&#30340;&#39044;&#27979;&#65292;&#24182;&#23558;&#20854;&#31934;&#24230;&#21644;&#25928;&#29575;&#19982;&#20854;&#20182;MLIP&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;CHGNet&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;CHGNet&#26159;&#29992;&#20110;&#24102;&#30005;&#21407;&#23376;&#24314;&#27169;&#30340;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#21152;&#36895;&#26032;&#26448;&#26009;&#30340;&#35774;&#35745;&#21644;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The simulation of large-scale systems with complex electron interactions remains one of the greatest challenges for the atomistic modeling of materials. Although classical force fields often fail to describe the coupling between electronic states and ionic rearrangements, the more accurate \textit{ab-initio} molecular dynamics suffers from computational complexity that prevents long-time and large-scale simulations, which are essential to study many technologically relevant phenomena, such as reactions, ion migrations, phase transformations, and degradation.  In this work, we present the Crystal Hamiltonian Graph neural Network (CHGNet) as a novel machine-learning interatomic potential (MLIP), using a graph-neural-network-based force field to model a universal potential energy surface. CHGNet is pretrained on the energies, forces, stresses, and magnetic moments from the Materials Project Trajectory Dataset, which consists of over 10 years of density functional theory static and relaxat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#39318;&#27425;&#33021;&#22815;&#22312;&#32771;&#34385;&#23545;&#40784;&#25968;&#25454;&#30340;&#21516;&#26102;&#35299;&#20915;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#26377;&#26356;&#31616;&#21333;&#12289;&#26041;&#24046;&#26356;&#20302;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#21407;&#21017;&#24615;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.11419</link><description>&lt;p&gt;
&#23545;&#40784;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;
&lt;/p&gt;
&lt;p&gt;
Aligned Diffusion Schr\"odinger Bridges. (arXiv:2302.11419v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#39318;&#27425;&#33021;&#22815;&#22312;&#32771;&#34385;&#23545;&#40784;&#25968;&#25454;&#30340;&#21516;&#26102;&#35299;&#20915;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#26377;&#26356;&#31616;&#21333;&#12289;&#26041;&#24046;&#26356;&#20302;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#21407;&#21017;&#24615;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#30340;&#36793;&#38469;&#35266;&#23519;&#24674;&#22797;&#38543;&#26426;&#21160;&#24577;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;(DSB)&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#25104;&#21151;&#30340;&#24212;&#29992;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#23578;&#26410;&#21033;&#29992;&#22810;&#31181;&#29983;&#29289;&#29616;&#35937;&#20013;&#33258;&#28982;&#20986;&#29616;&#30340;&#23545;&#40784;&#25968;&#25454;&#32467;&#26500;&#26469;&#35299;&#20915;DSB&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#39318;&#27425;&#22312;&#32771;&#34385;&#23545;&#40784;&#25968;&#25454;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;DSB&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#38752;&#20004;&#20010;&#20108;&#21313;&#24180;&#21382;&#21490;&#30340;&#24605;&#24819;&#32467;&#21512;&#36215;&#26469;&#65306;&#32463;&#20856;&#30340;&#34203;&#23450;&#35860;&#26725;&#29702;&#35770;&#21644;Doob&#30340;$h$-&#21464;&#25442;&#12290;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#26356;&#31616;&#21333;&#65292;&#26041;&#24046;&#26356;&#20302;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#21407;&#21017;&#24615;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#12290;&#36825;&#26368;&#32456;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#23548;&#33268;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#21018;&#24615;&#34507;&#30333;&#36136;&#23545;&#25509;&#21644;&#32454;&#32990;&#20998;&#21270;&#36807;&#31243;&#30340;&#26102;&#38388;&#28436;&#21270;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Schr\"odinger bridges (DSB) have recently emerged as a powerful framework for recovering stochastic dynamics via their marginal observations at different time points. Despite numerous successful applications, existing algorithms for solving DSBs have so far failed to utilize the structure of aligned data, which naturally arises in many biological phenomena. In this paper, we propose a novel algorithmic framework that, for the first time, solves DSBs while respecting the data alignment. Our approach hinges on a combination of two decades-old ideas: The classical Schr\"odinger bridge theory and Doob's $h$-transform. Compared to prior methods, our approach leads to a simpler training procedure with lower variance, which we further augment with principled regularization schemes. This ultimately leads to sizeable improvements across experiments on synthetic and real data, including the tasks of rigid protein docking and temporal evolution of cellular differentiation processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26631;&#31614;&#31354;&#38388;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#21040;&#21463;&#38480;&#21046;&#30340;&#39044;&#27979;&#38598;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#32039;&#20945;&#22320;&#34920;&#31034;&#20026;&#37096;&#20998;&#31243;&#24207;&#65292;&#24182;&#21033;&#29992;&#20102;&#32534;&#31243;&#35821;&#35328;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#65292;&#20351;&#24471;&#27491;&#30830;&#30340;&#31243;&#24207;&#20197;&#39640;&#32622;&#20449;&#24230;&#20986;&#29616;&#22312;&#38598;&#21512;&#20013;&#12290;&#24212;&#29992;&#26041;&#38754;&#21253;&#25324;Codex&#39118;&#26684;&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.08703</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;PAC&#39044;&#27979;&#38598;
&lt;/p&gt;
&lt;p&gt;
PAC Prediction Sets for Large Language Models of Code. (arXiv:2302.08703v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26631;&#31614;&#31354;&#38388;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#21040;&#21463;&#38480;&#21046;&#30340;&#39044;&#27979;&#38598;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#32039;&#20945;&#22320;&#34920;&#31034;&#20026;&#37096;&#20998;&#31243;&#24207;&#65292;&#24182;&#21033;&#29992;&#20102;&#32534;&#31243;&#35821;&#35328;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#65292;&#20351;&#24471;&#27491;&#30830;&#30340;&#31243;&#24207;&#20197;&#39640;&#32622;&#20449;&#24230;&#20986;&#29616;&#22312;&#38598;&#21512;&#20013;&#12290;&#24212;&#29992;&#26041;&#38754;&#21253;&#25324;Codex&#39118;&#26684;&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#38598;&#26368;&#36817;&#34987;&#35777;&#26126;&#26159;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#20063;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25216;&#26415;&#22823;&#22810;&#38024;&#23545;&#26631;&#31614;&#31354;&#38388;&#31616;&#21333;&#30340;&#35774;&#32622;&#65292;&#22240;&#27492;&#39044;&#27979;&#38598;&#21487;&#20197;&#26159;&#20219;&#24847;&#23376;&#38598;&#12290;&#23545;&#20110;&#26631;&#31614;&#31354;&#38388;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#38382;&#39064;&#65292;&#21363;&#20351;&#21253;&#21547;&#25152;&#26377;&#26631;&#31614;&#30340;&#39044;&#27979;&#38598;&#20063;&#21487;&#33021;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#22312;&#20195;&#30721;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#32771;&#34385;&#21040;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;&#39044;&#27979;&#38598;&#65292;&#21487;&#20197;&#32039;&#20945;&#22320;&#34920;&#31034;&#20026;&#37096;&#20998;&#31243;&#24207;&#65292;&#36825;&#20123;&#37096;&#20998;&#31243;&#24207;&#20013;&#30340;&#37096;&#20998;&#24050;&#32463;&#34987;&#26367;&#25442;&#20026;&#21344;&#20301;&#31526;&#12290;&#37492;&#20110;&#35757;&#32451;&#22909;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;&#32534;&#31243;&#35821;&#35328;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#29983;&#25104;&#19968;&#32452;&#31243;&#24207;&#65292;&#20197;&#20415;&#27491;&#30830;&#30340;&#31243;&#24207;&#20197;&#39640;&#32622;&#20449;&#24230;&#20986;&#29616;&#22312;&#38598;&#21512;&#20013;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26377;&#24456;&#22810;&#26377;&#20215;&#20540;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#21253;&#25324;Codex&#39118;&#26684;&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction sets have recently been shown to be a promising strategy for quantifying the uncertainty of deep neural networks in a way that provides theoretical guarantees. However, existing techniques have largely targeted settings where the space of labels is simple, so prediction sets can be arbitrary subsets of labels. For structured prediction problems where the space of labels is exponential in size, even prediction sets containing a small fraction of all labels can be exponentially large. In the context of code generation, we propose a solution that considers a restricted set of prediction sets that can compactly be represented as partial programs, which are programs with portions replaced with holes. Given a trained code generation model, our algorithm leverages a programming language's abstract syntax tree to generate a set of programs such that the correct program is in the set with high-confidence. Valuable applications of our algorithm include a Codex-style code generator wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#32422;&#26463;&#20915;&#31574;Transformer&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#26435;&#34913;&#65292;&#20855;&#26377;&#23398;&#20064;&#33258;&#36866;&#24212;&#12289;&#23433;&#20840;&#12289;&#40065;&#26834;&#19988;&#39640;&#25253;&#37228;&#30340;&#20248;&#21183;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.07351</link><description>&lt;p&gt;
&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Constrained Decision Transformer for Offline Safe Reinforcement Learning. (arXiv:2302.07351v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#32422;&#26463;&#20915;&#31574;Transformer&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#26435;&#34913;&#65292;&#20855;&#26377;&#23398;&#20064;&#33258;&#36866;&#24212;&#12289;&#23433;&#20840;&#12289;&#40065;&#26834;&#19988;&#39640;&#25253;&#37228;&#30340;&#20248;&#21183;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#35757;&#32451;&#32422;&#26463;&#28385;&#36275;&#31574;&#30053;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65306;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#23433;&#20840;&#31574;&#30053;&#12290;&#25105;&#20204;&#20174;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#949;-&#21487;&#20943;&#27010;&#24565;&#26469;&#34920;&#24449;&#38382;&#39064;&#38590;&#24230;&#12290;&#23433;&#20840;&#21644;&#20219;&#21153;&#32489;&#25928;&#20043;&#38388;&#30340;&#22266;&#26377;&#26435;&#34913;&#28608;&#21457;&#20102;&#25105;&#20204;&#25552;&#20986;&#32422;&#26463;&#20915;&#31574;Transformer&#65288;CDT&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#26435;&#34913;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#33258;&#36866;&#24212;&#12289;&#23433;&#20840;&#12289;&#40065;&#26834;&#19988;&#39640;&#25253;&#37228;&#30340;&#31574;&#30053;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#19982;&#25152;&#26377;&#20219;&#21153;&#30456;&#21516;&#30340;&#36229;&#21442;&#25968;&#19979;&#65292;CDT&#22312;&#20445;&#25345;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#30340;&#21516;&#26102;&#65292;&#20197;&#22823;&#24133;&#36229;&#36807;&#20854;&#21464;&#20307;&#21644;&#24378;&#31163;&#32447;&#23433;&#20840;RL&#22522;&#32447;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#36866;&#29992;&#20110;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#29616;&#23454;RL&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) trains a constraint satisfaction policy by interacting with the environment. We aim to tackle a more challenging problem: learning a safe policy from an offline dataset. We study the offline safe RL problem from a novel multi-objective optimization perspective and propose the $\epsilon$-reducible concept to characterize problem difficulties. The inherent trade-offs between safety and task performance inspire us to propose the constrained decision transformer (CDT) approach, which can dynamically adjust the trade-offs during deployment. Extensive experiments show the advantages of the proposed method in learning an adaptive, safe, robust, and high-reward policy. CDT outperforms its variants and strong offline safe RL baselines by a large margin with the same hyperparameters across all tasks, while keeping the zero-shot adaptation capability to different constraint thresholds, making our approach more suitable for real-world RL under constraints. The code
&lt;/p&gt;</description></item><item><title>PATCorrect&#26159;&#19968;&#31181;&#22522;&#20110;&#38899;&#32032;&#22686;&#24378;&#30340;&#38750;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#38899;&#32032;&#27169;&#24577;&#30340;&#34920;&#31034;&#26469;&#26368;&#22823;&#38480;&#24230;&#22320;&#38477;&#20302;ASR&#31995;&#32479;&#20013;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#65292;&#24182;&#22312;&#20302;&#24310;&#36831;&#38656;&#27714;&#30340;&#23454;&#38469;&#29983;&#20135;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.05040</link><description>&lt;p&gt;
PATCorrect&#65306;&#22522;&#20110;&#38899;&#32032;&#22686;&#24378;&#30340;&#38750;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#29992;&#20110;ASR&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction. (arXiv:2302.05040v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05040
&lt;/p&gt;
&lt;p&gt;
PATCorrect&#26159;&#19968;&#31181;&#22522;&#20110;&#38899;&#32032;&#22686;&#24378;&#30340;&#38750;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#38899;&#32032;&#27169;&#24577;&#30340;&#34920;&#31034;&#26469;&#26368;&#22823;&#38480;&#24230;&#22320;&#38477;&#20302;ASR&#31995;&#32479;&#20013;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#65292;&#24182;&#22312;&#20302;&#24310;&#36831;&#38656;&#27714;&#30340;&#23454;&#38469;&#29983;&#20135;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#31995;&#32479;&#20135;&#29983;&#30340;&#35821;&#38899;&#21040;&#25991;&#26412;&#38169;&#35823;&#20250;&#23545;&#19979;&#28216;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#38169;&#35823;&#20462;&#27491;&#27169;&#22411;&#20316;&#20026;&#21518;&#22788;&#29702;&#25991;&#26412;&#32534;&#36753;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;ASR&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#23545;&#31526;&#21512;&#24037;&#19994;&#32423;&#29983;&#20135;&#31995;&#32479;&#20302;&#24310;&#36831;&#38656;&#27714;&#30340;&#39640;&#25928;&#27169;&#22411;&#36827;&#34892;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PATCorrect&#36825;&#19968;&#26032;&#22411;&#38750;&#33258;&#22238;&#24402;(NAR)&#26041;&#27861;&#65292;&#22522;&#20110;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#38899;&#32032;&#27169;&#24577;&#30340;&#34920;&#31034;&#65292;&#20197;&#38477;&#20302;&#21333;&#35789;&#38169;&#35823;&#29575;(WER)&#65292;&#24182;&#22312;&#19981;&#21516;&#36136;&#37327;&#30340;&#36755;&#20837;&#36716;&#24405;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;PATCorrect&#22312;&#33521;&#35821;&#35821;&#26009;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;NAR&#26041;&#27861;&#65292;&#19982;&#20165;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#24635;&#20307;WER&#38477;&#20302;&#20102;11.62%&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#30340;WER&#38477;&#20302;&#29575;&#20026;9.46%&#12290;&#27492;&#22806;&#65292;&#20854;&#25512;&#29702;&#24310;&#36831;&#22312;&#27627;&#31186;&#32423;&#21035;&#65292;&#38750;&#24120;&#36866;&#21512;&#38656;&#35201;&#20302;&#24310;&#36831;&#31995;&#32479;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-to-text errors made by automatic speech recognition (ASR) systems negatively impact downstream models. Error correction models as a post-processing text editing method have been recently developed for refining the ASR outputs. However, efficient models that meet the low latency requirements of industrial grade production systems have not been well studied. We propose PATCorrect-a novel non-autoregressive (NAR) approach based on multi-modal fusion leveraging representations from both text and phoneme modalities, to reduce word error rate (WER) and perform robustly with varying input transcription quality. We demonstrate that PATCorrect consistently outperforms state-of-the-art NAR method on English corpus across different upstream ASR systems, with an overall 11.62% WER reduction (WERR) compared to 9.46% WERR achieved by other methods using text only modality. Besides, its inference latency is at tens of milliseconds, making it ideal for systems with low latency requirements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31639;&#27861;&#38598;&#20307;&#34892;&#21160;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#24182;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#32467;&#26500;&#22797;&#26434;&#21644;&#38598;&#20307;&#35268;&#27169;&#22823;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2302.04262</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#31639;&#27861;&#38598;&#20307;&#34892;&#21160;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Collective Action in Machine Learning. (arXiv:2302.04262v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31639;&#27861;&#38598;&#20307;&#34892;&#21160;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#24182;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#32467;&#26500;&#22797;&#26434;&#21644;&#38598;&#20307;&#35268;&#27169;&#22823;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22312;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#23383;&#24179;&#21488;&#19978;&#36827;&#34892;&#31639;&#27861;&#38598;&#20307;&#34892;&#21160;&#36827;&#34892;&#20102;&#21407;&#21017;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#25551;&#36848;&#20102;&#19968;&#32676;&#20154;&#19982;&#20844;&#21496;&#30340;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20132;&#20114;&#30340;&#24773;&#20917;&#12290;&#38598;&#20307;&#27719;&#32858;&#21442;&#19982;&#20010;&#20307;&#30340;&#25968;&#25454;&#24182;&#36890;&#36807;&#19968;&#31181;&#31639;&#27861;&#31574;&#30053;&#25351;&#23548;&#21442;&#19982;&#32773;&#20462;&#25913;&#33258;&#24049;&#30340;&#25968;&#25454;&#20197;&#23454;&#29616;&#38598;&#20307;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#22522;&#26412;&#30340;&#23398;&#20064;&#29702;&#35770;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#36825;&#31181;&#27169;&#22411;&#30340;&#32467;&#26524;&#65306;&#38750;&#21442;&#25968;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65292;&#21442;&#25968;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#12290;&#22312;&#27599;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#35843;&#30340;&#31639;&#27861;&#31574;&#30053;&#65292;&#24182;&#26681;&#25454;&#38598;&#20307;&#35268;&#27169;&#30340;&#22823;&#23567;&#26469;&#34920;&#24449;&#33258;&#28982;&#30340;&#25104;&#21151;&#26631;&#20934;&#12290;&#20026;&#20102;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#23545;&#28041;&#21450;&#25968;&#20197;&#19975;&#35745;&#33258;&#30001;&#32844;&#19994;&#24179;&#21488;&#31616;&#21382;&#30340;&#25216;&#33021;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#23454;&#39564;&#12290;&#36890;&#36807; BERT &#27169;&#22411;&#30340;&#20004;&#21315;&#22810;&#27425;&#35757;&#32451;&#36816;&#34892;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#38598;&#20307;&#34892;&#21160;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#27604;&#38598;&#20013;&#24335;&#23398;&#20064;&#31639;&#27861;&#21644;&#29420;&#31435;&#20462;&#25913;&#25968;&#25454;&#30340;&#38750;&#21327;&#35843;&#26041;&#27861;&#35201;&#22909;&#24471;&#22810;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#31639;&#27861;&#38598;&#20307;&#34892;&#21160;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#20381;&#36182;&#20110;&#38598;&#20307;&#30340;&#35268;&#27169;&#21644;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate a principled study of algorithmic collective action on digital platforms that deploy machine learning algorithms. We propose a simple theoretical model of a collective interacting with a firm's learning algorithm. The collective pools the data of participating individuals and executes an algorithmic strategy by instructing participants how to modify their own data to achieve a collective goal. We investigate the consequences of this model in three fundamental learning-theoretic settings: the case of a nonparametric optimal learning algorithm, a parametric risk minimizer, and gradient-based optimization. In each setting, we come up with coordinated algorithmic strategies and characterize natural success criteria as a function of the collective's size. Complementing our theory, we conduct systematic experiments on a skill classification task involving tens of thousands of resumes from a gig platform for freelancers. Through more than two thousand model training runs of a BERT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#39640;&#25928;&#22270;&#22330;&#31215;&#20998;&#65292;&#36866;&#29992;&#20110;&#28857;&#20113;&#32593;&#26684;&#22270;&#25110;&#949;-&#26368;&#36817;&#37051;&#22270;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.00942</link><description>&lt;p&gt;
&#39640;&#25928;&#22270;&#22330;&#31215;&#20998;&#22120;&#22312;&#28857;&#20113;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Graph Field Integrators Meet Point Clouds. (arXiv:2302.00942v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#39640;&#25928;&#22270;&#22330;&#31215;&#20998;&#65292;&#36866;&#29992;&#20110;&#28857;&#20113;&#32593;&#26684;&#22270;&#25110;&#949;-&#26368;&#36817;&#37051;&#22270;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#31639;&#27861;&#31867;&#21035;&#65292;&#29992;&#20110;&#23545;&#32534;&#30721;&#28857;&#20113;&#30340;&#22270;&#24418;&#36827;&#34892;&#39640;&#25928;&#22330;&#31215;&#20998;&#12290;&#31532;&#19968;&#31867;&#31639;&#27861;&#20351;&#29992;&#28857;&#20113;&#32593;&#26684;&#22270;&#30340;&#26377;&#30028;&#20111;&#26684;&#65292;&#31532;&#20108;&#31867;&#31639;&#27861;&#21017;&#20351;&#29992;&#28857;&#20113;&#30340;&#27969;&#34892;&#30340;&#949;-&#26368;&#36817;&#37051;&#22270;&#34920;&#31034;&#26041;&#27861;&#12290;&#20004;&#31181;&#31639;&#27861;&#37117;&#21487;&#20197;&#34987;&#30475;&#20316; Fast Multipole Methods(FMMs) &#30340;&#21487;&#34892;&#26367;&#20195;&#65292;&#20294;&#36866;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#25991;&#31456;&#37325;&#28857;&#30740;&#31350;&#22522;&#20110;&#28857;&#20043;&#38388;&#27493;&#38271;&#20998;&#24067;&#65288;&#22914;&#26368;&#30701;&#36335;&#24452;&#36317;&#31163;&#65289;&#25152;&#24341;&#21457;&#30340;&#20960;&#20309;&#23398;&#12290;&#36890;&#36807;&#25552;&#20379;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25991;&#31456;&#33719;&#24471;&#20102;&#32467;&#26500;&#22270;&#35770;&#30340;&#26032;&#32467;&#26524;&#12290;&#25991;&#31456;&#36824;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#21018;&#24615;&#21644;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#34920;&#38754;&#25554;&#20540;&#65288;&#29305;&#21035;&#26159;&#29992;&#20110;&#32593;&#26684;&#21160;&#24577;&#24314;&#27169;&#65289;&#65292;&#28857;&#20113;&#30340;Wasserstein&#36317;&#31163;&#35745;&#31639;&#20197;&#21450;Gromov-Wasserstein&#21464;&#20307;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two new classes of algorithms for efficient field integration on graphs encoding point clouds. The first class, SeparatorFactorization(SF), leverages the bounded genus of point cloud mesh graphs, while the second class, RFDiffusion(RFD), uses popular epsilon-nearest-neighbor graph representations for point clouds. Both can be viewed as providing the functionality of Fast Multipole Methods (FMMs), which have had a tremendous impact on efficient integration, but for non-Euclidean spaces. We focus on geometries induced by distributions of walk lengths between points (e.g., shortest-path distance). We provide an extensive theoretical analysis of our algorithms, obtaining new results in structural graph theory as a byproduct. We also perform exhaustive empirical evaluation, including on-surface interpolation for rigid and deformable objects (particularly for mesh-dynamics modeling), Wasserstein distance computations for point clouds, and the Gromov-Wasserstein variant.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#38543;&#26426;&#32593;&#32476;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102; Bayes &#26368;&#20248;&#27979;&#35797;&#35823;&#24046;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#23725;&#22238;&#24402;&#21644;&#26680;&#22238;&#24402;&#33021;&#22815;&#36798;&#21040;&#26368;&#20248;&#34920;&#29616;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#30340;&#27979;&#35797;&#35823;&#24046;&#20063;&#21487;&#20197;&#20174;&#24179;&#26041;&#32423;&#30340;&#26679;&#26412;&#25968;&#37327;&#20013;&#33719;&#24471;&#25509;&#36817;&#20110;&#38646;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.00375</link><description>&lt;p&gt;
&#28145;&#24230;&#38543;&#26426;&#32593;&#32476;&#30340; Bayes &#26368;&#20248;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayes-optimal Learning of Deep Random Networks of Extensive-width. (arXiv:2302.00375v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#38543;&#26426;&#32593;&#32476;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102; Bayes &#26368;&#20248;&#27979;&#35797;&#35823;&#24046;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#23725;&#22238;&#24402;&#21644;&#26680;&#22238;&#24402;&#33021;&#22815;&#36798;&#21040;&#26368;&#20248;&#34920;&#29616;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#30340;&#27979;&#35797;&#35823;&#24046;&#20063;&#21487;&#20197;&#20174;&#24179;&#26041;&#32423;&#30340;&#26679;&#26412;&#25968;&#37327;&#20013;&#33719;&#24471;&#25509;&#36817;&#20110;&#38646;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#23398;&#20064;&#19968;&#20010;&#28145;&#24230;&#24191;&#24230;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20854;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26679;&#26412;&#25968;&#37327;&#12289;&#36755;&#20837;&#32500;&#25968;&#21644;&#32593;&#32476;&#23485;&#24230;&#25104;&#27604;&#20363;&#22686;&#21152;&#26102;&#30340;&#28176;&#36817;&#24773;&#20917;&#65292;&#24182;&#20026;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#25552;&#20986;&#20102; Bayes &#26368;&#20248;&#27979;&#35797;&#35823;&#24046;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35745;&#31639;&#20102;&#23725;&#22238;&#24402;&#12289;&#26680;&#20989;&#25968;&#21644;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#30340;&#27979;&#35797;&#35823;&#24046;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26368;&#20248;&#27491;&#21017;&#21270;&#30340;&#23725;&#22238;&#24402;&#20197;&#21450;&#26680;&#22238;&#24402;&#21487;&#20197;&#36798;&#21040; Bayes &#26368;&#20248;&#34920;&#29616;&#65292;&#32780;&#36923;&#36753;&#25439;&#22833;&#20989;&#25968;&#23545;&#20110;&#20998;&#31867;&#38382;&#39064;&#20960;&#20046;&#33021;&#36798;&#21040;&#26368;&#20248;&#30340;&#27979;&#35797;&#35823;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#23383;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#26679;&#26412;&#25968;&#37327;&#22686;&#38271;&#36895;&#24230;&#24555;&#20110;&#32500;&#25968;&#26102;&#65292;&#23725;&#22238;&#24402;&#21644;&#26680;&#26041;&#27861;&#21464;&#24471;&#27425;&#20248;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20174;&#24179;&#26041;&#32423;&#30340;&#26679;&#26412;&#25968;&#37327;&#20013;&#33719;&#24471;&#25509;&#36817;&#20110;&#38646;&#30340;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning a target function corresponding to a deep, extensive-width, non-linear neural network with random Gaussian weights. We consider the asymptotic limit where the number of samples, the input dimension and the network width are proportionally large. We propose a closed-form expression for the Bayes-optimal test error, for regression and classification tasks. We further compute closed-form expressions for the test errors of ridge regression, kernel and random features regression. We find, in particular, that optimally regularized ridge regression, as well as kernel regression, achieve Bayes-optimal performances, while the logistic loss yields a near-optimal test error for classification. We further show numerically that when the number of samples grows faster than the dimension, ridge and kernel methods become suboptimal, while neural networks achieve test error close to zero from quadratically many samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#31561;&#21464;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#65292;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#32676;&#20307;&#23545;&#31216;&#24615;&#26469;&#23454;&#29616;&#26356;&#31232;&#30095;&#30340;&#27169;&#22411;&#65292;&#24182;&#20445;&#25345;&#38544;&#31169;&#20445;&#35777;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;DP-SGD&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#25152;&#38754;&#20020;&#32593;&#32476;&#35268;&#27169;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.13104</link><description>&lt;p&gt;
&#31561;&#21464;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#65306;DP-SGD&#20026;&#20160;&#20040;&#38656;&#35201;&#26356;&#31232;&#30095;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Equivariant Differentially Private Deep Learning: Why DP-SGD Needs Sparser Models. (arXiv:2301.13104v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#31561;&#21464;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#65292;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#32676;&#20307;&#23545;&#31216;&#24615;&#26469;&#23454;&#29616;&#26356;&#31232;&#30095;&#30340;&#27169;&#22411;&#65292;&#24182;&#20445;&#25345;&#38544;&#31169;&#20445;&#35777;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;DP-SGD&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#25152;&#38754;&#20020;&#32593;&#32476;&#35268;&#27169;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#36890;&#36807;&#20462;&#21098;&#21644;&#28155;&#21152;&#22122;&#22768;&#26469;&#38480;&#21046;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#35760;&#24518;&#30340;&#31169;&#26377;&#20449;&#24687;&#37327;&#12290;&#22240;&#27492;&#65292;&#20855;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#32593;&#32476;&#38656;&#35201;&#30456;&#24212;&#26356;&#24378;&#30340;&#25200;&#21160;&#65292;&#23548;&#33268;&#22823;&#22411;&#27169;&#22411;&#38590;&#20197;&#23398;&#20064;&#26377;&#29992;&#20449;&#24687;&#65292;&#20351;&#24471;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#35757;&#32451;&#20219;&#21153;&#19978;&#20351;&#29992;DP-SGD&#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#36890;&#36807;&#35757;&#32451;&#36866;&#24212;&#24615;&#25216;&#26415;&#65292;&#22914;&#22823;&#35268;&#27169;&#25968;&#25454;&#22686;&#24378;&#21644;&#22823;&#25209;&#37327;&#22823;&#23567;&#65292;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#25216;&#26415;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;DP-SGD&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#24182;&#38477;&#20302;&#20102;&#20854;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#31232;&#30095;&#27169;&#22411;&#35774;&#35745;&#21407;&#21017;&#26469;&#35299;&#20915;&#36825;&#20123;&#22797;&#26434;&#20219;&#21153;&#65292;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#30701;&#30340;&#26102;&#38388;&#65292;&#20174;&#32780;&#25104;&#20026;DP-SGD&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#31561;&#21464;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#26469;&#23454;&#29616;&#36825;&#31181;&#31232;&#30095;&#24615;&#35774;&#35745;&#65292;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#32676;&#20307;&#23545;&#31216;&#24615;&#26469;&#33719;&#24471;&#26356;&#31232;&#30095;&#30340;&#27169;&#22411;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Stochastic Gradient Descent (DP-SGD) limits the amount of private information deep learning models can memorize during training. This is achieved by clipping and adding noise to the model's gradients, and thus networks with more parameters require proportionally stronger perturbation. As a result, large models have difficulties learning useful information, rendering training with DP-SGD exceedingly difficult on more challenging training tasks. Recent research has focused on combating this challenge through training adaptations such as heavy data augmentation and large batch sizes. However, these techniques further increase the computational overhead of DP-SGD and reduce its practical applicability. In this work, we propose using the principle of sparse model design to solve precisely such complex tasks with fewer parameters, higher accuracy, and in less time, thus serving as a promising direction for DP-SGD. We achieve such sparsity by design by introducing equiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;MPNN&#19982;&#22270;&#36716;&#25442;&#22120;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#24182;&#35777;&#26126;&#20102;&#24102;&#26377;&#34394;&#25311;&#33410;&#28857;&#30340;MPNN&#21487;&#20197;&#20219;&#24847;&#36924;&#36817;GT&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38416;&#26126;&#20102;&#20004;&#31181;&#22270;&#23398;&#20064;&#33539;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#33021;&#21147;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2301.11956</link><description>&lt;p&gt;
MPNN&#19982;&#22270;&#36716;&#25442;&#22120;&#20043;&#38388;&#30340;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
On the Connection Between MPNN and Graph Transformer. (arXiv:2301.11956v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;MPNN&#19982;&#22270;&#36716;&#25442;&#22120;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#24182;&#35777;&#26126;&#20102;&#24102;&#26377;&#34394;&#25311;&#33410;&#28857;&#30340;MPNN&#21487;&#20197;&#20219;&#24847;&#36924;&#36817;GT&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38416;&#26126;&#20102;&#20004;&#31181;&#22270;&#23398;&#20064;&#33539;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#33021;&#21147;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#23398;&#20064;&#31639;&#27861;&#33539;&#20363;&#8212;&#8212;&#22270;&#36716;&#25442;&#22120;&#65288;GT&#65289;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#27969;&#34892;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#20301;&#32622;&#23884;&#20837;&#65292;GT&#21487;&#20197;&#20219;&#24847;&#36924;&#36817;MPNN&#65292;&#36825;&#24847;&#21619;&#30528;GT&#33267;&#23569;&#19982;MPNN&#19968;&#26679;&#24378;&#22823;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21453;&#21521;&#36830;&#25509;&#65292;&#24182;&#23637;&#31034;&#20102;&#24102;&#26377;&#34394;&#25311;&#33410;&#28857;&#65288;VN&#65289;&#30340;MPNN&#36275;&#22815;&#24378;&#22823;&#65292;&#21487;&#20197;&#20219;&#24847;&#36924;&#36817;GT&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22914;&#26524;&#32771;&#34385;&#19968;&#31181;&#32447;&#24615;&#21464;&#25442;&#22120;&#8212;&#8212;&#25152;&#35859;&#30340;&#34920;&#29616;&#32773;/&#32447;&#24615;&#21464;&#25442;&#22120;&#65292;&#21017;&#20855;&#26377;O&#65288;1&#65289;&#28145;&#24230;&#21644;O&#65288;1&#65289;&#23485;&#24230;&#30340;MPNN + VN&#21487;&#20197;&#36924;&#36817;&#34920;&#29616;&#32773;/&#32447;&#24615;&#21464;&#25442;&#22120;&#30340;&#33258;&#25105;&#27880;&#24847;&#23618;&#12290;&#25509;&#19979;&#26469;&#65292;&#36890;&#36807;MPNN + VN&#19982;DeepSets&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;O(n^d)&#23485;&#24230;&#21644;O(1)&#28145;&#24230;&#30340;MPNN + VN&#21487;&#20197;&#36924;&#36817;GT&#30340;&#20219;&#20309;&#23618;&#65292;&#20854;&#20013;n&#26159;&#22270;&#20013;&#30340;&#33410;&#28857;&#25968;&#65292;d&#26159;&#22270;&#30340;&#30452;&#24452;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38416;&#26126;&#20102;&#20004;&#31181;&#22270;&#23398;&#20064;&#33539;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#33021;&#21147;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Transformer (GT) recently has emerged as a new paradigm of graph learning algorithms, outperforming the previously popular Message Passing Neural Network (MPNN) on multiple benchmarks. Previous work (Kim et al., 2022) shows that with proper position embedding, GT can approximate MPNN arbitrarily well, implying that GT is at least as powerful as MPNN. In this paper, we study the inverse connection and show that MPNN with virtual node (VN), a commonly used heuristic with little theoretical understanding, is powerful enough to arbitrarily approximate the self-attention layer of GT.  In particular, we first show that if we consider one type of linear transformer, the so-called Performer/Linear Transformer (Choromanski et al., 2020; Katharopoulos et al., 2020), then MPNN + VN with only O(1) depth and O(1) width can approximate a self-attention layer in Performer/Linear Transformer. Next, via a connection between MPNN + VN and DeepSets, we prove the MPNN + VN with O(n^d) width and O(1)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; PLay&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#25351;&#21335;&#22312;&#30690;&#37327;&#22270;&#31354;&#38388;&#20013;&#29983;&#25104;&#21442;&#25968;&#21270;&#26465;&#20214;&#24067;&#23616;&#12290;&#30456;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#20026;&#19987;&#19994;&#24067;&#23616;&#35774;&#35745;&#24102;&#26469;&#20102;&#26032;&#39062;&#21644;&#20114;&#21160;&#30340;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2301.11529</link><description>&lt;p&gt;
PLay&#65306;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#29983;&#25104;&#21442;&#25968;&#26465;&#20214;&#21270;&#30340;&#24067;&#23616;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
PLay: Parametrically Conditioned Layout Generation using Latent Diffusion. (arXiv:2301.11529v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; PLay&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#25351;&#21335;&#22312;&#30690;&#37327;&#22270;&#31354;&#38388;&#20013;&#29983;&#25104;&#21442;&#25968;&#21270;&#26465;&#20214;&#24067;&#23616;&#12290;&#30456;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#20026;&#19987;&#19994;&#24067;&#23616;&#35774;&#35745;&#24102;&#26469;&#20102;&#26032;&#39062;&#21644;&#20114;&#21160;&#30340;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23616;&#35774;&#35745;&#26159;&#21508;&#31181;&#35774;&#35745;&#39046;&#22495;&#65288;&#21253;&#25324;&#29992;&#25143;&#30028;&#38754;&#12289;&#25991;&#26723;&#21644;&#22270;&#24418;&#35774;&#35745;&#65289;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#30001;&#20110;&#27492;&#20219;&#21153;&#38656;&#35201;&#35774;&#35745;&#24072;&#20184;&#20986;&#32321;&#29712;&#30340;&#25163;&#21160;&#21162;&#21147;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#23581;&#35797;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#33258;&#21160;&#21270;&#27492;&#36807;&#31243;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#30452;&#35266;&#30340;&#29992;&#25143;&#25511;&#21046;&#21644;&#23454;&#29616;&#35774;&#35745;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; PLay&#65292;&#23427;&#20174;&#29992;&#25143;&#25351;&#23450;&#30340;&#25351;&#21335;&#20013;&#29983;&#25104;&#30690;&#37327;&#22270;&#31354;&#38388;&#20013;&#30340;&#21442;&#25968;&#21270;&#26465;&#20214;&#24067;&#23616;&#12290;&#36825;&#20123;&#25351;&#21335;&#36890;&#24120;&#30001;&#35774;&#35745;&#24072;&#22312;&#24403;&#21069;&#23454;&#36341;&#20013;&#29992;&#20110;&#34920;&#31034;&#20182;&#20204;&#30340;&#35774;&#35745;&#24847;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324; FID &#21644; FD-VG &#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#29992;&#25143;&#30740;&#31350;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#23427;&#20026;&#19987;&#19994;&#24067;&#23616;&#35774;&#35745;&#27969;&#31243;&#24102;&#26469;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#20114;&#21160;&#30340;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Layout design is an important task in various design fields, including user interface, document, and graphic design. As this task requires tedious manual effort by designers, prior works have attempted to automate this process using generative models, but commonly fell short of providing intuitive user controls and achieving design objectives. In this paper, we build a conditional latent diffusion model, PLay, that generates parametrically conditioned layouts in vector graphic space from user-specified guidelines, which are commonly used by designers for representing their design intents in current practices. Our method outperforms prior works across three datasets on metrics including FID and FD-VG, and in user study. Moreover, it brings a novel and interactive experience to professional layout design processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#30340;&#27010;&#29575;&#22240;&#26524;&#30693;&#35782;&#24212;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30896;&#25758;&#22238;&#24402;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#20551;&#35774;&#31354;&#38388;&#20026;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26102;&#20855;&#26377;&#20005;&#26684;&#27491;&#30340;&#24191;&#20041;&#25910;&#30410;&#65292;&#22312;&#21512;&#25104;&#21644;&#27668;&#20505;&#27169;&#22411;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11214</link><description>&lt;p&gt;
&#22238;&#25253;&#24681;&#24800;&#65306;&#22238;&#24402;&#22914;&#20309;&#20174;&#27010;&#29575;&#22240;&#26524;&#30693;&#35782;&#20013;&#21463;&#30410;
&lt;/p&gt;
&lt;p&gt;
Returning The Favour: When Regression Benefits From Probabilistic Causal Knowledge. (arXiv:2301.11214v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#30340;&#27010;&#29575;&#22240;&#26524;&#30693;&#35782;&#24212;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30896;&#25758;&#22238;&#24402;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#20551;&#35774;&#31354;&#38388;&#20026;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26102;&#20855;&#26377;&#20005;&#26684;&#27491;&#30340;&#24191;&#20041;&#25910;&#30410;&#65292;&#22312;&#21512;&#25104;&#21644;&#27668;&#20505;&#27169;&#22411;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#22238;&#24402;&#20219;&#21153;&#20013;&#32463;&#24120;&#34987;&#24573;&#30053;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;DAG&#20013;&#30001;&#30896;&#25758;&#32467;&#26500;&#24341;&#36215;&#30340;&#29420;&#31435;&#24615;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#36825;&#21487;&#20197;&#38480;&#21046;&#22238;&#24402;&#20551;&#35774;&#31354;&#38388;&#24182;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30896;&#25758;&#22238;&#24402;&#65292;&#19968;&#31181;&#23558;&#19968;&#20010;&#30896;&#25758;&#20013;&#30340;&#27010;&#29575;&#22240;&#26524;&#30693;&#35782;&#32435;&#20837;&#22238;&#24402;&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;&#24403;&#20551;&#35774;&#31354;&#38388;&#20026;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#20855;&#26377;&#20005;&#26684;&#27491;&#30340;&#24191;&#20041;&#25910;&#30410;&#65292;&#24182;&#25552;&#20379;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38381;&#21512;&#24418;&#24335;&#20272;&#35745;&#37327;&#12290;&#22312;&#21512;&#25104;&#21644;&#27668;&#20505;&#27169;&#22411;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A directed acyclic graph (DAG) provides valuable prior knowledge that is often discarded in regression tasks in machine learning. We show that the independences arising from the presence of collider structures in DAGs provide meaningful inductive biases, which constrain the regression hypothesis space and improve predictive performance. We introduce collider regression, a framework to incorporate probabilistic causal knowledge from a collider in a regression problem. When the hypothesis space is a reproducing kernel Hilbert space, we prove a strictly positive generalisation benefit under mild assumptions and provide closed-form estimators of the empirical risk minimiser. Experiments on synthetic and climate model data demonstrate performance gains of the proposed methodology.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#23621;&#21516;&#36136;&#24615;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476; (NHGCN) &#27169;&#22411;&#65292;&#21033;&#29992;&#26032;&#25351;&#26631; Neighborhood Homophily (NH) &#27979;&#37327;&#33410;&#28857;&#37051;&#22495;&#20013;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#25110;&#32431;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126; NHGCN &#27169;&#22411;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#24182;&#26174;&#33879;&#36229;&#36807;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.09851</link><description>&lt;p&gt;
&#22522;&#20110;&#37051;&#23621;&#21516;&#36136;&#24615;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Neighborhood Homophily-based Graph Convolutional Network. (arXiv:2301.09851v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#23621;&#21516;&#36136;&#24615;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476; (NHGCN) &#27169;&#22411;&#65292;&#21033;&#29992;&#26032;&#25351;&#26631; Neighborhood Homophily (NH) &#27979;&#37327;&#33410;&#28857;&#37051;&#22495;&#20013;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#25110;&#32431;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126; NHGCN &#27169;&#22411;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#24182;&#26174;&#33879;&#36229;&#36807;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#24418;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#26159;&#24322;&#26500;&#30340;&#65292;&#36825;&#25361;&#25112;&#20102;&#32463;&#20856;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#36136;&#24615;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#26222;&#36866;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#30740;&#31350;&#21152;&#28145;&#32593;&#32476;&#25110;&#36830;&#25509;&#20013;&#38388;&#34920;&#31034;&#65292;&#20294;&#36825;&#24182;&#19981;&#20250;&#26412;&#36136;&#19978;&#25913;&#21464;&#37051;&#23621;&#32858;&#21512;&#24182;&#24341;&#20837;&#22122;&#22768;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;&#25351;&#26631;&#26469;&#34920;&#24449;&#21516;&#36136;&#24615;&#65292;&#20294;&#24456;&#23569;&#32771;&#34385;&#25152;&#25552;&#20986;&#25351;&#26631;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631; Neighborhood Homophily (NH)&#65292;&#29992;&#20110;&#27979;&#37327;&#33410;&#28857;&#37051;&#22495;&#20013;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#25110;&#32431;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35813;&#25351;&#26631;&#34701;&#20837;&#21040;&#32463;&#20856;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476; (GCN) &#32467;&#26500;&#20013;&#65292;&#25552;&#20986;&#20102; NHGCN &#27169;&#22411;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#37051;&#23621;&#34987;&#39044;&#20272;&#30340; NH &#20540;&#20998;&#32452;&#65292;&#20174;&#19981;&#21516;&#36890;&#36947;&#36827;&#34892;&#32858;&#21512;&#12290;&#22810;&#39033;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;NHGCN &#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have been proved powerful in graph-oriented tasks. However, many real-world graphs are heterophilous, challenging the homophily assumption of classical GNNs. To solve the universality problem, many studies deepen networks or concatenate intermediate representations, which does not inherently change neighbor aggregation and introduces noise. Recent studies propose new metrics to characterize the homophily, but rarely consider the correlation of the proposed metrics and models. In this paper, we first design a new metric, Neighborhood Homophily (\textit{NH}), to measure the label complexity or purity in node neighborhoods. Furthermore, we incorporate the metric into the classical graph convolutional network (GCN) architecture and propose \textbf{N}eighborhood \textbf{H}omophily-based \textbf{G}raph \textbf{C}onvolutional \textbf{N}etwork (\textbf{NHGCN}). In this framework, neighbors are grouped by estimated \textit{NH} values and aggregated from different ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Peekaboo&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#20351;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#35821;&#20041;&#32454;&#20998;&#24182;&#22522;&#30784;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#37325;&#26032;&#22521;&#35757;&#12290;&#36825;&#39033;&#25216;&#26415;&#30340;&#25512;&#29702;&#26102;&#38388;&#20248;&#21270;&#36807;&#31243;&#21487;&#20197;&#22312;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#20851;&#32852;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20998;&#21106;&#25513;&#27169;&#12290;</title><link>http://arxiv.org/abs/2211.13224</link><description>&lt;p&gt;
Peekaboo&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26159;&#38646;-shot&#32454;&#20998;&#22120;
&lt;/p&gt;
&lt;p&gt;
Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors. (arXiv:2211.13224v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Peekaboo&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#20351;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#35821;&#20041;&#32454;&#20998;&#24182;&#22522;&#30784;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#37325;&#26032;&#22521;&#35757;&#12290;&#36825;&#39033;&#25216;&#26415;&#30340;&#25512;&#29702;&#26102;&#38388;&#20248;&#21270;&#36807;&#31243;&#21487;&#20197;&#22312;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#20851;&#32852;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20998;&#21106;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#21019;&#24314;&#36924;&#30495;&#22270;&#20687;&#26041;&#38754;&#30340;&#26174;&#30528;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#40092;&#26377;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#23450;&#20301;&#25110;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#27809;&#26377;&#25509;&#35302;&#21040;&#26412;&#22320;&#21270;&#20449;&#24687;&#30340;&#35757;&#32451;&#22914;&#20309;&#22312;&#26080;&#38656;&#32454;&#20998;&#29305;&#23450;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20026;&#26465;&#20214;&#24314;&#31435;&#21508;&#31181;&#35821;&#20041;&#30701;&#35821;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25512;&#29702;&#26102;&#38388;&#20248;&#21270;&#36807;&#31243;&#65292;&#33021;&#22815;&#22312;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#20851;&#32852;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20998;&#21106;&#25513;&#27169;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#8212;&#8212;Peekaboo&#65292;&#26159;&#19968;&#31181;&#39318;&#27454;&#26080;&#35757;&#32451;&#24320;&#25918;&#35789;&#27719;&#26080;&#30417;&#30563;&#35821;&#20041;&#25509;&#22320;&#25216;&#26415;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Pascal VOC&#25968;&#25454;&#38598;&#19978;&#23545;Peekaboo&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#35821;&#20041;&#32454;&#20998;&#65292;&#20197;&#21450;&#22312;RefCOCO&#25968;&#25454;&#38598;&#19978;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#29992;&#20110;&#24341;&#29992;&#32454;&#20998;&#65292;&#26174;&#31034;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#30340;&#31454;&#20105;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;Peekaboo&#36890;&#36807;&#26465;&#20214;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#29983;&#25104;&#24102;&#36879;&#26126;&#32972;&#26223;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, text-to-image diffusion models have shown remarkable capabilities in creating realistic images from natural language prompts. However, few works have explored using these models for semantic localization or grounding. In this work, we explore how an off-the-shelf text-to-image diffusion model, trained without exposure to localization information, can ground various semantic phrases without segmentation-specific re-training. We introduce an inference time optimization process capable of generating segmentation masks conditioned on natural language prompts. Our proposal, Peekaboo, is a first-of-its-kind zero-shot, open-vocabulary, unsupervised semantic grounding technique leveraging diffusion models without any training. We evaluate Peekaboo on the Pascal VOC dataset for unsupervised semantic segmentation and the RefCOCO dataset for referring segmentation, showing results competitive with promising results. We also demonstrate how Peekaboo can be used to generate images with tr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#38544;&#24335;&#27969;&#20307;&#38381;&#21512;&#39033;&#30340;&#22810;&#30697;&#20415;&#27969;&#20307;&#27169;&#22411;&#65292;&#33021;&#22815;&#37325;&#29616;Landau&#38459;&#23612;&#30340;&#21160;&#21147;&#23398;&#29305;&#24449;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#19968;&#20010;&#34920;&#24449;&#27969;&#20307;-&#32454;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#27969;&#20307;&#27169;&#22411;&#30340;&#24212;&#29992;&#33539;&#22260;&#25193;&#23637;&#21040;&#22810;&#23610;&#24230;&#31995;&#32479;&#65292;&#20026;&#30740;&#31350;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#24378;&#22823;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2211.01021</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;Landau&#38459;&#23612;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Modeling of Landau Damping by Physics-Informed Neural Networks. (arXiv:2211.01021v2 [physics.plasm-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#38544;&#24335;&#27969;&#20307;&#38381;&#21512;&#39033;&#30340;&#22810;&#30697;&#20415;&#27969;&#20307;&#27169;&#22411;&#65292;&#33021;&#22815;&#37325;&#29616;Landau&#38459;&#23612;&#30340;&#21160;&#21147;&#23398;&#29305;&#24449;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#19968;&#20010;&#34920;&#24449;&#27969;&#20307;-&#32454;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#27969;&#20307;&#27169;&#22411;&#30340;&#24212;&#29992;&#33539;&#22260;&#25193;&#23637;&#21040;&#22810;&#23610;&#24230;&#31995;&#32479;&#65292;&#20026;&#30740;&#31350;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#24378;&#22823;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#35266;&#31561;&#31163;&#23376;&#20307;&#29289;&#29702;&#38382;&#39064;&#20013;&#65292;&#21160;&#21147;&#23398;&#26041;&#27861;&#19968;&#33324;&#24456;&#20934;&#30830;&#65292;&#20294;&#23545;&#20110;&#22823;&#23610;&#24230;&#25110;&#22810;&#23610;&#24230;&#31995;&#32479;&#32780;&#35328;&#35745;&#31639;&#24320;&#38144;&#36739;&#22823;&#12290;&#31561;&#31163;&#23376;&#20307;&#29289;&#29702;&#23398;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#22914;&#20309;&#23558;&#21160;&#21147;&#23398;&#29289;&#29702;&#23398;&#25972;&#21512;&#21040;&#27969;&#20307;&#27169;&#22411;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#22797;&#26434;&#30340;&#35299;&#26512;&#38381;&#21512;&#39033;&#23454;&#29616;&#12290;&#26412;&#30740;&#31350;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#30697;&#20415;&#27969;&#20307;&#27169;&#22411;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#21253;&#21547;&#38544;&#24335;&#27969;&#20307;&#38381;&#21512;&#39033;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#21644;&#26799;&#24230;&#22686;&#24378;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;gPINN&#65289;&#23545;&#23569;&#37327;&#31232;&#30095;&#37319;&#26679;&#30340;&#21160;&#21147;&#23398;&#20223;&#30495;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#20351;&#29992;PINN&#25110;gPINN&#26500;&#24314;&#30340;&#22810;&#30697;&#20415;&#27969;&#20307;&#27169;&#22411;&#33021;&#22815;&#37325;&#29616;Landau&#38459;&#23612;&#30340;&#30005;&#22330;&#33021;&#37327;&#26102;&#21464;&#28436;&#21270;&#65292;&#21253;&#25324;&#20854;&#38459;&#23612;&#29575;&#65292;&#20197;&#21450;&#21160;&#21147;&#23398;&#20223;&#30495;&#20013;&#30340;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#21464;&#37327;&#65292;&#21363;&#22810;&#30697;&#20415;&#38647;&#35834;&#24212;&#21147;&#24352;&#37327;&#65292;&#35813;&#21464;&#37327;&#34920;&#24449;&#27969;&#20307;-&#32454;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#65292;&#26159;&#29702;&#35299;&#31561;&#31163;&#23376;&#20307;&#22810;&#23610;&#24230;&#25928;&#24212;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#23558;&#27969;&#20307;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#25193;&#23637;&#21040;&#22810;&#23610;&#24230;&#31995;&#32479;&#65292;&#20026;&#30740;&#31350;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kinetic approaches are generally accurate in dealing with microscale plasma physics problems but are computationally expensive for large-scale or multiscale systems. One of the long-standing problems in plasma physics is the integration of kinetic physics into fluid models, which is often achieved through sophisticated analytical closure terms. In this study, we successfully construct a multi-moment fluid model with an implicit fluid closure included in the neural network using machine learning. The multi-moment fluid model is trained with a small fraction of sparsely sampled data from kinetic simulations of Landau damping, using the physics-informed neural network (PINN) and the gradient-enhanced physics-informed neural network (gPINN). The multi-moment fluid model constructed using either PINN or gPINN reproduces the time evolution of the electric field energy, including its damping rate, and the plasma dynamics from the kinetic simulations. For the first time, we introduce a new var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861; DDA&#65292;&#36890;&#36807;&#23558;&#27979;&#35797;&#36755;&#20837;&#25237;&#24433;&#21040;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#28304;&#22495;&#20013;&#26469;&#26356;&#26032;&#30446;&#26631;&#25968;&#25454;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#26356;&#20026;&#31283;&#20581;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#25439;&#22351;&#12289;&#26550;&#26500;&#21644;&#25968;&#25454;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2207.03442</link><description>&lt;p&gt;
&#22238;&#21040;&#28304;&#22836;: &#25193;&#25955;&#39537;&#21160;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Back to the Source: Diffusion-Driven Test-Time Adaptation. (arXiv:2207.03442v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861; DDA&#65292;&#36890;&#36807;&#23558;&#27979;&#35797;&#36755;&#20837;&#25237;&#24433;&#21040;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#28304;&#22495;&#20013;&#26469;&#26356;&#26032;&#30446;&#26631;&#25968;&#25454;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#26356;&#20026;&#31283;&#20581;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#25439;&#22351;&#12289;&#26550;&#26500;&#21644;&#25968;&#25454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#21033;&#29992;&#27979;&#35797;&#36755;&#20837;&#26469;&#25552;&#39640;&#28304;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#32463;&#36807;&#31227;&#21160;&#30340;&#30446;&#26631;&#25968;&#25454;&#26102;&#30340;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#30446;&#26631;&#22495;&#20013;&#37325;&#22797;&#35757;&#32451;&#26469;&#26356;&#26032;&#28304;&#27169;&#22411;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#37325;&#26032;&#35757;&#32451;&#23545;&#20110;&#25968;&#25454;&#37327;&#21644;&#39034;&#24207;&#20197;&#21450;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#38750;&#24120;&#25935;&#24863;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#25152;&#26377;&#27979;&#35797;&#36755;&#20837;&#25237;&#24433;&#21040;&#20855;&#26377;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#28304;&#22495;&#20013;&#26469;&#26356;&#26032;&#30446;&#26631;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#25193;&#25955;&#39537;&#21160;&#36866;&#24212;&#26041;&#27861; DDA &#22312;&#25152;&#26377;&#22495;&#20013;&#20849;&#20139;&#20854;&#29992;&#20110;&#20998;&#31867;&#21644;&#29983;&#25104;&#30340;&#27169;&#22411;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#28304;&#22495;&#20013;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#27979;&#35797;&#26399;&#38388;&#22266;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#22270;&#20687;&#24341;&#23548;&#21644;&#33258;&#23398;&#20064;&#26469;&#22686;&#24378;&#25193;&#25955;&#65292;&#20197;&#33258;&#21160;&#20915;&#23450;&#36866;&#24212;&#31243;&#24230;&#12290;&#30456;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#65292;DDA &#30340;&#36755;&#20837;&#36866;&#24212;&#23545;&#20110; ImageNet-C &#25968;&#25454;&#38598;&#20013;&#21508;&#31181;&#25439;&#22351;&#12289;&#26550;&#26500;&#21644;&#25968;&#25454;&#24773;&#20917;&#26356;&#20026;&#31283;&#20581;&#12290;&#22312;&#36755;&#20837;&#36880;&#20010;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#65292;DDA &#25104;&#21151;&#22320;&#20811;&#26381;&#20102;&#27169;&#22411;&#36866;&#24212;&#25152;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time adaptation harnesses test inputs to improve the accuracy of a model trained on source data when tested on shifted target data. Existing methods update the source model by (re-)training on each target domain. While effective, re-training is sensitive to the amount and order of the data and the hyperparameters for optimization. We instead update the target data, by projecting all test inputs toward the source domain with a generative diffusion model. Our diffusion-driven adaptation method, DDA, shares its models for classification and generation across all domains. Both models are trained on the source domain, then fixed during testing. We augment diffusion with image guidance and self-ensembling to automatically decide how much to adapt. Input adaptation by DDA is more robust than prior model adaptation approaches across a variety of corruptions, architectures, and data regimes on the ImageNet-C benchmark. With its input-wise updates, DDA succeeds where model adaptation degrad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#35299;&#20915;&#38797;&#28857;&#38382;&#39064;&#30340;&#32553;&#25918;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#35299;&#20915;GAN&#35757;&#32451;&#38382;&#39064;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.08303</link><description>&lt;p&gt;
&#20851;&#20110;&#35299;&#20915;&#38797;&#28857;&#38382;&#39064;&#30340;&#32553;&#25918;&#26041;&#27861;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Scaled Methods for Saddle Point Problems. (arXiv:2206.08303v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35299;&#20915;&#38797;&#28857;&#38382;&#39064;&#30340;&#32553;&#25918;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#35299;&#20915;GAN&#35757;&#32451;&#38382;&#39064;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#33258;&#36866;&#24212;&#32553;&#25918;&#26041;&#27861;&#22312;&#35299;&#20915;&#38797;&#28857;&#38382;&#39064;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;Adam&#22312;&#35299;&#20915;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#21253;&#25324;GAN&#35757;&#32451;&#26041;&#38754;&#30340;&#27969;&#34892;&#12290;&#26412;&#25991;&#23545;&#20197;&#19979;&#35299;&#20915;SPP&#30340;&#32553;&#25918;&#25216;&#26415;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65306;&#20247;&#25152;&#21608;&#30693;&#30340;Adam&#21644;RmsProp&#32553;&#25918;&#20197;&#21450;&#22522;&#20110;Hutchison&#36817;&#20284;&#20844;&#24335;&#30340;&#36739;&#26032;&#30340;AdaHessian&#21644;OASIS&#12290;&#25105;&#20204;&#20351;&#29992;Extra Gradient&#21450;&#20854;&#24102;&#26377;&#36127;&#21160;&#37327;&#30340;&#25913;&#36827;&#29256;&#26412;&#20316;&#20026;&#22522;&#26412;&#26041;&#27861;&#12290;GAN&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#20165;Adam&#65292;&#32780;&#19988;&#20854;&#20182;&#19981;&#22826;&#27969;&#34892;&#30340;&#26041;&#27861;&#20063;&#20855;&#26377;&#33391;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods with adaptive scaling of different features play a key role in solving saddle point problems, primarily due to Adam's popularity for solving adversarial machine learning problems, including GANS training. This paper carries out a theoretical analysis of the following scaling techniques for solving SPPs: the well-known Adam and RmsProp scaling and the newer AdaHessian and OASIS based on Hutchison approximation. We use the Extra Gradient and its improved version with negative momentum as the basic method. Experimental studies on GANs show good applicability not only for Adam, but also for other less popular methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#24322;&#26500;&#29615;&#22659;&#19979;&#36827;&#34892;&#27169;&#22411;&#24182;&#34892;&#35757;&#32451;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21033;&#29992;&#26356;&#22810;&#20998;&#24067;&#24335;&#12289;&#24322;&#26500;&#21644;&#20302;&#24102;&#23485;&#20114;&#32852;&#35745;&#31639;&#36164;&#28304;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2206.01288</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#22522;&#30784;&#27169;&#22411;&#30340;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Decentralized Training of Foundation Models in Heterogeneous Environments. (arXiv:2206.01288v4 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#24322;&#26500;&#29615;&#22659;&#19979;&#36827;&#34892;&#27169;&#22411;&#24182;&#34892;&#35757;&#32451;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21033;&#29992;&#26356;&#22810;&#20998;&#24067;&#24335;&#12289;&#24322;&#26500;&#21644;&#20302;&#24102;&#23485;&#20114;&#32852;&#35745;&#31639;&#36164;&#28304;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;GPT-3&#21644;PaLM&#65289;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#65292;&#24448;&#24448;&#38656;&#35201;&#25968;&#19975;&#20010;GPU&#36830;&#32493;&#36816;&#34892;&#25968;&#26376;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#19987;&#29992;&#38598;&#32676;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#38598;&#32676;&#25317;&#26377;&#24555;&#36895;&#30340;&#21516;&#26500;&#20114;&#32852;&#21644;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#36719;&#20214;&#31995;&#32479;&#65292;&#25903;&#25345;&#25968;&#25454;&#24182;&#34892;&#21644;&#27169;&#22411;/&#31649;&#36947;&#24182;&#34892;&#12290;&#36825;&#26679;&#30340;&#19987;&#29992;&#38598;&#32676;&#25104;&#26412;&#39640;&#26114;&#19988;&#38590;&#20197;&#33719;&#24471;&#12290;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;&#20998;&#24067;&#24335;&#12289;&#24322;&#26500;&#21644;&#24102;&#23485;&#36739;&#20302;&#30340;&#35745;&#31639;&#36164;&#28304;&#21602;&#65311;&#20197;&#24448;&#30740;&#31350;&#24322;&#26500;&#12289;&#20998;&#24067;&#24335;&#22330;&#26223;&#20027;&#35201;&#20851;&#27880;&#23567;&#22411;&#27169;&#22411;&#65292;&#21487;&#20197;&#20197;&#32431;&#25968;&#25454;&#24182;&#34892;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#29992;&#20110;&#27169;&#22411;&#24182;&#34892;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65288;&#22914;Megatron&#65289;&#20165;&#28041;&#21450;&#21516;&#26500;&#25968;&#25454;&#20013;&#24515;&#29615;&#22659;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#22312;&#20998;&#24067;&#24335;&#24322;&#26500;&#29615;&#22659;&#20013;&#20351;&#29992;&#27169;&#22411;&#24182;&#34892;&#36827;&#34892;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;QHD&#31163;&#32447;&#31574;&#30053;&#65292;&#23427;&#22522;&#20110;&#36229;&#32500;&#24378;&#21270;&#23398;&#20064;&#19982;&#33041;&#21551;&#21457;&#35745;&#31639;&#65292;&#23454;&#29616;&#31283;&#20581;&#21644;&#23454;&#26102;&#23398;&#20064;&#12290;QHD&#30456;&#27604;&#20110;DQN&#20855;&#26377;&#26356;&#39640;&#25928;&#29575;&#19988;&#36866;&#29992;&#20110;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20855;&#26377;&#22312;&#32447;&#21644;&#23454;&#26102;&#23398;&#20064;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2205.06978</link><description>&lt;p&gt;
&#22522;&#20110;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#39640;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Off-Policy Reinforcement Learning via Brain-Inspired Computing. (arXiv:2205.06978v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;QHD&#31163;&#32447;&#31574;&#30053;&#65292;&#23427;&#22522;&#20110;&#36229;&#32500;&#24378;&#21270;&#23398;&#20064;&#19982;&#33041;&#21551;&#21457;&#35745;&#31639;&#65292;&#23454;&#29616;&#31283;&#20581;&#21644;&#23454;&#26102;&#23398;&#20064;&#12290;QHD&#30456;&#27604;&#20110;DQN&#20855;&#26377;&#26356;&#39640;&#25928;&#29575;&#19988;&#36866;&#29992;&#20110;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20855;&#26377;&#22312;&#32447;&#21644;&#23454;&#26102;&#23398;&#20064;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#24320;&#21019;&#20102;&#22686;&#24378;&#29616;&#26377;&#26234;&#33021;&#31995;&#32479;&#30340;&#26032;&#26426;&#20250;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#21253;&#25324;&#22797;&#26434;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;RL&#31639;&#27861;&#65292;&#22914;Deep Q-Networks&#65288;DQN&#65289;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QHD&#65292;&#19968;&#31181;&#22522;&#20110;&#36229;&#32500;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#32447;&#31574;&#30053;&#65292;&#27169;&#20223;&#22823;&#33041;&#30340;&#23646;&#24615;&#65292;&#23454;&#29616;&#31283;&#20581;&#21644;&#23454;&#26102;&#23398;&#20064;&#12290;QHD&#20381;&#38752;&#36731;&#37327;&#32423;&#30340;&#33041;&#21551;&#21457;&#27169;&#22411;&#65292;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#26700;&#38754;&#21644;&#21151;&#29575;&#38480;&#21046;&#30340;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#65292;QHD&#30340;&#25972;&#20307;&#25928;&#29575;&#26174;&#33879;&#20248;&#20110;DQN&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#39640;&#25110;&#21487;&#27604;&#36739;&#30340;&#22238;&#25253;&#12290;QHD&#20063;&#36866;&#29992;&#20110;&#39640;&#24230;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20855;&#26377;&#26497;&#22823;&#30340;&#22312;&#32447;&#21644;&#23454;&#26102;&#23398;&#20064;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#25903;&#25345;&#23567;&#30340;&#32463;&#39564;&#37325;&#25918;&#25209;&#37327;&#65292;&#19982;DQN&#30456;&#27604;&#25552;&#20379;12.3&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#30830;&#20445;&#26368;&#23567;&#30340;&#36136;&#37327;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;QHD&#19982;&#20808;&#36827;&#30340;RL&#31639;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has opened up new opportunities to enhance existing smart systems that generally include a complex decision-making process. However, modern RL algorithms, e.g., Deep Q-Networks (DQN), are based on deep neural networks, resulting in high computational costs. In this paper, we propose QHD, an off-policy value-based Hyperdimensional Reinforcement Learning, that mimics brain properties toward robust and real-time learning. QHD relies on a lightweight brain-inspired model to learn an optimal policy in an unknown environment. On both desktop and power-limited embedded platforms, QHD achieves significantly better overall efficiency than DQN while providing higher or comparable rewards. QHD is also suitable for highly-efficient reinforcement learning with great potential for online and real-time learning. Our solution supports a small experience replay batch size that provides 12.3 times speedup compared to DQN while ensuring minimal quality loss. Our evaluation sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#26799;&#24230;&#21152;&#26435;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.01464</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#26799;&#24230;&#21152;&#26435;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Value Gradient weighted Model-Based Reinforcement Learning. (arXiv:2204.01464v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#26799;&#24230;&#21152;&#26435;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#26159;&#19968;&#31181;&#39640;&#25928;&#33719;&#21462;&#25511;&#21046;&#31574;&#30053;&#30340;&#25216;&#26415;&#65292;&#20294;&#27169;&#22411;&#35823;&#24046;&#24448;&#24448;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;MBRL&#20013;&#30340;&#27169;&#22411;&#36890;&#24120;&#20165;&#29992;&#20110;&#37325;&#24314;&#21160;&#24577;&#65292;&#29305;&#21035;&#26159;&#29366;&#24577;&#35266;&#23519;&#20540;&#65292;&#32780;&#27169;&#22411;&#35823;&#24046;&#23545;&#31574;&#30053;&#30340;&#24433;&#21709;&#19981;&#20250;&#34987;&#35757;&#32451;&#30446;&#26631;&#25429;&#25417;&#21040;&#12290;&#36825;&#23548;&#33268;MBRL&#30340;&#30446;&#26631;&#19982;&#23454;&#38469;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#65292;&#20174;&#32780;&#24433;&#21709;&#31574;&#30053;&#21644;&#20215;&#20540;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#26799;&#24230;&#21152;&#26435;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;VaGraM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning (MBRL) is a sample efficient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely fitted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition would suggest that value-aware model learning would fix this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel method for value-aware model learning which improves the perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21435;&#20559;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;Riesz&#34920;&#24449;&#23884;&#22871;&#22343;&#20540;&#22238;&#24402;&#65292;&#36991;&#20813;&#20102;&#22312;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#20013;&#38656;&#35201;&#35299;&#20915;&#36741;&#21161;&#20542;&#21521;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2203.13887</link><description>&lt;p&gt;
&#21160;&#24577;&#27835;&#30103;&#25928;&#24212;&#21644;&#19968;&#33324;&#23884;&#22871;&#20989;&#25968;&#30340;&#33258;&#21160;&#21435;&#20559;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automatic Debiased Machine Learning for Dynamic Treatment Effects and General Nested Functionals. (arXiv:2203.13887v5 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21435;&#20559;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;Riesz&#34920;&#24449;&#23884;&#22871;&#22343;&#20540;&#22238;&#24402;&#65292;&#36991;&#20813;&#20102;&#22312;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#20013;&#38656;&#35201;&#35299;&#20915;&#36741;&#21161;&#20542;&#21521;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#33258;&#21160;&#21435;&#20559;&#26426;&#22120;&#23398;&#20064;&#30340;&#24605;&#24819;&#25193;&#23637;&#21040;&#20102;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#21644;&#26356;&#19968;&#33324;&#30340;&#23884;&#22871;&#20989;&#25968;&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#31163;&#25955;&#22788;&#29702;&#30340;&#22810;&#37325;&#31283;&#20581;&#20844;&#24335;&#21487;&#20197;&#29992;&#23884;&#22871;&#22343;&#20540;&#22238;&#24402;&#30340;&#36882;&#24402; Riesz &#34920;&#31034;&#26469;&#37325;&#26032;&#34920;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#19968;&#31181;&#36882;&#24402; Riesz &#34920;&#31034;&#20272;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#20272;&#35745;&#21435;&#20559;&#36716;&#21270;&#30340;&#20462;&#27491;&#65292;&#32780;&#26080;&#38656;&#25551;&#36848;&#26657;&#27491;&#39033;&#30340;&#24418;&#24335;&#65292;&#20363;&#22914;&#20498;&#25968;&#27010;&#29575;&#21152;&#26435;&#39033;&#30340;&#20056;&#31215;&#65292;&#22914;&#22312;&#21160;&#24577;&#26426;&#21046;&#20013;&#36827;&#34892;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#20013;&#25152;&#20570;&#30340;&#37027;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#25439;&#22833;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20854;&#26368;&#23567;&#21270;&#22120;&#26159;&#21435;&#20559;&#36716;&#21270;&#30340;&#20462;&#27491;&#30340;&#20056;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#38656;&#35201;&#35299;&#20915;&#36741;&#21161;&#20542;&#21521;&#27169;&#22411;&#65292;&#24182;&#30452;&#25509;&#20248;&#21270;&#30446;&#26631;&#21435;&#20559;&#24615;&#20462;&#27491;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20123;&#23454;&#38469;&#38382;&#39064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend the idea of automated debiased machine learning to the dynamic treatment regime and more generally to nested functionals. We show that the multiply robust formula for the dynamic treatment regime with discrete treatments can be re-stated in terms of a recursive Riesz representer characterization of nested mean regressions. We then apply a recursive Riesz representer estimation learning algorithm that estimates de-biasing corrections without the need to characterize how the correction terms look like, such as for instance, products of inverse probability weighting terms, as is done in prior work on doubly robust estimation in the dynamic regime. Our approach defines a sequence of loss minimization problems, whose minimizers are the mulitpliers of the de-biasing correction, hence circumventing the need for solving auxiliary propensity models and directly optimizing for the mean squared error of the target de-biasing correction. We provide further applications of our approach to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36890;&#20449;&#32593;&#32476;&#20013;&#22788;&#29702;&#36830;&#25509;&#25925;&#38556;&#24341;&#36215;&#30340;&#32602;&#27454;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#29616;&#26377;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#24809;&#32602;&#65292;&#22312;&#23454;&#36341;&#20013;&#36824;&#33719;&#24471;&#20102;&#36229;&#36807;12,000&#20493;&#30340;&#36895;&#24230;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2201.12263</link><description>&lt;p&gt;
RiskNet:&#19981;&#21487;&#38752;&#36164;&#28304;&#32593;&#32476;&#30340;&#31070;&#32463;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RiskNet: Neural Risk Assessment in Networks of Unreliable Resources. (arXiv:2201.12263v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36890;&#20449;&#32593;&#32476;&#20013;&#22788;&#29702;&#36830;&#25509;&#25925;&#38556;&#24341;&#36215;&#30340;&#32602;&#27454;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#29616;&#26377;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#24809;&#32602;&#65292;&#22312;&#23454;&#36341;&#20013;&#36824;&#33719;&#24471;&#20102;&#36229;&#36807;12,000&#20493;&#30340;&#36895;&#24230;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#36890;&#20449;&#32593;&#32476;&#20013;&#30340;&#25925;&#38556;&#24341;&#36215;&#30340;&#32602;&#27454;&#20998;&#24067;&#65292;&#20854;&#20013;&#36830;&#25509;&#21463;&#20849;&#20139;&#20110;&#24037;&#20316;&#36335;&#24452;&#21644;&#22791;&#29992;&#36335;&#24452;&#20043;&#38388;&#30340;&#36164;&#28304;&#20445;&#25252;&#12290;&#35813;GNN&#31639;&#27861;&#20165;&#20351;&#29992;&#36890;&#36807;Barab\'asi-Albert&#27169;&#22411;&#29983;&#25104;&#30340;&#38543;&#26426;&#22270;&#36827;&#34892;&#35757;&#32451;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25152;&#24471;&#30340;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#29616;&#26377;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#24809;&#32602;&#12290;GNN&#28040;&#38500;&#20102;&#22312;&#30740;&#31350;&#32593;&#32476;&#25299;&#25169;&#26102;&#27169;&#25311;&#22797;&#26434;&#25925;&#38556;&#22330;&#26223;&#30340;&#38656;&#35201;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25972;&#20010;&#35774;&#35745;&#25805;&#20316;&#20165;&#21463;&#29616;&#20195;&#30828;&#20214;&#30340;4ms&#30340;&#38480;&#21046;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#36229;&#36807;12,000&#20493;&#30340;&#36895;&#24230;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a graph neural network (GNN)-based method to predict the distribution of penalties induced by outages in communication networks, where connections are protected by resources shared between working and backup paths. The GNN-based algorithm is trained only with random graphs generated with the Barab\'asi-Albert model. Even though, the obtained test results show that we can precisely model the penalties in a wide range of various existing topologies. GNNs eliminate the need to simulate complex outage scenarios for the network topologies under study. In practice, the whole design operation is limited by 4ms on modern hardware. This way, we can gain as much as over 12,000 times in the speed improvement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GraphDINO&#65292;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19977;&#32500;&#31070;&#32463;&#20803;&#24418;&#24577;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#26032;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;AC-Attention&#65292;&#22312;&#22810;&#20010;&#22823;&#33041;&#21306;&#22495;&#20869;&#65292;GraphDINO &#26174;&#31034;&#20986;&#20102;&#20248;&#20110;&#20854;&#23427;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2112.12482</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#22312;&#31070;&#32463;&#20803;&#24418;&#24577;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Graph Representation Learning for Neuronal Morphologies. (arXiv:2112.12482v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GraphDINO&#65292;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19977;&#32500;&#31070;&#32463;&#20803;&#24418;&#24577;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#26032;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;AC-Attention&#65292;&#22312;&#22810;&#20010;&#22823;&#33041;&#21306;&#22495;&#20869;&#65292;GraphDINO &#26174;&#31034;&#20986;&#20102;&#20248;&#20110;&#20854;&#23427;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#22914;&#31070;&#32463;&#31185;&#23398;&#20013;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#23545;&#22823;&#33041;&#20013;&#32454;&#32990;&#31867;&#22411;&#30340;&#22810;&#26679;&#24418;&#24577;&#36827;&#34892;&#24314;&#27169;&#26159;&#20854;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GraphDINO&#65292;&#19968;&#31181;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19977;&#32500;&#31070;&#32463;&#20803;&#24418;&#24577;&#30340;&#20302;&#32500;&#34920;&#31034;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;GraphDINO &#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#29992;&#20110;&#31354;&#38388;&#23884;&#20837;&#24335;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#20351;Transformer&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204; (1)&#24320;&#21457;&#20102;&#38024;&#23545;&#31354;&#38388;&#23884;&#20837;&#24335;&#22270;&#24418;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292; (2) &#23545;&#20301;&#32622;&#32534;&#30721;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292; (3)&#24341;&#20837;&#20102;&#26032;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;AC-Attention&#65292;&#23427;&#32467;&#21512;&#20102;&#33410;&#28857;&#38388;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20840;&#23616;&#20132;&#20114;&#21644;&#20256;&#32479;&#30340;&#22270;&#24418;&#21367;&#31215;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#31181;&#31867;&#30340;&#12289;&#36328;&#36234;&#22810;&#20010;&#22823;&#33041;&#21306;&#22495;&#30340;&#31070;&#32463;&#20803;&#25968;&#25454;&#19978;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;GraphDINO &#22312;&#31070;&#32463;&#20803;&#24418;&#24577;&#23398;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#21040;&#25429;&#25417;&#31070;&#32463;&#20803;&#24418;&#24577;&#23398;&#30456;&#20851;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised graph representation learning has recently gained interest in several application domains such as neuroscience, where modeling the diverse morphology of cell types in the brain is one of the key challenges. It is currently unknown how many excitatory cortical cell types exist and what their defining morphological features are. Here we present GraphDINO, a purely data-driven approach to learn low-dimensional representations of 3D neuronal morphologies from unlabeled large-scale datasets. GraphDINO is a novel transformer-based representation learning method for spatially-embedded graphs. To enable self-supervised learning on transformers, we (1) developed data augmentation strategies for spatially-embedded graphs, (2) adapted the positional encoding and (3) introduced a novel attention mechanism, AC-Attention, which combines attention-based global interaction between nodes and classic graph convolutional processing. We show, in two different species and across multiple brain
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38754;&#23545;&#33258;&#36866;&#24212;&#36873;&#25321;&#25968;&#25454;&#26679;&#26412;&#24341;&#36215;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#20351;&#29992;&#22122;&#22768;&#21152;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#19981;&#20381;&#36182;&#20110;&#26597;&#35810;&#35268;&#27169;&#30340;&#35823;&#24046;&#20445;&#35777;&#65292;&#36825;&#19968;&#32467;&#26524;&#34920;&#26126;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#30340;&#38382;&#39064;&#22312;&#20110;&#26032;&#26597;&#35810;&#19982;&#36807;&#21435;&#26597;&#35810;&#30340;&#21327;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2106.10761</link><description>&lt;p&gt;
&#38754;&#23545;&#36866;&#24212;&#24615;&#27867;&#21270;&#65306;&#36125;&#21494;&#26031;&#35270;&#35282;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalization in the Face of Adaptivity: A Bayesian Perspective. (arXiv:2106.10761v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.10761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38754;&#23545;&#33258;&#36866;&#24212;&#36873;&#25321;&#25968;&#25454;&#26679;&#26412;&#24341;&#36215;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#20351;&#29992;&#22122;&#22768;&#21152;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#19981;&#20381;&#36182;&#20110;&#26597;&#35810;&#35268;&#27169;&#30340;&#35823;&#24046;&#20445;&#35777;&#65292;&#36825;&#19968;&#32467;&#26524;&#34920;&#26126;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#30340;&#38382;&#39064;&#22312;&#20110;&#26032;&#26597;&#35810;&#19982;&#36807;&#21435;&#26597;&#35810;&#30340;&#21327;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#36873;&#25321;&#26679;&#26412;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#65292;&#31616;&#21333;&#30340;&#22122;&#22768;&#21152;&#31639;&#27861;&#21487;&#20197;&#36991;&#20813;&#36825;&#19968;&#38382;&#39064;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22122;&#22768;&#21152;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#19981;&#20381;&#36182;&#20110;&#26597;&#35810;&#35268;&#27169;&#30340;&#35823;&#24046;&#20445;&#35777;&#65292;&#36825;&#19968;&#32467;&#26524;&#26469;&#28304;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#30340;&#38382;&#39064;&#22312;&#20110;&#26032;&#26597;&#35810;&#19982;&#36807;&#21435;&#26597;&#35810;&#30340;&#21327;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Repeated use of a data sample via adaptively chosen queries can rapidly lead to overfitting, wherein the empirical evaluation of queries on the sample significantly deviates from their mean with respect to the underlying data distribution. It turns out that simple noise addition algorithms suffice to prevent this issue, and differential privacy-based analysis of these algorithms shows that they can handle an asymptotically optimal number of queries. However, differential privacy's worst-case nature entails scaling such noise to the range of the queries even for highly-concentrated queries, or introducing more complex algorithms.  In this paper, we prove that straightforward noise-addition algorithms already provide variance-dependent guarantees that also extend to unbounded queries. This improvement stems from a novel characterization that illuminates the core problem of adaptive data analysis. We show that the harm of adaptivity results from the covariance between the new query and a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DeepMVI&#65292;&#19968;&#31181;&#29992;&#20110;&#22635;&#20805;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#32570;&#22833;&#20540;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#31934;&#32454;&#21644;&#31895;&#31890;&#24230;&#27169;&#24335;&#65292;&#20197;&#21450;&#36328;&#20998;&#31867;&#32500;&#24230;&#30340;&#30456;&#20851;&#31995;&#21015;&#36235;&#21183;&#12290;&#32463;&#36807;&#25913;&#36827;&#65292;DeepMVI&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2103.01600</link><description>&lt;p&gt;
&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#30340;&#32570;&#22833;&#20540;&#22635;&#20805;&#26041;&#27861;DeepMVI&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Missing Value Imputation on Multidimensional Time Series. (arXiv:2103.01600v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.01600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeepMVI&#65292;&#19968;&#31181;&#29992;&#20110;&#22635;&#20805;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#32570;&#22833;&#20540;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#31934;&#32454;&#21644;&#31895;&#31890;&#24230;&#27169;&#24335;&#65292;&#20197;&#21450;&#36328;&#20998;&#31867;&#32500;&#24230;&#30340;&#30456;&#20851;&#31995;&#21015;&#36235;&#21183;&#12290;&#32463;&#36807;&#25913;&#36827;&#65292;DeepMVI&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeepMVI&#65292;&#19968;&#31181;&#29992;&#20110;&#22635;&#20805;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#32570;&#22833;&#20540;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#30001;&#20110;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#38271;&#26102;&#38388;&#25968;&#25454;&#32858;&#21512;&#20250;&#20135;&#29983;&#22823;&#37327;&#32570;&#22833;&#25968;&#25454;&#65292;&#22240;&#27492;&#23545;&#20110;&#21487;&#38752;&#30340;&#25968;&#25454;&#20998;&#26512;&#65292;&#38656;&#35201;&#20180;&#32454;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#32467;&#21512;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#31934;&#32454;&#21644;&#31895;&#31890;&#24230;&#27169;&#24335;&#65292;&#20197;&#21450;&#36328;&#20998;&#31867;&#32500;&#24230;&#30340;&#30456;&#20851;&#31995;&#21015;&#36235;&#21183;&#12290;&#22312;&#32463;&#36807;&#22810;&#27425;&#19981;&#25104;&#21151;&#30340;&#23581;&#35797;&#20043;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#24049;&#30340;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#25324;&#20855;&#26377;&#26032;&#22411;&#21367;&#31215;&#31383;&#21475;&#29305;&#24449;&#21644;&#26680;&#22238;&#24402;&#30340;&#26102;&#38388;&#21464;&#24418;&#22120;&#12290;&#22312;&#21508;&#31181;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#22635;&#20805;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27604;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DeepMVI, a deep learning method for missing value imputation in multidimensional time-series datasets. Missing values are commonplace in decision support platforms that aggregate data over long time stretches from disparate sources, and reliable data analytics calls for careful handling of missing data. One strategy is imputing the missing values, and a wide variety of algorithms exist spanning simple interpolation, matrix factorization methods like SVD, statistical models like Kalman filters, and recent deep learning methods. We show that often these provide worse results on aggregate analytics compared to just excluding the missing data. DeepMVI uses a neural network to combine fine-grained and coarse-grained patterns along a time series, and trends from related series across categorical dimensions. After failing with off-the-shelf neural architectures, we design our own network that includes a temporal transformer with a novel convolutional window feature, and kernel regr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEAD&#30340;&#20248;&#21270;&#22120;&#65292;&#23427;&#22522;&#20110;&#29289;&#29702;&#23398;&#20013;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#26469;&#25913;&#36827;&#21338;&#24328;&#20248;&#21270;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#22312;&#20108;&#27425;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20013;&#23637;&#31034;&#20102;&#32447;&#24615;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2010.13846</link><description>&lt;p&gt;
LEAD&#65306;&#29992;&#20110;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#30340;&#26368;&#23567;&#20316;&#29992;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
LEAD: Least-Action Dynamics for Min-Max Optimization. (arXiv:2010.13846v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.13846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEAD&#30340;&#20248;&#21270;&#22120;&#65292;&#23427;&#22522;&#20110;&#29289;&#29702;&#23398;&#20013;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#26469;&#25913;&#36827;&#21338;&#24328;&#20248;&#21270;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#22312;&#20108;&#27425;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20013;&#23637;&#31034;&#20102;&#32447;&#24615;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#24314;&#27169;&#65288;&#22914;&#29983;&#25104;&#24615;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65289;&#37325;&#26032;&#29123;&#36215;&#20102;&#20154;&#20204;&#23545;&#21452;&#20154;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#30340;&#20852;&#36259;&#12290;&#35813;&#31867;&#21338;&#24328;&#20248;&#21270;&#30340;&#19968;&#20010;&#20027;&#35201;&#38590;&#39064;&#26159;&#26059;&#36716;&#21160;&#21147;&#23398;&#38459;&#30861;&#20854;&#25910;&#25947;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21338;&#24328;&#20248;&#21270;&#20849;&#20139;&#31890;&#23376;&#31995;&#32479;&#21463;&#22810;&#37325;&#21147;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#29289;&#29702;&#23398;&#20013;&#30340;&#24037;&#20855;&#26469;&#25913;&#36827;&#20248;&#21270;&#21160;&#21147;&#23398;&#12290;&#21463;&#29289;&#29702;&#26694;&#26550;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#29992;&#20110;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#30340;&#20248;&#21270;&#22120;LEAD&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#26446;&#20122;&#26222;&#35834;&#22827;&#31283;&#23450;&#24615;&#29702;&#35770;&#21644;&#35889;&#20998;&#26512;&#65292;&#22312;&#36830;&#32493;&#21644;&#31163;&#25955;&#26102;&#38388;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;LEAD&#22312;&#19968;&#31867;&#20108;&#27425;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20013;&#30340;&#25910;&#25947;&#29305;&#24615;&#65292;&#23637;&#31034;&#20102;&#32447;&#24615;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;CIFAR-10&#22270;&#20687;&#29983;&#25104;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;GAN&#35757;&#32451;&#20013;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial formulations such as generative adversarial networks (GANs) have rekindled interest in two-player min-max games. A central obstacle in the optimization of such games is the rotational dynamics that hinder their convergence. In this paper, we show that game optimization shares dynamic properties with particle systems subject to multiple forces, and one can leverage tools from physics to improve optimization dynamics. Inspired by the physical framework, we propose LEAD, an optimizer for min-max games. Next, using Lyapunov stability theory and spectral analysis, we study LEAD's convergence properties in continuous and discrete time settings for a class of quadratic min-max games to demonstrate linear convergence to the Nash equilibrium. Finally, we empirically evaluate our method on synthetic setups and CIFAR-10 image generation to demonstrate improvements in GAN training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#24191;&#20102;McDiarmid&#19981;&#31561;&#24335;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20855;&#26377;&#26377;&#30028;&#24046;&#24322;&#30340;&#20989;&#25968;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#19968;&#33324;&#24230;&#37327;&#31354;&#38388;&#30340;&#38598;&#20013;&#24615;&#12290;</title><link>http://arxiv.org/abs/1511.05240</link><description>&lt;p&gt;
McDiarmid&#19981;&#31561;&#24335;&#30340;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
An extension of McDiarmid's inequality. (arXiv:1511.05240v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1511.05240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#24191;&#20102;McDiarmid&#19981;&#31561;&#24335;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20855;&#26377;&#26377;&#30028;&#24046;&#24322;&#30340;&#20989;&#25968;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#19968;&#33324;&#24230;&#37327;&#31354;&#38388;&#30340;&#38598;&#20013;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#25512;&#24191;&#35770;&#35777;&#25512;&#24191;McDiarmid&#19981;&#31561;&#24335;&#65292;&#20351;&#23427;&#36866;&#29992;&#20110;&#20855;&#26377;&#26377;&#30028;&#24046;&#24322;&#30340;&#20989;&#25968;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#39640;&#27010;&#29575;&#38598;&#21512;&#12290;&#36825;&#20123;&#20989;&#25968;&#38598;&#20013;&#20110;&#23427;&#20204;&#30340;&#26465;&#20214;&#26399;&#26395;&#21608;&#22260;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#19968;&#33324;&#24230;&#37327;&#31354;&#38388;&#30340;&#38598;&#20013;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We generalize McDiarmid's inequality for functions with bounded differences on a high probability set, using an extension argument. Those functions concentrate around their conditional expectations. We further extend the results to concentration in general metric spaces.
&lt;/p&gt;</description></item></channel></rss>